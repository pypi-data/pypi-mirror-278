import streamlit as st
import boto3

from bedrock_types import SystemContentBlock, ConverseResponse

client = boto3.client("bedrock-runtime", region_name="us-east-1")
model_id = "meta.llama3-8b-instruct-v1:0"


system_prompt = SystemContentBlock(
    text="You are an economist with access to lots of data."
)

# user_message =


"""
Request Example

POST /model/anthropic.claude-3-sonnet-20240229-v1:0/converse HTTP/1.1
Content-type: application/json

{
    "messages": [
        {
            "role": "user",
            "content": [
                {
                    "text": "Write an article about impact of high inflation to GDP of a country"
                }
            ]
        }
    ],
    "system": [{"text" : "You are an economist with access to lots of data"}]
    "inferenceConfig": {
        "maxTokens": 1000,
        "temperature": 0.5
    }
}
"""


"""
Response Example

HTTP/1.1 200
Content-type: application/json

{
    "output": {
        "message": {
            "content": [
                {
                    "text": "<text generated by the model>"
                }
            ],
            "role": "assistant"
        }
    },
    "stopReason": "end_turn",
    "usage": {
        "inputTokens": 30,
        "outputTokens": 628,
        "totalTokens": 658
    },
    "metrics": {
        "latencyMs": 1275
    }
}
"""


"""
Request with addtional model fields

POST /model/anthropic.claude-3-sonnet-20240229-v1:0/converse HTTP/1.1
Content-type: application/json

{
    "messages": [
        {
            "role": "user",
            "content": [
                {
                    "text": "Provide general steps to debug a BSOD on a Windows laptop."
                }
            ]
        }
    ],
    "system": [{"text" : "You are a tech support expert who helps resolve technical issues. Signal 'SUCCESS' if you can resolve the issue, otherwise 'FAILURE'"}],
    "inferenceConfig": {
        "stopSequences": [ "SUCCESS", "FAILURE" ]
    },
    "additionalModelRequestFields": {
        "top_k": 200
    },
    "additionalModelResponseFieldPaths": [
        "/stop_sequence"
    ]
}

"""

"""
Example response with addtional response
HTTP/1.1 200
Content-type: application/json

{
    "output": {
        "message": {
            "content": [
                {
                    "text": "<text generated by the model>"
                }
            ],
            "role": "assistant"
        }
    },
    "additionalModelResponseFields": {
        "stop_sequence": "SUCCESS"
    },
    "stopReason": "stop_sequence",
    "usage": {
        "inputTokens": 51,
        "outputTokens": 442,
        "totalTokens": 493
    },
    "metrics": {
        "latencyMs": 7944
    }
}
"""


# response = {
#     "output": {
#         "message": {
#             "content": [{"text": "<text generated by the model>"}],
#             "role": "assistant",
#         }
#     },
#     "additionalModelResponseFields": {"stop_sequence": "SUCCESS"},
#     "stopReason": "stop_sequence",
#     "usage": {"inputTokens": 51, "outputTokens": 442, "totalTokens": 493},
#     "metrics": {"latencyMs": 7944},
# }


# parsed = ConverseResponse.model_validate(response)
# print(parsed)
# print(parsed.model_dump_json())


model_id = "anthropic.claude-3-sonnet-20240229-v1:0"
temperature = 0.5
top_k = 200

# Base inference parameters to use.
inference_config = {"temperature": temperature}
# Additional inference parameters to use.
additional_model_fields = {"top_k": top_k}

input_text = "Tell me a joke"
message = {"role": "user", "content": [{"text": input_text}]}
messages = [message]
system_text = system_prompt.text
system_prompts = [{"text": system_text}]

# response = client.converse(
#     modelId=model_id,
#     messages=messages,
#     system=system_prompts,
#     inferenceConfig=inference_config,
#     additionalModelRequestFields=additional_model_fields,
# )


import json
import streamlit as st

# dumped = json.dumps(response, indent=4)
# print(dumped)

# parsed = ConverseResponse.model_validate(response)
# print(parsed.model_dump_json(indent=4, exclude_unset=True))
# print(parsed.ResponseMetadata)


class StationNotFoundError(Exception):
    """Raised when a radio station isn't found."""

    pass


def get_top_song(call_sign):
    """Returns the most popular song for the requested station.
    Args:
        call_sign (str): The call sign for the station for which you want
        the most popular song.

    Returns:
        response (json): The most popular song and artist.
    """

    song = ""
    artist = ""
    if call_sign == "WZPZ":
        song = "Elemental Hotel"
        artist = "8 Storey Hike"

    else:
        raise StationNotFoundError(f"Station {call_sign} not found.")

    return song, artist


input_text = "What is the most popular song on WZPZ?"
messages = [{"role": "user", "content": [{"text": input_text}]}]
# Define the tool to send to the model.
tool_config = {
    "tools": [
        {
            "toolSpec": {
                "name": "top_song",
                "description": "Get the most popular song played on a radio station.",
                "inputSchema": {
                    "json": {
                        "type": "object",
                        "properties": {
                            "sign": {
                                "type": "string",
                                "description": "The call sign for the radio station for which you want the most popular song. Example calls signs are WZPZ and WKRP.",
                            }
                        },
                        "required": ["sign"],
                    }
                },
            }
        }
    ]
}


response = client.converse_stream(
    modelId=model_id,
    messages=messages,
    system=system_prompts,
    inferenceConfig=inference_config,
    additionalModelRequestFields=additional_model_fields,
)
st.write(response)
stream = response.get("stream")
st.write(stream)
# dumped = json.dumps(response, indent=4)
# print(dumped)
# stream = response.get("stream")
# if stream:
#     for event in stream:

#         if "messageStart" in event:
#             print(f"\nRole: {event['messageStart']['role']}")

#         if "contentBlockDelta" in event:
#             print(event["contentBlockDelta"]["delta"]["text"], end="")

#         if "messageStop" in event:
#             print(f"\nStop reason: {event['messageStop']['stopReason']}")

#         if "metadata" in event:
#             metadata = event["metadata"]
#             if "usage" in metadata:
#                 print("\nToken usage")
#                 print(f"Input tokens: {metadata['usage']['inputTokens']}")
#                 print(f":Output tokens: {metadata['usage']['outputTokens']}")
#                 print(f":Total tokens: {metadata['usage']['totalTokens']}")
#             if "metrics" in event["metadata"]:
#                 print(f"Latency: {metadata['metrics']['latencyMs']} milliseconds")
