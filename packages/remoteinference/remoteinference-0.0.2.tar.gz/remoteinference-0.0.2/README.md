# llm_inference

Simple package to perform remote inference on language models of different providers.

## getting started
Install the package
```python
pip install remoteinference
```
If you have a LLM running on a remote server using llama.cpp for example you can initalize the model by running
```python

```

