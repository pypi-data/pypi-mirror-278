{
    "paper_id": "N19-1008",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:19:56.468194Z"
    },
    "title": "Giving Attention to the Unexpected: Using Prosody Innovations in Disfluency Detection",
    "authors": [
        {
            "first": "Vicky",
            "middle": [],
            "last": "Zayats",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Washington",
                "location": {}
            },
            "email": ""
        },
        {
            "first": "Mari",
            "middle": [],
            "last": "Ostendorf",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Washington",
                "location": {}
            },
            "email": ""
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Disfluencies in spontaneous speech are known to be associated with prosodic disruptions. However, most algorithms for disfluency detection use only word transcripts. Integrating prosodic cues has proved difficult because of the many sources of variability affecting the acoustic correlates. This paper introduces a new approach to extracting acoustic-prosodic cues using text-based distributional prediction of acoustic cues to derive vector z-score features (innovations). We explore both early and late fusion techniques for integrating text and prosody, showing gains over a high-accuracy text-only model.",
    "pdf_parse": {
        "paper_id": "N19-1008",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Disfluencies in spontaneous speech are known to be associated with prosodic disruptions. However, most algorithms for disfluency detection use only word transcripts. Integrating prosodic cues has proved difficult because of the many sources of variability affecting the acoustic correlates. This paper introduces a new approach to extracting acoustic-prosodic cues using text-based distributional prediction of acoustic cues to derive vector z-score features (innovations). We explore both early and late fusion techniques for integrating text and prosody, showing gains over a high-accuracy text-only model.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Speech disfluencies are frequent events in spontaneous speech. The rate of disfluencies varies with the speaker and context; one study observed disfluencies once in every 20 words, affecting up to one third of utterances (Shriberg, 1994) . Disfluencies are important to account for, both because of the challenge that the disrupted grammatical flow poses for natural language processing of spoken transcripts and because of the information that they provide about the speaker.",
                "cite_spans": [
                    {
                        "start": 221,
                        "end": 237,
                        "text": "(Shriberg, 1994)",
                        "ref_id": "BIBREF20"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Most work on disfluency detection builds on the framework that annotates a disfluency in terms of a reparandum followed by an interruption point (+), an optional interregnum ({ }), and then the repair, if any. A few simple examples are given below: Based on the similarity/differences between the reparandum and the repair, disfluencies are often categorized into three types: repetition (the first example), rephrase (the next example), and restart (the last example).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The interruption point is associated with a disruption in the realization of a prosodic phrase, which could involve cutting words off or elongation associated with hesitation, followed by a prosodic reset at the start of the repair. There may also be emphasis in the repair to highlight the correction.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Researchers have been working on automatic disfluency detection for many years (Lickley, 1994; Shriberg et al., 1997; Charniak and Johnson, 2001; Johnson and Charniak, 2004; Lease et al., 2006; Qian and Liu, 2013; Zayats et al., 2016) , motivated in part by early work on parsing speech that assumed reliable detection of the interruption point (Nakatani and Hirschberg, 1994; Shriberg and Stolcke, 1997; Liu et al., 2006) . The first efforts to integrate prosody with word cues for disfluency detection (Baron et al., 2002; Snover et al., 2004) found gains from using prosody, but word cues played the primary role. In subsequent work (Qian and Liu, 2013; Honnibal and Johnson, 2014; Wang et al., 2017) , more effective models of word transcripts have been the main source of performance gains. The success of recent neural network systems raises the question of what the role is for prosody in future work. In the next section, we hypothesize where prosody might help and look at the relative frequency of these cases and the performance of a high accuracy disfluency detection algorithm in these contexts.",
                "cite_spans": [
                    {
                        "start": 79,
                        "end": 94,
                        "text": "(Lickley, 1994;",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 95,
                        "end": 117,
                        "text": "Shriberg et al., 1997;",
                        "ref_id": null
                    },
                    {
                        "start": 118,
                        "end": 145,
                        "text": "Charniak and Johnson, 2001;",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 146,
                        "end": 173,
                        "text": "Johnson and Charniak, 2004;",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 174,
                        "end": 193,
                        "text": "Lease et al., 2006;",
                        "ref_id": null
                    },
                    {
                        "start": 194,
                        "end": 213,
                        "text": "Qian and Liu, 2013;",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 214,
                        "end": 234,
                        "text": "Zayats et al., 2016)",
                        "ref_id": "BIBREF34"
                    },
                    {
                        "start": 345,
                        "end": 376,
                        "text": "(Nakatani and Hirschberg, 1994;",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 377,
                        "end": 404,
                        "text": "Shriberg and Stolcke, 1997;",
                        "ref_id": null
                    },
                    {
                        "start": 405,
                        "end": 422,
                        "text": "Liu et al., 2006)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 504,
                        "end": 524,
                        "text": "(Baron et al., 2002;",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 525,
                        "end": 545,
                        "text": "Snover et al., 2004)",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 636,
                        "end": 656,
                        "text": "(Qian and Liu, 2013;",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 657,
                        "end": 684,
                        "text": "Honnibal and Johnson, 2014;",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 685,
                        "end": 703,
                        "text": "Wang et al., 2017)",
                        "ref_id": "BIBREF29"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "With the premise that there is a potential for prosody to benefit disfluency detection, we then propose a new approach to extracting prosodic features. A major challenge for all efforts to incorporate prosodic cues in spoken language understanding is the substantial variability in the acoustic correlates of prosody. For example, duration cues are expected to be useful -disfluencies are often associated with duration lengthening related to hesitation. However, duration varies with phonetic context, word function, prosodic phrase structure, speaking rate, etc. To account for some of this variability, various feature normalization techniques are used, but typically these account for only limited contexts, e.g. phonetic context for duration or speaker pitch range for fundamental frequency. In our work, we introduce a mechanism for normalization using the full sentence context. We train a sequential neural prediction model to estimate distributions of acoustic features for each word, given the word sequence of a sentence. Then, the actual observed acoustic feature is used to find the prediction error, normalized by the estimated variance. We refer to the resulting features as innovations, which can be thought of as a non-linear version of the innovations in a Kalman filter. The innovations will be large when the acoustic cues do not reflect the expected prosodic structure, such as during hesitations, disfluencies, and contrastive or emphatic stress. The idea is to provide prosodic cues that are less redundant with the textual cues. We assess the new prosodic features in experiments on disfluency detection using the Switchboard corpus, exploring both early and late fusion techniques to integrate innovations with text features. Our analysis shows that prosody does help with detecting some of the more difficult types of disfluencies.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "This paper has three main contributions. First, our analysis of a high performance disfluency detection algorithm confirms hypotheses about contexts where text-only models have high error rates. Second, we introduce a novel representation of prosodic cues, i.e. the innovation vector resulting from predicting prosodic cues given the whole sentence context. Analyses of the innovation distributions show expected patterns of prosodic cues at interruption points. Finally, we demonstrate improved disfluency detection performance on Switchboard by integrating prosody and textbased features in a neural network architecture, while comparing early and late fusion approaches.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Disfluency detection algorithms based on text alone rely on the fact that disfluencies often involve parallel syntactic structure in the reparandum and the repair, as illustrated in the previous examples. In these cases, pattern match provides a strong cue to the disfluency. In associated with disfluencies, and these are relatively easy for a text-based model to learn. In some cases, an interregnum word (or words) provides a word cue to the interruption point. In the Switchboard corpus, only 15% of interruption points are followed by an interregnum, but it can provide a good cue when present. Prosody mainly serves to help identify the interruption point. Thus, for these types of disfluencies, it makes sense that prosodic cues would not really be needed.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "How Might Prosody Help?",
                "sec_num": "2"
            },
            {
                "text": "Because disfluencies with a parallel syntactic structure do represent a substantial fraction of disfluencies in spontaneous speech, text-based algorithms have been relatively effective. The best models achieve F-scores of 86-91% 1 (Lou and Johnson, 2017; Zayats and Ostendorf, 2018; Wang et al., 2017 Wang et al., , 2018)) . We hypothesize that many er-1 It is difficult to directly compare published results, because there are different approaches to tokenization that have a non-trivial impact on performance but are not well documented in the literature. Those differences include handling of fragment words, turn boundaries, and tokenization. For example, some studies use fragment features explicitly, while others omit them because speech recognition systems often miss them. Turn boundaries that do not end with a slash unit pose an ambiguity during speaker overlap: cross-turn 'sentences' can either be combined into a longer sentence or separated based on the turn boundary, which impacts what can be detected. Lastly, there are differences in whether contractions and possessives are split into two tokens, and whether conversational terms such as \"you know\" are combined into a single token. ",
                "cite_spans": [
                    {
                        "start": 240,
                        "end": 254,
                        "text": "Johnson, 2017;",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 255,
                        "end": 282,
                        "text": "Zayats and Ostendorf, 2018;",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 283,
                        "end": 300,
                        "text": "Wang et al., 2017",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 301,
                        "end": 322,
                        "text": "Wang et al., , 2018))",
                        "ref_id": "BIBREF27"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "How Might Prosody Help?",
                "sec_num": "2"
            },
            {
                "text": "Rephrasing does not always involve a simple \"rough copy\" of a repair. In order to confirm that there is potential for prosody to help in these contexts, we first categorize the disfluencies. To avoid hand-labeling of categories, we distinguished disfluencies based on surface forms (repetition, rephrase, restart) and length of the disfluency reparandum. Word counts for the different categories are given in Table 1 .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 415,
                        "end": 416,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Non-trivial rephrasing:",
                "sec_num": null
            },
            {
                "text": "Conditioning on the different contexts, we analyze errors in the development set made by the high accuracy text-based disfluency detection system that is the baseline for this study (Zayats and Ostendorf, 2018) . For this model, trained on Switchboard, the performance is 87.4 F-score (P=93.3, R=82.2) on the development set and 87.5 (P=93.1, R=82.5) on the test set. For each class, we measured the disfluency detection recall (relative frequency of reparandum tokens that were predicted correctly), as well as the percentage of tokens associated with each class. The results in Table 2 confirm that error rates are higher for restarts, longer rephrasings, and complex disfluencies.",
                "cite_spans": [
                    {
                        "start": 182,
                        "end": 210,
                        "text": "(Zayats and Ostendorf, 2018)",
                        "ref_id": "BIBREF33"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Non-trivial rephrasing:",
                "sec_num": null
            },
            {
                "text": "Rephrase disfluencies include both short lexical access errors, as well as non-trivial rewordings, which tend to be longer and involve content words. Table 3 breaks down performance for different lengths and word class to explore this difference. We found that rephrase disfluencies that contain content words are harder for the model to detect, compared to rephrases with function words only, and error increases for longer disfluencies.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 156,
                        "end": 157,
                        "text": "3",
                        "ref_id": "TABREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Non-trivial rephrasing:",
                "sec_num": null
            },
            {
                "text": "Finally, the relative frequency of false positives in fluent repetitions is 0.35. Since fluent repetitions account for only 4% of all repetitions, the impact on overall performance is small.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Non-trivial rephrasing:",
                "sec_num": null
            },
            {
                "text": "The ultimate goal of a disfluency detection system is to perform well in domains other than Switchboard. Other datasets are likely to have different distributions of disfluencies, often with a higher frequency of those that are hard to detect, such as restarts and repairs (Zayats et al., 2014) . In addition, due to the differences in vocabulary, disfluencies with content words are more likely to get misdetected if there is a domain mismatch. Thus, we hypothesize that prosody features can have a greater impact in a domain transfer scenario.",
                "cite_spans": [
                    {
                        "start": 273,
                        "end": 294,
                        "text": "(Zayats et al., 2014)",
                        "ref_id": "BIBREF35"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Non-trivial rephrasing:",
                "sec_num": null
            },
            {
                "text": "Integrating prosodic cues has proved difficult because of the many sources of variability affecting the acoustic correlates, while systems that only use text achieve high performance. In this work, we propose a new approach that operates on differences in information found in text and prosody. In order to calculate such differences, we introduce innovation features, similar to the concept of innovations in Kalman filters. The key idea is to predict prosodic features based on text information, and then use the difference between the predicted and observed prosodic signal (innovations) x i is a contcatenation of token, POS and identity features embeddings at time i; r i , j is a concatenation of stress and phone embeddings for phone j in token i; p i is a vector of prosodic cues; g i and h i are hidden states of token level and phone level LSTMs, correspondingly.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Method",
                "sec_num": "3"
            },
            {
                "text": "as a new feature that is additionally used to predict disfluencies. Let a prosody cue, p i at time i be an observation associated with a sentence transcript containing n word tokens, x 0 . . . x n . This observation can be modeled as a function of the sentence context H(x 0 . . . x n ) perturbed with Gaussian noise v i \u223c N (0, \u03c3 2 i ):",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Method",
                "sec_num": "3"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "p i = H(x 0 . . . x n ) + v i",
                        "eq_num": "(1)"
                    }
                ],
                "section": "Method",
                "sec_num": "3"
            },
            {
                "text": "v i can be viewed as a difference in information found between text and prosody. This difference can be measured using a z-score, which is a measure of how many standard deviations below or above the population mean an observation is. This framework can be viewed as a non-linear extension of a Kalman filter, where both H and \u03c3 2 i are parametrized using a neural network. Since disfluencies are irregularities in spoken language, they can be considered anomalies to fluent speech flow. A prosody flow that is unusual for a given word sequence, such as one that happens at interruption points, will likely have higher deviation from the predicted distribution. This anomaly in speech flow provides a strong signal when extracted using innovations, which is complementary to the text cues. In the next sections we give more details about the neural network architecture for text encoding, prosodic cues and innovation features, as well as an overview of the whole system.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Method",
                "sec_num": "3"
            },
            {
                "text": "We use both context around a word as well as subword information in text encoding for prosody prediction. Our text encoding consists of two bidirectional LSTMs: one on the token level and another on the phone level. First, we use pretrained word embeddings (Levy and Goldberg, 2014) , part-of-speech tags embeddings, and identity features (whether the word is a filled pause, discourse marker, or incomplete) as inputs to a word-level bidirectional LSTM. Then, for each phone in a word we concatenate the phone embedding, its stress embedding, and the hidden state of the word-level LSTM for the corresponding token. The resulting phone feature vector is used as input to the second bidirectional LSTM. The last hidden state h i of this second LSTM for token i summarizes the phone, stress and context information of that token, which we use to predict word-level prosodic cues. We use 3 categories of stress features in our experiments: primary, secondary and a non-stress phone.",
                "cite_spans": [
                    {
                        "start": 257,
                        "end": 282,
                        "text": "(Levy and Goldberg, 2014)",
                        "ref_id": "BIBREF10"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Text Encoding for Prosody Prediction",
                "sec_num": "3.1"
            },
            {
                "text": "Our prosodic cues include: Pause. Given a pause before a word, r i , our pause cues are scaled as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Prosodic Cues",
                "sec_num": "3.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "r i = min(1, ln (1 + r i ))",
                        "eq_num": "(2)"
                    }
                ],
                "section": "Prosodic Cues",
                "sec_num": "3.2"
            },
            {
                "text": "Pause information is extracted on a word-level using Mississippi State (MsState) time alignments (more details on data preprocessing in Section 4.1.) We use scaled real-valued pause information. Scaling pause lengths this way, including the threshold for pauses longer than 1 sec (which are rare), makes the pause distribution less skewed. Word Duration. Similar to pause information, we extract word duration information using MsState time alignments. We do not need to do the standard word-based duration normalization, since the idea behind the innovation model is to normalize prosodic features using a richer context representation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Prosodic Cues",
                "sec_num": "3.2"
            },
            {
                "text": "Fundamental frequency (F0) and Energy (E). Similar to Tran et al. (2018) , we use three F0 features and three energy features. The three F0 features include normalized cross correlation function (NCCF), log-pitch weighted by probability of voicing (POV), and the estimated delta of log pitch. The three energy features include the log of total energy, the log of total energy from lower 20 mel-frequency bands and the log of total energy from higher 20 mel-frequency bands. The contour features are extracted from 25-ms frames with 10-ms hops using Kaldi (Povey et al., 2011) .",
                "cite_spans": [
                    {
                        "start": 54,
                        "end": 72,
                        "text": "Tran et al. (2018)",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 555,
                        "end": 575,
                        "text": "(Povey et al., 2011)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Prosodic Cues",
                "sec_num": "3.2"
            },
            {
                "text": "Our model is trained to predict the mean of these features across the frames in a word.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Prosodic Cues",
                "sec_num": "3.2"
            },
            {
                "text": "MFCCs. In addition to features used in Tran et al. (2018) , we also use 13 mel-frequency cepstral coefficients, averaged at the word level, similar to F0 and energy features as described above.",
                "cite_spans": [
                    {
                        "start": 39,
                        "end": 57,
                        "text": "Tran et al. (2018)",
                        "ref_id": "BIBREF26"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Prosodic Cues",
                "sec_num": "3.2"
            },
            {
                "text": "Given a word-level text encoding h i , for each token in a sentence we predict each of the k prosodic cues p i k listed above. We assume that the predicted prosody cues conditioned on text have a Gaussian distribution:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Prosody Innovation Cues",
                "sec_num": "3.3"
            },
            {
                "text": "p i k |h i \u223c N (\u00b5 i,k , \u03c3 2 i,k ) \u00b5 i,k = f (W k 1 h i + b k 1 ) \u03c3 2 i,k = sof tplus(W k 2 h i + b k 2 ) (3) W k 1 , b k 1 , W k 2 , b k 2 are learnable parameters; the ac- tivation function sof tplus(x) = log(1 + exp(x))",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Prosody Innovation Cues",
                "sec_num": "3.3"
            },
            {
                "text": "ensures that the variance is always positive; f is an activation function, which is sof tplus for pauses and durations, and tanh for the rest of the prosodic cues. The objective function is a sum of the negative log-likelihood of prosodic cues p i k given text encoding. Then, given the predicted",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Prosody Innovation Cues",
                "sec_num": "3.3"
            },
            {
                "text": "\u00b5 i,k , \u03c3 2 i,k",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Prosody Innovation Cues",
                "sec_num": "3.3"
            },
            {
                "text": "and true values of prosodic cues p i k , we calculate z-scores for each of the cues, which should have high absolute value for tokens with unusual prosodic behaviour:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Prosody Innovation Cues",
                "sec_num": "3.3"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "z k i = p i k -\u00b5 i,k \u03c3 i,k",
                        "eq_num": "(4)"
                    }
                ],
                "section": "Prosody Innovation Cues",
                "sec_num": "3.3"
            },
            {
                "text": "The prosody prediction module is illustrated in Figure 1a .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 55,
                        "end": 57,
                        "text": "1a",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Prosody Innovation Cues",
                "sec_num": "3.3"
            },
            {
                "text": "These z-scores, or innovations, are used as additional features in our disfluency detection model. We train the prosody prediction model only on sentences that do not contain any disfluencies. Any unusual behaviours in disfluency regions, therefore, should have large innovation values predicted by our model.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Prosody Innovation Cues",
                "sec_num": "3.3"
            },
            {
                "text": "Following (Zayats and Ostendorf, 2018) , we use a bidirectional LSTM-CRF model as our disfluency detection framework. This framework uses a BIO tagging approach, where we predict whether each token is a part of a reparandum, repair or both. Following previous studies, the overall performance is measured in F-score of correctly predicted disfluencies in the reparandum. Previous work used textual features only. Here, we evaluate the importance of innovation cues with two types of multimodal fusion -early and late fusion. In early fusion, we concatenate innovations and/or prosody features with the rest of the textual features used in the framework at the input to LSTM layer. In late fusion, we create two separate models -one with only textual features and another with innovations and/or prosody features. Then we do a linear interpolation of the states of two models just before feeding the result to the CRF layer:",
                "cite_spans": [
                    {
                        "start": 10,
                        "end": 38,
                        "text": "(Zayats and Ostendorf, 2018)",
                        "ref_id": "BIBREF33"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Disfluency Detection System",
                "sec_num": "3.4"
            },
            {
                "text": "u shared i = \u03b1u prosody i + (1 -\u03b1)u text i (5)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Disfluency Detection System",
                "sec_num": "3.4"
            },
            {
                "text": "We tune the interpolation weight \u03b1 and report the best in our experiments section. We train our model jointly, optimizing both prosodic cues prediction and disfluency detection. The schematic view of the late fusion system is presented in Fig- ure 1b .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 244,
                        "end": 250,
                        "text": "ure 1b",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Disfluency Detection System",
                "sec_num": "3.4"
            },
            {
                "text": "In our experiments we evaluate the usefulness of innovation features, and compare it to baselines with text-only or raw prosodic cues. For each model configuration, we run 10 experiments with different random seeds. This alleviates the potential of making wrong conclusions due to \"lucky/unlucky\" random seeds. We report both the mean and best scores among the 10 runs.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "4"
            },
            {
                "text": "Switchboard (Godfrey et al., 1992) 4 : F1 scores on disfluency detection when using a single set of features (text-only, raw prosody features or innovation features), with early fusion and late fusion. \"Raw\" indicates the usage of original prosodic features (Section 3.2), while \"innovations\" indicate the usage of innovation features (Section 3.3). but it 's just you know leak leak leak everywhere people should know that that 's an option and i think you do accomplish more after that i mean [ it was + it ] interesting thing [ about gas is when + i mean about battery powered cars is ] Table 5 : Examples of sentences where prosody innovations help. Words in red are correctly labeled when using prosody but not with text only. The first three show fluent phrases; the last two have disfluencies that are missed without prosody.",
                "cite_spans": [
                    {
                        "start": 12,
                        "end": 34,
                        "text": "(Godfrey et al., 1992)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 35,
                        "end": 36,
                        "text": "4",
                        "ref_id": null
                    },
                    {
                        "start": 596,
                        "end": 597,
                        "text": "5",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Data Preprocessing",
                "sec_num": "4.1"
            },
            {
                "text": "containing 1126 files hand-annotated with disfluencies. Because human transcribers are imperfect, the original transcripts contained errors. MsState researchers ran a clean-up project which hand-corrected the transcripts and word alignments (Deshmukh et al., 1998) . In this work, we use the MsState version of the word alignments, which allows us to extract more reliable prosodic features. Since the corrected version of Switchboard does not contain updated disfluency annotations, we corrected the annotations using a semiautomated approach: we used a text-based disfluency detection algorithm to re-annotate tokens that were corrected by MsState, while keeping the rest of the original disfluency annotations. The result is referred to as a silver annotation. Most of the corrected tokens are repetitions and restarts. To assess the quality of the automatic mapping of disfluencies, we hand-annotated a subset (6.6k tokens, 453 sentences) of the test data and evaluated the performance of the silver annotation against the gold annotation, which has an F1 score of 90.1 (Prec 90.1, Rec 90.1). Comparing the performance estimates from gold and silver annotations on this subset, we find that the silver annotations give some-what lower F1 scores (2-3% absolute), both due to lower precision and recall scores.",
                "cite_spans": [
                    {
                        "start": 241,
                        "end": 264,
                        "text": "(Deshmukh et al., 1998)",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data Preprocessing",
                "sec_num": "4.1"
            },
            {
                "text": "Our experiments evaluate the use of innovations with two popular multimodal fusion approaches: early fusion and late fusion. Our baselines include models with text-only, prosody cues only (raw), and innovation features only as inputs. Since innovations require both text and raw prosodic cues, this baseline is multimodal. In addition, for the late fusion experiments, we show the optimal value of \u03b1, the interpolation weight from Equation 5. All experiment results are presented in Table 4 .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 489,
                        "end": 490,
                        "text": "4",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "4.2"
            },
            {
                "text": "We found that innovations are helpful in both early and late fusion frameworks, while late fusion performs better on average. The interpolation weight \u03b1 for the late fusion experiments is high when innovations are used, which further indicates that innovation features are useful in overall prediction. Interestingly, innovation features alone perform surprisingly well. We also take a closer look at the importance of joint training of the disfluency detection system with prosody prediction. To do this, we pretrain the prosody pre-i like to run [about + oh about ] [two + two and a half ] miles the old-timers even the people who are technologists do n't know how to operate i do n't know whether that 's because they you know sort of give up hope it must be really challenging to um try to juggle a job Table 6 : Examples of the sentences where prosody innovations hurt. Words in red are incorrectly labeled when using prosody but not with text only. The first shows a disfluency missed when using prosody; the other three are fluent regions with false detections.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 813,
                        "end": 814,
                        "text": "6",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "4.2"
            },
            {
                "text": "diction part of the model first. Then, we train the full model with innovation inputs while freezing the part of the network responsible for predicting prosodic cues. The mean F-score of this disjointly trained model is 49.27% on the dev set, compared to 80.86% for the jointly trained model. This result suggests that training the system end-to-end in a multitask setup is very important.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "4.2"
            },
            {
                "text": "In order to better understand the impact of the prosody innovations, we perform an error analysis where we compare the predictions of two models: a late fusion model that uses both text and innovation features, and a baseline model that uses text only. All of the analysis is done on the dev set with the model that has the median performance out of 10 that were trained.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Error analysis",
                "sec_num": "5.1"
            },
            {
                "text": "First, we extract all the sentences where the number of disfluency detection errors using the innovation model is lower than when using the textonly model (168 sentences). Examples of such sentences are presented in Table 5 . By looking at the sentences where the model with innovations performs better, we see fluent repetitions and other ambiguous cases where audio is useful for correctly identifying disfluencies.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 222,
                        "end": 223,
                        "text": "5",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Error analysis",
                "sec_num": "5.1"
            },
            {
                "text": "On the other hand, in Table 6 , we have examples of sentences that have a higher number of errors when prosody is used (143 sentences). In the first example, the labeling of \"two\" as fluent by the model with prosody is arguably correct, with the repetition indicating a range rather than a correction. The next involves a parenthetical phrase, the start of which may be confused with an interruption point. In the last two cases, there is a prosodic disruption and an interegnum, but no correction.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 28,
                        "end": 29,
                        "text": "6",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Error analysis",
                "sec_num": "5.1"
            },
            {
                "text": "In order to understand whether incorporating prosody through our model supports the hypotheses in Section 2, we compare the performance of two models for different categories of disfluen- cies. We found that using prosody innovations improves detection of: non-repetition disfluencies (from 68.2% to 73.7%), particularly for disfluencies with content words (65.2% to 71.0%); long repairs (64.0% to 72.7% and 40.0% to 64.6% for disfluencies with length of repair greater than 3 and 5 correspondingly); and restarts (from 36.0% to 37.4%). Prosodic innovations also help decrease the rate of false positives for fluent repetitions: the false positives rate decreased from 46.5% to 38.4%. However, the prosody model increases the false positives in other contexts, such as in the examples in Table 6 .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 794,
                        "end": 795,
                        "text": "6",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Error analysis",
                "sec_num": "5.1"
            },
            {
                "text": "In order to understand what the model actually learns with respect to innovations, we look at innovation distributions for words preceding interruption points compared to fluent words. The histograms are presented in Figure 2 . As expected, we see that words preceding interruption points have atypically longer duration and lower energy. The intonation features did not show substantial distribution differences, probably due to the overly simplistic word-level averaging strategy.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 224,
                        "end": 225,
                        "text": "2",
                        "ref_id": "FIGREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Innovation Predictors",
                "sec_num": "5.2"
            },
            {
                "text": "Most work on disfluency detection falls into three main categories: sequence tagging, noisy-channel and parsing-based approaches. Sequence tagging approaches rely on BIO tagging with recurrent neural networks (Hough and Schlangen, 2015; Zayats et al., 2016; Wang et al., 2016; Zayats and Ostendorf, 2018; Lou et al., 2018) . Noisy channel models operate on a relationship between the reparandum and repair for identifying disfluencies (Charniak and Johnson, 2001; Zwarts et al., 2010) . Lou and Johnson (2017) used a neural language model to rerank sentences using the noisy channel model. Another line of work combined parsing and disfluency removal tasks (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014; Tran et al., 2018) . Recently a transitionbased neural model architecture was proposed for disfluency detection (Wang et al., 2017) . The current state of the art in disfluency detection (Wang et al., 2018) uses a neural machine translation framework with a transformer architecture and additional simulated data. All of the models mentioned above rely heavily on pattern match features, hand-crafted or automatically extracted, that help to identify repetitions and disfluencies with parallel syntactic structure.",
                "cite_spans": [
                    {
                        "start": 209,
                        "end": 236,
                        "text": "(Hough and Schlangen, 2015;",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 237,
                        "end": 257,
                        "text": "Zayats et al., 2016;",
                        "ref_id": "BIBREF34"
                    },
                    {
                        "start": 258,
                        "end": 276,
                        "text": "Wang et al., 2016;",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 277,
                        "end": 304,
                        "text": "Zayats and Ostendorf, 2018;",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 305,
                        "end": 322,
                        "text": "Lou et al., 2018)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 435,
                        "end": 463,
                        "text": "(Charniak and Johnson, 2001;",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 464,
                        "end": 484,
                        "text": "Zwarts et al., 2010)",
                        "ref_id": "BIBREF36"
                    },
                    {
                        "start": 657,
                        "end": 686,
                        "text": "(Rasooli and Tetreault, 2013;",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 687,
                        "end": 714,
                        "text": "Honnibal and Johnson, 2014;",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 715,
                        "end": 733,
                        "text": "Tran et al., 2018)",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 827,
                        "end": 846,
                        "text": "(Wang et al., 2017)",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 902,
                        "end": 921,
                        "text": "(Wang et al., 2018)",
                        "ref_id": "BIBREF27"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "6"
            },
            {
                "text": "While prosodic features are useful for detecting interruption points (Nakatani and Hirschberg, 1994; Shriberg and Stolcke, 1997; Shriberg, 1999; Liu et al., 2006) , recent methods on disfluency detection predominantly rely on lexical information exclusively. An exception is (Ferguson et al., 2015) , which showed some gains using a simple concatenation of pause and word duration features. Similar to disfluency detection, parsing has seen little use of prosody in recent studies. However, Tran et al. (2018) recently demonstrated that that a neural model using pause, word and rhyme duration, f0 and energy helps in spoken language parsing, specifically in the regions that contain disfluencies.",
                "cite_spans": [
                    {
                        "start": 69,
                        "end": 100,
                        "text": "(Nakatani and Hirschberg, 1994;",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 101,
                        "end": 128,
                        "text": "Shriberg and Stolcke, 1997;",
                        "ref_id": null
                    },
                    {
                        "start": 129,
                        "end": 144,
                        "text": "Shriberg, 1999;",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 145,
                        "end": 162,
                        "text": "Liu et al., 2006)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 275,
                        "end": 298,
                        "text": "(Ferguson et al., 2015)",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 491,
                        "end": 509,
                        "text": "Tran et al. (2018)",
                        "ref_id": "BIBREF26"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "6"
            },
            {
                "text": "Early fusion and late fusion are the two most popular types of modality fusion techniques. In recent years, more interesting modality fusion approaches were introduced, most of them where the fusion happens inside the model (Zadeh et al., 2017; Chen et al., 2017; Zadeh et al., 2018) . Those methods usually require the model to learn interactions between modalities implicitly, by backpropagating the errors based on the main objective func-tion with respect to the task. Other multimodal representation learning approaches learn a shared representation between multiple modalities (Andrew et al., 2013; Ryan Kiros, 2014; Xu et al., 2015; Suzuki et al., 2016) , often targeting unsupervised translation from one modality to the other. In our work we use innovations as a novel representation learning approach, where our emphasis is on looking into complementary cues rather than similarity between multiple modalities.",
                "cite_spans": [
                    {
                        "start": 224,
                        "end": 244,
                        "text": "(Zadeh et al., 2017;",
                        "ref_id": "BIBREF31"
                    },
                    {
                        "start": 245,
                        "end": 263,
                        "text": "Chen et al., 2017;",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 264,
                        "end": 283,
                        "text": "Zadeh et al., 2018)",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 583,
                        "end": 604,
                        "text": "(Andrew et al., 2013;",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 605,
                        "end": 622,
                        "text": "Ryan Kiros, 2014;",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 623,
                        "end": 639,
                        "text": "Xu et al., 2015;",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 640,
                        "end": 660,
                        "text": "Suzuki et al., 2016)",
                        "ref_id": "BIBREF25"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "6"
            },
            {
                "text": "In this paper, we introduce a novel approach to extracting acoustic-prosodic cues with the goal of improving disfluency detection, but also with the intention of impacting spoken language processing more generally. Our initial analysis of a textonly disfluency detection system shows that despite high performance of such models, there exists a big gap in the performance of text-based approaches for some types of disfluencies, such as restarts and non-trivial or long rephrases. Thus, prosody cues, which can be indicative of interruption points, have a potential to contribute towards detection of more difficult types of disfluencies. Since the acoustic-prosodic cues carry information related to multiple phenomena, it can be difficult to isolate the cues that are relevant to specific events, such as interruption points. In this work, we introduce a novel approach where we extract relevant acoustic-prosodic information using textbased distributional prediction of acoustic cues to derive vector z-score features, or innovations. The innovations point to irregularities in prosody flow that are not predicted by the text, helping to better isolate signals relevant to disfluency detection that are not simply redundant with textual cues. We explore both early and late fusion approaches to combine innovations with text-based features. Our experiments show that innovation features are better predictors of disfluencies compared to the original acoustic cues.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusions",
                "sec_num": "7"
            },
            {
                "text": "Our analysis of the errors and of the innovation features point to a limitation of the current work, which is in the modeling of F0 features. The current model obtains word-based F0 (and energy) features by simply averaging the values over the duration of the word, which loses any distinctions between rising and falling F0. By leveraging polynomial contour models, we expect to improve both intonation and energy features, which we hope will reduce some of the false detections associated with emphasis and unexpected fluent phrase boundaries.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusions",
                "sec_num": "7"
            },
            {
                "text": "An important next step is to test the system using ASR rather than hand transcripts. It is possible that errors in the transcripts could hurt the residual prediction, but if prosody is used to refine the recognition hypothesis, this could actually lead to improved recognition. Finally, we expect that the innovation model of prosody can benefit other NLP tasks, such as sarcasm and intent detection, as well as detecting paralinguist information.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusions",
                "sec_num": "7"
            }
        ],
        "back_matter": [],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Deep canonical correlation analysis",
                "authors": [
                    {
                        "first": "Galen",
                        "middle": [],
                        "last": "Andrew",
                        "suffix": ""
                    },
                    {
                        "first": "Raman",
                        "middle": [],
                        "last": "Arora",
                        "suffix": ""
                    },
                    {
                        "first": "Jeff",
                        "middle": [],
                        "last": "Bilmes",
                        "suffix": ""
                    },
                    {
                        "first": "Karen",
                        "middle": [],
                        "last": "Livescu",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "International Conference on Machine Learning",
                "volume": "",
                "issue": "",
                "pages": "1247--1255",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Galen Andrew, Raman Arora, Jeff Bilmes, and Karen Livescu. 2013. Deep canonical correlation analysis. In International Conference on Machine Learning, pages 1247-1255.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Automatic punctuation and disfluency detection in multi-party meetings using prosodic and lexical cues",
                "authors": [
                    {
                        "first": "Don",
                        "middle": [],
                        "last": "Baron",
                        "suffix": ""
                    },
                    {
                        "first": "Elizabeth",
                        "middle": [],
                        "last": "Shriberg",
                        "suffix": ""
                    },
                    {
                        "first": "Andreas",
                        "middle": [],
                        "last": "Stolcke",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Seventh International Conference on Spoken Language Processing",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Don Baron, Elizabeth Shriberg, and Andreas Stolcke. 2002. Automatic punctuation and disfluency detec- tion in multi-party meetings using prosodic and lex- ical cues. In Seventh International Conference on Spoken Language Processing.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Edit detection and parsing for transcribed speech",
                "authors": [
                    {
                        "first": "E",
                        "middle": [],
                        "last": "Charniak",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Johnson",
                        "suffix": ""
                    }
                ],
                "year": 2001,
                "venue": "Proc. NAACL",
                "volume": "",
                "issue": "",
                "pages": "118--126",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "E. Charniak and M. Johnson. 2001. Edit detection and parsing for transcribed speech. In Proc. NAACL, pages 118-126.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Multimodal sentiment analysis with wordlevel fusion and reinforcement learning",
                "authors": [
                    {
                        "first": "Minghai",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Sen",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Paul",
                        "middle": [
                            "Pu"
                        ],
                        "last": "Liang",
                        "suffix": ""
                    },
                    {
                        "first": "Tadas",
                        "middle": [],
                        "last": "Baltru\u0161aitis",
                        "suffix": ""
                    },
                    {
                        "first": "Amir",
                        "middle": [],
                        "last": "Zadeh",
                        "suffix": ""
                    },
                    {
                        "first": "Louis-Philippe",
                        "middle": [],
                        "last": "Morency",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the 19th ACM International Conference on Multimodal Interaction",
                "volume": "",
                "issue": "",
                "pages": "163--171",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Minghai Chen, Sen Wang, Paul Pu Liang, Tadas Bal- tru\u0161aitis, Amir Zadeh, and Louis-Philippe Morency. 2017. Multimodal sentiment analysis with word- level fusion and reinforcement learning. In Proceed- ings of the 19th ACM International Conference on Multimodal Interaction, pages 163-171. ACM.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Resegmentation of switchboard",
                "authors": [
                    {
                        "first": "Neeraj",
                        "middle": [],
                        "last": "Deshmukh",
                        "suffix": ""
                    },
                    {
                        "first": "Aravind",
                        "middle": [],
                        "last": "Ganapathiraju",
                        "suffix": ""
                    },
                    {
                        "first": "Andi",
                        "middle": [],
                        "last": "Gleeson",
                        "suffix": ""
                    },
                    {
                        "first": "Jonathan",
                        "middle": [],
                        "last": "Hamaker",
                        "suffix": ""
                    },
                    {
                        "first": "Joseph",
                        "middle": [],
                        "last": "Picone",
                        "suffix": ""
                    }
                ],
                "year": 1998,
                "venue": "Fifth international conference on spoken language processing",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Neeraj Deshmukh, Aravind Ganapathiraju, Andi Glee- son, Jonathan Hamaker, and Joseph Picone. 1998. Resegmentation of switchboard. In Fifth interna- tional conference on spoken language processing.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Disfluency detection with a semi-markov model and prosodic features",
                "authors": [
                    {
                        "first": "James",
                        "middle": [],
                        "last": "Ferguson",
                        "suffix": ""
                    },
                    {
                        "first": "Greg",
                        "middle": [],
                        "last": "Durrett",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Klein",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proc. NAACL HLT",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "James Ferguson, Greg Durrett, and Dan Klein. 2015. Disfluency detection with a semi-markov model and prosodic features. In Proc. NAACL HLT.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Switchboard: Telephone speech corpus for research and development",
                "authors": [
                    {
                        "first": "J",
                        "middle": [
                            "J"
                        ],
                        "last": "Godfrey",
                        "suffix": ""
                    },
                    {
                        "first": "E",
                        "middle": [
                            "C"
                        ],
                        "last": "Holliman",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Mcdaniel",
                        "suffix": ""
                    }
                ],
                "year": 1992,
                "venue": "Proc. ICASSP",
                "volume": "I",
                "issue": "",
                "pages": "517--520",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "J. J. Godfrey, E. C. Holliman, and J. McDaniel. 1992. Switchboard: Telephone speech corpus for research and development. In Proc. ICASSP, volume I, pages 517-520.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Joint incremental disfluency detection and dependency parsing",
                "authors": [
                    {
                        "first": "Matthew",
                        "middle": [],
                        "last": "Honnibal",
                        "suffix": ""
                    },
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Johnson",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Transactions of the Association for Computational Linguistics",
                "volume": "2",
                "issue": "1",
                "pages": "131--142",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Matthew Honnibal and Mark Johnson. 2014. Joint incremental disfluency detection and dependency parsing. Transactions of the Association for Com- putational Linguistics, 2(1):131-142.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Recurrent neural networks for incremental disfluency detection",
                "authors": [
                    {
                        "first": "Julian",
                        "middle": [],
                        "last": "Hough",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Schlangen",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proc. Interspeech",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Julian Hough and David Schlangen. 2015. Recurrent neural networks for incremental disfluency detec- tion. In Proc. Interspeech.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "A tag-based noisy channel model of speech repairs",
                "authors": [
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Johnson",
                        "suffix": ""
                    },
                    {
                        "first": "E",
                        "middle": [],
                        "last": "Charniak",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proc. ACL",
                "volume": "14",
                "issue": "",
                "pages": "169--177",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "M. Johnson and E. Charniak. 2004. A tag-based noisy channel model of speech repairs. In Proc. ACL. Matthew Lease, Mark Johnson, and Eugene Charniak. 2006. Recognizing disfluencies in conversational speech. IEEE Trans. Audio, Speech, and Language Processing, 14(5):169-177.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Dependencybased word embeddings",
                "authors": [
                    {
                        "first": "Omer",
                        "middle": [],
                        "last": "Levy",
                        "suffix": ""
                    },
                    {
                        "first": "Yoav",
                        "middle": [],
                        "last": "Goldberg",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics",
                "volume": "2",
                "issue": "",
                "pages": "302--308",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Omer Levy and Yoav Goldberg. 2014. Dependency- based word embeddings. In Proceedings of the 52nd Annual Meeting of the Association for Computa- tional Linguistics (Volume 2: Short Papers), vol- ume 2, pages 302-308.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Detecting disfluency in spontaneous speech",
                "authors": [
                    {
                        "first": "Robin",
                        "middle": [
                            "J"
                        ],
                        "last": "Lickley",
                        "suffix": ""
                    }
                ],
                "year": 1994,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Robin J Lickley. 1994. Detecting disfluency in spon- taneous speech. Ph.D. thesis, University of Edin- burgh.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Enriching speech recognition with automatic detection of sentence boundaries and disfluencies",
                "authors": [
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "E",
                        "middle": [],
                        "last": "Shriberg",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Stolcke",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Hillard",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Ostendorf",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Harper",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "IEEE Trans. Audio, Speech and Language Processing",
                "volume": "14",
                "issue": "",
                "pages": "1526--1540",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Y. Liu, E. Shriberg, A. Stolcke, D. Hillard, M. Osten- dorf, and M. Harper. 2006. Enriching speech recog- nition with automatic detection of sentence bound- aries and disfluencies. IEEE Trans. Audio, Speech and Language Processing, 14:1526-1540.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Disfluency detection using auto-correlational neural networks",
                "authors": [
                    {
                        "first": "Jamshid",
                        "middle": [],
                        "last": "Paria",
                        "suffix": ""
                    },
                    {
                        "first": "Peter",
                        "middle": [],
                        "last": "Lou",
                        "suffix": ""
                    },
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Anderson",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Johnson",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1808.09092"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Paria Jamshid Lou, Peter Anderson, and Mark Johnson. 2018. Disfluency detection using auto-correlational neural networks. arXiv preprint arXiv:1808.09092.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Disfluency detection using a noisy channel model and a deep neural language model",
                "authors": [
                    {
                        "first": "Paria",
                        "middle": [],
                        "last": "Jamshid",
                        "suffix": ""
                    },
                    {
                        "first": "Lou",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Johnson",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
                "volume": "2",
                "issue": "",
                "pages": "547--553",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Paria Jamshid Lou and Mark Johnson. 2017. Disflu- ency detection using a noisy channel model and a deep neural language model. In Proceedings of the 55th Annual Meeting of the Association for Compu- tational Linguistics (Volume 2: Short Papers), vol- ume 2, pages 547-553.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "A corpus-based study of repair cues in spontaneous speech",
                "authors": [
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Nakatani",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Hirschberg",
                        "suffix": ""
                    }
                ],
                "year": 1994,
                "venue": "Journal of the Acoustical Society of America",
                "volume": "",
                "issue": "",
                "pages": "1603--1616",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "C. Nakatani and J. Hirschberg. 1994. A corpus-based study of repair cues in spontaneous speech. Journal of the Acoustical Society of America, pages 1603- 1616.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "The Kaldi Speech Recognition Toolkit",
                "authors": [
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Povey",
                        "suffix": ""
                    },
                    {
                        "first": "Arnab",
                        "middle": [],
                        "last": "Ghoshal",
                        "suffix": ""
                    },
                    {
                        "first": "Gilles",
                        "middle": [],
                        "last": "Boulianne",
                        "suffix": ""
                    },
                    {
                        "first": "Lukas",
                        "middle": [],
                        "last": "Burget",
                        "suffix": ""
                    },
                    {
                        "first": "Ondrej",
                        "middle": [],
                        "last": "Glembek",
                        "suffix": ""
                    },
                    {
                        "first": "Nagendra",
                        "middle": [],
                        "last": "Goel",
                        "suffix": ""
                    },
                    {
                        "first": "Mirko",
                        "middle": [],
                        "last": "Hannemann",
                        "suffix": ""
                    },
                    {
                        "first": "Petr",
                        "middle": [],
                        "last": "Motlicek",
                        "suffix": ""
                    },
                    {
                        "first": "Yanmin",
                        "middle": [],
                        "last": "Qian",
                        "suffix": ""
                    },
                    {
                        "first": "Petr",
                        "middle": [],
                        "last": "Schwarz",
                        "suffix": ""
                    },
                    {
                        "first": "Jan",
                        "middle": [],
                        "last": "Silovsky",
                        "suffix": ""
                    },
                    {
                        "first": "Georg",
                        "middle": [],
                        "last": "Stemmer",
                        "suffix": ""
                    },
                    {
                        "first": "Karel",
                        "middle": [],
                        "last": "Vesely",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas Burget, Ondrej Glembek, Nagendra Goel, Mirko Hannemann, Petr Motlicek, Yanmin Qian, Petr Schwarz, Jan Silovsky, Georg Stemmer, and Karel Vesely. 2011. The Kaldi Speech Recognition Toolkit. In Proc. ASRU.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Disuency detection using multi-step stacked learning",
                "authors": [
                    {
                        "first": "Xian",
                        "middle": [],
                        "last": "Qian",
                        "suffix": ""
                    },
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proc. NAACL HLT",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xian Qian and Yang Liu. 2013. Disuency detection using multi-step stacked learning. In Proc. NAACL HLT.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Joint parsing and disfluency detection in linear time",
                "authors": [
                    {
                        "first": "Mohammad",
                        "middle": [],
                        "last": "Sadegh",
                        "suffix": ""
                    },
                    {
                        "first": "Rasooli",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Joel",
                        "middle": [
                            "R"
                        ],
                        "last": "Tetreault",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proc. EMNLP",
                "volume": "",
                "issue": "",
                "pages": "124--129",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mohammad Sadegh Rasooli and Joel R Tetreault. 2013. Joint parsing and disfluency detection in lin- ear time. In Proc. EMNLP, pages 124-129.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Unifying visual-semantic embeddings with multimodal neural language models",
                "authors": [
                    {
                        "first": "Richard",
                        "middle": [
                            "S"
                        ],
                        "last": "Zemel",
                        "suffix": ""
                    },
                    {
                        "first": "Ryan",
                        "middle": [],
                        "last": "Kiros",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Richard S. Zemel Ryan Kiros, Ruslan Salakhut- dinov. 2014. Unifying visual-semantic embed- dings with multimodal neural language models. https://arxiv.org/abs/1411.2539.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Preliminaries to a theory of speech disfluencies",
                "authors": [
                    {
                        "first": "E",
                        "middle": [],
                        "last": "Shriberg",
                        "suffix": ""
                    }
                ],
                "year": 1994,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "E. Shriberg. 1994. Preliminaries to a theory of speech disfluencies. Ph.D. thesis, Department of Psychol- ogy, University of California, Berkeley, CA.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Phonetic consequences of speech disfluency",
                "authors": [
                    {
                        "first": "E",
                        "middle": [],
                        "last": "Shriberg",
                        "suffix": ""
                    }
                ],
                "year": 1999,
                "venue": "Proc. International conference of Phonetics Sciences",
                "volume": "",
                "issue": "",
                "pages": "619--622",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "E. Shriberg. 1999. Phonetic consequences of speech disfluency. In Proc. International conference of Phonetics Sciences, pages 619-622.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "A prosody-only decision-tree model for disfluency detection",
                "authors": [
                    {
                        "first": "E",
                        "middle": [],
                        "last": "Shriberg",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Stolcke",
                        "suffix": ""
                    }
                ],
                "year": 1997,
                "venue": "Proc. Eurospeech",
                "volume": "",
                "issue": "",
                "pages": "2383--2386",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "E. Shriberg and A. Stolcke. 1997. A prosody-only decision-tree model for disfluency detection. In Proc. Eurospeech, pages 2383-2386.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "A prosody only decision-tree model for disfluency detection",
                "authors": [
                    {
                        "first": "Elizabeth",
                        "middle": [],
                        "last": "Shriberg",
                        "suffix": ""
                    },
                    {
                        "first": "Rebecca",
                        "middle": [],
                        "last": "Bates",
                        "suffix": ""
                    },
                    {
                        "first": "Andreas",
                        "middle": [],
                        "last": "Stolcke",
                        "suffix": ""
                    }
                ],
                "year": 1997,
                "venue": "Fifth European Conference on Speech Communication and Technology",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Elizabeth Shriberg, Rebecca Bates, and Andreas Stol- cke. 1997. A prosody only decision-tree model for disfluency detection. In Fifth European Conference on Speech Communication and Technology.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "A lexically-driven algorithm for disfluency detection",
                "authors": [
                    {
                        "first": "Matthew",
                        "middle": [],
                        "last": "Snover",
                        "suffix": ""
                    },
                    {
                        "first": "Bonnie",
                        "middle": [],
                        "last": "Dorr",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Schwartz",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proceedings of HLT-NAACL 2004: Short Papers",
                "volume": "",
                "issue": "",
                "pages": "157--160",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Matthew Snover, Bonnie Dorr, and Richard Schwartz. 2004. A lexically-driven algorithm for disfluency detection. In Proceedings of HLT-NAACL 2004: Short Papers, pages 157-160. Association for Com- putational Linguistics.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Joint multimodal learning with deep generative models",
                "authors": [
                    {
                        "first": "Masahiro",
                        "middle": [],
                        "last": "Suzuki",
                        "suffix": ""
                    },
                    {
                        "first": "Kotaro",
                        "middle": [],
                        "last": "Nakayama",
                        "suffix": ""
                    },
                    {
                        "first": "Yutaka",
                        "middle": [],
                        "last": "Matsuo",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1611.01891"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Masahiro Suzuki, Kotaro Nakayama, and Yutaka Mat- suo. 2016. Joint multimodal learning with deep gen- erative models. arXiv preprint arXiv:1611.01891.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Parsing speech: a neural approach to integrating lexical and acoustic-prosodic information",
                "authors": [
                    {
                        "first": "Trang",
                        "middle": [],
                        "last": "Tran",
                        "suffix": ""
                    },
                    {
                        "first": "Shubham",
                        "middle": [],
                        "last": "Toshniwal",
                        "suffix": ""
                    },
                    {
                        "first": "Mohit",
                        "middle": [],
                        "last": "Bansal",
                        "suffix": ""
                    },
                    {
                        "first": "Kevin",
                        "middle": [],
                        "last": "Gimpel",
                        "suffix": ""
                    },
                    {
                        "first": "Karen",
                        "middle": [],
                        "last": "Livescu",
                        "suffix": ""
                    },
                    {
                        "first": "Mari",
                        "middle": [],
                        "last": "Ostendorf",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proc. NAACL",
                "volume": "",
                "issue": "",
                "pages": "69--81",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Trang Tran, Shubham Toshniwal, Mohit Bansal, Kevin Gimpel, Karen Livescu, and Mari Ostendorf. 2018. Parsing speech: a neural approach to integrating lex- ical and acoustic-prosodic information. In Proc. NAACL, pages 69-81.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Semi-supervised disfluency detection",
                "authors": [
                    {
                        "first": "Feng",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Zhen",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Qianqian",
                        "middle": [],
                        "last": "Dong",
                        "suffix": ""
                    },
                    {
                        "first": "Shuang",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Bo",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 27th International Conference on Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "3529--3538",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Feng Wang, Wei Chen, Zhen Yang, Qianqian Dong, Shuang Xu, and Bo Xu. 2018. Semi-supervised dis- fluency detection. In Proceedings of the 27th Inter- national Conference on Computational Linguistics, pages 3529-3538.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "A neural attention model for disfluency detection",
                "authors": [
                    {
                        "first": "Shaolei",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Wanxiang",
                        "middle": [],
                        "last": "Che",
                        "suffix": ""
                    },
                    {
                        "first": "Ting",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers",
                "volume": "",
                "issue": "",
                "pages": "278--287",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Shaolei Wang, Wanxiang Che, and Ting Liu. 2016. A neural attention model for disfluency detection. In Proceedings of COLING 2016, the 26th Inter- national Conference on Computational Linguistics: Technical Papers, pages 278-287.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Transition-based disfluency detection using lstms",
                "authors": [
                    {
                        "first": "Shaolei",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Wanxiang",
                        "middle": [],
                        "last": "Che",
                        "suffix": ""
                    },
                    {
                        "first": "Yue",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Meishan",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Ting",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "2785--2794",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Shaolei Wang, Wanxiang Che, Yue Zhang, Meishan Zhang, and Ting Liu. 2017. Transition-based dis- fluency detection using lstms. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2785-2794.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "Show, attend and tell: Neural image caption generation with visual attention",
                "authors": [
                    {
                        "first": "Kelvin",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Jimmy",
                        "middle": [],
                        "last": "Ba",
                        "suffix": ""
                    },
                    {
                        "first": "Ryan",
                        "middle": [],
                        "last": "Kiros",
                        "suffix": ""
                    },
                    {
                        "first": "Kyunghyun",
                        "middle": [],
                        "last": "Cho",
                        "suffix": ""
                    },
                    {
                        "first": "Aaron",
                        "middle": [],
                        "last": "Courville",
                        "suffix": ""
                    },
                    {
                        "first": "Ruslan",
                        "middle": [],
                        "last": "Salakhudinov",
                        "suffix": ""
                    },
                    {
                        "first": "Rich",
                        "middle": [],
                        "last": "Zemel",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "International conference on machine learning",
                "volume": "",
                "issue": "",
                "pages": "2048--2057",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. 2015. Show, attend and tell: Neural image caption generation with visual atten- tion. In International conference on machine learn- ing, pages 2048-2057.",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "Tensor fusion network for multimodal sentiment analysis",
                "authors": [
                    {
                        "first": "Amir",
                        "middle": [],
                        "last": "Zadeh",
                        "suffix": ""
                    },
                    {
                        "first": "Minghai",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Soujanya",
                        "middle": [],
                        "last": "Poria",
                        "suffix": ""
                    },
                    {
                        "first": "Erik",
                        "middle": [],
                        "last": "Cambria",
                        "suffix": ""
                    },
                    {
                        "first": "Louis-Philippe",
                        "middle": [],
                        "last": "Morency",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1707.07250"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Amir Zadeh, Minghai Chen, Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. 2017. Ten- sor fusion network for multimodal sentiment analy- sis. arXiv preprint arXiv:1707.07250.",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "Multi-attention recurrent network for human communication comprehension",
                "authors": [
                    {
                        "first": "Amir",
                        "middle": [],
                        "last": "Zadeh",
                        "suffix": ""
                    },
                    {
                        "first": "Paul",
                        "middle": [
                            "Pu"
                        ],
                        "last": "Liang",
                        "suffix": ""
                    },
                    {
                        "first": "Soujanya",
                        "middle": [],
                        "last": "Poria",
                        "suffix": ""
                    },
                    {
                        "first": "Prateek",
                        "middle": [],
                        "last": "Vij",
                        "suffix": ""
                    },
                    {
                        "first": "Erik",
                        "middle": [],
                        "last": "Cambria",
                        "suffix": ""
                    },
                    {
                        "first": "Louis-Philippe",
                        "middle": [],
                        "last": "Morency",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1802.00923"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Amir Zadeh, Paul Pu Liang, Soujanya Poria, Prateek Vij, Erik Cambria, and Louis-Philippe Morency. 2018. Multi-attention recurrent network for hu- man communication comprehension. arXiv preprint arXiv:1802.00923.",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "Robust crossdomain disfluency detection with pattern match networks",
                "authors": [
                    {
                        "first": "Vicky",
                        "middle": [],
                        "last": "Zayats",
                        "suffix": ""
                    },
                    {
                        "first": "Mari",
                        "middle": [],
                        "last": "Ostendorf",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1811.07236"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Vicky Zayats and Mari Ostendorf. 2018. Robust cross- domain disfluency detection with pattern match net- works. arXiv preprint arXiv:1811.07236.",
                "links": null
            },
            "BIBREF34": {
                "ref_id": "b34",
                "title": "Disfluency detection using a bidirectional LSTM",
                "authors": [
                    {
                        "first": "Vicky",
                        "middle": [],
                        "last": "Zayats",
                        "suffix": ""
                    },
                    {
                        "first": "Mari",
                        "middle": [],
                        "last": "Ostendorf",
                        "suffix": ""
                    },
                    {
                        "first": "Hannaneh",
                        "middle": [],
                        "last": "Hajishirzi",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proc. Interspeech",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Vicky Zayats, Mari Ostendorf, and Hannaneh Ha- jishirzi. 2016. Disfluency detection using a bidirec- tional LSTM. In Proc. Interspeech.",
                "links": null
            },
            "BIBREF35": {
                "ref_id": "b35",
                "title": "Multidomain disfluency and repair detection",
                "authors": [
                    {
                        "first": "Victoria",
                        "middle": [],
                        "last": "Zayats",
                        "suffix": ""
                    },
                    {
                        "first": "Mari",
                        "middle": [],
                        "last": "Ostendorf",
                        "suffix": ""
                    },
                    {
                        "first": "Hannaneh",
                        "middle": [],
                        "last": "Hajishirzi",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proc. Interspeech",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Victoria Zayats, Mari Ostendorf, and Hannaneh Ha- jishirzi. 2014. Multidomain disfluency and repair detection. In Proc. Interspeech.",
                "links": null
            },
            "BIBREF36": {
                "ref_id": "b36",
                "title": "Detecting speech repairs incrementally using a noisy channel approach",
                "authors": [
                    {
                        "first": "Simon",
                        "middle": [],
                        "last": "Zwarts",
                        "suffix": ""
                    },
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Johnson",
                        "suffix": ""
                    },
                    {
                        "first": "Robert",
                        "middle": [],
                        "last": "Dale",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proc. Coling",
                "volume": "",
                "issue": "",
                "pages": "1371--1378",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Simon Zwarts, Mark Johnson, and Robert Dale. 2010. Detecting speech repairs incrementally using a noisy channel approach. In Proc. Coling, pages 1371- 1378.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "it's + {uh} it's] almost... [ was it, + {I mean} , did you ] put... [I just + I] enjoy working... [By + ] it was attached to...",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "can + still has the option of]... to keep them [in + uh quiet ]...",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "Contexts with fluent repetitions often include expressing a strong stance. a long long time ago... she has very very black and white...",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF3": {
                "num": null,
                "text": "Figure1: Prosody prediction (left) and late fusion (right) models. x i is a contcatenation of token, POS and identity features embeddings at time i; r i , j is a concatenation of stress and phone embeddings for phone j in token i; p i is a vector of prosodic cues; g i and h i are hidden states of token level and phone level LSTMs, correspondingly.",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF4": {
                "num": null,
                "text": "Figure 2: Histogram of innovations for word duration and energy features for words preceding an interruption point vs. fluent words.",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table><tr><td/><td colspan=\"4\">Reparandum Length</td><td>% in</td></tr><tr><td>Type</td><td>1-2</td><td colspan=\"4\">3-5 6-8 8+ type</td></tr><tr><td colspan=\"4\">repetition 1894 419 12</td><td>1</td><td>46%</td></tr><tr><td>rephrase</td><td colspan=\"3\">794 585 66</td><td>-</td><td>28%</td></tr><tr><td>restart</td><td>196</td><td>14</td><td>-</td><td>-</td><td>4%</td></tr><tr><td>nested*</td><td colspan=\"5\">149 262 158 118 13%</td></tr><tr><td colspan=\"6\">Table 1: Total word counts associated with reparanda</td></tr><tr><td colspan=\"6\">of different lengths and types of disfluencies. *Counts</td></tr><tr><td colspan=\"6\">for nested disfluencies exclude repetition tokens.</td></tr><tr><td/><td colspan=\"4\">Reparandum Length</td><td/></tr><tr><td>Type</td><td>1-2</td><td>3-5</td><td>6-8</td><td colspan=\"2\">8+ overall</td></tr><tr><td colspan=\"3\">repetition 0.99 0.99</td><td>1</td><td>1</td><td>0.99</td></tr><tr><td>rephrase</td><td colspan=\"3\">0.75 0.66 0.44</td><td>-</td><td>0.70</td></tr><tr><td>restart</td><td>0.41</td><td>0</td><td>-</td><td>-</td><td>0.39</td></tr><tr><td>nested  *</td><td colspan=\"4\">0.79 0.66 0.62 0.21</td><td>0.62</td></tr><tr><td colspan=\"6\">Table 2: Percent of reparandum tokens that were cor-</td></tr><tr><td colspan=\"6\">rectly predicted as disfluent. *Statistics for nested dis-</td></tr><tr><td colspan=\"4\">fluencies exclude repetition tokens.</td><td/><td/></tr></table>",
                "type_str": "table",
                "text": "addition, ungrammatical function word sequences are frequently",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table><tr><td>: Relative frequency of rephrases correctly pre-</td></tr><tr><td>dicted as disfluent for disfluencies that contain a con-</td></tr><tr><td>tent word in both the reparandum and repair (content-</td></tr><tr><td>content), either the reparandum or repair (content-</td></tr><tr><td>function) or in neither. Percentages in parentheses</td></tr><tr><td>show the fraction of tokens belong to each category.</td></tr></table>",
                "type_str": "table",
                "text": "are associated with contexts where we expect that prosodic cues are useful, specifically the five cases below, with examples from the development set.Restarts: Some disfluencies have no repair; the speaker simply restarts the sentence with no obvious parallel phrase.[ it would be + ] I think it's clear... well [the +] uh i think what changed...",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table><tr><td/><td>Model</td><td>dev</td><td>test</td><td>\u03b1</td></tr><tr><td/><td/><td colspan=\"2\">mean best mean best</td></tr><tr><td>single</td><td>text raw innovations</td><td colspan=\"2\">86.54 86.80 86.47 86.96 35.00 37.33 35.78 37.70 80.86 81.51 80.28 82.15</td><td>---</td></tr><tr><td>early</td><td colspan=\"3\">text + raw text + innovations text + raw + innovations 86.35 86.69 86.55 86.44 86.46 86.65 86.24 86.53 86.53 86.77 86.54 87.00</td><td>---</td></tr><tr><td/><td>text + raw</td><td colspan=\"3\">86.71 87.05 86.35 86.71 0.2</td></tr><tr><td>late</td><td>text + innovations</td><td colspan=\"3\">86.98 87.48 86.68 87.02 0.5</td></tr><tr><td/><td colspan=\"4\">text + raw + innovations 86.95 87.30 86.60 86.87 0.5</td></tr></table>",
                "type_str": "table",
                "text": "is a collection of telephone conversations between strangers,",
                "html": null,
                "num": null
            }
        }
    }
}