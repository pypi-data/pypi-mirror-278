{
    "paper_id": "P06-1071",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:13:19.721341Z"
    },
    "title": "A Progressive Feature Selection Algorithm for Ultra Large Feature Spaces",
    "authors": [
        {
            "first": "Qi",
            "middle": [],
            "last": "Zhang",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Fudan University",
                "location": {
                    "postCode": "200433",
                    "settlement": "Shanghai",
                    "country": "P.R. China"
                }
            },
            "email": "qi_zhang@fudan.edu.cn"
        },
        {
            "first": "Fuliang",
            "middle": [],
            "last": "Weng",
            "suffix": "",
            "affiliation": {},
            "email": "fuliang.weng@rtc.bosch.com"
        },
        {
            "first": "Zhe",
            "middle": [],
            "last": "Feng",
            "suffix": "",
            "affiliation": {},
            "email": "zhe.feng@rtc.bosch.com"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Recent developments in statistical modeling of various linguistic phenomena have shown that additional features give consistent performance improvements. Quite often, improvements are limited by the number of features a system is able to explore. This paper describes a novel progressive training algorithm that selects features from virtually unlimited feature spaces for conditional maximum entropy (CME) modeling. Experimental results in edit region identification demonstrate the benefits of the progressive feature selection (PFS) algorithm: the PFS algorithm maintains the same accuracy performance as previous CME feature selection algorithms (e.g., Zhou et al., 2003) when the same feature spaces are used. When additional features and their combinations are used, the PFS gives 17.66% relative improvement over the previously reported best result in edit region identification on Switchboard corpus (Kahn et al., 2005) , which leads to a 20% relative error reduction in parsing the Switchboard corpus when gold edits are used as the upper bound.",
    "pdf_parse": {
        "paper_id": "P06-1071",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Recent developments in statistical modeling of various linguistic phenomena have shown that additional features give consistent performance improvements. Quite often, improvements are limited by the number of features a system is able to explore. This paper describes a novel progressive training algorithm that selects features from virtually unlimited feature spaces for conditional maximum entropy (CME) modeling. Experimental results in edit region identification demonstrate the benefits of the progressive feature selection (PFS) algorithm: the PFS algorithm maintains the same accuracy performance as previous CME feature selection algorithms (e.g., Zhou et al., 2003) when the same feature spaces are used. When additional features and their combinations are used, the PFS gives 17.66% relative improvement over the previously reported best result in edit region identification on Switchboard corpus (Kahn et al., 2005) , which leads to a 20% relative error reduction in parsing the Switchboard corpus when gold edits are used as the upper bound.",
                "cite_spans": [
                    {
                        "start": 908,
                        "end": 927,
                        "text": "(Kahn et al., 2005)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Conditional Maximum Entropy (CME) modeling has received a great amount of attention within natural language processing community for the past decade (e.g., Berger et al., 1996; Reynar and Ratnaparkhi, 1997; Koeling, 2000; Malouf, 2002; Zhou et al., 2003; Riezler and Vasserman, 2004) . One of the main advantages of CME modeling is the ability to incorporate a variety of features in a uniform framework with a sound mathematical foundation. Recent improvements on the original incremental feature selection (IFS) algorithm, such as Malouf (2002) and Zhou et al. (2003) , greatly speed up the feature selection process. However, like many other statistical modeling algorithms, such as boosting (Schapire and Singer, 1999) and support vector machine (Vapnik 1995) , the algorithm is limited by the size of the defined feature space. Past results show that larger feature spaces tend to give better results. However, finding a way to include an unlimited amount of features is still an open research problem.",
                "cite_spans": [
                    {
                        "start": 156,
                        "end": 176,
                        "text": "Berger et al., 1996;",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 177,
                        "end": 206,
                        "text": "Reynar and Ratnaparkhi, 1997;",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 207,
                        "end": 221,
                        "text": "Koeling, 2000;",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 222,
                        "end": 235,
                        "text": "Malouf, 2002;",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 236,
                        "end": 254,
                        "text": "Zhou et al., 2003;",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 255,
                        "end": 283,
                        "text": "Riezler and Vasserman, 2004)",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 533,
                        "end": 546,
                        "text": "Malouf (2002)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 551,
                        "end": 569,
                        "text": "Zhou et al. (2003)",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 695,
                        "end": 722,
                        "text": "(Schapire and Singer, 1999)",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 750,
                        "end": 763,
                        "text": "(Vapnik 1995)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this paper, we propose a novel progressive feature selection (PFS) algorithm that addresses the feature space size limitation. The algorithm is implemented on top of the Selective Gain Computation (SGC) algorithm (Zhou et al., 2003) , which offers fast training and high quality models. Theoretically, the new algorithm is able to explore an unlimited amount of features. Because of the improved capability of the CME algorithm, we are able to consider many new features and feature combinations during model construction.",
                "cite_spans": [
                    {
                        "start": 216,
                        "end": 235,
                        "text": "(Zhou et al., 2003)",
                        "ref_id": "BIBREF22"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "To demonstrate the effectiveness of our new algorithm, we conducted a number of experiments on the task of identifying edit regions, a practical task in spoken language processing. Based on the convention from Shriberg (1994) and Charniak and Johnson (2001) , a disfluent spoken utterance is divided into three parts: the reparandum, the part that is repaired; the inter-regnum, which can be filler words or empty; and the repair/repeat, the part that replaces or repeats the reparandum. The first two parts combined are called an edit or edit region. An example is shown below: interregnum It is, you know, this is a tough problem.",
                "cite_spans": [
                    {
                        "start": 210,
                        "end": 225,
                        "text": "Shriberg (1994)",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 230,
                        "end": 257,
                        "text": "Charniak and Johnson (2001)",
                        "ref_id": "BIBREF1"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In section 2, we briefly review the CME modeling and SGC algorithm. Then, section 3 gives a detailed description of the PFS algorithm. In section 4, we describe the Switchboard corpus, features used in the experiments, and the effectiveness of the PFS with different feature spaces. Section 5 concludes the paper.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "reparandum repair",
                "sec_num": null
            },
            {
                "text": "Before presenting the PFS algorithm, we first give a brief review of the conditional maximum entropy modeling, its training process, and the SGC algorithm. This is to provide the background and motivation for our PFS algorithm.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Background",
                "sec_num": "2"
            },
            {
                "text": "The goal of CME is to find the most uniform conditional distribution of y given observation x, ( )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conditional Maximum Entropy Model",
                "sec_num": "2.1"
            },
            {
                "text": "x y p",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conditional Maximum Entropy Model",
                "sec_num": "2.1"
            },
            {
                "text": ", subject to constraints specified by a set of features ( ) y x f i , , where features typically take the value of either 0 or 1 (Berger et al., 1996) . More precisely, we want to maximize ( )",
                "cite_spans": [
                    {
                        "start": 129,
                        "end": 150,
                        "text": "(Berger et al., 1996)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conditional Maximum Entropy Model",
                "sec_num": "2.1"
            },
            {
                "text": "( ) ( ) ( ) ( ) x y p x y p x p p H y x log , \u2211 - = (1)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conditional Maximum Entropy Model",
                "sec_num": "2.1"
            },
            {
                "text": "given the constraints:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conditional Maximum Entropy Model",
                "sec_num": "2.1"
            },
            {
                "text": "( ) ( ) i i f E f E = (2) where ( ) ( ) ( ) \u2211 = y x i i y x f y x p f E , , , ~",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conditional Maximum Entropy Model",
                "sec_num": "2.1"
            },
            {
                "text": "is the empirical expected feature count from the training data and",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conditional Maximum Entropy Model",
                "sec_num": "2.1"
            },
            {
                "text": "( ) ( ) ( ) ( ) \u2211 = y x i i y x f x y p x p f E , , ~",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conditional Maximum Entropy Model",
                "sec_num": "2.1"
            },
            {
                "text": "is the feature expectation from the conditional model ( )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conditional Maximum Entropy Model",
                "sec_num": "2.1"
            },
            {
                "text": "x y p",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conditional Maximum Entropy Model",
                "sec_num": "2.1"
            },
            {
                "text": ". This results in the following exponential model:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conditional Maximum Entropy Model",
                "sec_num": "2.1"
            },
            {
                "text": "( ) ( ) ( ) \u239f \u239f \u23a0 \u239e \u239c \u239c \u239d \u239b = \u2211 j j j y x f x Z x y p , exp 1 \u03bb (3)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conditional Maximum Entropy Model",
                "sec_num": "2.1"
            },
            {
                "text": "where \u03bb j is the weight corresponding to the feature f j , and Z(x) is a normalization factor. A variety of different phenomena, including lexical, structural, and semantic aspects, in natural language processing tasks can be expressed in terms of features. For example, a feature can be whether the word in the current position is a verb, or the word is a particular lexical item. A feature can also be about a particular syntactic subtree, or a dependency relation (e.g., Charniak and Johnson, 2005) .",
                "cite_spans": [
                    {
                        "start": 474,
                        "end": 501,
                        "text": "Charniak and Johnson, 2005)",
                        "ref_id": "BIBREF2"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conditional Maximum Entropy Model",
                "sec_num": "2.1"
            },
            {
                "text": "In real world applications, the number of possible features can be in the millions or beyond. Including all the features in a model may lead to data over-fitting, as well as poor efficiency and memory overflow. Good feature selection algorithms are required to produce efficient and high quality models. This leads to a good amount of work in this area (Ratnaparkhi et al., 1994; Berger et al., 1996; Pietra et al, 1997; Zhou et al., 2003; Riezler and Vasserman, 2004) In the most basic approach, such as Ratnaparkhi et al. (1994) and Berger et al. (1996) , training starts with a uniform distribution over all values of y and an empty feature set. For each candidate feature in a predefined feature space, it computes the likelihood gain achieved by including the feature in the model. The feature that maximizes the gain is selected and added to the current model. This process is repeated until the gain from the best candidate feature only gives marginal improvement. The process is very slow, because it has to re-compute the gain for every feature at each selection stage, and the computation of a parameter using Newton's method becomes expensive, considering that it has to be repeated many times.",
                "cite_spans": [
                    {
                        "start": 353,
                        "end": 379,
                        "text": "(Ratnaparkhi et al., 1994;",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 380,
                        "end": 400,
                        "text": "Berger et al., 1996;",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 401,
                        "end": 420,
                        "text": "Pietra et al, 1997;",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 421,
                        "end": 439,
                        "text": "Zhou et al., 2003;",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 440,
                        "end": 468,
                        "text": "Riezler and Vasserman, 2004)",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 505,
                        "end": 530,
                        "text": "Ratnaparkhi et al. (1994)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 535,
                        "end": 555,
                        "text": "Berger et al. (1996)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Selective Gain Computation Algorithm",
                "sec_num": "2.2"
            },
            {
                "text": "The idea behind the SGC algorithm (Zhou et al., 2003) is to use the gains computed in the previous step as approximate upper bounds for the subsequent steps. The gain for a feature needs to be re-computed only when the feature reaches the top of a priority queue ordered by gain. In other words, this happens when the feature is the top candidate for inclusion in the model. If the re-computed gain is smaller than that of the next candidate in the list, the feature is re-ranked according to its newly computed gain, and the feature now at the top of the list goes through the same gain re-computing process.",
                "cite_spans": [
                    {
                        "start": 34,
                        "end": 53,
                        "text": "(Zhou et al., 2003)",
                        "ref_id": "BIBREF22"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Selective Gain Computation Algorithm",
                "sec_num": "2.2"
            },
            {
                "text": "This heuristics comes from evidences that the gains become smaller and smaller as more and more good features are added to the model. This can be explained as follows: assume that the Maximum Likelihood (ML) estimation lead to the best model that reaches a ML value. The ML value is the upper bound. Since the gains need to be positive to proceed the process, the difference between the Likelihood of the current and the ML value becomes smaller and smaller. In other words, the possible gain each feature may add to the model gets smaller. Experiments in Zhou et al. (2003) also confirm the prediction that the gains become smaller when more and more features are added to the model, and the gains do not get unexpectively bigger or smaller as the model grows. Furthermore, the experiments in Zhou et al. (2003) show no significant advantage for looking ahead beyond the first element in the feature list. The SGC algorithm runs hundreds to thousands of times faster than the original IFS algorithm without degrading classification performance. We used this algorithm for it enables us to find high quality CME models quickly.",
                "cite_spans": [
                    {
                        "start": 556,
                        "end": 574,
                        "text": "Zhou et al. (2003)",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 794,
                        "end": 812,
                        "text": "Zhou et al. (2003)",
                        "ref_id": "BIBREF22"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Selective Gain Computation Algorithm",
                "sec_num": "2.2"
            },
            {
                "text": "The original SGC algorithm uses a technique proposed by Darroch and Ratcliff (1972) and elaborated by Goodman (2002) : when considering a feature f i , the algorithm only modifies those un-normalized conditional probabilities:",
                "cite_spans": [
                    {
                        "start": 56,
                        "end": 83,
                        "text": "Darroch and Ratcliff (1972)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 102,
                        "end": 116,
                        "text": "Goodman (2002)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Selective Gain Computation Algorithm",
                "sec_num": "2.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "( ) ( ) \u2211 j j j y x f , exp",
                        "eq_num": "\u03bb"
                    }
                ],
                "section": "Selective Gain Computation Algorithm",
                "sec_num": "2.2"
            },
            {
                "text": "for (x, y) that satisfy f i (x, y)=1, and subsequently adjusts the corresponding normalizing factors Z(x) in ( 3). An implementation often uses a mapping table, which maps features to the training instance pairs (x, y).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Selective Gain Computation Algorithm",
                "sec_num": "2.2"
            },
            {
                "text": "In general, the more contextual information is used, the better a system performs. However, richer context can lead to combinatorial explosion of the feature space. When the feature space is huge (e.g., in the order of tens of millions of features or even more), the SGC algorithm exceeds the memory limitation on commonly available computing platforms with gigabytes of memory.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Progressive Feature Selection Algorithm",
                "sec_num": "3"
            },
            {
                "text": "To address the limitation of the SGC algorithm, we propose a progressive feature selection algorithm that selects features in multiple rounds. The main idea of the PFS algorithm is to split the feature space into tractable disjoint sub-spaces such that the SGC algorithm can be performed on each one of them. In the merge step, the features that SGC selects from different sub-spaces are merged into groups. Instead of re-generating the feature-to-instance mapping table for each sub-space during the time of splitting and merging, we create the new mapping table from the previous round's tables by collecting those entries that correspond to the selected features. Then, the SGC algorithm is performed on each of the feature groups and new features are selected from each of them. In other words, the feature space splitting and subspace merging are performed mainly on the feature-to-instance mapping tables. This is a key step that leads to this very efficient PFS algorithm.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Progressive Feature Selection Algorithm",
                "sec_num": "3"
            },
            {
                "text": "At the beginning of each round for feature selection, a uniform prior distribution is always assumed for the new CME model. A more precise description of the PFS algorithm is given in Table 1 , and it is also graphically illustrated in Figure 1 .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 190,
                        "end": 191,
                        "text": "1",
                        "ref_id": null
                    },
                    {
                        "start": 243,
                        "end": 244,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Progressive Feature Selection Algorithm",
                "sec_num": "3"
            },
            {
                "text": "Feature space",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Given:",
                "sec_num": null
            },
            {
                "text": "F (0) = {f 1 (0) , f 2 (0) , \u2026, f N (0) }, step_num = m, select_factor = s 1. Split the feature space into N 1 parts {F 1 (1) , F 2 (1) , \u2026, F N1 (1) } = split(F (0) ) 2. for k=1 to m-1 do //2.1 Feature selection for each feature space F i (k) do FS i (k) = SGC(F i (k) , s) //2.2 Combine selected features {F 1 (k+1) , \u2026, F Nk+1 (k+1) } = merge(FS 1 (k) , \u2026, FS Nk (k) ) 3. Final feature selection & optimization F (m) = merge(FS 1 (m-1) , \u2026, FS Nm-1 (m-1) ) FS (m) = SGC(F (m) , s) M final = Opt(FS (m) ) Table 1. The PFS algorithm. M ) 2 ( 1 F ) 1 ( 1 FS ) 1 ( 1 i FS M M ) 1 ( 2 i FS M ) 1 ( 1 N FS L select Step 1",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Given:",
                "sec_num": null
            },
            {
                "text": "Step m",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Given:",
                "sec_num": null
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": ") 1 (",
                        "eq_num": "1"
                    }
                ],
                "section": "Given:",
                "sec_num": null
            },
            {
                "text": "F ) 1 ( 1 i F M M ) 1 ( 2 i F M ) 1 ( 1 N F ) 2 ( 1 FS ) 2 ( 2 N FS ) (m F M merge Step 2 ) 0 ( F Split select merge select ) 2 ( 2 N F M final ) (m FS optimize Figure 1. Graphic illustration of PFS algorithm.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Given:",
                "sec_num": null
            },
            {
                "text": "In Table 1 , SGC() invokes the SGC algorithm, and Opt() optimizes feature weights. The functions split() and merge() are used to split and merge the feature space respectively.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 9,
                        "end": 10,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Given:",
                "sec_num": null
            },
            {
                "text": "Two variations of the split() function are investigated in the paper and they are described below:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Given:",
                "sec_num": null
            },
            {
                "text": "1. random-split: randomly split a feature space into n-disjoint subspaces, and select an equal amount of features for each feature subspace. 2. dimension-based-split: split a feature space into disjoint subspaces based on fea-ture dimensions/variables, and select the number of features for each feature subspace with a certain distribution. We use a simple method for merge() in the experiments reported here, i.e., adding together the features from a set of selected feature subspaces.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Given:",
                "sec_num": null
            },
            {
                "text": "One may image other variations of the split() function, such as allowing overlapping subspaces. Other alternatives for merge() are also possible, such as randomly grouping the selected feature subspaces in the dimension-based split. Due to the limitation of the space, they are not discussed here.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Given:",
                "sec_num": null
            },
            {
                "text": "This approach can in principle be applied to other machine learning algorithms as well.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Given:",
                "sec_num": null
            },
            {
                "text": "In this section, we will demonstrate the benefits of the PFS algorithm for identifying edit regions.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments with PFS for Edit Region Identification",
                "sec_num": "4"
            },
            {
                "text": "The main reason that we use this task is that the edit region detection task uses features from several levels, including prosodic, lexical, and syntactic ones. It presents a big challenge to find a set of good features from a huge feature space. First we will present the additional features that the PFS algorithm allows us to include. Then, we will briefly introduce the variant of the Switchboard corpus used in the experiments. Finally, we will compare results from two variants of the PFS algorithm.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments with PFS for Edit Region Identification",
                "sec_num": "4"
            },
            {
                "text": "In spoken utterances, disfluencies, such as selfediting, pauses and repairs, are common phenomena. Charniak and Johnson (2001) and Kahn et al. (2005) have shown that improved edit region identification leads to better parsing accuracy -they observe a relative reduction in parsing f-score error of 14% (2% absolute) between automatic and oracle edit removal.",
                "cite_spans": [
                    {
                        "start": 99,
                        "end": 126,
                        "text": "Charniak and Johnson (2001)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 131,
                        "end": 149,
                        "text": "Kahn et al. (2005)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Edit Region Identification Task",
                "sec_num": "4.1"
            },
            {
                "text": "The focus of our work is to show that our new PFS algorithm enables the exploration of much larger feature spaces for edit identification -including prosodic features, their confidence scores, and various feature combinations -and consequently, it further improves edit region identification. Memory limitation prevents us from including all of these features in experiments using the boosting method described in Johnson and Charniak (2004) and Zhang and Weng (2005) . We couldn't use the new features with the SGC algorithm either for the same reason.",
                "cite_spans": [
                    {
                        "start": 414,
                        "end": 441,
                        "text": "Johnson and Charniak (2004)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 446,
                        "end": 467,
                        "text": "Zhang and Weng (2005)",
                        "ref_id": "BIBREF21"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Edit Region Identification Task",
                "sec_num": "4.1"
            },
            {
                "text": "The features used here are grouped according to variables, which define feature sub-spaces as in Charniak and Johnson (2001) and Zhang and Weng (2005) . In this work, we use a total of 62 variables, which include 161 variables from Charniak and Johnson (2001) and Johnson and Charniak (2004) , an additional 29 variables from Zhang and Weng (2005) , 11 hierarchical POS tag variables, and 8 prosody variables (labels and their confidence scores). Furthermore, we explore 377 combinations of these 62 variables, which include 40 combinations from Zhang and Weng (2005) . The complete list of the variables is given in Table 2 , and the combinations used in the experiments are given in Table 3 . One additional note is that some features are obtained after the rough copy procedure is performed, where we used the same procedure as the one by Zhang and Weng (2005) . For a fair comparison with the work by Kahn et al. (2005) , word fragment information is retained.",
                "cite_spans": [
                    {
                        "start": 97,
                        "end": 124,
                        "text": "Charniak and Johnson (2001)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 129,
                        "end": 150,
                        "text": "Zhang and Weng (2005)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 232,
                        "end": 259,
                        "text": "Charniak and Johnson (2001)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 264,
                        "end": 291,
                        "text": "Johnson and Charniak (2004)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 326,
                        "end": 347,
                        "text": "Zhang and Weng (2005)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 546,
                        "end": 567,
                        "text": "Zhang and Weng (2005)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 842,
                        "end": 863,
                        "text": "Zhang and Weng (2005)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 905,
                        "end": 923,
                        "text": "Kahn et al. (2005)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 623,
                        "end": 624,
                        "text": "2",
                        "ref_id": "TABREF0"
                    },
                    {
                        "start": 691,
                        "end": 692,
                        "text": "3",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Edit Region Identification Task",
                "sec_num": "4.1"
            },
            {
                "text": "In order to include prosodic features and be able to compare with the state-oft-art, we use the University of Washington re-segmented Switchboard corpus, described in Kahn et al. (2005) . In this corpus, the Switchboard sentences were segmented into V5-style sentence-like units (SUs) (LDC, 2004) . The resulting sentences fit more closely with the boundaries that can be detected through automatic procedures (e.g., Liu et al., 2005) . Because the edit region identification results on the original Switchboard are not directly comparable with the results on the newly segmented data, the state-of-art results reported by Charniak and Johnson (2001) and Johnson and Charniak (2004) are repeated on this new corpus by Kahn et al. (2005) .",
                "cite_spans": [
                    {
                        "start": 167,
                        "end": 185,
                        "text": "Kahn et al. (2005)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 285,
                        "end": 296,
                        "text": "(LDC, 2004)",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 417,
                        "end": 434,
                        "text": "Liu et al., 2005)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 623,
                        "end": 650,
                        "text": "Charniak and Johnson (2001)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 655,
                        "end": 682,
                        "text": "Johnson and Charniak (2004)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 718,
                        "end": 736,
                        "text": "Kahn et al. (2005)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Re-segmented Switchboard Data",
                "sec_num": "4.2"
            },
            {
                "text": "The re-segmented UW Switchboard corpus is labeled with a simplified subset of the ToBI prosodic system (Ostendorf et al., 2001) . The three simplified labels in the subset are p, 1 and 4, where p refers to a general class of disfluent boundaries (e.g., word fragments, abruptly shortened words, and hesitation); 4 refers to break level 4, which describes a boundary that has a boundary tone and phrase-final lengthening; Table 3 . All the variable combinations used in the experiments.",
                "cite_spans": [
                    {
                        "start": 103,
                        "end": 127,
                        "text": "(Ostendorf et al., 2001)",
                        "ref_id": "BIBREF13"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 427,
                        "end": 428,
                        "text": "3",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "The Re-segmented Switchboard Data",
                "sec_num": "4.2"
            },
            {
                "text": "and 1 is used to include the break index levels BL 0, 1, 2, and 3. Since the majority of the corpus is labeled via automatic methods, the fscores for the prosodic labels are not high. In particular, 4 and p have f-scores of about 70% and 60% respectively (Wong et al., 2005) . Therefore, in our experiments, we also take prosody confidence scores into consideration.",
                "cite_spans": [
                    {
                        "start": 255,
                        "end": 274,
                        "text": "(Wong et al., 2005)",
                        "ref_id": "BIBREF20"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Re-segmented Switchboard Data",
                "sec_num": "4.2"
            },
            {
                "text": "Besides the symbolic prosody labels, the corpus preserves the majority of the previously annotated syntactic information as well as edit region labels.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Re-segmented Switchboard Data",
                "sec_num": "4.2"
            },
            {
                "text": "In following experiments, to make the results comparable, the same data subsets described in Kahn et al. (2005) are used for training, developing and testing.",
                "cite_spans": [
                    {
                        "start": 93,
                        "end": 111,
                        "text": "Kahn et al. (2005)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Re-segmented Switchboard Data",
                "sec_num": "4.2"
            },
            {
                "text": "The best result on the UW Switchboard for edit region identification uses a TAG-based approach (Kahn et al., 2005) . On the original Switchboard corpus, Zhang and Weng (2005) reported nearly 20% better results using the boosting method with a much larger feature space 2 . To allow comparison with the best past results, we create a new CME baseline with the same set of features as that used in Zhang and Weng (2005) .",
                "cite_spans": [
                    {
                        "start": 95,
                        "end": 114,
                        "text": "(Kahn et al., 2005)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 153,
                        "end": 174,
                        "text": "Zhang and Weng (2005)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 396,
                        "end": 417,
                        "text": "Zhang and Weng (2005)",
                        "ref_id": "BIBREF21"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "4.3"
            },
            {
                "text": "We design a number of experiments to test the following hypotheses:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "4.3"
            },
            {
                "text": "1. PFS can include a huge number of new features, which leads to an overall performance improvement. 2. Richer context, represented by the combinations of different variables, has a positive impact on performance. 3. When the same feature space is used, PFS performs equally well as the original SGC algorithm. The new models from the PFS algorithm are trained on the training data and tuned on the development data. The results of our experiments on the test data are summarized in Table 4 . The first three lines show that the TAG-based approach is outperformed by the new CME baseline (line 3) using all the features in Zhang and Weng (2005) . However, the improvement from 2 PFS is not applied to the boosting algorithm at this time because it would require significant changes to the available algorithm. CME is significantly smaller than the reported results using the boosting method. In other words, using CME instead of boosting incurs a performance hit.",
                "cite_spans": [
                    {
                        "start": 623,
                        "end": 644,
                        "text": "Zhang and Weng (2005)",
                        "ref_id": "BIBREF21"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 489,
                        "end": 490,
                        "text": "4",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "4.3"
            },
            {
                "text": "The next four lines in Table 4 show that additional combinations of the feature variables used in Zhang and Weng (2005) give an absolute improvement of more than 1%. This improvement is realized through increasing the search space to more than 20 million features, 8 times the maximum size that the original boosting and CME algorithms are able to handle.",
                "cite_spans": [
                    {
                        "start": 98,
                        "end": 119,
                        "text": "Zhang and Weng (2005)",
                        "ref_id": "BIBREF21"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 29,
                        "end": 30,
                        "text": "4",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Results on test data",
                "sec_num": null
            },
            {
                "text": "Table 4 shows that prosody labels alone make no difference in performance. Instead, for each position in the sentence, we compute the entropy of the distribution of the labels' confidence scores. We normalize the entropy to the range [0, 1], according to the formula below:",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "4",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Results on test data",
                "sec_num": null
            },
            {
                "text": "( ) ( ) Uniform H p H score - = 1 (4)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results on test data",
                "sec_num": null
            },
            {
                "text": "Including this feature does result in a good improvement. In the table, cut2 means that we equally divide the feature scores into 10 buckets and any number below 0.2 is ignored. The total contribution from the combined feature variables leads to a 1.9% absolute improvement. This confirms the first two hypotheses.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results on test data",
                "sec_num": null
            },
            {
                "text": "When Gaussian smoothing (Chen and Rosenfeld, 1999) , labeled as +Gau, and postprocessing (Zhang and Weng, 2005) , labeled as +post, are added, we observe 17.66% relative improvement (or 3.85% absolute) over the previous best f-score of 78.2 from Kahn et al. (2005) .",
                "cite_spans": [
                    {
                        "start": 24,
                        "end": 50,
                        "text": "(Chen and Rosenfeld, 1999)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 89,
                        "end": 111,
                        "text": "(Zhang and Weng, 2005)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 246,
                        "end": 264,
                        "text": "Kahn et al. (2005)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results on test data",
                "sec_num": null
            },
            {
                "text": "To test hypothesis 3, we are constrained to the feature spaces that both PFS and SGC algorithms can process. Therefore, we take all the variables from Zhang and Weng (2005) as the feature space for the experiments. The results are listed in Table 5 . We observed no f-score degradation with PFS. Surprisingly, the total amount of time PFS spends on selecting its best features is smaller than the time SGC uses in selecting its best features. This confirms our hypothesis 3. Zhang and Weng (2005) .",
                "cite_spans": [
                    {
                        "start": 151,
                        "end": 172,
                        "text": "Zhang and Weng (2005)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 475,
                        "end": 496,
                        "text": "Zhang and Weng (2005)",
                        "ref_id": "BIBREF21"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 247,
                        "end": 248,
                        "text": "5",
                        "ref_id": "TABREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Results on test data",
                "sec_num": null
            },
            {
                "text": "The last set of experiments for edit identification is designed to find out what split strategies PFS algorithm should adopt in order to obtain good results. Two different split strategies are tested here. In all the experiments reported so far, we use 10 random splits, i.e., all the features are randomly assigned to 10 subsets of equal size. We may also envision a split strategy that divides the features based on feature variables (or dimensions), such as word-based, tag-based, etc. The four dimensions used in the experiments are listed as the top categories in Tables 2 and 3 , and the results are given in Table 6 . In Table 6 , the first two columns show criteria for splitting feature spaces and the number of features to be allocated for each group. Random and Dimension mean random-split and dimension-based-split, respectively. When the criterion is Random, the features are allocated to different groups randomly, and each group gets the same number of features. In the case of dimensionbased split, we determine the number of features allocated for each dimension in two ways. When the split is Uniform, the same number of features is allocated for each dimension. When the split is Prior, the number of features to be allocated in each dimension is determined in proportion to the importance of each dimension. To determine the importance, we use the distribution of the selected features from each dimension in the model \"+ HTag + HTagComb + WTComb + RCComb + PComb: cut2\", namely: Word-based 15%, Tag-based 70%, RoughCopy-based 7.5% and Prosody-based 7.5%3 . From the results, we can see no significant difference between the random-split and the dimension-based-split.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 576,
                        "end": 577,
                        "text": "2",
                        "ref_id": "TABREF0"
                    },
                    {
                        "start": 582,
                        "end": 583,
                        "text": "3",
                        "ref_id": null
                    },
                    {
                        "start": 621,
                        "end": 622,
                        "text": "6",
                        "ref_id": null
                    },
                    {
                        "start": 634,
                        "end": 635,
                        "text": "6",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Results on test data",
                "sec_num": null
            },
            {
                "text": "To see whether the improvements are translated into parsing results, we have conducted one more set of experiments on the UW Switchboard corpus. We apply the latest version of Charniak's parser (2005-08-16 ) and the same procedure as Charniak and Johnson (2001) and Kahn et al. (2005) to the output from our best edit detector in this paper. To make it more comparable with the results in Kahn et al. (2005) , we repeat the same experiment with the gold edits, using the latest parser. Both results are listed in Table 7 . The difference between our best detector and the gold edits in parsing (1.51%) is smaller than the difference between the TAG-based detector and the gold edits (1.9%). In other words, if we use the gold edits as the upper bound, we see a relative error reduction of 20.5%. ",
                "cite_spans": [
                    {
                        "start": 176,
                        "end": 205,
                        "text": "Charniak's parser (2005-08-16",
                        "ref_id": null
                    },
                    {
                        "start": 234,
                        "end": 261,
                        "text": "Charniak and Johnson (2001)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 266,
                        "end": 284,
                        "text": "Kahn et al. (2005)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 389,
                        "end": 407,
                        "text": "Kahn et al. (2005)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 519,
                        "end": 520,
                        "text": "7",
                        "ref_id": "TABREF5"
                    }
                ],
                "eq_spans": [],
                "section": "Results on test data",
                "sec_num": null
            },
            {
                "text": "This paper presents our progressive feature selection algorithm that greatly extends the feature space for conditional maximum entropy modeling. The new algorithm is able to select features from feature space in the order of tens of millions in practice, i.e., 8 times the maximal size previous algorithms are able to process, and unlimited space size in theory. Experiments on edit region identification task have shown that the increased feature space leads to 17.66% relative improvement (or 3.85% absolute) over the best result reported by Kahn et al. (2005) , and 10.65% relative improvement (or 2.14% absolute) over the new baseline SGC algorithm with all the variables from Zhang and Weng (2005) .",
                "cite_spans": [
                    {
                        "start": 544,
                        "end": 562,
                        "text": "Kahn et al. (2005)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 681,
                        "end": 702,
                        "text": "Zhang and Weng (2005)",
                        "ref_id": "BIBREF21"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "5"
            },
            {
                "text": "We also show that symbolic prosody labels together with confidence scores are useful in edit region identification task.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "5"
            },
            {
                "text": "In addition, the improvements in the edit identification lead to a relative 20% error reduction in parsing disfluent sentences when gold edits are used as the upper bound.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "5"
            },
            {
                "text": "Among the original 18 variables, two variables, P f and T f are not used in our experiments, because they are mostly covered by the other variables. Partial word flags only contribute to 3 features in the final selected feature list.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "It is a bit of cheating to use the distribution from the selected model. However, even with this distribution, we do not see any improvement over the version with randomsplit.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "This work is partly sponsored by a NIST ATP funding. The authors would like to express their many thanks to Mari Ostendorf and Jeremy Kahn for providing us with the re-segmented UW Switchboard Treebank and the corresponding prosodic labels. Our thanks also go to Jeff Russell for his careful proof reading, and the anonymous reviewers for their useful comments. All the remaining errors are ours.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgement",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "A Maximum Entropy Approach to Natural Language Processing",
                "authors": [
                    {
                        "first": "Adam",
                        "middle": [
                            "L"
                        ],
                        "last": "Berger",
                        "suffix": ""
                    },
                    {
                        "first": "Stephen",
                        "middle": [
                            "A"
                        ],
                        "last": "Della Pietra",
                        "suffix": ""
                    },
                    {
                        "first": "Vincent",
                        "middle": [
                            "J"
                        ],
                        "last": "Della",
                        "suffix": ""
                    },
                    {
                        "first": "Pietra",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 1996,
                "venue": "Computational Linguistics",
                "volume": "22",
                "issue": "1",
                "pages": "39--71",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Adam L. Berger, Stephen A. Della Pietra, and Vin- cent J. Della Pietra. 1996. A Maximum Entropy Approach to Natural Language Processing. Com- putational Linguistics, 22 (1): 39-71.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Edit Detection and Parsing for Transcribed Speech",
                "authors": [
                    {
                        "first": "Eugene",
                        "middle": [],
                        "last": "Charniak",
                        "suffix": ""
                    },
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Johnson",
                        "suffix": ""
                    }
                ],
                "year": 2001,
                "venue": "Proceedings of the 2 nd Meeting of the North American Chapter",
                "volume": "",
                "issue": "",
                "pages": "118--126",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Eugene Charniak and Mark Johnson. 2001. Edit De- tection and Parsing for Transcribed Speech. In Proceedings of the 2 nd Meeting of the North Ameri- can Chapter of the Association for Computational Linguistics, 118-126, Pittsburgh, PA, USA.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Coarse-tofine n-best Parsing and MaxEnt Discriminative Reranking",
                "authors": [
                    {
                        "first": "Eugene",
                        "middle": [],
                        "last": "Charniak",
                        "suffix": ""
                    },
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Johnson",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of the 43 rd Annual Meeting of Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "173--180",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Eugene Charniak and Mark Johnson. 2005. Coarse-to- fine n-best Parsing and MaxEnt Discriminative Reranking. In Proceedings of the 43 rd Annual Meeting of Association for Computational Linguis- tics, 173-180, Ann Arbor, MI, USA.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "A Gaussian Prior for Smoothing Maximum Entropy Models",
                "authors": [
                    {
                        "first": "Stanley",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Ronald",
                        "middle": [],
                        "last": "Rosenfeld",
                        "suffix": ""
                    }
                ],
                "year": 1999,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Stanley Chen and Ronald Rosenfeld. 1999. A Gaus- sian Prior for Smoothing Maximum Entropy Mod- els. Technical Report CMUCS-99-108, Carnegie Mellon University.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Generalized Iterative Scaling for Log-Linear Models",
                "authors": [
                    {
                        "first": "John",
                        "middle": [
                            "N"
                        ],
                        "last": "Darroch",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Ratcliff",
                        "suffix": ""
                    }
                ],
                "year": 1972,
                "venue": "Annals of Mathematical Statistics",
                "volume": "43",
                "issue": "5",
                "pages": "1470--1480",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "John N. Darroch and D. Ratcliff. 1972. Generalized Iterative Scaling for Log-Linear Models. In Annals of Mathematical Statistics, 43(5): 1470-1480.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Inducing Features of Random Fields",
                "authors": [
                    {
                        "first": "Stephen",
                        "middle": [
                            "A"
                        ],
                        "last": "Della Pietra",
                        "suffix": ""
                    },
                    {
                        "first": "Vincent",
                        "middle": [
                            "J"
                        ],
                        "last": "Della Pietra",
                        "suffix": ""
                    },
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Lafferty",
                        "suffix": ""
                    }
                ],
                "year": 1997,
                "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "volume": "19",
                "issue": "",
                "pages": "380--393",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Stephen A. Della Pietra, Vincent J. Della Pietra, and John Lafferty. 1997. Inducing Features of Random Fields. In IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(4): 380-393.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Sequential Conditional Generalized Iterative Scaling",
                "authors": [
                    {
                        "first": "Joshua",
                        "middle": [],
                        "last": "Goodman",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Proceedings of the 40 th Annual Meeting of Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "9--16",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Joshua Goodman. 2002. Sequential Conditional Gen- eralized Iterative Scaling. In Proceedings of the 40 th Annual Meeting of Association for Computa- tional Linguistics, 9-16, Philadelphia, PA, USA.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "A TAGbased noisy-channel model of speech repairs",
                "authors": [
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Johnson",
                        "suffix": ""
                    },
                    {
                        "first": "Eugene",
                        "middle": [],
                        "last": "Charniak",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proceedings of the 42 nd Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "33--39",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mark Johnson, and Eugene Charniak. 2004. A TAG- based noisy-channel model of speech repairs. In Proceedings of the 42 nd Annual Meeting of the As- sociation for Computational Linguistics, 33-39, Barcelona, Spain.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Effective Use of Prosody in Parsing Conversational Speech",
                "authors": [
                    {
                        "first": "Jeremy",
                        "middle": [
                            "G"
                        ],
                        "last": "Kahn",
                        "suffix": ""
                    },
                    {
                        "first": "Matthew",
                        "middle": [],
                        "last": "Lease",
                        "suffix": ""
                    },
                    {
                        "first": "Eugene",
                        "middle": [],
                        "last": "Charniak",
                        "suffix": ""
                    },
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Johnson",
                        "suffix": ""
                    },
                    {
                        "first": "Mari",
                        "middle": [],
                        "last": "Ostendorf",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of the 2005 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "233--240",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jeremy G. Kahn, Matthew Lease, Eugene Charniak, Mark Johnson, and Mari Ostendorf. 2005. Effec- tive Use of Prosody in Parsing Conversational Speech. In Proceedings of the 2005 Conference on Empirical Methods in Natural Language Process- ing, 233-240, Vancouver, Canada.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Chunking with Maximum Entropy Models",
                "authors": [
                    {
                        "first": "Rob",
                        "middle": [],
                        "last": "Koeling",
                        "suffix": ""
                    }
                ],
                "year": 2000,
                "venue": "Proceedings of the CoNLL-2000 and LLL-2000",
                "volume": "",
                "issue": "",
                "pages": "139--141",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Rob Koeling. 2000. Chunking with Maximum En- tropy Models. In Proceedings of the CoNLL-2000 and LLL-2000, 139-141, Lisbon, Portugal.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Simple MetaData Annotation Specification",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Ldc",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "of Linguistic Data Consortium",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "LDC. 2004. Simple MetaData Annotation Specifica- tion. Technical Report of Linguistic Data Consor- tium. (http://www.ldc.upenn.edu/Projects/MDE).",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Structural Metadata Research in the EARS Program",
                "authors": [
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Elizabeth",
                        "middle": [],
                        "last": "Shriberg",
                        "suffix": ""
                    },
                    {
                        "first": "Andreas",
                        "middle": [],
                        "last": "Stolcke",
                        "suffix": ""
                    },
                    {
                        "first": "Barbara",
                        "middle": [],
                        "last": "Peskin",
                        "suffix": ""
                    },
                    {
                        "first": "Jeremy",
                        "middle": [],
                        "last": "Ang",
                        "suffix": ""
                    },
                    {
                        "first": "Dustin",
                        "middle": [],
                        "last": "Hillard",
                        "suffix": ""
                    },
                    {
                        "first": "Mari",
                        "middle": [],
                        "last": "Ostendorf",
                        "suffix": ""
                    },
                    {
                        "first": "Marcus",
                        "middle": [],
                        "last": "Tomalin",
                        "suffix": ""
                    },
                    {
                        "first": "Phil",
                        "middle": [],
                        "last": "Woodland",
                        "suffix": ""
                    },
                    {
                        "first": "Mary",
                        "middle": [],
                        "last": "Harper",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of the 30 th ICASSP",
                "volume": "V",
                "issue": "",
                "pages": "957--960",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yang Liu, Elizabeth Shriberg, Andreas Stolcke, Bar- bara Peskin, Jeremy Ang, Dustin Hillard, Mari Os- tendorf, Marcus Tomalin, Phil Woodland and Mary Harper. 2005. Structural Metadata Research in the EARS Program. In Proceedings of the 30 th ICASSP, volume V, 957-960, Philadelphia, PA, USA.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "A Comparison of Algorithms for Maximum Entropy Parameter Estimation",
                "authors": [
                    {
                        "first": "Robert",
                        "middle": [],
                        "last": "Malouf",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Proceedings of the 6 th Conference on Natural Language Learning (CoNLL-2002)",
                "volume": "",
                "issue": "",
                "pages": "49--55",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Robert Malouf. 2002. A Comparison of Algorithms for Maximum Entropy Parameter Estimation. In Proceedings of the 6 th Conference on Natural Lan- guage Learning (CoNLL-2002), 49-55, Taibei, Taiwan.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "A Prosodically Labeled Database of Spontaneous Speech",
                "authors": [
                    {
                        "first": "Mari",
                        "middle": [],
                        "last": "Ostendorf",
                        "suffix": ""
                    },
                    {
                        "first": "Izhak",
                        "middle": [],
                        "last": "Shafran",
                        "suffix": ""
                    },
                    {
                        "first": "Stefanie",
                        "middle": [],
                        "last": "Shattuck-Hufnagel",
                        "suffix": ""
                    },
                    {
                        "first": "Leslie",
                        "middle": [],
                        "last": "Charmichael",
                        "suffix": ""
                    },
                    {
                        "first": "William",
                        "middle": [],
                        "last": "Byrne",
                        "suffix": ""
                    }
                ],
                "year": 2001,
                "venue": "Proceedings of the ISCA Workshop of Prosody in Speech Recognition and Understanding",
                "volume": "",
                "issue": "",
                "pages": "119--121",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mari Ostendorf, Izhak Shafran, Stefanie Shattuck- Hufnagel, Leslie Charmichael, and William Byrne. 2001. A Prosodically Labeled Database of Sponta- neous Speech. In Proceedings of the ISCA Work- shop of Prosody in Speech Recognition and Under- standing, 119-121, Red Bank, NJ, USA.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "A Maximum Entropy Model for Prepositional Phrase Attachment",
                "authors": [
                    {
                        "first": "Adwait",
                        "middle": [],
                        "last": "Ratnaparkhi",
                        "suffix": ""
                    },
                    {
                        "first": "Jeff",
                        "middle": [],
                        "last": "Reynar",
                        "suffix": ""
                    },
                    {
                        "first": "Salim",
                        "middle": [],
                        "last": "Roukos",
                        "suffix": ""
                    }
                ],
                "year": 1994,
                "venue": "Proceedings of the ARPA Workshop on Human Language Technology",
                "volume": "",
                "issue": "",
                "pages": "250--255",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Adwait Ratnaparkhi, Jeff Reynar and Salim Roukos. 1994. A Maximum Entropy Model for Preposi- tional Phrase Attachment. In Proceedings of the ARPA Workshop on Human Language Technology, 250-255, Plainsboro, NJ, USA.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "A Maximum Entropy Approach to Identifying Sentence Boundaries",
                "authors": [
                    {
                        "first": "Jeffrey",
                        "middle": [
                            "C"
                        ],
                        "last": "Reynar",
                        "suffix": ""
                    },
                    {
                        "first": "Adwait",
                        "middle": [],
                        "last": "Ratnaparkhi",
                        "suffix": ""
                    }
                ],
                "year": 1997,
                "venue": "Proceedings of the 5 th Conference on Applied Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "16--19",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997. A Maximum Entropy Approach to Identifying Sen- tence Boundaries. In Proceedings of the 5 th Con- ference on Applied Natural Language Processing, 16-19, Washington D.C., USA.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Incremental Feature Selection and L1 Regularization for Relaxed Maximum-entropy Modeling",
                "authors": [
                    {
                        "first": "Stefan",
                        "middle": [],
                        "last": "Riezler",
                        "suffix": ""
                    },
                    {
                        "first": "Alexander",
                        "middle": [],
                        "last": "Vasserman",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "174--181",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Stefan Riezler and Alexander Vasserman. 2004. In- cremental Feature Selection and L1 Regularization for Relaxed Maximum-entropy Modeling. In Pro- ceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, 174- 181, Barcelona, Spain.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Improved Boosting Algorithms Using Confidencerated Predictions",
                "authors": [
                    {
                        "first": "Robert",
                        "middle": [
                            "E"
                        ],
                        "last": "Schapire",
                        "suffix": ""
                    },
                    {
                        "first": "Yoram",
                        "middle": [],
                        "last": "Singer",
                        "suffix": ""
                    }
                ],
                "year": 1999,
                "venue": "Machine Learning",
                "volume": "37",
                "issue": "3",
                "pages": "297--336",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Robert E. Schapire and Yoram Singer, 1999. Im- proved Boosting Algorithms Using Confidence- rated Predictions. Machine Learning, 37(3): 297- 336.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Preliminaries to a Theory of Speech Disfluencies",
                "authors": [
                    {
                        "first": "Elizabeth",
                        "middle": [],
                        "last": "Shriberg",
                        "suffix": ""
                    }
                ],
                "year": 1994,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Elizabeth Shriberg. 1994. Preliminaries to a Theory of Speech Disfluencies. Ph.D. Thesis, University of California, Berkeley.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "The Nature of Statistical Learning Theory",
                "authors": [
                    {
                        "first": "Vladimir",
                        "middle": [],
                        "last": "Vapnik",
                        "suffix": ""
                    }
                ],
                "year": 1995,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Vladimir Vapnik. 1995. The Nature of Statistical Learning Theory. Springer, New York, NY, USA.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Using Weakly Supervised Learning to Improve Prosody Labeling",
                "authors": [
                    {
                        "first": "Darby",
                        "middle": [],
                        "last": "Wong",
                        "suffix": ""
                    },
                    {
                        "first": "Mari",
                        "middle": [],
                        "last": "Ostendorf",
                        "suffix": ""
                    },
                    {
                        "first": "Jeremy",
                        "middle": [
                            "G"
                        ],
                        "last": "Kahn",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Darby Wong, Mari Ostendorf, Jeremy G. Kahn. 2005. Using Weakly Supervised Learning to Improve Prosody Labeling. Technical Report UWEETR- 2005-0003, University of Washington.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Exploring Features for Identifying Edited Regions in Disfluent Sentences",
                "authors": [
                    {
                        "first": "Qi",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Fuliang",
                        "middle": [],
                        "last": "Weng",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proc. of the 9 th International Workshop on Parsing Technologies",
                "volume": "",
                "issue": "",
                "pages": "179--185",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Qi Zhang and Fuliang Weng. 2005. Exploring Fea- tures for Identifying Edited Regions in Disfluent Sentences. In Proc. of the 9 th International Work- shop on Parsing Technologies, 179-185, Vancou- ver, Canada.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "A Fast Algorithm for Feature Selection in Conditional Maximum Entropy Modeling",
                "authors": [
                    {
                        "first": "Yaqian",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Fuliang",
                        "middle": [],
                        "last": "Weng",
                        "suffix": ""
                    },
                    {
                        "first": "Lide",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Hauke",
                        "middle": [],
                        "last": "Schmidt",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "153--159",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yaqian Zhou, Fuliang Weng, Lide Wu, and Hauke Schmidt. 2003. A Fast Algorithm for Feature Se- lection in Conditional Maximum Entropy Model- ing. In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Process- ing, 153-159, Sapporo, Japan.",
                "links": null
            }
        },
        "ref_entries": {
            "TABREF0": {
                "content": "<table><tr><td/><td>Categories</td><td/><td>Short Description</td><td>Number of Combinations</td></tr><tr><td>Tags</td><td colspan=\"2\">HTagComb</td><td>Combinations among Hierarchical POS Tags</td><td>55</td></tr><tr><td>Words</td><td/><td colspan=\"2\">OrthWordComb Combinations among Orthographic Words</td><td>55</td></tr><tr><td>Tags</td><td>WTComb</td><td>WTTComb</td><td>Combinations of Orthographic Words and POS Tags; Combination among POS Tags</td><td>176</td></tr><tr><td>Rough Copy</td><td colspan=\"2\">RCComb</td><td>Combinations of HTag Rough Copy and Word Rough Copy</td><td>55</td></tr><tr><td>Prosody</td><td colspan=\"2\">PComb</td><td>Combinations among Prosody, and with Words</td><td>36</td></tr></table>",
                "type_str": "table",
                "text": "A complete list of variables used in the experiments.",
                "html": null,
                "num": null
            },
            "TABREF1": {
                "content": "<table><tr><td>Feature Space Codes</td><td>number of features</td><td>Precision</td><td>Recall</td><td>F-Value</td></tr><tr><td>TAG-based result on UW-SWBD reported in Kahn et al. (2005)</td><td/><td/><td/><td>78.20</td></tr><tr><td>CME with all the variables from Zhang and Weng (2005)</td><td>2412382</td><td>89.42</td><td>71.22</td><td>79.29</td></tr><tr><td>CME with all the variables from Zhang and Weng (2005) + post</td><td>2412382</td><td>87.15</td><td>73.78</td><td>79.91</td></tr><tr><td>+HTag +HTagComb +WTComb +RCComb</td><td>17116957</td><td>90.44</td><td>72.53</td><td>80.50</td></tr><tr><td>+HTag +HTagComb +WTComb +RCComb +PL 0 \u2026 PL 3</td><td>17116981</td><td>88.69</td><td>74.01</td><td>80.69</td></tr><tr><td>+HTag +HTagComb +WTComb +RCComb +PComb: without cut</td><td>20445375</td><td>89.43</td><td>73.78</td><td>80.86</td></tr><tr><td>+HTag +HTagComb +WTComb +RCComb +PComb: cut2</td><td>19294583</td><td>88.95</td><td>74.66</td><td>81.18</td></tr><tr><td>+HTag +HTagComb +WTComb +RCComb +PComb: cut2 +Gau</td><td>19294583</td><td>90.37</td><td>74.40</td><td>81.61</td></tr><tr><td>+HTag +HTagComb +WTComb +RCComb +PComb: cut2 +post</td><td>19294583</td><td>86.88</td><td>77.29</td><td>81.80</td></tr><tr><td>+HTag +HTagComb +WTComb +RCComb +PComb: cut2 +Gau +post</td><td>19294583</td><td>87.79</td><td>77.02</td><td>82.05</td></tr></table>",
                "type_str": "table",
                "text": "Summary of experimental results with PFS.",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table><tr><td colspan=\"4\">Results on test data Split / Non-split Precision Recall F-Value</td></tr><tr><td>non-split</td><td>89.42</td><td>71.22</td><td>79.29</td></tr><tr><td>split by 4 parts</td><td>89.67</td><td>71.68</td><td>79.67</td></tr><tr><td>split by 10 parts</td><td>89.65</td><td>71.29</td><td>79.42</td></tr></table>",
                "type_str": "table",
                "text": "Comparison between PFS and SGC with all the variables from",
                "html": null,
                "num": null
            },
            "TABREF5": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Parsing F-score various different edit region identification results.",
                "html": null,
                "num": null
            }
        }
    }
}