{
    "paper_id": "H05-1039",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:17:49.167296Z"
    },
    "title": "Combining Deep Linguistics Analysis and Surface Pattern Learning: A Hybrid Approach to Chinese Definitional Question Answering",
    "authors": [
        {
            "first": "Fuchun",
            "middle": [],
            "last": "Peng",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "BBN Technologies",
                "location": {
                    "addrLine": "50 Moulton Street",
                    "postCode": "02138",
                    "settlement": "Cambridge",
                    "region": "MA"
                }
            },
            "email": "fpeng@bbn.com"
        },
        {
            "first": "Ralph",
            "middle": [],
            "last": "Weischedel",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "BBN Technologies",
                "location": {
                    "addrLine": "50 Moulton Street",
                    "postCode": "02138",
                    "settlement": "Cambridge",
                    "region": "MA"
                }
            },
            "email": "rweisched@bbn.com"
        },
        {
            "first": "Ana",
            "middle": [],
            "last": "Licuanan",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "BBN Technologies",
                "location": {
                    "addrLine": "50 Moulton Street",
                    "postCode": "02138",
                    "settlement": "Cambridge",
                    "region": "MA"
                }
            },
            "email": ""
        },
        {
            "first": "Jinxi",
            "middle": [],
            "last": "Xu",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "BBN Technologies",
                "location": {
                    "addrLine": "50 Moulton Street",
                    "postCode": "02138",
                    "settlement": "Cambridge",
                    "region": "MA"
                }
            },
            "email": ""
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "We explore a hybrid approach for Chinese definitional question answering by combining deep linguistic analysis with surface pattern learning. We answer four questions in this study: 1) How helpful are linguistic analysis and pattern learning? 2) What kind of questions can be answered by pattern matching? 3) How much annotation is required for a pattern-based system to achieve good performance? 4) What linguistic features are most useful? Extensive experiments are conducted on biographical questions and other definitional questions. Major findings include: 1) linguistic analysis and pattern learning are complementary; both are required to make a good definitional QA system; 2) pattern matching is very effective in answering biographical questions while less effective for other definitional questions; 3) only a small amount of annotation is required for a pattern learning system to achieve good performance on biographical questions; 4) the most useful linguistic features are copulas and appositives; relations also play an important role; only some propositions convey vital facts.",
    "pdf_parse": {
        "paper_id": "H05-1039",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "We explore a hybrid approach for Chinese definitional question answering by combining deep linguistic analysis with surface pattern learning. We answer four questions in this study: 1) How helpful are linguistic analysis and pattern learning? 2) What kind of questions can be answered by pattern matching? 3) How much annotation is required for a pattern-based system to achieve good performance? 4) What linguistic features are most useful? Extensive experiments are conducted on biographical questions and other definitional questions. Major findings include: 1) linguistic analysis and pattern learning are complementary; both are required to make a good definitional QA system; 2) pattern matching is very effective in answering biographical questions while less effective for other definitional questions; 3) only a small amount of annotation is required for a pattern learning system to achieve good performance on biographical questions; 4) the most useful linguistic features are copulas and appositives; relations also play an important role; only some propositions convey vital facts.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Due to the ever increasing large amounts of online textual data, learning from textual data is becoming more and more important. Traditional document retrieval systems return a set of relevant documents and leave the users to locate the specific information they are interested in. Question answering, which combines traditional document retrieval and information extraction, solves this problem directly by returning users the specific answers. Research in textual question answering has made substantial advances in the past few years (Voorhees, 2004) .",
                "cite_spans": [
                    {
                        "start": 537,
                        "end": 553,
                        "text": "(Voorhees, 2004)",
                        "ref_id": "BIBREF13"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Most question answering research has been focusing on factoid questions where the goal is to return a list of facts about a concept. Definitional questions, however, remain largely unexplored. Definitional questions differ from factoid questions in that the goal is to return the relevant \"answer nuggets\" of information about a query. Identifying such answer nuggets requires more advanced language processing techniques. Definitional QA systems are not only interesting as a research challenge. They also have the potential to be a valuable complement to static knowledge sources like encyclopedias. This is because they create definitions dynamically, and thus answer definitional questions about terms which are new or emerging (Blair-Goldensoha et al., 2004) .",
                "cite_spans": [
                    {
                        "start": 732,
                        "end": 763,
                        "text": "(Blair-Goldensoha et al., 2004)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "One success in factoid question answering is pattern based systems, either manually constructed (Soubbotin and Soubbotin, 2002) or machine learned (Cui et al., 2004) . However, it is unknown whether such pure pattern based systems work well on definitional questions where answers are more diverse.",
                "cite_spans": [
                    {
                        "start": 96,
                        "end": 127,
                        "text": "(Soubbotin and Soubbotin, 2002)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 147,
                        "end": 165,
                        "text": "(Cui et al., 2004)",
                        "ref_id": "BIBREF1"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Deep linguistic analysis has been found useful in factoid question answering (Moldovan et al., 2002) and has been used for definitional questions (Xu et al., 2004; Harabagiu et al., 2003) . Linguistic analy-sis is useful because full parsing captures long distance dependencies between the answers and the query terms, and provides more information for inference. However, merely linguistic analysis may not be enough. First, current state of the art linguistic analysis such as parsing, co-reference, and relation extraction is still far below human performance. Errors made in this stage will propagate and lower system accuracy. Second, answers to some types of definitional questions may have strong local dependencies that can be better captured by surface patterns. Thus we believe that combining linguistic analysis and pattern learning would be complementary and be beneficial to the whole system.",
                "cite_spans": [
                    {
                        "start": 77,
                        "end": 100,
                        "text": "(Moldovan et al., 2002)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 146,
                        "end": 163,
                        "text": "(Xu et al., 2004;",
                        "ref_id": null
                    },
                    {
                        "start": 164,
                        "end": 187,
                        "text": "Harabagiu et al., 2003)",
                        "ref_id": "BIBREF2"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Work in combining linguistic analysis with patterns include Weischedel et al. (2004) and Jijkoun et al. (2004) where manually constructed patterns are used to augment linguistic features. However, manual pattern construction critically depends on the domain knowledge of the pattern designer and often has low coverage (Jijkoun et al., 2004) . Automatic pattern derivation is more appealing (Ravichandran and Hovy, 2002) .",
                "cite_spans": [
                    {
                        "start": 60,
                        "end": 84,
                        "text": "Weischedel et al. (2004)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 89,
                        "end": 110,
                        "text": "Jijkoun et al. (2004)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 319,
                        "end": 341,
                        "text": "(Jijkoun et al., 2004)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 391,
                        "end": 420,
                        "text": "(Ravichandran and Hovy, 2002)",
                        "ref_id": "BIBREF10"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this work, we explore a hybrid approach to combining deep linguistic analysis with automatic pattern learning. We are interested in answering the following four questions for Chinese definitional question answering: How helpful are linguistic analysis and pattern learning in definitional question answering? If pattern learning is useful, what kind of question can pattern matching answer? How much human annotation is required for a pattern based system to achieve reasonable performance?",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "To our knowledge, this is the first formal study of these questions in Chinese definitional QA. To answer these questions, we perform extensive experiments on Chinese TDT4 data (Linguistic Data Consortium, 2002 Consortium, -2003)) . We separate definitional questions into biographical (Who-is) questions and other definitional (What-is) questions. We annotate some question-answer snippets for pattern learning and we perform deep linguistic analysis including parsing, tagging, name entity recognition, co-reference, and relation detection.",
                "cite_spans": [
                    {
                        "start": 194,
                        "end": 210,
                        "text": "Consortium, 2002",
                        "ref_id": null
                    },
                    {
                        "start": 211,
                        "end": 230,
                        "text": "Consortium, -2003))",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "If linguistic analysis is helpful, what linguistic features are most useful?",
                "sec_num": null
            },
            {
                "text": "The architecture of our QA system is shown in Figure 1 . Given a question, we first use simple rules to classify it as a \"Who-is\" or \"What-is\" question and detect key words. Then we use a HMM-based IR system (Miller et al., 1999) for document retrieval by treating the question keywords as a query. To speed up processing, we only use the top 1000 relevant documents. We then select relevant sentences among the returned relevant documents. A sentence is considered relevant if it contains the query keyword or contains a word that is co-referent to the query term. Coreference is determined using an information extraction engine, SERIF (Ramshaw et al., 2001) . We then conduct deep linguistic analysis and pattern matching to extract candidate answers. We rank all candidate answers by predetermined feature ordering. At the same time, we perform redundancy detection based on \u00a1 -gram overlap.",
                "cite_spans": [
                    {
                        "start": 208,
                        "end": 229,
                        "text": "(Miller et al., 1999)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 638,
                        "end": 660,
                        "text": "(Ramshaw et al., 2001)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 53,
                        "end": 54,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "A Hybrid Approach to Definitional Question Answering",
                "sec_num": "2"
            },
            {
                "text": "We use SERIF (Ramshaw et al., 2001) , a linguistic analysis engine, to perform full parsing, name entity detection, relation detection, and co-reference resolution. We extract the following linguistic features: 1. Copula: a copula is a linking verb such as \"is\" or \"become\". An example of a copula feature is \"Bill Gates is the CEO of Microsoft\". In this case, \"CEO of Microsoft\" will be extracted as an answer to \"Who is Bill Gates?\". To extract copulas, SERIF traverses the parse trees of the sentences and extracts copulas based on rules.",
                "cite_spans": [
                    {
                        "start": 13,
                        "end": 35,
                        "text": "(Ramshaw et al., 2001)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Deep Linguistic Analysis",
                "sec_num": "2.1"
            },
            {
                "text": "In Chinese, the rule for identifying a copula is the POS tag \"VC\", standing for \"Verb Copula\".",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Deep Linguistic Analysis",
                "sec_num": "2.1"
            },
            {
                "text": "The only copula verb in Chinese is \" \u00a2 \".",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Deep Linguistic Analysis",
                "sec_num": "2.1"
            },
            {
                "text": "2. Apposition: appositions are a pair of noun phrases in which one modifies the other. For example, In \"Tony Blair, the British Prime Minister, ...\", the phrase \"the British Prime Minister\" is in apposition to \"Blair\". Extraction of appositive features is similar to that of copula. SERIF traverses the parse tree and identifies appositives based on rules. A detailed description of the algorithm is documented ). The most common roles include logical subject, logical object, and object of a prepositional phrase that modifies the predicate. For example, \"Smith went to Spain\" is represented as a proposition, went(logical subject: Smith, PP-to: Spain).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Deep Linguistic Analysis",
                "sec_num": "2.1"
            },
            {
                "text": "The SERIF linguistic analysis engine also extracts relations between two objects. SERIF can extract 24 binary relations defined in the ACE guidelines (Linguistic Data Consortium, 2002) , such as spouse-of, staff-of, parent-of, management-of and so forth. Based on question types, we use different relations, as listed in Table 1 . Many relevant sentences do not contain the query key words. Instead, they contain words that are coreferent to the query. For example, in \"Yesterday UN Secretary General Anan Requested Every Side..., He said ... \". The pronoun \"He\" in the second sentence refers to \"Anan\" in the first sentence. To select such sentences, we conduct co-reference resolution using SERIF.",
                "cite_spans": [
                    {
                        "start": 150,
                        "end": 184,
                        "text": "(Linguistic Data Consortium, 2002)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 327,
                        "end": 328,
                        "text": "1",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Relations:",
                "sec_num": "4."
            },
            {
                "text": "In addition, SERIF also provides name tagging, identifying 29 types of entity names or descriptions, such as locations, persons, organizations, and diseases.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Relations:",
                "sec_num": "4."
            },
            {
                "text": "We also select complete sentences mentioning the term being defined as backup answers if no other features are identified.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Relations:",
                "sec_num": "4."
            },
            {
                "text": "The component performance of our linguistic analysis is shown in Table 2 ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 71,
                        "end": 72,
                        "text": "2",
                        "ref_id": "TABREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Relations:",
                "sec_num": "4."
            },
            {
                "text": "We use two kinds of patterns: manually constructed patterns and automatically derived patterns. A manual pattern is a commonly used linguistic expression that specifies aliases, super/subclass and membership relations of a term (Xu et al., 2004) . For example, the expression \"tsunamis, also known as tidal waves\" gives an alternative term for tsunamis. We use 23 manual patterns for Who-is questions and 14 manual patterns for What-is questions.",
                "cite_spans": [
                    {
                        "start": 228,
                        "end": 245,
                        "text": "(Xu et al., 2004)",
                        "ref_id": "BIBREF15"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Surface Pattern Learning",
                "sec_num": "2.2"
            },
            {
                "text": "We also classify some special propositions as manual patterns since they are specified by computational linguists. After a proposition is extracted, it is matched against a list of predefined predicates. If it is on the list, it is considered special and will be ranked higher. In total, we designed 22 special propositions for Who-is questions, such as",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Surface Pattern Learning",
                "sec_num": "2.2"
            },
            {
                "text": "(become), \u00a2 \u00a4\u00a3 \u00a1 (elected as), and \u00a5 \u00a7\u00a6 (resign), 14 for What-is questions, such as \u00a9 (located at), \u00a9 (created at), and \u00a1 (also known as). However, it is hard to manually construct such patterns since it largely depends on the knowledge of the pattern designer. Thus, we prefer patterns that can be automatically derived from training data. Some annotators labeled question-answer snippets. Given a query question, the annotators were asked to highlight the strings that can answer the question. Though such a process still requires annotators to have knowledge of what can be answers, it does not require a computational linguist. Our pattern learning procedure is illustrated in Figure 2 . To obtain patterns, we conduct full parsing to obtain the full parse tree for a sentence. In our current Chinese annotation: ! #\" \"$ !% #& !' !( \" ) (0 21 #1 43 5 ANSWER)(6 87 @9 A @B QTERM), C ED GF H I 8P Q !R TS \" @$ U V EW EX Y English translation: (U.S. Secretary of the State ANWER) (Albright QTERM), who visited North Korea for the \"icebreaking trip\", had a historical meeting with the leader of North Korea, Kim Jong Il. Here we obtain the snippet (q sr r ut wv NR/ANSWER)(TERM).",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 688,
                        "end": 689,
                        "text": "2",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "\u00a1",
                "sec_num": null
            },
            {
                "text": "$ VV)(\" PU)(% VV)(& NN)(' `( NN)(\" PU)( ) DEC)(0 a1 NR)( 1 \u00a73 5 NR)(6 `7 NR)(9 `A `B NR)(, PU) (C bD NT)( F DT)( ! NR)(I !P NR)( Q cR \u00a4S NN)(d #$ VV)(U !V W JJ)(X Y NN).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "\u00a1",
                "sec_num": null
            },
            {
                "text": "( NN/BKGD)(\" PU/BKGD)( ) DEC/BKGD)(0 1 NR/ANSWER)( 1 3 5 NR/ANSWER)(6 7 NR/QTERM)(9 A fB NR/QTERM)(, PU/BKGD) (C D NT/BKGD)( F DT/BKGD)( g NR/BKGD)(I hP NR/BKGD)(i R S NN/BKGD)(\" !$ VV/BKGD)(U pV W JJ/BKGD)(X Y NN/BKGD)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "\u00a1",
                "sec_num": null
            },
            {
                "text": "We generalize a pattern using three heuristics (this particular example does not generalize). First, we replace all Chinese sequences longer than 3 characters with their POS tags, under the theory that long sequences are too specific. Second, we also replace NT (time noun, such as x \u00a4y ), DT (determiner, such as , ), cardinals (CD, such as , , ) and M (measurement word such as \u00a2\u00a1 \u00a4\u00a3 ) with their POS tags. Third, we ignore adjectives.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "\u00a1",
                "sec_num": null
            },
            {
                "text": "After obtaining all patterns, we run them on the training data to calculate their precision and recall. We select patterns whose precision is above 0.6 and which fire at least 5 times in training data (parameters are determined with a held out dataset).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "\u00a1",
                "sec_num": null
            },
            {
                "text": "We produced a list of questions and asked annotators to identify answer snippets from TDT4 data. To produce as many training answer snippets as possible, annotators were asked to label answers exhaustively; that is, the same answer can be labeled multiple times in different places. However, we remove duplicate answers for test questions since we are only interested in unique answers in evaluation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data Sets",
                "sec_num": "3.1"
            },
            {
                "text": "We separate questions into two types, biographical (Who-is) questions, and other definitional questions (What-is). For \"Who-is\" questions, we used 204 questions for pattern learning, 10 for parameter tuning and another 42 questions for testing. For \"What-is\" questions, we used 44 for training and another 44 for testing.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data Sets",
                "sec_num": "3.1"
            },
            {
                "text": "The TREC question answering evaluation is based on human judgments (Voorhees, 2004) . However, such a manual procedure is costly and time consuming. Recently, researchers have started automatic question answering evaluation (Xu et al., 2004; Lin and Demner-Fushman, 2005; Soricut and Brill, 2004) . We use Rouge, an automatic evaluation metric that was originally used for summarization evaluation (Lin and Hovy, 2003) and was recently found useful for evaluating definitional question answering (Xu et al., 2004) . Rouge is based on \u00a1 -gram co-occurrence. An \u00a1 -gram is a sequence of \u00a1 consecutive Chinese characters.",
                "cite_spans": [
                    {
                        "start": 67,
                        "end": 83,
                        "text": "(Voorhees, 2004)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 224,
                        "end": 241,
                        "text": "(Xu et al., 2004;",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 242,
                        "end": 271,
                        "text": "Lin and Demner-Fushman, 2005;",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 272,
                        "end": 296,
                        "text": "Soricut and Brill, 2004)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 398,
                        "end": 418,
                        "text": "(Lin and Hovy, 2003)",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 496,
                        "end": 513,
                        "text": "(Xu et al., 2004)",
                        "ref_id": "BIBREF15"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation",
                "sec_num": "3.2"
            },
            {
                "text": "Given a reference answer \u00a5 and a system answer \u00a6 , the Rouge score is defined as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation",
                "sec_num": "3.2"
            },
            {
                "text": "\u00a7 \u00a9 \u00a7 \"! # %$ '& )( 10 2 3 3 4 5 6 7 98 A@ AB C D FE \"G IH QP SR \"T \u00a7 U! A %D V& B C D FE W \u00a7 XD V&",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation",
                "sec_num": "3.2"
            },
            {
                "text": "where Y is the maximum length of \u00a1 -grams, `\u00a1 a \u00a1 cb Wd e gf h Ui qp \"\u00a5 sr \u00a6 r \u00a1 ut is the number of common \u00a1 - grams of \u00a5 and \u00a6 , and `\u00a1 a \u00a1 cb p \"\u00a5 sr \u00a1 ut is the number of \u00a1 -grams in \u00a5 . If Y is too small, stop words and bi-grams of such words will dominate the score; If Y is too large, there will be many questions without answers. We select Y to be 3, 4, 5 and 6.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation",
                "sec_num": "3.2"
            },
            {
                "text": "To make scores of different systems comparable, we truncate system output for the same question by the same cutoff length. We score answers truncated at length v times that of the reference answers, where v is set to be 1, 2, and 3. The rationale is that people would like to read at least the same length of the reference answer. On the other hand, since the state of the art system answer is still far from human performance, it is reasonable to produce answers somewhat longer than the references (Xu et al., 2004) .",
                "cite_spans": [
                    {
                        "start": 500,
                        "end": 517,
                        "text": "(Xu et al., 2004)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation",
                "sec_num": "3.2"
            },
            {
                "text": "In summary, we run experiments with parameters Y xw y r Q #r r and v w \u00a4 r r y , and take the average over all of the 12 runs.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation",
                "sec_num": "3.2"
            },
            {
                "text": "We set the pure linguistic analysis based system as the baseline and compare it to other configurations. Table 3 and Table 4 show the results on \"Who-is\" and \"What-is\" questions respectively. The baseline (Run 1) is the result of using pure linguistic features; Run 2 is the result of adding manual patterns to the baseline system; Run 3 is the result of using learned patterns only. Run 4 is the result of adding learned patterns to the baseline system. Run 5 is the result of adding both manual patterns and learned patterns to the system. The first question we want to answer is how helpful the linguistic analysis and pattern learning are for definitional QA. Comparing Run 1 and 3, we can see that both pure linguistic analysis and pure pattern based systems achieve comparable performance; Combining them together improves performance (Run 4) for \"who is\" questions, but only slightly for \"what is\" questions. This indicates that linguistic analysis and pattern learning are complementary to each other, and both are helpful for biographical QA.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 111,
                        "end": 112,
                        "text": "3",
                        "ref_id": null
                    },
                    {
                        "start": 123,
                        "end": 124,
                        "text": "4",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Overall Results",
                "sec_num": "3.3"
            },
            {
                "text": "The second question we want to answer is what kind of questions can be answered with pattern matching. From these two tables, we can see that patterns are very effective in \"Who-is\" questions while less effective in \"What-is\" questions. Learned patterns improve the baseline from 0.3399 to 0.3860; manual patterns improve the baseline to 0.3657; combining both manual and learned patterns improve it to 0.4026, an improvement of 18.4% compared to the baseline. However, the effect of patterns on \"What-is\" is smaller, with an improvement of only 3.5%. However, the baseline performance on \"What-is\" is also much worse than that of \"Who-is\" questions. We will analyze the reasons in Section 4.3. This indicates that answering general definitional questions is much more challenging than answering biographical questions and deserves more research. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overall Results",
                "sec_num": "3.3"
            },
            {
                "text": "The third question is how much annotation is needed for a pattern based system to achieve good performance. We run experiments with portions of training data on biographical questions, which produce different number of patterns. Table 5 shows the details of the number of training snippets used and the number of patterns produced and selected. The performance of different system is illustrated in Figure 6 . With only 10% of the training data (549 snippets, about two person days of annotation), learned patterns achieve good performance of 0.3285, considering the performance of 0.3399 of a well tuned system with deep linguistic features. Performance saturates with 2742 training snippets (50% training, 10 person days annotation) at a Rouge score of 0.3590, comparable to the performance of a well tuned system with full linguistic features and manual patterns (Run 2 in Table 3 ). There could even be a slight, insignificant performance decrease with more training data because our sampling is sequential instead of random. Some portions of training data might be more useful than others. ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 235,
                        "end": 236,
                        "text": "5",
                        "ref_id": "TABREF3"
                    },
                    {
                        "start": 406,
                        "end": 407,
                        "text": "6",
                        "ref_id": null
                    },
                    {
                        "start": 882,
                        "end": 883,
                        "text": "3",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "How much annotation is needed",
                "sec_num": "4.1"
            },
            {
                "text": "The fourth question we want to answer is: what features are most useful in definitional question answering? To evaluate the contribution of each individual feature, we turn off all other features and test the system on a held out data (10 questions). We calculate the coverage of each feature, measured by Rouge. We also calculate the precision of each feature with the following formula, which is very similar to Rouge except that the denominator here is based on system output `\u00a1 a \u00a1 cb p \u00a6 r \u00a1 ut instead of ref- erence `\u00a1 a \u00a1 cb p \"\u00a5 sr \u00a1 ut . The notations are the same as those in Rouge.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Contributions of different features",
                "sec_num": "4.2"
            },
            {
                "text": "\u00a3 \u00a5\u00a4 \u00a6\u00a3 CD \u00a7 U! A X$ '& ( 0 2 3 3 4 5 6 7 98 A@ B C D E G IH QP SR \"T \u00a7 X! A %D V& B D E ! A XD V&",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "\u00a2\u00a1 C B",
                "sec_num": null
            },
            {
                "text": "Figure 7 is the precision-recall scatter plot of the features measured on \"who is\" questions. Interestingly, the learned patterns have the highest coverage and precision. The copula feature has the second highest precision; however, it has the lowest coverage. This is because there are not many copulas in the dataset. Appositive and manual pattern features have the same level of contribution. Surprisingly, the relation feature has a high coverage. This suggests that relations could be more useful if relation detection were more accurate; general propositions are not more useful than whole sentences since almost every sentence has a proposition, and since the high value propositions are identified by the lexical head of the proposition and grouped with the manual patterns.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "7",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "\u00a2\u00a1 C B",
                "sec_num": null
            },
            {
                "text": "Figure 7 : Feature precision recall scatter plot (measured on the biographical questions)",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "7",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "\u00a2\u00a1 C B",
                "sec_num": null
            },
            {
                "text": "We have seen that \"What-is\" questions are more challenging than \"Who-is\" questions. We compare the precision and coverage of each feature for \"Whois\" and \"What-is\" in Table 6 and Table 7 . We see that although the precisions of the features are higher for \"What-is\", their coverage is too low. The most useful features for \"What-is\" questions are propositions and raw sentences, which are the worst two features for \"Who-is\". Basically, this means that most of the answers for \"What-is\" are from whole sentences. Neither linguistic analysis nor pattern matching works as efficiently as in biographical questions. To identify the challenges of \"What-is\" questions, we conducted an error analysis. The answers for \"What-is\" are much more diverse and are hard to capture. For example, the reference answers for the question of \" \u00a7 \u00a9\u00a8\u00a2 ! / What is the international space station?\" include the weight of the space station, the distance from the space station to the earth, the inner structure of the space station, and the cost of its construction. Such attributes are hard to capture with patterns, and they do not contain any of the useful linguistic features we currently have (copula, appositive, proposition, relation). Identifying more useful features for such answers remains for future work.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 173,
                        "end": 174,
                        "text": "6",
                        "ref_id": "TABREF4"
                    },
                    {
                        "start": 179,
                        "end": 186,
                        "text": "Table 7",
                        "ref_id": "TABREF5"
                    }
                ],
                "eq_spans": [],
                "section": "Who-is versus What-is questions",
                "sec_num": "4.3"
            },
            {
                "text": "5 Related Work Ravichandran and Hovy (2002) presents a method that learns patterns from online data using some seed questions and answer anchors. The advantage is that it does not require human annotation. However, it only works for certain types of questions that have fixed anchors, such as \"where was X born\". For general definitional questions, we do not know what the anchors should be. Thus we prefer using small amounts of human annotation to derive patterns. Cui et al. (2004) uses a similar approach for unsupervised pattern learning and generalization to soft pattern matching. However, the method is actually used for sentence selection rather than answer snippet selection. Combining information extraction with surface patterns has also seen some success. Jikoun et al. (2004) shows that information extraction can help improve the recall of a pattern based system. Xu et al. (2004) also shows that manually constructed patterns are very important in answering English definitional questions. Hildebrandt et al. (2004) uses manual surface patterns for target extraction to augment database and dictionary lookup. Blair-Goldensohn et al. (2004) apply supervised learning for definitional predicates and then apply summarization methods for question answering.",
                "cite_spans": [
                    {
                        "start": 15,
                        "end": 43,
                        "text": "Ravichandran and Hovy (2002)",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 467,
                        "end": 484,
                        "text": "Cui et al. (2004)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 769,
                        "end": 789,
                        "text": "Jikoun et al. (2004)",
                        "ref_id": null
                    },
                    {
                        "start": 879,
                        "end": 895,
                        "text": "Xu et al. (2004)",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 1006,
                        "end": 1031,
                        "text": "Hildebrandt et al. (2004)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 1126,
                        "end": 1156,
                        "text": "Blair-Goldensohn et al. (2004)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Who-is versus What-is questions",
                "sec_num": "4.3"
            },
            {
                "text": "We have explored a hybrid approach for definitional question answering by combining deep linguistic analysis and surface pattern learning. For the first time, we have answered four questions regarding Chinese definitional QA: deep linguistic analysis and automatic pattern learning are complementary and may be combined; patterns are powerful in answering biographical questions; only a small amount of annotation (2 days) is required to obtain good performance in a biographical QA system; copulas and appositions are the most useful linguistic features; relation extraction also helps.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusions and Future Work",
                "sec_num": "6"
            },
            {
                "text": "Answering \"What-is\" questions is more challenging than answering \"Who-is\" questions. To improve the performance on \"What-is\" questions, we could divide \"What-is\" questions into finer classes such as organization, location, disease, and general substance, and process them specifically.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusions and Future Work",
                "sec_num": "6"
            },
            {
                "text": "Our current pattern matching is based on simple POS tagging which captures only limited syntactic information. We generalize words to their corresponding POS tags. Another possible improvement is to generalize using automatically derived word clusters, which provide semantic information.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusions and Future Work",
                "sec_num": "6"
            }
        ],
        "back_matter": [
            {
                "text": "This material is based upon work sup-ported by the Advanced Research and Development Activity (ARDA) under Contract No. NBCHC040039. We are grateful to Linnea Micciulla for proof reading and three anonymous reviewers for suggestions on improving the paper.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgements",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Answering Definitional Questions: A Hybrid Approach. New Directions In Question Answering",
                "authors": [
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Blair-Goldensoha",
                        "suffix": ""
                    },
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Mckeown",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Hazen",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Schlaikjer",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "47--58",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "S. Blair-Goldensoha, K. McKeown, and A. Hazen Schlaikjer. 2004. Answering Definitional Questions: A Hybrid Approach. New Directions In Question An- swering., pages 47-58.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Unsupervised Learning of Soft Patterns for Definitional Question Answering",
                "authors": [
                    {
                        "first": "H",
                        "middle": [],
                        "last": "Cui",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Kan",
                        "suffix": ""
                    },
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Chua",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "WWW 2004",
                "volume": "",
                "issue": "",
                "pages": "90--99",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "H. Cui, M. Kan, and T. Chua. 2004. Unsupervised Learning of Soft Patterns for Definitional Question Answering. In WWW 2004, pages 90-99.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Answer Mining by Combining Extraction Techniques with Abductive Reasoning",
                "authors": [
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Harabagiu",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Moldovan",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Clark",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Bowden",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Williams",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Bensley",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "TREC2003 Proceedings",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "S. Harabagiu, D. Moldovan, C. Clark, M. Bowden, J. Williams, and J. Bensley. 2003. Answer Mining by Combining Extraction Techniques with Abductive Reasoning. In TREC2003 Proceedings.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Answering Definition Questions with Multiple Knowledge Sources",
                "authors": [
                    {
                        "first": "W",
                        "middle": [],
                        "last": "Hildebrandt",
                        "suffix": ""
                    },
                    {
                        "first": "B",
                        "middle": [],
                        "last": "Katz",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "HLT-NAACL 2004",
                "volume": "",
                "issue": "",
                "pages": "49--56",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "W. Hildebrandt, B. Katz, and J. Lin. 2004. Answer- ing Definition Questions with Multiple Knowledge Sources. In HLT-NAACL 2004, pages 49-56.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Information Extraction for Question Answering: Improving Recall Through Syntactic Patterns",
                "authors": [
                    {
                        "first": "V",
                        "middle": [],
                        "last": "Jijkoun",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Rijke",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Mur",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "COLING",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "V. Jijkoun, M. Rijke, and J. Mur. 2004. Information Extraction for Question Answering: Improving Recall Through Syntactic Patterns. In COLING 2004.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Automatically Evaluating Answers to Definition Questions",
                "authors": [
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Demner-Fushman",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "ACL2005",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "J. Lin and D. Demner-Fushman. 2005. Automati- cally Evaluating Answers to Definition Questions. In ACL2005. to appear.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Automatic Evaluation of Summaries Using N-gram Co-occurrence Statistics",
                "authors": [
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "E",
                        "middle": [],
                        "last": "Hovy",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "HLT-NAACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "C. Lin and E. Hovy. 2003. Automatic Evaluation of Summaries Using N-gram Co-occurrence Statistics. In HLT-NAACL 2003.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "A Hidden Markov Model Information Retrieval System",
                "authors": [
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Miller",
                        "suffix": ""
                    },
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Leek",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Schwartz",
                        "suffix": ""
                    }
                ],
                "year": 1999,
                "venue": "SI-GIR 1999",
                "volume": "",
                "issue": "",
                "pages": "214--221",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "D. Miller, T. Leek, and R. Schwartz. 1999. A Hidden Markov Model Information Retrieval System. In SI- GIR 1999, pages 214 -221.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Performance Issues and Error Analysis in an Open-Domain Question Answering System",
                "authors": [
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Moldovan",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Pasca",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Harabagiu",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Surdeanu",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "ACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "D. Moldovan, M. Pasca, S. Harabagiu, and M. Sur- deanu. 2002. Performance Issues and Error Analysis in an Open-Domain Question Answering System. In ACL2002.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Experiments in Multi-Model Automatic Content Extraction",
                "authors": [
                    {
                        "first": "L",
                        "middle": [],
                        "last": "Ramshaw",
                        "suffix": ""
                    },
                    {
                        "first": "E",
                        "middle": [],
                        "last": "Boshee",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Bautus",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Miller",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Stone",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Weischedel",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Zamanian",
                        "suffix": ""
                    }
                ],
                "year": 2001,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "L. Ramshaw, E. Boshee, S. Bautus, S. Miller, R. Stone, R. Weischedel, and A. Zamanian. 2001. Experi- ments in Multi-Model Automatic Content Extraction. In HLT2001.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Learning surface text patterns for a Question Answering System",
                "authors": [
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Ravichandran",
                        "suffix": ""
                    },
                    {
                        "first": "E",
                        "middle": [],
                        "last": "Hovy",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "ACL2002",
                "volume": "",
                "issue": "",
                "pages": "41--47",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "D. Ravichandran and E. Hovy. 2002. Learning surface text patterns for a Question Answering System. In ACL2002, pages 41-47.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "A Unified Framework For Automatic Evaluation Using N-Gram Co-occurrence Statistics",
                "authors": [
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Soricut",
                        "suffix": ""
                    },
                    {
                        "first": "E",
                        "middle": [],
                        "last": "Brill",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "ACL 2004",
                "volume": "",
                "issue": "",
                "pages": "613--620",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "R. Soricut and E. Brill. 2004. A Unified Framework For Automatic Evaluation Using N-Gram Co-occurrence Statistics. In ACL 2004, pages 613-620.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Use of Patterns for Detection of Likely Answer Strings: A Systematic Approach",
                "authors": [
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Soubbotin",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Soubbotin",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "TREC2002 Proceedings",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "M. Soubbotin and S. Soubbotin. 2002. Use of Patterns for Detection of Likely Answer Strings: A Systematic Approach. In TREC2002 Proceedings.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Overview of the TREC 2003 Question Answering Track",
                "authors": [
                    {
                        "first": "E",
                        "middle": [],
                        "last": "Voorhees",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "TREC Proceedings",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "E. Voorhees. 2004. Overview of the TREC 2003 Ques- tion Answering Track. In TREC Proceedings.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "A Hybrid Approach to Answering Biographical Questions. New Directions In Question Answering",
                "authors": [
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Weischedel",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Licuanan",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "59--70",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "R. Weischedel, J. Xu, and A. Licuanan. 2004. A Hybrid Approach to Answering Biographical Questions. New Directions In Question Answering., pages 59-70.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Evaluation of an Extraction-based Approach to Answering Definitional Questions",
                "authors": [
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Weischedel",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Licuanan",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "SIGIR 2004",
                "volume": "",
                "issue": "",
                "pages": "418--424",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "J. Xu, R. Weischedel, and A. Licuanan. 2004. Evaluation of an Extraction-based Approach to Answering Defini- tional Questions. In SIGIR 2004, pages 418-424.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 2: Surface Pattern Learning",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Figure 3: Answer annotation example",
                "uris": null,
                "fig_num": "3",
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "Figure 4: POS tagging",
                "uris": null,
                "fig_num": "4",
                "type_str": "figure"
            },
            "FIGREF3": {
                "num": null,
                "text": "Figure 5: Combined POS and Answer tagging",
                "uris": null,
                "fig_num": "5",
                "type_str": "figure"
            },
            "FIGREF4": {
                "num": null,
                "text": "(1)+ learned patterns 0.3860 (5) (2)+ learned patterns 0.4026 Table 3: Results on Who-is (Biographical) Ques-Results on \"What-is\" (Other Definitional) Questions",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "TABREF1": {
                "content": "<table><tr><td>Relations used for Who-Is questions</td></tr><tr><td>ROLE/MANAGEMENT, ROLE/GENERAL-STAFF,</td></tr><tr><td>ROLE/CITIZEN-OF, ROLE/FOUNDER,</td></tr><tr><td>ROLE/OWNER, AT/RESIDENCE,</td></tr><tr><td>SOC/SPOUSE, SOC/PARENT,</td></tr><tr><td>ROLE/MEMBER, SOC/OTHER-PROFESSIONAL</td></tr><tr><td>Relation used for What-Is questions</td></tr><tr><td>AT/BASED-IN, AT/LOCATED, PART/PART-OF</td></tr></table>",
                "type_str": "table",
                "text": "Relations used in our system",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table><tr><td/><td>Pre. Recall</td><td>F</td></tr><tr><td>Parsing</td><td colspan=\"2\">0.813 0.828 0.820</td></tr><tr><td>Co-reference</td><td colspan=\"2\">0.920 0.897 0.908</td></tr><tr><td colspan=\"3\">Name-entity detection 0.765 0.753 0.759</td></tr></table>",
                "type_str": "table",
                "text": ". Linguistic analysis component performance for Chinese",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table><tr><td/><td colspan=\"3\">Training Patterns Patterns</td></tr><tr><td/><td colspan=\"3\">snippets learned selected</td></tr><tr><td>10% train</td><td>549</td><td>56</td><td>33</td></tr><tr><td>30% train</td><td>1645</td><td>144</td><td>88</td></tr><tr><td>50% train</td><td>2742</td><td>211</td><td>135</td></tr><tr><td>70% train</td><td>3839</td><td>281</td><td>183</td></tr><tr><td>90% train</td><td>4935</td><td>343</td><td>222</td></tr><tr><td>100% train</td><td>5483</td><td>381</td><td>266</td></tr><tr><td colspan=\"4\">Figure 6: How much annotation is required (mea-</td></tr><tr><td colspan=\"3\">sured on biographical questions)</td><td/></tr></table>",
                "type_str": "table",
                "text": "Number of patterns with different size of training data",
                "html": null,
                "num": null
            },
            "TABREF4": {
                "content": "<table><tr><td>feature</td><td colspan=\"2\">who-is what-is</td></tr><tr><td>copula</td><td>0.567</td><td>0.797</td></tr><tr><td>appositive</td><td colspan=\"2\">0.3460 0.3657</td></tr><tr><td>proposition</td><td colspan=\"2\">0.1162 0.1837</td></tr><tr><td>relation</td><td colspan=\"2\">0.3509 0.4422</td></tr><tr><td>sentence</td><td colspan=\"2\">0.1074 0.1556</td></tr><tr><td colspan=\"3\">learned patterns 0.6542 0.6858</td></tr><tr><td>feature</td><td colspan=\"2\">who-is what-is</td></tr><tr><td>copula</td><td>0.055</td><td>0.049</td></tr><tr><td>appositive</td><td colspan=\"2\">0.2028 0.0026</td></tr><tr><td>proposition</td><td colspan=\"2\">0.2101 0.1683</td></tr><tr><td>relation</td><td colspan=\"2\">0.2722 0.043</td></tr><tr><td>sentence</td><td colspan=\"2\">0.1619 0.1717</td></tr><tr><td colspan=\"3\">learned patterns 0.3517 0.0860</td></tr></table>",
                "type_str": "table",
                "text": "Feature Precision Comparison",
                "html": null,
                "num": null
            },
            "TABREF5": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Feature Coverage Comparison",
                "html": null,
                "num": null
            }
        }
    }
}