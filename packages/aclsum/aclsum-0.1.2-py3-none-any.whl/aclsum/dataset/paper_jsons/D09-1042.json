{
    "paper_id": "D09-1042",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:26:53.744699Z"
    },
    "title": "Natural Language Generation with Tree Conditional Random Fields",
    "authors": [
        {
            "first": "Wei",
            "middle": [],
            "last": "Lu",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "National University of Singapore",
                "location": {}
            },
            "email": "luwei@nus.edu.sg"
        },
        {
            "first": "Hwee",
            "middle": [
                "Tou"
            ],
            "last": "Ng",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "National University of Singapore",
                "location": {}
            },
            "email": ""
        },
        {
            "first": "Wee",
            "middle": [
                "Sun"
            ],
            "last": "Lee",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "National University of Singapore",
                "location": {}
            },
            "email": ""
        },
        {
            "first": "Singapore-Mit",
            "middle": [],
            "last": "Alliance",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "National University of Singapore",
                "location": {}
            },
            "email": ""
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "This paper presents an effective method for generating natural language sentences from their underlying meaning representations. The method is built on top of a hybrid tree representation that jointly encodes both the meaning representation as well as the natural language in a tree structure. By using a tree conditional random field on top of the hybrid tree representation, we are able to explicitly model phrase-level dependencies amongst neighboring natural language phrases and meaning representation components in a simple and natural way. We show that the additional dependencies captured by the tree conditional random field allows it to perform better than directly inverting a previously developed hybrid tree semantic parser. Furthermore, we demonstrate that the model performs better than a previous state-of-the-art natural language generation model. Experiments are performed on two benchmark corpora with standard automatic evaluation metrics.",
    "pdf_parse": {
        "paper_id": "D09-1042",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "This paper presents an effective method for generating natural language sentences from their underlying meaning representations. The method is built on top of a hybrid tree representation that jointly encodes both the meaning representation as well as the natural language in a tree structure. By using a tree conditional random field on top of the hybrid tree representation, we are able to explicitly model phrase-level dependencies amongst neighboring natural language phrases and meaning representation components in a simple and natural way. We show that the additional dependencies captured by the tree conditional random field allows it to perform better than directly inverting a previously developed hybrid tree semantic parser. Furthermore, we demonstrate that the model performs better than a previous state-of-the-art natural language generation model. Experiments are performed on two benchmark corpora with standard automatic evaluation metrics.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "One of the ultimate goals in the field of natural language processing (NLP) is to enable computers to converse with humans through human languages.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "To achieve this goal, two important issues need to be studied. First, it is important for computers to capture the meaning of a natural language sentence in a meaning representation. Second, computers should be able to produce a humanunderstandable natural language sentence from its meaning representation. These two tasks are referred to as semantic parsing and natural language generation (NLG), respectively.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this paper, we use corpus-based statistical methods for constructing a natural language generation system. Given a set of pairs, where each pair consists of a natural language (NL) sentence and its formal meaning representation (MR), a learning method induces an algorithm that can be used for performing language generation from other previously unseen meaning representations.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "A crucial question in any natural language processing system is the representation used. Meaning representations can be in the form of a tree structure. In Lu et al. (2008) , we introduced a hybrid tree framework together with a probabilistic generative model to tackle semantic parsing, where tree structured meaning representations are used. The hybrid tree gives a natural joint tree representation of a natural language sentence and its meaning representation.",
                "cite_spans": [
                    {
                        "start": 156,
                        "end": 172,
                        "text": "Lu et al. (2008)",
                        "ref_id": "BIBREF12"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "A joint generative model for natural language and its meaning representation, such as that used in Lu et al. (2008) has several advantages over various previous approaches designed for semantic parsing. First, unlike most previous approaches, the generative approach models a simultaneous generation process for both NL and MR. One elegant property of such a joint generative model is that it allows the modeling of both semantic parsing and natural language generation within the same process. Second, the generative process proceeds as a recursive top-down Markov process in a way that takes advantage of the tree structure of the MR. The hybrid tree generative model proposed in Lu et al. (2008) was shown to give stateof-the-art accuracy in semantic parsing on benchmark corpora.",
                "cite_spans": [
                    {
                        "start": 99,
                        "end": 115,
                        "text": "Lu et al. (2008)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 682,
                        "end": 698,
                        "text": "Lu et al. (2008)",
                        "ref_id": "BIBREF12"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "While semantic parsing with hybrid trees has been studied in Lu et al. (2008) , its inverse task -NLG with hybrid trees -has not yet been explored. We believe that the properties that make the hybrid trees effective for semantic parsing also make them effective for NLG. In this paper, we develop systems for the generation task by building on top of the generative model introduced in Lu et al. (2008) (referred to as the LNLZ08 system).",
                "cite_spans": [
                    {
                        "start": 61,
                        "end": 77,
                        "text": "Lu et al. (2008)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 386,
                        "end": 402,
                        "text": "Lu et al. (2008)",
                        "ref_id": "BIBREF12"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "We first present a baseline model by directly \"inverting\" the LNLZ08 system, where an NL sentence is generated word by word. We call this model the direct inversion model. This model is unable to model some long range global dependencies over the entire NL sentence to be generated. To tackle several weaknesses exhibited by the baseline model, we next introduce an alternative, novel model that performs generation at the phrase level. Motivated by conditional random fields (CRF) (Lafferty et al., 2001) , a different parameterization of the conditional probability of the hybrid tree that enables the model to encode some longer range dependencies amongst phrases and MRs is used. This novel model is referred to as the tree CRF-based model.",
                "cite_spans": [
                    {
                        "start": 482,
                        "end": 505,
                        "text": "(Lafferty et al., 2001)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Evaluation results for both models are presented, through which we demonstrate that the tree CRF-based model performs better than the direct inversion model. We also compare the tree CRFbased model against the previous state-of-the-art model of Wong and Mooney (2007) . Furthermore, we evaluate our model on a dataset annotated with several natural languages other than English (Japanese, Spanish, and Turkish). Evaluation results show that our proposed tree CRF-based model outperforms the previous model.",
                "cite_spans": [
                    {
                        "start": 245,
                        "end": 267,
                        "text": "Wong and Mooney (2007)",
                        "ref_id": "BIBREF21"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "There have been substantial earlier research efforts on investigating methods for transforming MR to their corresponding NL sentences. Most of the recent systems tackled the problem through the architecture of chart generation introduced by Kay (1996) . Examples of such systems include the chart generator for Head-Driven Phrase Structure Grammar (HPSG) (Carroll et al., 1999; Carroll and Oepen, 2005; Nakanishi et al., 2005) , and more recently for Combinatory Categorial Grammar (CCG) (White and Baldridge, 2003; White, 2004) . However, most of these systems only focused on surface realization (inflection and ordering of NL words) and ignored lexical selection (learning the mappings from MR domain concepts to NL words).",
                "cite_spans": [
                    {
                        "start": 241,
                        "end": 251,
                        "text": "Kay (1996)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 355,
                        "end": 377,
                        "text": "(Carroll et al., 1999;",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 378,
                        "end": 402,
                        "text": "Carroll and Oepen, 2005;",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 403,
                        "end": 426,
                        "text": "Nakanishi et al., 2005)",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 488,
                        "end": 515,
                        "text": "(White and Baldridge, 2003;",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 516,
                        "end": 528,
                        "text": "White, 2004)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "The recent work by Wong and Mooney (2007) explored methods for generation by inverting a system originally designed for semantic parsing. They introduced a system named WASP -1 that employed techniques from statistical machine translation using Synchronous Context-Free Grammar (SCFG) (Aho and Ullman, 1972) . The system took in a linearized MR tree as input, and translated it into a natural language sentence as output. Unlike most previous systems, their system integrated both lexical selection and surface realization in a single framework. The performance of the system was enhanced by incorporating models borrowed from PHARAOH (Koehn, 2004) . Experiments show that this new hybrid system named WASP -1 ++ gives state-of-the-art accuracies and outperforms the direct translation model obtained from PHARAOH, when evaluated on two corpora. We will compare our system's performance against that of WASP -1 ++ in Section 5. Following most previous works in this area (Kate et al., 2005; Ge and Mooney, 2005; Kate and Mooney, 2006; Wong and Mooney, 2006; Lu et al., 2008) , we consider MRs in the form of tree structures. An example MR and its corresponding natural language sentence are shown in Figure 1 . The MR is a tree consisting of nodes called MR productions. For example, the node \"QUERY : answer(RIVER)\" is one MR production. Each MR production consists of a semantic category (\"QUERY\"), a function symbol (\"answer\") which can be optionally omitted, as well as an argument list which possibly contains Now we give a brief overview of the hybrid tree framework and the LNLZ08 system that was presented in Lu et al. (2008) . The training corpus required by the LNLZ08 system contains example pairs d (i) = ( m (i) , w (i) ) for i = 1 . . . N , where each m (i) is an MR, and each w (i) is an NL sentence. The system makes the assumption that the entire training corpus is generated from an underlying generative model, which is specified by the parameter set \u2126.",
                "cite_spans": [
                    {
                        "start": 19,
                        "end": 41,
                        "text": "Wong and Mooney (2007)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 285,
                        "end": 307,
                        "text": "(Aho and Ullman, 1972)",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 635,
                        "end": 648,
                        "text": "(Koehn, 2004)",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 971,
                        "end": 990,
                        "text": "(Kate et al., 2005;",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 991,
                        "end": 1011,
                        "text": "Ge and Mooney, 2005;",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 1012,
                        "end": 1034,
                        "text": "Kate and Mooney, 2006;",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 1035,
                        "end": 1057,
                        "text": "Wong and Mooney, 2006;",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 1058,
                        "end": 1074,
                        "text": "Lu et al., 2008)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 1617,
                        "end": 1633,
                        "text": "Lu et al. (2008)",
                        "ref_id": "BIBREF12"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 1207,
                        "end": 1208,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "The parameter set \u2126 includes the following: the MR model parameter \u03c1(m j |m i , arg k ) which models the generation of an MR production m j from its parent MR production m i as its k-th child, the emission parameter \u03b8(t|m i , \u039b) that is responsible for generation of an NL word or a semantic category t from the MR production m i (the parent of t) under the context \u039b (such as the token to the left of the current token), and the pattern parameter \u03c6(r|m i ), which models the selection of a hybrid pattern r that defines globally how the NL words and semantic categories are interleaved given a parent MR production m i . All these parameters are estimated from the corpus during the training phase. The list of possible hybrid patterns is given in Table 1 (at most two child semantic categories are allowed -MR productions with more child semantic categories are transformed into those with two).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "In the table, m refers to the MR production, the symbol w denotes an NL word sequence and is optional if it appears inside []. The symbol Y and Z refer to the first and second semantic category under the MR production m respectively.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "# RHS Hybrid Pattern # Patterns 0 m \u2192 w 1 1 m \u2192 [w]Y[w] 4 2 m \u2192 [w]Y[w]Z[w] 8 m \u2192 [w]Z[w]Y[w]",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "8",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "Table 1 : The list of possible hybrid patterns, [] denotes optional",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "The generative process recursively creates MR productions as well as NL words at each generation step in a top-down manner. This process results in a hybrid tree for each MR-NL pair. The list of children under each MR production in the hybrid tree forms a hybrid sequence. One example hybrid tree for the MR-NL pair given in Figure 1 is shown in Figure 2 . In this hybrid tree T 1 , the list of children under the production RIVER : longest(RIVER) forms the hybrid sequence \"the longest RIVER : exclude(RIVER 1 RIVER 2 )\". The yield of the hybrid tree is exactly the NL sentence. The MR can also be recovered from the hybrid tree by recording all the internal nodes of the tree, subject to the reordering operation required by the hybrid pattern.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 332,
                        "end": 333,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    },
                    {
                        "start": 353,
                        "end": 354,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "To illustrate, consider the generation of the hybrid tree T 1 shown in Figure 2 . The model first generates an MR production from its parent MR production (empty as the MR production is the root in the MR). Next, it selects a hybrid pattern m \u2192 wY from the predefined list of hybrid patterns, which puts a constraint on the set of all allowable hybrid sequences that can be generated: the hybrid sequence must be an NL word sequence followed by a semantic category. Finally, actual NL words and semantic categories are generated from the parent MR production. Now the generation for one level is complete, and the above process repeats at the newly generated MR productions, until the complete NL sentence and MR are both generated.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 78,
                        "end": 79,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "Mathematically, the above generative process yields the following formula that models the joint probability for the MR-NL pair, assuming the context \u039b for the emission parameter is the preceding word or semantic category (i.e., the bigram model is assumed, as discussed in Lu et al. (2008) ):",
                "cite_spans": [
                    {
                        "start": 273,
                        "end": 289,
                        "text": "Lu et al. (2008)",
                        "ref_id": "BIBREF12"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "p T 1 ( w, m) = \u03c1(QUERY : answer(RIVER)|-, arg 1 ) \u00d7\u03c6(m \u2192 wY|QUERY : answer(RIVER)) \u00d7\u03b8(what|QUERY : answer(RIVER), BEGIN) \u00d7\u03b8(is|QUERY : answer(RIVER), what) \u00d7\u03b8(RIVER|QUERY : answer(RIVER), is) \u00d7\u03b8(END|QUERY : answer(RIVER), RIVER) \u00d7\u03c1(RIVER : longest(RIVER)| QUERY : answer(RIVER), arg 1 ) \u00d7 . . . (1)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "where T 1 ( w, m) denotes the hybrid tree T 1 which contains the NL sentence w and MR m.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "For each MR-NL pair in the training set, there can be potentially many possible hybrid trees associated with the pair. However, the correct hybrid tree is completely unknown during training. The correct hybrid tree is therefore treated as a hidden variable. An efficient inside-outside style algorithm (Baker, 1979) coupled with further dynamic programming techniques is used for efficient parameter estimation.",
                "cite_spans": [
                    {
                        "start": 302,
                        "end": 315,
                        "text": "(Baker, 1979)",
                        "ref_id": "BIBREF1"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "During the testing phase, the system makes use of the learned model parameters to determine the most probable hybrid tree given a new natural language sentence. The MR contained in that hybrid tree is the output of the system. Dynamic programming techniques similar to those of training are also employed for efficient decoding.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "The generative model used in the LNLZ08 system has a natural symmetry, allowing for easy transformation from NL to MR, as well as from MR to NL. This provides the starting point for our work in \"inverting\" the LNLZ08 system to generate natural language sentences from the underlying meaning representations.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "The task of generating NL sentences from MRs can be defined as follows. Given a training corpus consisting of MRs paired with their NL sentences, one needs to develop algorithms that learn how to effectively \"paraphrase\" MRs with natural language sentences. During testing, the system should be able to output the most probable NL \"paraphrase\" for a given new MR.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generation with Hybrid Trees",
                "sec_num": "4"
            },
            {
                "text": "The LNLZ08 system models p(T ( w, m)), the joint generative process for the hybrid tree containing both NL and MR. This term can be rewritten in the following way:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generation with Hybrid Trees",
                "sec_num": "4"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "p(T ( w, m)) = p( m) \u00d7 p (T ( w, m)| m)",
                        "eq_num": "(2)"
                    }
                ],
                "section": "Generation with Hybrid Trees",
                "sec_num": "4"
            },
            {
                "text": "In other words, we reach an alternative view of the joint generative process as follows. We choose to generate the complete MR m first. Given m, we generate hybrid sequences below each of its MR production, which gives us a complete hybrid tree T ( w, m). The NL sentence w can be constructed from this hybrid tree exactly.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generation with Hybrid Trees",
                "sec_num": "4"
            },
            {
                "text": "We define an operation yield(T ) which returns the NL sentence as the yield of the hybrid tree T . Given an MR m, we find the most probable NL sentence w * as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generation with Hybrid Trees",
                "sec_num": "4"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "w * = yield argmax T p(T | m)",
                        "eq_num": "(3)"
                    }
                ],
                "section": "Generation with Hybrid Trees",
                "sec_num": "4"
            },
            {
                "text": "In other words, we first find the most probable hybrid tree T that contains the provided MR m. Next we return the yield of T as the most probable NL sentence.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generation with Hybrid Trees",
                "sec_num": "4"
            },
            {
                "text": "Different assumptions can be made in the process of finding the most probable hybrid tree. We first describe a simple model which is a direct inversion of the LNLZ08 system. This model, as a baseline model, generates a complete NL sentence word by word. Next, a more sophisticated model that exploits NL phrase-level dependencies is built that tackles some weaknesses of the simple baseline model.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generation with Hybrid Trees",
                "sec_num": "4"
            },
            {
                "text": "Assume that a pre-order traversal of the MR m gives us the list of MR productions m 1 , m 2 , . . . , m S , where S is the number of MR productions in m. Based on the independence assumption made by the LNLZ08 system, each MR production independently generates a hybrid sequence. Denote the hybrid sequence generated under the MR production m s as h s , for s = 1, . . . , S. We call the list of hybrid sequences h = h 1 , h 2 , . . . , h S a hybrid sequence list associated with this particular MR. Thus, our goal is to find the optimal hybrid sequence list h * for the given MR m, which is formulated as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Direct Inversion Model",
                "sec_num": "4.1"
            },
            {
                "text": "h * = h * 1 , . . . , h * S = argmax h 1 ,...,h S S s=1 p(h s |m s ) (4)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Direct Inversion Model",
                "sec_num": "4.1"
            },
            {
                "text": "The optimal hybrid sequence list defines the optimal hybrid tree whose yield gives the optimal NL sentence.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Direct Inversion Model",
                "sec_num": "4.1"
            },
            {
                "text": "Due to the strong independence assumption introduced by the LNLZ08 system, the hybrid tree generation process is in fact highly decomposable. Optimization of the hybrid sequence list h 1 , . . . , h S can be performed individually since they are independent of one another. Thus, mathematically, for s = 1, . . . , S, we have:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Direct Inversion Model",
                "sec_num": "4.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "h * s = argmax hs p(h s |m s )",
                        "eq_num": "(5)"
                    }
                ],
                "section": "Direct Inversion Model",
                "sec_num": "4.1"
            },
            {
                "text": "The LNLZ08 system presented three models for the task of transforming NL to MR. In this inverse task, for generation of a hybrid sequence, we choose to use the bigram model (model II). We choose this model mainly due to its stronger ability in modeling dependencies between adjacent NL words, which we believe to be quite important in this NL generation task. With the bigram model assumption, the optimal hybrid sequence that can be generated from each MR production is defined as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Direct Inversion Model",
                "sec_num": "4.1"
            },
            {
                "text": "h * s = argmax hs p(h s |m s ) = argmax hs \u03c6(r|m s ) \u00d7 |hs|+1 j=1 \u03b8(t j |m s , t j-1 ) (6)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Direct Inversion Model",
                "sec_num": "4.1"
            },
            {
                "text": "where t i is either an NL word or a semantic category with t 0 \u2261 BEGIN and t |hs|+1 \u2261 END, and r is the hybrid pattern that matches the hybrid sequence h s , which is equivalent to t 1 , . . . , t |hs| . Equivalently, we can view the problem in the log-space:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Direct Inversion Model",
                "sec_num": "4.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "h * s = argmin hs -log \u03c6(r|m s ) + |hs|+1 j=1 -log \u03b8(t j |m s , t j-1 )",
                        "eq_num": "(7)"
                    }
                ],
                "section": "Direct Inversion Model",
                "sec_num": "4.1"
            },
            {
                "text": "Note the term -log \u03c6(r|m s ) is a constant for a particular MR production m s and a particular hybrid pattern r. This search problem can be equivalently cast as the shortest path problem which can be solved efficiently with Dijkstra's algorithm (Cormen et al., 2001) . We define a set of states. Each state represents a single NL word or a semantic category, including the special symbols BEGIN and END. A directed path between two different states t u and t v is associated with a distance measure -log \u03b8(t v |m s , t u ), which is non-negative. The task now is to find the shortest path between BEGIN and END1 . The sequence of words appearing in this path is simply the most probable hybrid sequence under this MR production m s . We build this model by directly inverting the LNLZ08 system, and this model is therefore referred to as the direct inversion model.",
                "cite_spans": [
                    {
                        "start": 245,
                        "end": 266,
                        "text": "(Cormen et al., 2001)",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Direct Inversion Model",
                "sec_num": "4.1"
            },
            {
                "text": "A major weakness of this baseline model is that it encodes strong independence assumptions during the hybrid tree generation process. Though shown to be effective in the task of transforming NL to MR, such independence assumptions may introduce difficulties in this NLG task. For example, consider the MR shown in Figure 1 . The generation steps of the hybrid sequences from the two adjacent MR productions QUERY : answer(RIVER) and RIVER : longest(RIVER) are completely independent of each other. This may harm the fluency of the generated NL sentence, especially when a transition from one hybrid sequence to another is required. In fact, due to such an independence assumption, the model always generates the same hybrid sequence from the same MR production, regardless of its context such as parent or child MR productions. Such a limitation points to the importance of better utilizing the tree structure of the MR for this NLG task. Furthermore, due to the bigram assumption, the model is unable to capture longer range dependencies amongst the words or semantic categories in each hybrid sequence.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 321,
                        "end": 322,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Direct Inversion Model",
                "sec_num": "4.1"
            },
            {
                "text": "To tackle the above issues, we explore ways of relaxing various assumptions, which leads to an ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Direct Inversion Model",
                "sec_num": "4.1"
            },
            {
                "text": "Based on the belief that using known phrases usually leads to better fluency in the NLG task (Wong and Mooney, 2007) , we explore methods for generating an NL sentence at phrase level rather than at word level. This is done by generating hybrid sequences as complete objects, rather than sequentially one word or semantic category at a time, from MR productions. We assume that each MR production can generate a complete hybrid sequence below it from a finite set of possible hybrid sequences. Each such hybrid sequence is called a candidate hybrid sequence associated with that particular MR production. Given a set of candidate hybrid sequences associated with each MR production, the generation task is to find the optimal hybrid sequence list h * for a given MR m:",
                "cite_spans": [
                    {
                        "start": 93,
                        "end": 116,
                        "text": "(Wong and Mooney, 2007)",
                        "ref_id": "BIBREF21"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Tree CRF-Based Model",
                "sec_num": "4.2"
            },
            {
                "text": "h * = argmax h p(h| m) (8)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Tree CRF-Based Model",
                "sec_num": "4.2"
            },
            {
                "text": "Figure 3 shows a complete MR, as well as a possible tree that contains hybrid sequences associated with the MR productions. For example, in the figure the MR production RIVER : traverse(STATE) is associated with the hybrid sequence run through STATE 1 . Each MR production can be associated with potentially many different hybrid sequences. The task is to determine the most probable list of hybrid sequences as the ones appearing on the right of Figure 3 , one for each MR production.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "3",
                        "ref_id": "FIGREF2"
                    },
                    {
                        "start": 454,
                        "end": 455,
                        "text": "3",
                        "ref_id": "FIGREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Tree CRF-Based Model",
                "sec_num": "4.2"
            },
            {
                "text": "To make better use of the tree structure of MR, we take the approach of modeling the conditional distribution using a log-linear model. Following the conditional random fields (CRF) framework (Lafferty et al., 2001) , we can define the probability of the hybrid sequence list given the complete MR m, as follows:",
                "cite_spans": [
                    {
                        "start": 192,
                        "end": 215,
                        "text": "(Lafferty et al., 2001)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Tree CRF-Based Model",
                "sec_num": "4.2"
            },
            {
                "text": "p(h| m) = 1 Z( m) exp i\u2208V k \u00b5 k g k (h i , m, i) + (i,j)\u2208E k \u03bb k f k (h i , h j , m, i, j) (9)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Tree CRF-Based Model",
                "sec_num": "4.2"
            },
            {
                "text": "where V is the set of all the vertices in the tree, and E is the set of the edges in the tree, consisting of parent-child pairs. The function Z( m) is the normalization function. Note that the dependencies among the features here form a tree, unlike the sequence models used in Lafferty et al. (2001) . The function f k (h i , h j , m, i, j) is a feature function of the entire MR tree m and the hybrid sequences at vertex i and j. These features are usually referred to as the edge features in the CRF framework. The function g k (h i , m, i) is a feature function of the hybrid sequence at vertex i and the entire MR tree. These features are usually referred to as the vertex features. The parameters \u03bb k and \u00b5 k are learned from the training data.",
                "cite_spans": [
                    {
                        "start": 278,
                        "end": 300,
                        "text": "Lafferty et al. (2001)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Tree CRF-Based Model",
                "sec_num": "4.2"
            },
            {
                "text": "In this task, we are given only MR-NL pairs and do not have the hybrid tree corresponding to each MR as training data. Now we describe how the set of candidate hybrid sequences for each MR production is obtained as well as how the training data for this model is constructed. After the joint generative model is learned as done in Lu et al. (2008) , we first use a Viterbi algorithm to find the optimal hybrid tree for each MR-NL pair in the training set. From each optimal hybrid tree, we extract the hybrid sequence h i below each MR production m i . Using this process on the training MR-NL pairs, we can obtain a set of candidate hybrid sequences that can be associated with each MR production. The optimal hybrid tree generated by the Viterbi algorithm in this way is considered the \"correct\" hybrid tree for the MR-NL pair and is used as training data. While this does not provide hand-labeled training data, we believe the hybrid trees generated this way form a high quality training set as both the MR and NL are available when Viterbi decoding is performed, guaranteeing that the generated hybrid tree has the correct yield.",
                "cite_spans": [
                    {
                        "start": 331,
                        "end": 347,
                        "text": "Lu et al. (2008)",
                        "ref_id": "BIBREF12"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Tree CRF-Based Model",
                "sec_num": "4.2"
            },
            {
                "text": "There exist several advantages of such a model over the simple generative model. First, this model allows features that specifically model the dependencies between neighboring hybrid sequences in the tree to be used. In addition, the model can efficiently capture long range dependencies between MR productions and hybrid sequences since each hybrid sequence is allowed to depend on the entire MR tree.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Tree CRF-Based Model",
                "sec_num": "4.2"
            },
            {
                "text": "For features, we employ four types of simple features, as presented below. Note that the first three types of features are vertex features, and the last are edge features. Examples are given based on Figure 3 . All the features are indicator functions, i.e., a feature takes value 1 if a certain combination is present, and 0 otherwise. The last three features explicitly encode information from the tree structure of MR.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 207,
                        "end": 208,
                        "text": "3",
                        "ref_id": "FIGREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Tree CRF-Based Model",
                "sec_num": "4.2"
            },
            {
                "text": "Hybrid sequence features : one hybrid sequence together with the associated MR production.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Tree CRF-Based Model",
                "sec_num": "4.2"
            },
            {
                "text": "For example:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Tree CRF-Based Model",
                "sec_num": "4.2"
            },
            {
                "text": "g 1 : run through STATE 1 , RIVER : traverse(STATE) ;",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Tree CRF-Based Model",
                "sec_num": "4.2"
            },
            {
                "text": "Two-level hybrid sequence features : one hybrid sequence, its associated MR production, and the parent MR production. For example:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Tree CRF-Based Model",
                "sec_num": "4.2"
            },
            {
                "text": "g 2 : run through STATE 1 , RIVER : traverse(STATE), RIVER : exclude(RIVER 1 , RIVER 2 ) ;",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Tree CRF-Based Model",
                "sec_num": "4.2"
            },
            {
                "text": "Three-level hybrid sequence features : one hybrid sequence, its associated MR production, the parent MR production, and the grandparent MR production. For example:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Tree CRF-Based Model",
                "sec_num": "4.2"
            },
            {
                "text": "g 3 : run through STATE 1 , RIVER : traverse(STATE), RIVER : exclude(RIVER 1 , RIVER 2 ), RIVER : longest(RIVER) ;",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Tree CRF-Based Model",
                "sec_num": "4.2"
            },
            {
                "text": "Adjacent hybrid sequence features : two adjacent hybrid sequences, together with their associated MR productions. For example:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Tree CRF-Based Model",
                "sec_num": "4.2"
            },
            {
                "text": "f 1 : run through STATE 1 , RIVER 1 that does not RIVER 2 , RIVER : traverse(STATE), RIVER : exclude(RIVER 1 , RIVER 2 ) .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Tree CRF-Based Model",
                "sec_num": "4.2"
            },
            {
                "text": "For training, we use the feature forest model (Miyao and Tsujii, 2008) , which was originally designed as an efficient algorithm for solving maximum entropy models for data with complex structures. The model enables efficient training over packed trees that potentially represent exponential number of trees. The tree conditional random fields model can be effectively represented using the feature forest model. The model has also been successfully applied to the HPSG parsing task.",
                "cite_spans": [
                    {
                        "start": 46,
                        "end": 70,
                        "text": "(Miyao and Tsujii, 2008)",
                        "ref_id": "BIBREF14"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Tree CRF-Based Model",
                "sec_num": "4.2"
            },
            {
                "text": "To train the model, we run the Viterbi algorithm on the trained LNLZ08 model and perform convex optimization using the feature forest model. The LNLZ08 model is trained using an EM algorithm with time complexity O(M N 3 D) per EM iteration, where M and N are respectively the maximum number of MR productions and NL words for each MR-NL pair, and D is the number of training instances. The time complexity of the Viterbi algorithm is also O(M N 3 D). For training the feature forest, we use the Amis toolkit (Miyao and Tsujii, 2002) which utilizes the GIS algorithm. The time complexity for each iteration of the GIS algorithm is O(M K 2 D), where K is the maximum number of candidate hybrid sequences associated with each MR production. Finally, the time complexity for generating a natural language sentence from a particular MR is O(M K 2 ).",
                "cite_spans": [
                    {
                        "start": 508,
                        "end": 532,
                        "text": "(Miyao and Tsujii, 2002)",
                        "ref_id": "BIBREF13"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Tree CRF-Based Model",
                "sec_num": "4.2"
            },
            {
                "text": "In this section, we present the results of our systems when evaluated on two standard benchmark corpora. The first corpus is GEOQUERY, which contains Prolog-based MRs that can be used to query a US geographic database (Kate et al., 2005) . Our task for this domain is to generate NL sentences from the formal queries. The second corpus is ROBOCUP. This domain contains MRs which are instructions written in a formal language called CLANG. Our task for this domain is to generate NL sentences from the coaching advice written in CLANG. GEOQUERY ( 880 The GEOQUERY domain contains 880 instances, while the ROBOCUP domain contains 300 instances. The average NL sentence length for the two corpora are 7.57 and 22.52 respectively. Following the evaluation methodology of Wong and Mooney (2007) , we performed 4 runs of the standard 10-fold cross validation and report the averaged performance in this section using the standard automatic evaluation metric BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) 2 . The BLEU and NIST scores of the WASP -1 ++ system reported in this section are obtained from the published paper of Wong and Mooney (2007) . Note that to make our experimental results directly comparable to Wong and Mooney (2007) , we used the identical training and test data splits for the 4 runs of 10-fold cross validation used by Wong and Mooney (2007) on both corpora.",
                "cite_spans": [
                    {
                        "start": 218,
                        "end": 237,
                        "text": "(Kate et al., 2005)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 767,
                        "end": 789,
                        "text": "Wong and Mooney (2007)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 957,
                        "end": 980,
                        "text": "(Papineni et al., 2002)",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 990,
                        "end": 1008,
                        "text": "(Doddington, 2002)",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 1129,
                        "end": 1151,
                        "text": "Wong and Mooney (2007)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 1220,
                        "end": 1242,
                        "text": "Wong and Mooney (2007)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 1348,
                        "end": 1370,
                        "text": "Wong and Mooney (2007)",
                        "ref_id": "BIBREF21"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "5"
            },
            {
                "text": "Our system has the advantage of always producing an NL sentence given any input MR, even if there exist unseen MR productions in the input MR. We can achieve this by simply skipping those unseen MR productions during the generation process. However, in order to make a fair comparison against WASP -1 ++, which can only generate NL sentences for 97% of the input MRs, we also do not generate any NL sentence in the case of observing an unseen MR production. All the evaluations discussed in this section follow this evalu-2 We used the official evaluation script (version 11b) provided by http://www.nist.gov/.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "5"
            },
            {
                "text": "ation methodology, but we notice that empirically our system is able to achieve higher BLEU/NIST scores if we allow generation for those MRs that include unseen MR productions.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "5"
            },
            {
                "text": "We compare the performance of our two models in Table 2 . From the table, we observe that the tree CRF-based model outperforms the direct inversion model on both domains. This validates our earlier belief that some long range dependencies are important for the generation task. In addition, while the direct inversion model performs reasonably well on the ROBOCUP domain, it performs substantially worse on the GEOQUERY domain where the sentence length is shorter. We note that the evaluation metrics are strongly correlated with the cumulative matching n-grams between the output and the reference sentence (n ranges from 1 to 4 for BLEU, and 1 to 5 for NIST). The direct inversion model fails to capture the transitional behavior from one phrase to another, which makes it more vulnerable to n-gram mismatch, especially when evaluated on the GEOQUERY corpus where phrase-to-phrase transitions are more frequent. On the other hand, the tree CRF-based model does not suffer from this problem, mainly due to its ability to model such dependencies between neighboring phrases. Sample outputs from the two models are shown in Figure 4 ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 54,
                        "end": 55,
                        "text": "2",
                        "ref_id": "TABREF3"
                    },
                    {
                        "start": 1130,
                        "end": 1131,
                        "text": "4",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Comparison between the two models",
                "sec_num": "5.1"
            },
            {
                "text": "We also compare the performance of our tree CRFbased model against the previous state-of-the-art system WASP -1 ++ in Table 3 . Our tree CRF-based model achieves better performance on both corpora. We are unable to carry out statistical significance tests since the detailed BLEU and NIST scores of the cross validation runs of WASP -1 ++ as reported in the published paper of Wong and Mooney (2007) are not available.",
                "cite_spans": [
                    {
                        "start": 377,
                        "end": 399,
                        "text": "Wong and Mooney (2007)",
                        "ref_id": "BIBREF21"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 124,
                        "end": 125,
                        "text": "3",
                        "ref_id": "TABREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Comparison with previous model",
                "sec_num": "5.2"
            },
            {
                "text": "The results confirm our earlier discussions: the dependencies between the generated NL words are important and need to be properly modeled. The WASP -1 ++ system uses a log-linear model which incorporates two major techniques to attempt to model such dependencies. First, a backoff language model is used to capture dependencies at adjacent word level. Second, a technique that merges smaller translation rules into a single rigid rule is used to capture dependencies at phrase level (Wong, 2007) . In contrast, the proposed tree CRF-based model is able to explicitly and flexibly exploit phrase-level features that model dependencies between adjacent phrases. In fact, with the hybrid tree framework, the better treatment of the tree structure of MR enables us to model some crucial dependencies between the complete MR tree and generated NL phrases. We believe that this property plays an important role in improving the quality of the generated sentences in terms of fluency, which is assessed by the evaluation metrics. Furthermore, WASP -1 ++ employs minimum error rate training (Och, 2003) to directly optimize the evaluation metrics. We have not done so but still obtain better performance. In future, we plan to explore ways to directly optimize the evaluation metrics in our system.",
                "cite_spans": [
                    {
                        "start": 484,
                        "end": 496,
                        "text": "(Wong, 2007)",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 1084,
                        "end": 1095,
                        "text": "(Och, 2003)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Comparison with previous model",
                "sec_num": "5.2"
            },
            {
                "text": "Following the work of Wong and Mooney (2007) , we also evaluated our system's performance on a subset of the GEOQUERY corpus with 250 instances, where sentences of 4 natural languages (English, Japanese, Spanish, and Turkish) are available. The evaluation results are shown in Table 4. Our tree CRF-based model achieves better performance on this task compared to WASP -1 ++. We are again unable to conduct statistical significance tests for the same reason reported earlier.",
                "cite_spans": [
                    {
                        "start": 22,
                        "end": 44,
                        "text": "Wong and Mooney (2007)",
                        "ref_id": "BIBREF21"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments on different languages",
                "sec_num": "5.3"
            },
            {
                "text": "In this paper, we presented two novel models for the task of generating natural language sentences from given meaning representations, under a hybrid tree framework. We first built a simple direct inversion model as a baseline. Next, to address the limitations associated with the direct inversion model, a tree CRF-based model was introduced. We evaluated both models on standard benchmark corpora. Evaluation results show that the tree CRF-based model performs better than the direct inversion model, and that the tree CRF-based model also outperforms WASP -1 ++, which was a previous state-of-the-art system reported in the literature.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusions",
                "sec_num": "6"
            },
            {
                "text": "In addition, we should make sure that the generated hybrid sequence t0 . . . t |hs|+1 is a valid hybrid sequence that comply with the hybrid pattern r. For example, the MR production STATE : loc 1(RIVER) can generate the following hybrid sequence \"BEGIN have RIVER END\" but not this hybrid sequence \"BEGIN have END\". This can be achieved by finding the shortest path from BEGIN to RIVER, which then gets concatenated to the shortest path from RIVER to END.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "The authors would like to thank Seung-Hoon Na for his suggestions on the presentation of this paper, Yuk Wah Wong for answering various questions related to the WASP -1 ++ system, and the anonymous reviewers for their thoughtful comments on this work.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgments",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "The Theory of Parsing, Translation and Compiling",
                "authors": [
                    {
                        "first": "Alfred",
                        "middle": [
                            "V"
                        ],
                        "last": "Aho",
                        "suffix": ""
                    },
                    {
                        "first": "Jeffrey",
                        "middle": [
                            "D"
                        ],
                        "last": "Ullman",
                        "suffix": ""
                    }
                ],
                "year": 1972,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alfred V. Aho and Jeffrey D. Ullman. 1972. The Theory of Parsing, Translation and Compiling. Prentice-Hall, Englewood Clis, NJ.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Trainable grammars for speech recognition",
                "authors": [
                    {
                        "first": "James",
                        "middle": [
                            "K"
                        ],
                        "last": "Baker",
                        "suffix": ""
                    }
                ],
                "year": 1979,
                "venue": "Proceedings of the Spring Conference of the",
                "volume": "",
                "issue": "",
                "pages": "547--550",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "James K. Baker. 1979. Trainable grammars for speech recognition. In Proceedings of the Spring Confer- ence of the Acoustical Society of America, pages 547-550, Boston, MA, June.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "High efficiency realization for a wide-coverage unification grammar",
                "authors": [
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Carroll",
                        "suffix": ""
                    },
                    {
                        "first": "Stephan",
                        "middle": [],
                        "last": "Oepen",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of the 2nd International Joint Conference on Natural Language Processing (IJCNLP 2005)",
                "volume": "",
                "issue": "",
                "pages": "165--176",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "John Carroll and Stephan Oepen. 2005. High ef- ficiency realization for a wide-coverage unification grammar. In Proceedings of the 2nd International Joint Conference on Natural Language Processing (IJCNLP 2005), pages 165-176.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "An efficient chart generator for (semi-) lexicalist grammars",
                "authors": [
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Carroll",
                        "suffix": ""
                    },
                    {
                        "first": "Ann",
                        "middle": [],
                        "last": "Copestake",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Flickinger",
                        "suffix": ""
                    },
                    {
                        "first": "Victor",
                        "middle": [],
                        "last": "Poznanski",
                        "suffix": ""
                    }
                ],
                "year": 1999,
                "venue": "Proceedings of the 7th European Workshop on Natural Language Generation (EWNLG 1999)",
                "volume": "",
                "issue": "",
                "pages": "86--95",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "John Carroll, Ann Copestake, Dan Flickinger, and Vic- tor Poznanski. 1999. An efficient chart generator for (semi-) lexicalist grammars. In Proceedings of the 7th European Workshop on Natural Language Generation (EWNLG 1999), pages 86-95.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Introduction to Algorithms",
                "authors": [
                    {
                        "first": "H",
                        "middle": [],
                        "last": "Thomas",
                        "suffix": ""
                    },
                    {
                        "first": "Charles",
                        "middle": [
                            "E"
                        ],
                        "last": "Cormen",
                        "suffix": ""
                    },
                    {
                        "first": "Ronald",
                        "middle": [
                            "L"
                        ],
                        "last": "Leiserson",
                        "suffix": ""
                    },
                    {
                        "first": "Clifford",
                        "middle": [],
                        "last": "Rivest",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Stein",
                        "suffix": ""
                    }
                ],
                "year": 2001,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2001. Introduction to Algorithms (Second Edition). MIT Press.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Automatic evaluation of machine translation quality using n-gram cooccurrence statistics",
                "authors": [
                    {
                        "first": "George",
                        "middle": [],
                        "last": "Doddington",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Proceedings of the 2nd International Conference on Human Language Technology Research (HLT 2002)",
                "volume": "",
                "issue": "",
                "pages": "138--145",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "George Doddington. 2002. Automatic evaluation of machine translation quality using n-gram co- occurrence statistics. In Proceedings of the 2nd In- ternational Conference on Human Language Tech- nology Research (HLT 2002), pages 138-145.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "A statistical semantic parser that integrates syntax and semantics",
                "authors": [
                    {
                        "first": "Ruifang",
                        "middle": [],
                        "last": "Ge",
                        "suffix": ""
                    },
                    {
                        "first": "Raymond",
                        "middle": [
                            "J"
                        ],
                        "last": "Mooney",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of the 9th Conference on Computational Natural Language Learning",
                "volume": "",
                "issue": "",
                "pages": "9--16",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ruifang Ge and Raymond J. Mooney. 2005. A statis- tical semantic parser that integrates syntax and se- mantics. In Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL 2005), pages 9-16.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Using string-kernels for learning semantic parsers",
                "authors": [
                    {
                        "first": "Rohit",
                        "middle": [
                            "J"
                        ],
                        "last": "Kate",
                        "suffix": ""
                    },
                    {
                        "first": "Raymond",
                        "middle": [
                            "J"
                        ],
                        "last": "Mooney",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics (COLING/ACL 2006)",
                "volume": "",
                "issue": "",
                "pages": "913--920",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Rohit J. Kate and Raymond J. Mooney. 2006. Us- ing string-kernels for learning semantic parsers. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Lin- guistics (COLING/ACL 2006), pages 913-920.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Learning to transform natural to formal languages",
                "authors": [
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Rohit",
                        "suffix": ""
                    },
                    {
                        "first": "Yuk",
                        "middle": [
                            "Wah"
                        ],
                        "last": "Kate",
                        "suffix": ""
                    },
                    {
                        "first": "Raymond",
                        "middle": [
                            "J"
                        ],
                        "last": "Wong",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Mooney",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of the 20th National Conference on Artificial Intelligence (AAAI 2005)",
                "volume": "",
                "issue": "",
                "pages": "1062--1068",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Rohit J. Kate, Yuk Wah Wong, and Raymond J. Mooney. 2005. Learning to transform natural to for- mal languages. In Proceedings of the 20th National Conference on Artificial Intelligence (AAAI 2005), pages 1062-1068.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Chart generation",
                "authors": [
                    {
                        "first": "Martin",
                        "middle": [],
                        "last": "Kay",
                        "suffix": ""
                    }
                ],
                "year": 1996,
                "venue": "Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics (ACL 1996)",
                "volume": "",
                "issue": "",
                "pages": "200--204",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Martin Kay. 1996. Chart generation. In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics (ACL 1996), pages 200- 204.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Pharaoh: a beam search decoder for phrase-based statistical machine translation models",
                "authors": [
                    {
                        "first": "Philipp",
                        "middle": [],
                        "last": "Koehn",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proceedings of the 6th Conference of the Association for Machine Translation in the Americas (AMTA 2004)",
                "volume": "",
                "issue": "",
                "pages": "115--124",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Philipp Koehn. 2004. Pharaoh: a beam search de- coder for phrase-based statistical machine transla- tion models. In Proceedings of the 6th Conference of the Association for Machine Translation in the Americas (AMTA 2004), pages 115-124.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
                "authors": [
                    {
                        "first": "John",
                        "middle": [
                            "D"
                        ],
                        "last": "Lafferty",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Mccallum",
                        "suffix": ""
                    },
                    {
                        "first": "Fernando",
                        "middle": [
                            "C N"
                        ],
                        "last": "Pereira",
                        "suffix": ""
                    }
                ],
                "year": 2001,
                "venue": "Proceedings of the 18th International Conference on Machine Learning (ICML",
                "volume": "",
                "issue": "",
                "pages": "282--289",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the 18th Inter- national Conference on Machine Learning (ICML 2001), pages 282-289.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "A generative model for parsing natural language to meaning representations",
                "authors": [
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Lu",
                        "suffix": ""
                    },
                    {
                        "first": "Hwee Tou",
                        "middle": [],
                        "last": "Ng",
                        "suffix": ""
                    },
                    {
                        "first": "Wee",
                        "middle": [],
                        "last": "Sun Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Luke",
                        "middle": [
                            "S"
                        ],
                        "last": "Zettlemoyer",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "783--792",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S. Zettlemoyer. 2008. A generative model for pars- ing natural language to meaning representations. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP 2008), pages 783-792.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Maximum entropy estimation for feature forests",
                "authors": [
                    {
                        "first": "Yusuke",
                        "middle": [],
                        "last": "Miyao",
                        "suffix": ""
                    },
                    {
                        "first": "Jun'ichi",
                        "middle": [],
                        "last": "Tsujii",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Proceedings of the 2nd International Conference on Human Language Technology Research (HLT 2002)",
                "volume": "",
                "issue": "",
                "pages": "292--297",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yusuke Miyao and Jun'ichi Tsujii. 2002. Maximum entropy estimation for feature forests. In Proceed- ings of the 2nd International Conference on Human Language Technology Research (HLT 2002), pages 292-297.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Feature forest models for probabilistic HPSG parsing",
                "authors": [
                    {
                        "first": "Yusuke",
                        "middle": [],
                        "last": "Miyao",
                        "suffix": ""
                    },
                    {
                        "first": "Jun'ichi",
                        "middle": [],
                        "last": "Tsujii",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Computational Linguistics",
                "volume": "34",
                "issue": "1",
                "pages": "35--80",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yusuke Miyao and Jun'ichi Tsujii. 2008. Feature for- est models for probabilistic HPSG parsing. Compu- tational Linguistics, 34(1):35-80.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Probabilistic models for disambiguation of an HPSG-based chart generator",
                "authors": [
                    {
                        "first": "Hiroko",
                        "middle": [],
                        "last": "Nakanishi",
                        "suffix": ""
                    },
                    {
                        "first": "Yusuke",
                        "middle": [],
                        "last": "Miyao",
                        "suffix": ""
                    },
                    {
                        "first": "Jun'ichi",
                        "middle": [],
                        "last": "Tsujii",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of the 9th International Workshop on Parsing Technologies (IWPT 2005)",
                "volume": "5",
                "issue": "",
                "pages": "93--102",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Hiroko Nakanishi, Yusuke Miyao, and Jun'ichi Tsujii. 2005. Probabilistic models for disambiguation of an HPSG-based chart generator. In Proceedings of the 9th International Workshop on Parsing Technologies (IWPT 2005), volume 5, pages 93-102.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Minimum error rate training in statistical machine translation",
                "authors": [
                    {
                        "first": "Franz",
                        "middle": [
                            "J"
                        ],
                        "last": "Och",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL 2003)",
                "volume": "",
                "issue": "",
                "pages": "160--167",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Franz J. Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association for Compu- tational Linguistics (ACL 2003), pages 160-167.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Bleu: a method for automatic evaluation of machine translation",
                "authors": [
                    {
                        "first": "Kishore",
                        "middle": [],
                        "last": "Papineni",
                        "suffix": ""
                    },
                    {
                        "first": "Salim",
                        "middle": [],
                        "last": "Roukos",
                        "suffix": ""
                    },
                    {
                        "first": "Todd",
                        "middle": [],
                        "last": "Ward",
                        "suffix": ""
                    },
                    {
                        "first": "Wei-Jing",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL 2002)",
                "volume": "",
                "issue": "",
                "pages": "311--318",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic eval- uation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Compu- tational Linguistics (ACL 2002), pages 311-318.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Adapting chart realization to CCG",
                "authors": [
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "White",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Baldridge",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Proceedings of the 9th European Workshop on Natural Language Generation (EWNLG 2003)",
                "volume": "",
                "issue": "",
                "pages": "119--126",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Michael White and Jason Baldridge. 2003. Adapting chart realization to CCG. In Proceedings of the 9th European Workshop on Natural Language Genera- tion (EWNLG 2003), pages 119-126.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Reining in CCG chart realization",
                "authors": [
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "White",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proceeding of the 3rd International Conference on Natural Language Generation (INLG 2004)",
                "volume": "",
                "issue": "",
                "pages": "182--191",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Michael White. 2004. Reining in CCG chart realiza- tion. In Proceeding of the 3rd International Confer- ence on Natural Language Generation (INLG 2004), pages 182-191.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Learning for semantic parsing with statistical machine translation",
                "authors": [
                    {
                        "first": "Yuk",
                        "middle": [
                            "Wah"
                        ],
                        "last": "Wong",
                        "suffix": ""
                    },
                    {
                        "first": "Raymond",
                        "middle": [
                            "J"
                        ],
                        "last": "Mooney",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Proceedings of the Human Language Technology Conference of the North American Chapter",
                "volume": "",
                "issue": "",
                "pages": "439--446",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yuk Wah Wong and Raymond J. Mooney. 2006. Learning for semantic parsing with statistical ma- chine translation. In Proceedings of the Human Lan- guage Technology Conference of the North Ameri- can Chapter of the Association for Computational Linguistics (HLT/NAACL 2006), pages 439-446.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Generation by inverting a semantic parser that uses statistical machine translation",
                "authors": [
                    {
                        "first": "Yuk",
                        "middle": [
                            "Wah"
                        ],
                        "last": "Wong",
                        "suffix": ""
                    },
                    {
                        "first": "Raymond",
                        "middle": [
                            "J"
                        ],
                        "last": "Mooney",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Proceedings of the Human Language Technology Conference of the North American Chapter",
                "volume": "",
                "issue": "",
                "pages": "172--179",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yuk Wah Wong and Raymond J. Mooney. 2007. Generation by inverting a semantic parser that uses statistical machine translation. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (NAACL/HLT 2007), pages 172-179.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Learning for Semantic Parsing and Natural Language Generation Using Statistical Machine Translation Techniques",
                "authors": [
                    {
                        "first": "Yuk",
                        "middle": [
                            "Wah"
                        ],
                        "last": "Wong",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yuk Wah Wong. 2007. Learning for Semantic Parsing and Natural Language Generation Using Statistical Machine Translation Techniques. Ph.D. thesis, The University of Texas at Austin.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 1: An example MR paired with its NL sentence.",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Figure 2: One possible hybrid tree T 1",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "Figure 3: An MR (left) and its associated hybrid sequences (right)",
                "uris": null,
                "fig_num": "3",
                "type_str": "figure"
            },
            "TABREF3": {
                "content": "<table><tr><td/><td/><td/><td colspan=\"2\">) ROBOCUP (300)</td></tr><tr><td/><td>BLEU</td><td>NIST</td><td>BLEU</td><td>NIST</td></tr><tr><td colspan=\"3\">Direct inversion model 0.3973 5.5466</td><td colspan=\"2\">0.5468 6.6738</td></tr><tr><td colspan=\"3\">Tree CRF-based model 0.5733 6.7459</td><td colspan=\"2\">0.6220 6.9845</td></tr><tr><td/><td colspan=\"4\">GEOQUERY (880) ROBOCUP (300)</td></tr><tr><td/><td>BLEU</td><td>NIST</td><td>BLEU</td><td>NIST</td></tr><tr><td>WASP -1 ++</td><td colspan=\"2\">0.5370 6.4808</td><td colspan=\"2\">0.6022 6.8976</td></tr><tr><td colspan=\"3\">Tree CRF-based model 0.5733 6.7459</td><td colspan=\"2\">0.6220 6.9845</td></tr></table>",
                "type_str": "table",
                "text": "Results of automatic evaluation of both models (bold type indicates the best performing system).",
                "html": null,
                "num": null
            },
            "TABREF4": {
                "content": "<table><tr><td/><td colspan=\"2\">English</td><td colspan=\"2\">Japanese</td><td colspan=\"2\">Spanish</td><td colspan=\"2\">Turkish</td></tr><tr><td/><td>BLEU</td><td>NIST</td><td>BLEU</td><td>NIST</td><td>BLEU</td><td>NIST</td><td>BLEU</td><td>NIST</td></tr><tr><td>WASP -1 ++</td><td colspan=\"8\">0.6035 5.7133 0.6585 4.6648 0.6175 5.7293 0.4824 4.3283</td></tr><tr><td colspan=\"9\">Tree CRF-based model 0.6265 5.8907 0.6788 4.8486 0.6382 5.8488 0.5096 4.5033</td></tr></table>",
                "type_str": "table",
                "text": "Results of automatic evaluation of our tree CRF-based model and WASP -1 ++.",
                "html": null,
                "num": null
            },
            "TABREF5": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Results on the GEOQUERY-250 corpus with 4 natural languages.",
                "html": null,
                "num": null
            },
            "TABREF6": {
                "content": "<table><tr><td>Reference:</td><td/></tr><tr><td>Reference:</td><td>if DR2C7 is true then players 2 , 3 , 7 and 8</td></tr><tr><td/><td>should pass to player 4</td></tr><tr><td colspan=\"2\">Direct inversion model: if DR2C7 , then players 2 , 3 7 and 8 should</td></tr><tr><td/><td>ball to player 4</td></tr><tr><td colspan=\"2\">Tree CRF-based model: if the condition DR2C7 is true then players 2 ,</td></tr><tr><td/><td>3 , 7 and 8 should pass to player 4</td></tr><tr><td colspan=\"2\">Figure 4: Sample outputs from the two models, for GEOQUERY domain (top) and ROBOCUP domain</td></tr><tr><td>(bottom) respectively.</td><td/></tr></table>",
                "type_str": "table",
                "text": ". what is the largest state bordering texas Direct inversion model: what the largest states border texas Tree CRF-based model: what is the largest state that borders texas",
                "html": null,
                "num": null
            }
        }
    }
}