{
    "paper_id": "2021",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:02:30.123788Z"
    },
    "title": "Biomedical Concept Normalization by Leveraging Hypernyms",
    "authors": [
        {
            "first": "Cheng",
            "middle": [],
            "last": "Yan",
            "suffix": "",
            "affiliation": {
                "laboratory": "National Laboratory of Pattern Recognition",
                "institution": "CAS",
                "location": {
                    "settlement": "Beijing",
                    "country": "China"
                }
            },
            "email": "cheng.yan@nlpr.ia.ac.cn"
        },
        {
            "first": "Yuanzhe",
            "middle": [],
            "last": "Zhang",
            "suffix": "",
            "affiliation": {
                "laboratory": "National Laboratory of Pattern Recognition",
                "institution": "CAS",
                "location": {
                    "settlement": "Beijing",
                    "country": "China"
                }
            },
            "email": "yzzhang@nlpr.ia.ac.cn"
        },
        {
            "first": "Kang",
            "middle": [],
            "last": "Liu",
            "suffix": "",
            "affiliation": {
                "laboratory": "National Laboratory of Pattern Recognition",
                "institution": "CAS",
                "location": {
                    "settlement": "Beijing",
                    "country": "China"
                }
            },
            "email": "kliu@nlpr.ia.ac.cn"
        },
        {
            "first": "Jun",
            "middle": [],
            "last": "Zhao",
            "suffix": "",
            "affiliation": {
                "laboratory": "National Laboratory of Pattern Recognition",
                "institution": "CAS",
                "location": {
                    "settlement": "Beijing",
                    "country": "China"
                }
            },
            "email": "jzhao@nlpr.ia.ac.cn"
        },
        {
            "first": "Yafei",
            "middle": [],
            "last": "Shi",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Unisound AI Technology Co",
                "location": {
                    "settlement": "Ltd"
                }
            },
            "email": "shiyafei@unisound.com"
        },
        {
            "first": "Shengping",
            "middle": [],
            "last": "Liu",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Unisound AI Technology Co",
                "location": {
                    "settlement": "Ltd"
                }
            },
            "email": "liushengping@unisound.com"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Biomedical Concept Normalization (BCN) is widely used in biomedical text processing as a fundamental module. Owing to numerous surface variants of biomedical concepts, BCN still remains challenging and unsolved. In this paper, we exploit biomedical concept hypernyms to facilitate BCN. We propose Biomedical Concept Normalizer with Hypernyms (BCNH), a novel framework that adopts list-wise training to make use of both hypernyms and synonyms, and also employs norm constraint on the representation of hypernym-hyponym entity pairs. The experimental results show that BCNH outperform the previous state-of-the-art model on the NCBI dataset. Code will be available at https://github.com/ yan-cheng/BCNH.",
    "pdf_parse": {
        "paper_id": "2021",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Biomedical Concept Normalization (BCN) is widely used in biomedical text processing as a fundamental module. Owing to numerous surface variants of biomedical concepts, BCN still remains challenging and unsolved. In this paper, we exploit biomedical concept hypernyms to facilitate BCN. We propose Biomedical Concept Normalizer with Hypernyms (BCNH), a novel framework that adopts list-wise training to make use of both hypernyms and synonyms, and also employs norm constraint on the representation of hypernym-hyponym entity pairs. The experimental results show that BCNH outperform the previous state-of-the-art model on the NCBI dataset. Code will be available at https://github.com/ yan-cheng/BCNH.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Biomedical Concept Normalization (BCN) plays an important and prerequisite role in biomedical text processing. The goal of BCN is to link the entity mention in the context to its normalized CUI (Unique Concept Identifier) in the biomedical dictionaries such as UMLS (Bodenreider, 2004 ), SNOMED-CT (Spackman et al., 1997) and MedDRA (Brown et al., 1999) . Figure 1 is an example of BCN from NCBI dataset (Dogan et al., 2014) , the mention B-cell non-Hodgkins lymphomas should be linked to D016393 Lymphoma, B-Cell in the MEDIC (Davis et al., 2012) dictionary.",
                "cite_spans": [
                    {
                        "start": 266,
                        "end": 284,
                        "text": "(Bodenreider, 2004",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 298,
                        "end": 321,
                        "text": "(Spackman et al., 1997)",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 326,
                        "end": 353,
                        "text": "MedDRA (Brown et al., 1999)",
                        "ref_id": null
                    },
                    {
                        "start": 404,
                        "end": 424,
                        "text": "(Dogan et al., 2014)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 527,
                        "end": 547,
                        "text": "(Davis et al., 2012)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 363,
                        "end": 364,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Recent works on BCN usually adopt encoders like CNN (Li et al., 2017) , LSTM (Phan et al., 2019) , ELMo (Peters et al., 2018; Schumacher et al., 2020) or BioBERT (Lee et al., 2020; Fakhraei et al., 2019; Ji et al., 2020) to embed both the mention and the concept's name entities, and then feed the representations to the following classifier or ranking network to determine the corresponding concept in the biomedical dictionary. However, biomedical dictionaries are generally sparse in nature: a concept is usually provided with only CUI, referred name (recommended concept name string), synonyms (acceptable name variants, synonyms), and related concepts (mainly hypernym concepts). Therefore, effectively using the limited information in the biomedical dictionary where the candidate entities came from is paramount for the BCN task.",
                "cite_spans": [
                    {
                        "start": 52,
                        "end": 69,
                        "text": "(Li et al., 2017)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 77,
                        "end": 96,
                        "text": "(Phan et al., 2019)",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 104,
                        "end": 125,
                        "text": "(Peters et al., 2018;",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 126,
                        "end": 150,
                        "text": "Schumacher et al., 2020)",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 162,
                        "end": 180,
                        "text": "(Lee et al., 2020;",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 181,
                        "end": 203,
                        "text": "Fakhraei et al., 2019;",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 204,
                        "end": 220,
                        "text": "Ji et al., 2020)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "For concept's synonym entities, recent BNE (Phan et al., 2019) and BIOSYN (Sung et al., 2020) tries to make full use of them by synonym marginalization to enhance biomedical entity representation and achieved consistent performance improvement. Unfortunately, previous works generally ignore concept hypernym hierarchy structure, which is exactly the initial motivation of biomedical dictionary: organization of thousands of concepts under a unified and multi-level hierarchical classification schema.",
                "cite_spans": [
                    {
                        "start": 43,
                        "end": 62,
                        "text": "(Phan et al., 2019)",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 74,
                        "end": 93,
                        "text": "(Sung et al., 2020)",
                        "ref_id": "BIBREF18"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "We believe that leveraging hypernym information in the biomedical dictionary can improve the BCN performance based on two intuitions. First, hard negative sampling (Fakhraei et al., 2019; Phan et al., 2019) is vital for the BCN model's discriminating ability and a hypernym is a hard negative example for its hyponym naturally. Second, injecting the hypernym hierarchy information during the training process is beneficial for encoders, since currently used encoders like BioBERT only encodes the context semantics in biomedical corpora instead of the biomedical concept structural information.",
                "cite_spans": [
                    {
                        "start": 164,
                        "end": 187,
                        "text": "(Fakhraei et al., 2019;",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 188,
                        "end": 206,
                        "text": "Phan et al., 2019)",
                        "ref_id": "BIBREF15"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "To this end, we propose Biomedical Concept Normalizer with Hypernyms (BCNH), a novel framework combining the list-wise cross entropy loss with norm constraint on hypernym-hyponym entity pairs. Concretely, we reformulate the candidate target list as a three-level relevance list to consider both synonyms and hypernyms, and apply the list-wise cross entropy loss. On the one hand, synonyms help to encode surface name variants, on the other hand, hypernyms help encode hierarchical structural information. We also apply the norm constraint on the embedding of hypernym-hyponym entity pairs to further preserve the principal hypernym relation. Specifically, for a hypernym-hyponym entity pair (e hyper , e hypo ), we constraint that the norm of hypernym entity e hyper is larger than that of e hyper in a multi-task manner. We conduct experiments on the NCBI dataset and outperforms the previous state-of-the-art model.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "To sum up, the contributions of this paper are as follows. First, for the first time, we reformulate the candidate target list as a three-level relevance list and apply the list-wise loss to attend all candidate entities. Second, we innovatively use norm constraint to model the hypernym-hyponym relation, preserving the hierarchy structure information inside the entity representation. The proposed BCNH outperforms the previous state-of-the-art model on the NCBI dataset, leading to an improvement of 0.73 % on top1 accuracy.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The architecture of our framework is illustrated in Figure 2 . Our model is composed of three parts: candidate generator to generate the candidate entities from the dictionary, list-wise ranker to train the encoder, hypernym normalizer to apply the hypernym-hyponym norm constraint.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 59,
                        "end": 60,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Methodology",
                "sec_num": "2"
            },
            {
                "text": "We reuse the iterative candidate generator module from BIOSYN (Sung et al., 2020) . Each mention m and entity e i in dictionary D = {e 1 , e 2 , \u2022 \u2022 \u2022 } are represented first with sparse representations and dense representations. The sparse representations of m and e i are denoted as (v s m , v s e i ) which is calculated based on the character-level n-grams statistics computed over all entities from D. The dense representations of m and e i are denoted as (v d m , v d e i ), which are obtained from the pre-trained BioBERT.",
                "cite_spans": [
                    {
                        "start": 62,
                        "end": 81,
                        "text": "(Sung et al., 2020)",
                        "ref_id": "BIBREF18"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Iterative Candidate Generator",
                "sec_num": "2.1"
            },
            {
                "text": "The candidate generator then computes the similarity score between mention m and each entity e i by combining the sparse similarity score S sparse (m, e i ) with dense similarity score S dense (m, e i ):",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Iterative Candidate Generator",
                "sec_num": "2.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "z i = S dense (m, e i ) + \u03bbS sparse (m, e i ) (1) S dense (m, e i ) = f (v d m , v d e i ) (2) S sparse (m, e i ) = f (v s m , v s e i )",
                        "eq_num": "(3)"
                    }
                ],
                "section": "Iterative Candidate Generator",
                "sec_num": "2.1"
            },
            {
                "text": "where function f is the inner product and \u03bb is a trainable sparse score scalar weight. In the end, the top k 1 entities with the highest similarity scores are selected into candidate list [e 1 , e 2 , \u2022 \u2022 \u2022 , e k 1 ], and their similarity score list is denoted as",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Iterative Candidate Generator",
                "sec_num": "2.1"
            },
            {
                "text": "z = [z 1 , z 2 , \u2022 \u2022 \u2022 , z k 1 ].",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Iterative Candidate Generator",
                "sec_num": "2.1"
            },
            {
                "text": "The candidate list is pre-computed and iteratively updated at the beginning of every training step.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Iterative Candidate Generator",
                "sec_num": "2.1"
            },
            {
                "text": "At inference time, the entity e * \u2208 D with top similarity score is retrieved, and the CUI of entity e * is returned as predicted CUI.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Iterative Candidate Generator",
                "sec_num": "2.1"
            },
            {
                "text": "For mention m and its top k 1 candidate list [e 1 , e 2 , \u2022 \u2022 \u2022 , e k 1 ], we reformulate the targets of candidate list as a three-level relevance score list. The relevance score is defined as the degree of relevance between mention m and candidate entity e i . Specifically, for a candidate entity e i , the relevance score y i is set to 2 if e i is synonym of m, 1 if e i is hypernym of m, 0 if neither synonym or hypernym. Therefore, we have a pseudo relevance score target y = [y 1 , y 2 , \u2022 \u2022 \u2022 , y k 1 ] and the candidate similarity",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "List-wise Ranker",
                "sec_num": "2.2"
            },
            {
                "text": "score list z = [z 1 , z 2 , \u2022 \u2022 \u2022 , z k 1 ].",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "List-wise Ranker",
                "sec_num": "2.2"
            },
            {
                "text": "The list-wise cross entropy loss (LCE) (Cao et al., 2007) then is applied on the relevance score y and candidate similarity score z. The objective of learning candidate similarity is formalized as minimization of the total LCE losses on all examples:",
                "cite_spans": [
                    {
                        "start": 39,
                        "end": 57,
                        "text": "(Cao et al., 2007)",
                        "ref_id": "BIBREF2"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "List-wise Ranker",
                "sec_num": "2.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "ListLoss = 1 M M j=1 LCE(y j , z j ) (4) LCE(y j , z j ) = - k 1 i=1 P y i log(P z i )",
                        "eq_num": "(5)"
                    }
                ],
                "section": "List-wise Ranker",
                "sec_num": "2.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "P y i = e y i k 1 i=1 e yn , P z i = e z i k 1 i=1 e zn (",
                        "eq_num": "6"
                    }
                ],
                "section": "List-wise Ranker",
                "sec_num": "2.2"
            },
            {
                "text": ")",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "List-wise Ranker",
                "sec_num": "2.2"
            },
            {
                "text": "where M is the number of mentions in the training dataset.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "List-wise Ranker",
                "sec_num": "2.2"
            },
            {
                "text": "Leveraging hypernyms for the list-wise learning targets can be interpreted as a hard negative sampling technique (Kalantidis et al., 2020) , which is crucial under the contrastive learning framework.",
                "cite_spans": [
                    {
                        "start": 113,
                        "end": 138,
                        "text": "(Kalantidis et al., 2020)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "List-wise Ranker",
                "sec_num": "2.2"
            },
            {
                "text": "Though we take hypernyms into account by the listwise training, the hypernym hierarchy information inside the dictionary is still absent in concept entity representation. It has been proven in (Vuli\u0107 and Mrk\u0161i\u0107, 2018) that the asymmetric norm distance is an effective way to encode the hierarchical ordering between hypernym and hyponym entities.",
                "cite_spans": [
                    {
                        "start": 193,
                        "end": 217,
                        "text": "(Vuli\u0107 and Mrk\u0161i\u0107, 2018)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Hypernym Normalizer",
                "sec_num": "2.3"
            },
            {
                "text": "During training, we prepare a k 2 length hypernym list (e h 1 , e h 2 , \u2022 \u2022 \u2022 ) for mention m. We denote the norm distance between mention m and its all hyponyms as N ormLoss:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Hypernym Normalizer",
                "sec_num": "2.3"
            },
            {
                "text": "N ormLoss = 1 k 2 k 2 i=1 |v d m | -|v d h i | |v d m | + |v d h i | (7)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Hypernym Normalizer",
                "sec_num": "2.3"
            },
            {
                "text": "By minimizing the N ormLoss, we constraint that the norm of hypernym embedding vector v d h i is larger than the mention embedding vector v d m under the intuition that the norm constraint fine-tunes norm values in the Euclidean embedding space to reflect the hierarchical organization of biomedical concept entities.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Hypernym Normalizer",
                "sec_num": "2.3"
            },
            {
                "text": "In the end, the BCNH jointly optimizes cost:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Hypernym Normalizer",
                "sec_num": "2.3"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "Loss = ListLoss + N ormLoss",
                        "eq_num": "(8)"
                    }
                ],
                "section": "Hypernym Normalizer",
                "sec_num": "2.3"
            },
            {
                "text": "3 Experiments",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Hypernym Normalizer",
                "sec_num": "2.3"
            },
            {
                "text": "Dataset We train and evaluate our model on the NCBI Disease corpus, a collection of 793 PubMed abstracts with disease mentions and their concepts corresponding to the MEDIC dictionary. In this work, we use the MEDIC of version February 1, 2021 that contains 13,103 CUIs, 74,215 synonyms, and 21,999 hypernyms.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental setup",
                "sec_num": "3.1"
            },
            {
                "text": "Preprocessing We follow the same dataset preprocessing including lower-casing, punctuation removing, abbreviations expanding, composite mentions splitting in previous works (Leaman and Lu, 2016; Wright, 2019; Phan et al., 2019; Sung et al., 2020) . We use the top k accuracy metric to evaluate the task.",
                "cite_spans": [
                    {
                        "start": 173,
                        "end": 194,
                        "text": "(Leaman and Lu, 2016;",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 195,
                        "end": 208,
                        "text": "Wright, 2019;",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 209,
                        "end": 227,
                        "text": "Phan et al., 2019;",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 228,
                        "end": 246,
                        "text": "Sung et al., 2020)",
                        "ref_id": "BIBREF18"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental setup",
                "sec_num": "3.1"
            },
            {
                "text": "Hyper-parameters We set all the parameters in the candidate generator exactly the same with BIOSYN for fair comparison. Our model only introduces a new hyper parameter k 2 = 10 in our experiments. When the hypernyms of mention m in the dictionary is more than k 2 , we truncate it to k 2 ; and pad null entity if less than k 2 . The Adam optimizer (Kingma and Ba, 2014) is used to minimize the final loss.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental setup",
                "sec_num": "3.1"
            },
            {
                "text": "The main results are shown in Table 1 . Our proposed BCNH outperforms the previous state-of-theart model BIOSYN (Sung et al., 2020) on Acc@1 and Acc@5 with an improvement of 0.73% and 1.18%, respectively. Our model also obtains a smaller confidence interval.",
                "cite_spans": [
                    {
                        "start": 112,
                        "end": 131,
                        "text": "(Sung et al., 2020)",
                        "ref_id": "BIBREF18"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 36,
                        "end": 37,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "3.2"
            },
            {
                "text": "Acc@1 Acc@5 Sieve-Based (D'Souza and Ng, 2015) 84.7 -Taggerone (Leaman and Lu, 2016) 87.7 -CNN Ranking (Li et al., 2017) 86.1 -NormCo (Wright, 2019) 87.8 -BNE (Phan et al., 2019) 87.7 -BERT Ranking (Ji et al., 2020) 89.1 -TripletNet (Mondal et al., 2019) 90.0 -BIOSYN \u2020 (Sung et al., 2020) 89.88 \u00b1 0.22 93.82 \u00b1 0.26 BCNH (Ours) 90.61 \u00b1 0.17 95.00 \u00b1 0.14 \u2020 Since the original result is reported for a different version MEDIC dictionary, we use the author's provided code to evaluate the BIOSYN model with different seeds 10 times with 95% confidence. The results of other models are directly cited.",
                "cite_spans": [
                    {
                        "start": 63,
                        "end": 84,
                        "text": "(Leaman and Lu, 2016)",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 103,
                        "end": 120,
                        "text": "(Li et al., 2017)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 134,
                        "end": 148,
                        "text": "(Wright, 2019)",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 159,
                        "end": 178,
                        "text": "(Phan et al., 2019)",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 198,
                        "end": 215,
                        "text": "(Ji et al., 2020)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 233,
                        "end": 254,
                        "text": "(Mondal et al., 2019)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 270,
                        "end": 289,
                        "text": "(Sung et al., 2020)",
                        "ref_id": "BIBREF18"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Models",
                "sec_num": null
            },
            {
                "text": "Table 1 : Acc@1 and Acc@5 on NCBI dataset.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Models",
                "sec_num": null
            },
            {
                "text": "For comparison, we list the top 10 predictions of an example mention B-cell non-Hodgkins lymphomas (D016393) from the NCBI test dataset in Table 2 . BIOSYN fails to rank the synonyms before the hypernyms when the mention string is closer to hypernyms than to synonyms while BCNH manages to return fine-grained results. ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 145,
                        "end": 146,
                        "text": "2",
                        "ref_id": "TABREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Models",
                "sec_num": null
            },
            {
                "text": "We conduct the ablation study to figure out the contributions of the two proposed components. The results are presented in Table 3 . The first experiment reports the results of BIOSYN and the second reports for BIOSYN with a joint hypernym norm constraint. The third experiment reports the results of BCNH with list-wise training only, and the last experiment reports for BCNH with both list-wise training and norm constraint. The results demonstrate that norm constraint indeed endows the concept entity representation with the hypernym-hyponym hierarchy structure. It also verifies that hypernyms are beneficial for harder negative sampling and paying attention to all candidate entities including hypernyms list-wisely is more appropriate than marginalization solely on the synonyms.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 129,
                        "end": 130,
                        "text": "3",
                        "ref_id": "TABREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Ablation study",
                "sec_num": null
            },
            {
                "text": "In this paper, we propose BCNH to leverage hypernyms in the biomedical concept normalization task. We adopts both list-wise training and norm constraint with the help of hypernym information. The experimental results on the NCBI dataset show that BCNH outperforms previous state-of-the-art models.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "4"
            }
        ],
        "back_matter": [
            {
                "text": "This work is supported by the National Natural Science Foundation of China (No.61922085, No.61976211, No.61906196) and the Key Research Program of the Chinese Academy of Sciences (Grant NO. ZDBS-SSW-JSC006). This work is also supported by Beijing Academy of Artificial Intelligence (BAAI2019QN0301), the Open Project of Beijing Key Laboratory of Mental Disroders (2019JSJB06) and in part by the Youth Innovation Promotion Association CAS.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgements",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "The unified medical language system (umls): integrating biomedical terminology",
                "authors": [
                    {
                        "first": "Olivier",
                        "middle": [],
                        "last": "Bodenreider",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Nucleic acids research",
                "volume": "32",
                "issue": "suppl_1",
                "pages": "267--D270",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Olivier Bodenreider. 2004. The unified medical lan- guage system (umls): integrating biomedical termi- nology. Nucleic acids research, 32(suppl_1):D267- D270.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "The medical dictionary for regulatory activities (meddra)",
                "authors": [
                    {
                        "first": "Louise",
                        "middle": [],
                        "last": "Elliot G Brown",
                        "suffix": ""
                    },
                    {
                        "first": "Sue",
                        "middle": [],
                        "last": "Wood",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Wood",
                        "suffix": ""
                    }
                ],
                "year": 1999,
                "venue": "Drug safety",
                "volume": "20",
                "issue": "2",
                "pages": "109--117",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Elliot G Brown, Louise Wood, and Sue Wood. 1999. The medical dictionary for regulatory activities (meddra). Drug safety, 20(2):109-117.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Learning to rank: from pairwise approach to listwise approach",
                "authors": [
                    {
                        "first": "Zhe",
                        "middle": [],
                        "last": "Cao",
                        "suffix": ""
                    },
                    {
                        "first": "Tao",
                        "middle": [],
                        "last": "Qin",
                        "suffix": ""
                    },
                    {
                        "first": "Tie-Yan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Ming-Feng",
                        "middle": [],
                        "last": "Tsai",
                        "suffix": ""
                    },
                    {
                        "first": "Hang",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Proceedings of the 24th international conference on Machine learning",
                "volume": "",
                "issue": "",
                "pages": "129--136",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. 2007. Learning to rank: from pairwise ap- proach to listwise approach. In Proceedings of the 24th international conference on Machine learning, pages 129-136.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Medic: a practical disease vocabulary used at the comparative toxicogenomics database",
                "authors": [
                    {
                        "first": "Allan",
                        "middle": [],
                        "last": "Peter",
                        "suffix": ""
                    },
                    {
                        "first": "Davis",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Thomas",
                        "middle": [
                            "C"
                        ],
                        "last": "Wiegers",
                        "suffix": ""
                    },
                    {
                        "first": "Carolyn",
                        "middle": [
                            "J"
                        ],
                        "last": "Michael C Rosenstein",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Mattingly",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Database",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Allan Peter Davis, Thomas C Wiegers, Michael C Rosenstein, and Carolyn J Mattingly. 2012. Medic: a practical disease vocabulary used at the compara- tive toxicogenomics database. Database, 2012.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Ncbi disease corpus: a resource for disease name recognition and concept normalization",
                "authors": [
                    {
                        "first": "Rezarta",
                        "middle": [],
                        "last": "Islamaj Dogan",
                        "suffix": ""
                    },
                    {
                        "first": "Robert",
                        "middle": [],
                        "last": "Leaman",
                        "suffix": ""
                    },
                    {
                        "first": "Zhiyong",
                        "middle": [],
                        "last": "Lu",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Journal of biomedical informatics",
                "volume": "47",
                "issue": "",
                "pages": "1--10",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Rezarta Islamaj Dogan, Robert Leaman, and Zhiyong Lu. 2014. Ncbi disease corpus: a resource for dis- ease name recognition and concept normalization. Journal of biomedical informatics, 47:1-10.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Sieve-based entity linking for the biomedical domain",
                "authors": [
                    {
                        "first": "D'",
                        "middle": [],
                        "last": "Jennifer",
                        "suffix": ""
                    },
                    {
                        "first": "Vincent",
                        "middle": [],
                        "last": "Souza",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Ng",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing",
                "volume": "2",
                "issue": "",
                "pages": "297--302",
                "other_ids": {
                    "DOI": [
                        "10.3115/v1/P15-2049"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jennifer D'Souza and Vincent Ng. 2015. Sieve-based entity linking for the biomedical domain. In Pro- ceedings of the 53rd Annual Meeting of the Associ- ation for Computational Linguistics and the 7th In- ternational Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 297- 302, Beijing, China. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Nseen: Neural semantic embedding for entity normalization",
                "authors": [
                    {
                        "first": "Shobeir",
                        "middle": [],
                        "last": "Fakhraei",
                        "suffix": ""
                    },
                    {
                        "first": "Joel",
                        "middle": [],
                        "last": "Mathew",
                        "suffix": ""
                    },
                    {
                        "first": "Jos\u00e9",
                        "middle": [],
                        "last": "Luis",
                        "suffix": ""
                    },
                    {
                        "first": "Ambite",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Joint European Conference on Machine Learning and Knowledge Discovery in Databases",
                "volume": "",
                "issue": "",
                "pages": "665--680",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Shobeir Fakhraei, Joel Mathew, and Jos\u00e9 Luis Ambite. 2019. Nseen: Neural semantic embedding for en- tity normalization. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 665-680. Springer.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Bertbased ranking for biomedical entity normalization",
                "authors": [
                    {
                        "first": "Zongcheng",
                        "middle": [],
                        "last": "Ji",
                        "suffix": ""
                    },
                    {
                        "first": "Qiang",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Hua",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "AMIA Summits on Translational Science Proceedings",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zongcheng Ji, Qiang Wei, and Hua Xu. 2020. Bert- based ranking for biomedical entity normalization. AMIA Summits on Translational Science Proceed- ings, 2020:269.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Hard negative mixing for contrastive learning",
                "authors": [
                    {
                        "first": "Yannis",
                        "middle": [],
                        "last": "Kalantidis",
                        "suffix": ""
                    },
                    {
                        "first": "Bulent",
                        "middle": [],
                        "last": "Mert",
                        "suffix": ""
                    },
                    {
                        "first": "Noe",
                        "middle": [],
                        "last": "Sariyildiz",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Pion",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "33",
                "issue": "",
                "pages": "21798--21809",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yannis Kalantidis, Mert Bulent Sariyildiz, Noe Pion, Philippe Weinzaepfel, and Diane Larlus. 2020. Hard negative mixing for contrastive learning. In Ad- vances in Neural Information Processing Systems, volume 33, pages 21798-21809. Curran Associates, Inc.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Adam: A method for stochastic optimization",
                "authors": [
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Diederik",
                        "suffix": ""
                    },
                    {
                        "first": "Jimmy",
                        "middle": [],
                        "last": "Kingma",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Ba",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1412.6980"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Taggerone: joint named entity recognition and normalization with semi-markov models",
                "authors": [
                    {
                        "first": "Robert",
                        "middle": [],
                        "last": "Leaman",
                        "suffix": ""
                    },
                    {
                        "first": "Zhiyong",
                        "middle": [],
                        "last": "Lu",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Bioinformatics",
                "volume": "32",
                "issue": "18",
                "pages": "2839--2846",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Robert Leaman and Zhiyong Lu. 2016. Tag- gerone: joint named entity recognition and normal- ization with semi-markov models. Bioinformatics, 32(18):2839-2846.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Biobert: a pre-trained biomedical language representation model for biomedical text mining",
                "authors": [
                    {
                        "first": "Jinhyuk",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Wonjin",
                        "middle": [],
                        "last": "Yoon",
                        "suffix": ""
                    },
                    {
                        "first": "Sungdong",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "Donghyeon",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "Sunkyu",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "Chan",
                        "middle": [],
                        "last": "Ho",
                        "suffix": ""
                    },
                    {
                        "first": "So",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Jaewoo",
                        "middle": [],
                        "last": "Kang",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Bioinformatics",
                "volume": "36",
                "issue": "4",
                "pages": "1234--1240",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. 2020. Biobert: a pre-trained biomed- ical language representation model for biomedical text mining. Bioinformatics, 36(4):1234-1240.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Cnn-based ranking for biomedical entity normalization",
                "authors": [
                    {
                        "first": "Haodi",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Qingcai",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Buzhou",
                        "middle": [],
                        "last": "Tang",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaolong",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Hua",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Baohua",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Dong",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "BMC bioinformatics",
                "volume": "18",
                "issue": "11",
                "pages": "79--86",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Haodi Li, Qingcai Chen, Buzhou Tang, Xiaolong Wang, Hua Xu, Baohua Wang, and Dong Huang. 2017. Cnn-based ranking for biomedical entity nor- malization. BMC bioinformatics, 18(11):79-86.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Amitava Bhattacharyya, and Mahanandeeshwar Gattu",
                "authors": [
                    {
                        "first": "Ishani",
                        "middle": [],
                        "last": "Mondal",
                        "suffix": ""
                    },
                    {
                        "first": "Sukannya",
                        "middle": [],
                        "last": "Purkayastha",
                        "suffix": ""
                    },
                    {
                        "first": "Sudeshna",
                        "middle": [],
                        "last": "Sarkar",
                        "suffix": ""
                    },
                    {
                        "first": "Pawan",
                        "middle": [],
                        "last": "Goyal",
                        "suffix": ""
                    },
                    {
                        "first": "Jitesh",
                        "middle": [],
                        "last": "Pillai",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 2nd Clinical Natural Language Processing Workshop",
                "volume": "",
                "issue": "",
                "pages": "95--100",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/W19-1912"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ishani Mondal, Sukannya Purkayastha, Sudeshna Sarkar, Pawan Goyal, Jitesh Pillai, Amitava Bhat- tacharyya, and Mahanandeeshwar Gattu. 2019. Medical entity linking using triplet network. In Pro- ceedings of the 2nd Clinical Natural Language Pro- cessing Workshop, pages 95-100, Minneapolis, Min- nesota, USA. Association for Computational Lin- guistics.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Deep contextualized word representations",
                "authors": [
                    {
                        "first": "Matthew",
                        "middle": [],
                        "last": "Peters",
                        "suffix": ""
                    },
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Neumann",
                        "suffix": ""
                    },
                    {
                        "first": "Mohit",
                        "middle": [],
                        "last": "Iyyer",
                        "suffix": ""
                    },
                    {
                        "first": "Matt",
                        "middle": [],
                        "last": "Gardner",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [],
                        "last": "Clark",
                        "suffix": ""
                    },
                    {
                        "first": "Kenton",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Luke",
                        "middle": [],
                        "last": "Zettlemoyer",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "volume": "1",
                "issue": "",
                "pages": "2227--2237",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/N18-1202"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word rep- resentations. In Proceedings of the 2018 Confer- ence of the North American Chapter of the Associ- ation for Computational Linguistics: Human Lan- guage Technologies, Volume 1 (Long Papers), pages 2227-2237, New Orleans, Louisiana. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Robust representation learning of biomedical names",
                "authors": [
                    {
                        "first": "Minh",
                        "middle": [
                            "C"
                        ],
                        "last": "Phan",
                        "suffix": ""
                    },
                    {
                        "first": "Aixin",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Yi",
                        "middle": [],
                        "last": "Tay",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "3275--3285",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/P19-1317"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Minh C. Phan, Aixin Sun, and Yi Tay. 2019. Ro- bust representation learning of biomedical names. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3275-3285, Florence, Italy. Association for Compu- tational Linguistics.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Clinical concept linking with contextualized neural representations",
                "authors": [
                    {
                        "first": "Elliot",
                        "middle": [],
                        "last": "Schumacher",
                        "suffix": ""
                    },
                    {
                        "first": "Andriy",
                        "middle": [],
                        "last": "Mulyar",
                        "suffix": ""
                    },
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Dredze",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "8585--8592",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2020.acl-main.760"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Elliot Schumacher, Andriy Mulyar, and Mark Dredze. 2020. Clinical concept linking with contextualized neural representations. In Proceedings of the 58th Annual Meeting of the Association for Computa- tional Linguistics, pages 8585-8592, Online. Asso- ciation for Computational Linguistics.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Snomed rt: a reference terminology for health care",
                "authors": [
                    {
                        "first": "Keith",
                        "middle": [
                            "E"
                        ],
                        "last": "Kent A Spackman",
                        "suffix": ""
                    },
                    {
                        "first": "Roger",
                        "middle": [
                            "A"
                        ],
                        "last": "Campbell",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "C\u00f4t\u00e9",
                        "suffix": ""
                    }
                ],
                "year": 1997,
                "venue": "Proceedings of the AMIA annual fall symposium",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kent A Spackman, Keith E Campbell, and Roger A C\u00f4t\u00e9. 1997. Snomed rt: a reference terminology for health care. In Proceedings of the AMIA annual fall symposium, page 640. American Medical Informat- ics Association.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Biomedical entity representations with synonym marginalization",
                "authors": [
                    {
                        "first": "Mujeen",
                        "middle": [],
                        "last": "Sung",
                        "suffix": ""
                    },
                    {
                        "first": "Hwisang",
                        "middle": [],
                        "last": "Jeon",
                        "suffix": ""
                    },
                    {
                        "first": "Jinhyuk",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Jaewoo",
                        "middle": [],
                        "last": "Kang",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "3641--3650",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2020.acl-main.335"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Mujeen Sung, Hwisang Jeon, Jinhyuk Lee, and Jaewoo Kang. 2020. Biomedical entity representations with synonym marginalization. In Proceedings of the 58th Annual Meeting of the Association for Compu- tational Linguistics, pages 3641-3650, Online. As- sociation for Computational Linguistics.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Specialising word vectors for lexical entailment",
                "authors": [
                    {
                        "first": "Ivan",
                        "middle": [],
                        "last": "Vuli\u0107",
                        "suffix": ""
                    },
                    {
                        "first": "Nikola",
                        "middle": [],
                        "last": "Mrk\u0161i\u0107",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "volume": "1",
                "issue": "",
                "pages": "1134--1145",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/N18-1103"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ivan Vuli\u0107 and Nikola Mrk\u0161i\u0107. 2018. Specialising word vectors for lexical entailment. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Hu- man Language Technologies, Volume 1 (Long Pa- pers), pages 1134-1145, New Orleans, Louisiana. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "NormCo: Deep disease normalization for biomedical knowledge base construction",
                "authors": [
                    {
                        "first": "Dustin",
                        "middle": [],
                        "last": "Wright",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dustin Wright. 2019. NormCo: Deep disease normal- ization for biomedical knowledge base construction. Ph.D. thesis, UC San Diego.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 1: A biomedical concept normalization example.",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Figure 2: The architecture of BCNH.",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table><tr><td>BIOSYN</td><td>BCNH</td></tr><tr><td>nonhodgkins lymphoma  \u2020</td><td>cell lymphomas  *</td></tr><tr><td>non hodgkins lymphoma  \u2020</td><td>b cell lymphoma  *</td></tr><tr><td>lymphoma non hodgkins  \u2020</td><td>lymphomas b cell  *</td></tr><tr><td>lymphoma nonhodgkin  \u2020</td><td>lymphoma b cell  *</td></tr><tr><td>lymphoma nonhodgkins  \u2020</td><td>lymphoma non hodgkin  \u2020</td></tr><tr><td>lymphoma nonhodgkin s  \u2020</td><td>lymphoma nonhodgkin  \u2020</td></tr><tr><td>nonhodgkin s lymphoma  \u2020</td><td>non hodgkins lymphoma  \u2020</td></tr><tr><td>lymphoma non hodgkin  \u2020</td><td>lymphoma non hodgkins  \u2020</td></tr><tr><td>hodgkins lymphoma</td><td>nonhodgkins lymphoma  \u2020</td></tr><tr><td colspan=\"2\">lymphoma non hodgkin s  \u2020 lymphoma non hodgkin s  \u2020</td></tr></table>",
                "type_str": "table",
                "text": "Changes in the top 10 predictions given the mention from the NCBI test set. Synonyms having correct CUIs are indicated with an asterisk * , hypernyms are indicated with a dagger \u2020 .",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Ablation study results for each component.",
                "html": null,
                "num": null
            }
        }
    }
}