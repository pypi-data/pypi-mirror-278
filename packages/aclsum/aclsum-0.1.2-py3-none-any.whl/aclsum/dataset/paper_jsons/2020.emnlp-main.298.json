{
    "paper_id": "2020",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:07:51.682387Z"
    },
    "title": "Learning from Context or Names? An Empirical Study on Neural Relation Extraction",
    "authors": [
        {
            "first": "Hao",
            "middle": [],
            "last": "Peng",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Tsinghua University",
                "location": {
                    "settlement": "Beijing",
                    "country": "China"
                }
            },
            "email": "h-peng17@mails.tsinghua.edu.cn"
        },
        {
            "first": "Tianyu",
            "middle": [],
            "last": "Gao",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Princeton University",
                "location": {
                    "settlement": "Princeton",
                    "region": "NJ",
                    "country": "USA"
                }
            },
            "email": "tianyug@princeton.edu"
        },
        {
            "first": "Xu",
            "middle": [],
            "last": "Han",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Tsinghua University",
                "location": {
                    "settlement": "Beijing",
                    "country": "China"
                }
            },
            "email": ""
        },
        {
            "first": "Yankai",
            "middle": [],
            "last": "Lin",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Tencent Inc",
                "location": {
                    "addrLine": "WeChat AI",
                    "country": "China"
                }
            },
            "email": ""
        },
        {
            "first": "Peng",
            "middle": [],
            "last": "Li",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Tencent Inc",
                "location": {
                    "addrLine": "WeChat AI",
                    "country": "China"
                }
            },
            "email": ""
        },
        {
            "first": "Zhiyuan",
            "middle": [],
            "last": "Liu",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Tsinghua University",
                "location": {
                    "settlement": "Beijing",
                    "country": "China"
                }
            },
            "email": ""
        },
        {
            "first": "Maosong",
            "middle": [],
            "last": "Sun",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Tsinghua University",
                "location": {
                    "settlement": "Beijing",
                    "country": "China"
                }
            },
            "email": ""
        },
        {
            "first": "Jie",
            "middle": [],
            "last": "Zhou",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Tencent Inc",
                "location": {
                    "addrLine": "WeChat AI",
                    "country": "China"
                }
            },
            "email": ""
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Neural models have achieved remarkable success on relation extraction (RE) benchmarks. However, there is no clear understanding which type of information affects existing RE models to make decisions and how to further improve the performance of these models. To this end, we empirically study the effect of two main information sources in text: textual context and entity mentions (names). We find that (i) while context is the main source to support the predictions, RE models also heavily rely on the information from entity mentions, most of which is type information, and (ii) existing datasets may leak shallow heuristics via entity mentions and thus contribute to the high performance on RE benchmarks. Based on the analyses, we propose an entity-masked contrastive pre-training framework for RE to gain a deeper understanding on both textual context and type information while avoiding rote memorization of entities or use of superficial cues in mentions. We carry out extensive experiments to support our views, and show that our framework can improve the effectiveness and robustness of neural models in different RE scenarios. All the code and datasets are released at https://github.com/thunlp/ RE-Context-or-Names.",
    "pdf_parse": {
        "paper_id": "2020",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Neural models have achieved remarkable success on relation extraction (RE) benchmarks. However, there is no clear understanding which type of information affects existing RE models to make decisions and how to further improve the performance of these models. To this end, we empirically study the effect of two main information sources in text: textual context and entity mentions (names). We find that (i) while context is the main source to support the predictions, RE models also heavily rely on the information from entity mentions, most of which is type information, and (ii) existing datasets may leak shallow heuristics via entity mentions and thus contribute to the high performance on RE benchmarks. Based on the analyses, we propose an entity-masked contrastive pre-training framework for RE to gain a deeper understanding on both textual context and type information while avoiding rote memorization of entities or use of superficial cues in mentions. We carry out extensive experiments to support our views, and show that our framework can improve the effectiveness and robustness of neural models in different RE scenarios. All the code and datasets are released at https://github.com/thunlp/ RE-Context-or-Names.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Relation extraction (RE) aims at extracting relational facts between entities from text, e.g., extracting the fact (SpaceX, founded by, Elon Musk) from the sentence in Figure 1 . Utilizing the structured knowledge captured by RE, we can construct or complete knowledge graphs (KGs), and eventually support downstream applications like question answering (Bordes et al., 2014) , dialog systems (Madotto et al., 2018) engines (Xiong et al., 2017) . With the recent advance of deep learning, neural relation extraction (NRE) models (Socher et al., 2012; Liu et al., 2013; Baldini Soares et al., 2019) have achieved the latest state-of-the-art results and some of them are even comparable with human performance on several public RE benchmarks.",
                "cite_spans": [
                    {
                        "start": 354,
                        "end": 375,
                        "text": "(Bordes et al., 2014)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 393,
                        "end": 415,
                        "text": "(Madotto et al., 2018)",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 424,
                        "end": 444,
                        "text": "(Xiong et al., 2017)",
                        "ref_id": "BIBREF34"
                    },
                    {
                        "start": 529,
                        "end": 550,
                        "text": "(Socher et al., 2012;",
                        "ref_id": "BIBREF31"
                    },
                    {
                        "start": 551,
                        "end": 568,
                        "text": "Liu et al., 2013;",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 569,
                        "end": 597,
                        "text": "Baldini Soares et al., 2019)",
                        "ref_id": "BIBREF1"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 175,
                        "end": 176,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The success of NRE models on current RE benchmarks makes us wonder which type of information these models actually grasp to help them extract correct relations. The analysis of this problem may indicate the nature of these models and reveal their remaining problems to be further explored. Generally, in a typical RE setting, there are two main sources of information in text that might help RE models classify relations: textual context and entity mentions (names).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "From human intuition, textual context should be the main source of information for RE. Researchers have reached a consensus that there exist interpretable patterns in textual context that express relational facts. For example, in Figure 1 , \"... be founded ... by ...\" is a pattern for the relation founded by. The early RE systems (Huffman, 1995; Califf and Mooney, 1997) formalize patterns into string templates and determine relations by matching these templates. The later neural models (Socher et al., 2012; Liu et al., 2013) prefer to encode patterns into distributed representations and then predict relations via representation matching. Compared with rigid string templates, distributed representations used in neural models are more generalized and perform better.",
                "cite_spans": [
                    {
                        "start": 332,
                        "end": 347,
                        "text": "(Huffman, 1995;",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 348,
                        "end": 372,
                        "text": "Califf and Mooney, 1997)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 491,
                        "end": 512,
                        "text": "(Socher et al., 2012;",
                        "ref_id": "BIBREF31"
                    },
                    {
                        "start": 513,
                        "end": 530,
                        "text": "Liu et al., 2013)",
                        "ref_id": "BIBREF17"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 237,
                        "end": 238,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Besides, entity mentions also provide much information for relation classification. As shown in Figure 1 , we can acquire the types of entities from their mentions, which could help to filter out those impossible relations. Besides, if these entities can be linked to KGs, models can introduce external knowledge from KGs to help RE (Zhang et al., 2019; Peters et al., 2019) . Moreover, for pre-trained language models, which are widely adopted for recent RE models, there may be knowledge about entities inherently stored in their parameters after pre-training (Petroni et al., 2019) .",
                "cite_spans": [
                    {
                        "start": 333,
                        "end": 353,
                        "text": "(Zhang et al., 2019;",
                        "ref_id": "BIBREF39"
                    },
                    {
                        "start": 354,
                        "end": 374,
                        "text": "Peters et al., 2019)",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 562,
                        "end": 584,
                        "text": "(Petroni et al., 2019)",
                        "ref_id": "BIBREF26"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 103,
                        "end": 104,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this paper, we carry out extensive experiments to study to what extent RE models rely on the two information sources. We find out that:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "(1) Both context and entity mentions are crucial for RE. As shown in our experiments, while context is the main source to support classification, entity mentions also provide critical information, most of which is the type information of entities.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "(2) Existing RE benchmarks may leak shallow cues via entity mentions, which contribute to the high performance of existing models. Our experiments show that models still can achieve high performance only given entity mentions as input, suggesting that there exist biased statistical cues from entity mentions in these datasets.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The above observations demonstrate how existing models work on RE datasets, and suggest a way to further improve RE models: we should enhance them via better understanding context and utilizing entity types, while preventing them from simply memorizing entities or exploiting biased cues in mentions. From these points, we investigate an entity-masked contrastive pre-training framework for RE. We use Wikidata to gather sentences that may express the same relations, and let the model learn which sentences are close and which are not in relational semantics by a contrastive objective. In this process, we randomly mask entity mentions to avoid being biased by them. We show its effectiveness across several settings and benchmarks, and suggest that better pre-training technique is a reliable direction towards better RE.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "To study which type of information affects existing neural RE models to make decisions, we first introduce some preliminaries of RE models and settings and then conduct pilot experiments as well as empirical analyses in this section.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Pilot Experiment and Analysis",
                "sec_num": "2"
            },
            {
                "text": "There are various NRE models proposed in previous work (refer to Section 5), and we select the following three representative neural models for our pilot experiments and analyses:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Models and Dataset",
                "sec_num": "2.1"
            },
            {
                "text": "CNN We use the convolutional neural networks described in Nguyen and Grishman (2015) and augment the inputs with part-of-speech, named entity recognition and position embeddings following Zhang et al. (2017) .",
                "cite_spans": [
                    {
                        "start": 58,
                        "end": 84,
                        "text": "Nguyen and Grishman (2015)",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 188,
                        "end": 207,
                        "text": "Zhang et al. (2017)",
                        "ref_id": "BIBREF38"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Models and Dataset",
                "sec_num": "2.1"
            },
            {
                "text": "BERT BERT is a pre-trained language model that has been widely used in NLP tasks. We use BERT for RE following Baldini Soares et al. (2019) . In short, we highlight entity mentions in sentences by special markers and use the concatenations of entity representations for classification.",
                "cite_spans": [
                    {
                        "start": 119,
                        "end": 139,
                        "text": "Soares et al. (2019)",
                        "ref_id": "BIBREF1"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Models and Dataset",
                "sec_num": "2.1"
            },
            {
                "text": "Matching the blanks (MTB) MTB (Baldini Soares et al., 2019) is an RE-oriented pre-trained model based on BERT. It is pre-trained by classifying whether two sentences mention the same entity pair with entity mentions randomly masked. It is fine-tuned for RE in the same way as BERT. Since it is not publicly released, we pre-train a BERT base version of MTB and give the details in Appendix A.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Models and Dataset",
                "sec_num": "2.1"
            },
            {
                "text": "There are also a number of public benchmarks for RE, and we select the largest supervised RE dataset TACRED (Zhang et al., 2017) in our pilot experiments. TACRED is a supervised RE dataset with 106, 264 instances and 42 relations, which also provides type annotations for each entity.",
                "cite_spans": [
                    {
                        "start": 108,
                        "end": 128,
                        "text": "(Zhang et al., 2017)",
                        "ref_id": "BIBREF38"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Models and Dataset",
                "sec_num": "2.1"
            },
            {
                "text": "Note that we use more models and datasets in our main experiments, of which we give detailed descriptions and analyses in Section 4.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Models and Dataset",
                "sec_num": "2.1"
            },
            {
                "text": "We use several input formats for RE, based on which we can observe the effects of context and entity mentions in controllable experiments. The following two formats are adopted by previous literature and are close to the real-world RE scenarios: Besides the above settings, we also adopt three synthetic settings to study how much information context or mentions contribute to RE respectively: Only Context (OnlyC) To analyze the contribution of textual context to RE, we replace all entity mentions with the special tokens [SUBJ] and [OBJ] . In this case, the information source of entity mentions is totally blocked. Only Mention (OnlyM) In this setting, we only provide entity mentions and discard all the other textual context for the input. Only Type (OnlyT) This is similar to OnlyM, except we only provide entity types in this case.",
                "cite_spans": [
                    {
                        "start": 535,
                        "end": 540,
                        "text": "[OBJ]",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Settings",
                "sec_num": "2.2"
            },
            {
                "text": "Context+Mention (C+M)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Settings",
                "sec_num": "2.2"
            },
            {
                "text": "Table 1 shows a detailed comparison across different input formats and models on TACRED. From the results we can see that:",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Result Analysis",
                "sec_num": "2.3"
            },
            {
                "text": "(1) Both textual context and entity mentions provide critical information to support relation classification, and the most useful information in entity mentions is type information. As shown in Table 1 , OnlyC, OnlyM and OnlyT suffer a significant performance drop compared to C+M and C+T, indicating that relying on only one source is not enough, and both context and entity mentions are necessary for correct prediction. Besides, we also observe that C+T achieves comparable results on TACRED with C+M for BERT and MTB. This demonstrates that most of the information provided by entity mentions is their type information. We also provide several case studies in Section 2.4, which further verify this conclusion.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 200,
                        "end": 201,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Result Analysis",
                "sec_num": "2.3"
            },
            {
                "text": "(2) There are superficial cues leaked by mentions in existing RE datasets, which may contribute to the high performance of RE models. We observe high performance on OnlyM with all three models on TACRED, and this phenomenon also exists in other datasets (see Table 5 ). We also take a deep look into the performance drop of OnlyC compared to C+M in Section 2.4, and find out that in some cases that models cannot well understand the context, they turn to rely on shallow heuristics from mentions. It inspires us to further improve models in extracting relations from context while preventing them from rote memorization of entity mentions.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 265,
                        "end": 266,
                        "text": "5",
                        "ref_id": "TABREF9"
                    }
                ],
                "eq_spans": [],
                "section": "Result Analysis",
                "sec_num": "2.3"
            },
            {
                "text": "We notice that CNN results are a little inconsistent with BERT and MTB: CNN on OnlyC is almost the same as OnlyM, and C+M is 5% lower than C+T. We believe that it is mainly due to the limited encoding power of CNN, which cannot fully utilize context and is more easily to overfit the shallow cues of entity mentions in the datasets. We use red and blue to highlight the subject and object entities.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Result Analysis",
                "sec_num": "2.3"
            },
            {
                "text": "To further understand how performance varies on different input formats, we carry out a thorough case study on TACRED. We choose to demonstrate the BERT examples here because BERT represents the state-of-the-art class of models and we have observed a similar result on MTB. First we compare C+M and C+T. We find out that C+M shares 95.7% correct predictions with C+T, and 68.1% wrong predictions of C+M are the same as C+T. It indicates that most information models take advantage of from entity mentions is their type information. We also list some of the unique errors of C+M and C+T in Table 2. C+M may be biased by the entity distributions in the training set. For the two examples in Table 2 , \"Washington\" is only involved in per:stateorprovince of residence and \"Bank of America Corp.\" is only involved in no relation in the training set, and this bias may cause the error. On the other hand, C+T may have difficulty to correctly understand the text without specific entity mentions. As shown in the example, after replacing mentions with their types, the model is confused by \"general assembly\" and fails to detect the relation between \"Ruben\" and \"Ruben van Assouw\". It suggests that entity mentions provide information other than types to help models understand the text.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 695,
                        "end": 696,
                        "text": "2",
                        "ref_id": "TABREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Case Study on TACRED",
                "sec_num": "2.4"
            },
            {
                "text": "We also study why OnlyC suffers such a significant drop compared to C+M. In Table 3 , we cluster all the unique wrong predictions made by OnlyC (compared to C+M) into three classes. \"Wrong\" represents sentences with clear patterns but misun-derstood by the model. \"No pattern\" means that after masking the entity mentions, it is hard to tell what relation it is even for humans. \"Confusing\" indicates that after masking the entities, the sentence becomes ambiguous (e.g., confusing cities and countries). As shown in Table 3 , in almost half (42%) of the unique wrong predictions of On-lyC, the sentence has a clear relational pattern but the model fails to extract it, which suggests that in C+M, the model may rely on shallow heuristics from entity mentions to correctly predict the sentences. In the rest cases, entity mentions indeed provide critical information for classification.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 82,
                        "end": 83,
                        "text": "3",
                        "ref_id": "TABREF6"
                    },
                    {
                        "start": 523,
                        "end": 524,
                        "text": "3",
                        "ref_id": "TABREF6"
                    }
                ],
                "eq_spans": [],
                "section": "Case Study on TACRED",
                "sec_num": "2.4"
            },
            {
                "text": "From the observations in Section 2, we know that both context and entity type information is beneficial for RE models. However, in some cases RE models cannot well understand the relational patterns in context and rely on the shallow cues of entity mentions for classification. In order to enhance the ability to grasp entity types and extract relational facts from context, we propose the entity-masked contrastive pre-training framework for RE. We start with the motivation and process of relational contrastive example generation, and then go through the pre-training objective details.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Contrastive Pre-training for RE",
                "sec_num": "3"
            },
            {
                "text": "We expect that by pre-training specifically towards RE, our model can be more effective at encoding relational representations from textual context and modeling entity types from mentions. To do so, we adopt the idea of contrastive learning (Hadsell et al., 2006) , which aims to learn representations by pulling \"neighbors\" together and pushing \"nonneighbors\" apart. After this, \"neighbor\" instances will have similar representations. So it is important to define \"neighbors\" in contrastive learning and we utilize the information from KGs to to that. Inspired by distant supervision (Mintz et al., 2009) , we assume that sentences with entity pairs sharing the same relation in KGs are \"neighbors\".",
                "cite_spans": [
                    {
                        "start": 241,
                        "end": 263,
                        "text": "(Hadsell et al., 2006)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 585,
                        "end": 605,
                        "text": "(Mintz et al., 2009)",
                        "ref_id": "BIBREF23"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Relational Contrastive Example Generation",
                "sec_num": "3.1"
            },
            {
                "text": "Formally, denote the KG we use as K, which is composed of relational facts. Denote two random sentences as X A and X B , which have entity mentions h A , t A and h B , t B respectively. We define X A and X B as \"neighbors\" if there is a relation r such that (h A , r, t A ) \u2208 K and (h B , r, t B ) \u2208 K. We take Wikidata as the KG since it can be easily linked to the Wikipedia corpus used for pretraining. When training, we first sample a relation r with respect to its proportion in the KG, and then sample a sentence pair (X A , X B ) linked to r. To learn contrastively, we randomly sample N sentences X i B , 1 \u2264 i \u2264 N so they can form N negative pairs with X A . The model needs to classify which sentence among all the postive and negative samples has the same relation with X A .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Relational Contrastive Example Generation",
                "sec_num": "3.1"
            },
            {
                "text": "To avoid memorizing entity mentions or extracting shallow features from them during pre-training, we randomly mask entity mentions with the special token [BLANK] . We use P BLANK to denote the ratio of replaced entities and set P BLANK = 0.7 following Baldini Soares et al. (2019) . Note that masking all mentions during pre-training is also not a good option since it will create a gap be-tween pre-training and fine-tuning and also block the pre-trained models from utilizing entity mention information (e.g., learning entity types).",
                "cite_spans": [
                    {
                        "start": 154,
                        "end": 161,
                        "text": "[BLANK]",
                        "ref_id": null
                    },
                    {
                        "start": 274,
                        "end": 280,
                        "text": "(2019)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Relational Contrastive Example Generation",
                "sec_num": "3.1"
            },
            {
                "text": "Take an example to understand our data generation process: In Figure 2 , there are two sentences \"SpaceX was founded in 2002 by Elon Musk\" and \"As the co-founder of Microsoft, Bill Gates ...\" where both (SpaceX, founded by, Elon Musk) and (Microsoft, founded by, Bill Gates) exist in the KG. We expect the two sentences to have similar representations reflecting the relation. On the other hand, for the other two sentences in the right part of the figure, since their entity pairs do not have the relation founded by, they are regarded as negative samples and are expected to have diverse representations from the left one. During pre-training, each entity mention has a probability of P BLANK to be masked.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 69,
                        "end": 70,
                        "text": "2",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Relational Contrastive Example Generation",
                "sec_num": "3.1"
            },
            {
                "text": "The main problem of the generation process is that the sentence may express no relation between the entities at all, or express the relation different from what we expect. For example, a sentence mentioning \"SpaceX\" and \"Elon Musk\" may express the relation founded by, CEO or CTO, or simply does not express any relation between them. An example could be \"Elon Musk answers reporters' questions on a SpaceX press conference\", which expresses no clear relation between the two. However, we argue that the noise problem is not critical for our pre-training framework: Our goal is to get relatively better representations towards RE compared to raw pre-trained models like BERT, rather than to directly train an RE model for downstream tasks, so noise in the data is acceptable. With the help of the generated relational contrastive examples, our model can learn to better grasp type information from mentions and extract relational semantics from textual context: (1) The paired two sentences, which mention different entity pairs but share the same relation, prompt the model to discover the connections between these entity mentions for the relation. Besides, the entity masking strategy can effectively avoid simply memorizing entities. This eventually encourages the model to exploit entity type information. (2) Our generation strategy provides a diverse set of textual context expressing the same relation to the model, which motivates the model to learn to extract the relational patterns from a variety of expressions.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Relational Contrastive Example Generation",
                "sec_num": "3.1"
            },
            {
                "text": "Compared with our model, MTB (Baldini Soares et al., 2019) takes a more strict rule which requires the two sampled sentences to share the same entity pair. While it reduces the noise, the model also samples data with less diversity and loses the chance to learn type information.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Relational Contrastive Example Generation",
                "sec_num": "3.1"
            },
            {
                "text": "In our contrastive pre-training, we use the same Transformer architecture (Vaswani et al., 2017) as BERT. Denote the Transformer encoder as ENC and the output at the position i as ENC i (\u2022). For the input format, we use special markers to highlight the entity mentions following Baldini Soares et al. (2019) . For example, for the sentence \"SpaceX was founded by Elon Musk.\", the input sequence is \"",
                "cite_spans": [
                    {
                        "start": 74,
                        "end": 96,
                        "text": "(Vaswani et al., 2017)",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 301,
                        "end": 307,
                        "text": "(2019)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training Objectives",
                "sec_num": "3.2"
            },
            {
                "text": "[CLS][E1] SpaceX [/E1] was founded by [E2] Elon Musk [/E2] . [SEP]\".",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training Objectives",
                "sec_num": "3.2"
            },
            {
                "text": "During the pre-training, we have two objectives: contrastive pre-training objective and masked language modeling objective.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training Objectives",
                "sec_num": "3.2"
            },
            {
                "text": "Pre-training Objective As shown in Figure 2 , given the positive sentence pair (x A , x B ), and negative sentence pairs",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 42,
                        "end": 43,
                        "text": "2",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Contrastive",
                "sec_num": null
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "(x A , x i B ), 1 \u2264 i \u2264 N , we first use the Transformer encoder to get relation-aware representation for x in {x A , x B } \u222a {x i B } N i=1 : x = ENC h (x) \u2295 ENC t (x),",
                        "eq_num": "(1)"
                    }
                ],
                "section": "Contrastive",
                "sec_num": null
            },
            {
                "text": "where h and t are the positions of special tokens [E1] and [E2], and \u2295 stands for concatenation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Contrastive",
                "sec_num": null
            },
            {
                "text": "With the sentence representation, we have the following training objective:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Contrastive",
                "sec_num": null
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "L CP = -log e x T A x B e x T A x B + i\u2264N i=1 e x T A x i B .",
                        "eq_num": "(2)"
                    }
                ],
                "section": "Contrastive",
                "sec_num": null
            },
            {
                "text": "By optimizing the model with respect to L CP , we expect representations for x A and x B to be closer and eventually sentences with similar relations will have similar representations. Masked Language Modeling Objective To maintain the ability of language understanding inherited from BERT and avoid catastrophic forgetting (McCloskey and Cohen, 1989) , we also adopt the masked language modeling (MLM) objective from BERT. MLM randomly masks tokens in the inputs and by letting the model predict the masked tokens, MLM learns contextual representation that contains rich semantic and syntactic knowledge. Denote the MLM loss as L M LM .",
                "cite_spans": [
                    {
                        "start": 324,
                        "end": 351,
                        "text": "(McCloskey and Cohen, 1989)",
                        "ref_id": "BIBREF21"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Contrastive",
                "sec_num": null
            },
            {
                "text": "Eventually, we have the following training loss:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Contrastive",
                "sec_num": null
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "L = L CP + L M LM .",
                        "eq_num": "(3)"
                    }
                ],
                "section": "Contrastive",
                "sec_num": null
            },
            {
                "text": "In this section, we explore the effectiveness of our relational contrastive pre-training across two typical RE tasks and several RE datasets.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiment",
                "sec_num": "4"
            },
            {
                "text": "For comprehensive experiments, we evaluate our models on various RE tasks and datasets. Supervised RE This is the most widely-adopted setting in RE, where there is a pre-defined relation set R and each sentence x in the dataset expresses one of the relations in R. In some benchmarks, there is a special relation named N/A or no relation, indicating that the sentence does not express any relation between the given entities, or their relation is not included in R.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "RE Tasks",
                "sec_num": "4.1"
            },
            {
                "text": "For supervised RE datasets, we use TACRED (Zhang et al., 2017 ), SemEval-2010 Task 8 (Hendrickx et al., 2009) , Wiki80 (Han et al., 2019) and ChemProt (Kringelum et al., 2016) . Table 4 shows the comparison between the datasets. Few-Shot RE Few-shot learning is a recently emerged topic to study how to train a model with only a handful of examples for new tasks. A typical setting for few-shot RE is N -way K-shot RE (Han et al., 2018) , where for each evaluation episode, N relation types, K examples for each type and several query examples (all belonging to one of the N relations) are sampled, and models are required to classify the queries based on given N \u00d7 K samples. We take FewRel (Han et al., 2018; Gao et al., 2019) as the dataset and list its statistics in Table 4 .",
                "cite_spans": [
                    {
                        "start": 42,
                        "end": 61,
                        "text": "(Zhang et al., 2017",
                        "ref_id": "BIBREF38"
                    },
                    {
                        "start": 62,
                        "end": 77,
                        "text": "), SemEval-2010",
                        "ref_id": null
                    },
                    {
                        "start": 78,
                        "end": 109,
                        "text": "Task 8 (Hendrickx et al., 2009)",
                        "ref_id": null
                    },
                    {
                        "start": 119,
                        "end": 137,
                        "text": "(Han et al., 2019)",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 151,
                        "end": 175,
                        "text": "(Kringelum et al., 2016)",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 418,
                        "end": 436,
                        "text": "(Han et al., 2018)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 692,
                        "end": 710,
                        "text": "(Han et al., 2018;",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 711,
                        "end": 728,
                        "text": "Gao et al., 2019)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 184,
                        "end": 185,
                        "text": "4",
                        "ref_id": "TABREF7"
                    },
                    {
                        "start": 777,
                        "end": 778,
                        "text": "4",
                        "ref_id": "TABREF7"
                    }
                ],
                "eq_spans": [],
                "section": "RE Tasks",
                "sec_num": "4.1"
            },
            {
                "text": "We use Prototypical Networks as in Snell et al. (2017) ; Han et al. (2018) and make a little change:",
                "cite_spans": [
                    {
                        "start": 35,
                        "end": 54,
                        "text": "Snell et al. (2017)",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 57,
                        "end": 74,
                        "text": "Han et al. (2018)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "RE Tasks",
                "sec_num": "4.1"
            },
            {
                "text": "(1) We take the representation as described in Section 3.2 instead of using [CLS]. (2) We use dot production instead of Euclidean distance to measure the similarities between instances. We find out that this method outperforms original Prototypical Networks in Han et al. (2018) by a large margin.",
                "cite_spans": [
                    {
                        "start": 261,
                        "end": 278,
                        "text": "Han et al. (2018)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "RE Tasks",
                "sec_num": "4.1"
            },
            {
                "text": "Besides BERT and MTB we have introduced in Section 2.1, we also evaluate our proposed contrastive pre-training framework for RE (CP). We write the detailed hyper-parameter settings of both the pre-training and fine-tuning process for all the models in Appendix A and B.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "RE Models",
                "sec_num": "4.2"
            },
            {
                "text": "Note that since MTB and CP use Wikidata for pre-training, and Wiki80 and FewRel are constructed based on Wikidata, we exclude all entity pairs in test sets of Wiki80 and FewRel from pretraining data to avoid test set leakage.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "RE Models",
                "sec_num": "4.2"
            },
            {
                "text": "Table 5 and 6 show a detailed comparison between BERT, MTB and our proposed contrastive pre-trained models. Both MTB and CP improve model performance across various settings and datasets, demonstrating the power of RE-oriented pre-training. Compared to MTB, CP has achieved even higher results, proving the effectiveness of our proposed contrastive pre-training framework. To be more specific, we observe that:",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "5",
                        "ref_id": "TABREF9"
                    }
                ],
                "eq_spans": [],
                "section": "Strength of Contrastive Pre-training",
                "sec_num": "4.3"
            },
            {
                "text": "(1) CP improves model performance on all C+M, OnlyC and OnlyM settings, indicating that our pretraining framework enhances models on both context understanding and type information extraction.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Strength of Contrastive Pre-training",
                "sec_num": "4.3"
            },
            {
                "text": "(2) The performance gain on C+M and OnlyC is universal, even for ChemProt and FewRel 2.0, which are from biomedical domain. Our models trained on Wikipedia perform well on biomedical datasets, suggesting that CP learns relational patterns that are effective across different domains.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Strength of Contrastive Pre-training",
                "sec_num": "4.3"
            },
            {
                "text": "(3) CP also shows a prominent improvement of OnlyM on TACRED, Wiki80 and FewRel 1.0, which are closely related to Wikipedia. It indicates that our model has a better ability to extract type information from mentions. Both promotions on context and mentions eventually lead to better RE results of CP (better C+M results).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Strength of Contrastive Pre-training",
                "sec_num": "4.3"
            },
            {
                "text": "(4) The performance gain made by our contrastive pre-training model is more significant on 1995; Califf and Mooney, 1997) , feature-based methods (Kambhatla, 2004; Zhou et al., 2005) , kernel-based methods (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005) , graphical models (Roth and Yih, 2002 , 2004 ), etc. Since Socher et al. (2012) propose to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015) . To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 2018; Gao et al., 2019) to learn to extract new relations by only a handful of samples.",
                "cite_spans": [
                    {
                        "start": 91,
                        "end": 96,
                        "text": "1995;",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 97,
                        "end": 121,
                        "text": "Califf and Mooney, 1997)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 146,
                        "end": 163,
                        "text": "(Kambhatla, 2004;",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 164,
                        "end": 182,
                        "text": "Zhou et al., 2005)",
                        "ref_id": "BIBREF40"
                    },
                    {
                        "start": 206,
                        "end": 234,
                        "text": "(Culotta and Sorensen, 2004;",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 235,
                        "end": 260,
                        "text": "Bunescu and Mooney, 2005)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 280,
                        "end": 299,
                        "text": "(Roth and Yih, 2002",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 300,
                        "end": 306,
                        "text": ", 2004",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 307,
                        "end": 341,
                        "text": "), etc. Since Socher et al. (2012)",
                        "ref_id": null
                    },
                    {
                        "start": 438,
                        "end": 456,
                        "text": "(Liu et al., 2013;",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 457,
                        "end": 475,
                        "text": "Zeng et al., 2014;",
                        "ref_id": "BIBREF36"
                    },
                    {
                        "start": 476,
                        "end": 497,
                        "text": "Zhang and Wang, 2015)",
                        "ref_id": "BIBREF37"
                    },
                    {
                        "start": 596,
                        "end": 616,
                        "text": "(Mintz et al., 2009;",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 617,
                        "end": 634,
                        "text": "Min et al., 2013;",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 635,
                        "end": 655,
                        "text": "Riedel et al., 2010;",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 656,
                        "end": 674,
                        "text": "Zeng et al., 2015;",
                        "ref_id": "BIBREF35"
                    },
                    {
                        "start": 675,
                        "end": 692,
                        "text": "Lin et al., 2016)",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 771,
                        "end": 789,
                        "text": "(Han et al., 2018;",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 790,
                        "end": 807,
                        "text": "Gao et al., 2019)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Strength of Contrastive Pre-training",
                "sec_num": "4.3"
            },
            {
                "text": "Pre-training for RE With the recent advance of pre-trained language models (Devlin et al., 2019) (Zhang et al., 2019; Peters et al., 2019; Liu et al., 2020) . We do not discuss this line of work here for their promotion comes from relational knowledge of external sources, while we focus on text itself in the paper.",
                "cite_spans": [
                    {
                        "start": 75,
                        "end": 96,
                        "text": "(Devlin et al., 2019)",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 97,
                        "end": 117,
                        "text": "(Zhang et al., 2019;",
                        "ref_id": "BIBREF39"
                    },
                    {
                        "start": 118,
                        "end": 138,
                        "text": "Peters et al., 2019;",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 139,
                        "end": 156,
                        "text": "Liu et al., 2020)",
                        "ref_id": "BIBREF18"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Strength of Contrastive Pre-training",
                "sec_num": "4.3"
            },
            {
                "text": "Analysis of RE Han et al. (2020) suggest to study how RE models learn from context and mentions. Alt et al. (2020) also point out that there may exist shallow cues in entity mentions. However, there have not been systematical analyses about the topic and to the best of our knowledge, we are the first one to thoroughly carry out these studies.",
                "cite_spans": [
                    {
                        "start": 12,
                        "end": 32,
                        "text": "RE Han et al. (2020)",
                        "ref_id": null
                    },
                    {
                        "start": 97,
                        "end": 114,
                        "text": "Alt et al. (2020)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Strength of Contrastive Pre-training",
                "sec_num": "4.3"
            },
            {
                "text": "In this paper, we thoroughly study how textual context and entity mentions affect RE models respectively. Experiments and case studies prove that (i) both context and entity mentions (mainly as type information) provide critical information for relation extraction, and (ii) existing RE datasets may leak superficial cues through entity mentions and models may not have the strong abilities to understand context as we expect. From these points, we propose an entity-masked contrastive pre-training framework for RE to better understand textual context and entity types, and experimental results prove the effectiveness of our method.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "6"
            },
            {
                "text": "In the future, we will continue to explore better RE pre-training techniques, especially with a focus on open relation extraction and relation discovery. These problems require models to encode good relational representation with limited or even zero annotations, and we believe that our pre-trained RE models will make a good impact in the area.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "6"
            },
            {
                "text": "https://github.com/thunlp/FewRel",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://en.wikipedia.org/wiki/F1_ score",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "This work is supported by the National Key Research and Development Program of China (No. 2018YFB1004503), the National Natural Science Foundation of China (NSFC No. 61532010) and Beijing Academy of Artificial Intelligence (BAAI). This work is also supported by the Pattern Recognition Center, WeChat AI, Tencent Inc. Gao is supported by 2019 Tencent Rhino-Bird Elite Training Program. Gao is also supported by Tsinghua University Initiative Scientific Research Program.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgments",
                "sec_num": null
            },
            {
                "text": "Pre-training Dataset We construct a dataset for pre-training following the method in the paper. We use Wikipedia articles as corpus and Wikidata (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014) as the knowledge graph. Firstly, We use anchors to link entity mentions in Wikipedia corpus with entities in Wikidata. Then, in order to link more unanchored entity mentions, we adopt spaCy 1 to find all possible entity mentions, and link them to entities in Wikidata via name matching. Finally, we get a pre-training dataset containing 744 relations and 867, 278 sentences. We release this dataset together with our source code at our GitHub repository 2 .We also use this dataset for MTB, which is slightly different from the original paper (Baldini Soares et al., 2019) . The original MTB takes all entity pairs into consideration, even if they do not have a relationship in Wikidata. Using the above dataset means that we filter out these entity pairs. We do this out of training efficiency, for those entity pairs that do not have a relation are likely to express little relational information, and thus contribute little to the pre-training.Data Sampling Strategy For MTB (Baldini Soares et al., 2019) , we follow the same sampling strategy as in the original paper. For pre-training our contrastive model, we regard sentences labeled with the same relation as a \"bag\". Any sentence pair whose sentences are in the same bag is treated as a positive pair and as a negative pair otherwise. So there will be a large amount of possible positive samples and negative samples. We dynamically sample positive pairs of a relation with respect to the number of sentences in the bag.Hyperparameters We use Huggingface's Transformers 3 to implement models for both pre-training and fine-tuning and use AdamW (Loshchilov and Hutter, 2019) N/2 negative pairs. For CP, batch size N means that a batch contains 2N sentences, which form N positive pairs. For negative samples, we pair the sentence in each pair with sentences in other pairs. We set hyperparameters according to results on supervised RE dataset TACRED (micro F 1 ). Table 7 shows hyperparameters for pre-training MTB and our contrastive model (CP). The batch size of our implemented MTB is different from that in Baldini Soares et al. (2019) , because in our experiments, MTB with a batch size of 256 performs better on TACRED than the batch size of 2048.Pre-training Efficiency MTB and our contrastive model have the same architecture as BERT BASE (Devlin et al., 2019) Multiple Trial Settings For all the results on supervised RE, we run each experiment 5 times using 5 different seeds (42, 43, 44, 45, 46) and select the median of 5 results as the final reported number.For few-shot RE, as the model varies little with different seeds and it is evaluated in a sampling manner, we just run one trial with 10000 evaluation episodes, which is large enough for the result to converge. We report accuracy (proportion of correct instances in all instances) for Wiki80 and FewRel, and micro F 1 9 for all the other datasets.",
                "cite_spans": [
                    {
                        "start": 145,
                        "end": 175,
                        "text": "(Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014)",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 719,
                        "end": 748,
                        "text": "(Baldini Soares et al., 2019)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 1154,
                        "end": 1183,
                        "text": "(Baldini Soares et al., 2019)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 1779,
                        "end": 1808,
                        "text": "(Loshchilov and Hutter, 2019)",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 2267,
                        "end": 2273,
                        "text": "(2019)",
                        "ref_id": null
                    },
                    {
                        "start": 2481,
                        "end": 2502,
                        "text": "(Devlin et al., 2019)",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 2620,
                        "end": 2624,
                        "text": "(42,",
                        "ref_id": null
                    },
                    {
                        "start": 2625,
                        "end": 2628,
                        "text": "43,",
                        "ref_id": null
                    },
                    {
                        "start": 2629,
                        "end": 2632,
                        "text": "44,",
                        "ref_id": null
                    },
                    {
                        "start": 2633,
                        "end": 2636,
                        "text": "45,",
                        "ref_id": null
                    },
                    {
                        "start": 2637,
                        "end": 2640,
                        "text": "46)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 2104,
                        "end": 2105,
                        "text": "7",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "A Pre-training Details",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "TACRED revisited: A thorough evaluation of the TACRED relation extraction task",
                "authors": [
                    {
                        "first": "Christoph",
                        "middle": [],
                        "last": "Alt",
                        "suffix": ""
                    },
                    {
                        "first": "Aleksandra",
                        "middle": [],
                        "last": "Gabryszak",
                        "suffix": ""
                    },
                    {
                        "first": "Leonhard",
                        "middle": [],
                        "last": "Hennig",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of ACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Christoph Alt, Aleksandra Gabryszak, and Leonhard Hennig. 2020. TACRED revisited: A thorough eval- uation of the TACRED relation extraction task. In Proceedings of ACL.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Matching the blanks: Distributional similarity for relation learning",
                "authors": [
                    {
                        "first": "Baldini",
                        "middle": [],
                        "last": "Livio",
                        "suffix": ""
                    },
                    {
                        "first": "Nicholas",
                        "middle": [],
                        "last": "Soares",
                        "suffix": ""
                    },
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Fitzgerald",
                        "suffix": ""
                    },
                    {
                        "first": "Tom",
                        "middle": [],
                        "last": "Ling",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Kwiatkowski",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of ACL",
                "volume": "",
                "issue": "",
                "pages": "2895--2905",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/P19-1279"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Livio Baldini Soares, Nicholas FitzGerald, Jeffrey Ling, and Tom Kwiatkowski. 2019. Matching the blanks: Distributional similarity for relation learn- ing. In Proceedings of ACL, pages 2895-2905.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Question answering with subgraph embeddings",
                "authors": [
                    {
                        "first": "Antoine",
                        "middle": [],
                        "last": "Bordes",
                        "suffix": ""
                    },
                    {
                        "first": "Sumit",
                        "middle": [],
                        "last": "Chopra",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Weston",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of EMNLP",
                "volume": "",
                "issue": "",
                "pages": "615--620",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Antoine Bordes, Sumit Chopra, and Jason Weston. 2014. Question answering with subgraph embed- dings. In Proceedings of EMNLP, pages 615-620.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "A shortest path dependency kernel for relation extraction",
                "authors": [
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Razvan",
                        "suffix": ""
                    },
                    {
                        "first": "Raymond",
                        "middle": [
                            "J"
                        ],
                        "last": "Bunescu",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Mooney",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of EMNLP",
                "volume": "",
                "issue": "",
                "pages": "724--731",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Razvan C Bunescu and Raymond J Mooney. 2005. A shortest path dependency kernel for relation extrac- tion. In Proceedings of EMNLP, pages 724-731.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Relational learning of pattern-match rules for information extraction",
                "authors": [
                    {
                        "first": "Mary",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Elaine",
                        "middle": [],
                        "last": "Califf",
                        "suffix": ""
                    },
                    {
                        "first": "Raymond",
                        "middle": [
                            "J"
                        ],
                        "last": "Mooney",
                        "suffix": ""
                    }
                ],
                "year": 1997,
                "venue": "Proceedings of CoNLL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mary Elaine Califf and Raymond J. Mooney. 1997. Re- lational learning of pattern-match rules for informa- tion extraction. In Proceedings of CoNLL.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Dependency tree kernels for relation extraction",
                "authors": [
                    {
                        "first": "Aron",
                        "middle": [],
                        "last": "Culotta",
                        "suffix": ""
                    },
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Sorensen",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proceedings of ACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Aron Culotta and Jeffrey Sorensen. 2004. Dependency tree kernels for relation extraction. In Proceedings of ACL, page 423.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
                "authors": [
                    {
                        "first": "Jacob",
                        "middle": [],
                        "last": "Devlin",
                        "suffix": ""
                    },
                    {
                        "first": "Ming-Wei",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "Kenton",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Kristina",
                        "middle": [],
                        "last": "Toutanova",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of NAACL-HLT",
                "volume": "",
                "issue": "",
                "pages": "4171--4186",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of NAACL-HLT, pages 4171-4186.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "FewRel 2.0: Towards more challenging few-shot relation classification",
                "authors": [
                    {
                        "first": "Tianyu",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Xu",
                        "middle": [],
                        "last": "Han",
                        "suffix": ""
                    },
                    {
                        "first": "Hao",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    },
                    {
                        "first": "Zhiyuan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Peng",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Maosong",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Jie",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of EMNLP-IJCNLP",
                "volume": "",
                "issue": "",
                "pages": "6251--6256",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/D19-1649"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Tianyu Gao, Xu Han, Hao Zhu, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie Zhou. 2019. FewRel 2.0: Towards more challenging few-shot relation classifi- cation. In Proceedings of EMNLP-IJCNLP, pages 6251-6256.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Dimensionality reduction by learning an invariant mapping",
                "authors": [
                    {
                        "first": "Raia",
                        "middle": [],
                        "last": "Hadsell",
                        "suffix": ""
                    },
                    {
                        "first": "Sumit",
                        "middle": [],
                        "last": "Chopra",
                        "suffix": ""
                    },
                    {
                        "first": "Yann",
                        "middle": [],
                        "last": "Lecun",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)",
                "volume": "2",
                "issue": "",
                "pages": "1735--1742",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Raia Hadsell, Sumit Chopra, and Yann LeCun. 2006. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Confer- ence on Computer Vision and Pattern Recognition (CVPR'06), volume 2, pages 1735-1742. IEEE.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "More data, more relations, more context and more openness: A review and outlook for relation extraction",
                "authors": [
                    {
                        "first": "Xu",
                        "middle": [],
                        "last": "Han",
                        "suffix": ""
                    },
                    {
                        "first": "Tianyu",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Yankai",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Hao",
                        "middle": [],
                        "last": "Peng",
                        "suffix": ""
                    },
                    {
                        "first": "Yaoliang",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Chaojun",
                        "middle": [],
                        "last": "Xiao",
                        "suffix": ""
                    },
                    {
                        "first": "Zhiyuan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Peng",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Maosong",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Jie",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xu Han, Tianyu Gao, Yankai Lin, Hao Peng, Yao- liang Yang, Chaojun Xiao, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie Zhou. 2020. More data, more relations, more context and more openness: A re- view and outlook for relation extraction.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "OpenNRE: An open and extensible toolkit for neural relation extraction",
                "authors": [
                    {
                        "first": "Xu",
                        "middle": [],
                        "last": "Han",
                        "suffix": ""
                    },
                    {
                        "first": "Tianyu",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Yuan",
                        "middle": [],
                        "last": "Yao",
                        "suffix": ""
                    },
                    {
                        "first": "Deming",
                        "middle": [],
                        "last": "Ye",
                        "suffix": ""
                    },
                    {
                        "first": "Zhiyuan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Maosong",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of EMNLP-IJCNLP: System Demonstrations",
                "volume": "",
                "issue": "",
                "pages": "169--174",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/D19-3029"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Xu Han, Tianyu Gao, Yuan Yao, Deming Ye, Zhiyuan Liu, and Maosong Sun. 2019. OpenNRE: An open and extensible toolkit for neural relation extraction. In Proceedings of EMNLP-IJCNLP: System Demon- strations, pages 169-174.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "FewRel: A large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation",
                "authors": [
                    {
                        "first": "Xu",
                        "middle": [],
                        "last": "Han",
                        "suffix": ""
                    },
                    {
                        "first": "Hao",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    },
                    {
                        "first": "Pengfei",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Ziyun",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Yuan",
                        "middle": [],
                        "last": "Yao",
                        "suffix": ""
                    },
                    {
                        "first": "Zhiyuan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Maosong",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of EMNLP",
                "volume": "",
                "issue": "",
                "pages": "4803--4809",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan Yao, Zhiyuan Liu, and Maosong Sun. 2018. FewRel: A large-scale supervised few-shot relation classifica- tion dataset with state-of-the-art evaluation. In Pro- ceedings of EMNLP, pages 4803-4809.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "SemEval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals",
                "authors": [
                    {
                        "first": "Iris",
                        "middle": [],
                        "last": "Hendrickx",
                        "suffix": ""
                    },
                    {
                        "first": "Nam",
                        "middle": [],
                        "last": "Su",
                        "suffix": ""
                    },
                    {
                        "first": "Zornitsa",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "Preslav",
                        "middle": [],
                        "last": "Kozareva",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Nakov",
                        "suffix": ""
                    },
                    {
                        "first": "\u00d3",
                        "middle": [],
                        "last": "Diarmuid",
                        "suffix": ""
                    },
                    {
                        "first": "Sebastian",
                        "middle": [],
                        "last": "S\u00e9aghdha",
                        "suffix": ""
                    },
                    {
                        "first": "Marco",
                        "middle": [],
                        "last": "Pad\u00f3",
                        "suffix": ""
                    },
                    {
                        "first": "Lorenza",
                        "middle": [],
                        "last": "Pennacchiotti",
                        "suffix": ""
                    },
                    {
                        "first": "Stan",
                        "middle": [],
                        "last": "Romano",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Szpakowicz",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions",
                "volume": "",
                "issue": "",
                "pages": "94--99",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid \u00d3 S\u00e9aghdha, Sebastian Pad\u00f3, Marco Pennacchiotti, Lorenza Romano, and Stan Szpakowicz. 2009. SemEval-2010 task 8: Multi-way classification of semantic relations be- tween pairs of nominals. In Proceedings of the Workshop on Semantic Evaluations: Recent Achieve- ments and Future Directions (SEW-2009), pages 94- 99.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Learning information extraction patterns from examples",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Scott B Huffman",
                        "suffix": ""
                    }
                ],
                "year": 1995,
                "venue": "Proceedings of IJ-CAI",
                "volume": "",
                "issue": "",
                "pages": "246--260",
                "other_ids": {
                    "DOI": [
                        "10.1007/3-540-60925-3_51"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Scott B Huffman. 1995. Learning information extrac- tion patterns from examples. In Proceedings of IJ- CAI, pages 246-260.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Combining lexical, syntactic, and semantic features with maximum entropy models for extracting relations",
                "authors": [
                    {
                        "first": "Nanda",
                        "middle": [],
                        "last": "Kambhatla",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proceedings of ACL",
                "volume": "",
                "issue": "",
                "pages": "178--181",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Nanda Kambhatla. 2004. Combining lexical, syntactic, and semantic features with maximum entropy mod- els for extracting relations. In Proceedings of ACL, pages 178-181.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "ChemProt-3.0: A global chemical biology diseases mapping",
                "authors": [
                    {
                        "first": "Jens",
                        "middle": [],
                        "last": "Kringelum",
                        "suffix": ""
                    },
                    {
                        "first": "Sonny",
                        "middle": [],
                        "last": "Kim Kjaerulff",
                        "suffix": ""
                    },
                    {
                        "first": "S\u00f8ren",
                        "middle": [],
                        "last": "Brunak",
                        "suffix": ""
                    },
                    {
                        "first": "Ole",
                        "middle": [],
                        "last": "Lund",
                        "suffix": ""
                    },
                    {
                        "first": "Tudor",
                        "middle": [
                            "I"
                        ],
                        "last": "Oprea",
                        "suffix": ""
                    },
                    {
                        "first": "Olivier",
                        "middle": [],
                        "last": "Taboureau",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jens Kringelum, Sonny Kim Kjaerulff, S\u00f8ren Brunak, Ole Lund, Tudor I Oprea, and Olivier Taboureau. 2016. ChemProt-3.0: A global chemical biology dis- eases mapping. Database, 2016.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Neural relation extraction with selective attention over instances",
                "authors": [
                    {
                        "first": "Yankai",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Shiqi",
                        "middle": [],
                        "last": "Shen",
                        "suffix": ""
                    },
                    {
                        "first": "Zhiyuan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Huanbo",
                        "middle": [],
                        "last": "Luan",
                        "suffix": ""
                    },
                    {
                        "first": "Maosong",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of ACL",
                "volume": "",
                "issue": "",
                "pages": "2124--2133",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan, and Maosong Sun. 2016. Neural relation extraction with selective attention over instances. In Proceed- ings of ACL, pages 2124-2133.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Convolution neural network for relation extraction",
                "authors": [
                    {
                        "first": "Chunyang",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Wenbo",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Wenhan",
                        "middle": [],
                        "last": "Chao",
                        "suffix": ""
                    },
                    {
                        "first": "Wanxiang",
                        "middle": [],
                        "last": "Che",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of ICDM",
                "volume": "",
                "issue": "",
                "pages": "231--242",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chunyang Liu, Wenbo Sun, Wenhan Chao, and Wanx- iang Che. 2013. Convolution neural network for re- lation extraction. In Proceedings of ICDM, pages 231-242.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "K-BERT: Enabling language representation with knowledge graph",
                "authors": [
                    {
                        "first": "Weijie",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Peng",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Zhe",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Zhiruo",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Qi",
                        "middle": [],
                        "last": "Ju",
                        "suffix": ""
                    },
                    {
                        "first": "Haotang",
                        "middle": [],
                        "last": "Deng",
                        "suffix": ""
                    },
                    {
                        "first": "Ping",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of AAAI",
                "volume": "",
                "issue": "",
                "pages": "2901--2908",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Weijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju, Haotang Deng, and Ping Wang. 2020. K-BERT: Enabling language representation with knowledge graph. In Proceedings of AAAI, pages 2901-2908.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Decoupled weight decay regularization",
                "authors": [
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Loshchilov",
                        "suffix": ""
                    },
                    {
                        "first": "Frank",
                        "middle": [],
                        "last": "Hutter",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of ICLR 2019",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In Proceedings of ICLR 2019.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Mem2Seq: Effectively incorporating knowledge bases into end-to-end task-oriented dialog systems",
                "authors": [
                    {
                        "first": "Andrea",
                        "middle": [],
                        "last": "Madotto",
                        "suffix": ""
                    },
                    {
                        "first": "Chien-Sheng",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Pascale",
                        "middle": [],
                        "last": "Fung",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of ACL",
                "volume": "",
                "issue": "",
                "pages": "1468--1478",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/P18-1136"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Andrea Madotto, Chien-Sheng Wu, and Pascale Fung. 2018. Mem2Seq: Effectively incorporating knowl- edge bases into end-to-end task-oriented dialog sys- tems. In Proceedings of ACL, pages 1468-1478.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Catastrophic interference in connectionist networks: The sequential learning problem",
                "authors": [
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Mccloskey",
                        "suffix": ""
                    },
                    {
                        "first": "Neal",
                        "middle": [
                            "J"
                        ],
                        "last": "Cohen",
                        "suffix": ""
                    }
                ],
                "year": 1989,
                "venue": "Psychology of learning and motivation",
                "volume": "24",
                "issue": "",
                "pages": "109--165",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Michael McCloskey and Neal J Cohen. 1989. Catas- trophic interference in connectionist networks: The sequential learning problem. In Psychology of learn- ing and motivation, volume 24, pages 109-165. El- sevier.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Distant supervision for relation extraction with an incomplete knowledge base",
                "authors": [
                    {
                        "first": "Bonan",
                        "middle": [],
                        "last": "Min",
                        "suffix": ""
                    },
                    {
                        "first": "Ralph",
                        "middle": [],
                        "last": "Grishman",
                        "suffix": ""
                    },
                    {
                        "first": "Li",
                        "middle": [],
                        "last": "Wan",
                        "suffix": ""
                    },
                    {
                        "first": "Chang",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Gondek",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of NAACL",
                "volume": "",
                "issue": "",
                "pages": "777--782",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Bonan Min, Ralph Grishman, Li Wan, Chang Wang, and David Gondek. 2013. Distant supervision for re- lation extraction with an incomplete knowledge base. In Proceedings of NAACL, pages 777-782.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Distant supervision for relation extraction without labeled data",
                "authors": [
                    {
                        "first": "Mike",
                        "middle": [],
                        "last": "Mintz",
                        "suffix": ""
                    },
                    {
                        "first": "Steven",
                        "middle": [],
                        "last": "Bills",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of ACL-IJCNLP",
                "volume": "",
                "issue": "",
                "pages": "1003--1011",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf- sky. 2009. Distant supervision for relation extrac- tion without labeled data. In Proceedings of ACL- IJCNLP, pages 1003-1011.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Relation extraction: Perspective from convolutional neural networks",
                "authors": [
                    {
                        "first": "Huu",
                        "middle": [],
                        "last": "Thien",
                        "suffix": ""
                    },
                    {
                        "first": "Ralph",
                        "middle": [],
                        "last": "Nguyen",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Grishman",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the NAACL Workshop on Vector Space Modeling for NLP",
                "volume": "",
                "issue": "",
                "pages": "39--48",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Thien Huu Nguyen and Ralph Grishman. 2015. Rela- tion extraction: Perspective from convolutional neu- ral networks. In Proceedings of the NAACL Work- shop on Vector Space Modeling for NLP, pages 39- 48.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Knowledge enhanced contextual word representations",
                "authors": [
                    {
                        "first": "Matthew",
                        "middle": [
                            "E"
                        ],
                        "last": "Peters",
                        "suffix": ""
                    },
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Neumann",
                        "suffix": ""
                    },
                    {
                        "first": "Robert",
                        "middle": [],
                        "last": "Logan",
                        "suffix": ""
                    },
                    {
                        "first": "Roy",
                        "middle": [],
                        "last": "Schwartz",
                        "suffix": ""
                    },
                    {
                        "first": "Vidur",
                        "middle": [],
                        "last": "Joshi",
                        "suffix": ""
                    },
                    {
                        "first": "Sameer",
                        "middle": [],
                        "last": "Singh",
                        "suffix": ""
                    },
                    {
                        "first": "Noah",
                        "middle": [
                            "A"
                        ],
                        "last": "Smith",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of EMNLP-IJCNLP",
                "volume": "",
                "issue": "",
                "pages": "43--54",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/D19-1005"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Matthew E. Peters, Mark Neumann, Robert Logan, Roy Schwartz, Vidur Joshi, Sameer Singh, and Noah A. Smith. 2019. Knowledge enhanced con- textual word representations. In Proceedings of EMNLP-IJCNLP, pages 43-54.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Language models as knowledge bases?",
                "authors": [
                    {
                        "first": "Fabio",
                        "middle": [],
                        "last": "Petroni",
                        "suffix": ""
                    },
                    {
                        "first": "Tim",
                        "middle": [],
                        "last": "Rockt\u00e4schel",
                        "suffix": ""
                    },
                    {
                        "first": "Sebastian",
                        "middle": [],
                        "last": "Riedel",
                        "suffix": ""
                    },
                    {
                        "first": "Patrick",
                        "middle": [],
                        "last": "Lewis",
                        "suffix": ""
                    },
                    {
                        "first": "Anton",
                        "middle": [],
                        "last": "Bakhtin",
                        "suffix": ""
                    },
                    {
                        "first": "Yuxiang",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Alexander",
                        "middle": [],
                        "last": "Miller",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of EMNLP-IJCNLP",
                "volume": "",
                "issue": "",
                "pages": "2463--2473",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/D19-1250"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Fabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. Language models as knowl- edge bases? In Proceedings of EMNLP-IJCNLP, pages 2463-2473.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Modeling relations and their mentions without labeled text",
                "authors": [
                    {
                        "first": "Sebastian",
                        "middle": [],
                        "last": "Riedel",
                        "suffix": ""
                    },
                    {
                        "first": "Limin",
                        "middle": [],
                        "last": "Yao",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Mccallum",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of ECML-PKDD",
                "volume": "",
                "issue": "",
                "pages": "148--163",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling relations and their mentions with- out labeled text. In Proceedings of ECML-PKDD, pages 148-163.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Probabilistic reasoning for entity & relation recognition",
                "authors": [
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Roth",
                        "suffix": ""
                    },
                    {
                        "first": "Wen-Tau",
                        "middle": [],
                        "last": "Yih",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Proceedings of COLING",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dan Roth and Wen-tau Yih. 2002. Probabilistic reason- ing for entity & relation recognition. In Proceedings of COLING.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "A linear programming formulation for global inference in natural language tasks",
                "authors": [
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Roth",
                        "suffix": ""
                    },
                    {
                        "first": "Wen-Tau",
                        "middle": [],
                        "last": "Yih",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proceedings of CoNLL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dan Roth and Wen-tau Yih. 2004. A linear program- ming formulation for global inference in natural lan- guage tasks. In Proceedings of CoNLL.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "Prototypical networks for few-shot learning",
                "authors": [
                    {
                        "first": "Jake",
                        "middle": [],
                        "last": "Snell",
                        "suffix": ""
                    },
                    {
                        "first": "Kevin",
                        "middle": [],
                        "last": "Swersky",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Zemel",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of NIPS",
                "volume": "",
                "issue": "",
                "pages": "4077--4087",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jake Snell, Kevin Swersky, and Richard Zemel. 2017. Prototypical networks for few-shot learning. In Pro- ceedings of NIPS, pages 4077-4087.",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "Semantic compositionality through recursive matrix-vector spaces",
                "authors": [
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    },
                    {
                        "first": "Brody",
                        "middle": [],
                        "last": "Huval",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [
                            "Y"
                        ],
                        "last": "Ng",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Proceedings of EMNLP",
                "volume": "",
                "issue": "",
                "pages": "1201--1211",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Richard Socher, Brody Huval, Christopher D Manning, and Andrew Y Ng. 2012. Semantic compositional- ity through recursive matrix-vector spaces. In Pro- ceedings of EMNLP, pages 1201-1211.",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "Attention is all you need",
                "authors": [
                    {
                        "first": "Ashish",
                        "middle": [],
                        "last": "Vaswani",
                        "suffix": ""
                    },
                    {
                        "first": "Noam",
                        "middle": [],
                        "last": "Shazeer",
                        "suffix": ""
                    },
                    {
                        "first": "Niki",
                        "middle": [],
                        "last": "Parmar",
                        "suffix": ""
                    },
                    {
                        "first": "Jakob",
                        "middle": [],
                        "last": "Uszkoreit",
                        "suffix": ""
                    },
                    {
                        "first": "Llion",
                        "middle": [],
                        "last": "Jones",
                        "suffix": ""
                    },
                    {
                        "first": "Aidan",
                        "middle": [
                            "N"
                        ],
                        "last": "Gomez",
                        "suffix": ""
                    },
                    {
                        "first": "\u0141ukasz",
                        "middle": [],
                        "last": "Kaiser",
                        "suffix": ""
                    },
                    {
                        "first": "Illia",
                        "middle": [],
                        "last": "Polosukhin",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of NIPS",
                "volume": "",
                "issue": "",
                "pages": "5998--6008",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proceedings of NIPS, pages 5998- 6008.",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "Wikidata: A free collaborative knowledgebase",
                "authors": [
                    {
                        "first": "Denny",
                        "middle": [],
                        "last": "Vrande\u010di\u0107",
                        "suffix": ""
                    },
                    {
                        "first": "Markus",
                        "middle": [],
                        "last": "Kr\u00f6tzsch",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of CACM",
                "volume": "57",
                "issue": "10",
                "pages": "78--85",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Denny Vrande\u010di\u0107 and Markus Kr\u00f6tzsch. 2014. Wiki- data: A free collaborative knowledgebase. Proceed- ings of CACM, 57(10):78-85.",
                "links": null
            },
            "BIBREF34": {
                "ref_id": "b34",
                "title": "Explicit semantic ranking for academic search via knowledge graph embedding",
                "authors": [
                    {
                        "first": "Chenyan",
                        "middle": [],
                        "last": "Xiong",
                        "suffix": ""
                    },
                    {
                        "first": "Russell",
                        "middle": [],
                        "last": "Power",
                        "suffix": ""
                    },
                    {
                        "first": "Jamie",
                        "middle": [],
                        "last": "Callan",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of WWW",
                "volume": "",
                "issue": "",
                "pages": "1271--1279",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chenyan Xiong, Russell Power, and Jamie Callan. 2017. Explicit semantic ranking for academic search via knowledge graph embedding. In Proceed- ings of WWW, pages 1271-1279.",
                "links": null
            },
            "BIBREF35": {
                "ref_id": "b35",
                "title": "Distant supervision for relation extraction via piecewise convolutional neural networks",
                "authors": [
                    {
                        "first": "Daojian",
                        "middle": [],
                        "last": "Zeng",
                        "suffix": ""
                    },
                    {
                        "first": "Kang",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Yubo",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Jun",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of EMNLP",
                "volume": "",
                "issue": "",
                "pages": "1753--1762",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao. 2015. Distant supervision for relation extraction via piecewise convolutional neural networks. In Pro- ceedings of EMNLP, pages 1753-1762.",
                "links": null
            },
            "BIBREF36": {
                "ref_id": "b36",
                "title": "Relation classification via convolutional deep neural network",
                "authors": [
                    {
                        "first": "Daojian",
                        "middle": [],
                        "last": "Zeng",
                        "suffix": ""
                    },
                    {
                        "first": "Kang",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Siwei",
                        "middle": [],
                        "last": "Lai",
                        "suffix": ""
                    },
                    {
                        "first": "Guangyou",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Jun",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of COLING",
                "volume": "",
                "issue": "",
                "pages": "2335--2344",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou, and Jun Zhao. 2014. Relation classification via con- volutional deep neural network. In Proceedings of COLING, pages 2335-2344.",
                "links": null
            },
            "BIBREF37": {
                "ref_id": "b37",
                "title": "Relation classification via recurrent neural network",
                "authors": [
                    {
                        "first": "Dongxu",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Dong",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1508.01006"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Dongxu Zhang and Dong Wang. 2015. Relation classi- fication via recurrent neural network. arXiv preprint arXiv:1508.01006.",
                "links": null
            },
            "BIBREF38": {
                "ref_id": "b38",
                "title": "Positionaware attention and supervised data improve slot filling",
                "authors": [
                    {
                        "first": "Yuhao",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Victor",
                        "middle": [],
                        "last": "Zhong",
                        "suffix": ""
                    },
                    {
                        "first": "Danqi",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Gabor",
                        "middle": [],
                        "last": "Angeli",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of EMNLP",
                "volume": "",
                "issue": "",
                "pages": "35--45",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yuhao Zhang, Victor Zhong, Danqi Chen, Gabor An- geli, and Christopher D Manning. 2017. Position- aware attention and supervised data improve slot fill- ing. In Proceedings of EMNLP, pages 35-45.",
                "links": null
            },
            "BIBREF39": {
                "ref_id": "b39",
                "title": "ERNIE: Enhanced language representation with informative entities",
                "authors": [
                    {
                        "first": "Zhengyan",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Xu",
                        "middle": [],
                        "last": "Han",
                        "suffix": ""
                    },
                    {
                        "first": "Zhiyuan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Xin",
                        "middle": [],
                        "last": "Jiang",
                        "suffix": ""
                    },
                    {
                        "first": "Maosong",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Qun",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of ACL",
                "volume": "",
                "issue": "",
                "pages": "1441--1451",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/P19-1139"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. 2019. ERNIE: En- hanced language representation with informative en- tities. In Proceedings of ACL, pages 1441-1451.",
                "links": null
            },
            "BIBREF40": {
                "ref_id": "b40",
                "title": "Exploring various knowledge in relation extraction",
                "authors": [
                    {
                        "first": "Guodong",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Jian",
                        "middle": [],
                        "last": "Su",
                        "suffix": ""
                    },
                    {
                        "first": "Jie",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Min",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of ACL",
                "volume": "",
                "issue": "",
                "pages": "427--434",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Guodong Zhou, Jian Su, Jie Zhang, and Min Zhang. 2005. Exploring various knowledge in relation ex- traction. In Proceedings of ACL, pages 427-434.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 2: Our contrastive pre-training framework for RE. We assign relations to sentences by linking entity pairs in sentences to Wikidata and checking their relations in the KG. We assume that sentences with the same relation should have similar representations, and those with different relations should be pushed apart. Entity mentions are randomly masked (boxes with colored background) to avoid simple memorization. Compared to MTB (in the dotted box), our method samples data with better diversity, which can not only increase the coverage of entity types and diverse context but also reduce the possibility of memorizing entity names.",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table><tr><td colspan=\"2\">founded by</td></tr><tr><td>Type: organization</td><td>Type: person</td></tr><tr><td>ID: Q193701</td><td>ID: Q317521</td></tr><tr><td>Other info:</td><td>Other info:</td></tr><tr><td>country: US</td><td>citizenship: US</td></tr><tr><td>product: Falcon</td><td>occupation: entrepreneur</td></tr><tr><td>\u2026</td><td>\u2026</td></tr></table>",
                "type_str": "table",
                "text": "and search SpaceX was founded in 2002 by Elon Musk .",
                "html": null,
                "num": null
            },
            "TABREF1": {
                "content": "<table><tr><td colspan=\"5\">Model C+M C+T OnlyC OnlyM OnlyT</td></tr><tr><td>CNN</td><td>0.547 0.591</td><td>0.441</td><td>0.434</td><td>0.295</td></tr><tr><td>BERT</td><td>0.683 0.686</td><td>0.570</td><td>0.466</td><td>0.277</td></tr><tr><td>MTB</td><td>0.691 0.696</td><td>0.581</td><td>0.433</td><td>0.304</td></tr><tr><td colspan=\"5\">Table 1: TACRED results (micro F 1 ) with CNN, BERT</td></tr><tr><td colspan=\"3\">and MTB on different settings.</td><td/><td/></tr><tr><td colspan=\"5\">(with both context and highlighted entity men-</td></tr><tr><td colspan=\"5\">tions) is provided. To let the models know where</td></tr><tr><td colspan=\"5\">the entity mentions are, we use position embed-</td></tr><tr><td colspan=\"5\">dings (Zeng et al., 2014) for the CNN model and</td></tr><tr><td colspan=\"5\">special entity markers (Zhang et al., 2019; Bal-</td></tr><tr><td colspan=\"5\">dini Soares et al., 2019) for the pre-trained BERT.</td></tr><tr><td colspan=\"5\">Context+Type (C+T) We replace entity men-</td></tr><tr><td colspan=\"5\">tions with their types provided in TACRED. We</td></tr><tr><td colspan=\"5\">use special tokens to represent them: for example,</td></tr><tr><td colspan=\"5\">we use [person] and [date] to represent an</td></tr><tr><td colspan=\"5\">entity with type person and date respectively.</td></tr><tr><td colspan=\"5\">Different from Zhang et al. (2017), we do not re-</td></tr><tr><td colspan=\"5\">peat the special tokens for entity-length times to</td></tr><tr><td colspan=\"4\">avoid leaking entity length information.</td><td/></tr></table>",
                "type_str": "table",
                "text": "This is the most widely-used RE setting, where the whole sentence",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Although her family was from Arkansas, she was born in Washington state, where ... Label:",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table><tr><td>Label: org:parents</td></tr><tr><td>Prediction: no relation</td></tr><tr><td>C+T</td></tr><tr><td>First, Label: no relation</td></tr><tr><td>Prediction: per:children</td></tr><tr><td>Label: no relation</td></tr><tr><td>Prediction: org:members</td></tr></table>",
                "type_str": "table",
                "text": "Dozens of lightly regulated subprime lenders, including New Century Financial Corp., have failed and troubled Countrywide Financial Corp. was acquired by Bank of America Corp. Natalie Hagemo says, she fought the Church of Scientology just to give birth to her daughter. Earlier this week Jakarta hosted the general assembly of the Organisation of Asia-Pacific News Agencies, ... The boy, identified by the Dutch foreign ministry as Ruben but more fully by Dutch media as Ruben van Assouw, ...",
                "html": null,
                "num": null
            },
            "TABREF4": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Wrong predictions made only by C+M and only by C+T, where red and blue represent subject and object entities respectively. As the examples suggest, C+M is more easily biased by the entity distribution in the training set and C+T loses some information from mentions that helps to understand the text.",
                "html": null,
                "num": null
            },
            "TABREF5": {
                "content": "<table><tr><td>Type</td><td>Example</td></tr><tr><td colspan=\"2\">Wrong ..., 42% Label: org:top members/employees</td></tr><tr><td/><td>Prediction: no relation</td></tr><tr><td/><td>Label: org:subsidiaries</td></tr><tr><td/><td>Prediction: no relation</td></tr><tr><td>31%</td><td>Label: per:religion</td></tr><tr><td/><td>Prediction: no relation</td></tr><tr><td colspan=\"2\">Confusing About a year later, she was transferred to Camp Hope, Iraq.</td></tr><tr><td>27%</td><td>Label: per:countries of residence</td></tr><tr><td/><td>Prediction: per:stateorprovinces of residence</td></tr></table>",
                "type_str": "table",
                "text": "Jacinto Suarez, Nicaraguan deputy to the Central American Parliament (PARLACEN) said Monday.US life insurance giant MetLife said on Monday it will acquire American International Group unit American Life Insurance company (ALICO) in a deal worth 155 billion dollars.No pattern On Monday, the judge questioned the leader of the Baptist group, Laura Silsby, who ...",
                "html": null,
                "num": null
            },
            "TABREF6": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Case study on unique wrong predictions made by OnlyC (compared to C+M). We sample 10% of the wrong predictions, filter the wrong-labeled instances and manually annotate the wrong types to get the proportions.",
                "html": null,
                "num": null
            },
            "TABREF7": {
                "content": "<table><tr><td>Dataset</td><td># Rel.</td><td colspan=\"2\"># Inst. % N/A</td></tr><tr><td>TACRED</td><td colspan=\"2\">42 106,264</td><td>79.5%</td></tr><tr><td>SemEval-2010 Task 8</td><td>19</td><td>10,717</td><td>17.4%</td></tr><tr><td>Wiki80</td><td>80</td><td>56,000</td><td>-</td></tr><tr><td>ChemProt</td><td>13</td><td>10,065</td><td>-</td></tr><tr><td>FewRel</td><td>100</td><td>70,000</td><td>-</td></tr></table>",
                "type_str": "table",
                "text": "Statistics for RE datasets used in the paper, including numbers of relations, numbers of instances and proportions of N/A instances. \"-\" for the last column means that there is no N/A relation in the dataset.",
                "html": null,
                "num": null
            },
            "TABREF9": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Results on supervised RE datasets TACRED (micro F 1 ), SemEval (micro F 1 ), Wiki80 (accuracy) and ChemProt (micro F 1 ). 1% / 10% indicate using 1% / 10% supervised training data respectively. We also add 1% and 10% settings, meaning using only 1% / 10% data of the training sets. It is to simulate a low-resource scenario and observe how model performance changes across different datasets and settings. Note that ChemProt only has 4, 169 training instances, which leads to the abnormal results on 1% ChemProt in Table 5. We give details about this problem in Appendix B.",
                "html": null,
                "num": null
            },
            "TABREF10": {
                "content": "<table><tr><td>Model</td><td colspan=\"12\">5-way 1-shot C+M OnlyC OnlyM C+M OnlyC OnlyM C+M OnlyC OnlyM C+M OnlyC OnlyM 5-way 5-shot 10-way 1-shot 10-way 5-shot</td></tr><tr><td/><td/><td/><td/><td/><td/><td>FewRel 1.0</td><td/><td/><td/><td/><td/><td/></tr><tr><td>BERT</td><td>0.911</td><td>0.866</td><td>0.701</td><td>0.946</td><td>0.925</td><td>0.804</td><td>0.842</td><td>0.779</td><td>0.575</td><td>0.908</td><td>0.876</td><td>0.715</td></tr><tr><td>MTB</td><td>0.911</td><td>0.879</td><td>0.727</td><td>0.954</td><td>0.939</td><td>0.835</td><td>0.843</td><td>0.779</td><td>0.568</td><td>0.918</td><td>0.892</td><td>0.742</td></tr><tr><td>CP</td><td>0.951</td><td>0.926</td><td>0.743</td><td>0.971</td><td>0.956</td><td>0.840</td><td>0.912</td><td>0.867</td><td>0.620</td><td>0.947</td><td>0.924</td><td>0.763</td></tr><tr><td/><td/><td/><td/><td/><td colspan=\"3\">FewRel 2.0 Domain Adaptation</td><td/><td/><td/><td/><td/></tr><tr><td>BERT</td><td>0.746</td><td>0.683</td><td>0.316</td><td>0.827</td><td>0.782</td><td>0.406</td><td>0.635</td><td>0.542</td><td>0.210</td><td>0.765</td><td>0.706</td><td>0.292</td></tr><tr><td>MTB</td><td>0.747</td><td>0.692</td><td>0.338</td><td>0.879</td><td>0.836</td><td>0.426</td><td>0.625</td><td>0.528</td><td>0.216</td><td>0.811</td><td>0.744</td><td>0.298</td></tr><tr><td>CP</td><td>0.797</td><td>0.745</td><td>0.335</td><td>0.849</td><td>0.840</td><td>0.437</td><td>0.681</td><td>0.601</td><td>0.213</td><td>0.798</td><td>0.738</td><td>0.297</td></tr></table>",
                "type_str": "table",
                "text": "Accuracy on FewRel dataset. FewRel 1.0 is trained and tested on Wikipedia domain. FewRel 2.0 is trained on Wikipedia domain but tested on biomedical domain.low-resource and few-shot settings. For C+M, we observe a promotion of 7% on 10-way 1-shot FewRel 1.0, 18% improvement on 1% setting of TACRED, and 24% improvement on 1% setting of Wiki80. There is also a similar trend for OnlyC and OnlyM. In the low resource and few-shot settings, it is harder for models to learn to extract relational patterns from context and easier to overfit to superficial cues of mentions, due to the limited training data. However, with the contrastive pre-training, our model can relatively take better use of textual context while avoiding being biased by entities, and outperform the other baselines by a large margin.",
                "html": null,
                "num": null
            },
            "TABREF11": {
                "content": "<table/>",
                "type_str": "table",
                "text": ", applying BERT-like models as the backbone of RE systems(Baldini Soares et al., 2019) has become a standard procedure. Based on BERT, Bal-dini Soares et al. (2019) propose matching the blanks, an RE-oriented pre-trained model to learn relational patterns from text. A different direction is to inject entity knowledge, in the form of entity embeddings, into BERT",
                "html": null,
                "num": null
            }
        }
    }
}