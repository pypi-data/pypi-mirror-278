{
    "paper_id": "D18-1204",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T11:58:52.854431Z"
    },
    "title": "Neural Related Work Summarization with a Joint Context-driven Attention Mechanism",
    "authors": [
        {
            "first": "Yongzhen",
            "middle": [],
            "last": "Wang",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Dalian Maritime University",
                "location": {
                    "settlement": "Dalian",
                    "country": "China"
                }
            },
            "email": ""
        },
        {
            "first": "Xiaozhong",
            "middle": [],
            "last": "Liu",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Indiana University Bloomington",
                "location": {
                    "settlement": "Bloomington",
                    "region": "IN",
                    "country": "USA"
                }
            },
            "email": ""
        },
        {
            "first": "Zheng",
            "middle": [],
            "last": "Gao",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Indiana University Bloomington",
                "location": {
                    "settlement": "Bloomington",
                    "region": "IN",
                    "country": "USA"
                }
            },
            "email": ""
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Conventional solutions to automatic related work summarization rely heavily on humanengineered features. In this paper, we develop a neural data-driven summarizer by leveraging the seq2seq paradigm, in which a joint context-driven attention mechanism is proposed to measure the contextual relevance within full texts and a heterogeneous bibliography graph simultaneously. Our motivation is to maintain the topic coherency between a related work section and its target document, where both the textual and graphic contexts play a big role in characterizing the relationship among scientific publications accurately. Experimental results on a large dataset show that our approach achieves a considerable improvement over a typical seq2seq summarizer and five classical summarization baselines.",
    "pdf_parse": {
        "paper_id": "D18-1204",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Conventional solutions to automatic related work summarization rely heavily on humanengineered features. In this paper, we develop a neural data-driven summarizer by leveraging the seq2seq paradigm, in which a joint context-driven attention mechanism is proposed to measure the contextual relevance within full texts and a heterogeneous bibliography graph simultaneously. Our motivation is to maintain the topic coherency between a related work section and its target document, where both the textual and graphic contexts play a big role in characterizing the relationship among scientific publications accurately. Experimental results on a large dataset show that our approach achieves a considerable improvement over a typical seq2seq summarizer and five classical summarization baselines.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "In scientific fields, scholars need to contextualize their contribution to help readers acquire an understanding of their research papers. For this purpose, the related work section of an article serves as a pivot to connect prior domain knowledge, in which the innovation and superiority of current work are displayed by a comparison with previous studies. While citation prediction can assist in drafting a reference collection (Nallapati et al., 2008) , consuming all these papers is still a laborious job, where authors must read every source document carefully and locate the most relevant content cautiously.",
                "cite_spans": [
                    {
                        "start": 430,
                        "end": 454,
                        "text": "(Nallapati et al., 2008)",
                        "ref_id": "BIBREF31"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "As a solution in saving authors' efforts, automatic related work summarization is essentially a topic-biased multi-document problem (Cong and Kan, 2010) , which relies heavily on humanengineered features to retrieve snippets from the references. Most recently, neural networks enable a data-driven architecture sequence-to-sequence (seq2seq) for natural language generation (Bahdanau et al., 2014 (Bahdanau et al., , 2016)) , where an encoder reads a sequence of words/sentences into a context vector, from which a decoder yields a sequence of specific outputs. Nonetheless, compared to scenarios like machine translation with an end-to-end nature, aligning a related work section to its source documents is far more challenging.",
                "cite_spans": [
                    {
                        "start": 132,
                        "end": 152,
                        "text": "(Cong and Kan, 2010)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 374,
                        "end": 396,
                        "text": "(Bahdanau et al., 2014",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 397,
                        "end": 423,
                        "text": "(Bahdanau et al., , 2016))",
                        "ref_id": "BIBREF2"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "To address the summarization alignment, former studies try to apply an attention mechanism to measure the saliency/novelty of each candidate word/sentence (Tan et al., 2017) , with the aim of locating the most representative content to retain primary coverage. However, toward summarizing a related work section, authors should be more creative when organizing text streams from the reference collection, where the selected content ought to highlight the topic bias of current work, rather than retell each reference in a compressed but balanced fashion. This motivates us to introduce the contextual relevance and characterize the relationship among scientific publications accurately.",
                "cite_spans": [
                    {
                        "start": 155,
                        "end": 173,
                        "text": "(Tan et al., 2017)",
                        "ref_id": "BIBREF37"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Generally speaking, for a pair of documents, a larger lexical overlap often implies a higher similarity in their research backgrounds. Yet such a hypothesis is not always true when sampling content from multiple relevant topics. Take \"DSSM\" 1 as an example, from viewpoint of the abstract similarity, those references investigating \"Information Retrieval\", \"Latent Semantic Model\" or \"Clickthrough Data Mining\" could be of more importance in correlation and should be greatly sampled for the related work section. But in reality, this article spends a bit larger chunk of texts (about 58%) to elaborate \"Deep Learning\" during the literature review, which is quite difficult for machines to grasp the contextual relevance therein. In addi-tion, other situations like emerging new concepts also suffer from the terminology variation or paraphrasing in varying degrees.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this study, we utilize a heterogeneous bibliography graph to embody the relationship within a scalable scholarly database. Over the recent past, there is a surge of interest in exploiting diverse relations to analyze bibliometrics, ranging from literature recommendation (Yu et al., 2015) to topic evolvement (Jensen et al., 2016) . In a graphical sense, interconnected papers transfer the credit among each other directly/indirectly through various patterns, such as paper citation, author collaboration, keyword association and releasing on series of venues, which constitutes the graphic context for outlining concerned topics. Unfortunately, a variety of edge types may pollute the information inquiry, where a slice of edges are not so important as the others on sampling content. Meanwhile, most existing solutions in mining heterogeneous graphs depend on the human supervision, e.g., hyperedge (Bu et al., 2010) and metapath (Swami et al., 2017) . This is usually not easy to access due to the complexity of graph schemas.",
                "cite_spans": [
                    {
                        "start": 274,
                        "end": 291,
                        "text": "(Yu et al., 2015)",
                        "ref_id": "BIBREF40"
                    },
                    {
                        "start": 312,
                        "end": 333,
                        "text": "(Jensen et al., 2016)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 904,
                        "end": 921,
                        "text": "(Bu et al., 2010)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 935,
                        "end": 955,
                        "text": "(Swami et al., 2017)",
                        "ref_id": "BIBREF36"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Our contribution is threefold: First, we explore the edge-type usefulness distribution (EUD) on a heterogeneous bibliography graph, which enables the relationship discovery (between any pair of papers) for sampling the interested information. Second, we develop a novel seq2seq summarizer for the automatic related work summarization, where a joint context-driven attention mechanism is proposed to measure the contextual relevance within both textual and graphic contexts. Third, we conduct experiments on 8,080 papers with native related work sections, and experimental results show that our approach outperforms a typical seq2seq summarizer and five classical summarization baselines significantly.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "This study touches on several strands of research within automatic related work summarization and seq2seq summarizer as follows.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "The idea of creating a related work section automatically is pioneered by Cong and Kan (2010) who design two rule-based strategies to extract sentences for general and detailed topics respectively. Subsequently, Hu and Wan (2014) 2015) which utilizes a feed-forward network for compressing sentences, and later is expanded by Chopra et al. (2016) with a recurrent neural network (RNN). On this basis, Nallapati et al. (2016a,c) and Chen et al. (2016) both present a set of RNN-based models to address various aspects of abstractive summarization. Typically, Cheng and Lapata (2016) propose a general seq2seq summarizer, where an encoder learns the representation of documents while a decoder generates each word/sentence using an attention mechanism. With further research, Nallapati et al. (2016b) extend the sentence compression by trying a hierarchical attention architecture and a limited vocabulary during the decoding phase. Next, Narayan et al. (2017) leverage the side information as an attention cue to locate focus regions for summaries. Recently, inspired by PageRank, Tan et al. (2017) introduce a graph-based attention mechanism to tackle the saliency problem. Nonetheless, these methods all discuss the single-document scenario, which is far from the nature of automatic related work summarization.",
                "cite_spans": [
                    {
                        "start": 74,
                        "end": 93,
                        "text": "Cong and Kan (2010)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 212,
                        "end": 229,
                        "text": "Hu and Wan (2014)",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 326,
                        "end": 346,
                        "text": "Chopra et al. (2016)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 401,
                        "end": 427,
                        "text": "Nallapati et al. (2016a,c)",
                        "ref_id": null
                    },
                    {
                        "start": 432,
                        "end": 450,
                        "text": "Chen et al. (2016)",
                        "ref_id": null
                    },
                    {
                        "start": 558,
                        "end": 581,
                        "text": "Cheng and Lapata (2016)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 774,
                        "end": 798,
                        "text": "Nallapati et al. (2016b)",
                        "ref_id": null
                    },
                    {
                        "start": 937,
                        "end": 958,
                        "text": "Narayan et al. (2017)",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 1080,
                        "end": 1097,
                        "text": "Tan et al. (2017)",
                        "ref_id": "BIBREF37"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "In this study, derived from the general seq2seq summarizer of Cheng and Lapata (2016) , we propose a joint context-driven attention mechanism to measure the contextual relevance within full texts and a heterogeneous bibliography graph simultaneously. To our best knowledge, we make the first attempt to develop a neural data-driven solution for the automatic related work summarization, and the practice of using the joint context as an attention cue is also less explored to date. Besides, this study is launched on a dataset with up to 8,080 papers, which is much greater than previous studies and makes our results more convincing.",
                "cite_spans": [
                    {
                        "start": 62,
                        "end": 85,
                        "text": "Cheng and Lapata (2016)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "Since text summarization via word-by-word generation is not mature at present (Cheng and Lapata, 2016; Nallapati et al., 2016b; Tan et al., 2017) , we adopt the extractive sentential fashion for our summarizer, where a related work section is created by extracting and linking sentences from a reference collection. Meanwhile, this study follows the mode of Cong and Kan (2010) who assume that the collection is given as part of the input, and do not consider the citation sentences of each reference.",
                "cite_spans": [
                    {
                        "start": 78,
                        "end": 102,
                        "text": "(Cheng and Lapata, 2016;",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 103,
                        "end": 127,
                        "text": "Nallapati et al., 2016b;",
                        "ref_id": null
                    },
                    {
                        "start": 128,
                        "end": 145,
                        "text": "Tan et al., 2017)",
                        "ref_id": "BIBREF37"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "To adapt the seq2seq paradigm, we formulate the automatic related work summarization into a sequential text generation problem as follows.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Problem Formulation",
                "sec_num": "3.1"
            },
            {
                "text": "Given an unedited paper t (target document) and its n-size reference collection R t = {r t 1:n }, we draw up a related work section for t by selecting sentences from R t . To be specific, each reference (source document) will be traversed one time sequentially, and without loss of generality, in the descending order of their significance to t. Consequently, all sentences to be selected are concatenated into an m-length sequence S t = {s t 1:m } to feed the summarizer. For each candidate sentence s t j , once being visited, a label y t j \u2208 {0, 1} will be determined synchronously based on whether or not this sentence should be covered into the output. Our objective is to maximize the loglikelihood probability of observed labels Y t = {y t 1:m } under R t , S t and summarizer parameters \u03b8, as shown below. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Problem Formulation",
                "sec_num": "3.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "max m j=1 log Pr(y t j | R t ; S t ; \u03b8)",
                        "eq_num": "(1)"
                    }
                ],
                "section": "Problem Formulation",
                "sec_num": "3.1"
            },
            {
                "text": "Prior works have illustrated that one of the most promising channels for information recommendation is the community network (Guo and Liu, 2015) . In this study, we verify this hypothesis toward the content sampling of scientific summarization, by investigating heterogeneous relations among different kinds of objects such as papers, authors, keywords and venues.",
                "cite_spans": [
                    {
                        "start": 125,
                        "end": 144,
                        "text": "(Guo and Liu, 2015)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Random Walk on Heterogeneous Bibliography Graph",
                "sec_num": "3.2"
            },
            {
                "text": "For measuring the relationship among scientific publications, we introduce a directed graph G = (V, E) to contain various bibliographical connections, as shown in Figure 1 , which involves four objects and ten edge types in total. Each edge e j,i \u2208 E is assigned a value \u03c0(e j,i ) z \u2208 [0, 1] to indicate the transition probability between two nodes v j , v i \u2208 V, where \u03c0(e j,i ) \u2208 R returns the unknown edge-type usefulness of e j,i , and z \u2208 R is a normalizing weight. For most of edge types, we model the weight as one divided by the number of outgoing links of the same kind. But regarding the \"contribution\" category, the weight modeling is accomplished by PageRank with Priors (White and Smyth, 2003) . Note that different edge types usually take very uneven importance in one particular task (Yu et al., 2015) , and it is quite difficult to enable the classical heterogeneous graph mining without expert defined paths for random walk (Bu et al., 2010; Swami et al., 2017) .",
                "cite_spans": [
                    {
                        "start": 683,
                        "end": 706,
                        "text": "(White and Smyth, 2003)",
                        "ref_id": "BIBREF38"
                    },
                    {
                        "start": 799,
                        "end": 816,
                        "text": "(Yu et al., 2015)",
                        "ref_id": "BIBREF40"
                    },
                    {
                        "start": 941,
                        "end": 958,
                        "text": "(Bu et al., 2010;",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 959,
                        "end": 978,
                        "text": "Swami et al., 2017)",
                        "ref_id": "BIBREF36"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 170,
                        "end": 171,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Random Walk on Heterogeneous Bibliography Graph",
                "sec_num": "3.2"
            },
            {
                "text": "In this study, we propose an unsupervised approach to capture the connectivity diversity, by introducing an optimal EUD for navigating random walkers on the heterogeneous bibliography graph. Given a target document t, the optimized useful-ness assignment can help those walkers lock a topn recommendation Rt to best match the reference collection R t , as shown in Eq. 2. On this basis, a well-performing algorithm node2vec (Grover and Leskovec, 2016 ) is adopted to conduct an unsupervised random walk to vectorize every node \u2200v * \u2208 V into a d-dimensional embedding \u03d5(v * ) \u2208 R d so that any edge \u2200e * \u2208 E can be calculated therefrom. Specifically, we employ evolutionary algorithm (EA) to tune the EUD, which enjoys advantages over conventional gradient methods in both convergence speed and accuracy.",
                "cite_spans": [
                    {
                        "start": 424,
                        "end": 450,
                        "text": "(Grover and Leskovec, 2016",
                        "ref_id": "BIBREF15"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Random Walk on Heterogeneous Bibliography Graph",
                "sec_num": "3.2"
            },
            {
                "text": "arg max t n j=1 log Pr(r t j \u2208 Rt | EUD) (2)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Random Walk on Heterogeneous Bibliography Graph",
                "sec_num": "3.2"
            },
            {
                "text": "EA Setup We use an array of real numbers x 1:10 to code an individual in the population, where x j \u2208 [0, 1] denotes the usefulness of j-th edge type. Given an EUD, PageRank (Page, 1998) runs on graph to infer the relative importance of each node for each target document, and a fitness function is applied to judge how well this EUD satisfies locating the ground truth references as Eq. 3, in which if r t j belongs to Rt , then \u03b1(r t j , Rt ) \u2208 N returns the ranking of r t j within Rt , and otherwise a big penalty coefficient to prevent irrelevant references to be recommended. Like most other optimizations, this procedure starts with a randomly generated population.",
                "cite_spans": [
                    {
                        "start": 173,
                        "end": 185,
                        "text": "(Page, 1998)",
                        "ref_id": "BIBREF34"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Random Walk on Heterogeneous Bibliography Graph",
                "sec_num": "3.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "max 1 t n j=1 j -\u03b1(r t j , Rt )",
                        "eq_num": "(3)"
                    }
                ],
                "section": "Random Walk on Heterogeneous Bibliography Graph",
                "sec_num": "3.2"
            },
            {
                "text": "EA Operator We choose the operator from differential evolution (Das and Suganthan, 2011) to generate offsprings for each individual. The basic idea is to utilize the difference between different individuals to disturb each trial object. First, three distinct individuals x r 1 1:10 , x r 2 1:10 , x r 3 1:10 are sampled randomly from current population to create a variant x var 1:10 , as shown in Eq. 4, where f \u2208 R indicates the scaling factor. Next, x var 1:10 is crossed with a trial object x tri 1:10 to build a hybrid one x hyb 1:10 as Eq. 5, in which c \u2208 [0, 1] denotes the crossover factor and u \u2208 [0, 1] represents an uniform random number. At last, the fitnesses of x tri 1:10 and x hyb 1:10 are compared, and the better one will be saved as the offspring into a new round of evolution.",
                "cite_spans": [
                    {
                        "start": 63,
                        "end": 88,
                        "text": "(Das and Suganthan, 2011)",
                        "ref_id": "BIBREF13"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Random Walk on Heterogeneous Bibliography Graph",
                "sec_num": "3.2"
            },
            {
                "text": "x var j = x r 1 j + f \u00d7 (x r 2 j -x r 3 j ) (4) x hyb j = \uf8f1 \uf8f2 \uf8f3 x var j , if u \u2264 c",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Random Walk on Heterogeneous Bibliography Graph",
                "sec_num": "3.2"
            },
            {
                "text": "x tri j , otherwise",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Random Walk on Heterogeneous Bibliography Graph",
                "sec_num": "3.2"
            },
            {
                "text": "(5)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Random Walk on Heterogeneous Bibliography Graph",
                "sec_num": "3.2"
            },
            {
                "text": "As Figure 2 shows, we model our seq2seq summarizer with a hierarchical encoder and an attentionbased decoder, as described below.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 10,
                        "end": 11,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Neural Extractive Summarization",
                "sec_num": "3.3"
            },
            {
                "text": "Hierarchical Encoder Our encoder consists of two major layers, namely a convolutional neural network (CNN) and a long-short-term memory (LSTM)-based RNN. Specifically, the CNN deals with word-level texts to derive sentencelevel meanings, which are then taken as inputs to the RNN for handling longer-range dependency within lager units like a paragraph and even a whole paper. This conforms to the nature of document that is composed from words, sentences and higher levels of abstraction (Narayan et al., 2017) . Consider a sentence of p words s t j = {w t j,1:p }, where each word w t j,i can be represented by a ddimensional embedding \u03c6(w t j,i ) \u2208 R d . Previous studies have illustrated the strength of CNN in presenting sentences, because of its capability to learn compressed expressions and address sentences with variable lengths (Kim, 2014) . First, a convolution kernel k \u2208 R d\u00d7q\u00d7d is applied to each possible window of q words to construct a list of feature maps as:",
                "cite_spans": [
                    {
                        "start": 489,
                        "end": 511,
                        "text": "(Narayan et al., 2017)",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 839,
                        "end": 850,
                        "text": "(Kim, 2014)",
                        "ref_id": "BIBREF22"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Neural Extractive Summarization",
                "sec_num": "3.3"
            },
            {
                "text": "g t j,i = tanh k \u00d7 \u03c6(w t j,i:i+q-1 ) + b (6)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Neural Extractive Summarization",
                "sec_num": "3.3"
            },
            {
                "text": "where b \u2208 R d denotes the bias term. Next, maxover-time pooling (Collobert et al., 2011 ) is performed on all generated features to obtain the sentence embedding as:",
                "cite_spans": [
                    {
                        "start": 64,
                        "end": 87,
                        "text": "(Collobert et al., 2011",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Neural Extractive Summarization",
                "sec_num": "3.3"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "\u03c6(s t j ) = max 1\u2264i\u2264d g t j,1:p-q+1 [i, :]",
                        "eq_num": "(7)"
                    }
                ],
                "section": "Neural Extractive Summarization",
                "sec_num": "3.3"
            },
            {
                "text": "where [i, :] denotes the i-th row of matrix. Given a sequence of sentences S t = {s t 1:m }, we then take the RNN to yield an equal-length array of hidden states, in which LSTM has proved to alleviate the vanishing gradient problem when training long sequences (Hochreiter and Schmidhuber, 1997) . Each hidden state can be viewed as a local representation with focusing on current and former sentences together, which is updated as: sentence, and average them to capture the information inside different n-grams. As Figure 2 (bottom) shows, the sentence s t j involves six words, and two kernels of widths two (orange) and three (green) abstract a set of five and four feature maps respectively. Meanwhile, since rhetorical structure theory (Mann and Thompson, 2009) points out that association must exist in any two parts of coherent texts, RNN is only applicable to manage the sentence relation within a single document, because we cannot expect the dependency between two sections from different references.",
                "cite_spans": [
                    {
                        "start": 261,
                        "end": 295,
                        "text": "(Hochreiter and Schmidhuber, 1997)",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 741,
                        "end": 766,
                        "text": "(Mann and Thompson, 2009)",
                        "ref_id": "BIBREF26"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 523,
                        "end": 524,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Neural Extractive Summarization",
                "sec_num": "3.3"
            },
            {
                "text": "h t j = LSTM \u03c6(s t j ), h t j-1 \u2208 R d .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Neural Extractive Summarization",
                "sec_num": "3.3"
            },
            {
                "text": "Attention-based Decoder Our decoder labels each sentence s t j as 0/1 sequentially, according to whether it is salient or novel enough, plus if relevant to the target document t or not. As shown in Figure 2 (top), the binary decision y t j is made by both the hidden state h t j and the context vector ht j from an attention mechanism (grey background).",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 205,
                        "end": 206,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Attention-based Decoder",
                "sec_num": null
            },
            {
                "text": "In particular, this attention (red dash line) is acted as an intermediate stage to determine which sentences to highlight so as to provide the contextual information for current decision (Bahdanau et al., 2014) . Given H t = {h t 1:m }, this decoder returns the probability of y t j = 1 as below:",
                "cite_spans": [
                    {
                        "start": 187,
                        "end": 210,
                        "text": "(Bahdanau et al., 2014)",
                        "ref_id": "BIBREF1"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Attention-based Decoder",
                "sec_num": null
            },
            {
                "text": "Pr(y t j = 1 | R t ; S t ; \u03b8) = sigmoid \u03b4(h t j , ht j ) (8)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Attention-based Decoder",
                "sec_num": null
            },
            {
                "text": "ht j = m i=1 a j,i h t i (9)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Attention-based Decoder",
                "sec_num": null
            },
            {
                "text": "where \u03b4(h t j , ht j ) \u2208 R denotes a fully connected layer with as input the concatenation of h t j and ht j , and a j,i \u2208 [0, 1] is the attention weight indicating how much the supporting sentence s t i contributes to extracting the candidate one s t j . Apart from saliency and novelty two traditional attention factors (Chen et al., 2016; Tan et al., 2017) , we focus on the contextual relevance within both textual and graphic contexts to distinguish the relationship from near to far, as shown in Eq. 10 and Eq. 11. To be specific: 1) h tT j W s h t i represents the saliency of s t i to s t j ; 2) -d tT j W n h t i indicates the novelty of s t i to the dynamic output d t j ; 3) \u03c6(t) T W t h t i denotes the relevance of s t i to t from the textual context; 4) \u03d5(t) T W g \u03d5(h t i ) refers to the relevance from the graphic context. More concretely, W * \u2208 R d characterizes the learnable matrix, \u03c6(t) returns the average of hidden states from t, \u03d5(t) and \u03d5(h t i ) return the node embeddings of both t and the source document that h t i belongs to respectively. Note that \u03c6(\u2022) and \u03d5(\u2022) represent two distinct embedding spaces, where the former reflects the lexical collocations of corpus, and the latter embodies the connectivity patterns of associated graph.",
                "cite_spans": [
                    {
                        "start": 322,
                        "end": 341,
                        "text": "(Chen et al., 2016;",
                        "ref_id": null
                    },
                    {
                        "start": 342,
                        "end": 359,
                        "text": "Tan et al., 2017)",
                        "ref_id": "BIBREF37"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Attention-based Decoder",
                "sec_num": null
            },
            {
                "text": "a j,i = h tT j W s h t i # saliency -d tT j W n h t i # novelty +\u03c6(t) T W t h t i # relevance 1 +\u03d5(t) T W g \u03d5(h t i ) # relevance 2",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Attention-based Decoder",
                "sec_num": null
            },
            {
                "text": "(10)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Attention-based Decoder",
                "sec_num": null
            },
            {
                "text": "d t j = j-1 i=1 Pr(y t j = 1 | R t ; S t ; \u03b8) \u00d7 h t i (11)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Attention-based Decoder",
                "sec_num": null
            },
            {
                "text": "The basic idea behind our attention mechanism is as follows: if a supporting sentence more resembles a candidate one, or overlaps less with the dynamic output, or is more relevant to the target document, then it can provide more contextual information to facilitate current decision on being extracted or not, thereby taking a higher weight in the generated context vector. This innovative attention will guide our goal related work section to maximize the representativeness of selected sentences (saliency & novelty), while minimizing the semantic distance to the target document (relevance). This is consistent with the way that scholars consume a reference collection, with the minmax objective in their minds.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Attention-based Decoder",
                "sec_num": null
            },
            {
                "text": "This section presents the experimental setup for assessing our approach, including 1) dataset used for training and testing, 2) implementation details, 3) contrast methods and evaluation metrics. Dataset We conduct experiments on a dataset2 created from the ACM digital library, where metadata and full texts are derived from PDF files. To be detailed, this dataset includes 371,891 papers, 779,810 authors, 9,204 keywords and 807 venues in total. Note that we ignore the keyword with frequency below a certain threshold, and adopt greedy matching of Guo et al. (2013) to generate pseudo keywords for papers lacking topic descriptions. For each target document, the references are traversed by the descending order of the cited number in related work section (primary) and in full paper (secondary) successively. We first apply a series of pre-processings such as lowercasing and stemming to standardize candidate sentences, then remove those which are too short/long (< 7 or > 80 words). On this basis, a total of 8,080 papers are selected to evaluate our approach, each containing more than 15 references found in the dataset and a related work section of at least 500 words. But as for the heterogeneous bibliography graph, all source data have to be imported to ensure the structural integrity of communities. Besides, this graph should be constructed year-byyear to preclude the effect of later publications on earlier ones.",
                "cite_spans": [
                    {
                        "start": 551,
                        "end": 568,
                        "text": "Guo et al. (2013)",
                        "ref_id": "BIBREF17"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Setup",
                "sec_num": "4.1"
            },
            {
                "text": "Implementation We use Tensorflow for implementation, where both the dimensions of embedding and hidden state are equally 128. For the CNN, word2vec (Mikolov et al., 2013) is utilized to initialize the word embeddings, which can be further tuned during the training phase. Meanwhile, we follow the work of Kim (2014) to apply a list of kernels with widths {3, 4, 5}. As for the RNN, each LSTM module is set to one single layer, and all input documents are padded to the same length, along with a mark to indicate the real number of sentences. Based on these settings, we train our summarizer using Adam with the default in Kingma and Ba (2014) , and perform mini-batch cross-entropy training with a batch of one target document for 20 epochs.",
                "cite_spans": [
                    {
                        "start": 148,
                        "end": 170,
                        "text": "(Mikolov et al., 2013)",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 305,
                        "end": 315,
                        "text": "Kim (2014)",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 622,
                        "end": 642,
                        "text": "Kingma and Ba (2014)",
                        "ref_id": "BIBREF23"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Setup",
                "sec_num": "4.1"
            },
            {
                "text": "To create training data for our summarizer, each reference needs to be annotated with the ground truth in advance, i.e., candidate sentences are tagged with 0/1 for indicating summary-worthy or not. Specifically, we follow a heuristic practice of Cao et al. (2016) and Nallapati et al. (2016b) to compute ROUGE-2 score (Lin and Hovy, 2003) for each sentence, in terms of the native related work sections (gold standards). Next, those sentences with high scores are chosen as the positive samples, and the rest as the negative ones, such that the total score of selected sentences is maximized with respect to the gold standard. As for testing, we relax the number of sentences to be selected, and focus on the classification probability from Eq. 8. In this study, cross validation is applied to split the dataset into ten parts equally at random, in which nine are used for training and the other one for testing. Evaluation We adopt the widely used toolkit ROUGE (Lin and Hovy, 2003) to evaluate the summarization performance automatically. In particular, we report ROUGE-1 and ROUGE-2 (unigram and bigram overlapping) as a way to assess the informativeness, and ROUGE-L (the longest common subsequence) as a means to assess the fluency, in terms of fixed bytes of gold standards.",
                "cite_spans": [
                    {
                        "start": 247,
                        "end": 264,
                        "text": "Cao et al. (2016)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 269,
                        "end": 293,
                        "text": "Nallapati et al. (2016b)",
                        "ref_id": null
                    },
                    {
                        "start": 319,
                        "end": 339,
                        "text": "(Lin and Hovy, 2003)",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 964,
                        "end": 984,
                        "text": "(Lin and Hovy, 2003)",
                        "ref_id": "BIBREF24"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Setup",
                "sec_num": "4.1"
            },
            {
                "text": "To validate the proposed attention mechanism, we compare our approach (denoted as P. S+N+Rteg+EUD ) against six variants, including: 1) P. void : a plain seq2seq summarizer without attentions; 2) P. S : use the saliency as an only attention factor; 3) P. S+N : leverage both the saliency and novelty; 4) P. S+N+Rt : incorporate the relevance from the textual context; 5) P. S+N+Rtog : gain the relevance from the graphic context of a homogeneous citation graph; 6) P. S+N+Rteg : utilize the heterogeneous bibliography graph, but with each edge type the same usefulness.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Setup",
                "sec_num": "4.1"
            },
            {
                "text": "In addition, we also select six representative summarization methods as a benchmark group. The first one is the general seq2seq summarizer by Cheng and Lapata (2016) , denoted as Point-erNet, which employs an attention mechanism to extract sentences directly after reading them. Following are five classical generic solutions, including: 1) Luhn (Luhn, 1958) : a heuristic summarization based on word frequency and distribution; 2) MMR (Carbonell and Goldstein, 1998 ): a diversity-based re-ranking to produce summaries; 3) LexRank (Erkan et al., 2004 ): a graph-based summary technique inspired by PageRank and HITS; 4) SumBasic (Nenkova and Vanderwende, 2005) : a frequency-based summarizer with duplication removal; 5) NltkSum (Acanfora et al., 2014) : a natural language tookit (NLTK)-based implementation for summarization.",
                "cite_spans": [
                    {
                        "start": 142,
                        "end": 165,
                        "text": "Cheng and Lapata (2016)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 346,
                        "end": 358,
                        "text": "(Luhn, 1958)",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 436,
                        "end": 466,
                        "text": "(Carbonell and Goldstein, 1998",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 532,
                        "end": 551,
                        "text": "(Erkan et al., 2004",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 630,
                        "end": 661,
                        "text": "(Nenkova and Vanderwende, 2005)",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 730,
                        "end": 753,
                        "text": "(Acanfora et al., 2014)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Setup",
                "sec_num": "4.1"
            },
            {
                "text": "For clarity, Luhn, LexRank and SumBasic are analogous to the work of Hu and Wan (2014) which extracts sentences scoring the highest in significance, and they are also contrasted in the latest studies on neural summarizers (Chen et al., 2016; Tan et al., 2017) . Meanwhile, MMR often serves as a part/post-processing of existing tech-niques to avoid the redundancy (Cohan and Goharian, 2017) , and we introduce NltkSum to investigate the impact of grammatical/semantic analysis to the automatic related work summarization. Note that former studies specially for this task require extensive human involvements (see Table 1 ), thus we cannot apply them to such a large dataset of this study.",
                "cite_spans": [
                    {
                        "start": 69,
                        "end": 86,
                        "text": "Hu and Wan (2014)",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 222,
                        "end": 241,
                        "text": "(Chen et al., 2016;",
                        "ref_id": null
                    },
                    {
                        "start": 242,
                        "end": 259,
                        "text": "Tan et al., 2017)",
                        "ref_id": "BIBREF37"
                    },
                    {
                        "start": 364,
                        "end": 390,
                        "text": "(Cohan and Goharian, 2017)",
                        "ref_id": "BIBREF10"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 619,
                        "end": 620,
                        "text": "1",
                        "ref_id": "TABREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Experimental Setup",
                "sec_num": "4.1"
            },
            {
                "text": "Table 2 reports the evaluation comparison over ROUGE metrics. From the top half, all scores appear a gradual upward trend with incorporation of saliency, novelty, relevance (from both textual and graphic contexts) and EUD into consideration one after another, which demonstrates the validity of our attention mechanism for summarizing related work sections. To be specific, we further reach the following conclusions:",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "2",
                        "ref_id": "TABREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Results and Discussion",
                "sec_num": "4.2"
            },
            {
                "text": "1) P. void vs. P. S vs. P. S+N : Both saliency and novelty are two effective factors to locate the required content for summaries, which is consistent with prior studies.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results and Discussion",
                "sec_num": "4.2"
            },
            {
                "text": "2) P. S+N vs. P. S+N+Rt : Contextual relevance does contribute to address the alignment between a related work section and its source documents.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results and Discussion",
                "sec_num": "4.2"
            },
            {
                "text": "3) P. S+N+Rt vs. P. S+N+Rtog : Textual context alone cannot provide entire evidence to characterize the relationship among scientific publications exactly. 4) P. S+N+Rtog vs. P. S+N+Rteg : Heterogeneous bibliography graph involves richer contextual information than a homogeneous citation graph. 5) P. S+N+Rteg vs. P. S+N+Rteg+EUD : EUD plays an indispensable role in organizing accurate contextual relevance on a heterogeneous graph. Continuing the \"DSSM\", Figure 3 cluster 3 under different attention factors. It can be seen that only after adding the relevance especially that from the graphic context into attentions, our summarizer can correctly sample the content from \"Deep Learning\" (yellow line), and eliminate that originated from \"Other Sources\" by a big margin (green line). As this example falls into the methodology transferring, a host of its involved word collocations are not idiomatic combinations yet, such as \"Deep Neural Network\" cooccurs with \"Clickthrough Data\" that is more frequently related to \"Latent Semantic Analysis\" at that time, which results in a somewhat biased textual context. By contrast, the graphic context will suffer less from this bias because it characterizes the connectivity patterns (real-time setup) instead of n-gram statistics, thus offering a more robust measure for the contextual relevance.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 465,
                        "end": 466,
                        "text": "3",
                        "ref_id": "FIGREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Results and Discussion",
                "sec_num": "4.2"
            },
            {
                "text": "The bottom half of Table 2 illustrates the superiority of our approach over six representative summarization methods. Above all, Luhn, LexRank and MMR three summarizers that simply exploit shallow text features (word frequency and associated sentence similarity) to measure either significance or redundancy fall far behind the plain variant P. void , which partly reflects the strength of seq2seq paradigm in summarizing a related work section. Second, with combination of significance and redundancy, SumBasic achieves a drastic increase on ROUGE-1 and a mild raise on 3 We pack the references cited in the same subsection of the related work section as one reference cluster. ROUGE-2 respectively, but it still cannot improve ROUGE-L marginally. This is because simple text statistics cannot present deeper levels of natural language understanding to catch larger-grained units of co-occurrence. Third, NltkSum benefits from a NLTK library so as to access grammatical/semantic supports, thereby having the best informativeness (ROUGE-1 and ROUGE-2) among the five generic baselines, and meanwhile a comparable fluency (ROUGE-L) with our approach. Finally, as a deep learning solution, although PointerNet takes both hidden states and previously labeled sentences into account, at each decoding step it focuses on only current and just one previous sentences, lacking a comprehensive consideration on saliency, novelty and more importantly the contextual relevance (< P. S+N ).",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 25,
                        "end": 26,
                        "text": "2",
                        "ref_id": "TABREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Results and Discussion",
                "sec_num": "4.2"
            },
            {
                "text": "To better verify the summarization performance, we also conduct a human evaluation on 35 papers containing more than 30 references in the dataset. We assign a number of raters to compare each generated related work section against the gold standard, and judge by three independent aspects as: 1) How compliant is the related work section to the target document? 2) How intuitive is the related work section for readers to grasp the key content? 3) How useful is the related work section for researchers to prepare their final literature reviews? Note that we do not allow any ties during the comparison, and each property is assessed with a 5-point scale of 1 (worst) to 5 (best). best-to-worst. Specifically, our approach comes the 1st on 40% of the time, which is followed by NltkSum that is considered the best on 21% of the time (almost half of ours), and Pointer-Net with quite equal proportions on each ranking. Furthermore, the other four summarizers account for obviously lower ratings in general. To attain the statistical significance, one-way analysis of variance (ANOVA) is performed on the obtained ratings, and the results show that our approach is better than all six contrast methods significantly (p < 0.01), which means that the conclusion drawn by Table 2 is sustained.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 1273,
                        "end": 1274,
                        "text": "2",
                        "ref_id": "TABREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Results and Discussion",
                "sec_num": "4.2"
            },
            {
                "text": "In this paper, we highlight the contextual relevance for the automatic related work summarization, and analyze the graphic context to characterize the relationship among scientific publications accurately. We develop a neural data-driven summarizer by leveraging the seq2seq paradigm, where a joint context-driven attention mechanism is proposed to measure the contextual relevance within full texts and a heterogeneous bibliography graph simultaneously. Extensive experiments demonstrate the validity of the proposed attention mechanism, and the superiority of our approach over six representative summarization baselines.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "5"
            },
            {
                "text": "In future work, an appealing direction is to organize the selected sentences in a logical fashion, e.g., by leveraging a topic hierarchy tree to determine the arrangement of the related work section (Cong and Kan, 2010) . We also would like to take the citation sentences of each reference into consideration, which is another concise and universal data source for scientific summarization (Chen and Hai, 2016; Cohan and Goharian, 2017) . At the end of this paper, we believe that extractive methods are by no means the final solutions for literature review generation due to plagiarism concerns, and we are going to put forward a fully abstractive version in further studies.",
                "cite_spans": [
                    {
                        "start": 199,
                        "end": 219,
                        "text": "(Cong and Kan, 2010)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 390,
                        "end": 410,
                        "text": "(Chen and Hai, 2016;",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 411,
                        "end": 436,
                        "text": "Cohan and Goharian, 2017)",
                        "ref_id": "BIBREF10"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "5"
            },
            {
                "text": "Learning deep structured semantic models for web search using clickthrough data(Huang et al.,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "2013)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "To help readers reproduce the experiment outcome, we share part of the experiment data while the copyrighted information is removed. https://github.com/kuadmu/ 2018EMNLP",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "We would like to thank the anonymous reviewers for their valuable comments. This work is partially supported by the National Science Foundation of China under grant No. 71271034.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgement",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Natural language processing: generating a summary of flood disasters",
                "authors": [
                    {
                        "first": "Joseph",
                        "middle": [],
                        "last": "Acanfora",
                        "suffix": ""
                    },
                    {
                        "first": "Marc",
                        "middle": [],
                        "last": "Evangelista",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Keimig",
                        "suffix": ""
                    },
                    {
                        "first": "Myron",
                        "middle": [],
                        "last": "Su",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Cell",
                "volume": "41",
                "issue": "2",
                "pages": "383--394",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Joseph Acanfora, Marc Evangelista, David Keimig, and Myron Su. 2014. Natural language process- ing: generating a summary of flood disasters. Cell, 41(2):383-94.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Neural machine translation by jointly learning to align and translate",
                "authors": [
                    {
                        "first": "Dzmitry",
                        "middle": [],
                        "last": "Bahdanau",
                        "suffix": ""
                    },
                    {
                        "first": "Kyunghyun",
                        "middle": [],
                        "last": "Cho",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1409.0473"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Endto-end attention-based large vocabulary speech recognition",
                "authors": [
                    {
                        "first": "Dzmitry",
                        "middle": [],
                        "last": "Bahdanau",
                        "suffix": ""
                    },
                    {
                        "first": "Jan",
                        "middle": [],
                        "last": "Chorowski",
                        "suffix": ""
                    },
                    {
                        "first": "Dmitriy",
                        "middle": [],
                        "last": "Serdyuk",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the 41st IEEE ICASSP International Conference on Acoustics, Speech and Signal Processing",
                "volume": "",
                "issue": "",
                "pages": "4945--4949",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dzmitry Bahdanau, Jan Chorowski, Dmitriy Serdyuk, Philemon Brakel, and Yoshua Bengio. 2016. End- to-end attention-based large vocabulary speech recognition. In Proceedings of the 41st IEEE ICASSP International Conference on Acoustics, Speech and Signal Processing, Shanghai, China, pages 4945-4949.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Music recommendation by unified hypergraph:combining social media information and music content",
                "authors": [
                    {
                        "first": "Jiajun",
                        "middle": [],
                        "last": "Bu",
                        "suffix": ""
                    },
                    {
                        "first": "Shulong",
                        "middle": [],
                        "last": "Tan",
                        "suffix": ""
                    },
                    {
                        "first": "Chun",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Can",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Hao",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Lijun",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaofei",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the ACM SIGMM International Conference on Multimedia",
                "volume": "",
                "issue": "",
                "pages": "391--400",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jiajun Bu, Shulong Tan, Chun Chen, Can Wang, Hao Wu, Lijun Zhang, and Xiaofei He. 2010. Music rec- ommendation by unified hypergraph:combining so- cial media information and music content. In Pro- ceedings of the ACM SIGMM International Con- ference on Multimedia, Amsterdam, Netherlands, pages 391-400.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Attsum: Joint learning of focusing and summarization with neural attention",
                "authors": [
                    {
                        "first": "Ziqiang",
                        "middle": [],
                        "last": "Cao",
                        "suffix": ""
                    },
                    {
                        "first": "Wenjie",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Sujian",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Furu",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Yanran",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1604.00125"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ziqiang Cao, Wenjie Li, Sujian Li, Furu Wei, and Yan- ran Li. 2016. Attsum: Joint learning of focusing and summarization with neural attention. arXiv preprint arXiv:1604.00125.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "The use of mmr, diversity-based reranking for reordering documents and producing summaries",
                "authors": [
                    {
                        "first": "Jaime",
                        "middle": [],
                        "last": "Carbonell",
                        "suffix": ""
                    },
                    {
                        "first": "Jade",
                        "middle": [],
                        "last": "Goldstein",
                        "suffix": ""
                    }
                ],
                "year": 1998,
                "venue": "Proceedings of the 21st International ACM SIGIR Conference on Research and Development in Information Retrieval",
                "volume": "",
                "issue": "",
                "pages": "335--336",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jaime Carbonell and Jade Goldstein. 1998. The use of mmr, diversity-based reranking for reordering doc- uments and producing summaries. In Proceedings of the 21st International ACM SIGIR Conference on Research and Development in Information Re- trieval, New York, USA, pages 335-336.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Summarization of related work through citations",
                "authors": [
                    {
                        "first": "Jingqiang",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Zhuge",
                        "middle": [],
                        "last": "Hai",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the 12th IEEE SKG International Conference on Semantics, Knowledge and Grids",
                "volume": "",
                "issue": "",
                "pages": "54--61",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jingqiang Chen and Zhuge Hai. 2016. Summarization of related work through citations. In Proceedings of the 12th IEEE SKG International Conference on Semantics, Knowledge and Grids, Beijing, China, pages 54-61.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Distraction-based neural networks for modeling documents",
                "authors": [
                    {
                        "first": "Qian",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaodan",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    },
                    {
                        "first": "Si",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Si",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Hui",
                        "middle": [],
                        "last": "Jiang",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the ACM IJCAI International Joint Conference on Artificial Intelligence",
                "volume": "",
                "issue": "",
                "pages": "2754--2760",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Qian Chen, Xiaodan Zhu, Si Wei, Si Wei, and Hui Jiang. 2016. Distraction-based neural networks for modeling documents. In Proceedings of the ACM IJCAI International Joint Conference on Artificial Intelligence, New York, USA, pages 2754-2760.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Neural summarization by extracting sentences and words",
                "authors": [
                    {
                        "first": "Jianpeng",
                        "middle": [],
                        "last": "Cheng",
                        "suffix": ""
                    },
                    {
                        "first": "Mirella",
                        "middle": [],
                        "last": "Lapata",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the 54th ACL Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jianpeng Cheng and Mirella Lapata. 2016. Neural summarization by extracting sentences and words. In Proceedings of the 54th ACL Annual Meeting of the Association for Computational Linguistics, Berlin, Germany.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Abstractive sentence summarization with attentive recurrent neural networks",
                "authors": [
                    {
                        "first": "Sumit",
                        "middle": [],
                        "last": "Chopra",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Auli",
                        "suffix": ""
                    },
                    {
                        "first": "Alexander",
                        "middle": [
                            "M"
                        ],
                        "last": "Rush",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the NAACL Conference of the North American Chapter of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "93--98",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sumit Chopra, Michael Auli, and Alexander M. Rush. 2016. Abstractive sentence summarization with at- tentive recurrent neural networks. In Proceedings of the NAACL Conference of the North American Chapter of the Association for Computational Lin- guistics, San Diego, USA, pages 93-98.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Scientific article summarization using citation-context and article's discourse structure",
                "authors": [
                    {
                        "first": "Arman",
                        "middle": [],
                        "last": "Cohan",
                        "suffix": ""
                    },
                    {
                        "first": "Nazli",
                        "middle": [],
                        "last": "Goharian",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "390--400",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1704.06619"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Arman Cohan and Nazli Goharian. 2017. Scien- tific article summarization using citation-context and article's discourse structure. arXiv preprint arXiv:1704.06619, pages 390-400.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Natural language processing (almost) from scratch",
                "authors": [
                    {
                        "first": "Ronan",
                        "middle": [],
                        "last": "Collobert",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Weston",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Karlen",
                        "suffix": ""
                    },
                    {
                        "first": "Koray",
                        "middle": [],
                        "last": "Kavukcuoglu",
                        "suffix": ""
                    },
                    {
                        "first": "Pavel",
                        "middle": [],
                        "last": "Kuksa",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Journal of Machine Learning Research",
                "volume": "12",
                "issue": "1",
                "pages": "2493--2537",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ronan Collobert, Jason Weston, Michael Karlen, Ko- ray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12(1):2493-2537.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Towards automated related work summarization",
                "authors": [
                    {
                        "first": "Duy",
                        "middle": [],
                        "last": "Vu",
                        "suffix": ""
                    },
                    {
                        "first": "Hoang",
                        "middle": [],
                        "last": "Cong",
                        "suffix": ""
                    },
                    {
                        "first": "Min",
                        "middle": [
                            "Yen"
                        ],
                        "last": "Kan",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the 23rd ACM COLING International Conference on Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "427--435",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Duy Vu Hoang Cong and Min Yen Kan. 2010. Towards automated related work summarization. In Pro- ceedings of the 23rd ACM COLING International Conference on Computational Linguistics, Beijing, China, pages 427-435.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Differential evolution: A survey of the state-of-the-art",
                "authors": [
                    {
                        "first": "Swagatam",
                        "middle": [],
                        "last": "Das",
                        "suffix": ""
                    },
                    {
                        "first": "Ponnuthurai",
                        "middle": [],
                        "last": "Nagaratnam Suganthan",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "IEEE Transactions on Evolutionary Computation",
                "volume": "15",
                "issue": "1",
                "pages": "4--31",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Swagatam Das and Ponnuthurai Nagaratnam Sugan- than. 2011. Differential evolution: A survey of the state-of-the-art. IEEE Transactions on Evolutionary Computation, 15(1):4-31.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Lexrank: graphbased lexical centrality as salience in text summarization",
                "authors": [
                    {
                        "first": "Radev",
                        "middle": [],
                        "last": "Erkan",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Dragomir",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Journal of Qiqihar Junior Teachers College",
                "volume": "22",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Erkan, Radev, and R Dragomir. 2004. Lexrank: graph- based lexical centrality as salience in text summa- rization. Journal of Qiqihar Junior Teachers Col- lege, 22:2004.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "node2vec: Scalable feature learning for networks",
                "authors": [
                    {
                        "first": "Aditya",
                        "middle": [],
                        "last": "Grover",
                        "suffix": ""
                    },
                    {
                        "first": "Jure",
                        "middle": [],
                        "last": "Leskovec",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
                "volume": "",
                "issue": "",
                "pages": "855--864",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for networks. In Proceed- ings of the 22nd ACM SIGKDD International Con- ference on Knowledge Discovery and Data Mining, San Francisco, Usa, pages 855-864.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Automatic feature generation on heterogeneous graph for music recommendation",
                "authors": [
                    {
                        "first": "Chun",
                        "middle": [],
                        "last": "Guo",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaozhong",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, Santiago",
                "volume": "",
                "issue": "",
                "pages": "807--810",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chun Guo and Xiaozhong Liu. 2015. Automatic fea- ture generation on heterogeneous graph for music recommendation. In Proceedings of the 38th In- ternational ACM SIGIR Conference on Research and Development in Information Retrieval, Santi- ago, Chile, pages 807-810.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Scientific metadata quality enhancement for scholarly publications",
                "authors": [
                    {
                        "first": "Chun",
                        "middle": [],
                        "last": "Guo",
                        "suffix": ""
                    },
                    {
                        "first": "Jinsong",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaozhong",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chun Guo, Jinsong Zhang, and Xiaozhong Liu. 2013. Scientific metadata quality enhancement for schol- arly publications. Ischools.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Long short-term memory",
                "authors": [
                    {
                        "first": "Sepp",
                        "middle": [],
                        "last": "Hochreiter",
                        "suffix": ""
                    },
                    {
                        "first": "Jrgen",
                        "middle": [],
                        "last": "Schmidhuber",
                        "suffix": ""
                    }
                ],
                "year": 1997,
                "venue": "Neural Computation",
                "volume": "9",
                "issue": "8",
                "pages": "1735--1780",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sepp Hochreiter and Jrgen Schmidhuber. 1997. Long short-term memory. Neural Computation, 9(8):1735-1780.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Automatic generation of related work sections in scientific papers: an optimization approach",
                "authors": [
                    {
                        "first": "Yue",
                        "middle": [],
                        "last": "Hu",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaojun",
                        "middle": [],
                        "last": "Wan",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of the ACL EMNLP Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "1624--1633",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yue Hu and Xiaojun Wan. 2014. Automatic gener- ation of related work sections in scientific papers: an optimization approach. In Proceedings of the ACL EMNLP Conference on Empirical Methods in Natural Language Processing, Doha, Qatar, pages 1624-1633.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Learning deep structured semantic models for web search using clickthrough data",
                "authors": [
                    {
                        "first": "Po-Sen",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaodong",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Jianfeng",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Li",
                        "middle": [],
                        "last": "Deng",
                        "suffix": ""
                    },
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Acero",
                        "suffix": ""
                    },
                    {
                        "first": "Larry",
                        "middle": [],
                        "last": "Heck",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of the 22nd ACM CIKM international Conference on Information & Knowledge Management",
                "volume": "",
                "issue": "",
                "pages": "2333--2338",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry Heck. 2013. Learning deep structured semantic models for web search using clickthrough data. In Proceedings of the 22nd ACM CIKM international Conference on Informa- tion & Knowledge Management, San Francisco, USA, pages 2333-2338.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Generation of topic evolution trees from heterogeneous bibliographic networks",
                "authors": [
                    {
                        "first": "Scott",
                        "middle": [],
                        "last": "Jensen",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaozhong",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Yingying",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Stasa",
                        "middle": [],
                        "last": "Milojevic",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Journal of Informetrics",
                "volume": "10",
                "issue": "2",
                "pages": "606--621",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Scott Jensen, Xiaozhong Liu, Yingying Yu, and Stasa Milojevic. 2016. Generation of topic evolution trees from heterogeneous bibliographic networks. Jour- nal of Informetrics, 10(2):606-621.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Convolutional neural networks for sentence classification",
                "authors": [
                    {
                        "first": "Yoon",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yoon Kim. 2014. Convolutional neural networks for sentence classification. Eprint Arxiv.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Adam: a method for stochastic optimization",
                "authors": [
                    {
                        "first": "Diederik",
                        "middle": [],
                        "last": "Kingma",
                        "suffix": ""
                    },
                    {
                        "first": "Jimmy",
                        "middle": [],
                        "last": "Ba",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Computer Science",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Diederik Kingma and Jimmy Ba. 2014. Adam: a method for stochastic optimization. Computer Sci- ence.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Automatic evaluation of summaries using n-gram cooccurrence statistics",
                "authors": [
                    {
                        "first": "Chin",
                        "middle": [],
                        "last": "Yew",
                        "suffix": ""
                    },
                    {
                        "first": "Lin",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Eduard",
                        "middle": [],
                        "last": "Hovy",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Proceedings of the NAACL The Annual Conference of the North American Chapter of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "71--78",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chin Yew Lin and Eduard Hovy. 2003. Auto- matic evaluation of summaries using n-gram co- occurrence statistics. In Proceedings of the NAACL The Annual Conference of the North American Chapter of the Association for Computational Lin- guistics, Stroudsburg, USA, pages 71-78.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "The automatic creation of literature abstracts",
                "authors": [
                    {
                        "first": "H",
                        "middle": [
                            "P"
                        ],
                        "last": "Luhn",
                        "suffix": ""
                    }
                ],
                "year": 1958,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "H. P. Luhn. 1958. The automatic creation of literature abstracts. IBM Corp.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Rhetorical structure theory: Toward a functional theory of text organization",
                "authors": [
                    {
                        "first": "William",
                        "middle": [
                            "C"
                        ],
                        "last": "Mann",
                        "suffix": ""
                    },
                    {
                        "first": "Sandra",
                        "middle": [
                            "A"
                        ],
                        "last": "Thompson",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Text & Talk",
                "volume": "8",
                "issue": "3",
                "pages": "243--281",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "William C. Mann and Sandra A. Thompson. 2009. Rhetorical structure theory: Toward a functional the- ory of text organization. Text & Talk, 8(3):243-281.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Efficient estimation of word representations in vector space",
                "authors": [
                    {
                        "first": "Tomas",
                        "middle": [],
                        "last": "Mikolov",
                        "suffix": ""
                    },
                    {
                        "first": "Kai",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Greg",
                        "middle": [],
                        "last": "Corrado",
                        "suffix": ""
                    },
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Dean",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Computer Science",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word represen- tations in vector space. Computer Science.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Sequence-to-sequence rnns for text summarization",
                "authors": [
                    {
                        "first": "Ramesh",
                        "middle": [],
                        "last": "Nallapati",
                        "suffix": ""
                    },
                    {
                        "first": "Bing",
                        "middle": [],
                        "last": "Xiang",
                        "suffix": ""
                    },
                    {
                        "first": "Bowen",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the International Conference on Learning Representations, Workshop track",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ramesh Nallapati, Bing Xiang, and Bowen Zhou. 2016a. Sequence-to-sequence rnns for text summa- rization. In Proceedings of the International Confer- ence on Learning Representations, Workshop track, San Juan, Puerto Rico.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Summarunner: A recurrent neural network based sequence model for extractive summarization of documents",
                "authors": [
                    {
                        "first": "Ramesh",
                        "middle": [],
                        "last": "Nallapati",
                        "suffix": ""
                    },
                    {
                        "first": "Feifei",
                        "middle": [],
                        "last": "Zhai",
                        "suffix": ""
                    },
                    {
                        "first": "Bowen",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1611.04230v1"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ramesh Nallapati, Feifei Zhai, and Bowen Zhou. 2016b. Summarunner: A recurrent neural network based sequence model for extractive summarization of documents. arXiv preprint arXiv:1611.04230v1.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "Cicero Nogueira Dos Santos, Caglar Gulcehre, and Bing Xiang. 2016c. Abstractive text summarization using sequenceto-sequence rnns and beyond",
                "authors": [
                    {
                        "first": "Ramesh",
                        "middle": [],
                        "last": "Nallapati",
                        "suffix": ""
                    },
                    {
                        "first": "Bowen",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1602.06023v5"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ramesh Nallapati, Bowen Zhou, Cicero Nogueira Dos Santos, Caglar Gulcehre, and Bing Xiang. 2016c. Abstractive text summarization using sequence- to-sequence rnns and beyond. arXiv preprint arXiv:1602.06023v5.",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "Joint latent topic models for text and citations",
                "authors": [
                    {
                        "first": "Ramesh",
                        "middle": [
                            "M"
                        ],
                        "last": "Nallapati",
                        "suffix": ""
                    },
                    {
                        "first": "Amr",
                        "middle": [],
                        "last": "Ahmed",
                        "suffix": ""
                    },
                    {
                        "first": "Eric",
                        "middle": [
                            "P"
                        ],
                        "last": "Xing",
                        "suffix": ""
                    },
                    {
                        "first": "William",
                        "middle": [
                            "W"
                        ],
                        "last": "Cohen",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
                "volume": "",
                "issue": "",
                "pages": "542--550",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ramesh M. Nallapati, Amr Ahmed, Eric P. Xing, and William W. Cohen. 2008. Joint latent topic models for text and citations. In Proceedings of the 14th ACM SIGKDD International Conference on Knowl- edge Discovery and Data Mining, Las Vegas, Usa, pages 542-550.",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "Neural extractive summarization with side information",
                "authors": [
                    {
                        "first": "Shashi",
                        "middle": [],
                        "last": "Narayan",
                        "suffix": ""
                    },
                    {
                        "first": "Nikos",
                        "middle": [],
                        "last": "Papasarantopoulos",
                        "suffix": ""
                    },
                    {
                        "first": "Shay",
                        "middle": [
                            "B"
                        ],
                        "last": "Cohen",
                        "suffix": ""
                    },
                    {
                        "first": "Mirella",
                        "middle": [],
                        "last": "Lapata",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1704.04530"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Shashi Narayan, Nikos Papasarantopoulos, Shay B. Cohen, and Mirella Lapata. 2017. Neural extrac- tive summarization with side information. arXiv preprint arXiv:1704.04530.",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "The impact of frequency on summarization",
                "authors": [
                    {
                        "first": "Ani",
                        "middle": [],
                        "last": "Nenkova",
                        "suffix": ""
                    },
                    {
                        "first": "Lucy",
                        "middle": [],
                        "last": "Vanderwende",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ani Nenkova and Lucy Vanderwende. 2005. The im- pact of frequency on summarization. Microsoft Re- search.",
                "links": null
            },
            "BIBREF34": {
                "ref_id": "b34",
                "title": "The pagerank citation ranking : Bringing order to the web, online manuscript",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Page",
                        "suffix": ""
                    }
                ],
                "year": 1998,
                "venue": "",
                "volume": "9",
                "issue": "",
                "pages": "1--14",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "L Page. 1998. The pagerank citation ranking : Bring- ing order to the web, online manuscript. Stanford Digital Libraries Working Paper, 9(1):1-14.",
                "links": null
            },
            "BIBREF35": {
                "ref_id": "b35",
                "title": "A neural attention model for abstractive sentence summarization",
                "authors": [
                    {
                        "first": "Sumit",
                        "middle": [],
                        "last": "Alexander M Rush",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Chopra",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Weston",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the ACL EMNLP Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "379--389",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alexander M Rush, Sumit Chopra, and Jason Weston. 2015. A neural attention model for abstractive sen- tence summarization. In Proceedings of the ACL EMNLP Conference on Empirical Methods in Nat- ural Language Processing, Lisbon, Portugal, pages 379-389.",
                "links": null
            },
            "BIBREF36": {
                "ref_id": "b36",
                "title": "metapath2vec: Scalable representation learning for heterogeneous networks",
                "authors": [
                    {
                        "first": "Ananthram",
                        "middle": [],
                        "last": "Swami",
                        "suffix": ""
                    },
                    {
                        "first": "Ananthram",
                        "middle": [],
                        "last": "Swami",
                        "suffix": ""
                    },
                    {
                        "first": "Ananthram",
                        "middle": [],
                        "last": "Swami",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
                "volume": "",
                "issue": "",
                "pages": "135--144",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ananthram Swami, Ananthram Swami, and Anan- thram Swami. 2017. metapath2vec: Scalable rep- resentation learning for heterogeneous networks. In Proceedings of the 23rd ACM SIGKDD Inter- national Conference on Knowledge Discovery and Data Mining, Halifax, Canada, pages 135-144.",
                "links": null
            },
            "BIBREF37": {
                "ref_id": "b37",
                "title": "Abstractive document summarization with a graph-based attentional neural model",
                "authors": [
                    {
                        "first": "Jiwei",
                        "middle": [],
                        "last": "Tan",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaojun",
                        "middle": [],
                        "last": "Wan",
                        "suffix": ""
                    },
                    {
                        "first": "Jianguo",
                        "middle": [],
                        "last": "Xiao",
                        "suffix": ""
                    },
                    {
                        "first": "Jiwei",
                        "middle": [],
                        "last": "Tan",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaojun",
                        "middle": [],
                        "last": "Wan",
                        "suffix": ""
                    },
                    {
                        "first": "Jianguo",
                        "middle": [],
                        "last": "Xiao",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the 55th ACL Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "1171--1181",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jiwei Tan, Xiaojun Wan, Jianguo Xiao, Jiwei Tan, Xi- aojun Wan, and Jianguo Xiao. 2017. Abstractive document summarization with a graph-based atten- tional neural model. In Proceedings of the 55th ACL Annual Meeting of the Association for Computa- tional Linguistics, Vancouver, Canada, pages 1171- 1181.",
                "links": null
            },
            "BIBREF38": {
                "ref_id": "b38",
                "title": "Algorithms for estimating relative importance in networks",
                "authors": [
                    {
                        "first": "Scott",
                        "middle": [],
                        "last": "White",
                        "suffix": ""
                    },
                    {
                        "first": "Padhraic",
                        "middle": [],
                        "last": "Smyth",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Proceedings of the 9th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
                "volume": "",
                "issue": "",
                "pages": "266--275",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Scott White and Padhraic Smyth. 2003. Algorithms for estimating relative importance in networks. In Proceedings of the 9th ACM SIGKDD International Conference on Knowledge Discovery and Data Min- ing, Washington, USA, pages 266-275.",
                "links": null
            },
            "BIBREF39": {
                "ref_id": "b39",
                "title": "Citation sentence identification and classification for related work summarization",
                "authors": [
                    {
                        "first": "H",
                        "middle": [],
                        "last": "Dwi",
                        "suffix": ""
                    },
                    {
                        "first": "Imaduddin",
                        "middle": [],
                        "last": "Widyantoro",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Amin",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of the ICACSIS International Conference on Advanced Computer Science and Information Systems",
                "volume": "",
                "issue": "",
                "pages": "291--296",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dwi H Widyantoro and Imaduddin Amin. 2014. Ci- tation sentence identification and classification for related work summarization. In Proceedings of the ICACSIS International Conference on Advanced Computer Science and Information Systems, pages 291-296.",
                "links": null
            },
            "BIBREF40": {
                "ref_id": "b40",
                "title": "Random walk and feedback on scholarly network",
                "authors": [
                    {
                        "first": "Yingying",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaozhong",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Zhuoren",
                        "middle": [],
                        "last": "Jiang",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the 1st ACM GSB@SIGIR International Workshop on Graph Search and Beyond",
                "volume": "",
                "issue": "",
                "pages": "33--37",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yingying Yu, Xiaozhong Liu, and Zhuoren Jiang. 2015. Random walk and feedback on scholarly network. In Proceedings of the 1st ACM GSB@SIGIR Inter- national Workshop on Graph Search and Beyond, Santiago, Chile, pages 33-37.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 1: Heterogeneous bibliography graph.",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Figure 2: Framework of our seq2seq summarizer.",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "Figure 3: Number of extracted words on each reference cluster under different attention factors.",
                "uris": null,
                "fig_num": "3",
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table><tr><td>Authors</td><td>Number of papers</td></tr><tr><td>Cong and Kan (2010)</td><td>20</td></tr><tr><td>Hu and Wan (2014)</td><td>1,050</td></tr><tr><td>Widyantoro and Amin (2014)</td><td>50</td></tr><tr><td>Chen and Hai (2016)</td><td>3</td></tr><tr><td colspan=\"2\">apply several regression models to learn the im-</td></tr><tr><td colspan=\"2\">portance of each sentence. Similarly, Widyan-</td></tr><tr><td colspan=\"2\">toro and Amin (2014) transform the summariza-</td></tr><tr><td colspan=\"2\">tion problem into classifying rhetorical categories</td></tr><tr><td colspan=\"2\">of sentences, where each sentence is represented</td></tr><tr><td colspan=\"2\">as a feature vector containing word frequency, sen-</td></tr><tr><td colspan=\"2\">tence length and etc. Most recently, Chen and</td></tr><tr><td colspan=\"2\">Hai (2016) construct a graph of representative key-</td></tr><tr><td colspan=\"2\">words, in which a minimum steiner tree is figured</td></tr><tr><td colspan=\"2\">out to guide the summarization as finding the least</td></tr><tr><td colspan=\"2\">number of sentences to cover the discriminated</td></tr><tr><td colspan=\"2\">nodes. In general, compared to traditional sum-</td></tr><tr><td colspan=\"2\">maries, the automatic related work summarization</td></tr><tr><td colspan=\"2\">receives less concerns over the past. However,</td></tr><tr><td colspan=\"2\">these existing solutions cannot work without man-</td></tr><tr><td colspan=\"2\">ual intervention, which limits the application scale</td></tr><tr><td colspan=\"2\">to an extremely small size (see Table 1).</td></tr><tr><td colspan=\"2\">The earliest seq2seq summarizer stems from</td></tr><tr><td>Rush et al. (</td><td/></tr></table>",
                "type_str": "table",
                "text": "exploit probabilistic latent semantic indexing to split candidate texts into different topic-biased parts, then Data scales of previous studies on automatic related work summarization.",
                "html": null,
                "num": null
            },
            "TABREF1": {
                "content": "<table><tr><td>t</td><td/><td/><td/><td/><td/><td>...</td><td/><td/><td>node embedding</td><td>\u00a3</td><td>( ) t</td><td>\u00a4</td><td>( ) t 1 r</td><td>\u00a4</td><td>( ) t 2 r</td><td>...</td><td>\u00a5</td><td>( ) t r n</td></tr><tr><td>t 1 r</td><td>h</td><td>t 1</td><td>h</td><td colspan=\"2\">t 2</td><td>...</td><td colspan=\"2\">t h i</td><td/><td/><td/><td>t 1 y</td><td>y</td><td>t 2</td><td>...</td><td>t y i</td></tr><tr><td>t 2 r</td><td colspan=\"2\">t h i+ 1</td><td colspan=\"2\">t h i+</td><td>2</td><td>...</td><td/><td/><td colspan=\"3\">context</td><td>t y i+ 1</td><td>t y i+</td><td>2</td><td>...</td></tr><tr><td>...</td><td/><td colspan=\"5\">hidden state</td><td/><td/><td colspan=\"3\">vector</td><td>binary decision</td></tr><tr><td>t r n</td><td colspan=\"2\">t h j\u00a2 1</td><td colspan=\"3\">t h j</td><td>...</td><td colspan=\"2\">t h m</td><td/><td/><td/><td>t y j\u00a2 1</td><td>t y j</td><td>...</td><td>t y m</td></tr><tr><td/><td/><td/><td/><td/><td colspan=\"4\">max-over-time</td><td/><td/><td/></tr><tr><td/><td/><td/><td/><td/><td/><td colspan=\"3\">pooling</td><td/><td/><td/></tr><tr><td/><td/><td/><td/><td/><td/><td/><td/><td/><td/><td/><td colspan=\"2\">convolution</td></tr><tr><td/><td/><td/><td/><td/><td/><td/><td/><td/><td/><td/><td/><td>\u00a1</td><td>( ) t ,1 w j</td></tr><tr><td/><td/><td/><td/><td/><td/><td/><td/><td/><td/><td/><td/><td>( ) t ,2 w j</td></tr><tr><td/><td/><td colspan=\"4\">average</td><td/><td>\u00a1</td><td>( ) t s j</td><td>feature map</td><td/><td/><td>...</td></tr><tr><td colspan=\"3\">sentence</td><td/><td/><td/><td/><td/><td/><td/><td/><td/><td>( ) t ,6 w j</td></tr><tr><td colspan=\"3\">embedding</td><td/><td/><td/><td/><td/><td/><td/><td/><td/><td>word embedding</td></tr><tr><td/><td/><td/><td/><td/><td/><td/><td/><td/><td/><td/><td/><td>Hierarchical Encoder</td></tr></table>",
                "type_str": "table",
                "text": "In practice, we use multiple kernels with various widths to produce a group of embeddings for each",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table><tr><td>Methods</td><td colspan=\"3\">ROUGE-1 ROUGE-2 ROUGE-L</td></tr><tr><td>P. void</td><td>26.85*</td><td>6.38*</td><td>14.22*</td></tr><tr><td>P. S</td><td>26.98*</td><td>6.48*</td><td>14.36*</td></tr><tr><td>P. S+N</td><td>27.29*</td><td>6.65*</td><td>14.43*</td></tr><tr><td>P. S+N+Rt</td><td>27.63*</td><td>6.72*</td><td>14.46*</td></tr><tr><td>P. S+N+Rtog</td><td>27.82*</td><td>7.00*</td><td>14.55*</td></tr><tr><td>P. S+N+Rteg</td><td>28.56*</td><td>7.40</td><td>14.70*</td></tr><tr><td colspan=\"2\">P. S+N+Rteg+EUD 29.18</td><td>7.63</td><td>14.89</td></tr><tr><td>Luhn</td><td>25.76*</td><td>5.08*</td><td>13.50*</td></tr><tr><td>MMR</td><td>25.55*</td><td>5.14*</td><td>13.99*</td></tr><tr><td>LexRank</td><td>25.07*</td><td>5.12*</td><td>13.95*</td></tr><tr><td>SumBasic</td><td>28.01*</td><td>5.44*</td><td>13.93*</td></tr><tr><td>NltkSum</td><td>28.07*</td><td>6.36*</td><td>14.87</td></tr><tr><td>PointerNet</td><td>27.06*</td><td>6.53*</td><td>14.41*</td></tr><tr><td colspan=\"4\">* indicates Wilcoxon signed-rank test p &lt; 0.01, compared with P. S+N+Rteg+EUD</td></tr></table>",
                "type_str": "table",
                "text": "visualizes the number of extracted words on each reference Rouge evaluation (%) on 8,080 papers from ACM digital library.",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table><tr><td>Methods</td><td>1st</td><td>2nd 3rd</td><td>4th</td><td>5th</td><td>6th</td><td>7th</td><td>Mean Ranking</td></tr><tr><td>Luhn</td><td colspan=\"6\">0.04 0.07 0.09 0.13 0.17 0.23 0.29</td><td>5.26</td></tr><tr><td>MMR</td><td colspan=\"6\">0.05 0.07 0.11 0.16 0.19 0.22 0.20</td><td>4.82</td></tr><tr><td>LexRank</td><td colspan=\"6\">0.06 0.09 0.11 0.14 0.17 0.19 0.27</td><td>4.93</td></tr><tr><td>SumBasic</td><td colspan=\"6\">0.09 0.13 0.18 0.18 0.18 0.15 0.10</td><td>4.10</td></tr><tr><td>NltkSum</td><td colspan=\"6\">0.21 0.21 0.20 0.15 0.10 0.07 0.04</td><td>3.00</td></tr><tr><td>PointerNet</td><td colspan=\"6\">0.14 0.20 0.18 0.15 0.13 0.11 0.08</td><td>3.54</td></tr><tr><td colspan=\"7\">P. S+N+Rteg+EUD 0.40 0.22 0.14 0.09 0.06 0.04 0.02</td><td>2.34</td></tr></table>",
                "type_str": "table",
                "text": "Table3displays how often raters rank each summarizer as the 1st, 2nd and so on, in terms of Human evaluation (proportion) on 35 papers with more than 30 references in the dataset.",
                "html": null,
                "num": null
            }
        }
    }
}