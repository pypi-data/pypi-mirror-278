{
    "paper_id": "P18-1256",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:33:03.766168Z"
    },
    "title": "Let's do it \"again\": A First Computational Approach to Detecting Adverbial Presupposition Triggers",
    "authors": [
        {
            "first": "Andre",
            "middle": [],
            "last": "Cianflone",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "MILA McGill University Montreal",
                "location": {
                    "settlement": "Montreal",
                    "region": "QC, QC",
                    "country": "Canada, Canada"
                }
            },
            "email": "andre.cianflone@mail"
        },
        {
            "first": "Yulan",
            "middle": [],
            "last": "Feng",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "MILA McGill University Montreal",
                "location": {
                    "settlement": "Montreal",
                    "region": "QC, QC",
                    "country": "Canada, Canada"
                }
            },
            "email": "yulan.feng@mail"
        },
        {
            "first": "Jad",
            "middle": [],
            "last": "Kabbara",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "MILA McGill University Montreal",
                "location": {
                    "settlement": "Montreal",
                    "region": "QC, QC",
                    "country": "Canada, Canada"
                }
            },
            "email": ""
        },
        {
            "first": "Jackie",
            "middle": [],
            "last": "Chi",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "MILA McGill University Montreal",
                "location": {
                    "settlement": "Montreal",
                    "region": "QC, QC",
                    "country": "Canada, Canada"
                }
            },
            "email": ""
        },
        {
            "first": "Kit",
            "middle": [],
            "last": "Cheung",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "MILA McGill University Montreal",
                "location": {
                    "settlement": "Montreal",
                    "region": "QC, QC",
                    "country": "Canada, Canada"
                }
            },
            "email": "jcheung@cs.mcgill.ca"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "We introduce the task of predicting adverbial presupposition triggers such as also and again. Solving such a task requires detecting recurring or similar events in the discourse context, and has applications in natural language generation tasks such as summarization and dialogue systems. We create two new datasets for the task, derived from the Penn Treebank and the Annotated English Gigaword corpora, as well as a novel attention mechanism tailored to this task. Our attention mechanism augments a baseline recurrent neural network without the need for additional trainable parameters, minimizing the added computational cost of our mechanism. We demonstrate that our model statistically outperforms a number of baselines, including an LSTM-based language model.",
    "pdf_parse": {
        "paper_id": "P18-1256",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "We introduce the task of predicting adverbial presupposition triggers such as also and again. Solving such a task requires detecting recurring or similar events in the discourse context, and has applications in natural language generation tasks such as summarization and dialogue systems. We create two new datasets for the task, derived from the Penn Treebank and the Annotated English Gigaword corpora, as well as a novel attention mechanism tailored to this task. Our attention mechanism augments a baseline recurrent neural network without the need for additional trainable parameters, minimizing the added computational cost of our mechanism. We demonstrate that our model statistically outperforms a number of baselines, including an LSTM-based language model.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "In pragmatics, presuppositions are assumptions or beliefs in the common ground between discourse participants when an utterance is made (Frege, 1892; Strawson, 1950; Stalnaker, 1973 Stalnaker, , 1998)) , and are ubiquitous in naturally occurring discourses (Beaver and Geurts, 2014) . Presuppositions underly spoken statements and written sentences and understanding them facilitates smooth communication. We refer to expressions that indicate the presence of presuppositions as presupposition triggers. These include definite descriptions, factive verbs and certain adverbs, among others. For example, consider the following statements:",
                "cite_spans": [
                    {
                        "start": 136,
                        "end": 149,
                        "text": "(Frege, 1892;",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 150,
                        "end": 165,
                        "text": "Strawson, 1950;",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 166,
                        "end": 181,
                        "text": "Stalnaker, 1973",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 182,
                        "end": 201,
                        "text": "Stalnaker, , 1998))",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 257,
                        "end": 282,
                        "text": "(Beaver and Geurts, 2014)",
                        "ref_id": "BIBREF1"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "(1) John is going to the restaurant again. * Authors (listed in alphabetical order) contributed equally.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "(2) John has been to the restaurant.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "(1) is only appropriate in the context where ( 2) is held to be true because of the presence of the presupposition trigger again. One distinguishing characteristic of presupposition is that it is unaffected by negation of the presupposing context, unlike other semantic phenomena such as entailment and implicature. The negation of (1), John is not going to the restaurant again., also presupposes (2).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Our focus in this paper is on adverbial presupposition triggers such as again, also and still. Adverbial presupposition triggers indicate the recurrence, continuation, or termination of an event in the discourse context, or the presence of a similar event. In one study of presuppositional triggers in English journalistic texts (Khaleel, 2010) , adverbial triggers were found to be the most commonly occurring presupposition triggers after existential triggers. 1 Despite their frequency, there has been little work on these triggers in the computational literature from a statistical, corpus-driven perspective.",
                "cite_spans": [
                    {
                        "start": 329,
                        "end": 344,
                        "text": "(Khaleel, 2010)",
                        "ref_id": "BIBREF14"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "As a first step towards language technology systems capable of understanding and using presuppositions, we propose to investigate the detection of contexts in which these triggers can be used. This task constitutes an interesting testing ground for pragmatic reasoning, because the cues that are indicative of contexts containing recurring or similar events are complex and often span more than one sentence, as illustrated in Sentences (1) and (2). Moreover, such a task has immediate practical consequences. For example, in language generation applications such as summarization and dialogue systems, adding presuppositional triggers in contextually appropriate loca-1 Presupposition of existence are triggered by possessive constructions, names or definite noun phrases.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "tions can improve the readability and coherence of the generated output.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "We create two datasets based on the Penn Treebank corpus (Marcus et al., 1993) and the English Gigaword corpus (Graff et al., 2007) , extracting contexts that include presupposition triggers as well as other similar contexts that do not, in order to form a binary classification task. In creating our datasets, we consider a set of five target adverbs: too, again, also, still, and yet. We focus on these adverbs in our investigation because these triggers are well known in the existing linguistic literature and commonly triggering presuppositions. We control for a number of potential confounding factors, such as class balance, and the syntactic governor of the triggering adverb, so that models cannot exploit these correlating factors without any actual understanding of the presuppositional properties of the context.",
                "cite_spans": [
                    {
                        "start": 57,
                        "end": 78,
                        "text": "(Marcus et al., 1993)",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 111,
                        "end": 131,
                        "text": "(Graff et al., 2007)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "We test a number of standard baseline classifiers on these datasets, including a logistic regression model and deep learning methods based on recurrent neural networks (RNN) and convolutional neural networks (CNN).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In addition, we investigate the potential of attention-based deep learning models for detecting adverbial triggers. Attention is a promising approach to this task because it allows a model to weigh information from multiple points in the previous context and infer long-range dependencies in the data (Bahdanau et al., 2015) . For example, the model could learn to detect multiple instances involving John and restaurants, which would be a good indication that again is appropriate in that context. Also, an attention-based RNN has achieved success in predicting article definiteness, which involves another class of presupposition triggers (Kabbara et al., 2016) .",
                "cite_spans": [
                    {
                        "start": 301,
                        "end": 324,
                        "text": "(Bahdanau et al., 2015)",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 641,
                        "end": 663,
                        "text": "(Kabbara et al., 2016)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "As another contribution, we introduce a new weighted pooling attention mechanism designed for predicting adverbial presupposition triggers. Our attention mechanism allows for a weighted averaging of our RNN hidden states where the weights are informed by the inputs, as opposed to a simple unweighted averaging. Our model uses a form of self-attention (Paulus et al., 2018; Vaswani et al., 2017) , where the input sequence acts as both the attention mechanism's query and key/value. Unlike other attention models, instead of simply averaging the scores to be weighted, our approach aggregates (learned) attention scores by learning a reweighting scheme of those scores through another level (dimension) of attention. Additionally, our mechanism does not introduce any new parameters when compared to our LSTM baseline, reducing its computational impact.",
                "cite_spans": [
                    {
                        "start": 352,
                        "end": 373,
                        "text": "(Paulus et al., 2018;",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 374,
                        "end": 395,
                        "text": "Vaswani et al., 2017)",
                        "ref_id": "BIBREF32"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "We compare our model using the novel attention mechanism against the baseline classifiers in terms of prediction accuracy. Our model outperforms these baselines for most of the triggers on the two datasets, achieving 82.42% accuracy on predicting the adverb \"also\" on the Gigaword dataset.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The contributions of this work are as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "1. We introduce the task of predicting adverbial presupposition triggers.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "2. We present new datasets for the task of detecting adverbial presupposition triggers, with a data extraction method that can be applied to other similar pre-processing tasks.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "3. We develop a new attention mechanism in an RNN architecture that is appropriate for the prediction of adverbial presupposition triggers, and show that its use results in better prediction performance over a number of baselines without introducing additional parameters.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "2 Related Work",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The discussion of presupposition can be traced back to Frege's work on the philosophy of language (Frege, 1892) , which later leads to the most commonly accepted view of presupposition called the Frege-Strawson theory (Kaplan, 1970; Strawson, 1950) . In this view, presuppositions are preconditions for sentences/statements to be true or false. To the best of our knowledge, there is no previous computational work that directly investigates adverbial presupposition. However in the fields of semantics and pragmatics, there exist linguistic studies on presupposition that involve adverbs such as \"too\" and \"again\" (e.g., (Blutner et al., 2003) , (Kang, 2012) ) as a pragmatic presupposition trigger. Also relevant to our work is (Kabbara et al., 2016) , which proposes using an attention-based LSTM network to predict noun phrase definiteness in English. Their work demonstrates the ability of these attention-based models to pick up on contextual cues for pragmatic reasoning.",
                "cite_spans": [
                    {
                        "start": 98,
                        "end": 111,
                        "text": "(Frege, 1892)",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 218,
                        "end": 232,
                        "text": "(Kaplan, 1970;",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 233,
                        "end": 248,
                        "text": "Strawson, 1950)",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 622,
                        "end": 644,
                        "text": "(Blutner et al., 2003)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 647,
                        "end": 659,
                        "text": "(Kang, 2012)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 730,
                        "end": 752,
                        "text": "(Kabbara et al., 2016)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Presupposition and pragmatic reasoning",
                "sec_num": "2.1"
            },
            {
                "text": "Many different classes of construction can trigger presupposition in an utterance, this includes but is not limited to stressed constituents, factive verbs, and implicative verbs (Zare et al., 2012) . In this work, we focus on the class of adverbial presupposition triggers.",
                "cite_spans": [
                    {
                        "start": 179,
                        "end": 198,
                        "text": "(Zare et al., 2012)",
                        "ref_id": "BIBREF33"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Presupposition and pragmatic reasoning",
                "sec_num": "2.1"
            },
            {
                "text": "Our task setup resembles the Cloze test used in psychology (Taylor, 1953; E. B. Coleman, 1968; Earl F. Rankin, 1969) and machine comprehension (Riloff and Thelen, 2000) , which tests text comprehension via a fill-in-the-blanks task. We similarly pre-process our samples such that they are roughly the same length, and have equal numbers of negative samples as positive ones. However, we avoid replacing the deleted words with a blank, so that our model has no clue regarding the exact position of the possibly missing trigger. Another related work on the Children's Book Test (Hill et al., 2015) notes that memories that encode sub-sentential chunks (windows) of informative text seem to be most useful to neural networks when interpreting and modelling language. Their finding inspires us to run initial experiments with different context windows and tune the size of chunks according to the Logistic Regression results on the development set.",
                "cite_spans": [
                    {
                        "start": 59,
                        "end": 73,
                        "text": "(Taylor, 1953;",
                        "ref_id": "BIBREF31"
                    },
                    {
                        "start": 74,
                        "end": 94,
                        "text": "E. B. Coleman, 1968;",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 95,
                        "end": 116,
                        "text": "Earl F. Rankin, 1969)",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 143,
                        "end": 168,
                        "text": "(Riloff and Thelen, 2000)",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 576,
                        "end": 595,
                        "text": "(Hill et al., 2015)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Presupposition and pragmatic reasoning",
                "sec_num": "2.1"
            },
            {
                "text": "In the context of encoder-decoder models, attention weights are usually based on an energy measure of the previous decoder hidden state and encoder hidden states. Many variations on attention computation exist. Sukhbaatar et al. (2015) propose an attention mechanism conditioned on a query and applied to a document. To generate summaries, Paulus et al. (2018) add an attention mechanism in the prediction layer, as opposed to the hidden states. Vaswani et al. (2017) suggest a model which learns an input representation by self-attending over inputs. While these methods are all tailored to their specific tasks, they all inspire our choice of a self-attending mechanism.",
                "cite_spans": [
                    {
                        "start": 211,
                        "end": 235,
                        "text": "Sukhbaatar et al. (2015)",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 340,
                        "end": 360,
                        "text": "Paulus et al. (2018)",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 446,
                        "end": 467,
                        "text": "Vaswani et al. (2017)",
                        "ref_id": "BIBREF32"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Attention",
                "sec_num": "2.2"
            },
            {
                "text": "We extract datasets from two corpora, namely the Penn Treebank (PTB) corpus (Marcus et al., 1993) and a subset (sections 000-760) of the third edition of the English Gigaword corpus (Graff et al., 2007) . For the PTB dataset, we use sections 22 and 23 for testing. For the Gigaword corpus, we use sections 700-760 for testing. For the remaining data, we randomly chose 10% of them for development, and the other 90% for training.",
                "cite_spans": [
                    {
                        "start": 76,
                        "end": 97,
                        "text": "(Marcus et al., 1993)",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 182,
                        "end": 202,
                        "text": "(Graff et al., 2007)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Corpora",
                "sec_num": "3.1"
            },
            {
                "text": "For each dataset, we consider a set of five target adverbs: too, again, also, still, and yet. We choose these five because they are commonly used adverbs that trigger presupposition. Since we are concerned with investigating the capacity of attentional deep neural networks in predicting the presuppositional effects in general, we frame the learning problem as a binary classification for predicting the presence of an adverbial presupposition (as opposed to the identity of the adverb).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Corpora",
                "sec_num": "3.1"
            },
            {
                "text": "On the Gigaword corpus, we consider each adverb separately, resulting in five binary classification tasks. This was not feasible for PTB because of its small size.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Corpora",
                "sec_num": "3.1"
            },
            {
                "text": "Finally, because of the commonalities between the adverbs in presupposing similar events, we create a dataset that unifies all instances of the five adverbs found in the Gigaword corpus, with a label \"1\" indicating the presence of any of these adverbs.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Corpora",
                "sec_num": "3.1"
            },
            {
                "text": "We define a sample in our dataset as a 3-tuple, consisting of a label (representing the target adverb, or 'none' for a negative sample), a list of tokens we extract (before/after the adverb), and a list of corresponding POS tags (Klein and Manning, 2002) . In each sample, we also add a special token \"@@@@\" right before the head word and the corresponding POS tag of the head word, both in positive and negative cases. We add such special tokens to identify the candidate context in the passage to the model. Figure 1 shows a single positive sample in our dataset.",
                "cite_spans": [
                    {
                        "start": 229,
                        "end": 254,
                        "text": "(Klein and Manning, 2002)",
                        "ref_id": "BIBREF17"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 517,
                        "end": 518,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Data extraction process",
                "sec_num": "3.2"
            },
            {
                "text": "We first extract positive contexts that contain a triggering adverb, then extract negative contexts that do not, controlling for a number of potential confounds. Our positive data consist of cases where the target adverb triggers presupposition by modifying a certain head word which, in most cases, is a verb. We define such head word as a governor of the target adverb.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data extraction process",
                "sec_num": "3.2"
            },
            {
                "text": "When extracting positive data, we scan through all the documents, searching for target adverbs. For each occurrence of a target adverb, we store the location and the governor of the adverb. Taking each occurrence of a governor as a pivot, we extract the 50 unlemmatized tokens preceding it, together with the tokens right after it up to the end of the sentence (where the adverb is)-with the adverb itself being removed. If there are less than 50 tokens before the adverb, we simply extract all of these tokens. In preliminary testing using a logistic regression classifier, we found that limiting the size to 50 tokens had higher accuracy than 25 or 100 tokens. As some head words themselves are stopwords, in the list of tokens, we do not remove any stopwords from the sample; otherwise, we would lose many important samples.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data extraction process",
                "sec_num": "3.2"
            },
            {
                "text": "We filter out the governors of \"too\" that have POS tags \"JJ\" and \"RB\" (adjectives and adverbs), because such cases corresponds to a different sense of \"too\" which indicates excess quantity and does not trigger presupposition (e.g., \"rely too heavily on\", \"it's too far from\").",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data extraction process",
                "sec_num": "3.2"
            },
            {
                "text": "After extracting the positive cases, we then use the governor information of positive cases to extract negative data. In particular, we extract sentences containing the same governors but not any of the target adverbs as negatives. In this way, models cannot rely on the identity of the governor alone to predict the class. This procedure also roughly balances the number of samples in the positive and negative classes.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data extraction process",
                "sec_num": "3.2"
            },
            {
                "text": "For each governor in a positive sample, we locate a corresponding context in the corpus where the governor occurs without being modified by any of the target adverbs. We then extract the surrounding tokens in the same fashion as above. Moreover, we try to control positionrelated confounding factors by two randomization approaches: 1) randomize the order of documents to be scanned, and 2) within each document, start scanning from a random location in the document. Note that the number of negative cases might not be exactly equal to the number of negative cases in all datasets because some governors appearing in positive cases are rare words, and we're unable to find any (or only few) occurrences that match them for the negative cases.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data extraction process",
                "sec_num": "3.2"
            },
            {
                "text": "In this section, we introduce our attention-based model. At a high level, our model extends a bidirectional LSTM model by computing correlations between the hidden states at each timestep, then applying an attention mechanism over these correlations. Our proposed weighted-pooling (WP) neural network architecture is shown in Figure 2 .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 333,
                        "end": 334,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Learning Model",
                "sec_num": "4"
            },
            {
                "text": "The input sequence u = {u 1 , u 2 , . . . , u T } consists of a sequence, of time length T , of onehot encoded word tokens, where the original tokens are those such as in Listing 1. Each token u t is embedded with pretrained embedding matrix W e \u2208 R |V |\u00d7d , where |V | corresponds to the number of tokens in vocabulary V , and d defines the size of the word embeddings. The embedded token vector x t \u2208 R d is retrieved simply with x t = u t W e . Optionally, x t may also include the token's POS tag. In such instances, the embedded token at time step t is concatenated with the POS tag's one-hot encoding p t : x t = u t W e ||p t , where || denotes the vector concatenation operator.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Model",
                "sec_num": "4"
            },
            {
                "text": "At each input time step t, a bi-directional LSTM (Hochreiter and Schmidhuber, 1997) encodes x t into hidden state h t \u2208 R s :",
                "cite_spans": [
                    {
                        "start": 49,
                        "end": 83,
                        "text": "(Hochreiter and Schmidhuber, 1997)",
                        "ref_id": "BIBREF10"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Model",
                "sec_num": "4"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "h t = - \u2192 h t || \u2190 - h t",
                        "eq_num": "(1)"
                    }
                ],
                "section": "Learning Model",
                "sec_num": "4"
            },
            {
                "text": "where",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Model",
                "sec_num": "4"
            },
            {
                "text": "- \u2192 h t = f (x t , h t-1",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Model",
                "sec_num": "4"
            },
            {
                "text": ") is computed by the forward LSTM, and \u2190h t = f (x t , h t+1 ) is computed by the backward LSTM. Concatenated vector h t is of size 2s, where s is a hyperparameter determining the size of the LSTM hidden states. Let matrix H \u2208 R 2s\u00d7T correspond to the concatenation of all hidden state vectors:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Model",
                "sec_num": "4"
            },
            {
                "text": "H = [h 1 ||h 2 || . . . ||h T ].",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Model",
                "sec_num": "4"
            },
            {
                "text": "(2)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Model",
                "sec_num": "4"
            },
            {
                "text": "Our model uses a form of self-attention (Paulus et al., 2018; Vaswani et al., 2017) , where the input sequence acts as both the attention mechanism's query and key/value. Since the location of a presupposition trigger can greatly vary from one sample to another, and because dependencies can be long range or short range, we model all possible word-pair interactions within a sequence. We calculate the energy between all input tokens with a pair-wise matching matrix:",
                "cite_spans": [
                    {
                        "start": 40,
                        "end": 61,
                        "text": "(Paulus et al., 2018;",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 62,
                        "end": 83,
                        "text": "Vaswani et al., 2017)",
                        "ref_id": "BIBREF32"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Model",
                "sec_num": "4"
            },
            {
                "text": "M = H H (3)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Model",
                "sec_num": "4"
            },
            {
                "text": "where M is a square matrix \u2208 R T \u00d7T . To get a single attention weight per time step, we adopt the attention-over-attention method (Cui et al., 2017) .",
                "cite_spans": [
                    {
                        "start": 131,
                        "end": 149,
                        "text": "(Cui et al., 2017)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Model",
                "sec_num": "4"
            },
            {
                "text": "With matrix M , we first compute row-wise attention score M r ij over M :",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Model",
                "sec_num": "4"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "M r ij = exp(e ij ) T t=1 exp(e it )",
                        "eq_num": "(4)"
                    }
                ],
                "section": "Learning Model",
                "sec_num": "4"
            },
            {
                "text": "where e ij = M ij . M r can be interpreted as a word-level attention distribution over all other words. Since we would like a single weight per word, we need an additional step to aggregate these attention scores. Instead of simply averaging the scores, we follow (Cui et al., 2017 )'s approach which learns the aggregation by an additional attention mechanism. We compute columnwise softmax M c ij over M :",
                "cite_spans": [
                    {
                        "start": 264,
                        "end": 281,
                        "text": "(Cui et al., 2017",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Model",
                "sec_num": "4"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "M c ij = exp(e ij ) T t=1 exp(e tj )",
                        "eq_num": "(5)"
                    }
                ],
                "section": "Learning Model",
                "sec_num": "4"
            },
            {
                "text": "The columns of M r are then averaged, forming vector \u03b2 \u2208 R T . Finally, \u03b2 is multiplied with the column-wise softmax matrix M c to get attention vector \u03b1:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Model",
                "sec_num": "4"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "\u03b1 = M r \u03b2. (",
                        "eq_num": "6"
                    }
                ],
                "section": "Learning Model",
                "sec_num": "4"
            },
            {
                "text": ")",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Model",
                "sec_num": "4"
            },
            {
                "text": "Note Equations ( 2) to ( 6) have described how we derived an attention score over our input without the introduction of any new parameters, potentially minimizing the computational effect of our attention mechanism.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Model",
                "sec_num": "4"
            },
            {
                "text": "As a last layer to their neural network, Cui et al. (2017) sum over \u03b1 to extract the most relevant input. However, we use \u03b1 as weights to combine all of our hidden states h t :",
                "cite_spans": [
                    {
                        "start": 41,
                        "end": 58,
                        "text": "Cui et al. (2017)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Model",
                "sec_num": "4"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "c = T t=1 \u03b1 t h t",
                        "eq_num": "(7)"
                    }
                ],
                "section": "Learning Model",
                "sec_num": "4"
            },
            {
                "text": "where c \u2208 R s . We follow the pooling with a dense layer z = \u03c3(W z c + b z ), where \u03c3 is a non-linear function, matrix W z \u2208 R 64\u00d7s and vector b z \u2208 R 64 are learned parameters. The presupposition trigger probability is computed with an affine transform followed by a softmax:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Model",
                "sec_num": "4"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "\u0177 = softmax(W o z + b o )",
                        "eq_num": "(8)"
                    }
                ],
                "section": "Learning Model",
                "sec_num": "4"
            },
            {
                "text": "where matrix W o \u2208 R 2\u00d764 and vector b o \u2208 R2 are learned parameters. The training objective minimizes:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Model",
                "sec_num": "4"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "J(\u03b8) = 1 m m t=1 E(\u0177, y)",
                        "eq_num": "(9)"
                    }
                ],
                "section": "Learning Model",
                "sec_num": "4"
            },
            {
                "text": "where E(\u2022 , \u2022) is the standard cross-entropy.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Model",
                "sec_num": "4"
            },
            {
                "text": "We compare the performance of our WP model against several models which we describe in this section. We carry out the experiments on both datasets described in Section 3. We also investigate the impact of POS tags and attention mechanism on the models' prediction accuracy.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "5"
            },
            {
                "text": "We compare our learning model against the following systems. The first is the most-frequentclass baseline (MFC) which simply labels all samples with the most frequent class of 1. The second is a logistic regression classifier (LogReg), in which the probabilities describing the possible outcomes of a single input x is modeled using a logistic function. We implement this baseline classifier with the scikit-learn package (Pedregosa et al., 2011) , with a CountVectorizer including bi-gram features. All of the other hyperparameters are set to default weights.",
                "cite_spans": [
                    {
                        "start": 422,
                        "end": 446,
                        "text": "(Pedregosa et al., 2011)",
                        "ref_id": "BIBREF24"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baselines",
                "sec_num": "5.1"
            },
            {
                "text": "The third is a variant LSTM recurrent neural network as introduced in (Graves, 2013) . The input is encoded by a bidirectional LSTM like the WP model detailed in Section 4. Instead of a self-attention mechanism, we simply mean-pool matrix H, the concatenation of all LSTM hidden states, across all time steps. This is followed by a fully connected layer and softmax function for the binary classification. Our WP model uses the same bidirectional LSTM as this baseline LSTM, and has the same number of parameters, allowing for a fair comparison of the two models. Such a standard LSTM model represents a state-of-the-art language model, as it outperforms more recent models on language modeling tasks when the number of model parameters is controlled for (Melis et al., 2017) .",
                "cite_spans": [
                    {
                        "start": 70,
                        "end": 84,
                        "text": "(Graves, 2013)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 755,
                        "end": 775,
                        "text": "(Melis et al., 2017)",
                        "ref_id": "BIBREF20"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baselines",
                "sec_num": "5.1"
            },
            {
                "text": "For the last model, we use a slight variant of the CNN sentence classification model of (Kim, 2014) based on the Britz tensorflow implementation 2 .",
                "cite_spans": [
                    {
                        "start": 88,
                        "end": 99,
                        "text": "(Kim, 2014)",
                        "ref_id": "BIBREF15"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baselines",
                "sec_num": "5.1"
            },
            {
                "text": "After tuning, we found the following hyperparameters to work best: 64 units in fully connected layers and 40 units for POS embeddings. We used dropout with probability 0.5 and mini-batch size of 64.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Hyperparameters & Additional Features",
                "sec_num": "5.2"
            },
            {
                "text": "For all models, we initialize word embeddings with word2vec (Mikolov et al., 2013) pretrained embeddings of size 300. Unknown words are randomly initialized to the same size as the word2vec embeddings. In early tests on the development datasets, we found that our neural networks would consistently perform better when fixing the word embeddings. All neural network performance reported in this paper use fixed embeddings.",
                "cite_spans": [
                    {
                        "start": 60,
                        "end": 82,
                        "text": "(Mikolov et al., 2013)",
                        "ref_id": "BIBREF21"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Hyperparameters & Additional Features",
                "sec_num": "5.2"
            },
            {
                "text": "Fully connected layers in the LSTM, CNN and WP model are regularized with dropout (Srivastava et al., 2014) . The model parameters for these neural networks are fine-tuned with the Adam algorithm (Kingma and Ba, 2015) . To stabilize the RNN training gradients (Pascanu et al., 2013) , we perform gradient clipping for gradients below threshold value -1, or above 1. To reduce overfitting, we stop training if the development set does not improve in accuracy for 10 epochs. All performance on the test set is reported using the best trained model as measured on the development set.",
                "cite_spans": [
                    {
                        "start": 82,
                        "end": 107,
                        "text": "(Srivastava et al., 2014)",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 196,
                        "end": 217,
                        "text": "(Kingma and Ba, 2015)",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 260,
                        "end": 282,
                        "text": "(Pascanu et al., 2013)",
                        "ref_id": "BIBREF22"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Hyperparameters & Additional Features",
                "sec_num": "5.2"
            },
            {
                "text": "In addition, we use the CoreNLP Part-of- In all of our models, we limit the maximum length of samples and POS tags to 60 tokens. For the CNN, sequences shorter than 60 tokens are zeropadded.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Hyperparameters & Additional Features",
                "sec_num": "5.2"
            },
            {
                "text": "Table 2 shows the performance obtained by the different models with and without POS tags. Overall, our attention model WP outperforms all other models in 10 out of 14 scenarios (combinations of datasets and whether or not POS tags are used). Importantly, our model outperforms the regular LSTM model without introducing additional parameters to the model, which highlights the advantage of WP's attention-based pooling method. For all models listed in Table 2 , we find that including POS tags benefits the detection of adverbial presupposition triggers in Gigaword and PTB datasets. Note that, in ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "2",
                        "ref_id": "TABREF2"
                    },
                    {
                        "start": 458,
                        "end": 459,
                        "text": "2",
                        "ref_id": "TABREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "6"
            },
            {
                "text": "Consider the following pair of samples that we randomly choose from the PTB dataset (shortened for readability): 1. ...Taped just as the market closed yesterday , it offers Ms. Farrell advising , \" We view the market here as going through a relatively normal cycle ... . We continue to feel that the stock market is the @@@@ place to be for long-term appreciation 2. ...More people are remaining independent longer presumably because they are better off physically and financially . Careers count most for the well-to-do many affluent people @@@@ place personal success and money above family",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Analysis",
                "sec_num": "7"
            },
            {
                "text": "In both cases, the head word is place. In Example 1, the word continue (emphasized in the above text) suggests that adverb still could be used to modify head word place (i.e., ... the stock market is still the place ...). Further, it is also easy to see that place refers to stock market, which has occurred in the previous context. Our model correctly predicts this sample as containing a presupposition, this despite the complexity of the coreference across the text.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Analysis",
                "sec_num": "7"
            },
            {
                "text": "In the second case of the usage of the same main head word place in Example 2, our model falsely predicts the presence of a presupposition. However, even a human could read the sentence as \"many people still place personal success and money above family\". This underlies the subtlety and difficulty of the task at hand. The longrange dependencies and interactions within sentences seen in these examples are what motivate the use of the various deep non-linear models presented in this work, which are useful in detecting these coreferences, particularly in the case of attention mechanisms.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Analysis",
                "sec_num": "7"
            },
            {
                "text": "In this work, we have investigated the task of predicting adverbial presupposition triggers and introduced several datasets for the task. Additionally, we have presented a novel weighted-pooling attention mechanism which is incorporated into a recurrent neural network model for predicting the presence of an adverbial presuppositional trigger. Our results show that the model outperforms the CNN and LSTM, and does not add any additional parameters over the standard LSTM model. This shows its promise in classification tasks involving capturing and combining relevant information from multiple points in the previous context.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "8"
            },
            {
                "text": "In future work, we would like to focus more on designing models that can deal with and be optimized for scenarios with severe data imbalance. We would like to also explore various applications of presupposition trigger prediction in language generation applications, as well as additional attention-based neural network architectures.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "8"
            },
            {
                "text": "http://www.wildml.com/2015/12/implementing-a-cnnfor-text-classification-in-tensorflow/",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "The authors would like to thank the reviewers for their valuable comments. This work was supported by the Centre de Recherche d'Informatique de Montr\u00e9al (CRIM), the Fonds de Recherche du Qu\u00e9bec -Nature et Technologies (FRQNT) and the Natural Sciences and Engineering Research Council of Canada (NSERC).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgements",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Neural machine translation by jointly learning to align and translate",
                "authors": [
                    {
                        "first": "Dzmitry",
                        "middle": [],
                        "last": "Bahdanau",
                        "suffix": ""
                    },
                    {
                        "first": "Kyunghyun",
                        "middle": [],
                        "last": "Cho",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Advances in Neural Information Processing Systems (NIPS 2015)",
                "volume": "",
                "issue": "",
                "pages": "649--657",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2015. Neural machine translation by jointly learning to align and translate. In Advances in Neu- ral Information Processing Systems (NIPS 2015), pages 649-657, Montreal, Canada.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Presupposition",
                "authors": [
                    {
                        "first": "David",
                        "middle": [
                            "I"
                        ],
                        "last": "Beaver",
                        "suffix": ""
                    },
                    {
                        "first": "Bart",
                        "middle": [],
                        "last": "Geurts",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "The Stanford Encyclopedia of Philosophy",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "David I. Beaver and Bart Geurts. 2014. Presupposition. In Edward N. Zalta, editor, The Stanford Encyclope- dia of Philosophy, winter 2014 edition. Metaphysics Research Lab, Stanford University.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Optimality Theory and Pragmatics. Palgrave Studies in Pragmatics, Language and Cognition",
                "authors": [
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Blutner",
                        "suffix": ""
                    },
                    {
                        "first": "H",
                        "middle": [],
                        "last": "Zeevat",
                        "suffix": ""
                    },
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Bach",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Bezuidenhout",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Breheny",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Glucksberg",
                        "suffix": ""
                    },
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Happ\u00e9",
                        "suffix": ""
                    },
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Recanati",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Wilson",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "R. Blutner, H. Zeevat, K. Bach, A. Bezuidenhout, R. Breheny, S. Glucksberg, F. Happ\u00e9, F. Recanati, and D. Wilson. 2003. Optimality Theory and Prag- matics. Palgrave Studies in Pragmatics, Language and Cognition. Palgrave Macmillan UK.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Attention-overattention neural networks for reading comprehension",
                "authors": [
                    {
                        "first": "Yiming",
                        "middle": [],
                        "last": "Cui",
                        "suffix": ""
                    },
                    {
                        "first": "Zhipeng",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Si",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Shijin",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Ting",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Guoping",
                        "middle": [],
                        "last": "Hu",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "593--602",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yiming Cui, Zhipeng Chen, Si Wei, Shijin Wang, Ting Liu, and Guoping Hu. 2017. Attention-over- attention neural networks for reading comprehen- sion. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Vol- ume 1: Long Papers), pages 593-602.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "A measure of information gained during prose learning",
                "authors": [
                    {
                        "first": "G",
                        "middle": [
                            "R"
                        ],
                        "last": "Miller",
                        "suffix": ""
                    },
                    {
                        "first": "E",
                        "middle": [
                            "B"
                        ],
                        "last": "Coleman",
                        "suffix": ""
                    }
                ],
                "year": 1968,
                "venue": "Reading Research Quarterly",
                "volume": "3",
                "issue": "3",
                "pages": "369--386",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "G. R. Miller E. B. Coleman. 1968. A measure of infor- mation gained during prose learning. Reading Re- search Quarterly, 3(3):369-386.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Comparable cloze and multiple-choice comprehension test scores",
                "authors": [
                    {
                        "first": "Joseph",
                        "middle": [
                            "W Culhane"
                        ],
                        "last": "Earl",
                        "suffix": ""
                    },
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Rankin",
                        "suffix": ""
                    }
                ],
                "year": 1969,
                "venue": "Journal of Reading",
                "volume": "13",
                "issue": "3",
                "pages": "193--198",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Joseph W. Culhane Earl F. Rankin. 1969. Compara- ble cloze and multiple-choice comprehension test scores. Journal of Reading, 13(3):193-198.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Zeitschrift f\u00fcr Philosophie und philosophische Kritik",
                "authors": [
                    {
                        "first": "Gottlob",
                        "middle": [],
                        "last": "Frege",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "",
                "volume": "100",
                "issue": "",
                "pages": "25--50",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Gottlob Frege. 1892. \u00dcber sinn und bedeutung. Zeitschrift f\u00fcr Philosophie und philosophische Kri- tik, 100:25-50.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "English gigaword third edition",
                "authors": [
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Graff",
                        "suffix": ""
                    },
                    {
                        "first": "Junbo",
                        "middle": [],
                        "last": "Kong",
                        "suffix": ""
                    },
                    {
                        "first": "Ke",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Kazuaki",
                        "middle": [],
                        "last": "Maeda",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. 2007. English gigaword third edition. Tech- nical report, Linguistic Data Consortium.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Generating sequences with recurrent neural networks",
                "authors": [
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Graves",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alex Graves. 2013. Generating sequences with recur- rent neural networks. CoRR, abs/1308.0850.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "The goldilocks principle: Reading children's books with explicit memory representations",
                "authors": [
                    {
                        "first": "Felix",
                        "middle": [],
                        "last": "Hill",
                        "suffix": ""
                    },
                    {
                        "first": "Antoine",
                        "middle": [],
                        "last": "Bordes",
                        "suffix": ""
                    },
                    {
                        "first": "Sumit",
                        "middle": [],
                        "last": "Chopra",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Weston",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Felix Hill, Antoine Bordes, Sumit Chopra, and Jason Weston. 2015. The goldilocks principle: Reading children's books with explicit memory representa- tions. CoRR, abs/1511.02301.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Long short-term memory",
                "authors": [
                    {
                        "first": "Sepp",
                        "middle": [],
                        "last": "Hochreiter",
                        "suffix": ""
                    },
                    {
                        "first": "J\u00fcrgen",
                        "middle": [],
                        "last": "Schmidhuber",
                        "suffix": ""
                    }
                ],
                "year": 1997,
                "venue": "Neural Computation",
                "volume": "9",
                "issue": "8",
                "pages": "1735--1780",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural Computation, 9(8):1735-1780.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Capturing pragmatic knowledge in article usage prediction using lstms",
                "authors": [
                    {
                        "first": "Jad",
                        "middle": [],
                        "last": "Kabbara",
                        "suffix": ""
                    },
                    {
                        "first": "Yulan",
                        "middle": [],
                        "last": "Feng",
                        "suffix": ""
                    },
                    {
                        "first": "Jackie",
                        "middle": [],
                        "last": "Chi",
                        "suffix": ""
                    },
                    {
                        "first": "Kit",
                        "middle": [],
                        "last": "Cheung",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "COLING",
                "volume": "",
                "issue": "",
                "pages": "2625--2634",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jad Kabbara, Yulan Feng, and Jackie Chi Kit Che- ung. 2016. Capturing pragmatic knowledge in arti- cle usage prediction using lstms. In COLING, pages 2625-2634.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "The use of too as a pragmatic presupposition trigger",
                "authors": [
                    {
                        "first": "Qiang",
                        "middle": [],
                        "last": "Kang",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Canadian Social Science",
                "volume": "8",
                "issue": "6",
                "pages": "165--169",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Qiang Kang. 2012. The use of too as a pragmatic presupposition trigger. Canadian Social Science, 8(6):165-169.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "What is Russell's theory of descriptions?",
                "authors": [
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Kaplan",
                        "suffix": ""
                    }
                ],
                "year": 1970,
                "venue": "Physics, Logic, and History",
                "volume": "",
                "issue": "",
                "pages": "277--295",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "David Kaplan. 1970. What is Russell's theory of de- scriptions? In Wolfgang Yourgrau and Allen D. Breck, editors, Physics, Logic, and History, pages 277-295. Plenum Press.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "An analysis of presupposition triggers in english journalistic texts",
                "authors": [
                    {
                        "first": "Layth",
                        "middle": [],
                        "last": "Muthana",
                        "suffix": ""
                    },
                    {
                        "first": "Khaleel",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Of College Of Education For Women",
                "volume": "21",
                "issue": "2",
                "pages": "523--551",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Layth Muthana Khaleel. 2010. An analysis of pre- supposition triggers in english journalistic texts. Of College Of Education For Women, 21(2):523-551.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Convolutional neural networks for sentence classification",
                "authors": [
                    {
                        "first": "Yoon",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "volume": "",
                "issue": "",
                "pages": "1746--1751",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yoon Kim. 2014. Convolutional neural networks for sentence classification. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1746-1751, Doha, Qatar.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Adam: A method for stochastic optimization",
                "authors": [
                    {
                        "first": "Diederik",
                        "middle": [],
                        "last": "Kingma",
                        "suffix": ""
                    },
                    {
                        "first": "Jimmy",
                        "middle": [],
                        "last": "Ba",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceeding of the 2015 International Conference on Learning Representation (ICLR 2015)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Diederik Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In Proceeding of the 2015 International Conference on Learning Representation (ICLR 2015), San Diego, California.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "A generative constituent-context model for improved grammar induction",
                "authors": [
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Klein",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL '02",
                "volume": "",
                "issue": "",
                "pages": "128--135",
                "other_ids": {
                    "DOI": [
                        "10.3115/1073083.1073106"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Dan Klein and Christopher D. Manning. 2002. A generative constituent-context model for improved grammar induction. In Proceedings of the 40th An- nual Meeting on Association for Computational Lin- guistics, ACL '02, pages 128-135, Stroudsburg, PA, USA. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "The Stanford CoreNLP natural language processing toolkit",
                "authors": [
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Christopher",
                        "suffix": ""
                    },
                    {
                        "first": "Mihai",
                        "middle": [],
                        "last": "Manning",
                        "suffix": ""
                    },
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Surdeanu",
                        "suffix": ""
                    },
                    {
                        "first": "Jenny",
                        "middle": [],
                        "last": "Bauer",
                        "suffix": ""
                    },
                    {
                        "first": "Steven",
                        "middle": [
                            "J"
                        ],
                        "last": "Finkel",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Bethard",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Mc-Closky",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Association for Computational Linguistics (ACL) System Demonstrations",
                "volume": "",
                "issue": "",
                "pages": "55--60",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David Mc- Closky. 2014. The Stanford CoreNLP natural lan- guage processing toolkit. In Association for Compu- tational Linguistics (ACL) System Demonstrations, pages 55-60.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Building a large annotated corpus of English: The Penn Treebank",
                "authors": [
                    {
                        "first": "Mitchell",
                        "middle": [
                            "P"
                        ],
                        "last": "Marcus",
                        "suffix": ""
                    },
                    {
                        "first": "Mary",
                        "middle": [
                            "A"
                        ],
                        "last": "Marcinkiewicz",
                        "suffix": ""
                    },
                    {
                        "first": "Beatrice",
                        "middle": [],
                        "last": "Santorini",
                        "suffix": ""
                    }
                ],
                "year": 1993,
                "venue": "Computational Linguistics",
                "volume": "19",
                "issue": "2",
                "pages": "313--330",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mitchell P. Marcus, Mary A. Marcinkiewicz, and Beat- rice Santorini. 1993. Building a large annotated cor- pus of English: The Penn Treebank. Computational Linguistics, 19(2):313-330.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "On the state of the art of evaluation in neural language models",
                "authors": [
                    {
                        "first": "G\u00e1bor",
                        "middle": [],
                        "last": "Melis",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Dyer",
                        "suffix": ""
                    },
                    {
                        "first": "Phil",
                        "middle": [],
                        "last": "Blunsom",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "G\u00e1bor Melis, Chris Dyer, and Phil Blunsom. 2017. On the state of the art of evaluation in neural language models. CoRR, abs/1707.05589.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Distributed representations of words and phrases and their compositionality",
                "authors": [
                    {
                        "first": "Tomas",
                        "middle": [],
                        "last": "Mikolov",
                        "suffix": ""
                    },
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    },
                    {
                        "first": "Kai",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Greg",
                        "middle": [
                            "S"
                        ],
                        "last": "Corrado",
                        "suffix": ""
                    },
                    {
                        "first": "Jeff",
                        "middle": [],
                        "last": "Dean",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "3111--3119",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor- rado, and Jeff Dean. 2013. Distributed representa- tions of words and phrases and their compositional- ity. In Advances in Neural Information Processing Systems, pages 3111-3119.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "On the difficulty of training recurrent neural networks",
                "authors": [
                    {
                        "first": "Razvan",
                        "middle": [],
                        "last": "Pascanu",
                        "suffix": ""
                    },
                    {
                        "first": "Tomas",
                        "middle": [],
                        "last": "Mikolov",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of the 30th International Conference on International Conference on Machine Learning",
                "volume": "28",
                "issue": "",
                "pages": "1310--1318",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. 2013. On the difficulty of training recurrent neu- ral networks. In Proceedings of the 30th Interna- tional Conference on International Conference on Machine Learning -Volume 28, ICML'13, pages 1310-1318.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "A deep reinforced model for abstractive summarization",
                "authors": [
                    {
                        "first": "Romain",
                        "middle": [],
                        "last": "Paulus",
                        "suffix": ""
                    },
                    {
                        "first": "Caiming",
                        "middle": [],
                        "last": "Xiong",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "International Conference on Learning Representations",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Romain Paulus, Caiming Xiong, and Richard Socher. 2018. A deep reinforced model for abstractive sum- marization. In International Conference on Learn- ing Representations, Vancouver, Canada.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Scikit-learn: Machine learning in Python",
                "authors": [
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Pedregosa",
                        "suffix": ""
                    },
                    {
                        "first": "G",
                        "middle": [],
                        "last": "Varoquaux",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Gramfort",
                        "suffix": ""
                    },
                    {
                        "first": "V",
                        "middle": [],
                        "last": "Michel",
                        "suffix": ""
                    },
                    {
                        "first": "B",
                        "middle": [],
                        "last": "Thirion",
                        "suffix": ""
                    },
                    {
                        "first": "O",
                        "middle": [],
                        "last": "Grisel",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Blondel",
                        "suffix": ""
                    },
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Prettenhofer",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Weiss",
                        "suffix": ""
                    },
                    {
                        "first": "V",
                        "middle": [],
                        "last": "Dubourg",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Vanderplas",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Passos",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Cournapeau",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Brucher",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Perrot",
                        "suffix": ""
                    },
                    {
                        "first": "E",
                        "middle": [],
                        "last": "Duchesnay",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Journal of Machine Learning Research",
                "volume": "12",
                "issue": "",
                "pages": "2825--2830",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten- hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas- sos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825-2830.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "A rule-based question answering system for reading comprehension tests",
                "authors": [
                    {
                        "first": "Ellen",
                        "middle": [],
                        "last": "Riloff",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Thelen",
                        "suffix": ""
                    }
                ],
                "year": 2000,
                "venue": "Proceedings of the 2000 ANLP/-NAACL Workshop on Reading Comprehension Tests As Evaluation for Computer-based Language Understanding Sytems",
                "volume": "6",
                "issue": "",
                "pages": "13--19",
                "other_ids": {
                    "DOI": [
                        "10.3115/1117595.1117598"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ellen Riloff and Michael Thelen. 2000. A rule-based question answering system for reading comprehen- sion tests. In Proceedings of the 2000 ANLP/- NAACL Workshop on Reading Comprehension Tests As Evaluation for Computer-based Language Un- derstanding Sytems -Volume 6, ANLP/NAACL- ReadingComp '00, pages 13-19, Stroudsburg, PA, USA. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Dropout: a simple way to prevent neural networks from overfitting",
                "authors": [
                    {
                        "first": "Nitish",
                        "middle": [],
                        "last": "Srivastava",
                        "suffix": ""
                    },
                    {
                        "first": "Geoffrey",
                        "middle": [
                            "E"
                        ],
                        "last": "Hinton",
                        "suffix": ""
                    },
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Krizhevsky",
                        "suffix": ""
                    },
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    },
                    {
                        "first": "Ruslan",
                        "middle": [],
                        "last": "Salakhutdinov",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Journal of machine learning research",
                "volume": "15",
                "issue": "1",
                "pages": "1929--1958",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from overfitting. Journal of machine learning re- search, 15(1):1929-1958.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Presuppositions",
                "authors": [
                    {
                        "first": "Robert",
                        "middle": [],
                        "last": "Stalnaker",
                        "suffix": ""
                    }
                ],
                "year": 1973,
                "venue": "Journal of philosophical logic",
                "volume": "2",
                "issue": "4",
                "pages": "447--457",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Robert Stalnaker. 1973. Presuppositions. Journal of philosophical logic, 2(4):447-457.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "On the representation of context",
                "authors": [
                    {
                        "first": "Robert",
                        "middle": [],
                        "last": "Stalnaker",
                        "suffix": ""
                    }
                ],
                "year": 1998,
                "venue": "Journal of Logic, Language and Information",
                "volume": "7",
                "issue": "1",
                "pages": "3--19",
                "other_ids": {
                    "DOI": [
                        "10.1023/A:1008254815298"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Robert Stalnaker. 1998. On the representation of con- text. Journal of Logic, Language and Information, 7(1):3-19.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "On referring",
                "authors": [
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Peter",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Strawson",
                        "suffix": ""
                    }
                ],
                "year": 1950,
                "venue": "Mind",
                "volume": "59",
                "issue": "235",
                "pages": "320--344",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Peter F. Strawson. 1950. On referring. Mind, 59(235):320-344.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "End-to-end memory networks",
                "authors": [
                    {
                        "first": "Sainbayar",
                        "middle": [],
                        "last": "Sukhbaatar",
                        "suffix": ""
                    },
                    {
                        "first": "Arthur",
                        "middle": [],
                        "last": "Szlam",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Weston",
                        "suffix": ""
                    },
                    {
                        "first": "Rob",
                        "middle": [],
                        "last": "Fergus",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "28",
                "issue": "",
                "pages": "2440--2448",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. 2015. End-to-end memory net- works. In Advances in Neural Information Process- ing Systems 28, pages 2440-2448.",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "Cloze procedure: A new tool for measuring readability",
                "authors": [
                    {
                        "first": "Wilson",
                        "middle": [
                            "L"
                        ],
                        "last": "Taylor",
                        "suffix": ""
                    }
                ],
                "year": 1953,
                "venue": "Journalism Quarterly",
                "volume": "30",
                "issue": "4",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Wilson L. Taylor. 1953. Cloze procedure: A new tool for measuring readability. Journalism Quarterly, 30(4):415.",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "Attention is all you need",
                "authors": [
                    {
                        "first": "Ashish",
                        "middle": [],
                        "last": "Vaswani",
                        "suffix": ""
                    },
                    {
                        "first": "Noam",
                        "middle": [],
                        "last": "Shazeer",
                        "suffix": ""
                    },
                    {
                        "first": "Niki",
                        "middle": [],
                        "last": "Parmar",
                        "suffix": ""
                    },
                    {
                        "first": "Llion",
                        "middle": [],
                        "last": "Jones",
                        "suffix": ""
                    },
                    {
                        "first": "Jakob",
                        "middle": [],
                        "last": "Uszkoreit",
                        "suffix": ""
                    },
                    {
                        "first": "Aidan",
                        "middle": [
                            "N"
                        ],
                        "last": "Gomez",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Kaiser",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "30",
                "issue": "",
                "pages": "5994--6004",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Llion Jones, Jakob Uszkoreit, Aidan N Gomez, and \u0141 ukasz Kaiser. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30, pages 5994-6004.",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "Presupposition trigger-a comparative analysis of broadcast news discourse",
                "authors": [
                    {
                        "first": "Javad",
                        "middle": [],
                        "last": "Zare",
                        "suffix": ""
                    },
                    {
                        "first": "Ehsan",
                        "middle": [],
                        "last": "Abbaspour",
                        "suffix": ""
                    },
                    {
                        "first": "Mahdi",
                        "middle": [],
                        "last": "Rajaee",
                        "suffix": ""
                    },
                    {
                        "first": "Nia",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "International Journal of Linguistics",
                "volume": "4",
                "issue": "3",
                "pages": "734--743",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Javad Zare, Ehsan Abbaspour, and Mahdi Rajaee Nia. 2012. Presupposition trigger-a comparative analy- sis of broadcast news discourse. International Jour- nal of Linguistics, 4(3):734-743.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 1: An example of an instance containing a presuppositional trigger from our dataset.",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Figure 2: Our weighted-pooling neural network architecture (WP). The tokenized input is embedded with pretrained word embeddings and possibly concatenated with one-hot encoded POS tags. The input is then encoded with a bi-directional LSTM, followed by our attention mechanism. The computed attention scores are then used as weights to average the encoded states, in turn connected to a fully connected layer to predict presupposition triggering.",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "TABREF1": {
                "content": "<table><tr><td/><td/><td>Training set</td><td/><td/><td>Test set</td><td/></tr><tr><td>Corpus</td><td>Positive</td><td>Negative</td><td>Total</td><td>Positive</td><td>Negative</td><td>Total</td></tr><tr><td>PTB</td><td>2,596</td><td>2,579</td><td>5,175</td><td>249</td><td>233</td><td>482</td></tr><tr><td>Gigaword yet</td><td>32,024</td><td>31,819</td><td>63,843</td><td>7950</td><td>7890</td><td>15840</td></tr><tr><td>Gigaword too</td><td>55,827</td><td>29,918</td><td>85,745</td><td>13987</td><td>7514</td><td>21501</td></tr><tr><td>Gigaword again</td><td>43,120</td><td>42,824</td><td>85,944</td><td>10935</td><td>10827</td><td>21762</td></tr><tr><td>Gigaword still</td><td>97,670</td><td>96,991</td><td>194,661</td><td>24509</td><td>24232</td><td>48741</td></tr><tr><td>Gigaword also</td><td>269,778</td><td>267,851</td><td>537,626</td><td>66878</td><td>66050</td><td>132928</td></tr><tr><td>Gigaword all</td><td>498,415</td><td>491,173</td><td>989,588</td><td>124255</td><td>123078</td><td>247333</td></tr></table>",
                "type_str": "table",
                "text": "Number of training samples in each dataset.",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table><tr><td/><td/><td/><td/><td colspan=\"2\">Accuracy</td><td/><td/><td/></tr><tr><td/><td/><td>WSJ</td><td/><td/><td colspan=\"2\">Gigaword</td><td/><td/></tr><tr><td>Models</td><td>Variants</td><td>All adverbs</td><td>All adverbs</td><td>Also</td><td>Still</td><td>Again</td><td>Too</td><td>Yet</td></tr><tr><td>MFC</td><td>-</td><td>51.66</td><td>50.24</td><td>50.32</td><td>50.29</td><td>50.25</td><td>65.06</td><td>50.19</td></tr><tr><td>LogReg</td><td>+ POS -POS</td><td>52.81 54.47</td><td>53.65 52.86</td><td>52.00 56.07</td><td>56.36 55.29</td><td>59.49 58.60</td><td>69.77 67.60</td><td>61.05 58.60</td></tr><tr><td>CNN</td><td>+ POS -POS</td><td>58.84 62.16</td><td>59.12 57.21</td><td>61.53 59.76</td><td>59.54 56.95</td><td>60.26 57.28</td><td>67.53 67.84</td><td>59.69 56.53</td></tr><tr><td>LSTM</td><td>+ POS -POS</td><td>74.23 73.18</td><td>60.58 58.86</td><td>81.48 81.16</td><td>60.72 58.97</td><td>61.81 59.93</td><td>69.70 68.32</td><td>59.13 55.71</td></tr><tr><td>WP</td><td>+ POS -POS</td><td>76.09 74.84</td><td>60.62 58.87</td><td>82.42 81.64</td><td>61.00 59.03</td><td>61.59 58.49</td><td>69.38 68.37</td><td>57.68 56.68</td></tr></table>",
                "type_str": "table",
                "text": "Performance of various models, including our weighted-pooled LSTM (WP). MFC refers to the most-frequent-class baseline, LogReg is the logistic regression baseline. LSTM and CNN correspond to strong neural network baselines. Note that we bold the performance numbers for the best performing model for each of the \"+ POS\" case and the \"-POS\" case.",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table><tr><td>, we bolded ac-</td></tr><tr><td>curacy figures that were within 0.1% of the best</td></tr><tr><td>performing WP model as McNemar's test did not</td></tr><tr><td>show that WP significantly outperformed the other</td></tr><tr><td>model in these cases (p value &gt; 0.05).</td></tr><tr><td>Table 3 shows the confusion matrix for the best</td></tr><tr><td>performing model (WP,+POS). The small differ-</td></tr><tr><td>ences in the off-diagonal entries inform us that</td></tr><tr><td>the model misclassifications are not particularly</td></tr><tr><td>skewed towards the presence or absence of pre-</td></tr></table>",
                "type_str": "table",
                "text": "",
                "html": null,
                "num": null
            }
        }
    }
}