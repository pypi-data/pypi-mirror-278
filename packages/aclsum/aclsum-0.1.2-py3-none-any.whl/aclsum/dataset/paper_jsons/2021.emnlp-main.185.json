{
    "paper_id": "2021",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T11:55:31.445017Z"
    },
    "title": "DialogueCSE: Dialogue-based Contrastive Learning of Sentence Embeddings",
    "authors": [
        {
            "first": "Che",
            "middle": [],
            "last": "Liu",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "DAMO Academy",
                "location": {
                    "settlement": "Alibaba Group"
                }
            },
            "email": ""
        },
        {
            "first": "Rui",
            "middle": [],
            "last": "Wang",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "DAMO Academy",
                "location": {
                    "settlement": "Alibaba Group"
                }
            },
            "email": ""
        },
        {
            "first": "Jinghua",
            "middle": [],
            "last": "Liu",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "DAMO Academy",
                "location": {
                    "settlement": "Alibaba Group"
                }
            },
            "email": ""
        },
        {
            "first": "Jian",
            "middle": [],
            "last": "Sun",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "DAMO Academy",
                "location": {
                    "settlement": "Alibaba Group"
                }
            },
            "email": "jian.sun@alibaba-inc.com"
        },
        {
            "first": "Fei",
            "middle": [],
            "last": "Huang",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "DAMO Academy",
                "location": {
                    "settlement": "Alibaba Group"
                }
            },
            "email": "f.huang@alibaba-inc.com"
        },
        {
            "first": "Luo",
            "middle": [],
            "last": "Si",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "DAMO Academy",
                "location": {
                    "settlement": "Alibaba Group"
                }
            },
            "email": "luo.si@alibaba-inc.com"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Learning sentence embeddings from dialogues has drawn increasing attention due to its low annotation cost and high domain adaptability. Conventional approaches employ the siamese-network for this task, which obtains the sentence embeddings through modeling the context-response semantic relevance by applying a feed-forward network on top of the sentence encoders. However, as the semantic textual similarity is commonly measured through the element-wise distance metrics (e.g. cosine and L2 distance), such architecture yields a large gap between training and evaluating. In this paper, we propose DialogueCSE, a dialogue-based contrastive learning approach to tackle this issue. DialogueCSE first introduces a novel matching-guided embedding (MGE) mechanism, which generates a contextaware embedding for each candidate response embedding (i.e. the context-free embedding) according to the guidance of the multi-turn context-response matching matrices. Then it pairs each context-aware embedding with its corresponding context-free embedding and finally minimizes the contrastive loss across all pairs. We evaluate our model on three multi-turn dialogue datasets: the Microsoft Dialogue Corpus, the Jing Dong Dialogue Corpus, and the E-commerce Dialogue Corpus. Evaluation results show that our approach significantly outperforms the baselines across all three datasets in terms of MAP and Spearman's correlation measures, demonstrating its effectiveness. Further quantitative experiments show that our approach achieves better performance when leveraging more dialogue context and remains robust when less training data is provided.",
    "pdf_parse": {
        "paper_id": "2021",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Learning sentence embeddings from dialogues has drawn increasing attention due to its low annotation cost and high domain adaptability. Conventional approaches employ the siamese-network for this task, which obtains the sentence embeddings through modeling the context-response semantic relevance by applying a feed-forward network on top of the sentence encoders. However, as the semantic textual similarity is commonly measured through the element-wise distance metrics (e.g. cosine and L2 distance), such architecture yields a large gap between training and evaluating. In this paper, we propose DialogueCSE, a dialogue-based contrastive learning approach to tackle this issue. DialogueCSE first introduces a novel matching-guided embedding (MGE) mechanism, which generates a contextaware embedding for each candidate response embedding (i.e. the context-free embedding) according to the guidance of the multi-turn context-response matching matrices. Then it pairs each context-aware embedding with its corresponding context-free embedding and finally minimizes the contrastive loss across all pairs. We evaluate our model on three multi-turn dialogue datasets: the Microsoft Dialogue Corpus, the Jing Dong Dialogue Corpus, and the E-commerce Dialogue Corpus. Evaluation results show that our approach significantly outperforms the baselines across all three datasets in terms of MAP and Spearman's correlation measures, demonstrating its effectiveness. Further quantitative experiments show that our approach achieves better performance when leveraging more dialogue context and remains robust when less training data is provided.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Sentence embeddings are used with success for a variety of NLP applications (Cer et al., 2018) and many prior methods have been proposed with different learning schemes. Kiros et al. (2015) ; Logeswaran and Lee (2018); Hill et al. (2016) train sentence encoders in a self-supervised manner with web pages and books. Conneau et al. (2017) ; Cer et al. (2018) ; Reimers and Gurevych (2019) propose to learn sentence embeddings on the supervised datasets such as SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2018) . Although the supervised-learning approaches achieve better performance, they suffer from high cost of annotation in building the training dataset, which makes them hard to adapt to other domains or languages.",
                "cite_spans": [
                    {
                        "start": 76,
                        "end": 94,
                        "text": "(Cer et al., 2018)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 170,
                        "end": 189,
                        "text": "Kiros et al. (2015)",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 219,
                        "end": 237,
                        "text": "Hill et al. (2016)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 316,
                        "end": 337,
                        "text": "Conneau et al. (2017)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 340,
                        "end": 357,
                        "text": "Cer et al. (2018)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 360,
                        "end": 387,
                        "text": "Reimers and Gurevych (2019)",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 465,
                        "end": 486,
                        "text": "(Bowman et al., 2015)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 496,
                        "end": 519,
                        "text": "(Williams et al., 2018)",
                        "ref_id": "BIBREF29"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Recently, learning sentence embeddings from dialogues has begun to attract increasing attention. Dialogues provide strong semantic relationships among conversational utterances and are usually easy to collect in large amounts. Such advantages make the dialogue-based self-supervised learning methods promising to achieve competitive or even superior performance against the supervised-learning methods, especially under the low-resource conditions.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "While promising, the issue of how to effectively exploit the dialogues for this task has not been sufficiently explored. Yang et al. (2018) propose to train an input-response prediction model on Reddit dataset (Al-Rfou et al., 2016) . Since they build their architecture based on the single-turn dialogue, the multi-turn dialogue history is not fully exploited. Henderson et al. (2020) demonstrate that introducing the multi-turn dialogue context can improve the sentence embedding performance. However, they concatenate the multi-turn dialogue context into a long token sequence, failing to model intersentence semantic relationships among the utterances. Recently, more advanced methods such as (Reimers and Gurevych, 2019) achieve better performance by employing BERT (Devlin et al., 2019) as the sentence encoder. These works have in common that they employ a feed-forward network with a non-linear activation on top of the sentence en-coders to model the context-response semantic relevance, thereby learning the sentence embeddings. However, such architecture presents two limitations: (1) It yields a large gap between training and evaluating, since the semantic textual similarity is commonly measured by the element-wise distance metrics such as cosine and L2 distance. (2) Concatenating all the utterances in the dialogue context inevitably introduces the noise as well as the redundant information, resulting in a poor result.",
                "cite_spans": [
                    {
                        "start": 121,
                        "end": 139,
                        "text": "Yang et al. (2018)",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 210,
                        "end": 232,
                        "text": "(Al-Rfou et al., 2016)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 362,
                        "end": 385,
                        "text": "Henderson et al. (2020)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 697,
                        "end": 725,
                        "text": "(Reimers and Gurevych, 2019)",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 771,
                        "end": 792,
                        "text": "(Devlin et al., 2019)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this paper, we propose DialogueCSE, a dialogue-based contrastive learning approach to tackle these issues. We hold that the semantic matching relationships between the context and the response can be implicitly modeled through contrastive learning, thus making it possible to eliminate the gap between training and evaluating. To this end, we introduce a novel matching-guided embedding (MGE) mechanism. Specifically, MGE first pairs each utterance in the context with the response and performs a token-level dot-product operation across all the utterance-response pairs to obtain the multi-turn matching matrices. Then the multi-turn matching matrices are used as guidance to generate a context-aware embedding for the response embedding (i.e. the context-free embedding). Finally, the context-aware embedding and the context-free embedding are paired as a training sample, whose label is determined by whether the context and the response are originally from the same dialogue. Our motivation is that once the context semantically matches the response, it has the ability to distill the context-aware information from the context-free embedding, which is exactly the learning objective of the sentence encoder that aims to produce context-aware sentence embeddings.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "We train our model on three multi-turn dialogue datasets: the Microsoft Dialogue Corpus (MDC) (Li et al., 2018) , the Jing Dong Dialogue Corpus (JDDC) (Chen et al., 2020) , and the E-commerce Dialogue Corpus (ECD) (Zhang et al., 2018) . To evaluate our model, we introduce two types of tasks: the semantic retrieval (SR) task and the dialogue-based semantic textual similarity (D-STS) task. Here we do not adopt the standard semantic textual similarity (STS) task (Cer et al., 2017) for two reasons: (1) As revealed in (Zhang et al., 2020) , the sentence embedding performance varies greatly as the domain of the training data changes. As a dialogue dataset is always about several certain domains, evaluating on the STS benchmark may mis-lead the evaluation of the model. (2) The dialoguebased sentence embeddings focus on context-aware rather than context-free semantic meanings, which may not be suitable to be evaluated through the context-free benchmarks. Since previous dialoguebased works have not set up a uniform benchmark, we construct two evaluation datasets for each dialogue corpus. A total of 18,964 retrieval samples and 4,000 sentence pairs are annotated by seven native speakers through the crowd-sourcing platform1 . The evaluation results indicate that DialogueCSE significantly outperforms the baselines on the three datasets in terms of both MAP and Spearman's correlation metrics, demonstrating its effectiveness. Further quantitative experiments show that Dia-logueCSE achieves better performance when leveraging more dialogue context and remains robust when less training data is provided. To sum up, our contributions are threefold:",
                "cite_spans": [
                    {
                        "start": 94,
                        "end": 111,
                        "text": "(Li et al., 2018)",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 151,
                        "end": 170,
                        "text": "(Chen et al., 2020)",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 214,
                        "end": 234,
                        "text": "(Zhang et al., 2018)",
                        "ref_id": "BIBREF34"
                    },
                    {
                        "start": 464,
                        "end": 482,
                        "text": "(Cer et al., 2017)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 519,
                        "end": 539,
                        "text": "(Zhang et al., 2020)",
                        "ref_id": "BIBREF33"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\u2022 We propose DialogueCSE, a dialogue-based contrastive learning approach with MGE mechanism for learning sentence embeddings from dialogues. As far as we know, this is the first attempt to apply contrastive learning in this area.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\u2022 We construct the dialogue-based sentence embedding evaluation benchmarks for three dialogue corpus. All of the datasets will be released to facilitate the follow-up researches.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\u2022 Extensive experiments show that Dia-logueCSE significantly outperforms the baselines, establishing the state-of-the-art results.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "2 Related Work",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Early works on sentence embeddings mainly focus on the self-supervised learning approaches. Kiros et al. (2015) train a seq2seq network by decoding the token-level sequences of the context in the corpus. Hill et al. (2016) propose to predict the neighboring sentences as bag-of-words instead of step-by-step decoding. Logeswaran and Lee (2018) perform sentence-level modeling by retrieving the ground-truth sentence from candidates under the given context, achieving consistently better performance compared to the previous token-level modeling approaches. The datasets used in these works are typically built upon the corpus of web pages and books (Zhu et al., 2015) . As the semantic connections are relatively weak in these corpora, the model performances in these works are inherently limited and hard to achieve further improvement.",
                "cite_spans": [
                    {
                        "start": 92,
                        "end": 111,
                        "text": "Kiros et al. (2015)",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 204,
                        "end": 222,
                        "text": "Hill et al. (2016)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 318,
                        "end": 343,
                        "text": "Logeswaran and Lee (2018)",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 649,
                        "end": 667,
                        "text": "(Zhu et al., 2015)",
                        "ref_id": "BIBREF35"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Self-supervised Learning Approaches",
                "sec_num": "2.1"
            },
            {
                "text": "Recently, the pre-trained language models such as BERT (Devlin et al., 2019) and GPT (Radford et al.) yield strong performances across many downstream tasks (Wang et al., 2018) . However, BERT's embeddings show poor performance without fine-tuning and many efforts have been devoted to alleviating this issue. Zhang et al. (2020) propose a self-supervised learning approach that derives meaningful BERT sentence embeddings by maximizing the mutual information between the global sentence embedding and all its local context embeddings. Li et al. (2020) argue that BERT induces a non-smooth anisotropic semantic space. They propose to use a flow-based generative module to transform BERT's embeddings into isotropic semantic space. Similar to this work, Su et al. (2021) replace the flow-based generative module with a simple but efficient linear mapping layer, achieving competitive results with reported experiments in BERT-flow.",
                "cite_spans": [
                    {
                        "start": 55,
                        "end": 76,
                        "text": "(Devlin et al., 2019)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 85,
                        "end": 101,
                        "text": "(Radford et al.)",
                        "ref_id": null
                    },
                    {
                        "start": 157,
                        "end": 176,
                        "text": "(Wang et al., 2018)",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 310,
                        "end": 329,
                        "text": "Zhang et al. (2020)",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 536,
                        "end": 552,
                        "text": "Li et al. (2020)",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 753,
                        "end": 769,
                        "text": "Su et al. (2021)",
                        "ref_id": "BIBREF27"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Self-supervised Learning Approaches",
                "sec_num": "2.1"
            },
            {
                "text": "Lately, the contrastive self-supervised learning approaches have shown their effectiveness and merit in this area. Wu et al. (2020); Giorgi et al. (2020) ; Meng et al. (2021) incorporate the data augmentation methods including the word-level deletion, reordering, substitution, and the sentencelevel corruption into the pre-training of deep Transformer models to improve the sentence representation ability, achieving significantly better performance than BERT especially on the sentence-level tasks (Wang et al., 2018; Cer et al., 2017; Conneau and Kiela, 2018) . Gao et al. (2021) apply a twice independent dropout to obtain two same-source embeddings from a single sentence as input. Through optimizing their cosine distance, SimCSE achieves remarkable gains over the previous baselines. Yan et al. (2021) empirically study more data augmentation strategies in learning sentence embeddings, and it also achieves remarkable performance as SimCSE. In this work, we propose the MGE mechanism to generate a context-aware embedding for each candidate response based on its context-free embedding. Different from previous methods built upon the data augmentation strategies, MGE leverages the context to accomplish this goal without any text corruption.",
                "cite_spans": [
                    {
                        "start": 115,
                        "end": 132,
                        "text": "Wu et al. (2020);",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 133,
                        "end": 153,
                        "text": "Giorgi et al. (2020)",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 156,
                        "end": 174,
                        "text": "Meng et al. (2021)",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 500,
                        "end": 519,
                        "text": "(Wang et al., 2018;",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 520,
                        "end": 537,
                        "text": "Cer et al., 2017;",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 538,
                        "end": 562,
                        "text": "Conneau and Kiela, 2018)",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 565,
                        "end": 582,
                        "text": "Gao et al. (2021)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 791,
                        "end": 808,
                        "text": "Yan et al. (2021)",
                        "ref_id": "BIBREF31"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Self-supervised Learning Approaches",
                "sec_num": "2.1"
            },
            {
                "text": "For dialogue, Yang et al. (2018) train a siamese transformer network with single-turn inputresponse pairs extracted from Reddit. Such architecture is further extended in (Reimers and Gurevych, 2019) by replacing the transformer encoder with BERT. Henderson et al. (2020) propose to leverage the dialogue context to improve the sentence embedding performance. They concatenate the multi-turn dialogue context into a long word sequence and adopt a similar architecture as (Yang et al., 2018) to model the context-response matching relationships. Our work is closely related to their works. We propose a novel dialogue-based contrastive learning approach, which directly models the context-response matching relationships without an intermediate MLP. We also consider the interactions between each utterance in the dialogue context and the response instead of simply treating the dialogue context as a long sequence.",
                "cite_spans": [
                    {
                        "start": 14,
                        "end": 32,
                        "text": "Yang et al. (2018)",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 170,
                        "end": 198,
                        "text": "(Reimers and Gurevych, 2019)",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 247,
                        "end": 270,
                        "text": "Henderson et al. (2020)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 470,
                        "end": 489,
                        "text": "(Yang et al., 2018)",
                        "ref_id": "BIBREF32"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Self-supervised Learning Approaches",
                "sec_num": "2.1"
            },
            {
                "text": "The supervised learning approaches mainly focus on training classification models with the SNLI and the MNLI datasets (Bowman et al., 2015; Williams et al., 2018) . Conneau et al. (2017) demonstrate the superior performance of the supervised learning model on both the STS-benchmark (Cer et al., 2017) and the SICK-R tasks (Marelli et al., 2014) . Based on this observation, Cer et al. (2018) further extend the supervised learning to the multi-task learning by introducing the QA prediction task, the Skip-Thought-like task (Henderson et al., 2017; Kiros et al., 2015) , and the NLI classification task, achieving significant improvement over InferSent. Reimers and Gurevych (2019) employ BERT as sentence encoders in the siamese-network and finetune them with the SNLI and the MNLI datasets, achieving the new state-of-the-art performance.",
                "cite_spans": [
                    {
                        "start": 118,
                        "end": 139,
                        "text": "(Bowman et al., 2015;",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 140,
                        "end": 162,
                        "text": "Williams et al., 2018)",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 165,
                        "end": 186,
                        "text": "Conneau et al. (2017)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 283,
                        "end": 301,
                        "text": "(Cer et al., 2017)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 323,
                        "end": 345,
                        "text": "(Marelli et al., 2014)",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 375,
                        "end": 392,
                        "text": "Cer et al. (2018)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 525,
                        "end": 549,
                        "text": "(Henderson et al., 2017;",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 550,
                        "end": 569,
                        "text": "Kiros et al., 2015)",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 655,
                        "end": 682,
                        "text": "Reimers and Gurevych (2019)",
                        "ref_id": "BIBREF25"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Supervised Learning Approaches",
                "sec_num": "2.2"
            },
            {
                "text": "Suppose that we have a dialogue dataset D = {S i } K i=1 , where S i = {u 1 , \u2022 \u2022 \u2022 , u k-1 , r, u k+1 , \u2022 \u2022 \u2022 , u t } is the i-th dia- logue session in D with t turn utterances. r is the response and C i = {u 1 , \u2022 \u2022 \u2022 u k-1 , u k+1 , \u2022 \u2022 \u2022 , u t }",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Problem Formulation",
                "sec_num": "3"
            },
            {
                "text": "is the bi-directional context around r. We omit the subscript i in the following paragraph and use S, C instead of S i , C i for brevity.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Problem Formulation",
                "sec_num": "3"
            },
            {
                "text": "To generate the contrastive training pairs, we introduce two embedding matrices for r, named context-free embedding matrix and context-aware embedding matrix. Specifically, we first encode r as an embedding matrix R. Since R is encoded independently of the dialogue context, it is treated as the context-free embedding matrix. Then we generate a corresponding embedding matrix R based on R according to the guidance of C. R is treated as the context-aware embedding matrix. As C and r are derived from the same dialogue, ( R, R) naturally forms a positive training pair. To construct a negative training pair, we first sample an utterance r from a dialogue randomly selected from D. r is encoded as the context-free embedding matrix R based on which a context-aware embedding matrix R is generated through the completely identical process. ( R , R ) is treated as a negative training pair. For each response r, we generate a positive training pair (since there is only one ground-truth response for each context) and multiple negative training pairs. All the training pairs are then passed through the contrastive learning module.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Problem Formulation",
                "sec_num": "3"
            },
            {
                "text": "It is worth to mention that there is no difference between sampling the response or the context as they are symmetrical in constructing the negative training pairs. But we prefer the former as it is more straightforward and in accordance with the previous retrieval-based works for dialogues. With all the training samples at hand, our goal is to minimize their contrastive loss, thus fine-tuning BERT as a context-aware sentence encoder.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Problem Formulation",
                "sec_num": "3"
            },
            {
                "text": "Figure 1 shows the model architecture. Our model is divided into three stages: sentence encoding, matching-guided embedding, and turn aggregation. We describe each part as below.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "1",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Our Approach",
                "sec_num": "4"
            },
            {
                "text": "We adopt BERT (Devlin et al., 2019) as the sentence encoder. Let u represent a certain utterance in C. u and r are first encoded as two sequences of output embeddings, which is formulated as:",
                "cite_spans": [
                    {
                        "start": 14,
                        "end": 35,
                        "text": "(Devlin et al., 2019)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Sentence Encoding",
                "sec_num": "4.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "{u 1 , u 2 , \u2022 \u2022 \u2022 , u n } = BERT(u), (1) {r 1 , r 2 , \u2022 \u2022 \u2022 , r n } = BERT(r),",
                        "eq_num": "(2)"
                    }
                ],
                "section": "Sentence Encoding",
                "sec_num": "4.1"
            },
            {
                "text": "where u i , r j represent the i-th and the j-th output embedding derived from u and r respectively. n is the maximum sequence length of both input sentences. \u2200i, j \u2208 1, 2, \u2022 \u2022 \u2022 , n, the shapes of u i and r j are 1 \u00d7 d, where d is the dimension of BERT's outputs. We stack {u 1 , u 2 , \u2022 \u2022 \u2022 , u n } and {r 1 , r 2 , \u2022 \u2022 \u2022 , r n } to obtain the context-free embedding matrices \u016a and R, whose shapes are both n \u00d7 d.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Sentence Encoding",
                "sec_num": "4.1"
            },
            {
                "text": "The matching-guided embedding mechanism performs a token-level matching operation on \u016a and R to form a matching matrix M, which is formulated as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Matching-Guided Embedding",
                "sec_num": "4.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "M = \u016a R T \u221a d ,",
                        "eq_num": "(3)"
                    }
                ],
                "section": "Matching-Guided Embedding",
                "sec_num": "4.2"
            },
            {
                "text": "Then it generates a refined embedding matrix R based on the context-free embedding matrix R, which is given by:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Matching-Guided Embedding",
                "sec_num": "4.2"
            },
            {
                "text": "R = M R (4)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Matching-Guided Embedding",
                "sec_num": "4.2"
            },
            {
                "text": "R is a new representation of r from the perspective of the utterance u. Note that as u is only a single turn utterance in C, we generate t -1 refined embedding matrices for r in total.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Matching-Guided Embedding",
                "sec_num": "4.2"
            },
            {
                "text": "After obtaining all of the refined embedding matrices across turns, we consider two strategies to fuse them to obtain the final context-aware embedding matrix R. The first strategy adopts a weighted sum operation based on the attention mechanism, formulated by:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Turn Aggregation",
                "sec_num": "4.3"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "R = i \u03b1 i Ri ,",
                        "eq_num": "(5)"
                    }
                ],
                "section": "Turn Aggregation",
                "sec_num": "4.3"
            },
            {
                "text": "where",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Turn Aggregation",
                "sec_num": "4.3"
            },
            {
                "text": "i \u2208 {1, \u2022 \u2022 \u2022 , k -1, k + 1, \u2022 \u2022 \u2022 , t}",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Turn Aggregation",
                "sec_num": "4.3"
            },
            {
                "text": "and Ri is the refined embedding matrix corresponding to the i-th turn utterance in the context. The attention weight \u03b1 i is decided by:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Turn Aggregation",
                "sec_num": "4.3"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "\u03b1 i = exp(FFN( Ri )) j exp(FFN( Rj )) , (",
                        "eq_num": "6"
                    }
                ],
                "section": "Turn Aggregation",
                "sec_num": "4.3"
            },
            {
                "text": ")",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Turn Aggregation",
                "sec_num": "4.3"
            },
            {
                "text": "where FFN is a two-layer feed-forward network with ReLU (Nair and Hinton, 2010) activation function. We denote this strategy as I 1 . The second strategy I 2 directly sums up all the refined embeddings across turns, which is defined as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Turn Aggregation",
                "sec_num": "4.3"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "R = 1 t -1 i Ri ,",
                        "eq_num": "(7)"
                    }
                ],
                "section": "Turn Aggregation",
                "sec_num": "4.3"
            },
            {
                "text": "For the negative sample r , we apply the same procedure to generate the context-free embedding matrix R and the context-aware embedding R . Each context-aware embedding matrix is then paired with its corresponding context-free embedding matrix to form a training pair.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Turn Aggregation",
                "sec_num": "4.3"
            },
            {
                "text": "As mentioned in the introduction, MGE holds several advantages in modeling the contextresponse semantic relationships. Firstly, the tokenlevel matching operation acts as a guide to distill the context-aware information from the contextfree embedding matrix. Meanwhile, it provides rich semantic matching information to assist the generation of the context-aware embedding matrix. Secondly, MGE is lightweight and computationally efficient, which makes the model easier to train than the siamese-network-based models. Finally and most importantly, the context-aware embedding R shares the same semantic space with R, which enables us to directly measure their cosine similarity. This is the key to successfully model the semantic matching relationships between the context and the response through contrastive learning.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Turn Aggregation",
                "sec_num": "4.3"
            },
            {
                "text": "We adopt the NT-Xent loss proposed in (Oord et al., 2018) to train our model. The loss L is formulated as:",
                "cite_spans": [
                    {
                        "start": 38,
                        "end": 57,
                        "text": "(Oord et al., 2018)",
                        "ref_id": "BIBREF22"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Objective",
                "sec_num": "4.4"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "L = - 1 N N i=1 log e sim( Ri , Ri )/\u03c4 M j=1 e sim( Rj , Rj )/\u03c4 , (",
                        "eq_num": "8"
                    }
                ],
                "section": "Learning Objective",
                "sec_num": "4.4"
            },
            {
                "text": ")",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Objective",
                "sec_num": "4.4"
            },
            {
                "text": "where N is the number of all the positive training samples and M is the number of all the training pairs associated with each positive training sample r. \u03c4 is the temperature hyper-parameter. sim(\u2022, \u2022) is the similarity function, defined as a token-level pooling operation followed by the cosine similarity.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Objective",
                "sec_num": "4.4"
            },
            {
                "text": "Once the model is trained, we take the mean pooling of BERT's output embeddings as the sentence embedding.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Objective",
                "sec_num": "4.4"
            },
            {
                "text": "We conduct experiments on three multi-turn dialogue datasets: the Microsoft Dialogue Corpus (MDC) (Li et al., 2018) , the Jing Dong Dialogue Corpus (JDDC) (Chen et al., 2020) , and the Ecommerce Dialogue Corpus (ECD) (Zhang et al., 2018) . Each utterance in these three datasets is originally assigned with an intent label, which is further leveraged by us in the heuristic strategy to construct the evaluation datasets. JD2 . Although the dataset collected from the realworld scenario is quite large, it contains much noise which brings great challenges for our model. The E-commerce Dialogue Corpus is a large-scale dialogue dataset collected from Taobao3 . The released dataset takes the form of the response selection task.",
                "cite_spans": [
                    {
                        "start": 98,
                        "end": 115,
                        "text": "(Li et al., 2018)",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 155,
                        "end": 174,
                        "text": "(Chen et al., 2020)",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 217,
                        "end": 237,
                        "text": "(Zhang et al., 2018)",
                        "ref_id": "BIBREF34"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "5"
            },
            {
                "text": "We recover it to the dialogue sessions by dropping the negative samples and splitting the context into multiple utterances. We pre-process these datasets by the following steps: (1) We combine the consecutive utterances of the same speaker.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training",
                "sec_num": "5.1.1"
            },
            {
                "text": "(2) We discard the dialogues with less than 4 turns in JDDC and ECD since such dialogues are usually incomplete in practice.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training",
                "sec_num": "5.1.1"
            },
            {
                "text": "We introduce the semantic retrieval (SR) and the dialogue-based STS (D-STS) tasks to evaluate our model. For the SR task, we construct evaluation datasets by the following steps: (1) we sample a large number of sentences with the intent labels as candidates.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation",
                "sec_num": "5.1.2"
            },
            {
                "text": "(2) the candidates are annotated with binary labels indicating whether the given sentence and its intent label are consistent. The inconsistent instances are directly discarded from the candidates.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation",
                "sec_num": "5.1.2"
            },
            {
                "text": "(3) for each sentence, we retrieval 100 sentences through BM25 (Robertson and Zaragoza, 2009) from the candidates, and assign each candidate sentence a label by whether its intent is consistent with the target sentence. We limit the number of positive samples to a maximum of 30 and keep approximately 7k, 7k, and 4k samples for MDC, JDDC, and ECD respectively. For the D-STS task, we sample the sentence pairs from the dialogues following the heuristic strategies proposed by (Cer et al., 2017) to ensure there are enough semantically similar samples. The heuristic strategies include unigram-based and w2v-based KNN retrieval methods and random sampling from the candidates with the same intent labels. The sentence pairs are further annotated through the crowd-sourcing platform, with five degrees ranging from 1 to 5 according to their semantic relevance. We use the median number of annotated results as the semantic relevance degrees, obtaining 1k, 2k, and 1k sentence pairs for MDC, JDDC, and ECD respectively.",
                "cite_spans": [
                    {
                        "start": 63,
                        "end": 93,
                        "text": "(Robertson and Zaragoza, 2009)",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 477,
                        "end": 495,
                        "text": "(Cer et al., 2017)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation",
                "sec_num": "5.1.2"
            },
            {
                "text": "All annotations are carried out by seven native speakers. For the SR task, we adopt the Mean average precision (MAP) and the Mean reciprocal rank (MRR) metrics. Following previous works, we adopt Spearman's correlation metric for the D-STS task to assess the quality of the dialogue-based sentence embeddings.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation",
                "sec_num": "5.1.2"
            },
            {
                "text": "We evaluate our model against the two groups of baselines: self-supervised learning methods and dialogue-based self-supervised learning methods. The former is not designed for dialogues while the latter is.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baselines",
                "sec_num": "5.2"
            },
            {
                "text": "In this line, we consider the BERT-based methods, which include BERT (Devlin et al., 2019) , domain-adaptive BERT (Gururangan et al., 2020) , BERT-flow (Li et al., 2020) , and BERT-whitening (Su et al., 2021) . \"Domain-adaptive BERT\" means that we run continue pre-training with the dialogue datasets. BERT-flow and BERT-whitening are two BERT-based variants that transform BERT's sentence embedding to the isotropic semantic space.",
                "cite_spans": [
                    {
                        "start": 69,
                        "end": 90,
                        "text": "(Devlin et al., 2019)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 114,
                        "end": 139,
                        "text": "(Gururangan et al., 2020)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 152,
                        "end": 169,
                        "text": "(Li et al., 2020)",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 191,
                        "end": 208,
                        "text": "(Su et al., 2021)",
                        "ref_id": "BIBREF27"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Self-supervised learning methods",
                "sec_num": "5.2.1"
            },
            {
                "text": "For BERT, we use the [CLS] token embedding (denoted as BERT-CLS) and the average of the sequence output embeddings (denoted as BERT-avg) as the sentence embedding, and the same is true for domain-adaptive BERT. It should be noted that in related sentence embedding researches, domainadaptive BERT is rarely considered since the training datasets are relatively small. Fortunately, the large-scale dialogue datasets allow us to explore whether the domain-adaptive pre-training is helpful for our tasks. We also adopt the average of GloVe word embeddings (Pennington et al., 2014 ) (denoted as Avg. GloVe) as the sentence embedding to compare with our results.",
                "cite_spans": [
                    {
                        "start": 553,
                        "end": 577,
                        "text": "(Pennington et al., 2014",
                        "ref_id": "BIBREF23"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Self-supervised learning methods",
                "sec_num": "5.2.1"
            },
            {
                "text": "In this line, we mainly consider the siamesenetworks commonly applied in dialogue-based researches. Considering none of the previous works (Yang et al., 2018; Henderson et al., 2020) employs the pre-trained language model as encoder, we re- implement two BERT-based siamese-network models according to their original approaches. The first baseline SiameseBERT s is a siamese-network which shares the architecture with (Yang et al., 2018; Reimers and Gurevych, 2019) . It is equipped with a non-linear activation function in the matching layer to model the heterogeneous matching relationships between the context and the response4 .",
                "cite_spans": [
                    {
                        "start": 139,
                        "end": 158,
                        "text": "(Yang et al., 2018;",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 159,
                        "end": 182,
                        "text": "Henderson et al., 2020)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 418,
                        "end": 437,
                        "text": "(Yang et al., 2018;",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 438,
                        "end": 465,
                        "text": "Reimers and Gurevych, 2019)",
                        "ref_id": "BIBREF25"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dialogue-based self-supervised learning methods",
                "sec_num": "5.2.2"
            },
            {
                "text": "The second baseline SiameseBERT m has the similar architecture as (Henderson et al., 2020) . It flattens the multi-turn context and takes the token sequence as input. There is also an MLP layer on top of the sentence encoders.",
                "cite_spans": [
                    {
                        "start": 66,
                        "end": 90,
                        "text": "(Henderson et al., 2020)",
                        "ref_id": "BIBREF13"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dialogue-based self-supervised learning methods",
                "sec_num": "5.2.2"
            },
            {
                "text": "Our approach is implemented in Tensorflow (Abadi et al., 2016) with CUDA 10.0 support. For all datasets, we continue pre-training BERT for approximately 0.5 epochs to improve its domain adaption ability as well as keeping the general domain information as much as possible. During the continue pre-training stage, we use a masking probability of 0.15, a learning rate of 2e-5, a batch size of 50, and a maximum of 10 masked LM predictions per sequence. During the contrastive learning stage, we freeze the bottom 6 layers of BERT to prevent catastrophic forgetting which simultaneously en-ables the model to be trained with larger batch size. Such a setting achieves the best performance in our experiments. The batch size, the learning rate, and the number of context turns are set to 20, 5e-5, and 3 respectively. The maximum sequence length is set to 100, 50, 50 for JDDC, MDC, and ECD for both continue pre-training stage and contrastive learning stage. All models are trained on 4 Tesla V100 GPUs.",
                "cite_spans": [
                    {
                        "start": 42,
                        "end": 62,
                        "text": "(Abadi et al., 2016)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementation Details",
                "sec_num": "5.3"
            },
            {
                "text": "Table 2 shows the main experimental results on the three datasets. From the There are even larger improvements between Di-alogueCSE and the domain-adaptive baselines including BERT(adapt) and its variants. We attribute this improvement to two main reasons: First, by introducing contrastive learning, DialogueCSE eliminates the gap between training and evaluating, gaining significant improvements on both SR and D-STS tasks. Second, DialogueCSE models the semantic relationships in each utterance-response pair, which distills the important information at turn-level from the multi-turn dialogue context and achieves better performance. Moreover, by comparing the performances of DialogueCSE I 1 and DialogueCSE I 2 , we find that the weighted sum aggregation strategy surprisingly brings a significant deterioration on all metrics. We consider that this is because the weighted sum operation breaks down the turn-level unbiased aggregation process. Since the attention mechanism tends to provide shortcuts for the model to achieve its learning objective, the long-tail utterances in the context may be partially ignored, thus leading to a decline in embedding performance. We hold that we can completely dismiss the weighted sum aggregation strategy in DialogueCSE since the tokenlevel matching operation in MGE has implicitly served this role.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "2",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Evaluation Results",
                "sec_num": "5.4"
            },
            {
                "text": "We also notice that BERT(adapt) achieves significantly better performance than the original BERT, especially on JDDC and ECD. It demonstrates the importance of continued pre-training with the indomain training data. Without such procedure, the in-domain data can't be fully exploited, making it difficult for the model to achieve satisfactory performance. This also indicates that the MLM pre-training task is indeed a powerful task to learn effective sentence embeddings from texts, especially when the domain training data is sufficient.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation Results",
                "sec_num": "5.4"
            },
            {
                "text": "We conduct comparison and hyper-parameter experiments in the following section to study how our model performs with different numbers of turns, data scales, temperature hyper-parameter, and numbers of negative samples.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discussion",
                "sec_num": "5.5"
            },
            {
                "text": "In this section, we choose SiameseBERT m as a comparison method. MAP and Spearman's correlation metrics are adopted in these experiments.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Comparison with Baseline",
                "sec_num": "5.5.1"
            },
            {
                "text": "Impact of turn number. Figure 2 shows the performance of our model and the baseline under different numbers of turns on all datasets. From the results, we observe that our model is indeed benefited from the multi-turn dialogue context, and it exhibits consistently better performance than the baseline. The performance of our model increases as the turn number increases until it approximately arrives at 3. When the turn number goes bigger, the performance of both models begins to drop.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 30,
                        "end": 31,
                        "text": "2",
                        "ref_id": "FIGREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Comparison with Baseline",
                "sec_num": "5.5.1"
            },
            {
                "text": "We believe that in this case, adding more dialogue context will bring too much noise. Since MGE acts as a noise filter at both token and turn level, it makes the model more robust when using more context turns. Impact of data scale. We further explore whether our model is robust when fewer training samples are given. we select JDDC and ECD in this experiment since they are large-scale and topically diverse, which is suitable for simulating a few-shot learning scenario. Figure 3 shows the performances of our model and the baseline under different numbers of training dialogues. As the figure reveals, the performance gaps between our model and the baseline are even larger when fewer training dialogue sessions are given. Particularly, when using only a few dialogues, our model can achieve even superior performance over the SiameseBERT trained on larger datasets, especially on the D-STS task. We think this is reasonable since the siamese-networks introduce a large amount parameters to model the semantic matching relationships, while our model accomplishes this goal without introducing any additional parameters.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 481,
                        "end": 482,
                        "text": "3",
                        "ref_id": "FIGREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Comparison with Baseline",
                "sec_num": "5.5.1"
            },
            {
                "text": "We further conduct experiments on JDDC and EDC to study how our model is influenced by the temperature \u03c4 and the number of negative samples. The MDC dataset is excluded here since the semantics of its utterances are highly centralized around a few top intents. Impact of temperature. Table 3 shows the experimental results with different \u03c4 values. We find that the Spearman's correlations increase monotonically as \u03c4 increases until 0.1 for JDDC and 0.2 for ECD, then they begin to drop. The MAP metrics also increase as \u03c4 increases until 0.1 for both datasets, but they remain stable as \u03c4 varies from 0.1 to 0.5. We consider this is due to the coarsegrained nature of the SR task. When \u03c4 approaches 0.1, our model can gradually distinguish among different fine-grained semantics, thus achieving better performance on both SR and D-STS tasks. As \u03c4 continues to increase, the model forces the sentence embeddings to be closer, resulting in a decrease in Spearman's correlation. However, as all positive samples in the candidates have identical labels, such degradation may not be fully reflected through the ranking metric (e.g. MAP) or even be covered as the number of retrieved positive samples changes.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 290,
                        "end": 291,
                        "text": "3",
                        "ref_id": "TABREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Hyper-parameter Evaluations",
                "sec_num": "5.5.2"
            },
            {
                "text": "Impact of negative samples. We vary the number of negative samples for each positive sample within {1, 4, 9, 19}. Table 4 shows the experimental results, from which we find that both metrics improve slightly when the number of negative samples increases. Considering the similar observation in (Gao et al., 2021; Yan et al., 2021) , we conclude this phenomenon may be related to the discrete nature of language. Specifically, as the generation of the sentence embeddings in our approach is guided and constrained by the token-level interaction mechanism, our model is more robust than the other contrastive learning approaches and is even effective when only one negative sample is provided.",
                "cite_spans": [
                    {
                        "start": 294,
                        "end": 312,
                        "text": "(Gao et al., 2021;",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 313,
                        "end": 330,
                        "text": "Yan et al., 2021)",
                        "ref_id": "BIBREF31"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 120,
                        "end": 121,
                        "text": "4",
                        "ref_id": "TABREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Hyper-parameter Evaluations",
                "sec_num": "5.5.2"
            },
            {
                "text": "In this work, we propose DialogueCSE, a dialoguebased contrastive learning approach to learn sentence embeddings from dialogues. We also propose uniform evaluation benchmarks for evaluating the quality of the dialogue-based sentence embeddings. Evaluation results show that DialogueCSE achieves the best result over the baselines while adding no additional parameters. In the next step, we will study how to introduce more interaction information to learn the sentence embeddings and try to incorporate the contrast learning method into the pre-training stage.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "6"
            },
            {
                "text": "All the datasets will be publicly available at https://github.com/wangruicn/DialogueCSE",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://www.jd.com",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://www.taobao.com",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "We use \"heterogeneous\" to describe the matching relationships for context-response pairs since they have different semantic meanings. As a comparison, the NIL-like sentence pairs have the \"homogeneous\" matching relationships.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Tensorflow: A system for large-scale machine learning",
                "authors": [
                    {
                        "first": "Mart\u00edn",
                        "middle": [],
                        "last": "Abadi",
                        "suffix": ""
                    },
                    {
                        "first": "Paul",
                        "middle": [],
                        "last": "Barham",
                        "suffix": ""
                    },
                    {
                        "first": "Jianmin",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Zhifeng",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Andy",
                        "middle": [],
                        "last": "Davis",
                        "suffix": ""
                    },
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Dean",
                        "suffix": ""
                    },
                    {
                        "first": "Matthieu",
                        "middle": [],
                        "last": "Devin",
                        "suffix": ""
                    },
                    {
                        "first": "Sanjay",
                        "middle": [],
                        "last": "Ghemawat",
                        "suffix": ""
                    },
                    {
                        "first": "Geoffrey",
                        "middle": [],
                        "last": "Irving",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Isard",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "12th USENIX symposium on operating systems design and implementation",
                "volume": "",
                "issue": "",
                "pages": "265--283",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mart\u00edn Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. 2016. Tensorflow: A system for large-scale machine learning. In 12th USENIX symposium on operating systems design and implementation (OSDI 16), pages 265-283.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Conversational contextual cues: The case of personalization and history for response ranking",
                "authors": [
                    {
                        "first": "Rami",
                        "middle": [],
                        "last": "Al-Rfou",
                        "suffix": ""
                    },
                    {
                        "first": "Marc",
                        "middle": [],
                        "last": "Pickett",
                        "suffix": ""
                    },
                    {
                        "first": "Javier",
                        "middle": [],
                        "last": "Snaider",
                        "suffix": ""
                    },
                    {
                        "first": "Yunhsuan",
                        "middle": [],
                        "last": "Sung",
                        "suffix": ""
                    },
                    {
                        "first": "Brian",
                        "middle": [],
                        "last": "Strope",
                        "suffix": ""
                    },
                    {
                        "first": "Ray",
                        "middle": [],
                        "last": "Kurzweil",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Rami Al-Rfou, Marc Pickett, Javier Snaider, Yun- hsuan Sung, Brian Strope, and Ray Kurzweil. 2016. Conversational contextual cues: The case of person- alization and history for response ranking. arXiv e- prints, pages arXiv-1606.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "A large annotated corpus for learning natural language inference",
                "authors": [
                    {
                        "first": "Samuel",
                        "middle": [],
                        "last": "Bowman",
                        "suffix": ""
                    },
                    {
                        "first": "Gabor",
                        "middle": [],
                        "last": "Angeli",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [],
                        "last": "Potts",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "632--642",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Samuel Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632-642.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Semeval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation",
                "authors": [
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Cer",
                        "suffix": ""
                    },
                    {
                        "first": "Mona",
                        "middle": [],
                        "last": "Diab",
                        "suffix": ""
                    },
                    {
                        "first": "Eneko",
                        "middle": [],
                        "last": "Agirre",
                        "suffix": ""
                    },
                    {
                        "first": "I\u00f1igo",
                        "middle": [],
                        "last": "Lopez-Gazpio",
                        "suffix": ""
                    },
                    {
                        "first": "Lucia",
                        "middle": [],
                        "last": "Specia",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the 11th International Workshop on Semantic Evaluation",
                "volume": "",
                "issue": "",
                "pages": "1--14",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Daniel Cer, Mona Diab, Eneko Agirre, I\u00f1igo Lopez- Gazpio, and Lucia Specia. 2017. Semeval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of the 11th International Workshop on Semantic Evalu- ation (SemEval-2017), pages 1-14.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Universal sentence encoder for english",
                "authors": [
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Cer",
                        "suffix": ""
                    },
                    {
                        "first": "Yinfei",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Sheng-Yi",
                        "middle": [],
                        "last": "Kong",
                        "suffix": ""
                    },
                    {
                        "first": "Nan",
                        "middle": [],
                        "last": "Hua",
                        "suffix": ""
                    },
                    {
                        "first": "Nicole",
                        "middle": [],
                        "last": "Limtiaco",
                        "suffix": ""
                    },
                    {
                        "first": "Rhomni",
                        "middle": [],
                        "last": "St John",
                        "suffix": ""
                    },
                    {
                        "first": "Noah",
                        "middle": [],
                        "last": "Constant",
                        "suffix": ""
                    },
                    {
                        "first": "Mario",
                        "middle": [],
                        "last": "Guajardo-Cespedes",
                        "suffix": ""
                    },
                    {
                        "first": "Steve",
                        "middle": [],
                        "last": "Yuan",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Tar",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
                "volume": "",
                "issue": "",
                "pages": "169--174",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, et al. 2018. Universal sentence encoder for english. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 169-174.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "The jddc corpus: A large-scale multi-turn chinese dialogue dataset for e-commerce customer service",
                "authors": [
                    {
                        "first": "Meng",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Ruixue",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Lei",
                        "middle": [],
                        "last": "Shen",
                        "suffix": ""
                    },
                    {
                        "first": "Shaozu",
                        "middle": [],
                        "last": "Yuan",
                        "suffix": ""
                    },
                    {
                        "first": "Jingyan",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Youzheng",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaodong",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Bowen",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of The 12th Language Resources and Evaluation Conference",
                "volume": "",
                "issue": "",
                "pages": "459--466",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Meng Chen, Ruixue Liu, Lei Shen, Shaozu Yuan, Jingyan Zhou, Youzheng Wu, Xiaodong He, and Bowen Zhou. 2020. The jddc corpus: A large-scale multi-turn chinese dialogue dataset for e-commerce customer service. In Proceedings of The 12th Lan- guage Resources and Evaluation Conference, pages 459-466.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Senteval: An evaluation toolkit for universal sentence representations",
                "authors": [
                    {
                        "first": "Alexis",
                        "middle": [],
                        "last": "Conneau",
                        "suffix": ""
                    },
                    {
                        "first": "Douwe",
                        "middle": [],
                        "last": "Kiela",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1803.05449"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Alexis Conneau and Douwe Kiela. 2018. Senteval: An evaluation toolkit for universal sentence representa- tions. arXiv preprint arXiv:1803.05449.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Supervised learning of universal sentence representations from natural language inference data",
                "authors": [
                    {
                        "first": "Alexis",
                        "middle": [],
                        "last": "Conneau",
                        "suffix": ""
                    },
                    {
                        "first": "Douwe",
                        "middle": [],
                        "last": "Kiela",
                        "suffix": ""
                    },
                    {
                        "first": "Holger",
                        "middle": [],
                        "last": "Schwenk",
                        "suffix": ""
                    },
                    {
                        "first": "Lo\u00efc",
                        "middle": [],
                        "last": "Barrault",
                        "suffix": ""
                    },
                    {
                        "first": "Antoine",
                        "middle": [],
                        "last": "Bordes",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "670--680",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00efc Barrault, and Antoine Bordes. 2017. Supervised learning of universal sentence representations from natural language inference data. In Proceedings of the 2017 Conference on Empirical Methods in Natu- ral Language Processing, pages 670-680.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
                "authors": [
                    {
                        "first": "Jacob",
                        "middle": [],
                        "last": "Devlin",
                        "suffix": ""
                    },
                    {
                        "first": "Ming-Wei",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "Kenton",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Kristina",
                        "middle": [],
                        "last": "Toutanova",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "volume": "1",
                "issue": "",
                "pages": "4171--4186",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 1 (Long and Short Papers), pages 4171-4186.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Simcse: Simple contrastive learning of sentence embeddings",
                "authors": [
                    {
                        "first": "Tianyu",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Xingcheng",
                        "middle": [],
                        "last": "Yao",
                        "suffix": ""
                    },
                    {
                        "first": "Danqi",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2104.08821"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. Simcse: Simple contrastive learning of sentence em- beddings. arXiv preprint arXiv:2104.08821.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Declutr: Deep contrastive learning for unsupervised textual representations",
                "authors": [
                    {
                        "first": "Osvald",
                        "middle": [],
                        "last": "John M Giorgi",
                        "suffix": ""
                    },
                    {
                        "first": "Gary",
                        "middle": [
                            "D"
                        ],
                        "last": "Nitski",
                        "suffix": ""
                    },
                    {
                        "first": "Bo",
                        "middle": [],
                        "last": "Bader",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2006.03659"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "John M Giorgi, Osvald Nitski, Gary D Bader, and Bo Wang. 2020. Declutr: Deep contrastive learn- ing for unsupervised textual representations. arXiv preprint arXiv:2006.03659.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Don't stop pretraining: Adapt language models to domains and tasks",
                "authors": [
                    {
                        "first": "Suchin",
                        "middle": [],
                        "last": "Gururangan",
                        "suffix": ""
                    },
                    {
                        "first": "Ana",
                        "middle": [],
                        "last": "Marasovi\u0107",
                        "suffix": ""
                    },
                    {
                        "first": "Swabha",
                        "middle": [],
                        "last": "Swayamdipta",
                        "suffix": ""
                    },
                    {
                        "first": "Kyle",
                        "middle": [],
                        "last": "Lo",
                        "suffix": ""
                    },
                    {
                        "first": "Iz",
                        "middle": [],
                        "last": "Beltagy",
                        "suffix": ""
                    },
                    {
                        "first": "Doug",
                        "middle": [],
                        "last": "Downey",
                        "suffix": ""
                    },
                    {
                        "first": "Noah",
                        "middle": [
                            "A"
                        ],
                        "last": "Smith",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "8342--8360",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Suchin Gururangan, Ana Marasovi\u0107, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A Smith. 2020. Don't stop pretraining: Adapt language models to domains and tasks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8342-8360.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Efficient natural language response suggestion for smart reply",
                "authors": [
                    {
                        "first": "Matthew",
                        "middle": [],
                        "last": "Henderson",
                        "suffix": ""
                    },
                    {
                        "first": "Rami",
                        "middle": [],
                        "last": "Al-Rfou",
                        "suffix": ""
                    },
                    {
                        "first": "Brian",
                        "middle": [],
                        "last": "Strope",
                        "suffix": ""
                    },
                    {
                        "first": "Yun-Hsuan",
                        "middle": [],
                        "last": "Sung",
                        "suffix": ""
                    },
                    {
                        "first": "L\u00e1szl\u00f3",
                        "middle": [],
                        "last": "Luk\u00e1cs",
                        "suffix": ""
                    },
                    {
                        "first": "Ruiqi",
                        "middle": [],
                        "last": "Guo",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1705.00652"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Matthew Henderson, Rami Al-Rfou, Brian Strope, Yun- Hsuan Sung, L\u00e1szl\u00f3 Luk\u00e1cs, Ruiqi Guo, Sanjiv Ku- mar, Balint Miklos, and Ray Kurzweil. 2017. Effi- cient natural language response suggestion for smart reply. arXiv preprint arXiv:1705.00652.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Convert: Efficient and accurate conversational representations from transformers",
                "authors": [
                    {
                        "first": "Matthew",
                        "middle": [],
                        "last": "Henderson",
                        "suffix": ""
                    },
                    {
                        "first": "I\u00f1igo",
                        "middle": [],
                        "last": "Casanueva",
                        "suffix": ""
                    },
                    {
                        "first": "Nikola",
                        "middle": [],
                        "last": "Mrk\u0161i\u0107",
                        "suffix": ""
                    },
                    {
                        "first": "Pei-Hao",
                        "middle": [],
                        "last": "Su",
                        "suffix": ""
                    },
                    {
                        "first": "Tsung-Hsien",
                        "middle": [],
                        "last": "Wen",
                        "suffix": ""
                    },
                    {
                        "first": "Ivan",
                        "middle": [],
                        "last": "Vuli\u0107",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings",
                "volume": "",
                "issue": "",
                "pages": "2161--2174",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Matthew Henderson, I\u00f1igo Casanueva, Nikola Mrk\u0161i\u0107, Pei-Hao Su, Tsung-Hsien Wen, and Ivan Vuli\u0107. 2020. Convert: Efficient and accurate conversa- tional representations from transformers. In Pro- ceedings of the 2020 Conference on Empirical Meth- ods in Natural Language Processing: Findings, pages 2161-2174.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Learning distributed representations of sentences from unlabelled data",
                "authors": [
                    {
                        "first": "Felix",
                        "middle": [],
                        "last": "Hill",
                        "suffix": ""
                    },
                    {
                        "first": "Kyunghyun",
                        "middle": [],
                        "last": "Cho",
                        "suffix": ""
                    },
                    {
                        "first": "Anna",
                        "middle": [],
                        "last": "Korhonen",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the 2016 Conference of the North American Chapter",
                "volume": "",
                "issue": "",
                "pages": "1367--1377",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Felix Hill, Kyunghyun Cho, and Anna Korhonen. 2016. Learning distributed representations of sentences from unlabelled data. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1367-1377.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Skip-thought vectors",
                "authors": [
                    {
                        "first": "Ryan",
                        "middle": [],
                        "last": "Kiros",
                        "suffix": ""
                    },
                    {
                        "first": "Yukun",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    },
                    {
                        "first": "Ruslan",
                        "middle": [],
                        "last": "Salakhutdinov",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [
                            "S"
                        ],
                        "last": "Zemel",
                        "suffix": ""
                    },
                    {
                        "first": "Raquel",
                        "middle": [],
                        "last": "Urtasun",
                        "suffix": ""
                    },
                    {
                        "first": "Antonio",
                        "middle": [],
                        "last": "Torralba",
                        "suffix": ""
                    },
                    {
                        "first": "Sanja",
                        "middle": [],
                        "last": "Fidler",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Advances in neural information processing systems",
                "volume": "",
                "issue": "",
                "pages": "3294--3302",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Skip-thought vectors. In Advances in neural information processing systems., page 3294--3302.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "On the sentence embeddings from pre-trained language models",
                "authors": [
                    {
                        "first": "Bohan",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Hao",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Junxian",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Mingxuan",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Yiming",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Lei",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "volume": "",
                "issue": "",
                "pages": "9119--9130",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Bohan Li, Hao Zhou, Junxian He, Mingxuan Wang, Yiming Yang, and Lei Li. 2020. On the sentence embeddings from pre-trained language models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9119-9130.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Microsoft dialogue challenge: Building end-to-end task-completion dialogue systems. arXiv e-prints",
                "authors": [
                    {
                        "first": "Xiujun",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Yu",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Siqi",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Sarah",
                        "middle": [],
                        "last": "Panda",
                        "suffix": ""
                    },
                    {
                        "first": "Jingjing",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Jianfeng",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xiujun Li, Yu Wang, Siqi Sun, Sarah Panda, Jingjing Liu, and Jianfeng Gao. 2018. Microsoft dialogue challenge: Building end-to-end task-completion dia- logue systems. arXiv e-prints, pages arXiv-1807.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "An efficient framework for learning sentence representations",
                "authors": [
                    {
                        "first": "Lajanugen",
                        "middle": [],
                        "last": "Logeswaran",
                        "suffix": ""
                    },
                    {
                        "first": "Honglak",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "International Conference on Learning Representations",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Lajanugen Logeswaran and Honglak Lee. 2018. An ef- ficient framework for learning sentence representa- tions. In International Conference on Learning Rep- resentations.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "A sick cure for the evaluation of compositional distributional semantic models",
                "authors": [
                    {
                        "first": "Marco",
                        "middle": [],
                        "last": "Marelli",
                        "suffix": ""
                    },
                    {
                        "first": "Stefano",
                        "middle": [],
                        "last": "Menini",
                        "suffix": ""
                    },
                    {
                        "first": "Marco",
                        "middle": [],
                        "last": "Baroni",
                        "suffix": ""
                    },
                    {
                        "first": "Luisa",
                        "middle": [],
                        "last": "Bentivogli",
                        "suffix": ""
                    },
                    {
                        "first": "Raffaella",
                        "middle": [],
                        "last": "Bernardi",
                        "suffix": ""
                    },
                    {
                        "first": "Roberto",
                        "middle": [],
                        "last": "Zamparelli",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC'14)",
                "volume": "",
                "issue": "",
                "pages": "216--223",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Marco Marelli, Stefano Menini, Marco Baroni, Luisa Bentivogli, Raffaella Bernardi, and Roberto Zampar- elli. 2014. A sick cure for the evaluation of compo- sitional distributional semantic models. In Proceed- ings of the Ninth International Conference on Lan- guage Resources and Evaluation (LREC'14), pages 216-223.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Coco-lm: Correcting and contrasting text sequences for language model pretraining",
                "authors": [
                    {
                        "first": "Yu",
                        "middle": [],
                        "last": "Meng",
                        "suffix": ""
                    },
                    {
                        "first": "Chenyan",
                        "middle": [],
                        "last": "Xiong",
                        "suffix": ""
                    },
                    {
                        "first": "Payal",
                        "middle": [],
                        "last": "Bajaj",
                        "suffix": ""
                    },
                    {
                        "first": "Saurabh",
                        "middle": [],
                        "last": "Tiwary",
                        "suffix": ""
                    },
                    {
                        "first": "Paul",
                        "middle": [],
                        "last": "Bennett",
                        "suffix": ""
                    },
                    {
                        "first": "Jiawei",
                        "middle": [],
                        "last": "Han",
                        "suffix": ""
                    },
                    {
                        "first": "Xia",
                        "middle": [],
                        "last": "Song",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2102.08473"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yu Meng, Chenyan Xiong, Payal Bajaj, Saurabh Ti- wary, Paul Bennett, Jiawei Han, and Xia Song. 2021. Coco-lm: Correcting and contrasting text sequences for language model pretraining. arXiv preprint arXiv:2102.08473.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Rectified linear units improve restricted boltzmann machines",
                "authors": [
                    {
                        "first": "Vinod",
                        "middle": [],
                        "last": "Nair",
                        "suffix": ""
                    },
                    {
                        "first": "Geoffrey",
                        "middle": [
                            "E"
                        ],
                        "last": "Hinton",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the 27th International Conference on International Conference on Machine Learning",
                "volume": "",
                "issue": "",
                "pages": "807--814",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Vinod Nair and Geoffrey E Hinton. 2010. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th International Conference on International Conference on Machine Learning, pages 807-814.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Representation learning with contrastive predictive coding",
                "authors": [
                    {
                        "first": "Aaron",
                        "middle": [],
                        "last": "Van Den Oord",
                        "suffix": ""
                    },
                    {
                        "first": "Yazhe",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Oriol",
                        "middle": [],
                        "last": "Vinyals",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1807.03748"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning with contrastive pre- dictive coding. arXiv preprint arXiv:1807.03748.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Glove: Global vectors for word representation",
                "authors": [
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Pennington",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)",
                "volume": "",
                "issue": "",
                "pages": "1532--1543",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word rep- resentation. In Proceedings of the 2014 conference on empirical methods in natural language process- ing (EMNLP), pages 1532-1543.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Language models are unsupervised multitask learners",
                "authors": [
                    {
                        "first": "Alec",
                        "middle": [],
                        "last": "Radford",
                        "suffix": ""
                    },
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Rewon",
                        "middle": [],
                        "last": "Child",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Luan",
                        "suffix": ""
                    },
                    {
                        "first": "Dario",
                        "middle": [],
                        "last": "Amodei",
                        "suffix": ""
                    },
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language mod- els are unsupervised multitask learners.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Sentencebert: Sentence embeddings using siamese bertnetworks",
                "authors": [
                    {
                        "first": "Nils",
                        "middle": [],
                        "last": "Reimers",
                        "suffix": ""
                    },
                    {
                        "first": "Iryna",
                        "middle": [],
                        "last": "Gurevych",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "volume": "",
                "issue": "",
                "pages": "3973--3983",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Nils Reimers and Iryna Gurevych. 2019. Sentence- bert: Sentence embeddings using siamese bert- networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu- ral Language Processing (EMNLP-IJCNLP), pages 3973-3983.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "The probabilistic relevance framework: Bm25 and beyond",
                "authors": [
                    {
                        "first": "Stephen",
                        "middle": [],
                        "last": "Robertson",
                        "suffix": ""
                    },
                    {
                        "first": "Hugo",
                        "middle": [],
                        "last": "Zaragoza",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Information Retrieval",
                "volume": "3",
                "issue": "4",
                "pages": "333--389",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Stephen Robertson and Hugo Zaragoza. 2009. The probabilistic relevance framework: Bm25 and be- yond. Information Retrieval, 3(4):333-389.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Whitening sentence representations for better semantics and faster retrieval",
                "authors": [
                    {
                        "first": "Jianlin",
                        "middle": [],
                        "last": "Su",
                        "suffix": ""
                    },
                    {
                        "first": "Jiarun",
                        "middle": [],
                        "last": "Cao",
                        "suffix": ""
                    },
                    {
                        "first": "Weijie",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Yangyiwen",
                        "middle": [],
                        "last": "Ou",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2103.15316"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jianlin Su, Jiarun Cao, Weijie Liu, and Yangyiwen Ou. 2021. Whitening sentence representations for bet- ter semantics and faster retrieval. arXiv preprint arXiv:2103.15316.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Glue: A multi-task benchmark and analysis platform for natural language understanding",
                "authors": [
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Amanpreet",
                        "middle": [],
                        "last": "Singh",
                        "suffix": ""
                    },
                    {
                        "first": "Julian",
                        "middle": [],
                        "last": "Michael",
                        "suffix": ""
                    },
                    {
                        "first": "Felix",
                        "middle": [],
                        "last": "Hill",
                        "suffix": ""
                    },
                    {
                        "first": "Omer",
                        "middle": [],
                        "last": "Levy",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Samuel R Bowman",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1804.07461"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 2018. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "A broad-coverage challenge corpus for sentence understanding through inference",
                "authors": [
                    {
                        "first": "Adina",
                        "middle": [],
                        "last": "Williams",
                        "suffix": ""
                    },
                    {
                        "first": "Nikita",
                        "middle": [],
                        "last": "Nangia",
                        "suffix": ""
                    },
                    {
                        "first": "Samuel",
                        "middle": [],
                        "last": "Bowman",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "volume": "1",
                "issue": "",
                "pages": "1112--1122",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sen- tence understanding through inference. In Proceed- ings of the 2018 Conference of the North American Chapter of the Association for Computational Lin- guistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112-1122.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "Clear: Contrastive learning for sentence representation",
                "authors": [
                    {
                        "first": "Zhuofeng",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Sinong",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Jiatao",
                        "middle": [],
                        "last": "Gu",
                        "suffix": ""
                    },
                    {
                        "first": "Madian",
                        "middle": [],
                        "last": "Khabsa",
                        "suffix": ""
                    },
                    {
                        "first": "Fei",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Hao",
                        "middle": [],
                        "last": "Ma",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2012.15466"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Zhuofeng Wu, Sinong Wang, Jiatao Gu, Madian Khabsa, Fei Sun, and Hao Ma. 2020. Clear: Con- trastive learning for sentence representation. arXiv preprint arXiv:2012.15466.",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "Consert: A contrastive framework for self-supervised sentence representation transfer",
                "authors": [
                    {
                        "first": "Yuanmeng",
                        "middle": [],
                        "last": "Yan",
                        "suffix": ""
                    },
                    {
                        "first": "Rumei",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Sirui",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Fuzheng",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Weiran",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2105.11741"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yuanmeng Yan, Rumei Li, Sirui Wang, Fuzheng Zhang, Wei Wu, and Weiran Xu. 2021. Con- sert: A contrastive framework for self-supervised sentence representation transfer. arXiv preprint arXiv:2105.11741.",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "Learning semantic textual similarity from conversations",
                "authors": [
                    {
                        "first": "Yinfei",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Steve",
                        "middle": [],
                        "last": "Yuan",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Cer",
                        "suffix": ""
                    },
                    {
                        "first": "Sheng-Yi",
                        "middle": [],
                        "last": "Kong",
                        "suffix": ""
                    },
                    {
                        "first": "Noah",
                        "middle": [],
                        "last": "Constant",
                        "suffix": ""
                    },
                    {
                        "first": "Petr",
                        "middle": [],
                        "last": "Pilar",
                        "suffix": ""
                    },
                    {
                        "first": "Heming",
                        "middle": [],
                        "last": "Ge",
                        "suffix": ""
                    },
                    {
                        "first": "Yun-Hsuan",
                        "middle": [],
                        "last": "Sung",
                        "suffix": ""
                    },
                    {
                        "first": "Brian",
                        "middle": [],
                        "last": "Strope",
                        "suffix": ""
                    },
                    {
                        "first": "Ray",
                        "middle": [],
                        "last": "Kurzweil",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of The Third Workshop on Representation Learning for NLP",
                "volume": "",
                "issue": "",
                "pages": "164--174",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yinfei Yang, Steve Yuan, Daniel Cer, Sheng-yi Kong, Noah Constant, Petr Pilar, Heming Ge, Yun-Hsuan Sung, Brian Strope, and Ray Kurzweil. 2018. Learn- ing semantic textual similarity from conversations. In Proceedings of The Third Workshop on Represen- tation Learning for NLP, pages 164-174.",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "An unsupervised sentence embedding method by mutual information maximization",
                "authors": [
                    {
                        "first": "Yan",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Ruidan",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Zuozhu",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Kwan",
                        "middle": [],
                        "last": "Hui Lim",
                        "suffix": ""
                    },
                    {
                        "first": "Lidong",
                        "middle": [],
                        "last": "Bing",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "volume": "",
                "issue": "",
                "pages": "1601--1610",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yan Zhang, Ruidan He, Zuozhu Liu, Kwan Hui Lim, and Lidong Bing. 2020. An unsupervised sentence embedding method by mutual information maxi- mization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1601-1610.",
                "links": null
            },
            "BIBREF34": {
                "ref_id": "b34",
                "title": "Modeling multiturn conversation with deep utterance aggregation",
                "authors": [
                    {
                        "first": "Zhuosheng",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Jiangtong",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Pengfei",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    },
                    {
                        "first": "Hai",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Gongshen",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 27th International Conference on Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "3740--3752",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zhuosheng Zhang, Jiangtong Li, Pengfei Zhu, Hai Zhao, and Gongshen Liu. 2018. Modeling multi- turn conversation with deep utterance aggregation. In Proceedings of the 27th International Conference on Computational Linguistics, pages 3740-3752.",
                "links": null
            },
            "BIBREF35": {
                "ref_id": "b35",
                "title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
                "authors": [
                    {
                        "first": "Yukun",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    },
                    {
                        "first": "Ryan",
                        "middle": [],
                        "last": "Kiros",
                        "suffix": ""
                    },
                    {
                        "first": "Rich",
                        "middle": [],
                        "last": "Zemel",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the IEEE",
                "volume": "",
                "issue": "",
                "pages": "19--27",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut- dinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE inter- national conference on computer vision, pages 19- 27.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF1": {
                "num": null,
                "text": "Figure 1: Model architecture. (1) We use BERT to encode the multi-turn dialogue context and the responses, all of the BERT encoders share the same parameters. (2) The matching-guided embedding (MGE) mechanism performs the token-level matching between each utterance and a response, generates multiple refined embeddings across turns. (3) All refined embedding matrices are aggregated to form a context-aware embedding matrix, which is further pooled along the sequence dimension.",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "Figure 2: Impact of turn number.",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "FIGREF3": {
                "num": null,
                "text": "Figure 3: Impact of data scale.",
                "uris": null,
                "fig_num": "3",
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table><tr><td>Dataset</td><td>MDC</td><td>JDDC</td><td>ECD</td></tr><tr><td># Total dialogues</td><td>10,087</td><td>1,024,196</td><td>1,020,000</td></tr><tr><td># Total turns</td><td>74,685</td><td>20,451,337</td><td>7,500,000</td></tr><tr><td># Total words</td><td colspan=\"3\">190,952 150,716,172 49,000,000</td></tr><tr><td># Total intents</td><td>11</td><td>289</td><td>207</td></tr></table>",
                "type_str": "table",
                "text": "Table1shows the statistics information of these three datasets. The Microsoft Dialogue Corpus is a task-oriented dialogue dataset. It consists of three domains, each with 11 identical intents. The Jing Dong Dialogue Corpus is a large-scale customer service dialogue dataset publicly available from Statistics of the datasets.",
                "html": null,
                "num": null
            },
            "TABREF1": {
                "content": "<table><tr><td>Model</td><td>Microsoft Corpus Corr. MAP MRR Corr. MAP MRR Corr. MAP MRR Jing Dong Corpus E-commerce Corpus</td></tr><tr><td/><td>Self-supervised models</td></tr><tr><td colspan=\"2\">Avg. GloVe embeddings 36.64 31.59 40.91 39.61 45.94 59.53 19.80 46.14 63.68</td></tr><tr><td>BERT-CLS</td><td>22.34 29.54 35.94 21.40 45.05 59.58 16.61 47.75 65.91</td></tr><tr><td>BERT-avg</td><td>40.95 32.10 43.01 50.89 49.08 64.54 43.68 51.77 70.79</td></tr><tr><td>BERT-flow</td><td>45.56 33.13 40.86 65.11 49.53 64.30 55.04 52.16 71.06</td></tr><tr><td>BERT-whitening</td><td>26.70 32.09 43.01 61.57 49.08 64.54 47.64 51.77 70.80</td></tr><tr><td>BERT(adapt)-CLS</td><td>27.35 31.30 39.83 26.49 48.51 65.70 33.91 51.75 74.68</td></tr><tr><td>BERT(adapt)-avg</td><td>42.81 32.53 43.49 72.60 53.03 66.99 74.26 59.32 76.89</td></tr><tr><td>BERT(adapt)-flow</td><td>50.17 34.32 41.62 73.32 53.42 67.00 74.31 59.77 76.48</td></tr><tr><td>BERT(adapt)-whitening</td><td>29.68 32.53 43.48 67.18 53.04 67.01 57.22 59.33 76.84</td></tr><tr><td/><td>Dialogue-based self-supervised models</td></tr><tr><td>SiameseBERTS</td><td>77.95 76.26 84.92 75.70 61.92 74.44 74.83 65.84 79.88</td></tr><tr><td>SiameseBERTM</td><td>76.70 73.81 85.09 76.85 62.45 74.64 75.45 66.24 80.58</td></tr><tr><td>DialogueCSEI 1</td><td>80.13 87.26 85.89 80.60 66.54 74.79 81.79 68.70 79.89</td></tr><tr><td>DialogueCSEI 2</td><td>82.36 91.40 90.45 81.22 68.02 79.52 83.94 69.32 81.20</td></tr></table>",
                "type_str": "table",
                "text": "Evaluation results on the dialogue-based semantic textual similarity (D-STS) task and the semantic retrieval (SR) task. Corr. refers to Spearman's correlation metric for the D-STS task. MAP and MRR are metrics for the SR task. Reported numbers are in percentages.",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table><tr><td>we can observe that</td></tr><tr><td>our model achieves the best performance in terms</td></tr><tr><td>of all metrics across the three datasets. Compared</td></tr><tr><td>to the results of the siamese-networks, our model</td></tr><tr><td>achieves at least 4.41 points (77.95 \u2192 82.36), 4.37</td></tr><tr><td>points (76.85 \u2192 81.22), and 8.49 points (75.45 \u2192</td></tr><tr><td>83.94) in terms of Spearman's correlation on MDC,</td></tr><tr><td>JDDC, and ECD respectively. It also improves the</td></tr><tr><td>MAP metric by 14.84 points (76.26 \u2192 91.40), 5.57</td></tr><tr><td>points (62.45 \u2192 68.02), and 3.08 points (66.24 \u2192</td></tr><tr><td>69.32) in terms of MAP metric on the three datasets.</td></tr></table>",
                "type_str": "table",
                "text": "",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table><tr><td>Temperature</td><td>0.05</td><td>0.1</td><td>0.2</td><td>0.5</td></tr></table>",
                "type_str": "table",
                "text": "JDDCCorr. 80.05 81.22 80.82 79.85 MAP 67.19 68.02 67.55 68.63 ECD Corr. 82.24 83.94 84.24 83.76 MAP 67.98 69.32 69.63 69.11 Impact of temperature.",
                "html": null,
                "num": null
            },
            "TABREF4": {
                "content": "<table><tr><td colspan=\"2\"># Negative samples</td><td>1</td><td>4</td><td>9</td><td>19</td></tr><tr><td>JDDC</td><td>Corr. MAP</td><td colspan=\"4\">80.60 80.85 81.22 81.56 67.48 67.69 68.02 68.63</td></tr><tr><td>ECD</td><td>Corr. MAP</td><td colspan=\"4\">82.55 83.14 83.94 84.12 68.56 68.87 69.32 69.56</td></tr></table>",
                "type_str": "table",
                "text": "Impact of negative samples.",
                "html": null,
                "num": null
            }
        }
    }
}