{
    "paper_id": "2020",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:30:58.334893Z"
    },
    "title": "Methods for Numeracy-Preserving Word Embeddings",
    "authors": [
        {
            "first": "Dhanasekar",
            "middle": [],
            "last": "Sundararaman",
            "suffix": "",
            "affiliation": {},
            "email": "dhanasekar.sundararaman@duke.edu"
        },
        {
            "first": "Shijing",
            "middle": [],
            "last": "Si",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Vivek",
            "middle": [],
            "last": "Subramanian",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Guoyin",
            "middle": [],
            "last": "Wang",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Devamanyu",
            "middle": [],
            "last": "Hazarika",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "National University of Singapore",
                "location": {}
            },
            "email": ""
        },
        {
            "first": "Lawrence",
            "middle": [],
            "last": "Carin",
            "suffix": "",
            "affiliation": {},
            "email": ""
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Word embedding models are typically able to capture the semantics of words via the distributional hypothesis, but fail to capture the numerical properties of numbers that appear in a text. This leads to problems with numerical reasoning involving tasks such as question answering. We propose a new methodology to assign and learn embeddings for numbers. Our approach creates Deterministic, Independentof-Corpus Embeddings (referred to as DICE) for numbers, such that their cosine similarity reflects the actual distance on the number line. DICE outperforms a wide range of pre-trained word embedding models across multiple examples of two tasks: (i) evaluating the ability to capture numeration and magnitude; and (ii) to perform list maximum, decoding, and addition. We further explore the utility of these embeddings in downstream applications by initializing numbers with our approach for the task of magnitude prediction. We also introduce a regularization approach to learn model-based embeddings of numbers in a contextual setting.",
    "pdf_parse": {
        "paper_id": "2020",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Word embedding models are typically able to capture the semantics of words via the distributional hypothesis, but fail to capture the numerical properties of numbers that appear in a text. This leads to problems with numerical reasoning involving tasks such as question answering. We propose a new methodology to assign and learn embeddings for numbers. Our approach creates Deterministic, Independentof-Corpus Embeddings (referred to as DICE) for numbers, such that their cosine similarity reflects the actual distance on the number line. DICE outperforms a wide range of pre-trained word embedding models across multiple examples of two tasks: (i) evaluating the ability to capture numeration and magnitude; and (ii) to perform list maximum, decoding, and addition. We further explore the utility of these embeddings in downstream applications by initializing numbers with our approach for the task of magnitude prediction. We also introduce a regularization approach to learn model-based embeddings of numbers in a contextual setting.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Word embeddings capture semantic relationships between words by operationalizing the distributional hypothesis (Harris, 1954; Firth, 1957) . They can be learned either non-contextually (Mikolov et al., 2013b; Pennington et al., 2014; Bojanowski et al., 2017) or contextually (Devlin et al., 2018; Peters et al., 2018) . Non-contextual embeddings have worked well on various language understanding and semantic tasks (Rumelhart et al., 1988; Mikolov et al., 2013a,b) . More recently, they have also been used as pre-trained word embeddings to aid more sophisticated contextual models for solving rigorous natural language processing (NLP) problems, including translation, paraphrasing, and sentence-similarity tasks (Kiros et al., 2015; Wieting et al., 2015) .",
                "cite_spans": [
                    {
                        "start": 111,
                        "end": 125,
                        "text": "(Harris, 1954;",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 126,
                        "end": 138,
                        "text": "Firth, 1957)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 185,
                        "end": 208,
                        "text": "(Mikolov et al., 2013b;",
                        "ref_id": null
                    },
                    {
                        "start": 209,
                        "end": 233,
                        "text": "Pennington et al., 2014;",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 234,
                        "end": 258,
                        "text": "Bojanowski et al., 2017)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 275,
                        "end": 296,
                        "text": "(Devlin et al., 2018;",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 297,
                        "end": 317,
                        "text": "Peters et al., 2018)",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 416,
                        "end": 440,
                        "text": "(Rumelhart et al., 1988;",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 441,
                        "end": 465,
                        "text": "Mikolov et al., 2013a,b)",
                        "ref_id": null
                    },
                    {
                        "start": 715,
                        "end": 735,
                        "text": "(Kiros et al., 2015;",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 736,
                        "end": 757,
                        "text": "Wieting et al., 2015)",
                        "ref_id": "BIBREF27"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "While word embeddings effectively capture semantic relationships between words, they are less effective at capturing numeric properties associated with numbers. Though numbers represent a significant percentage of tokens in a corpus, they are often overlooked. In non-contextual word embedding models, they are treated like any other word, which leads to misinterpretation. For instance, they exhibit unintuitive similarities with other words and do not contain strong prior information about the magnitude of the number they encode. In sentence similarity and reasoning tasks, failure to handle numbers causes as much as 29% of contradictions (De Marneffe et al., 2008) . In other data-intensive tasks where numbers are abundant, like neural machine translation, they are masked to hide the translation models inefficiency in dealing with them (Mitchell and Lapata, 2009) .",
                "cite_spans": [
                    {
                        "start": 644,
                        "end": 670,
                        "text": "(De Marneffe et al., 2008)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 845,
                        "end": 872,
                        "text": "(Mitchell and Lapata, 2009)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "There are a variety of tests proposed to measure the efficiency of number embeddings. For instance, Naik et al. (2019) shows that GloVe (Pennington et al., 2014) , word2vec (Mikolov et al., 2013b) , and fastText (Joulin et al., 2016; Bojanowski et al., 2017) fail to capture numeration and magnitude properties of a number. Numeration is the property of associating numbers with their corresponding word representations (\"3\" and \"three\") while magnitude represents a number's actual value (3 < 4). Further, Wallace et al. (2019) proposes several tests for analyzing numerical reasoning of number embeddings that include list maximum, decoding, and addition.",
                "cite_spans": [
                    {
                        "start": 100,
                        "end": 118,
                        "text": "Naik et al. (2019)",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 136,
                        "end": 161,
                        "text": "(Pennington et al., 2014)",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 173,
                        "end": 196,
                        "text": "(Mikolov et al., 2013b)",
                        "ref_id": null
                    },
                    {
                        "start": 212,
                        "end": 233,
                        "text": "(Joulin et al., 2016;",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 234,
                        "end": 258,
                        "text": "Bojanowski et al., 2017)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 507,
                        "end": 528,
                        "text": "Wallace et al. (2019)",
                        "ref_id": "BIBREF25"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this paper, we experimentally demonstrate that if the cosine similarity between word embeddings of two numbers reflects their actual distance on the number line, the resultant word embeddings are useful in downstream tasks. We first demonstrate how Deterministic, Independent-of-Corpus Embeddings (DICE) can be constructed such that they almost perfectly capture properties of numera-tion and magnitude. These non-contextual embeddings also perform well on related tests for numeracy (Wallace et al., 2019) .",
                "cite_spans": [
                    {
                        "start": 487,
                        "end": 509,
                        "text": "(Wallace et al., 2019)",
                        "ref_id": "BIBREF25"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "To demonstrate the efficacy of DICE for downstream tasks, we explore its utility in two experiments. First, we design a DICE embedding initialized Bi-LSTM network to classify the magnitude of masked numbers in the 600K dataset (Chen et al., 2019) . Second, given the popularity of modern contextual model-based embeddings, we devise a regularization procedure that emulates the hypothesis proposed by DICE and can be employed in any task-based fine-tuning process. We demonstrate that adding such regularization helps the model internalize notions of numeracy while learning task-based contextual embeddings for the numbers present in the text. We find promising results in a numerical reasoning task that involves numerical question answering based on a sub-split of the popular SQuAD dataset (Rajpurkar et al., 2016) . Our contribution can be summarized as follows:",
                "cite_spans": [
                    {
                        "start": 227,
                        "end": 246,
                        "text": "(Chen et al., 2019)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 794,
                        "end": 818,
                        "text": "(Rajpurkar et al., 2016)",
                        "ref_id": "BIBREF20"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\u2022 We propose a deterministic technique to learn numerical embeddings. DICE embeddings are learned independently of corpus and effectively capture properties of numeracy.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\u2022 We prove experimentally that the resultant embeddings learned using the above methods improve a model's ability to reason about numbers in a variety of tasks, including numeration, magnitude, list maximum, decoding, and addition.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\u2022 We also demonstrate that properties of DICE can be adapted to contextual models, like BERT (Devlin et al., 2018) , through a novel regularization technique for solving tasks involving numerical reasoning.",
                "cite_spans": [
                    {
                        "start": 93,
                        "end": 114,
                        "text": "(Devlin et al., 2018)",
                        "ref_id": "BIBREF5"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The major research lines in this area have been dedicated to (i) devising probing tests and curating resources to evaluate the numerical reasoning abilities of pre-trained embeddings, and (ii) proposing new models that learn these properties. Naik et al. (2019) surveyed a number of noncontextual word embedding models and highlighted the failure of those models in capturing two essential properties of numbers -numeration and magnitude. Chen et al. ( 2019) created a novel dataset named Numeracy-600k, a collection of approximately 600,000 sentences from market comments with a diverse set of numbers representing age, height, weight, year, etc. The authors use neural network models, including a GRU, BiGRU, CRNN, CNN-capsule, GRU-capsule, and BiGRUcapsule, to classify the magnitude of each number. Wallace et al. (2019) compares and contrasts the numerical reasoning ability of a variety of noncontextual as well as contextual embedding models. The authors also proposed three tests -list maximum, decoding, and addition -to judge the numerical reasoning ability of embeddings of numerals. They infer that word embedding models that perform the best on these three tests have captured the numerical properties of numbers well. Therefore, we consider these proposed tests in our evaluation. (Spithourakis and Riedel, 2018) used a variety of models to distinguish numbers from words, and demonstrated that this ability reduces model perplexity with neural machine translation. Weiss et al. (2018) found that neural networks are capable of reasoning numbers with explicit supervision.",
                "cite_spans": [
                    {
                        "start": 243,
                        "end": 261,
                        "text": "Naik et al. (2019)",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 803,
                        "end": 824,
                        "text": "Wallace et al. (2019)",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 1295,
                        "end": 1326,
                        "text": "(Spithourakis and Riedel, 2018)",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 1480,
                        "end": 1499,
                        "text": "Weiss et al. (2018)",
                        "ref_id": "BIBREF26"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "Numerically Augmented QANet (NAQANet) (Dua et al., 2019) was built by adding an output layer on top of QANet (Yu et al., 2018) to predict answers based on addition and subtraction over numbers in the DROP dataset. Our work, in contrast, offers a simple methodology that can be added to any model as a regularization technique. Our work is more similar to Jiang et al. (2019) , where embedding of a number is learned as a simple weighted average of its prototype embeddings. Such embeddings are used in tasks like word similarity, sequence labeling and have been proven to be effective.",
                "cite_spans": [
                    {
                        "start": 38,
                        "end": 56,
                        "text": "(Dua et al., 2019)",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 103,
                        "end": 126,
                        "text": "QANet (Yu et al., 2018)",
                        "ref_id": null
                    },
                    {
                        "start": 355,
                        "end": 374,
                        "text": "Jiang et al. (2019)",
                        "ref_id": "BIBREF10"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "To overcome NLP models inefficiency in dealing with numbers, we consider our method DICE to form embeddings. To begin, we embed numerals and word forms of numbers as vectors e i \u2208 R D , where i indexes numerals identified within a corpus. We first preprocess by parsing the corpora associated with each of our tasks (described below) for numbers in numeral and word forms to populate a number vocabulary. Then, the dimensionality of the embeddings required for that task is fixed. We explicitly associate the embeddings of a numeral and word forms of numbers to have the same embedding. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Methods",
                "sec_num": "3"
            },
            {
                "text": "In designing embeddings that capture the aforementioned properties of numeration and magnitude, we consider a deterministic, handcrafted approach (depicted in Figures 1a and 1b ). This method relies on the fact that tests for both numeration and magnitude are concerned with the correspondence in similarity between numbers in token space and numbers in embedding space. In token space, two numbers x, y \u2208 R, in numeral or word form (with the latter being mapped to its corresponding numeral form for comparison), can be compared using absolute difference, i.e.:",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 167,
                        "end": 169,
                        "text": "1a",
                        "ref_id": "FIGREF0"
                    },
                    {
                        "start": 174,
                        "end": 176,
                        "text": "1b",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "DICE embeddings",
                "sec_num": "3.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "d n (x, y) = |x -y|",
                        "eq_num": "(1)"
                    }
                ],
                "section": "DICE embeddings",
                "sec_num": "3.1"
            },
            {
                "text": "The absolute value ensures that two numbers are treated as equally distant regardless of whether x \u2265 y or y \u2265 x. On the other hand, two embeddings x, y \u2208 R D are typically compared via cosine similarity, given by:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "DICE embeddings",
                "sec_num": "3.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "s e (x, y) = x T y ||x|| 2 ||y|| 2 = cos(\u03b8)",
                        "eq_num": "(2)"
                    }
                ],
                "section": "DICE embeddings",
                "sec_num": "3.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "d e (x, y) = 1 -cos(\u03b8) (",
                        "eq_num": "3"
                    }
                ],
                "section": "DICE embeddings",
                "sec_num": "3.1"
            },
            {
                "text": ")",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "DICE embeddings",
                "sec_num": "3.1"
            },
            {
                "text": "where \u03b8 is the angle between x and y and d e (x, y) is their cosine distance. Normalization by the vector lengths ensures that the metric is independent of the lengths of the two vectors. Note that numerals are compared in terms of distance while their embeddings are compared by similarity. As cosine distance increases, the angle between x and y increases monotonically. A distance of zero is achieved when x and y are oriented in the same direction. When x \u22a5 y, the cosine distance is 1; and when x and y are antiparallel, cosine distance is 2.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "DICE embeddings",
                "sec_num": "3.1"
            },
            {
                "text": "We seek a mapping (x, y) \u2192 (x, y) such that d e monotonically increases as d n increases. We first bound the range of numbers for which we wish to compute embeddings by [a, b] \u2282 R and, without loss of generality, restrict x and y to be of unit length (i.e., ||x|| 2 = ||y|| 2 = 1). Since the cosine function decreases monotonically between 0 and \u03c0, we can simply employ a linear mapping to map distances",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "DICE embeddings",
                "sec_num": "3.1"
            },
            {
                "text": "s n \u2208 [0, |a -b|] to angles \u03b8 \u2208 [0, \u03c0]: \u03b8(s n ) = s n |a -b| \u03c0 (4)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "DICE embeddings",
                "sec_num": "3.1"
            },
            {
                "text": "This mapping achieves the desired direct relationship between s n and d e . Since there are infinitely many choices for x and y with angle \u03b8, we simply fix the direction of the vector corresponding to the numeral a. Numbers that fall outside [a, b] are mapped to a random angle in [-\u03c0, \u03c0]. In the corpora we considered, a and b are chosen such that numbers outside [a, b] represent a small fraction of the total set of numbers (approximately 2%).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "DICE embeddings",
                "sec_num": "3.1"
            },
            {
                "text": "We employ this mapping to generate numeral embeddings in R D . Figure 1a shows deterministic, independent-of-corpus embeddings of rank 2 (DICE-2). In this approach we represent angles as vectors in R 2 using the polar-to-Cartesian coordinate transformation:",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 70,
                        "end": 72,
                        "text": "1a",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "DICE embeddings",
                "sec_num": "3.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "[r, \u03b8] \u2192 [x 1 , x 2 ] = [r cos(\u03b8), r sin(\u03b8)]v",
                        "eq_num": "(5)"
                    }
                ],
                "section": "DICE embeddings",
                "sec_num": "3.1"
            },
            {
                "text": "where we choose r = 1 without loss of generality. We then sample a random matrix M \u2208 R D\u00d7D where D \u2265 2 and m ij \u223c N (0, 1) and perform a QR decomposition on M to obtain a matrix Q whose columns q i , i = 1, . . . , D constitute an orthonormal basis for R D . The DICE-2 embedding e \u2208 R D of each numeral is then given by e = Q 1:2 v, where the subscript on Q indicates taking the first two columns of Q.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "DICE embeddings",
                "sec_num": "3.1"
            },
            {
                "text": "In Figure 1b we consider DICE-D, in which we generate vectors in R D by applying a polarto-Cartesian transformation in D dimensions (Blumenson, 1960) :",
                "cite_spans": [
                    {
                        "start": 132,
                        "end": 149,
                        "text": "(Blumenson, 1960)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 10,
                        "end": 12,
                        "text": "1b",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Model",
                "sec_num": null
            },
            {
                "text": "v d = [sin(\u03b8)] d-1 cos(\u03b8), 1 \u2264 d < D [sin(\u03b8)] D , d = D (6)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model",
                "sec_num": null
            },
            {
                "text": "where the subscripts indicate the coordinate in v.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model",
                "sec_num": null
            },
            {
                "text": "We again apply a QR-decomposition on a random matrix M generated as above, except here we project v using all D basis vectors. This allows for a random rotation of the embeddings to avoid bias due to choosing e a1 = 1, e ai = 0 \u2200i = 1. We employ DICE-D embeddings throughout this paper as word embeddings are practically not 2 dimensional.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model",
                "sec_num": null
            },
            {
                "text": "To observe the numerical properties of DICE, we consider two tasks: Task 1 deals with the numeration (NUM) and magnitude (MAG) properties as proposed by (Naik et al., 2019) ; Task 2 performs list maximum, decoding, and addition as proposed by (Wallace et al., 2019) . We then experiment on two additional tasks to demonstrate the applications of DICE.",
                "cite_spans": [
                    {
                        "start": 153,
                        "end": 172,
                        "text": "(Naik et al., 2019)",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 243,
                        "end": 265,
                        "text": "(Wallace et al., 2019)",
                        "ref_id": "BIBREF25"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "4"
            },
            {
                "text": "In this task, proposed by Naik et al. (2019) , there are three tests for examining each property of numeration (NUM, 3 = \"three\") and magnitude (MAG, 3 < 4). For each of these tests, target numbers in its word or numeral form are evaluated against other numbers as follows:",
                "cite_spans": [
                    {
                        "start": 26,
                        "end": 44,
                        "text": "Naik et al. (2019)",
                        "ref_id": "BIBREF17"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Task 1: Exploring Numeracy",
                "sec_num": "4.1"
            },
            {
                "text": "\u2022 One-vs-All (OVA): The distance between the embedding vector of the target and its nearest neighbor should be smaller than the distance between the target and any other numeral in the data.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Task 1: Exploring Numeracy",
                "sec_num": "4.1"
            },
            {
                "text": "\u2022 Strict Contrastive (SC): The distance of the embedding vector of the target from its nearest neighbor should be smaller than its second nearest neighbor numeral.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Task 1: Exploring Numeracy",
                "sec_num": "4.1"
            },
            {
                "text": "\u2022 Broad Contrastive (BC): The distance of the embedding vector of the target numeral from its nearest neighbor should be smaller than its furthest neighbor.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Task 1: Exploring Numeracy",
                "sec_num": "4.1"
            },
            {
                "text": "Training Details. We use the Gigaword corpus obtained from the Linguistic Data Consortium to populate the list of numbers from the dataset. Parsing was performed using the text2digits1 Python module. As done by Naik et al. (2019) , we employ D = 300 for the DICE-D embeddings. Embeddings of numerals are assigned using the principle explained in Section 3.1, while the embedding of words that denote numbers (word form) simply points to the embedding of that numeral itself. We then perform the six tests (OVA-NUM / OVA-MAG, SC-NUM/ SC-MAG, BC-NUM / BC-MAG) on 130 combinations of numbers for NUM and 31, 860 combinations of numbers for MAG.",
                "cite_spans": [
                    {
                        "start": 211,
                        "end": 229,
                        "text": "Naik et al. (2019)",
                        "ref_id": "BIBREF17"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Task 1: Exploring Numeracy",
                "sec_num": "4.1"
            },
            {
                "text": "Evaluation. Following Naik et al. (2019) , we use accuracy to measure the efficiency of the embeddings. These tests require the fulfillment of certain clauses which are defined in Naik et al. (2019) .",
                "cite_spans": [
                    {
                        "start": 22,
                        "end": 40,
                        "text": "Naik et al. (2019)",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 180,
                        "end": 198,
                        "text": "Naik et al. (2019)",
                        "ref_id": "BIBREF17"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Task 1: Exploring Numeracy",
                "sec_num": "4.1"
            },
            {
                "text": "Results. Table 1 shows comparisons of the performance of embeddings created by each of the DICE methods on the MAG tests. Compared to the baselines, both DICE methods outperform all commonly employed non-contextual word embedding models in OVA, SC, and BC tests. This is attributed to the cosine distance property addressed in the DICE embeddings. Specifically, because the magnitude of the number is linearly related to its angle, sweeping through numbers in order guarantees an increase in angle along each axis. Numbers that are close to each other in magnitude are rotated further but in proportion to their magnitude. Thus, small and large numbers are ensured to lie near other small and large numbers, respectively, in terms of cosine distance.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 15,
                        "end": 16,
                        "text": "1",
                        "ref_id": "TABREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Task 1: Exploring Numeracy",
                "sec_num": "4.1"
            },
            {
                "text": "On the NUM tests, DICE achieves perfect accuracy. The primary reason DICE embeddings perform so well on numeracy tasks is that the preprocessing steps taken allow us to parse a corpus for word forms of numbers and explicitly set matching embeddings for both word and numeral forms of numbers. Each of these embeddings is guaranteed to be unique since a number's embedding is based on its magnitude, i.e., the larger the magnitude, the greater the angle of the embedding, with a maximum angle of \u03c0. This ensures that the numeral form of a number is always able to correctly identify its word form among all word forms in the corpus as that with the smallest cosine distance (which equals zero). Performance on OVA-NUM is a lower bound on the performance of SC-NUM and BC-NUM, so those tests are guaranteed to pass under our approach.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Task 1: Exploring Numeracy",
                "sec_num": "4.1"
            },
            {
                "text": "This task considers the operations proposed by (Wallace et al., 2019) -list maximum, decoding, and addition. List maximum deals with the task of predicting the maximum number given the embedding of five different numbers. Decoding deals with regressing the value of a number given its embedding. An additional task involves predicting the sum of two numbers given their embeddings.",
                "cite_spans": [
                    {
                        "start": 47,
                        "end": 69,
                        "text": "(Wallace et al., 2019)",
                        "ref_id": "BIBREF25"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Task 2: List Maximum, Decoding, and Addition",
                "sec_num": "4.2"
            },
            {
                "text": "Training Details. The list-maximum test presents to a Bi-LSTM neural network a set of five numbers of the same magnitude, and the network is trained to report the index of the maximum number. In the decoding test, a linear model and a feed-forward network are each trained to output the numeral corresponding to the word form of a number based on its embedding. Finally, in the addition test, a feed-forward network is trained to take in the embeddings of two numbers as its input and report the sum of the two numbers as its output. Each test is performed on three ranges of integers [0, 99], [0, 999], and [0, 9999], with an 80/20 split of training and testing data sampled randomly. The neural network is fed with the embedding of numbers; the task is either classification (in the case of list maximum) or prediction of a continuous number (in case of addition and decoding). We replicate the exact experimental conditions and perform the three tests with DICE embeddings. For the sake of consistency with the tests proposed by (Wallace et al., 2019) , we also only deal with positive in this experiment.",
                "cite_spans": [
                    {
                        "start": 1032,
                        "end": 1054,
                        "text": "(Wallace et al., 2019)",
                        "ref_id": "BIBREF25"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Task 2: List Maximum, Decoding, and Addition",
                "sec_num": "4.2"
            },
            {
                "text": "Evaluation. List maximum again uses accuracy as its metric while decoding and addition use root mean squared error (RMSE), since predictions are continuous.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Task 2: List Maximum, Decoding, and Addition",
                "sec_num": "4.2"
            },
            {
                "text": "Results. Given the strong performance of the DICE-D method on the NUM and MAG tests, we next consider its performance on tasks involv-ing neural network models. In their empirical study, (Wallace et al., 2019) compared a wide range of models that included a random baseline; character level models such as a character-CNN and character-LSTM, which were both untrained and trained; a so-called value embedding model in which numbers are embedded as their scalar value; traditional non-contextual word embedding models including Word2Vec and GloVe; contextual word embedding models including ELMo and BERT; and the Numerically Aware Question Answering (NAQA) Network, a strong numerical reasoning model proposed on the Discrete Reasoning over Paragraphs (DROP) dataset.",
                "cite_spans": [
                    {
                        "start": 187,
                        "end": 209,
                        "text": "(Wallace et al., 2019)",
                        "ref_id": "BIBREF25"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Task 2: List Maximum, Decoding, and Addition",
                "sec_num": "4.2"
            },
            {
                "text": "We compare the performance of our DICE-D embedding to that of the other models on each of the three tasks proposed by (Wallace et al., 2019) . Results are presented in Table 2 . We find that our DICE embedding exceeds the performance of more sophisticated models by large margins in all but four cases. In two of those four, our model fell short by only a few percentage points. We attribute the success of the DICE-D approach to the fact that the model is, by design, engineered to handle numeracy. Just as the value embedding modelwhich proved to be reasonably successful in all three tasks across a wide range of numbers -captures numeracy through the magnitude of embeddings, our model captures numeracy through the angle corresponding to the embeddings.",
                "cite_spans": [
                    {
                        "start": 118,
                        "end": 140,
                        "text": "(Wallace et al., 2019)",
                        "ref_id": "BIBREF25"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 174,
                        "end": 175,
                        "text": "2",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Task 2: List Maximum, Decoding, and Addition",
                "sec_num": "4.2"
            },
            {
                "text": "The value embedding model, however, breaks down as the range of the processed numbers grows. This is likely because, as demonstrated by Trask et al. (2018) , networks trained on numeracy tasks typically struggle to learn an identity mapping. We reason that our model outperforms the value embedding model because the network learns to associate features between the set of inputs such that the input vectors can be scaled, rotated, and translated in D dimensions to achieve the desired goal.",
                "cite_spans": [
                    {
                        "start": 136,
                        "end": 155,
                        "text": "Trask et al. (2018)",
                        "ref_id": "BIBREF24"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Task 2: List Maximum, Decoding, and Addition",
                "sec_num": "4.2"
            },
            {
                "text": "More precisely, for a neural network to learn addition, numbers must be embedded such that their vector embeddings can be consistently shifted, rotated, and scaled to yield the embedding of another number (see Figure 1c ). The choice of embedding is essential as it may be impractical for a network to learn a transformation for all embeddings that obeys this property (without memorization). DICE is quite similar to the value embedding system, which directly encodes a number's value in its embeddings. However, DICE performs bet-ter due to its compatibility with neural networks, whose layers are better suited for learning rotations and scaling than identity mappings.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 217,
                        "end": 219,
                        "text": "1c",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Task 2: List Maximum, Decoding, and Addition",
                "sec_num": "4.2"
            },
            {
                "text": "Finally, both the value embedding models for a small number range and the character level models remain somewhat competitive, suggesting again that exploring a digit-by-digit embedding of numerals may provide a means of improving our model further.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Task 2: List Maximum, Decoding, and Addition",
                "sec_num": "4.2"
            },
            {
                "text": "We examine the importance of good initialization for number embedding vectors (Kocmi and Bojar, 2017) , particularly for better contextual understanding. In particular, we experiment on the magnitude classification task, which requires the prediction of magnitudes for masked numbers. The task is based on the 600K dataset proposed by Chen et al. (2019) , which requires classification into one of seven categories corresponding to powers of 10 in {0, 1, 2, 3, 4, 5, 6}.",
                "cite_spans": [
                    {
                        "start": 78,
                        "end": 101,
                        "text": "(Kocmi and Bojar, 2017)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 335,
                        "end": 353,
                        "text": "Chen et al. (2019)",
                        "ref_id": "BIBREF2"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Magnitude Classification",
                "sec_num": "5.1"
            },
            {
                "text": "Training Details. We use a bi-LSTM (Hochreiter and Schmidhuber, 1997) with soft attention (Chorowski et al., 2015) to classify the magnitude of masked numbers. Numerals are initialized with corresponding DICE embeddings, and the target number is masked by substituting a random vector. Each token x n in a sequence of length N is associated with a forward and backward LSTM cell. The hidden state h n of each token is given by the sum of the hidden states of the forward and backward cells:",
                "cite_spans": [
                    {
                        "start": 90,
                        "end": 114,
                        "text": "(Chorowski et al., 2015)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Magnitude Classification",
                "sec_num": "5.1"
            },
            {
                "text": "h n = \u2190 - h n + - \u2192 h n .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Magnitude Classification",
                "sec_num": "5.1"
            },
            {
                "text": "To generate a context vector c for the entire sentence, we compute attention scores \u03b1 n by taking the inner product of each hidden state h n with a learned weight vector w. The resulting scores are passed through a softmax function, and the weights are used to form a convex combination of the h n that represents the context c of the sentence. Logits are obtained by taking the inner product of c with trained embeddings for each of the seven categories, and cross-entropy loss is minimized. More details on training can be found in Appendix A.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Magnitude Classification",
                "sec_num": "5.1"
            },
            {
                "text": "Evaluation. Following Chen et al. (2019) , we use micro and macro F1 scores for classifying the magnitude of a number. and hidden vectors within the LSTM cells on the performance of the BiLSTM-attention model, we perform ablation experiments. We vary the embedding size of tokens while keeping other hyperparameters constant, and observe the results on Tables 4. From Table 4 the BiLSTM with DICE model achieves the best micro-F1 score when the embedding dimension is 256. However, the macro-F1 score peaks when the embedding dimension is 512.",
                "cite_spans": [
                    {
                        "start": 22,
                        "end": 40,
                        "text": "Chen et al. (2019)",
                        "ref_id": "BIBREF2"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 374,
                        "end": 375,
                        "text": "4",
                        "ref_id": "TABREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Magnitude Classification",
                "sec_num": "5.1"
            },
            {
                "text": "These results suggest that while DICE embeddings yield superior performance in non-contextual numerical tasks, such as computing the maximum and performing basic mathematical operations, data agnostic embeddings such as DICE may not be ideal for textual reasoning tasks in which words surrounding a number provide important information regarding the magnitude of the number. Hence, we introduce a model-based regularization method that utilizes the DICE principles to learn number embeddings in 5.2.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results.",
                "sec_num": null
            },
            {
                "text": "In the previous section, we demonstrated how DICE could be explicitly incorporated for numbers in the text. Here, we propose a methodology that help models implicitly internalize the properties of DICE. Our approach involves a regularization method (an auxiliary loss) that can be adopted in the fine-tuning of any contextual NLP model, such as BERT. Auxiliary losses have shown to work well for a variety of NLP downstream tasks (Shen et al., 2019) .",
                "cite_spans": [
                    {
                        "start": 430,
                        "end": 449,
                        "text": "(Shen et al., 2019)",
                        "ref_id": "BIBREF22"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model-Based Numeracy Embeddings",
                "sec_num": "5.2"
            },
            {
                "text": "During the task-specific training of any model, the proposed auxiliary loss L num can be applied to the input embeddings of numbers available in a minibatch. For any two contextual numerical embeddings x, y obtained from the final hidden layer of the model, the L num loss for the pair of numbers (x, y) is calculated as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model-Based Numeracy Embeddings",
                "sec_num": "5.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "L num = 2 |x -y| |x| + |y| -d cos (x, y) 2",
                        "eq_num": "(7)"
                    }
                ],
                "section": "Model-Based Numeracy Embeddings",
                "sec_num": "5.2"
            },
            {
                "text": "where",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model-Based Numeracy Embeddings",
                "sec_num": "5.2"
            },
            {
                "text": "d cos (x, y) = 1 -x T y",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model-Based Numeracy Embeddings",
                "sec_num": "5.2"
            },
            {
                "text": "x 2 y 2 is the cosine distance between the embeddings x and y. In essence, L num follows the same motivation as DICE where cosine distance between the embeddings of two numbers are encouraged to be proportional to their (scaled) absolute magnitude distance on the number line.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model-Based Numeracy Embeddings",
                "sec_num": "5.2"
            },
            {
                "text": "Training Details. To evaluate the proposed L num , we test the regularization on the task of question answering (QA) involving numerical answers. In particular, we take the popular Stanford Question Answering Dataset (SQuAD 1.1) (Rajpurkar et al., 2016) dataset and create sub-splits (ranges from [1, 30000] ) where the (i) training QA pairs have answers strictly containing numerical digits (Sub-split 1, less than 10K examples), and (ii) training QA pairs have answers containing a number as one of their tokens, for e.g. \"10 apples\" (Sub-split 2, slightly more than 10K examples). We create these splits to evaluate BERT model's reasoning involving numbers to pick these answers. We choose BERT-base-uncased as baseline model and train it on both the datasets. Within each batch, we calculate L num by randomly sampling a pair of numbers x, y from the available numbers in the contexts. The corresponding embeddings of the numbers are x and y, which are extracted from the last hidden layer of the BERT model. We then enforce the distance of embeddings to match the difference between number values by L num . The scores are reported on the development set (less than 1000 examples) as the test set cannot be pruned for our purpose. The assumption here is that the BERT model needs to perform numerical reasoning to come up with answers for these particular kinds of QA pairs. The models were trained on Nvidia Tesla P100 GPU. More details on choosing According to the same statistics, the average age of people living in Newcastle is 37.8 (the national average being 38.6). Many people in the city have Scottish or Irish ancestors. There is a strong presence of Border Reiver surnames, such as Armstrong, Charlton, Elliot, Johnstone, Kerr, Hall, Nixon, Little and Robson. There are also small but significant Chinese, Jewish and Eastern European (Polish, Czech Roma) populations. There are also estimated to be between 500 and 2,000 Bolivians in Newcastle, forming up to 1% of the populationthe largest such percentage of any UK city. Although the reciprocating steam engine \u2026 use, various companies \u2026 alternative to internal combustion engines. The company Energiprojekt AB in Sweden \u2026 the power of steam. The efficiency \u2026 steam engine reaches some 27-30% on high-pressure engines. It is a single-step, 5-cylinder engine (no compound) with superheated steam and consumes approx. 4 kg (8.8 lb) of steam per kWh.",
                "cite_spans": [
                    {
                        "start": 229,
                        "end": 253,
                        "text": "(Rajpurkar et al., 2016)",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 297,
                        "end": 300,
                        "text": "[1,",
                        "ref_id": null
                    },
                    {
                        "start": 301,
                        "end": 307,
                        "text": "30000]",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model-Based Numeracy Embeddings",
                "sec_num": "5.2"
            },
            {
                "text": "How many cylinders does the Energiprojekt AB engine have? Evaluation. Exact Match is a binary measure (i.e., true/false) of whether the predicted output matches the ground truth answer exactly. This evaluation is performed after the string normalization (uncased, articles removed, etc.). F1 is the harmonic mean of precision and recall.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Context Question",
                "sec_num": null
            },
            {
                "text": "Results. Results in Table 5 show that the BERT model with numeracy objective achieves an improvement of 0.48 F1 points when the answers are purely numerical digits. When the BERT model is trained on QA pairs with answers containing at least a number with several words, and evaluated on pairs with answers containing only numbers, we see an improvement of 1.12 F1 points over the baseline model.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 26,
                        "end": 27,
                        "text": "5",
                        "ref_id": "TABREF7"
                    }
                ],
                "eq_spans": [],
                "section": "Context Question",
                "sec_num": null
            },
            {
                "text": "The BERT-base model on the original SQuAD data was finetuned for 3 epochs owing to its complexity. However, we find that 1 epoch is sufficient to capture the complexity of the pruned SQuAD data. Table 5 shows BERT + L num consistently performs better than BERT-base across epochs.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 201,
                        "end": 202,
                        "text": "5",
                        "ref_id": "TABREF7"
                    }
                ],
                "eq_spans": [],
                "section": "Context Question",
                "sec_num": null
            },
            {
                "text": "Interestingly, BERT-base performs worse when finetuned with QA pairs containing a mix of words and numbers as answers (sub-split 2). This informs us that the baseline model learns to pick numbers better but fails to do as well when fine-tuned with a mix of words and numbers. In both the cases, the evaluation set consists of pruned SQuAD dev set QA pairs with answers strictly containing numerical digits only. We find that BERT + L num gives the maximum improvement on sub-split 2 data highlighting the efficiency of our regularization technique to learn numerical embeddings.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Context Question",
                "sec_num": null
            },
            {
                "text": "Figure 2 shows some qualitative examples where the BERT + L num performs better than BERT-base (Sub-split 2). In this analysis, we found that the During our experiments, we observed the potential issue of weak signals from the loss when the availability of numerical pairs is sparse. In the future, our efforts would be to overcome this issue to ensure further gains.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Context Question",
                "sec_num": null
            },
            {
                "text": "In this work, we methodologically assign and learn embeddings for numbers to reflect their numerical properties. We validate our proposed approach with several experiments that test number embeddings. The tests that evaluate the numeral embeddings are fundamentally applicable to all real numbers. Finally, we introduced an approach to jointly learn embeddings of numbers and words that preserve numerical properties and evaluated them on a contextual word embedding based model. In our future work, we would like to extend this idea to unseen numbers in vocabulary as a function of seen ones.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "6"
            },
            {
                "text": "The Bi-LSTM with attention model initialized with DICE embeddings were trained on the market comments data. The model was trained for a fixed number of 9 epochs. We found that the micro and macro F1 scores peaked for a certain epoch and then flattened out. We picked the best micro and macro pair the model obtained in that certain epoch.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Training details for Magnitude Classification Experiment",
                "sec_num": null
            },
            {
                "text": "Our model involves a regularization method (an auxiliary loss) that can be adopted in the fine-tuning of BERT. This loss was finetuned with a hyperparameter \u03bb and added to the existing BERT classification loss for detecting the correct span. The hyperparameter search space is between 0, 1. We sweeped through the values manually within the search space and found that the best model that gave the maximum improvement in F1 scores had a hyperparameter value of 10 -3 . The values were sweeped based on the observed performance. The performance faded as the hyperparameter was set to a higher value (closer to 1). ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B Hyperparameter for BERT + L num",
                "sec_num": null
            },
            {
                "text": "When did Tesla make the induction motor? The second main legislative body is the Council, which is composed of different ministers of the member states. \u2026 (a distinct body) that the TEU article 15 defines as providing the 'necessary impetus for its development and shall define the general political directions and priorities'. \u2026 The minister must have the authority to represent and bin the member states in decisions. When voting takes place it is weighted \u2026 dominated by larger member states. In total there are 352 votes, \u2026 , if not consensus. TEU article 16(4) and TFEU article 238(3) define this to mean at least 55 per cent of the Council members (not votes) representing 65 per cent of the population of the EU: currently this means around 74 per cent, or 260 of the 352 votes. This is critical during the legislative process.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Context Question",
                "sec_num": null
            },
            {
                "text": "What are the total number of votes to be counted during the voting process? ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Context Question",
                "sec_num": null
            },
            {
                "text": "https://pypi.org/project/text2digits/",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "The authors would like to thank Aakanksha Naik for her help in the early stages of this work, and the anonymous reviewers as well for their insightful comments.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgments",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "A derivation of n-dimensional spherical coordinates",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Le Blumenson",
                        "suffix": ""
                    }
                ],
                "year": 1960,
                "venue": "The American Mathematical Monthly",
                "volume": "67",
                "issue": "1",
                "pages": "63--66",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "LE Blumenson. 1960. A derivation of n-dimensional spherical coordinates. The American Mathematical Monthly, 67(1):63-66.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Enriching word vectors with subword information",
                "authors": [
                    {
                        "first": "Piotr",
                        "middle": [],
                        "last": "Bojanowski",
                        "suffix": ""
                    },
                    {
                        "first": "Edouard",
                        "middle": [],
                        "last": "Grave",
                        "suffix": ""
                    },
                    {
                        "first": "Armand",
                        "middle": [],
                        "last": "Joulin",
                        "suffix": ""
                    },
                    {
                        "first": "Tomas",
                        "middle": [],
                        "last": "Mikolov",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Transactions of the Association for Computational Linguistics",
                "volume": "5",
                "issue": "",
                "pages": "135--146",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching word vectors with subword information. Transactions of the Associa- tion for Computational Linguistics, 5:135-146.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Numeracy-600k: Learning numeracy for detecting exaggerated information in market comments",
                "authors": [
                    {
                        "first": "Chung-Chi",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Hen-Hsen",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Hiroya",
                        "middle": [],
                        "last": "Takamura",
                        "suffix": ""
                    },
                    {
                        "first": "Hsin-Hsi",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "6307--6313",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chung-Chi Chen, Hen-Hsen Huang, Hiroya Takamura, and Hsin-Hsi Chen. 2019. Numeracy-600k: Learn- ing numeracy for detecting exaggerated information in market comments. In Proceedings of the 57th An- nual Meeting of the Association for Computational Linguistics, pages 6307-6313.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Attention-based models for speech recognition",
                "authors": [
                    {
                        "first": "Jan",
                        "middle": [
                            "K"
                        ],
                        "last": "Chorowski",
                        "suffix": ""
                    },
                    {
                        "first": "Dzmitry",
                        "middle": [],
                        "last": "Bahdanau",
                        "suffix": ""
                    },
                    {
                        "first": "Dmitriy",
                        "middle": [],
                        "last": "Serdyuk",
                        "suffix": ""
                    },
                    {
                        "first": "Kyunghyun",
                        "middle": [],
                        "last": "Cho",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Advances in neural information processing systems",
                "volume": "",
                "issue": "",
                "pages": "577--585",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jan K Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua Bengio. 2015. Attention-based models for speech recogni- tion. In Advances in neural information processing systems, pages 577-585.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Finding contradictions in text",
                "authors": [
                    {
                        "first": "Marie-Catherine",
                        "middle": [],
                        "last": "De Marneffe",
                        "suffix": ""
                    },
                    {
                        "first": "Anna",
                        "middle": [
                            "N"
                        ],
                        "last": "Rafferty",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Proceedings of ACL-08: HLT",
                "volume": "",
                "issue": "",
                "pages": "1039--1047",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Marie-Catherine De Marneffe, Anna N Rafferty, and Christopher D Manning. 2008. Finding contradic- tions in text. In Proceedings of ACL-08: HLT, pages 1039-1047.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
                "authors": [
                    {
                        "first": "Jacob",
                        "middle": [],
                        "last": "Devlin",
                        "suffix": ""
                    },
                    {
                        "first": "Ming-Wei",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "Kenton",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Kristina",
                        "middle": [],
                        "last": "Toutanova",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1810.04805"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understand- ing. arXiv preprint arXiv:1810.04805.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs",
                "authors": [
                    {
                        "first": "Dheeru",
                        "middle": [],
                        "last": "Dua",
                        "suffix": ""
                    },
                    {
                        "first": "Yizhong",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Pradeep",
                        "middle": [],
                        "last": "Dasigi",
                        "suffix": ""
                    },
                    {
                        "first": "Gabriel",
                        "middle": [],
                        "last": "Stanovsky",
                        "suffix": ""
                    },
                    {
                        "first": "Sameer",
                        "middle": [],
                        "last": "Singh",
                        "suffix": ""
                    },
                    {
                        "first": "Matt",
                        "middle": [],
                        "last": "Gardner",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proc. of NAACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019. DROP: A reading comprehension benchmark requir- ing discrete reasoning over paragraphs. In Proc. of NAACL.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "A synopsis of linguistic theory, 1930-1955",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "John R Firth",
                        "suffix": ""
                    }
                ],
                "year": 1957,
                "venue": "Studies in linguistic analysis",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "John R Firth. 1957. A synopsis of linguistic theory, 1930-1955. Studies in linguistic analysis.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Distributional structure. word",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Harris",
                        "suffix": ""
                    }
                ],
                "year": 1954,
                "venue": "",
                "volume": "10",
                "issue": "",
                "pages": "146--162",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "ZS Harris. 1954. Distributional structure. word, 10 (2-3): 146-162. reprinted in fodor, j. a and katz, jj (eds.), readings in the philosophy of language.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Long short-term memory",
                "authors": [
                    {
                        "first": "Sepp",
                        "middle": [],
                        "last": "Hochreiter",
                        "suffix": ""
                    },
                    {
                        "first": "J\u00fcrgen",
                        "middle": [],
                        "last": "Schmidhuber",
                        "suffix": ""
                    }
                ],
                "year": 1997,
                "venue": "Neural computation",
                "volume": "9",
                "issue": "8",
                "pages": "1735--1780",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735-1780.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Learning numeral embeddings",
                "authors": [
                    {
                        "first": "Chengyue",
                        "middle": [],
                        "last": "Jiang",
                        "suffix": ""
                    },
                    {
                        "first": "Zhonglin",
                        "middle": [],
                        "last": "Nian",
                        "suffix": ""
                    },
                    {
                        "first": "Kaihao",
                        "middle": [],
                        "last": "Guo",
                        "suffix": ""
                    },
                    {
                        "first": "Shanbo",
                        "middle": [],
                        "last": "Chu",
                        "suffix": ""
                    },
                    {
                        "first": "Yinggong",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Libin",
                        "middle": [],
                        "last": "Shen",
                        "suffix": ""
                    },
                    {
                        "first": "Haofen",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Kewei",
                        "middle": [],
                        "last": "Tu",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2001.00003"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Chengyue Jiang, Zhonglin Nian, Kaihao Guo, Shanbo Chu, Yinggong Zhao, Libin Shen, Haofen Wang, and Kewei Tu. 2019. Learning numeral embeddings. arXiv preprint arXiv:2001.00003.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Bag of tricks for efficient text classification",
                "authors": [
                    {
                        "first": "Armand",
                        "middle": [],
                        "last": "Joulin",
                        "suffix": ""
                    },
                    {
                        "first": "Edouard",
                        "middle": [],
                        "last": "Grave",
                        "suffix": ""
                    },
                    {
                        "first": "Piotr",
                        "middle": [],
                        "last": "Bojanowski",
                        "suffix": ""
                    },
                    {
                        "first": "Tomas",
                        "middle": [],
                        "last": "Mikolov",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1607.01759"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. 2016. Bag of tricks for efficient text classification. arXiv preprint arXiv:1607.01759.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Skip-thought vectors",
                "authors": [
                    {
                        "first": "Ryan",
                        "middle": [],
                        "last": "Kiros",
                        "suffix": ""
                    },
                    {
                        "first": "Yukun",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Ruslan",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Salakhutdinov",
                        "suffix": ""
                    },
                    {
                        "first": "Raquel",
                        "middle": [],
                        "last": "Zemel",
                        "suffix": ""
                    },
                    {
                        "first": "Antonio",
                        "middle": [],
                        "last": "Urtasun",
                        "suffix": ""
                    },
                    {
                        "first": "Sanja",
                        "middle": [],
                        "last": "Torralba",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Fidler",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Advances in neural information processing systems",
                "volume": "",
                "issue": "",
                "pages": "3294--3302",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Skip-thought vectors. In Advances in neural information processing systems, pages 3294-3302.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "An exploration of word embedding initialization in deep-learning tasks",
                "authors": [
                    {
                        "first": "Tom",
                        "middle": [],
                        "last": "Kocmi",
                        "suffix": ""
                    },
                    {
                        "first": "Ond\u0159ej",
                        "middle": [],
                        "last": "Bojar",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1711.09160"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Tom Kocmi and Ond\u0159ej Bojar. 2017. An exploration of word embedding initialization in deep-learning tasks. arXiv preprint arXiv:1711.09160.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Efficient estimation of word representations in vector space",
                "authors": [
                    {
                        "first": "Tomas",
                        "middle": [],
                        "last": "Mikolov",
                        "suffix": ""
                    },
                    {
                        "first": "Kai",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Greg",
                        "middle": [],
                        "last": "Corrado",
                        "suffix": ""
                    },
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Dean",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1301.3781"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Tomas Mikolov, Kai Chen, Greg Corrado, and Jef- frey Dean. 2013a. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Distributed representations of words and phrases and their compositionality",
                "authors": [
                    {
                        "first": "Tomas",
                        "middle": [],
                        "last": "Mikolov",
                        "suffix": ""
                    },
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    },
                    {
                        "first": "Kai",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Greg",
                        "middle": [
                            "S"
                        ],
                        "last": "Corrado",
                        "suffix": ""
                    },
                    {
                        "first": "Jeff",
                        "middle": [],
                        "last": "Dean",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Advances in neural information processing systems",
                "volume": "",
                "issue": "",
                "pages": "3111--3119",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor- rado, and Jeff Dean. 2013b. Distributed representa- tions of words and phrases and their compositional- ity. In Advances in neural information processing systems, pages 3111-3119.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Language models based on semantic composition",
                "authors": [
                    {
                        "first": "Jeff",
                        "middle": [],
                        "last": "Mitchell",
                        "suffix": ""
                    },
                    {
                        "first": "Mirella",
                        "middle": [],
                        "last": "Lapata",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing",
                "volume": "1",
                "issue": "",
                "pages": "430--439",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jeff Mitchell and Mirella Lapata. 2009. Language mod- els based on semantic composition. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1-Volume 1, pages 430-439. Association for Computational Lin- guistics.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Exploring numeracy in word embeddings",
                "authors": [
                    {
                        "first": "Aakanksha",
                        "middle": [],
                        "last": "Naik",
                        "suffix": ""
                    },
                    {
                        "first": "Abhilasha",
                        "middle": [],
                        "last": "Ravichander",
                        "suffix": ""
                    },
                    {
                        "first": "Carolyn",
                        "middle": [],
                        "last": "Rose",
                        "suffix": ""
                    },
                    {
                        "first": "Eduard",
                        "middle": [],
                        "last": "Hovy",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "3374--3380",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Aakanksha Naik, Abhilasha Ravichander, Carolyn Rose, and Eduard Hovy. 2019. Exploring numeracy in word embeddings. In Proceedings of the 57th An- nual Meeting of the Association for Computational Linguistics, pages 3374-3380.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Glove: Global vectors for word representation",
                "authors": [
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Pennington",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)",
                "volume": "",
                "issue": "",
                "pages": "1532--1543",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word rep- resentation. In Proceedings of the 2014 conference on empirical methods in natural language process- ing (EMNLP), pages 1532-1543.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Deep contextualized word representations",
                "authors": [
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Matthew E Peters",
                        "suffix": ""
                    },
                    {
                        "first": "Mohit",
                        "middle": [],
                        "last": "Neumann",
                        "suffix": ""
                    },
                    {
                        "first": "Matt",
                        "middle": [],
                        "last": "Iyyer",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [],
                        "last": "Gardner",
                        "suffix": ""
                    },
                    {
                        "first": "Kenton",
                        "middle": [],
                        "last": "Clark",
                        "suffix": ""
                    },
                    {
                        "first": "Luke",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Zettlemoyer",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1802.05365"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word repre- sentations. arXiv preprint arXiv:1802.05365.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Squad: 100,000+ questions for machine comprehension of text",
                "authors": [
                    {
                        "first": "Pranav",
                        "middle": [],
                        "last": "Rajpurkar",
                        "suffix": ""
                    },
                    {
                        "first": "Jian",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Konstantin",
                        "middle": [],
                        "last": "Lopyrev",
                        "suffix": ""
                    },
                    {
                        "first": "Percy",
                        "middle": [],
                        "last": "Liang",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1606.05250"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Learning representations by back-propagating errors",
                "authors": [
                    {
                        "first": "Geoffrey",
                        "middle": [
                            "E"
                        ],
                        "last": "David E Rumelhart",
                        "suffix": ""
                    },
                    {
                        "first": "Ronald",
                        "middle": [
                            "J"
                        ],
                        "last": "Hinton",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Williams",
                        "suffix": ""
                    }
                ],
                "year": 1988,
                "venue": "Cognitive modeling",
                "volume": "5",
                "issue": "3",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "David E Rumelhart, Geoffrey E Hinton, Ronald J Williams, et al. 1988. Learning representations by back-propagating errors. Cognitive modeling, 5(3):1.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Learning compressed sentence representations for on-device text processing",
                "authors": [
                    {
                        "first": "Dinghan",
                        "middle": [],
                        "last": "Shen",
                        "suffix": ""
                    },
                    {
                        "first": "Pengyu",
                        "middle": [],
                        "last": "Cheng",
                        "suffix": ""
                    },
                    {
                        "first": "Dhanasekar",
                        "middle": [],
                        "last": "Sundararaman",
                        "suffix": ""
                    },
                    {
                        "first": "Xinyuan",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Qian",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Meng",
                        "middle": [],
                        "last": "Tang",
                        "suffix": ""
                    },
                    {
                        "first": "Asli",
                        "middle": [],
                        "last": "Celikyilmaz",
                        "suffix": ""
                    },
                    {
                        "first": "Lawrence",
                        "middle": [],
                        "last": "Carin",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "107--116",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dinghan Shen, Pengyu Cheng, Dhanasekar Sundarara- man, Xinyuan Zhang, Qian Yang, Meng Tang, Asli Celikyilmaz, and Lawrence Carin. 2019. Learning compressed sentence representations for on-device text processing. In Proceedings of the 57th Annual Meeting of the Association for Computational Lin- guistics, pages 107-116.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Numeracy for language models: Evaluating and improving their ability to predict numbers",
                "authors": [
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Georgios",
                        "suffix": ""
                    },
                    {
                        "first": "Sebastian",
                        "middle": [],
                        "last": "Spithourakis",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Riedel",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1805.08154"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Georgios P Spithourakis and Sebastian Riedel. 2018. Numeracy for language models: Evaluating and improving their ability to predict numbers. arXiv preprint arXiv:1805.08154.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Neural arithmetic logic units",
                "authors": [
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Trask",
                        "suffix": ""
                    },
                    {
                        "first": "Felix",
                        "middle": [],
                        "last": "Hill",
                        "suffix": ""
                    },
                    {
                        "first": "Scott",
                        "middle": [
                            "E"
                        ],
                        "last": "Reed",
                        "suffix": ""
                    },
                    {
                        "first": "Jack",
                        "middle": [],
                        "last": "Rae",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Dyer",
                        "suffix": ""
                    },
                    {
                        "first": "Phil",
                        "middle": [],
                        "last": "Blunsom",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "8035--8044",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Andrew Trask, Felix Hill, Scott E Reed, Jack Rae, Chris Dyer, and Phil Blunsom. 2018. Neural arith- metic logic units. In Advances in Neural Informa- tion Processing Systems, pages 8035-8044.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Do nlp models know numbers? probing numeracy in embeddings",
                "authors": [
                    {
                        "first": "Eric",
                        "middle": [],
                        "last": "Wallace",
                        "suffix": ""
                    },
                    {
                        "first": "Yizhong",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Sujian",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Sameer",
                        "middle": [],
                        "last": "Singh",
                        "suffix": ""
                    },
                    {
                        "first": "Matt",
                        "middle": [],
                        "last": "Gardner",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1909.07940"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Eric Wallace, Yizhong Wang, Sujian Li, Sameer Singh, and Matt Gardner. 2019. Do nlp models know num- bers? probing numeracy in embeddings. arXiv preprint arXiv:1909.07940.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "On the practical computational power of finite precision rnns for language recognition",
                "authors": [
                    {
                        "first": "Gail",
                        "middle": [],
                        "last": "Weiss",
                        "suffix": ""
                    },
                    {
                        "first": "Yoav",
                        "middle": [],
                        "last": "Goldberg",
                        "suffix": ""
                    },
                    {
                        "first": "Eran",
                        "middle": [],
                        "last": "Yahav",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1805.04908"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Gail Weiss, Yoav Goldberg, and Eran Yahav. 2018. On the practical computational power of finite pre- cision rnns for language recognition. arXiv preprint arXiv:1805.04908.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Towards universal paraphrastic sentence embeddings",
                "authors": [
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Wieting",
                        "suffix": ""
                    },
                    {
                        "first": "Mohit",
                        "middle": [],
                        "last": "Bansal",
                        "suffix": ""
                    },
                    {
                        "first": "Kevin",
                        "middle": [],
                        "last": "Gimpel",
                        "suffix": ""
                    },
                    {
                        "first": "Karen",
                        "middle": [],
                        "last": "Livescu",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1511.08198"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "John Wieting, Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2015. Towards universal para- phrastic sentence embeddings. arXiv preprint arXiv:1511.08198.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Qanet: Combining local convolution with global self-attention for reading comprehension",
                "authors": [
                    {
                        "first": "Adams",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Yu",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Dohan",
                        "suffix": ""
                    },
                    {
                        "first": "Minh-Thang",
                        "middle": [],
                        "last": "Luong",
                        "suffix": ""
                    },
                    {
                        "first": "Rui",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Kai",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Mohammad",
                        "middle": [],
                        "last": "Norouzi",
                        "suffix": ""
                    },
                    {
                        "first": "Quoc V",
                        "middle": [],
                        "last": "Le",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1804.09541"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, and Quoc V Le. 2018. Qanet: Combining local convolution with global self-attention for reading comprehen- sion. arXiv preprint arXiv:1804.09541.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 1: Proposed DICE embeddings. Vectors are colored according to numeral magnitude. Note that addition of two numbers in this embedding is performed by a shift, scaling, and rotation. Scaling depends only on the vector being added, as illustrated in sub-figure (c) in which the two black lines, corresponding to identical e j , have the same length.",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Figure 2: Qualitative examples where BERT + L num performed better than BERT-base",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "baseline model picks the whole sentence or paragraph involving the numerical value (Figure2 B) as the answer. Our method picks numbers within the classification span (Figure2B) and sometimes helps the BERT model to accurately pick up correct numbers (Figure 2 A), contributing to exact match and F1. More such examples are shown in Appendix C.",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF4": {
                "num": null,
                "text": "Figure 3: Qualitative examples where BERT + L num performed better than BERT base.",
                "uris": null,
                "fig_num": "3",
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table><tr><td/><td>OVA</td><td>SC</td><td>BC</td></tr><tr><td>Random</td><td colspan=\"3\">0.04 48.92 49.34</td></tr><tr><td>Glove 6B-200D</td><td colspan=\"3\">15.88 62.21 83.94</td></tr><tr><td>Glove 6B-300D</td><td colspan=\"3\">18.41 62.92 83.98</td></tr><tr><td colspan=\"4\">Glove-840B-300D 5.18 55.58 91.86</td></tr><tr><td>FastText-Wiki</td><td colspan=\"3\">13.94 59.96 96.15</td></tr><tr><td>FastText-CC</td><td colspan=\"3\">7.83 53.89 85.40</td></tr><tr><td>Skip-gram-5</td><td colspan=\"3\">8.85 55.40 96.42</td></tr><tr><td>Skip-gram-Dep</td><td colspan=\"3\">3.32 51.99 94.60</td></tr><tr><td>DICE-D (ours)</td><td colspan=\"3\">95.63 99.66 99.64</td></tr></table>",
                "type_str": "table",
                "text": "Performance (% accuracy) on numeracy tests.",
                "html": null,
                "num": null
            },
            "TABREF1": {
                "content": "<table><tr><td/><td colspan=\"3\">List maximum (accuracy)</td><td colspan=\"3\">Decoding (RMSE)</td><td colspan=\"3\">Addition (RMSE)</td></tr><tr><td>Integer range</td><td colspan=\"9\">[0, 99] [0, 999] [0, 9999] [0, 99] [0, 999] [0, 9999] [0, 99] [0, 999] [0, 9999]</td></tr><tr><td>Random vectors</td><td>0.16</td><td>0.23</td><td>0.21</td><td>29.86</td><td>292.88</td><td>2882.62</td><td>42.03</td><td>410.33</td><td>4389.39</td></tr><tr><td>Untrained CNN</td><td>0.97</td><td>0.87</td><td>0.84</td><td>2.64</td><td>9.67</td><td>44.40</td><td>1.41</td><td>14.43</td><td>69.14</td></tr><tr><td>Untrained LSTM</td><td>0.70</td><td>0.66</td><td>0.55</td><td>7.61</td><td>46.5</td><td>210.34</td><td>5.11</td><td>45.69</td><td>510.19</td></tr><tr><td>Value embedding</td><td>0.99</td><td>0.88</td><td>0.68</td><td>1.20</td><td>11.23</td><td>275.50</td><td>0.30</td><td>15.98</td><td>654.33</td></tr><tr><td>Pretrained</td><td/><td/><td/><td/><td/><td/><td/><td/><td/></tr><tr><td>Word2Vec</td><td>0.90</td><td>0.78</td><td>0.71</td><td>2.34</td><td>18.77</td><td>333.47</td><td>0.75</td><td>21.23</td><td>210.07</td></tr><tr><td>GloVE</td><td>0.90</td><td>0.78</td><td>0.72</td><td>2.23</td><td>13.77</td><td>174.21</td><td>0.80</td><td>16.51</td><td>180.31</td></tr><tr><td>ELMo</td><td>0.98</td><td>0.88</td><td>0.76</td><td>2.35</td><td>13.48</td><td>62.20</td><td>0.94</td><td>15.50</td><td>45.71</td></tr><tr><td>BERT</td><td>0.95</td><td>0.62</td><td>0.52</td><td>3.21</td><td>29.00</td><td>431.78</td><td>4.56</td><td>67.81</td><td>454.78</td></tr><tr><td>Learned</td><td/><td/><td/><td/><td/><td/><td/><td/><td/></tr><tr><td>Char-CNN</td><td>0.97</td><td>0.93</td><td>0.88</td><td>2.50</td><td>4.92</td><td>11.57</td><td>1.19</td><td>7.75</td><td>15.09</td></tr><tr><td>Char-LSTM</td><td>0.98</td><td>0.92</td><td>0.76</td><td>2.55</td><td>8.65</td><td>18.33</td><td>1.21</td><td>15.11</td><td>25.37</td></tr><tr><td>DROP-trained</td><td/><td/><td/><td/><td/><td/><td/><td/><td/></tr><tr><td>NAQANet</td><td>0.91</td><td>0.81</td><td>0.72</td><td>2.99</td><td>14.19</td><td>62.17</td><td>1.11</td><td>11.33</td><td>90.01</td></tr><tr><td>NAQANet (w/out GloVe)</td><td>0.88</td><td>0.90</td><td>0.82</td><td>2.87</td><td>5.34</td><td>35.39</td><td>1.45</td><td>9.91</td><td>60.70</td></tr><tr><td>Ours</td><td/><td/><td/><td/><td/><td/><td/><td/><td/></tr><tr><td>DICE-D</td><td>0.98</td><td>0.87</td><td>0.96</td><td>0.43</td><td>0.83</td><td>3.16</td><td>0.75</td><td>2.79</td><td>29.95</td></tr></table>",
                "type_str": "table",
                "text": "Experimental results on list maximum, decoding, and addition using the DICE-D method.",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table><tr><td>Model</td><td colspan=\"2\">Micro-F1 Macro-F1</td></tr><tr><td>LR</td><td>62.49</td><td>30.81</td></tr><tr><td>CNN</td><td>69.27</td><td>35.96</td></tr><tr><td>GRU</td><td>70.92</td><td>38.43</td></tr><tr><td>BiGRU</td><td>71.49</td><td>39.94</td></tr><tr><td>CRNN</td><td>69.50</td><td>36.15</td></tr><tr><td>CNN-capsule</td><td>63.11</td><td>29.41</td></tr><tr><td>GRU-capsule</td><td>70.73</td><td>33.57</td></tr><tr><td>BiGRU-capsule</td><td>71.49</td><td>34.18</td></tr><tr><td>BiLSTM with DICE</td><td>75.56</td><td>46.80</td></tr></table>",
                "type_str": "table",
                "text": "Table 3 shows significant improvements in the F1 score achieved by the model. To investigate the effects of dimensions of the embedding",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table><tr><td colspan=\"3\">Embedding Size Micro-F1 Macro-F1</td></tr><tr><td>32</td><td>74.63</td><td>45.92</td></tr><tr><td>64</td><td>74.90</td><td>45.99</td></tr><tr><td>128</td><td>75.55</td><td>46.36</td></tr><tr><td>256</td><td>75.56</td><td>45.56</td></tr><tr><td>512</td><td>74.14</td><td>46.80</td></tr></table>",
                "type_str": "table",
                "text": "Performance (%) on classifying number magnitude on the Numeracy-600k dataset.",
                "html": null,
                "num": null
            },
            "TABREF4": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Performance (%) of BiLSTM-attention with DICE model on the Numeracy-600k dataset by varying the embedding dimensions of input tokens.",
                "html": null,
                "num": null
            },
            "TABREF7": {
                "content": "<table/>",
                "type_str": "table",
                "text": "F1 scores of BERT-base model on SQuAD 1.1 sub-splits (all scores are statistically significant with a variance of 0.01). Sub-split 1: both training and testing splits contains only numerical answers; Sub-split 2: train split contains atleast one number in the answer and testing split contains only numerical answers.",
                "html": null,
                "num": null
            },
            "TABREF8": {
                "content": "<table><tr><td>C)</td><td>Context</td><td colspan=\"2\">Question</td></tr><tr><td/><td/><td colspan=\"3\">In what years did Spain</td></tr><tr><td/><td/><td colspan=\"3\">and Portugal join the</td></tr><tr><td/><td/><td colspan=\"3\">European Union?</td></tr><tr><td/><td/><td colspan=\"2\">Answer</td></tr><tr><td/><td/><td colspan=\"2\">Ground truth:</td><td>1985</td></tr><tr><td/><td/><td>BERT :</td><td/></tr><tr><td/><td/><td colspan=\"3\">greece in 1979 , spain</td></tr><tr><td/><td/><td colspan=\"3\">and portugal 1985</td></tr><tr><td/><td/><td>BERT +</td><td>\u2112 num</td><td>:</td><td>1985</td></tr><tr><td colspan=\"4\">Figure 3 provides additional samples where BERT</td></tr><tr><td colspan=\"5\">+ L num outperformed the baseline BERT model.</td></tr><tr><td colspan=\"4\">Similar to previous observations, our regularized</td></tr><tr><td colspan=\"4\">approach is able to pinpoint the correct number as</td></tr><tr><td colspan=\"5\">opposed to selecting a substring via pattern match-</td></tr><tr><td>ing.</td><td/><td/><td/></tr></table>",
                "type_str": "table",
                "text": "The principal Treaties that form the European Union began \u2026 institutions were established through the Treaty of Rome 1957 and the Maastricht Treaty 1992 (now: TFEU). Minor amendments were made during the 1960s and 1970s. Major amending treaties \u2026 the Single European Act 1986, to further the development of a more social Europe in the Treaty of Amsterdam 1997, and \u2026 EU institutions in the Treaty of Nice 2001 and the Treaty of Lisbon 2007. Since its establishment, \u2026 the UK, Ireland, Denmark and Norway in 1972 \u2026, Greece in 1979, Spain and Portugal 1985, Austria, Finland, Norway and Sweden in 1994 \u2026, the Czech Republic, Cyprus, \u2026 Slovakia and Slovenia in 2004 \u2026 One of the things Tesla developed at that laboratory in 1887 was an induction motor that ran on alternating current, \u2026 highvoltage transmission. The motor used \u2026 turn the motor (a principle Tesla claimed to have c o n c e i v e d i n 1 8 8 2 ) . T h i s i n n o v a t i v e e l e c t r i c m o t o r, patented in May 1888, was a simple self-starting design that did not need a commutator, \u2026 constantly servicing and replacing mechanical brushes.",
                "html": null,
                "num": null
            }
        }
    }
}