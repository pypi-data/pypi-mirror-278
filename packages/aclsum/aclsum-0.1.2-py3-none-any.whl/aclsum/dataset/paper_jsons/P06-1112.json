{
    "paper_id": "P06-1112",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:10:21.468178Z"
    },
    "title": "Exploring Correlation of Dependency Relation Paths for Answer Extraction",
    "authors": [
        {
            "first": "Dan",
            "middle": [],
            "last": "Shen",
            "suffix": "",
            "affiliation": {},
            "email": "dshen@coli.uni-sb.de"
        },
        {
            "first": "Dietrich",
            "middle": [],
            "last": "Klakow",
            "suffix": "",
            "affiliation": {},
            "email": "klakow@lsv.uni-saarland.de"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "In this paper, we explore correlation of dependency relation paths to rank candidate answers in answer extraction. Using the correlation measure, we compare dependency relations of a candidate answer and mapped question phrases in sentence with the corresponding relations in question. Different from previous studies, we propose an approximate phrase mapping algorithm and incorporate the mapping score into the correlation measure. The correlations are further incorporated into a Maximum Entropy-based ranking model which estimates path weights from training. Experimental results show that our method significantly outperforms state-ofthe-art syntactic relation-based methods by up to 20% in MRR.",
    "pdf_parse": {
        "paper_id": "P06-1112",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "In this paper, we explore correlation of dependency relation paths to rank candidate answers in answer extraction. Using the correlation measure, we compare dependency relations of a candidate answer and mapped question phrases in sentence with the corresponding relations in question. Different from previous studies, we propose an approximate phrase mapping algorithm and incorporate the mapping score into the correlation measure. The correlations are further incorporated into a Maximum Entropy-based ranking model which estimates path weights from training. Experimental results show that our method significantly outperforms state-ofthe-art syntactic relation-based methods by up to 20% in MRR.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Answer Extraction is one of basic modules in open domain Question Answering (QA). It is to further process relevant sentences extracted with Passage / Sentence Retrieval and pinpoint exact answers using more linguistic-motivated analysis. Since QA turns to find exact answers rather than text snippets in recent years, answer extraction becomes more and more crucial.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Typically, answer extraction works in the following steps:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\u2022 Recognize expected answer type of a question.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\u2022 Annotate relevant sentences with various types of named entities.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\u2022 Regard the phrases annotated with the expected answer type as candidate answers.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\u2022 Rank candidate answers.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In the above work flow, answer extraction heavily relies on named entity recognition (NER). On one hand, NER reduces the number of candidate answers and eases answer ranking. On the other hand, the errors from NER directly degrade answer extraction performance. To our knowledge, most top ranked QA systems in TREC are supported by effective NER modules which may identify and classify more than 20 types of named entities (NE), such as abbreviation, music, movie, etc. However, developing such named entity recognizer is not trivial. Up to now, we haven't found any paper relevant to QA-specific NER development. So, it is hard to follow their work. In this paper, we just use a general MUC-based NER, which makes our results reproducible.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "A general MUC-based NER can't annotate a large number of NE classes. In this case, all noun phrases in sentences are regarded as candidate answers, which makes candidate answer sets much larger than those filtered by a well developed NER. The larger candidate answer sets result in the more difficult answer extraction. Previous methods working on surface word level, such as density-based ranking and pattern matching, may not perform well. Deeper linguistic analysis has to be conducted. This paper proposes a statistical method which exploring correlation of dependency relation paths to rank candidate answers. It is motivated by the observation that relations between proper answers and question phrases in candidate sentences are always similar to the corresponding relations in question. For example, the question \"What did Alfred Nobel invent?\" and the candidate sentence \"... in the will of Swedish industrialist Alfred Nobel, who invented dynamite.\"",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "For each question, firstly, dependency relation paths are defined and extracted from the question and each of its candidate sentences. Secondly, the paths from the question and the candidate sentence are paired according to question phrase mapping score. Thirdly, correlation between two paths of each pair is calculated by employing Dynamic Time Warping algorithm. The input of the calculation is correlations between dependency relations, which are estimated from a set of training path pairs. Lastly, a Maximum Entropy-based ranking model is proposed to incorporate the path correlations and rank candidate answers. Furthermore, sentence supportive measure are presented according to correlations of relation paths among question phrases. It is applied to re-rank the candidate answers extracted from the different candidate sentences. Considering phrases may provide more accurate information than individual words, we extract dependency relations on phrase level instead of word level.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The experiment on TREC questions shows that our method significantly outperforms a densitybased method by 50% in MRR and three stateof-the-art syntactic-based methods by up to 20% in MRR. Furthermore, we classify questions by judging whether NER is used. We investigate how these methods perform on the two question sets. The results indicate that our method achieves better performance than the other syntactic-based methods on both question sets. Especially for more difficult questions, for which NER may not help, our method improves MRR by up to 31%.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The paper is organized as follows. Section 2 discusses related work and clarifies what is new in this paper. Section 3 presents relation path correlation in detail. Section 4 and 5 discuss how to incorporate the correlations for answer ranking and re-ranking. Section 6 reports experiment and results.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In recent years' TREC Evaluation, most top ranked QA systems use syntactic information in answer extraction. Next, we will briefly discuss the main usages. (Kaisser and Becker, 2004 ) match a question into one of predefined patterns, such as \"When did Jack Welch retire from GE?\" to the pattern \"When+did+NP+Verb+NPorPP\". For each question pattern, there is a set of syntactic structures for potential answer. Candidate answers are ranked by matching the syntactic structures. This method worked well on TREC questions. However, it is costing to manually construct question patterns and syntactic structures of the patterns. (Shen et al., 2005) classify question words into four classes target word, head word, subject word and verb. For each class, syntactic relation patterns which contain one question word and one proper answer are automatically extracted and scored from training sentences. Then, candidate answers are ranked by partial matching to the syntactic relation patterns using tree kernel. However, the criterion to classify the question words is not clear in their paper. Proper answers may have absolutely different relations with different subject words in sentences. They don't consider the corresponding relations in questions. (Tanev et al., 2004; Wu et al., 2005) compare syntactic relations in questions and those in answer sentences. (Tanev et al., 2004 ) reconstruct a basic syntactic template tree for a question, in which one of the nodes denotes expected answer position. Then, answer candidates for this question are ranked by matching sentence syntactic tree to the question template tree. Furthermore, the matching is weighted by lexical variations. (Wu et al., 2005) combine n-gram proximity search and syntactic relation matching. For syntactic relation matching, question tree and sentence subtree around a candidate answer are matched from node to node.",
                "cite_spans": [
                    {
                        "start": 156,
                        "end": 181,
                        "text": "(Kaisser and Becker, 2004",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 625,
                        "end": 644,
                        "text": "(Shen et al., 2005)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 1248,
                        "end": 1268,
                        "text": "(Tanev et al., 2004;",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 1269,
                        "end": 1285,
                        "text": "Wu et al., 2005)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 1358,
                        "end": 1377,
                        "text": "(Tanev et al., 2004",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 1681,
                        "end": 1698,
                        "text": "(Wu et al., 2005)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "Although the above systems apply the different methods to compare relations in question and answer sentences, they follow the same hypothesis that proper answers are more likely to have same relations in question and answer sentences. For example, in question \"Who founded the Black Panthers organization?\", where, the question word \"who\" has the dependency relations \"subj\" with \"found\" and \"subj obj nn\" with \"Black Panthers organization\", in sentence \"Hilliard introduced Bobby Seale, who co-founded the Black Panther Party here ...\", the proper answer \"Bobby Seale\" has the same relations with most question phrases. These methods achieve high precision, but poor recall due to relation variations. One meaning is often represented as different relation combinations. In the above example, appositive rela-tion frequently appears in answer sentences, such as \"Black Panther Party co-founder Bobby Seale is ordered bound and gagged ...\" and indicates proper answer Bobby Seale although it is asked in different way in the question. (Cui et al., 2004) propose an approximate dependency relation matching method for both passage retrieval and answer extraction. The similarity between two relations is measured by their co-occurrence rather than exact matching. They state that their method effectively overcomes the limitation of the previous exact matching methods. Lastly, they use the sum of similarities of all path pairs to rank candidate answers, which is based on the assumption that all paths have equal weights. However, it might not be true. For example, in question \"What book did Rachel Carson write in 1962?\", the phrase \"Rachel Carson\" looks like more important than \"1962\" since the former is question topic and the latter is a constraint for expected answer. In addition, lexical variations are not well considered and a weak relation path alignment algorithm is used in their work.",
                "cite_spans": [
                    {
                        "start": 1035,
                        "end": 1053,
                        "text": "(Cui et al., 2004)",
                        "ref_id": "BIBREF1"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "Based on the previous works, this paper explores correlation of dependency relation paths between questions and candidate sentences. Dynamic time warping algorithm is adapted to calculate path correlations and approximate phrase mapping is proposed to cope with phrase variations. Finally, maximum entropy-based ranking model is developed to incorporate the correlations and rank candidate answers.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "In this section, we discuss how the method performs in detail.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dependency Relation Path Correlation",
                "sec_num": "3"
            },
            {
                "text": "We parse questions and candidate sentences with MiniPar (Lin, 1994) , a fast and robust parser for grammatical dependency relations. Then, we extract relation paths from dependency trees.",
                "cite_spans": [
                    {
                        "start": 56,
                        "end": 67,
                        "text": "(Lin, 1994)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dependency Relation Path Extraction",
                "sec_num": "3.1"
            },
            {
                "text": "Dependency relation path is defined as a structure P =< N 1 , R, N 2 > where, N 1 , N 2 are two phrases and R is a relation sequence R =< r 1 , ..., r i > in which r i is one of the predefined dependency relations. Totally, there are 42 relations defined in MiniPar. A relation sequence R between two phrases N 1 , N 2 is extracted by traversing from the N 1 node to the N 2 node in a dependency tree. For each question, we extract relation paths among noun phrases, main verb and question word. The question word is further replaced with \"EAP\", which indicates the expected answer position. For each candidate sentence, we firstly extract relation paths between answer candidates and mapped question phrases. These paths will be used for answer ranking (Section 4). Secondly, we extract relation paths among mapped question phrases. These paths will be used for answer reranking (Section 5). Question phrase mapping will be discussed in Section 3.4. Figure 1 shows some relation paths extracted for an example question and candidate sentence.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 958,
                        "end": 959,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Dependency Relation Path Extraction",
                "sec_num": "3.1"
            },
            {
                "text": "Next, the relation paths in a question and each of its candidate sentences are paired according to their phrase similarity. For any two relation path P i and P j which are extracted from the question and the candidate sentence respectively, if Sim(N i1 , N j1 ) > 0 and Sim(N i2 , N j2 ) > 0, P i and P j are paired as < P i , P j >. The question phrase \"EAP\" is mapped to candidate answer phrase in the sentence. The similarity between two ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dependency Relation Path Extraction",
                "sec_num": "3.1"
            },
            {
                "text": "Comparing a proper answer and other wrong candidate answers in each sentence, we assume that relation paths between the proper answer and question phrases in the sentence are more correlated to the corresponding paths in question. So, for each path pair < P 1 , P 2 >, we measure the correlation between its two paths P 1 and P 2 .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dependency Relation Path Correlation",
                "sec_num": "3.2"
            },
            {
                "text": "We derive the correlations between paths by adapting dynamic time warping (DTW) algorithm (Rabiner et al., 1978) . DTW is to find an optimal alignment between two sequences which maximizes the accumulated correlation between two sequences. A sketch of the adapted algorithm is as follows.",
                "cite_spans": [
                    {
                        "start": 90,
                        "end": 112,
                        "text": "(Rabiner et al., 1978)",
                        "ref_id": "BIBREF5"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dependency Relation Path Correlation",
                "sec_num": "3.2"
            },
            {
                "text": "Let R 1 =< r 11 , ..., r 1n >, (n = 1, ..., N ) and R 2 =< r 21 , ..., r 2m >, (m = 1, ..., M ) denote two relation sequences. R 1 and R 2 consist of N and M relations respectively. R 1 (n) = r 1n and R 2 (m) = r 2m . Cor(r 1 , r 2 ) denotes the correlation between two individual relations r 1 , r 2 , which is estimated by a statistical model during training (Section 3.3). Given the correlations Cor(r 1n , r 2m ) for each pair of relations (r 1n , r 2m ) within R 1 and R 2 , the goal of DTW is to find a path, m = map(n), which map n onto the corresponding m such that the accumulated correlation Cor * along the path is maximized.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dependency Relation Path Correlation",
                "sec_num": "3.2"
            },
            {
                "text": "Cor * = max map(n) N n=1 Cor(R 1 (n), R 2 (map(n))",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dependency Relation Path Correlation",
                "sec_num": "3.2"
            },
            {
                "text": "A dynamic programming method is used to determine the optimum path map(n). The accumulated correlation Cor A to any grid point (n, m) can be recursively calculated as",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dependency Relation Path Correlation",
                "sec_num": "3.2"
            },
            {
                "text": "CorA(n, m) = Cor(r1n, r2m) + max q\u2264m CorA(n -1, q) Cor * = CorA(N, M )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dependency Relation Path Correlation",
                "sec_num": "3.2"
            },
            {
                "text": "The overall correlation measure has to be normalized as longer sequences normally give higher correlation value. So, the correlation between two sequences R 1 and R 2 is calculated as",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dependency Relation Path Correlation",
                "sec_num": "3.2"
            },
            {
                "text": "Cor(R 1 , R 2 ) = Cor * / max(N, M )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dependency Relation Path Correlation",
                "sec_num": "3.2"
            },
            {
                "text": "Finally, we define the correlation between two relation paths P 1 and P 2 as",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dependency Relation Path Correlation",
                "sec_num": "3.2"
            },
            {
                "text": "Cor(P 1 , P 2 ) = Cor(R 1 , R 2 ) \u00d7 Sim(N 11 , N 21 ) \u00d7 Sim(N 12 , N 22 )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dependency Relation Path Correlation",
                "sec_num": "3.2"
            },
            {
                "text": "Where, Sim(N 11 , N 21 ) and Sim(N 12 , N 22 ) are the phrase mapping score when pairing two paths, which will be described in Section 3.4. If two phrases are absolutely different Cor(N 11 , N 21 ) = 0 or Cor(N 12 , N 22 ) = 0, the paths may not be paired since Cor(P 1 , P 2 ) = 0.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dependency Relation Path Correlation",
                "sec_num": "3.2"
            },
            {
                "text": "In the above section, we have described how to measure path correlations. The measure requires relation correlations Cor(r 1 , r 2 ) as inputs. We apply a statistical method to estimate the relation correlations from a set of training path pairs. The training data collecting will be described in Section 6.1.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Relation Correlation Estimation",
                "sec_num": "3.3"
            },
            {
                "text": "For each question and its answer sentences in training data, we extract relation paths between \"EAP\" and other phrases in the question and paths between proper answer and mapped question phrases in the sentences. After pairing the question paths and the corresponding sentence paths, correlation of two relations is measured by their bipartite co-occurrence in all training path pairs. Mutual information-based measure (Cui et al., 2004 ) is employed to calculate the relation correlations.",
                "cite_spans": [
                    {
                        "start": 419,
                        "end": 436,
                        "text": "(Cui et al., 2004",
                        "ref_id": "BIBREF1"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Relation Correlation Estimation",
                "sec_num": "3.3"
            },
            {
                "text": "Cor(r Q i , r S j ) = log \u03b1 \u00d7 \u03b4(r Q i , r S j ) f Q (r Q i ) \u00d7 f S (r S j )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Relation Correlation Estimation",
                "sec_num": "3.3"
            },
            {
                "text": "where, r Q i and r S j are two relations in question paths and sentence paths respectively. f Q (r Q i ) and f S (r S j ) are the numbers of occurrences of r Q i in question paths and r S j in sentence paths respectively. \u03b4(r Q i , r S j ) is 1 when r Q i and r S j co-occur in a path pair, and 0 otherwise. \u03b1 is a factor to discount the co-occurrence value for long paths. It is set to the inverse proportion of the sum of path lengths of the path pair.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Relation Correlation Estimation",
                "sec_num": "3.3"
            },
            {
                "text": "Basic noun phrases (BNP) and verbs in questions are mapped to their candidate sentences. A BNP is defined as the smallest noun phrase in which there are no noun phrases embedded. To address lexical and format variations between phrases, we propose an approximate phrase mapping strategy.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Approximate Question Phrase Mapping",
                "sec_num": "3.4"
            },
            {
                "text": "A BNP is separated into a set of heads H = {h 1 , ..., h i } and a set of modifiers M = {m 1 , ...m j }. Some heuristic rules are applied to judge heads and modifiers: 1. If BNP is a named entity, all words are heads. 2. The last word of BNP is head. 3. Rest words are modifiers.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Approximate Question Phrase Mapping",
                "sec_num": "3.4"
            },
            {
                "text": "The similarity between two BNPs Sim(BN P q , BN P s ) is defined as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Approximate Question Phrase Mapping",
                "sec_num": "3.4"
            },
            {
                "text": "Sim(BN P q , BN P s ) = \u03bbSim(H q , H s ) + (1 -\u03bb)Sim(M q , M s ) Sim(H q , H s ) = h i \u2208Hq h j \u2208Hs Sim(h i ,h j ) |H q H s | Sim(M q , M s ) = m i \u2208M q m j \u2208M s Sim(m i ,m j ) |M q M s |",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Approximate Question Phrase Mapping",
                "sec_num": "3.4"
            },
            {
                "text": "Furthermore, the similarity between two heads Sim(h i , h j ) are defined as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Approximate Question Phrase Mapping",
                "sec_num": "3.4"
            },
            {
                "text": "\u2022 Sim = 1, if h i = h j after stemming; \u2022 Sim = 1, if h i = h j after format alternation; \u2022 Sim = SemSim(h i , h j )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Approximate Question Phrase Mapping",
                "sec_num": "3.4"
            },
            {
                "text": "These items consider morphological, format and semantic variations respectively. 1. The morphological variations match words after stemming, such as \"Rhodes scholars\" and \"Rhodes scholarships\". 2. The format alternations cope with special characters, such as \"-\" for \"Ice-T\" and \"Ice T\", \"&\" for \"Abercrombie and Fitch\" and \"Abercrombie & Fitch\". 3. The semantic similarity SemSim(h i , h j ) is measured using Word-Net and eXtended WordNet. We use the same semantic path finding algorithm, relation weights and semantic similarity measure as (Moldovan and Novischi, 2002) . For efficiency, only hypernym, hyponym and entailment relations are considered and search depth is set to 2 in our experiments.",
                "cite_spans": [
                    {
                        "start": 543,
                        "end": 572,
                        "text": "(Moldovan and Novischi, 2002)",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Approximate Question Phrase Mapping",
                "sec_num": "3.4"
            },
            {
                "text": "Particularly, the semantic variations are not considered for NE heads and modifiers. Modifier similarity Sim(m i , m j ) only consider the morphological and format variations. Moreover, verb similarity measure Sim(v 1 , v 2 ) is the same as head similarity measure Sim(h i , h j ).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Approximate Question Phrase Mapping",
                "sec_num": "3.4"
            },
            {
                "text": "According to path correlations of candidate answers, a Maximum Entropy (ME)-based model is applied to rank candidate answers. Unlike (Cui et al., 2004) , who rank candidate answers with the sum of the path correlations, ME model may estimate the optimal weights of the paths based on a training data set. (Berger et al., 1996) gave a good description of ME model. The model we use is similar to (Shen et al., 2005; Ravichandran et al., 2003) , which regard answer extraction as a ranking problem instead of a classification problem. We apply Generalized Iterative Scaling for model parameter estimation and Gaussian Prior for smoothing.",
                "cite_spans": [
                    {
                        "start": 133,
                        "end": 151,
                        "text": "(Cui et al., 2004)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 305,
                        "end": 326,
                        "text": "(Berger et al., 1996)",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 395,
                        "end": 414,
                        "text": "(Shen et al., 2005;",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 415,
                        "end": 441,
                        "text": "Ravichandran et al., 2003)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Candidate Answer Ranking",
                "sec_num": "4"
            },
            {
                "text": "If expected answer type is unknown during question processing or corresponding type of named entities isn't recognized in candidate sentences, we regard all basic noun phrases as candidate answers. Since a MUC-based NER loses many types of named entities, we have to handle larger candidate answer sets. Orthographic features, similar to (Shen et al., 2005) , are extracted to capture word format information of candidate answers, such as capitalizations, digits and lengths, etc. We expect they may help to judge what proper answers look like since most NER systems work on these features.",
                "cite_spans": [
                    {
                        "start": 338,
                        "end": 357,
                        "text": "(Shen et al., 2005)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Candidate Answer Ranking",
                "sec_num": "4"
            },
            {
                "text": "Next, we will discuss how to incorporate path correlations. Two facts are considered to affect path weights: question phrase type and path length. For each question, we divide question phrases into four types: target, topic, constraint and verb. Target is a kind of word which indicates the expected answer type of the question, such as \"party\" in \"What party led Australia from 1983 to 1996?\". Topic is the event/person that the question talks about, such as \"Australia\". Intuitively, it is the most important phrase of the question. Constraint are the other phrases of the question except topic, such as \"1983\" and \"1996\". Verb is the main verb of the question, such as \"lead\". Furthermore, since shorter path indicates closer relation between two phrases, we discount path correlation in long question path by dividing the correlation by the length of the question path. Lastly, we sum the discounted path correlations for each type of question phrases and fire it as a feature, such as \"Target Cor=c, where c is the correlation value for question target. ME-based ranking model incorporate the orthographic and path correlation features to rank candidate answers for each of candidate sentences.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Candidate Answer Ranking",
                "sec_num": "4"
            },
            {
                "text": "After ranking candidate answers, we select the highest ranked one from each candidate sentence. In this section, we are to re-rank them according to sentence supportive degree. We assume that a candidate sentence supports an answer if relations between mapped question phrases in the candidate sentence are similar to the corresponding ones in question. Relation paths between any two question phrases are extracted and paired. Then, correlation of each pair is calculated. Re-rank formula is defined as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Candidate Answer Re-ranking",
                "sec_num": "5"
            },
            {
                "text": "Score(answer) = \u03b1 \u00d7 i Cor(Pi1, Pi2)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Candidate Answer Re-ranking",
                "sec_num": "5"
            },
            {
                "text": "where, \u03b1 is answer ranking score. It is the normalized prediction value of the ME-based ranking model described in Section 4.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Candidate Answer Re-ranking",
                "sec_num": "5"
            },
            {
                "text": "i Cor(P i1 , P i2 ) is the sum of correlations of all path pairs. Finally, the answer with the highest score is returned.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Candidate Answer Re-ranking",
                "sec_num": "5"
            },
            {
                "text": "In this section, we set up experiments on TREC factoid questions and report evaluation results.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "6"
            },
            {
                "text": "The goal of answer extraction is to identify exact answers from given candidate sentence collections for questions. The candidate sentences are regarded as the most relevant sentences to the questions and retrieved by IR techniques. Qualities of the candidate sentences have a strong impact on answer extraction. It is meaningless to evaluate the questions of which none candidate sentences contain proper answer in answer extraction experiment. To our knowledge, most of current QA systems lose about half of questions in sentence retrieval stage. To make more questions evaluated in our experiments, for each of questions, we automatically build a candidate sentence set from TREC judgements rather than use sentence retrieval output.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiment Setup",
                "sec_num": "6.1"
            },
            {
                "text": "We use TREC99-03 questions for training and TREC04 questions for testing. As to build training data, we retrieve all of the sentences which contain proper answers from relevant documents according to TREC judgements and answer patterns.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiment Setup",
                "sec_num": "6.1"
            },
            {
                "text": "Then, We manually check the sentences and remove those in which answers cannot be supported. As to build candidate sentence sets for testing, we retrieve all of the sentences from relevant documents in judgements and keep those which contain at least one question key word. Therefore, each question has at least one proper candidate sentence which contains proper answer in its candidate sentence set.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiment Setup",
                "sec_num": "6.1"
            },
            {
                "text": "There are 230 factoid questions (27 NIL questions) in TREC04. NIL questions are excluded from our test set because TREC doesn't supply relevant documents and answer patterns for them. Therefore, we will evaluate 203 TREC04 questions. Five answer extraction methods are evaluated for comparison:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiment Setup",
                "sec_num": "6.1"
            },
            {
                "text": "\u2022 Density: Density-based method is used as baseline, in which we choose candidate answer with the shortest surface distance to question phrases.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiment Setup",
                "sec_num": "6.1"
            },
            {
                "text": "\u2022 SynPattern: Syntactic relation patterns (Shen et al., 2005) are automatically extracted from training set and are partially matched using tree kernel.",
                "cite_spans": [
                    {
                        "start": 42,
                        "end": 61,
                        "text": "(Shen et al., 2005)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiment Setup",
                "sec_num": "6.1"
            },
            {
                "text": "\u2022 StrictMatch: Strict relation matching follows the assumption in (Tanev et al., 2004; Wu et al., 2005) . We implement it by adapting relation correlation score. In stead of learning relation correlations during training, we predefine them as: Cor(r 1 , r 2 ) = 1 if r 1 = r 2 ; 0, otherwise.",
                "cite_spans": [
                    {
                        "start": 66,
                        "end": 86,
                        "text": "(Tanev et al., 2004;",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 87,
                        "end": 103,
                        "text": "Wu et al., 2005)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiment Setup",
                "sec_num": "6.1"
            },
            {
                "text": "\u2022 ApprMatch: Approximate relation matching (Cui et al., 2004) aligns two relation paths using fuzzy matching and ranks candidates according to the sum of all path similarities.",
                "cite_spans": [
                    {
                        "start": 43,
                        "end": 61,
                        "text": "(Cui et al., 2004)",
                        "ref_id": "BIBREF1"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiment Setup",
                "sec_num": "6.1"
            },
            {
                "text": "\u2022 CorME: It is the method proposed in this paper. Different from ApprMatch, ME-based ranking model is implemented to incorporate path correlations which assigns different weights for different paths respectively. Furthermore, phrase mapping score is incorporated into the path correlation measure.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiment Setup",
                "sec_num": "6.1"
            },
            {
                "text": "These methods are briefly described in Section 2. Performance is evaluated with Mean Reciprocal Rank (MRR). Furthermore, we list percentages of questions correctly answered in terms of top 5 answers and top 1 answer returned respectively. No answer validations are used to adjust answers. We further divide the questions into two classes according to whether NER is used in answer extraction. If the expected answer type of a question is unknown, such as \"How did James Dean die?\" or the type cannot be annotated by NER, such as \"What ethnic group/race are Crip members?\", we put the question in Qw/oNE set, otherwise, we put it in QwNE. For the questions in Qw/oNE, we extract all basic noun phrases and verb phrases as candidate answers. Then, answer extraction module has to work on the larger candidate sets. Using a MUC-based NER, the recognized types include person, location, organization, date, time and money. In TREC04 questions, 123 questions are put in QwNE and 80 questions in Qw/oNE. We evaluate the performance on QwNE and Qw/oNE respectively, as shown in Table 2 . The density-based method Density (0.11MRR) loses many questions in Qw/oNE, which indicates that using only surface word information is not sufficient for large candidate answer sets. On the contrary, SynPattern(0.36MRR), Strict-Pattern(0.36MRR), ApprMatch(0.42MRR) and CorME (0.47MRR) which capture syntactic information, perform much better than Density. Our method CorME outperforms the other syntacticbased methods on both QwNE and Qw/oNE. Es-pecially for more difficult questions Qw/oNE, the improvements (up to 31% in MRR) are more obvious. It indicates that our method can be used to further enhance state-of-the-art QA systems even if they have a good NER.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 1077,
                        "end": 1078,
                        "text": "2",
                        "ref_id": "TABREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Experiment Setup",
                "sec_num": "6.1"
            },
            {
                "text": "In addition, we evaluate component contributions of our method based on the main idea of relation path correlation. Three components are tested: 1. Appr. Mapping (Section 3.4). We replace approximate question phrase mapping with exact phrase mapping and withdraw the phrase mapping scores from path correlation measure. 2. Answer Ranking (Section 4). Instead of using ME model, we sum all of the path correlations to rank candidate answers, which is similar to (Cui et al., 2004) . 3. Answer Re-ranking (Section 5). We disable this component and select top 5 answers according to answer ranking scores. The contribution of each component is evaluated with the overall performance degradation after it is removed or replaced. Some findings are concluded from Table 3 . Performances degrade when replacing approximate phrase mapping or ME-based answer ranking, which indicates that both of them have positive effects on the systems. This may be also used to explain why CorME outperforms ApprMatch in Table 1 . However, removing answer re-ranking doesn't affect much. Since short questions, such as \"What does AARP stand for?\", frequently occur in TREC04, exploring the phrase relations for such questions isn't helpful.",
                "cite_spans": [
                    {
                        "start": 461,
                        "end": 479,
                        "text": "(Cui et al., 2004)",
                        "ref_id": "BIBREF1"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 764,
                        "end": 765,
                        "text": "3",
                        "ref_id": "TABREF4"
                    },
                    {
                        "start": 1005,
                        "end": 1006,
                        "text": "1",
                        "ref_id": "TABREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Experiment Setup",
                "sec_num": "6.1"
            },
            {
                "text": "In this paper, we propose a relation path correlation-based method to rank candidate answers in answer extraction. We extract and pair relation paths from questions and candidate sentences. Next, we measure the relation path correlation in each pair based on approximate phrase mapping score and relation sequence alignment, which is calculated by DTW algorithm. Lastly, a ME-based ranking model is proposed to incorporate the path correlations and rank candidate",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "7"
            },
            {
                "text": "answers. The experiment on TREC questions shows that our method significantly outperforms a density-based method by 50% in MRR and three state-of-the-art syntactic-based methods by up to 20% in MRR. Furthermore, the method is especially effective for difficult questions, for which NER may not help. Therefore, it may be used to further enhance state-of-the-art QA systems even if they have a good NER. In the future, we are to further evaluate the method based on the overall performance of a QA system and adapt it to sentence retrieval task.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "7"
            }
        ],
        "back_matter": [],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "A maximum entropy approach to natural language processing",
                "authors": [
                    {
                        "first": "Adam",
                        "middle": [
                            "L"
                        ],
                        "last": "Berger",
                        "suffix": ""
                    },
                    {
                        "first": "Stephen",
                        "middle": [
                            "A"
                        ],
                        "last": "Della Pietra",
                        "suffix": ""
                    },
                    {
                        "first": "Vincent",
                        "middle": [
                            "J"
                        ],
                        "last": "Della",
                        "suffix": ""
                    },
                    {
                        "first": "Pietra",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 1996,
                "venue": "Computational Linguisitics",
                "volume": "22",
                "issue": "",
                "pages": "39--71",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Adam L. Berger, Stephen A. Della Pietra, and Vin- cent J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Compu- tational Linguisitics, 22:39-71.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "National university of singapore at the trec-13 question answering",
                "authors": [
                    {
                        "first": "Hang",
                        "middle": [],
                        "last": "Cui",
                        "suffix": ""
                    },
                    {
                        "first": "Keya",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Renxu",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Tat-Seng",
                        "middle": [],
                        "last": "Chua",
                        "suffix": ""
                    },
                    {
                        "first": "Min-Yen",
                        "middle": [],
                        "last": "Kan",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proceedings of TREC2004",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Hang Cui, Keya Li, Renxu Sun, Tat-Seng Chua, and Min-Yen Kan. 2004. National university of singa- pore at the trec-13 question answering. In Proceed- ings of TREC2004, NIST.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Question answering by searching large corpora with linguistic methods",
                "authors": [
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Kaisser",
                        "suffix": ""
                    },
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Becker",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proceedings of TREC2004",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "M. Kaisser and T. Becker. 2004. Question answering by searching large corpora with linguistic methods. In Proceedings of TREC2004, NIST.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Principar-an efficient, broadcoverage, principle-based parser",
                "authors": [
                    {
                        "first": "Dekang",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    }
                ],
                "year": 1994,
                "venue": "Proceedings of COLING1994",
                "volume": "",
                "issue": "",
                "pages": "42--488",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dekang Lin. 1994. Principar-an efficient, broad- coverage, principle-based parser. In Proceedings of COLING1994, pages 42-488.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Lexical chains for question answering",
                "authors": [
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Moldovan",
                        "suffix": ""
                    },
                    {
                        "first": "Adrian",
                        "middle": [],
                        "last": "Novischi",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Proceedings of COLING",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dan Moldovan and Adrian Novischi. 2002. Lexical chains for question answering. In Proceedings of COLING2002.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Considerations in dynamic time warping algorithms for discrete word recognition",
                "authors": [
                    {
                        "first": "L",
                        "middle": [
                            "R"
                        ],
                        "last": "Rabiner",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [
                            "E"
                        ],
                        "last": "Rosenberg",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [
                            "E"
                        ],
                        "last": "Levinson",
                        "suffix": ""
                    }
                ],
                "year": 1978,
                "venue": "Proceedings of IEEE Transactions on acoustics, speech and signal processing",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "L. R. Rabiner, A. E. Rosenberg, and S. E. Levinson. 1978. Considerations in dynamic time warping al- gorithms for discrete word recognition. In Proceed- ings of IEEE Transactions on acoustics, speech and signal processing.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Statistical qa -classifier vs. re-ranker: What's the difference?",
                "authors": [
                    {
                        "first": "Eduard",
                        "middle": [],
                        "last": "Deepak Ravichandran",
                        "suffix": ""
                    },
                    {
                        "first": "Franz",
                        "middle": [
                            "Josef"
                        ],
                        "last": "Hovy",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Och",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Proceedings of ACL2003 workshop on Multilingual Summarization and Question Answering",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Deepak Ravichandran, Eduard Hovy, and Franz Josef Och. 2003. Statistical qa -classifier vs. re-ranker: What's the difference? In Proceedings of ACL2003 workshop on Multilingual Summarization and Ques- tion Answering.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Exploring syntactic relation patterns for question answering",
                "authors": [
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Shen",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Geert-Jan",
                        "suffix": ""
                    },
                    {
                        "first": "Dietrich",
                        "middle": [],
                        "last": "Kruijff",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Klakow",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of IJCNLP2005",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dan Shen, Geert-Jan M. Kruijff, and Dietrich Klakow. 2005. Exploring syntactic relation patterns for ques- tion answering. In Proceedings of IJCNLP2005.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Combining linguisitic processing and web mining for question answering: Itc-irst at trec-2004",
                "authors": [
                    {
                        "first": "H",
                        "middle": [],
                        "last": "Tanev",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Kouylekov",
                        "suffix": ""
                    },
                    {
                        "first": "B",
                        "middle": [],
                        "last": "Magnini",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "H. Tanev, M. Kouylekov, and B. Magnini. 2004. Com- bining linguisitic processing and web mining for question answering: Itc-irst at trec-2004. In Pro- ceedings of TREC2004, NIST.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "University at albany's ilqua in trec 2005",
                "authors": [
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [
                            "Y"
                        ],
                        "last": "Duan",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Shaikh",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Small",
                        "suffix": ""
                    },
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Strzalkowski",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of TREC2005",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "M. Wu, M. Y. Duan, S. Shaikh, S. Small, and T. Strza- lkowski. 2005. University at albany's ilqua in trec 2005. In Proceedings of TREC2005, NIST.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 1: Relation Paths for sample question and sentence. EAP indicates expected answer position; CA indicates candidate answer",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Figure 2: Paired Relation Path",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "TABREF2": {
                "content": "<table><tr><td/><td/><td colspan=\"2\">: Overall performance</td><td/><td/></tr><tr><td/><td>Density</td><td>SynPattern</td><td>StrictMatch</td><td>ApprMatch</td><td>CorME</td></tr><tr><td>MRR</td><td>0.45</td><td>0.56</td><td>0.57</td><td>0.60</td><td>0.67</td></tr><tr><td>Top1</td><td>0.36</td><td>0.53</td><td>0.49</td><td>0.53</td><td>0.62</td></tr><tr><td>Top5</td><td>0.56</td><td>0.60</td><td>0.67</td><td>0.70</td><td>0.74</td></tr><tr><td>6.2 Results</td><td/><td/><td colspan=\"3\">ME model, the weights are assigned, such as</td></tr><tr><td colspan=\"3\">Table 1 shows the overall performance of the five methods. The main observations from the table are as follows:</td><td colspan=\"3\">5.72 for topic path ; 3.44 for constraints path and 1.76 for target path. 2) CorME incorpo-rates approximate phrase mapping scores into path correlation measure.</td></tr><tr><td colspan=\"3\">1. The methods SynPattern, StrictMatch, Ap-</td><td/><td/><td/></tr><tr><td colspan=\"3\">prMatch and CorME significantly improve</td><td/><td/><td/></tr><tr><td colspan=\"3\">MRR by 25.0%, 26.8%, 34.5% and 50.1%</td><td/><td/><td/></tr><tr><td colspan=\"3\">over the baseline method Density. The im-</td><td/><td/><td/></tr><tr><td colspan=\"3\">provements may benefit from the various ex-</td><td/><td/><td/></tr><tr><td colspan=\"3\">plorations of syntactic relations.</td><td/><td/><td/></tr><tr><td colspan=\"3\">2. The performance of SynPattern (0.56MRR)</td><td/><td/><td/></tr><tr><td colspan=\"3\">and StrictMatch (0.57MRR) are close. Syn-</td><td/><td/><td/></tr><tr><td colspan=\"3\">Pattern matches relation sequences of can-</td><td/><td/><td/></tr><tr><td colspan=\"3\">didate answers with the predefined relation</td><td/><td/><td/></tr><tr><td colspan=\"3\">sequences extracted from a training data</td><td/><td/><td/></tr><tr><td colspan=\"3\">set, while StrictMatch matches relation se-</td><td/><td/><td/></tr><tr><td colspan=\"3\">quences of candidate answers with the cor-</td><td/><td/><td/></tr><tr><td colspan=\"3\">responding relation sequences in questions.</td><td/><td/><td/></tr><tr><td colspan=\"3\">But, both of them are based on the assump-</td><td/><td/><td/></tr><tr><td colspan=\"3\">tion that the more number of same rela-</td><td/><td/><td/></tr><tr><td colspan=\"3\">tions between two sequences, the more sim-</td><td/><td/><td/></tr><tr><td colspan=\"3\">ilar the sequences are. Furthermore, since</td><td/><td/><td/></tr><tr><td colspan=\"3\">most TREC04 questions only have one or two</td><td/><td/><td/></tr><tr><td colspan=\"3\">phrases and many questions have similar ex-</td><td/><td/><td/></tr><tr><td colspan=\"3\">pressions, SynPattern and StrictMatch don't</td><td/><td/><td/></tr><tr><td colspan=\"2\">make essential difference.</td><td/><td/><td/><td/></tr><tr><td colspan=\"3\">3. ApprMatch and CorME outperform SynPat-</td><td/><td/><td/></tr><tr><td colspan=\"3\">tern and StrictMatch by about 6.1% and</td><td/><td/><td/></tr><tr><td colspan=\"3\">18.4% improvement in MRR. Strict matching</td><td/><td/><td/></tr><tr><td colspan=\"3\">often fails due to various relation representa-</td><td/><td/><td/></tr><tr><td colspan=\"3\">tions in syntactic trees. However, such vari-</td><td/><td/><td/></tr><tr><td colspan=\"3\">ations of syntactic relations may be captured</td><td/><td/><td/></tr><tr><td colspan=\"3\">by ApprMatch and CorME using a MI-based</td><td/><td/><td/></tr><tr><td colspan=\"2\">statistical method.</td><td/><td/><td/><td/></tr><tr><td colspan=\"3\">4. CorME achieves the better performance by</td><td/><td/><td/></tr><tr><td colspan=\"3\">11.6% than ApprMatch. The improvement</td><td/><td/><td/></tr><tr><td colspan=\"3\">may benefit from two aspects: 1) ApprMatch</td><td/><td/><td/></tr><tr><td colspan=\"3\">assigns equal weights to the paths of a can-</td><td/><td/><td/></tr><tr><td colspan=\"3\">didate answer and question phrases, while</td><td/><td/><td/></tr><tr><td colspan=\"3\">CorME estimate the weights according to</td><td/><td/><td/></tr><tr><td colspan=\"3\">phrase type and path length. After training a</td><td/><td/><td/></tr></table>",
                "type_str": "table",
                "text": "",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table><tr><td>and Qw/oNE</td><td/><td/></tr><tr><td/><td>QwNE</td><td>Qw/oNE</td></tr><tr><td>Density</td><td>0.66</td><td>0.11</td></tr><tr><td>SynPattern</td><td>0.71</td><td>0.36</td></tr><tr><td>StrictMatch</td><td>0.70</td><td>0.36</td></tr><tr><td>ApprMatch</td><td>0.72</td><td>0.42</td></tr><tr><td>CorME</td><td>0.79</td><td>0.47</td></tr></table>",
                "type_str": "table",
                "text": "Performance on two question sets QwNE",
                "html": null,
                "num": null
            },
            "TABREF4": {
                "content": "<table><tr><td colspan=\"2\">: Component Contributions</td></tr><tr><td/><td>MRR</td></tr><tr><td>Overall</td><td>0.67</td></tr><tr><td>-Appr. Mapping</td><td>0.63</td></tr><tr><td>-Answer Ranking</td><td>0.62</td></tr><tr><td>-Answer Re-ranking</td><td>0.66</td></tr></table>",
                "type_str": "table",
                "text": "",
                "html": null,
                "num": null
            }
        }
    }
}