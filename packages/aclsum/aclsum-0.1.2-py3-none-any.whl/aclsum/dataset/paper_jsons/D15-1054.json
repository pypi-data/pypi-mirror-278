{
    "paper_id": "D15-1054",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:36:26.504034Z"
    },
    "title": "Joint Embedding of Query and Ad by Leveraging Implicit Feedback",
    "authors": [
        {
            "first": "Sungjin",
            "middle": [],
            "last": "Lee",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Yifan",
            "middle": [],
            "last": "Hu",
            "suffix": "",
            "affiliation": {},
            "email": "yifanhu@yahoo-inc.com"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Sponsored search is at the center of a multibillion dollar market established by search technology. Accurate ad click prediction is a key component for this market to function since the pricing mechanism heavily relies on the estimation of click probabilities. Lexical features derived from the text of both the query and ads play a significant role, complementing features based on historical click information. The purpose of this paper is to explore the use of word embedding techniques to generate effective text features that can capture not only lexical similarity between query and ads but also the latent user intents. We identify several potential weaknesses of the plain application of conventional word embedding methodologies for ad click prediction. These observations motivated us to propose a set of novel joint word embedding methods by leveraging implicit click feedback. We verify the effectiveness of these new word embedding models by adding features derived from the new models to the click prediction system of a commercial search engine. Our evaluation results clearly demonstrate the effectiveness of the proposed methods. To the best of our knowledge this work is the first successful application of word embedding techniques for the task of click prediction in sponsored search.",
    "pdf_parse": {
        "paper_id": "D15-1054",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Sponsored search is at the center of a multibillion dollar market established by search technology. Accurate ad click prediction is a key component for this market to function since the pricing mechanism heavily relies on the estimation of click probabilities. Lexical features derived from the text of both the query and ads play a significant role, complementing features based on historical click information. The purpose of this paper is to explore the use of word embedding techniques to generate effective text features that can capture not only lexical similarity between query and ads but also the latent user intents. We identify several potential weaknesses of the plain application of conventional word embedding methodologies for ad click prediction. These observations motivated us to propose a set of novel joint word embedding methods by leveraging implicit click feedback. We verify the effectiveness of these new word embedding models by adding features derived from the new models to the click prediction system of a commercial search engine. Our evaluation results clearly demonstrate the effectiveness of the proposed methods. To the best of our knowledge this work is the first successful application of word embedding techniques for the task of click prediction in sponsored search.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Sponsored search is a multibillion dollar market (Easley and Kleinberg, 2010) that makes most search engine revenue and is one of the most successful ways for advertisers to reach their intended audiences. When search engines deliver results to a user, sponsored advertisement impressions (ads) are shown alongside the organic search results (Figure 1 ). Typically the advertiser pays the search engine based on the pay-per-click model. In this model the advertiser pays only if the impression that accompanies the search results is clicked. The price is usually set by a generalized second-price (GSP) auction (Edelman et al., 2005) that encourages advertisers to bid truthfully. An advertiser wins if the expected revenue for this advertiser, which is the bid Figure 1 : Sponsored ads when \"pizza\" was searched at Yahoo! (www.yahoo.com).",
                "cite_spans": [
                    {
                        "start": 49,
                        "end": 77,
                        "text": "(Easley and Kleinberg, 2010)",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 611,
                        "end": 633,
                        "text": "(Edelman et al., 2005)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 350,
                        "end": 351,
                        "text": "1",
                        "ref_id": null
                    },
                    {
                        "start": 769,
                        "end": 770,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "price times the expected click probability (also know as click through rate, or CTR), is ranked the highest. The price the advertiser pays, known as cost-per-click (CPC), is the bid price for the second ranked advertiser times the ratio of the expected CTR between the second and first ranked advertisers. From this discussion it should be clear that CTR plays a key role in deciding both the ranking and the pricing of the ads. Therefore it is very important to predict CTR accurately.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The state of the art search engine typically uses a machine learning model to predict CTR by exploiting various features that have been found useful in practice. These include historical click performance features such as historical click probability for the query, the ad, the user, and a combination of these; contextual features such as temporal and geographical information; and text-based features such as query keywords or ad title and description. Among these, historical click performance features often have the most predictive power for queries, ads and users that have registered many impressions. For queries, ads and users that have not registered many impressions, however, historical CTR may have too high a variance to be useful. Hillard et al. (2011) observed that the number of impressions and clicks recorded on query-ad pairs have a very long tail: only 61% of queries has greater than three clicks. They also reported a drastic drop in the accuracy of the click prediction model when fewer historical observations are available. Furthermore, fine-grained historical CTR information takes a huge amount of space, which makes it costly to maintain. On the other hand, text features are always readily available, and thus are particularly useful for those cases for which there is insufficient historical information. Multiple researchers, for example (Richardson, 2007; Cheng and Cant\u00fa-Paz, 2010) , reported the usage of text features including simple lexical similarity scores between the query and ads, word or phrase overlaps and the number of overlapping words and characters. Such features rely on the assumption that query-ad overlap is correlated with perceived relevance. While this is true to a certain extent, the use of simple lexical similarity cannot capture semantic information such as synonyms, entities of the same type and strong relationships between entities (e.g. CEO-company, brandmodel, part-of) . Recently a host of studies on word embedding have been conducted; all map words into a vector space such that semantically relevant words are placed near each other in the space (Mikolov et al., 2013a; Pennington et al., 2014; Baroni et al., 2014) . The use of continuous word vectors has been shown to be helpful for a wide range of NLP tasks by better capturing both syntactic and semantic information than simple lexical features (Socher et al., 2012a) .",
                "cite_spans": [
                    {
                        "start": 746,
                        "end": 767,
                        "text": "Hillard et al. (2011)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 1370,
                        "end": 1388,
                        "text": "(Richardson, 2007;",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 1389,
                        "end": 1415,
                        "text": "Cheng and Cant\u00fa-Paz, 2010)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 1904,
                        "end": 1937,
                        "text": "CEO-company, brandmodel, part-of)",
                        "ref_id": null
                    },
                    {
                        "start": 2118,
                        "end": 2141,
                        "text": "(Mikolov et al., 2013a;",
                        "ref_id": null
                    },
                    {
                        "start": 2142,
                        "end": 2166,
                        "text": "Pennington et al., 2014;",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 2167,
                        "end": 2187,
                        "text": "Baroni et al., 2014)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 2373,
                        "end": 2395,
                        "text": "(Socher et al., 2012a)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "No previous research on sponsored search has successfully used word embeddings to generate text features. In this paper, we explore the use of word embeddings for click prediction. However, it is clear that conventional word embeddings (which solely rely on word co-occurrence in a context window) can only offer limited discriminative power because queries and ad text are typically very short. In addition, conventional word embeddings cannot capture user intents, preferences and desires. Wang et al. (2013) showed that specific frequently occurring lexical patterns, e.g., x% off, guaranteed return in x days and official site, are effective in triggering users desires, and thus lead to significant differences in CTR. Conventional word embeddings cannot capture these phenomena since they do not incorporate the implicit feedback users provide through clicks and non-clicks. These observations naturally lead us to leverage click feedback to infuse users' intentions and desires into the vector space.",
                "cite_spans": [
                    {
                        "start": 492,
                        "end": 510,
                        "text": "Wang et al. (2013)",
                        "ref_id": "BIBREF29"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The simplest way to harness click feedback is to train conventional word embedding models on a corpus that only includes clicked impressions, where each \"sentence\" is constructed by mixing the query and ad text. Having trained a word embedding model, we simply take the average of word vectors of the query and ads respectively to obtain sentence (or paragraph) vectors, which in turn are used to compute the similarity scores between the query and ads. Our experiments show that this method does improve click prediction performance. However, this method has several potential weaknesses. First, the use of only clicked impressions ignores the large amount of negative signals contained in the non-clicked ad impressions. Second, the use of indirect signals (word co-occurrences) can be noisy or even harmful to our ultimate goal (accurate click prediction) when it is combined with direct signals (impressions with click feedback). Third, without explicit consideration about the averaging step in the training process of word embedding models, a simple averaging scheme across word vectors may be a suboptimal. We therefore propose several joint word embedding models; all of these aim to put query vectors close to relevant ad vectors by explicitly utilizing both positive and negative click feedback. We evaluate all these models against a large sponsored search data set from a commercial search engine, and demonstrate that our proposed models significantly improve click prediction performance.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The rest of this paper is organized as follows. In Section 2 we present a brief summary of related work. In Section 3 we give some background information on ad click prediction in sponsored search. In Section 4 we describe our methods. In Section 5 we discuss our experiments. We finish with some conclusions and future directions in Section 6.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Text features for predicting click probability There have been many studies on the use of text features for click prediction. For example, Dembczynski et al. (2008) used a decision rule-based approach involving such lexical features as the number of words in ad title and description, the number of segments and length of the ad URL, and individual words and terms in ad title and description. Cheng et al. (2010) used a logistic regression model that used both historical click performance features and simple lexical features such as word or phrase overlap between query and ad title and description. Trofimov et al. (2012) used a variant of boosted decision trees with similar features. Richardson et al. (2007) specifically considered new ads (which lack historical click prediction data) and proposed to use the CTR for ad terms, the frequency of certain unigrams (e.g., dollar signs) and general English usage patterns, and simple lexical distance between the query and ads. In all this previous work, text features consisted only of surface-level text features. To the best of our knowledge, there is no previous work adopting semantic-level text features for the purpose of click prediction, in particular word embeddings to measure query-ad relevance. In a similar vein of research, Grbovic et al. (2015) adopted word embeddings to the task of query rewriting for a better match between queries and keywords that advertisers entered into an auction. Using the embeddings, semantically similar queries are mapped into vectors close in the embedding space, which allows expansion of a query via K-nearest neighbor search. Word embeddings for language processing Recently many NLP systems have obtained improved performance with less human engineering by adopting distributed word representations (Socher et al., 2012a) .",
                "cite_spans": [
                    {
                        "start": 139,
                        "end": 164,
                        "text": "Dembczynski et al. (2008)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 394,
                        "end": 413,
                        "text": "Cheng et al. (2010)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 603,
                        "end": 625,
                        "text": "Trofimov et al. (2012)",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 690,
                        "end": 714,
                        "text": "Richardson et al. (2007)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 1292,
                        "end": 1313,
                        "text": "Grbovic et al. (2015)",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 1803,
                        "end": 1825,
                        "text": "(Socher et al., 2012a)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "In particular, neural word embedding techniques are now known to be effective in capturing syntactic and semantic relationships, and more computationally efficient than many other competitors (Mikolov et al., 2013a; Pennington et al., 2014; Baroni et al., 2014) . On top of word embeddings, a new stream of research on sentence and paragraph embeddings has also emerged to tackle higher level tasks such as sentiment analysis, machine translation and semantic relatedness tasks. Recursive Neural Networks (RNNs) have been used for syntactic parsing (Socher et al., 2012b; Socher et al., 2013; Socher et al., 2014; Irsoy and Cardie, 2014) . Long Short-Term Memory (LSTM) networks have been applied to machine translation (Bahdanau et al., 2014; Sutskever et al., 2014) and semantic processing (Tai et al., 2015) . Interestingly Convolutional Neural Networks (CNNs), widely used for image processing, have recently emerged as a strong class of models for NLP tasks (Kim, 2014; Blunsom et al., 2014) . As apposed to the models above, PhraseVector (Le and Mikolov, 2014) takes a less structured but unsupervised approach by treating a piece of text as a token and performing word embedding-like training with an unlimited context window. None of this previous work exactly fits the click prediction task. Since queries and ads are much less structured than usual text, it is not attractive to use models with complex structures, such as RNNs, at the cost of speed and scalability. PhraseVector is less structured but it does not support compositionality, suffering from sparseness or requiring to train new vectors for each unseen query and ad. Interestingly, as reported in (Tai et al., 2015) , a simple averaging scheme (mean vector) was found to be very competitive to more complex models for high level semantic tasks despite its simplicity. These observations lead us to one of our models that aims to improve the mean vector method by directly optimizing mean vectors instead of word vectors.",
                "cite_spans": [
                    {
                        "start": 192,
                        "end": 215,
                        "text": "(Mikolov et al., 2013a;",
                        "ref_id": null
                    },
                    {
                        "start": 216,
                        "end": 240,
                        "text": "Pennington et al., 2014;",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 241,
                        "end": 261,
                        "text": "Baroni et al., 2014)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 549,
                        "end": 571,
                        "text": "(Socher et al., 2012b;",
                        "ref_id": null
                    },
                    {
                        "start": 572,
                        "end": 592,
                        "text": "Socher et al., 2013;",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 593,
                        "end": 613,
                        "text": "Socher et al., 2014;",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 614,
                        "end": 637,
                        "text": "Irsoy and Cardie, 2014)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 720,
                        "end": 743,
                        "text": "(Bahdanau et al., 2014;",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 744,
                        "end": 767,
                        "text": "Sutskever et al., 2014)",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 792,
                        "end": 810,
                        "text": "(Tai et al., 2015)",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 963,
                        "end": 974,
                        "text": "(Kim, 2014;",
                        "ref_id": null
                    },
                    {
                        "start": 975,
                        "end": 996,
                        "text": "Blunsom et al., 2014)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 1671,
                        "end": 1689,
                        "text": "(Tai et al., 2015)",
                        "ref_id": "BIBREF27"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "Joint embedding to bridge multiple views. Multiple studies have explored the task of bringing multiple views into the same vector space. For example, there is now a large body of research on joint modeling of text and image information (Frome et al., 2013; Karpathy and Fei-Fei, 2014; Socher et al., 2014) . The multimodal embedding space helps find appropriate alignments between image regions and corresponding pieces of text description. Joint embedding has also been applied to question answering (Wang et al., 2014) and semantic understanding (Yang et al., 2014) . In contrast to the tasks above, there is no natural component-wise correspondence between queries and ads; instead the relationship is more implicit and pragmatic. Because of this, our methods rely on global rather than componentlevel signals for model training.",
                "cite_spans": [
                    {
                        "start": 236,
                        "end": 256,
                        "text": "(Frome et al., 2013;",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 257,
                        "end": 284,
                        "text": "Karpathy and Fei-Fei, 2014;",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 285,
                        "end": 305,
                        "text": "Socher et al., 2014)",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 501,
                        "end": 520,
                        "text": "(Wang et al., 2014)",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 548,
                        "end": 567,
                        "text": "(Yang et al., 2014)",
                        "ref_id": "BIBREF31"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "We first present a high-level description of sponsored search. The process consists of several stages. First, given a user query, a list of candidate ads are retrieved, either by exactly matching query terms to the bid terms of the advertiser, or by first using query term expansion to obtain a longer list of matched ads. Some candidate ads may be filtered out based on metrics such as ad quality. Then, a click prediction model scores the candidate ads to estimate how likely it is that each will be clicked. This click probability serves a crucial role both in the user experience and in the revenue for the search engine. The ads with the highest click probabilities are placed in the search results page. The priceper-click for each ad shown is determined based on the click probabilities and the GSP auction.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baseline Click Prediction Model",
                "sec_num": "3"
            },
            {
                "text": "Our baseline click prediction model is formulated as a supervised learning problem. Specifically we use Logistic Regression (LR) since LR is well suited for probability estimation. Given a variety of features, the probability of a click is expressed as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baseline Click Prediction Model",
                "sec_num": "3"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "p(c|q, a, u) = 1 1 + exp i w i f i (q, a, u) ,",
                        "eq_num": "(1)"
                    }
                ],
                "section": "Baseline Click Prediction Model",
                "sec_num": "3"
            },
            {
                "text": "where c \u2208 {1, 0} is the label (1: click or 0: non-click), f i (q, a, u) is the ith feature derived for query-ad-user triple (q, a, u) and w i is the associated weight. The model is trained using a stochastic gradient descent algorithm on a per impression basis with l 1 regularization to avoid overfitting. An accurate LR model relies greatly on the effectiveness of its features. Our baseline model is furnished with a rich set of features that are typically used in commercial search engines. The first feature type is based on the historical CTR of user, query, ad triples (if there is enough historical information on this). We use two groups of features of this type: COEC based features and user factor features. The second feature type is based on query and ad text.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baseline Click Prediction Model",
                "sec_num": "3"
            },
            {
                "text": "Due to the significant decrease of CTR depending on the ad position, it has become common practice to use position-normalized CTR (a.k.a. Clicks Over Expected Clicks):",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baseline Click Prediction Model",
                "sec_num": "3"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "COEC = p c p p i p * CT R p ,",
                        "eq_num": "(2)"
                    }
                ],
                "section": "Baseline Click Prediction Model",
                "sec_num": "3"
            },
            {
                "text": "where the numerator is the total number of clicks received by the configuration of interest; the denominator is the expected clicks (ECs) that an average ad would receive after i p times impressions at position p, and CT R p is the average CTR at position p, calculated over all queries, ads and users. We use userindependent features derived from COEC statistics for specific query-ad pairs. However, many impressions are needed for these statistics to be reliable and therefore data for specific query-ad pairs can be sparse and noisy (only around 70% of queries and about 50% of query-URL, query-bid term pairs have historical CTR).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baseline Click Prediction Model",
                "sec_num": "3"
            },
            {
                "text": "To alleviate this problem, additional COEC statistics over aggregations of queries or ads are also used. The exact description of these aggregations is beyond the scope of this paper, but briefly we exploit the categorization of the ads in ad groups, campaigns, and accounts defined by advertisers. Since it is well known that personalization features are crucial to obtain accurate click prediction models (Cheng and Cant\u00fa-Paz, 2010) , we also use features that measure user factors relating to CTR. The user click feedback features capture the inclination of individual users to click on ads in general. The user-query click feedback features indicate the propensity of users to click for certain queries or groups of queries. Finally user-ad features dictate the user preferences on certain ads or advertisers. However the data sparseness problem becomes even more serious when it comes to user-specific features (we use a threshold of 100 for statistical confidence). For example, only about 5% of user-URL pairs and 1% of user, query, URL triples have historical CTR information. Therefore a set of segment-level features can be extracted as back-off features where users, queries and ads are clustered into groups, and group level historical CTRs are collected.",
                "cite_spans": [
                    {
                        "start": 407,
                        "end": 434,
                        "text": "(Cheng and Cant\u00fa-Paz, 2010)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baseline Click Prediction Model",
                "sec_num": "3"
            },
            {
                "text": "Our second major feature type involves the lexical similarity between query and ad text. These text features assume that users are more likely to click on ads that seem to be relevant to the query, and that perceived relevance is correlated with the degree of queryad overlap. These features include the number of overlapping words and characters in query-ad URL, queryad title, and query-ad description, and the number of words and characters in the query. The discrimination power of simple lexical features is relatively limited because query and ad text are typically very short.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baseline Click Prediction Model",
                "sec_num": "3"
            },
            {
                "text": "Finally, we use other contextual features that are helpful in predicting the click probabilities.; for example, time of day, day of week, and geographic information. To model interactions among features, some features are selected by domain knowledge to be conjoined. All together, our baseline model utilize a comprehensive set of historical CTR, lexical, and contextual features (over a hundred features in total). The fact that this baseline model is highly optimized makes the subsequent performance improvement from our proposed algorithm meaningful. This baseline model is used in production in part of a major search platform.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baseline Click Prediction Model",
                "sec_num": "3"
            },
            {
                "text": "We now describe several methods that jointly embed words in both the query and the ads into the same vector space. In our experiments, we incorporate these methods as features in our click prediction model. We start by defining the notation used in this section. A sponsored search dataset D is a set of tuples for each ad impression (q, t, d, y) where q \u2261 {q j } is a multiword query string, t \u2261 {t k }, d \u2261 {d l } are multiword ad title and description strings, and y is a binary indicator for whether the ad is clicked. We have two choices in defining the vocabulary V from which words are drawn: we can use a unified vocabulary for both query and ads or define a separate vocabulary for each -V \u2261 V q \u2295 V a . In our initial experiments the unified vocabulary constantly yielded better performances, thus we always use the unified vocabulary here. We use bold letters q j , t k , d l to denote the corresponding embedding representations of {q j , t k , d l }. Finally we use W to represent the vocabulary matrix; in W each column is a word vector.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Joint Embedding for Click Prediction",
                "sec_num": "4"
            },
            {
                "text": "Typically word embeddings are learned from a given text corpus through implicit supervision of predicting the current word given a window of its surrounding text or predicting each word in the window of the current word. The former approach is known as continuous bag-of-words (CBOW) and the latter Skip-gram. For simplicity's sake we use negative-sampling for training word embedding models (Mikolov et al., 2013b) . More formally we define the binary conditional probability for a pair of words (v, w):",
                "cite_spans": [
                    {
                        "start": 392,
                        "end": 415,
                        "text": "(Mikolov et al., 2013b)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Exploiting word2vec embedding",
                "sec_num": "4.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "1 p(v, w) = 1 1 + exp(-v T w) ,",
                        "eq_num": "(3)"
                    }
                ],
                "section": "Exploiting word2vec embedding",
                "sec_num": "4.1"
            },
            {
                "text": "The CBOW algorithm learns word embeddings by minimizing the following logloss of each impression i with regard to W :",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Exploiting word2vec embedding",
                "sec_num": "4.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "CB i (W ) = -log p(w i , \u00b5 C ) - v\u2208N (wi) log (1 -p(v, \u00b5 C )) ,",
                        "eq_num": "(4)"
                    }
                ],
                "section": "Exploiting word2vec embedding",
                "sec_num": "4.1"
            },
            {
                "text": "where the context C of a word w i comes from a window of size k around the word in a sentence of n words w 1 ,. . .,w n : C = w i-k ,. . .,w i-1 ,w i+1 ,. . .,w i+k . \u00b5 C is the averaged context vector of w i ; \u00b5 C =1 |C| v\u2208C v. N (w i ) is the set of negative examples which is drawn according to the unigram distribution of the corpus raised to the 3/4th power. Similarly to (Mikolov et al., 2013b) , we adopt a dynamic window size -for each word the actual window size is sampled uniformly from 1, . . . , k.",
                "cite_spans": [
                    {
                        "start": 377,
                        "end": 400,
                        "text": "(Mikolov et al., 2013b)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Exploiting word2vec embedding",
                "sec_num": "4.1"
            },
            {
                "text": "Similarly the Skip-gram algorithm minimizes the logloss of each impression i with regard to W :",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Exploiting word2vec embedding",
                "sec_num": "4.1"
            },
            {
                "text": "SK i (W ) = - v\u2208C log p(w i , v) - v\u2208N (wi) log (1 -p(w i , v)) .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Exploiting word2vec embedding",
                "sec_num": "4.1"
            },
            {
                "text": "(5)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Exploiting word2vec embedding",
                "sec_num": "4.1"
            },
            {
                "text": "In our first word embedding model that incorporates click feedback, we construct a corpus by taking only clicked impressions from D and then mixing (q, t, d) of each impression into a sentence. Then we simply train CBOW and Skip-gram models on the corpus.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Exploiting word2vec embedding",
                "sec_num": "4.1"
            },
            {
                "text": "Although the CBOW and Skip-gram models trained on a specially constructed corpus can capture signals from both click feedback and word co-occurrence, they have a couple of drawbacks. First, by ingesting only clicked impressions we \"waste\" the large amount of negative signals contained in the non-clicked impressions. Second, the incorporation of indirect signals such as word co-occurrences can be rather harmful for achieving accurate click prediction as these are very noisy compared to direct signals such as click feedback.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Joint word embedding using click feedback",
                "sec_num": "4.2"
            },
            {
                "text": "For our second word embedding model that incorporates click feedback, we define a joint word embedding model that minimizes the following weighted logloss:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Joint word embedding using click feedback",
                "sec_num": "4.2"
            },
            {
                "text": "JW i (W ) = \u03b7(y i ) l(y i , q i , t i ) + l(y i , q i , d i ) , (6)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Joint word embedding using click feedback",
                "sec_num": "4.2"
            },
            {
                "text": "where the component loss function l(\u2022, \u2022, \u2022) is defined as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Joint word embedding using click feedback",
                "sec_num": "4.2"
            },
            {
                "text": "l(y, a, b) = |a| k |b| l -y log p(a k , b l ) -(1 -y) log (1 -p(a k , b l )) .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Joint word embedding using click feedback",
                "sec_num": "4.2"
            },
            {
                "text": "(7) Here \u03b7(y i ) is a function that returns a small weight \u03b7 only to negative examples:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Joint word embedding using click feedback",
                "sec_num": "4.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "\u03b7(y i ) = \u03b7, if y i = 0, 1, otherwise.",
                        "eq_num": "(8)"
                    }
                ],
                "section": "Joint word embedding using click feedback",
                "sec_num": "4.2"
            },
            {
                "text": "The fact that the user did not click on an ad does not necessarily mean that the ad is not what the user wanted; it often means that the ad is just less favored than the clicked ads (Rendle et al., 2009) . Since the scope of this work is restricted to estimating click probability on a per-impression basis, we adopt a weighting scheme rather than optimizing rank-based loss over a set of related impressions.",
                "cite_spans": [
                    {
                        "start": 182,
                        "end": 203,
                        "text": "(Rendle et al., 2009)",
                        "ref_id": "BIBREF20"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Joint word embedding using click feedback",
                "sec_num": "4.2"
            },
            {
                "text": "The joint word embedding models defined in the previous sections do not define how to aggregate a variable length sequence of word vectors into a sentence (or paragraph) vector to facilitate the computation of sentence-level similarity scores. One approach to this aggregation task is mean vector: simply average the word-level embeddings across the sentence or paragraph. As noted by (Tai et al., 2015) , this approach is a strong competitor to more complex models such as RNNs or LSTMs despite its simple composition method. However, this method may generate suboptimal sentence vectors. With weight logloss, we aim to optimize sentence vectors instead of individual word vectors:",
                "cite_spans": [
                    {
                        "start": 385,
                        "end": 403,
                        "text": "(Tai et al., 2015)",
                        "ref_id": "BIBREF27"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Joint mean vector optimization",
                "sec_num": "4.3"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "JM i (W ) = \u03b7(y i ) l(y i , q i , t i ) + l(y i , q i , d i ) ,",
                        "eq_num": "(9)"
                    }
                ],
                "section": "Joint mean vector optimization",
                "sec_num": "4.3"
            },
            {
                "text": "where the component loss function l(\u2022, \u2022, \u2022) is defined as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Joint mean vector optimization",
                "sec_num": "4.3"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "l(y, a, b) = -y log p(\u00b5 a , \u00b5 b ) -(1 -y) log (1 -p(\u00b5 a , \u00b5 b )) ,",
                        "eq_num": "(10)"
                    }
                ],
                "section": "Joint mean vector optimization",
                "sec_num": "4.3"
            },
            {
                "text": "where \u00b5 s returns the average vector for the multiword string s, i.e.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Joint mean vector optimization",
                "sec_num": "4.3"
            },
            {
                "text": "\u00b5 s = 1 |s| |s| k s k .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Joint mean vector optimization",
                "sec_num": "4.3"
            },
            {
                "text": "Data The data used in our experiments were collected from a random bucket of the Yahoo! sponsored search traffic logs for a period of 4 weeks in October 2014.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "5"
            },
            {
                "text": "In the data there are approximately 65 million unique users, 150 million unique queries and 12 million unique ads. There are approximately 985 million ad impression events in total. We split the data into 3 partitions with respect to time: the first 14 days' data are used for training word embedding models, the next 7 days' data for training click prediction models, and the last 7 days' data for testing. Table 1 presents more detailed statistics for the data. Models We are interested in evaluating the usefulness of different word embedding models as features for click prediction, and already have a very good baseline system for this task. Consequently, in all experiments below, we used the same personalized historical CTRs, contextual and lexical features as the baseline system described in Section 3. We tested models with the following additional features derived from word embeddings:",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 414,
                        "end": 415,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "5"
            },
            {
                "text": "1. cosine similarity between the mean vectors of the query and ad title 2. cosine similarity between the mean vectors of the query and ad description 3. sum of 1 and 2 4. sigmoid function value for the dot product of the mean vectors of the query and ad title 5. sigmoid function value for the dot product of the query and ad description 6. sum of 4 and 5",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "5"
            },
            {
                "text": "All these continuous features are quantized into 50 bins. We compared five different word embedding algorithms:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "5"
            },
            {
                "text": "1. Skip-gram trained on Wikipedia (SK-WIKI) We used the skip-gram mode of word2vec 2 with a window size of 5 and negative sampling to train the SK-WIKI model. For all other models we used in-house implementation which employs AdagradRDA (Duchi et al., 2011) to minimize the loss functions introduced in Section 4. The dimension of a word vector is set to 100 for all algorithms 3 . We removed a set of prefixed stop-words and all words occurring fewer than 100 times; the resulting vocabulary comprised 126K unique words. To process our web-scale data, we implemented a multithread program where each thread randomly traverses over a partition of the data D to compute gradients and update the matrix W stored in a shared memory. For computational efficiency, the hogwild lock-free approach (Recht et al., 2011 ) is used. We set \u03b7 in Eq. 8 and Eq. 9 to 0.2 through grid search based on two-fold cross-validation. This small value indeed verifies the idea of weak negative feedback for unclicked impressions.",
                "cite_spans": [
                    {
                        "start": 237,
                        "end": 257,
                        "text": "(Duchi et al., 2011)",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 791,
                        "end": 810,
                        "text": "(Recht et al., 2011",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "5"
            },
            {
                "text": "In order to circumvent the severe influence of ad position on the click prediction model, only the impressions that are placed at the top position were used for training the click prediction model. Note that CTR at other positions can be derived from that of the ad at the top position through scaling. Dembczynski et al. (2008) showed that CTR can be decomposed as a product of the probability of an ad getting clicked given its being seen and the probability of an ad being seen at a particular position.",
                "cite_spans": [
                    {
                        "start": 303,
                        "end": 328,
                        "text": "Dembczynski et al. (2008)",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "5"
            },
            {
                "text": "We ordered query-ad pairs by the predicted score to compute AucLoss (i.e. 1 -AUC where AUC is the area under the Receiver Operating Characteristic (ROC) curve). The ROC AUC is known to have a correlation with the quality of ranking by the predicted score (Fawcett, 2006) ; thus is one of the most important 2 Available at https://code.google.com/p/word2vec/ 3 Higher vector dimensions such as 200 were also tried but did not give a significant improvement. Our experimental results (Table 2 ) show that the use of features derived from our proposed word embedding models significantly reduce AucLoss by up to 4.1%. For commercial search engines which have a very strong baseline AucLoss, a reduction of 1% can be considered large (McMahan et al., 2013) . Moreover, as expected, the more issues (as identified in Section 4) an algorithm addresses, the better performance it achieves.",
                "cite_spans": [
                    {
                        "start": 255,
                        "end": 270,
                        "text": "(Fawcett, 2006)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 730,
                        "end": 752,
                        "text": "(McMahan et al., 2013)",
                        "ref_id": "BIBREF15"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 489,
                        "end": 490,
                        "text": "2",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Results",
                "sec_num": null
            },
            {
                "text": "From the comparison between the SK-WIKI model and the rest, we can recognize the importance of word embedding models specialized to domain text and supervision signals. Also the difference between the CB-CI (SK-CI) model and the JIWV model indicates the noisiness of indirect signals such as word co-occurrence compared to direct signals like click feedback. Finally the gap between the JIWV and JWV models highlights the significance of considering compositionality in the word embedding training process.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": null
            },
            {
                "text": "Eyeballing the most similar words to several queries in the vector space is often helpful for getting some sense about how different methods influence the resulting vector spaces. that incorporate click feedback find more words related to products, services or websites instead of just conceptuatlly related words. Given that real products or services can be regarded as the best possible surrogates to user intents and desires, this demonstrates the effectiveness of our methods. This tendency gets stronger as a method takes into account both positive and negative click feedback.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": null
            },
            {
                "text": "Another very interesting observation comes from the fact that none of the methods except JMV successfully captures the composite meaning of \"metal watch\"; they tend to either find related words separately for each query word (e.g. \"watch\" is strongly associated to the sense of watching something like movie or other types of video) or find totally unrelated words (particularly SK-WIKI). This demonstrates that it is crucial to address compositionality in the very process of learning word vectors.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": null
            },
            {
                "text": "Table 4 shows the top 20 most similar words to the query \"princess costumes.\" In this example we can spot another surprising result. The JMV model pushes a lot of price related expressions to the top4 . This may imply that many parents search for lower cost costumes, clearly showing a clear psychological desire in the financial dimension. This observation confirms the findings in (Wang et al., 2013) about the significant role of certain ad expressions in triggering users' psychological desires. We also note that the CB-CI model returns a lot of misspells for \"costume(s)\", which would not be possible with simple lexical features of the baseline system. A close look at this example generally confirms the observations we made for the previous ex- ample. Finally Table 5 shows top the 20 most similar words for the query \"game for kids.\" Once again we found the same analysis holds for this case.",
                "cite_spans": [
                    {
                        "start": 383,
                        "end": 402,
                        "text": "(Wang et al., 2013)",
                        "ref_id": "BIBREF29"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "4",
                        "ref_id": "TABREF4"
                    },
                    {
                        "start": 775,
                        "end": 776,
                        "text": "5",
                        "ref_id": "TABREF6"
                    }
                ],
                "eq_spans": [],
                "section": "Results",
                "sec_num": null
            },
            {
                "text": "In this paper we explored the use of word embedding techniques to overcome the shortcomings of traditional lexical features for ad click prediction in sponsored search. We identified several potential weaknesses of the plain application of conventional word embedding methodologies: the lack of the right machinery to harness both positive and negative click feedback, the limited utility of pure word co-occurrence signals, and no consideration of vector composition in the word embedding training process. We proposed a set of new implicit feedback-based joint word embedding methods to address those issues. We evaluated the new word embedding methods in the context of a very good baseline click prediction system, on a large scale data set collected from Yahoo! search engine logs. Our experimental results clearly demonstrate the effectiveness of the proposed methods. We also presented several examples for qualitative analysis to advance our understanding on how each algorithm really contributes to the improved performance. To the best of our knowledge this work is the first successful application of word embedding techniques for the sponsored search task.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusions",
                "sec_num": "6"
            },
            {
                "text": "There are multiple interesting research directions for future work. One of these directions is to extend the vocabulary by identifying significant phrases (as well as words) before training word vectors. Hillard et al. (2011) employed Conditional Random Fields to divide queries with multiple words into segments and collected historical CTR on the segment level. We also like to investigate more structured embedding methods such as RNNs (probably for ad descriptions). In case the computational cost of such methods are too high to be practical for sponsored search, we can employ them only for a small fraction of ads filtered by faster methods.",
                "cite_spans": [
                    {
                        "start": 204,
                        "end": 225,
                        "text": "Hillard et al. (2011)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusions",
                "sec_num": "6"
            },
            {
                "text": "It may be possible to deal with the implicit negative feedback of unclicked ad impressions in a more principled way by adopting ranking-based loss functions. However, this is only possible with the extra cost of identifying and aggregating related ads into a single transaction.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusions",
                "sec_num": "6"
            },
            {
                "text": "Though not directly related to NLP, yet another promising direction is to jointly embed not only text data but also a variety of user activities (e.g., organic search results, mobile app usages, other daily activities) all together in the same vector space. Since many of the different sources contain their own unique information, we might be able to obtain a much better understanding about the user state and intent through this rich joint embedding space. Joint embedding with rich information can also help us to perform automatic clustering of users, eventually leading to powerful smoothing methods for personalized historical CTR statistics.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusions",
                "sec_num": "6"
            },
            {
                "text": "We use only a single vector for a word unlike(Mikolov et al.,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "2013b) where two vectors (\"input\" and \"output\") for a word are used. This halves the required space to store vectors without performance loss.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Joint individual word vector embedding (JIWV), see Section 4.2",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "We have not tried any normalization for numbers but it might be worth doing given the important role they play.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Neural machine translation by jointly learning to align and translate",
                "authors": [
                    {
                        "first": "Dzmitry",
                        "middle": [],
                        "last": "Bahdanau",
                        "suffix": ""
                    },
                    {
                        "first": "Kyunghyun",
                        "middle": [],
                        "last": "Cho",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1409.0473"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Dont count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors",
                "authors": [
                    {
                        "first": "Marco",
                        "middle": [],
                        "last": "Baroni",
                        "suffix": ""
                    },
                    {
                        "first": "Georgiana",
                        "middle": [],
                        "last": "Dinu",
                        "suffix": ""
                    },
                    {
                        "first": "Germ\u00e1n",
                        "middle": [],
                        "last": "Kruszewski",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics",
                "volume": "1",
                "issue": "",
                "pages": "238--247",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Marco Baroni, Georgiana Dinu, and Germ\u00e1n Kruszewski. 2014. Dont count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, volume 1, pages 238-247.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "A convolutional neural network for modelling sentences",
                "authors": [
                    {
                        "first": "Phil",
                        "middle": [],
                        "last": "Blunsom",
                        "suffix": ""
                    },
                    {
                        "first": "Edward",
                        "middle": [],
                        "last": "Grefenstette",
                        "suffix": ""
                    },
                    {
                        "first": "Nal",
                        "middle": [],
                        "last": "Kalchbrenner",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Phil Blunsom, Edward Grefenstette, Nal Kalchbrenner, et al. 2014. A convolutional neural network for modelling sentences. In Proceedings of the 52nd An- nual Meeting of the Association for Computational Linguistics. Proceedings of the 52nd Annual Meet- ing of the Association for Computational Linguis- tics.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Personalized click prediction in sponsored search",
                "authors": [
                    {
                        "first": "Haibin",
                        "middle": [],
                        "last": "Cheng",
                        "suffix": ""
                    },
                    {
                        "first": "Erick",
                        "middle": [],
                        "last": "Cant\u00fa-Paz",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the Third ACM International Conference on Web Search and Data Mining, WSDM '10",
                "volume": "",
                "issue": "",
                "pages": "351--360",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Haibin Cheng and Erick Cant\u00fa-Paz. 2010. Person- alized click prediction in sponsored search. In Pro- ceedings of the Third ACM International Conference on Web Search and Data Mining, WSDM '10, pages 351-360, New York, NY, USA. ACM.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Predicting ads click-through rate with decision rules",
                "authors": [
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Dembczynski",
                        "suffix": ""
                    },
                    {
                        "first": "W",
                        "middle": [],
                        "last": "Kotlowski",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Weiss",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Proceedings of WWW 08",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "K. Dembczynski, W.Kotlowski, and D.Weiss. 2008. Predicting ads click-through rate with decision rules. In Proceedings of WWW 08.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization",
                "authors": [
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Duchi",
                        "suffix": ""
                    },
                    {
                        "first": "Elad",
                        "middle": [],
                        "last": "Hazan",
                        "suffix": ""
                    },
                    {
                        "first": "Yoram",
                        "middle": [],
                        "last": "Singer",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "The Journal of Machine Learning Research",
                "volume": "12",
                "issue": "",
                "pages": "2121--2159",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. The Journal of Ma- chine Learning Research, 12:2121-2159.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Networks, Crowds, and Markets: Reasoning about a Highly Connected World",
                "authors": [
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Easley",
                        "suffix": ""
                    },
                    {
                        "first": "Jon",
                        "middle": [],
                        "last": "Kleinberg",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "David Easley and Jon Kleinberg. 2010. Networks, Crowds, and Markets: Reasoning about a Highly Connected World. Cambridge University Press.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Internet advertising and the generalized second price auction: Selling billions of dollars worth of keywords",
                "authors": [
                    {
                        "first": "Benjamin",
                        "middle": [],
                        "last": "Edelman",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Ostrovsky",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Schwarz",
                        "suffix": ""
                    },
                    {
                        "first": "Thank",
                        "middle": [],
                        "last": "Drew Fudenberg",
                        "suffix": ""
                    },
                    {
                        "first": "Louis",
                        "middle": [],
                        "last": "Kaplow",
                        "suffix": ""
                    },
                    {
                        "first": "Robin",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Paul",
                        "middle": [],
                        "last": "Milgrom",
                        "suffix": ""
                    },
                    {
                        "first": "Muriel",
                        "middle": [],
                        "last": "Niederle",
                        "suffix": ""
                    },
                    {
                        "first": "Ariel",
                        "middle": [],
                        "last": "Pakes",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "American Economic Review",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Benjamin Edelman, Michael Ostrovsky, Michael Schwarz, Thank Drew Fudenberg, Louis Kaplow, Robin Lee, Paul Milgrom, Muriel Niederle, and Ariel Pakes. 2005. Internet advertising and the generalized second price auction: Selling billions of dollars worth of keywords. American Economic Re- view, 97.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "An introduction to ROC analysis",
                "authors": [
                    {
                        "first": "Tom",
                        "middle": [],
                        "last": "Fawcett",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Pattern Recognition Letters",
                "volume": "27",
                "issue": "8",
                "pages": "861--874",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tom Fawcett. 2006. An introduction to ROC analysis. Pattern Recognition Letters, 27(8):861-874, June.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Devise: A deep visual-semantic embedding model",
                "authors": [
                    {
                        "first": "Andrea",
                        "middle": [],
                        "last": "Frome",
                        "suffix": ""
                    },
                    {
                        "first": "Greg",
                        "middle": [
                            "S"
                        ],
                        "last": "Corrado",
                        "suffix": ""
                    },
                    {
                        "first": "Jon",
                        "middle": [],
                        "last": "Shlens",
                        "suffix": ""
                    },
                    {
                        "first": "Samy",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    },
                    {
                        "first": "Jeff",
                        "middle": [],
                        "last": "Dean",
                        "suffix": ""
                    },
                    {
                        "first": "Tomas",
                        "middle": [],
                        "last": "Mikolov",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "2121--2129",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Tomas Mikolov, et al. 2013. Devise: A deep visual-semantic embedding model. In Advances in Neural Information Processing Sys- tems, pages 2121-2129.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Context-and content-aware embeddings for query rewriting in sponsored search",
                "authors": [
                    {
                        "first": "Mihajlo",
                        "middle": [],
                        "last": "Grbovic",
                        "suffix": ""
                    },
                    {
                        "first": "Nemanja",
                        "middle": [],
                        "last": "Djuric",
                        "suffix": ""
                    },
                    {
                        "first": "Vladan",
                        "middle": [],
                        "last": "Radosavljevic",
                        "suffix": ""
                    },
                    {
                        "first": "Fabrizio",
                        "middle": [],
                        "last": "Silvestri",
                        "suffix": ""
                    },
                    {
                        "first": "Narayan",
                        "middle": [],
                        "last": "Bhamidipati",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '15",
                "volume": "",
                "issue": "",
                "pages": "383--392",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mihajlo Grbovic, Nemanja Djuric, Vladan Radosavl- jevic, Fabrizio Silvestri, and Narayan Bhamidipati. 2015. Context-and content-aware embeddings for query rewriting in sponsored search. In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '15, pages 383-392, New York, NY, USA. ACM.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "The sum of its parts: reducing sparsity in click estimation with query segments",
                "authors": [
                    {
                        "first": "Dustin",
                        "middle": [],
                        "last": "Hillard",
                        "suffix": ""
                    },
                    {
                        "first": "Eren",
                        "middle": [],
                        "last": "Manavoglu",
                        "suffix": ""
                    },
                    {
                        "first": "Hema",
                        "middle": [],
                        "last": "Raghavan",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Leggetter",
                        "suffix": ""
                    },
                    {
                        "first": "Erick",
                        "middle": [],
                        "last": "Cant\u00fa-Paz",
                        "suffix": ""
                    },
                    {
                        "first": "Rukmini",
                        "middle": [],
                        "last": "Iyer",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Inf. Retr",
                "volume": "14",
                "issue": "3",
                "pages": "315--336",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dustin Hillard, Eren Manavoglu, Hema Raghavan, Chris Leggetter, Erick Cant\u00fa-Paz, and Rukmini Iyer. 2011. The sum of its parts: reducing sparsity in click estimation with query segments. Inf. Retr., 14(3):315-336.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Deep recursive neural networks for compositionality in language",
                "authors": [
                    {
                        "first": "Ozan",
                        "middle": [],
                        "last": "Irsoy",
                        "suffix": ""
                    },
                    {
                        "first": "Claire",
                        "middle": [],
                        "last": "Cardie",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "2096--2104",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ozan Irsoy and Claire Cardie. 2014. Deep recursive neural networks for compositionality in language. In Advances in Neural Information Processing Sys- tems, pages 2096-2104.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Deep visualsemantic alignments for generating image descriptions",
                "authors": [
                    {
                        "first": "Andrej",
                        "middle": [],
                        "last": "Karpathy",
                        "suffix": ""
                    },
                    {
                        "first": "Li",
                        "middle": [],
                        "last": "Fei-Fei",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Convolutional neural networks for sentence classification",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1412.2306"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Andrej Karpathy and Li Fei-Fei. 2014. Deep visual- semantic alignments for generating image descrip- tions. arXiv preprint arXiv:1412.2306. Yoon Kim. 2014. Convolutional neural net- works for sentence classification. arXiv preprint arXiv:1408.5882.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Distributed representations of sentences and documents",
                "authors": [
                    {
                        "first": "V",
                        "middle": [],
                        "last": "Quoc",
                        "suffix": ""
                    },
                    {
                        "first": "Tomas",
                        "middle": [],
                        "last": "Le",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Mikolov",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1405.4053"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Quoc V Le and Tomas Mikolov. 2014. Distributed representations of sentences and documents. arXiv preprint arXiv:1405.4053.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Ad Click Prediction: a View from the Trenches",
                "authors": [
                    {
                        "first": "H",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Brendan",
                        "middle": [],
                        "last": "Mcmahan",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Golovin",
                        "suffix": ""
                    },
                    {
                        "first": "Sharat",
                        "middle": [],
                        "last": "Chikkerur",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Martin",
                        "middle": [],
                        "last": "Wattenberg",
                        "suffix": ""
                    },
                    {
                        "first": "Arnar",
                        "middle": [],
                        "last": "Mar Hrafnkelsson",
                        "suffix": ""
                    },
                    {
                        "first": "Tom",
                        "middle": [],
                        "last": "Boulos",
                        "suffix": ""
                    },
                    {
                        "first": "Jeremy",
                        "middle": [],
                        "last": "Kubica",
                        "suffix": ""
                    },
                    {
                        "first": "Gary",
                        "middle": [],
                        "last": "Holt",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Sculley",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Young",
                        "suffix": ""
                    },
                    {
                        "first": "Dietmar",
                        "middle": [],
                        "last": "Ebner",
                        "suffix": ""
                    },
                    {
                        "first": "Julian",
                        "middle": [],
                        "last": "Grady",
                        "suffix": ""
                    },
                    {
                        "first": "Lan",
                        "middle": [],
                        "last": "Nie",
                        "suffix": ""
                    },
                    {
                        "first": "Todd",
                        "middle": [],
                        "last": "Phillips",
                        "suffix": ""
                    },
                    {
                        "first": "Eugene",
                        "middle": [],
                        "last": "Davydov",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining -KDD '13",
                "volume": "1222",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "H. Brendan McMahan, Daniel Golovin, Sharat Chikkerur, Dan Liu, Martin Wattenberg, Arnar Mar Hrafnkelsson, Tom Boulos, Jeremy Kubica, Gary Holt, D. Sculley, Michael Young, Dietmar Ebner, Julian Grady, Lan Nie, Todd Phillips, and Eugene Davydov. 2013. Ad Click Prediction: a View from the Trenches. In Proceedings of the 19th ACM SIGKDD international conference on Knowl- edge discovery and data mining -KDD '13, page 1222, New York, New York, USA, August. ACM Press.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Efficient estimation of word representations in vector space",
                "authors": [
                    {
                        "first": "Tomas",
                        "middle": [],
                        "last": "Mikolov",
                        "suffix": ""
                    },
                    {
                        "first": "Kai",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Greg",
                        "middle": [],
                        "last": "Corrado",
                        "suffix": ""
                    },
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Dean",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of Workshop at ICLR",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word represen- tations in vector space. In Proceedings of Workshop at ICLR.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Distributed representations of words and phrases and their compositionality",
                "authors": [
                    {
                        "first": "Tomas",
                        "middle": [],
                        "last": "Mikolov",
                        "suffix": ""
                    },
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    },
                    {
                        "first": "Kai",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Greg",
                        "middle": [
                            "S"
                        ],
                        "last": "Corrado",
                        "suffix": ""
                    },
                    {
                        "first": "Jeff",
                        "middle": [],
                        "last": "Dean",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "3111--3119",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor- rado, and Jeff Dean. 2013b. Distributed representa- tions of words and phrases and their compositional- ity. In Advances in Neural Information Processing Systems, pages 3111-3119.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Glove: Global vectors for word representation",
                "authors": [
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Pennington",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of the Empiricial Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word representation. Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014), 12.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Hogwild: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent",
                "authors": [
                    {
                        "first": "Benjamin",
                        "middle": [],
                        "last": "Recht",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [],
                        "last": "Re",
                        "suffix": ""
                    },
                    {
                        "first": "Stephen",
                        "middle": [],
                        "last": "Wright",
                        "suffix": ""
                    },
                    {
                        "first": "Feng",
                        "middle": [],
                        "last": "Niu",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "693--701",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. 2011. Hogwild: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent. In Ad- vances in Neural Information Processing Systems, pages 693-701.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Bpr: Bayesian personalized ranking from implicit feedback",
                "authors": [
                    {
                        "first": "Steffen",
                        "middle": [],
                        "last": "Rendle",
                        "suffix": ""
                    },
                    {
                        "first": "Christoph",
                        "middle": [],
                        "last": "Freudenthaler",
                        "suffix": ""
                    },
                    {
                        "first": "Zeno",
                        "middle": [],
                        "last": "Gantner",
                        "suffix": ""
                    },
                    {
                        "first": "Lars",
                        "middle": [],
                        "last": "Schmidt-Thieme",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI '09",
                "volume": "",
                "issue": "",
                "pages": "452--461",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Steffen Rendle, Christoph Freudenthaler, Zeno Gant- ner, and Lars Schmidt-Thieme. 2009. Bpr: Bayesian personalized ranking from implicit feed- back. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI '09, pages 452-461, Arlington, Virginia, United States. AUAI Press.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Predicting clicks: Estimating the click-through rate for new ads",
                "authors": [
                    {
                        "first": "Matthew",
                        "middle": [],
                        "last": "Richardson",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Proceedings of the 16th International World Wide Web Conference (WWW-07",
                "volume": "",
                "issue": "",
                "pages": "521--530",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Matthew Richardson. 2007. Predicting clicks: Esti- mating the click-through rate for new ads. In In Pro- ceedings of the 16th International World Wide Web Conference (WWW-07, pages 521-530. ACM Press.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Deep learning for nlp (without magic)",
                "authors": [
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Tutorial Abstracts of ACL 2012, ACL '12",
                "volume": "",
                "issue": "",
                "pages": "5--5",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Richard Socher, Yoshua Bengio, and Christopher D. Manning. 2012a. Deep learning for nlp (without magic). In Tutorial Abstracts of ACL 2012, ACL '12, pages 5-5, Stroudsburg, PA, USA. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Semantic compositionality through recursive matrix-vector spaces",
                "authors": [
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    },
                    {
                        "first": "Brody",
                        "middle": [],
                        "last": "Huval",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [
                            "Y"
                        ],
                        "last": "Ng",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning",
                "volume": "",
                "issue": "",
                "pages": "1201--1211",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Richard Socher, Brody Huval, Christopher D Manning, and Andrew Y Ng. 2012b. Semantic composition- ality through recursive matrix-vector spaces. In Pro- ceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Com- putational Natural Language Learning, pages 1201- 1211. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
                "authors": [
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    },
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Perelygin",
                        "suffix": ""
                    },
                    {
                        "first": "Jean",
                        "middle": [
                            "Y"
                        ],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Chuang",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [
                            "Y"
                        ],
                        "last": "Ng",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [],
                        "last": "Potts",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of the conference on empirical methods in natural language processing",
                "volume": "1631",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment tree- bank. In Proceedings of the conference on empirical methods in natural language processing (EMNLP), volume 1631, page 1642.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Grounded compositional semantics for finding and describing images with sentences",
                "authors": [
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    },
                    {
                        "first": "Andrej",
                        "middle": [],
                        "last": "Karpathy",
                        "suffix": ""
                    },
                    {
                        "first": "V",
                        "middle": [],
                        "last": "Quoc",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Le",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [
                            "Y"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Ng",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Transactions of the Association for Computational Linguistics",
                "volume": "2",
                "issue": "",
                "pages": "207--218",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Richard Socher, Andrej Karpathy, Quoc V Le, Christo- pher D Manning, and Andrew Y Ng. 2014. Grounded compositional semantics for finding and describing images with sentences. Transactions of the Association for Computational Linguistics, 2:207-218.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Sequence to sequence learning with neural networks",
                "authors": [
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    },
                    {
                        "first": "Oriol",
                        "middle": [],
                        "last": "Vinyals",
                        "suffix": ""
                    },
                    {
                        "first": "Quoc Vv",
                        "middle": [],
                        "last": "Le",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "3104--3112",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Sys- tems, pages 3104-3112.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Improved semantic representations from tree-structured long short-term memory networks",
                "authors": [
                    {
                        "first": "Kai",
                        "middle": [],
                        "last": "Sheng",
                        "suffix": ""
                    },
                    {
                        "first": "Tai",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1503.00075"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Kai Sheng Tai, Richard Socher, and Christopher D Manning. 2015. Improved semantic representa- tions from tree-structured long short-term memory networks. arXiv preprint arXiv:1503.00075.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Using boosted trees for click-through rate prediction for sponsored search",
                "authors": [
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Trofimov",
                        "suffix": ""
                    },
                    {
                        "first": "Anna",
                        "middle": [],
                        "last": "Kornetova",
                        "suffix": ""
                    },
                    {
                        "first": "Valery",
                        "middle": [],
                        "last": "Topinskiy",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Proceedings of the Sixth International Workshop on Data Mining for Online Advertising and Internet Economy, ADKDD '12",
                "volume": "2",
                "issue": "",
                "pages": "1--2",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ilya Trofimov, Anna Kornetova, and Valery Topinskiy. 2012. Using boosted trees for click-through rate pre- diction for sponsored search. In Proceedings of the Sixth International Workshop on Data Mining for Online Advertising and Internet Economy, ADKDD '12, pages 2:1-2:6, New York, NY, USA. ACM.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Psychological advertising: exploring user psychology for click prediction in sponsored search",
                "authors": [
                    {
                        "first": "Taifeng",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Jiang",
                        "middle": [],
                        "last": "Bian",
                        "suffix": ""
                    },
                    {
                        "first": "Shusen",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Yuyu",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Tie-Yan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining",
                "volume": "",
                "issue": "",
                "pages": "563--571",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Taifeng Wang, Jiang Bian, Shusen Liu, Yuyu Zhang, and Tie-Yan Liu. 2013. Psychological advertis- ing: exploring user psychology for click prediction in sponsored search. In Proceedings of the 19th ACM SIGKDD international conference on Knowl- edge discovery and data mining, pages 563-571. ACM.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "Knowledge graph and text jointly embedding",
                "authors": [
                    {
                        "first": "Zhen",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Jianwen",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Jianlin",
                        "middle": [],
                        "last": "Feng",
                        "suffix": ""
                    },
                    {
                        "first": "Zheng",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of the Empiricial Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. 2014. Knowledge graph and text jointly em- bedding. In Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014).",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "Joint relational embeddings for knowledge-based question answering",
                "authors": [
                    {
                        "first": "Min-Chul",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Nan",
                        "middle": [],
                        "last": "Duan",
                        "suffix": ""
                    },
                    {
                        "first": "Ming",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Hae-Chang",
                        "middle": [],
                        "last": "Rim",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "volume": "",
                "issue": "",
                "pages": "645--650",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Min-Chul Yang, Nan Duan, Ming Zhou, and Hae- Chang Rim. 2014. Joint relational embeddings for knowledge-based question answering. In Proceed- ings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 645-650.",
                "links": null
            }
        },
        "ref_entries": {
            "TABREF1": {
                "content": "<table><tr><td/><td>AucLoss Reduction (%)</td></tr><tr><td>Baseline + SK-WIKI</td><td>0.530</td></tr><tr><td>Baseline + SK-CI</td><td>0.595</td></tr><tr><td>Baseline + CB-CI</td><td>0.656</td></tr><tr><td>Baseline + JIWV</td><td>2.276</td></tr><tr><td>Baseline + JMV</td><td>4.114</td></tr></table>",
                "type_str": "table",
                "text": "metrics for click prediction(McMahan et al., 2013). Comparative evaluation results in AucLoss reduction from the baseline system",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table><tr><td>SK-WIKI</td><td>CB-CI</td><td>JIWV</td><td>JMV</td></tr><tr><td>watch (0.733)</td><td>metal (0.718)</td><td>wwhl (0.711)</td><td>tacticalwatch.com (0.701)</td></tr><tr><td>grind (0.687)</td><td>previews (0.652)</td><td>cbs (0.704)</td><td>watchrepairsusa.com (0.695)</td></tr><tr><td>grease (0.682)</td><td>yidio.com (0.648)</td><td>station (0.668)</td><td>omegas (0.689)</td></tr><tr><td>kites (0.676)</td><td>episodes (0.633)</td><td>putlocker.com (0.662)</td><td>watchco.com (0.682)</td></tr><tr><td>hammer (0.675)</td><td>steel (0.626)</td><td>iwatch (0.659)</td><td>wach (0.670)</td></tr><tr><td>spinning (0.672)</td><td>whatch (0.615)</td><td>kidizoom (0.658)</td><td>station (0.656)</td></tr><tr><td>flashing (0.671)</td><td>bobcometal.com (0.586)</td><td>freesports360.com (0.658)</td><td>shockwarehouse.com (0.628)</td></tr><tr><td>trash (0.670)</td><td>ridiculousness (0.582)</td><td>tedtalks (0.655)</td><td>freesports360.com (0.618)</td></tr><tr><td>flame (0.669)</td><td>www.abc.com (0.574)</td><td>11/10c (0.650)</td><td>akribos (0.616)</td></tr><tr><td>flaming (0.665)</td><td>a&amp;e (0.573)</td><td>movie2k (0.640)</td><td>18mm (0.616)</td></tr><tr><td>cigar (0.664)</td><td>utube (0.570)</td><td>nfl.com/now (0.637)</td><td>narutoget.com (0.614)</td></tr><tr><td>home-made (0.663)</td><td>instantly (0.565)</td><td>criminalsgonewilddvd (0.631)</td><td>watchstation.com (0.611)</td></tr><tr><td>glow (0.662)</td><td>khoobsurat (0.562)</td><td>espnnfllive.com (0.631)</td><td>authenticwatches.com (0.610)</td></tr><tr><td>bouncing (0.662)</td><td>outnumbered (0.561)</td><td>foxsports1 (0.623)</td><td>interrupted (0.603)</td></tr><tr><td>filler (0.662)</td><td>itv (0.559)</td><td>viooz (0.623)</td><td>criminalsgonewilddvd (0.601)</td></tr><tr><td>smoke (0.660)</td><td>premiere (0.556)</td><td>bubble (0.623)</td><td>whatch (0.600)</td></tr><tr><td>shoot (0.658)</td><td>films (0.555)</td><td>wewood (0.621)</td><td>$109.99 (0.597)</td></tr><tr><td>scoop (0.653)</td><td>stainless (0.554)</td><td>westclox (0.617)</td><td>tirebuyer.com (0.596)</td></tr><tr><td>noises (0.652)</td><td>fabricators (0.550)</td><td>potlocker (0.613)</td><td>skagen.com (0.588)</td></tr><tr><td>rocking (0.651)</td><td>fabrication (0.550)</td><td>nickelodeon (0.613)</td><td>wewood (0.584)</td></tr></table>",
                "type_str": "table",
                "text": "Table 3 lists the top 20 most similar words to the query \"metal watch.\" The top words for the SK-WIKI model are not semantically interesting in terms of capturing the intent or desires. This clearly shows the limitation of word embedding methods that only rely on indirect signals (i.e. word co-occurrences) from a generic text corpus (e.g. Wikipedia) for sponsored search click prediction. Noticeably the methods",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table><tr><td>SK-WIKI</td><td>CB-CI</td><td>JIWV</td><td>JMV</td></tr><tr><td>costumes (0.762)</td><td>princess (0.833)</td><td>costumes (0.831)</td><td>new-costumes.com (0.815)</td></tr><tr><td>bride (0.723)</td><td>costumed (0.814)</td><td>wonder (0.784)</td><td>coustume (0.808)</td></tr><tr><td>princesses (0.722)</td><td>customes (0.808)</td><td>sweetiegames.com (0.782)</td><td>namefully.com (0.800)</td></tr><tr><td>serene (0.708)</td><td>costume (0.806)</td><td>cleopatra (0.753)</td><td>$35.90 (0.789)</td></tr><tr><td>highness (0.676)</td><td>costomes (0.806)</td><td>new-costumes.com (0.752)</td><td>2-days (0.781)</td></tr><tr><td>bess (0.674)</td><td>costimes (0.803)</td><td>girls.simple (0.749)</td><td>$36.90 (0.776)</td></tr><tr><td>princess]] (0.671)</td><td>m.buycostumes.com (0.800)</td><td>werewolf (0.749)</td><td>costume (0.766)</td></tr><tr><td>attire (0.670)</td><td>custumes (0.796)</td><td>yoshi (0.747)</td><td>$28.90 (0.764)</td></tr><tr><td>princess (0.662)</td><td>officialprincesscostumes.com (0.792)</td><td>leia (0.747)</td><td>princess (0.758)</td></tr><tr><td>dresses (0.662)</td><td>custume (0.789)</td><td>merida (0.742)</td><td>cistumes (0.756)</td></tr><tr><td>highness (0.658)</td><td>coustome (0.789)</td><td>$49.90 (0.735)</td><td>spider-woman (0.756)</td></tr><tr><td>jewels (0.652)</td><td>cosyumes (0.787)</td><td>low-budget (0.727)</td><td>the-wristband-factory.com (0.750)</td></tr><tr><td>wedding (0.651)</td><td>coustume (0.786)</td><td>babies (0.727)</td><td>$17.90 (0.750)</td></tr><tr><td>robes (0.648)</td><td>coustums (0.786)</td><td>fembot (0.726)</td><td>sugarsmascotcostumes\u010bom (0.748)</td></tr><tr><td>prince (0.644)</td><td>buycostumes.com (0.785)</td><td>costums (0.722)</td><td>cotumes (0.748)</td></tr><tr><td>clothes (0.644)</td><td>codtume (0.784)</td><td>$3.90 (0.721)</td><td>coneheads (0.747)</td></tr><tr><td>dancing (0.644)</td><td>leia (0.784)</td><td>hermione (0.718)</td><td>$23.90 (0.739)</td></tr><tr><td>sophie (0.640)</td><td>coustumes (0.784)</td><td>supergirl (0.715)</td><td>$7.90 (0.739)</td></tr><tr><td>consorts (0.639)</td><td>costums (0.783)</td><td>toothless (0.713)</td><td>costomes (0.738)</td></tr><tr><td>glamorous (0.639)</td><td>coatumes (0.782)</td><td>starlord (0.709)</td><td>$19.90 (0.737)</td></tr></table>",
                "type_str": "table",
                "text": "Top 20 most similar words to \"metal watch\"",
                "html": null,
                "num": null
            },
            "TABREF4": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Top 20 most similar words to \"princess costumes\"",
                "html": null,
                "num": null
            },
            "TABREF6": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Top 20 most similar words to \"game for kids\"",
                "html": null,
                "num": null
            }
        }
    }
}