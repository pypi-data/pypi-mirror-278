{
    "paper_id": "D18-1133",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:16:45.785034Z"
    },
    "title": "Semantic Linking in Convolutional Neural Networks for Answer Sentence Selection",
    "authors": [
        {
            "first": "Massimo",
            "middle": [],
            "last": "Nicosia",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Trento",
                "location": {
                    "postCode": "38123",
                    "settlement": "Povo, Manhattan Beach",
                    "region": "(TN), CA",
                    "country": "Italy Amazon, USA"
                }
            },
            "email": "m.nicosia@gmail.com"
        },
        {
            "first": "Alessandro",
            "middle": [],
            "last": "Moschitti",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Trento",
                "location": {
                    "postCode": "38123",
                    "settlement": "Povo, Manhattan Beach",
                    "region": "(TN), CA",
                    "country": "Italy Amazon, USA"
                }
            },
            "email": ""
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "State-of-the-art networks that model relations between two pieces of text often use complex architectures and attention. In this paper, instead of focusing on architecture engineering, we take advantage of small amounts of labelled data that model semantic phenomena in text to encode matching features directly in the word representations. This greatly boosts the accuracy of our reference network, while keeping the model simple and fast to train. Our approach also beats a tree kernel model that uses similar input encodings, and neural models which use advanced attention and compare-aggregate mechanisms.",
    "pdf_parse": {
        "paper_id": "D18-1133",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "State-of-the-art networks that model relations between two pieces of text often use complex architectures and attention. In this paper, instead of focusing on architecture engineering, we take advantage of small amounts of labelled data that model semantic phenomena in text to encode matching features directly in the word representations. This greatly boosts the accuracy of our reference network, while keeping the model simple and fast to train. Our approach also beats a tree kernel model that uses similar input encodings, and neural models which use advanced attention and compare-aggregate mechanisms.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Modeling a match between pieces of text is at the core of many NLP tasks. Recently, manual feature engineering methods have been shadowed by neural network approaches. These networks model the interaction of two pieces of text, or word-toword interactions across sentences, using sophisticated attention mechanisms (Wang et al., 2016a; Santos et al., 2016) and compare-aggregate frameworks (He and Lin, 2016; Wang et al., 2017) .",
                "cite_spans": [
                    {
                        "start": 315,
                        "end": 335,
                        "text": "(Wang et al., 2016a;",
                        "ref_id": null
                    },
                    {
                        "start": 336,
                        "end": 356,
                        "text": "Santos et al., 2016)",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 390,
                        "end": 408,
                        "text": "(He and Lin, 2016;",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 409,
                        "end": 427,
                        "text": "Wang et al., 2017)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Architectural complexity is tied to longer training times 1 . Meaningful features may take long time to emerge by only leveraging word representations and the training data of the task at hand. This is especially problematic with little data, as it often happens in question answering (QA) tasks, e.g., answer sentence selection (Wang et al., 2007; Yang et al., 2015) . Thus, effective word representations are crucial in neural network models to get state-of-the-art performance.",
                "cite_spans": [
                    {
                        "start": 329,
                        "end": 348,
                        "text": "(Wang et al., 2007;",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 349,
                        "end": 367,
                        "text": "Yang et al., 2015)",
                        "ref_id": "BIBREF32"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this work, we try to answer the following research questions: (i) in addition to lexical links, can we incorporate higher-level semantic links between the words in a question and a candidate answer passage, and (ii) can we show that such information has an impact on the quality of our model, and also allows us to keep the architecture simple?",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "We show that modeling semantic relations improves the performance of a neural network for answer sentence selection with (i) a little number of semantic annotations, and (ii) a little increase in training time w.r.t. more complex architecture.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Traditional work on QA makes heavily use of syntactic and semantic features (Hickl et al., 2007; Ferrucci et al., 2010) . A different direction consists in using structural kernels on text encoded as trees (Severyn and Moschitti, 2012; Severyn et al., 2013a,b; Tymoshenko et al., 2014; Tymoshenko and Moschitti, 2015) . Recently, deep learning methods have been very successful in NLP tasks. Words and sentences are mapped into low dimensional representations using convolutional (Krizhevsky et al., 2012) and recurrent networks (Schuster and Paliwal, 1997) , and then adoperated for classification. Complex networks for such a task include attentive networks and compare-aggregate networks.",
                "cite_spans": [
                    {
                        "start": 76,
                        "end": 96,
                        "text": "(Hickl et al., 2007;",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 97,
                        "end": 119,
                        "text": "Ferrucci et al., 2010)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 206,
                        "end": 235,
                        "text": "(Severyn and Moschitti, 2012;",
                        "ref_id": null
                    },
                    {
                        "start": 236,
                        "end": 260,
                        "text": "Severyn et al., 2013a,b;",
                        "ref_id": null
                    },
                    {
                        "start": 261,
                        "end": 285,
                        "text": "Tymoshenko et al., 2014;",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 286,
                        "end": 317,
                        "text": "Tymoshenko and Moschitti, 2015)",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 480,
                        "end": 505,
                        "text": "(Krizhevsky et al., 2012)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 529,
                        "end": 557,
                        "text": "(Schuster and Paliwal, 1997)",
                        "ref_id": "BIBREF20"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "Attentive Networks (Bahdanau et al., 2015; Parikh et al., 2016; Yin et al., 2016) build a sentence representation by also considering the other sentence, weighting the contribution of its parts with the so-called attention mechanism.",
                "cite_spans": [
                    {
                        "start": 19,
                        "end": 42,
                        "text": "(Bahdanau et al., 2015;",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 43,
                        "end": 63,
                        "text": "Parikh et al., 2016;",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 64,
                        "end": 81,
                        "text": "Yin et al., 2016)",
                        "ref_id": "BIBREF34"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "Compare-Aggregate Networks (Wang and Jiang, 2017) apply several decompositions to each sentence in a pair. The resulting vectors are compared or composed with multiple functions, and possibly some attention mechanisms. All the intermediate results are then aggregated into a fixed size vector to quantify the final match.",
                "cite_spans": [
                    {
                        "start": 27,
                        "end": 49,
                        "text": "(Wang and Jiang, 2017)",
                        "ref_id": "BIBREF28"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "In this work, we take some elements of the traditional QA research, i.e., semantic features, and use them to model relationships between sentence pairs, in the context of a neural network, which is less complex than attentive and compareaggregate counterparts.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "Question Analysis is an important part of a QA system (Lally et al., 2012) and can give us syntactic and semantic clues that greatly help in scoring answer passages, and in identifying the final answer. Leveraging a relatively small number of annotated examples, we can automatically extract question properties that may be exploited by a QA model to increase the accuracy of its answers. We use classifiers to extract the question category and the question focus. Question Category. Questions can be broadly classified into categories according to a given taxonomy. When the category is indicative of the answer type, the latter can be furtherly characterized by the Lexical Answer Type (LAT), which according to Lally et al. (2012) is a word or noun phrase in the question that specifies the type of the answer without any attempt to understand its semantics. Question Focus. In the literature there are multiple definitions of question focus. According to Ferrucci et al. (2010) , the focus is the question part that substituted with the answer, renders the question a stand-alone statement. According to Bunescu and Huang (2010) , the focus is the \"set of all maximal noun phrases in the question that corefer with the answer\". Their definition allows a question to have multiple focuses or an implicit focus. Additionally, it is more tied to the LAT and indeed the focus can be used to infer the answer type. We adopt such definition since we build our question focus identifier using the annotated data they provide. Note that we do not consider multiword or implicit focus.",
                "cite_spans": [
                    {
                        "start": 54,
                        "end": 74,
                        "text": "(Lally et al., 2012)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 714,
                        "end": 733,
                        "text": "Lally et al. (2012)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 959,
                        "end": 981,
                        "text": "Ferrucci et al. (2010)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 1108,
                        "end": 1132,
                        "text": "Bunescu and Huang (2010)",
                        "ref_id": "BIBREF2"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Question Analysis",
                "sec_num": "3"
            },
            {
                "text": "Given a query or question q and a candidate answer passage a, the task of answer selection can be defined as learning a function f (q, a) that outputs a relevancy probability s \u2208 [0, 1]. Multiple answers associated with a question are sorted in descending order by the score s. A good an-swer selection system places the highest number of correct answers at the top of a candidate answer list. In this paper, we use convolutional neural networks, referred to as CNNs (Kim, 2014; Kalchbrenner et al., 2014) , to (i) classify a question into a category, (ii) identify the focus word in a question, and (iii) build a question and answer representations for QA.",
                "cite_spans": [
                    {
                        "start": 467,
                        "end": 478,
                        "text": "(Kim, 2014;",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 479,
                        "end": 505,
                        "text": "Kalchbrenner et al., 2014)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Answer Sentence Selection with CNNs",
                "sec_num": "4"
            },
            {
                "text": "A sentence s of length n is a sequence of words (w 1 , ..., w n ), which are drawn from a vocabulary V . Each word is encoded with an integer id from 1 to |V |, and then represented as a vector, w \u2208 R d , looked up into an embedding matrix, E \u2208 R d\u00d7|V | . The matrix E is obtained by concatenating all the embeddings of the words in V . The id 0 is used for padding and it is mapped to the zero vector. The i th column in E corresponds to the word with integer id i to facilitate the lookup.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Sentence Matrix Encoding",
                "sec_num": "4.1"
            },
            {
                "text": "We use CNNs for question analysis. The question category network applies convolutions of different width and then pooling on the question. The results are concatenated and fed to a multilayer perceptron (MLP) that outputs a probability distribution over the possible categories seen during training. The question focus network applies convolutions that operate on windows centered on each question word. Therefore, the input and output resolutions are the same. We stack a number of convolutions to increase the receptive field. Every output vector from the last convolution of the stack is passed through an MLP, which produces a scalar value. All those values are normalized across each sentence with a softmax, to form a probability distribution over the sentence tokens.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Question Analysis Networks",
                "sec_num": "4.2"
            },
            {
                "text": "Our neural model is based on the Severyn and Moschitti (2015, 2016) model (S&M from now on), showed in Figure 1 . This model is simple, fast and well studied. It has also been reproduced in other work (Rao et al., 2017; Chen et al., 2017; Sequiera et al., 2017) .",
                "cite_spans": [
                    {
                        "start": 201,
                        "end": 219,
                        "text": "(Rao et al., 2017;",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 220,
                        "end": 238,
                        "text": "Chen et al., 2017;",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 239,
                        "end": 261,
                        "text": "Sequiera et al., 2017)",
                        "ref_id": "BIBREF21"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 110,
                        "end": 111,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Answer Sentence Selection Network",
                "sec_num": "4.3"
            },
            {
                "text": "The S&M model embeds the question and answer passage and operates independent convolutional and max-pooling layers on each. A bilinear transformation (Bordes et al., 2014) produces a similarity value x sim for the pair. The similarity, the encoded question and passage, and a vec- tor of real valued features x f eat are concatenated in the join layer. The latter is fed to a hidden layer with a non-linearity, and the final softmax layer outputs the matching probability. The word vectors of the question and the answer are augmented with an additional feature, which is embedded in a small dimensional space. This feature signals if a word appears in both the question and answer. We found that the real valued features and the similarity matrix do not increase the network accuracy and we removed them from our model. This finding is consistent with recent reproduction papers by Rao et al. (2017) ; Sequiera et al. (2017) .",
                "cite_spans": [
                    {
                        "start": 150,
                        "end": 171,
                        "text": "(Bordes et al., 2014)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 883,
                        "end": 900,
                        "text": "Rao et al. (2017)",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 903,
                        "end": 925,
                        "text": "Sequiera et al. (2017)",
                        "ref_id": "BIBREF21"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Answer Sentence Selection Network",
                "sec_num": "4.3"
            },
            {
                "text": "We propose to add semantic features to the sentence matrix to establish links between words that go beyond lexical matching. Figure 2 describes our network. The key addition to the S&M model is the semantic overlap vector. Each word is therefore represented by concatenating three vectors: the word embedding vector, a feature embedding vector which can represent two values -if a word is contained or not in both question and answerand the semantic overlap embedding vector. The semantic vector w so , with dimensionality s, embeds a feature so which can assume C + 1 values, if we consider the C question classes plus a nomatch value. Each feature value is looked up into an embedding matrix W so \u2208 R s\u00d7|C|+1 . Analogously, the word overlap binary feature is looked up into an embedding matrix W wo \u2208 R r\u00d7|2| . The final word representation will be the concatenation of all these vectors: w = [w; w wo ; w so ].",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 132,
                        "end": 133,
                        "text": "2",
                        "ref_id": "FIGREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Our QA Network with Semantic Overlap",
                "sec_num": "4.4"
            },
            {
                "text": "Here we describe how the semantic word overlap feature is computed. For each question we collect the output of our question analysis CNNs. The question focus CNN determines which word in the question is the focus. The question category CNN assigns a class to the question. After that, each word is associated with a semantic overlap feature so (which will eventually be embedded using W wo ) according to the following strategy:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Our QA Network with Semantic Overlap",
                "sec_num": "4.4"
            },
            {
                "text": "1. for each word in the question which is not the question focus so is equal to 0. For the question focus word so is equal to the id of the question category (the question focus and category are output by our CNN classifiers);",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Our QA Network with Semantic Overlap",
                "sec_num": "4.4"
            },
            {
                "text": "2. for each answer word so is equal to 0, with the exeception of words covered by named entities (NEs), for which so is equal to the id of the question category that is compatible with their entity type, according to the mapping in Table 1 .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 238,
                        "end": 239,
                        "text": "1",
                        "ref_id": "TABREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Our QA Network with Semantic Overlap",
                "sec_num": "4.4"
            },
            {
                "text": "The W wo and W so matrices are parameters of the model, and they are learned during training. The question category and question focus annotations for the QA datasets are produced by our neural network classifiers. The NEs are obtained with an off-the-shelf processor2 , trained on OntoNotes (Weischedel et al., 2012) .",
                "cite_spans": [
                    {
                        "start": 292,
                        "end": 317,
                        "text": "(Weischedel et al., 2012)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Our QA Network with Semantic Overlap",
                "sec_num": "4.4"
            },
            {
                "text": "Here we describe how we train our networks for question analysis and then we present the answer sentence selection experiments. More details about preprocessing, training and hyperparameter choice can be found in the appendix.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Results",
                "sec_num": "5"
            },
            {
                "text": "Dataset. The CNN question classifier is trained on the UIUC dataset (Li and Roth, 2006) . We use the 6 coarse classes to train the classifier. The semantic overlap vectors of the question focus word boss, and the answer word claire are the same, because the latter is an entity of type Person. The question has HUM category. Ignoring stopwords, the word boss appears in the question and the answer, and this is reflected in the word overlap embedding space.",
                "cite_spans": [
                    {
                        "start": 68,
                        "end": 87,
                        "text": "(Li and Roth, 2006)",
                        "ref_id": "BIBREF14"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Question Classification",
                "sec_num": "5.1"
            },
            {
                "text": "Results. The classifier has accuracy of 91.2% on the UIUC test set. Our goal is to annotate new questions with reasonable accuracy. Since the model convergences well, we annotate the questions in the QA datasets after training on the UIUC data, and select the best model on the test data.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Question Classification",
                "sec_num": "5.1"
            },
            {
                "text": "Dataset. The CNN focus identifier is trained on the dataset from Bunescu and Huang (2010) , which contains the first 2,000 UIUC questions annotated with focus information. After removing the questions with implicit and multi-focus, we end up with 1,030 questions.",
                "cite_spans": [
                    {
                        "start": 65,
                        "end": 89,
                        "text": "Bunescu and Huang (2010)",
                        "ref_id": "BIBREF2"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Question Focus Identification",
                "sec_num": "5.2"
            },
            {
                "text": "Results. The cross-validation accuracy of the classifier is 92.3%. After convergence, we annotate the focus words in the QA datasets.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Question Focus Identification",
                "sec_num": "5.2"
            },
            {
                "text": "Dataset. We test our model on TrecQA (Wang et al., 2007) , one of the most popular benchmarks for answer selection. The dataset contains factoid questions and candidate answer sentences. We use the same splits of the original data, but we run our experiments using the larger provided training set (TRAIN-ALL). This is noisier data, which, on the other hand, gives us more examples for training. We remove from the dev. and test sets questions without answers, and questions with training instances with difficult negative examples. Our system beats several others that use word alignments and attention mechanisms. The better systems employ expensive bidirectional networks, sophisticated attention mechanisms, and extract multiple views of questions and answers for comparing and aggregating them.",
                "cite_spans": [
                    {
                        "start": 37,
                        "end": 56,
                        "text": "(Wang et al., 2007)",
                        "ref_id": "BIBREF27"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "TrecQA",
                "sec_num": "5.3"
            },
            {
                "text": "Dataset. TrecQA and its test set are small, so results may be unstable. In addition, lexical overlap between questions and answer candidates is high (Yih et al., 2013) . This means that simple lexical similarity features are highly discriminative. Therefore, we also experiment with Wik-iQA (Yang et al., 2015) , which is an order of magnitude larger than TrecQA. We use the Yin et al. (2016) experimental setting.",
                "cite_spans": [
                    {
                        "start": 149,
                        "end": 167,
                        "text": "(Yih et al., 2013)",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 291,
                        "end": 310,
                        "text": "(Yang et al., 2015)",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 375,
                        "end": 392,
                        "text": "Yin et al. (2016)",
                        "ref_id": "BIBREF34"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WikiQA",
                "sec_num": "5.4"
            },
            {
                "text": "Results. 2016) tree-kernel model by 1 MAP point. This model includes relational information in terms of question focus, entities and question categories too, but uses additional syntactic information (i.e., syntactic trees).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WikiQA",
                "sec_num": "5.4"
            },
            {
                "text": "Our network is able to make better use of the provided semantic clues. Surprisingly, CNN W O+SO also achieves higher MAP than the model by Wang et al. (2017) , which is a state-of-the-art complex approach mixing attention and interaction factors of multiple sentence perspectives.",
                "cite_spans": [
                    {
                        "start": 139,
                        "end": 157,
                        "text": "Wang et al. (2017)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WikiQA",
                "sec_num": "5.4"
            },
            {
                "text": "The results with the CNN W O+SO model suggest that the semantic overlap vectors are an effective way of linking questions and answers. This is especially true, given the results on WikiQA, where the questions and answers have little lexical overlap. With the additional semantic information, the CNN is able to better model the relevancy of candidate passages. It also surpasses the accuracy of more complex systems, which have higher training time. The annotation networks (which can be trained only once) and the answer selection networks take little time to train: from 10 to 20 minutes in total, depending on the number of question/answer pairs. CNNs are faster at training and inference time with respect to RNNs, especially when the latter incorporate attention mechanisms, which increase the number of computations. We argue that annotating a relatively small number of examples with semantic information, could be time well spent to increase model accuracy, without increasing its architectural complexity. We would like to add that we also experimented with RNNs (LSTM and GRU) in place of the CNN sentence model. Such encoders easily overfitted, requiring careful regularization, and did not yield better results for us.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discussion",
                "sec_num": "5.5"
            },
            {
                "text": "In this paper, we presented a neural network that models semantic links between questions and answers, in addition to lexical links. The annotations for establishing such links are produced by a set of fast neural components for question analysis, trained on publicly available datasets. The evaluation on two QA datasets shows that our approach can achieve state-of-the-art performance using a simple CNN, leading to a low complexity and training time. Our approach is an interesting first step towards a future architecture, in which we will jointly optimize the semantic annotators and the answer sentence selection model, in an end-to-end fashion. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion and Future Work",
                "sec_num": "6"
            },
            {
                "text": "In the future, we will also train the NE recognizer.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "The first author was supported by the Google Europe Doctoral Fellowship Award 2015. Many thanks to the anonymous reviewers, the Area and PC Chairs for their valuable work.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgments",
                "sec_num": null
            },
            {
                "text": "* Now at Google \u2020 This work was partially carried out when the author was at the University of Trento 1 http://dawn.cs.stanford.edu/ benchmark/",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "funding",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Neural machine translation by jointly learning to align and translate",
                "authors": [
                    {
                        "first": "Dzmitry",
                        "middle": [],
                        "last": "Bahdanau",
                        "suffix": ""
                    },
                    {
                        "first": "Kyunghyun",
                        "middle": [],
                        "last": "Cho",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2015. Neural machine translation by jointly learning to align and translate. ICLR.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Open question answering with weakly supervised embedding models",
                "authors": [
                    {
                        "first": "Antoine",
                        "middle": [],
                        "last": "Bordes",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Weston",
                        "suffix": ""
                    },
                    {
                        "first": "Nicolas",
                        "middle": [],
                        "last": "Usunier",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases",
                "volume": "8724",
                "issue": "",
                "pages": "165--180",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Antoine Bordes, Jason Weston, and Nicolas Usunier. 2014. Open question answering with weakly su- pervised embedding models. In Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases -Volume 8724, ECML PKDD 2014, pages 165-180, New York, NY, USA. Springer-Verlag New York, Inc.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Towards a general model of answer typing: Question focus identification",
                "authors": [
                    {
                        "first": "Razvan",
                        "middle": [],
                        "last": "Bunescu",
                        "suffix": ""
                    },
                    {
                        "first": "Yunfeng",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "CICLing",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Razvan Bunescu and Yunfeng Huang. 2010. Towards a general model of answer typing: Question focus identification. In CICLing.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "On the benefit of incorporating external features in a neural architecture for answer sentence selection",
                "authors": [
                    {
                        "first": "Ruey-Cheng",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Evi",
                        "middle": [],
                        "last": "Yulianti",
                        "suffix": ""
                    },
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Sanderson",
                        "suffix": ""
                    },
                    {
                        "first": "W",
                        "middle": [
                            "Bruce"
                        ],
                        "last": "Croft",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '17",
                "volume": "",
                "issue": "",
                "pages": "1017--1020",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ruey-Cheng Chen, Evi Yulianti, Mark Sanderson, and W. Bruce Croft. 2017. On the benefit of incorporat- ing external features in a neural architecture for an- swer sentence selection. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '17, pages 1017-1020, New York, NY, USA. ACM.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Building watson: An overview of the deepqa project",
                "authors": [
                    {
                        "first": "David",
                        "middle": [
                            "A"
                        ],
                        "last": "Ferrucci",
                        "suffix": ""
                    },
                    {
                        "first": "Eric",
                        "middle": [
                            "W"
                        ],
                        "last": "Brown",
                        "suffix": ""
                    },
                    {
                        "first": "Jennifer",
                        "middle": [],
                        "last": "Chu-Carroll",
                        "suffix": ""
                    },
                    {
                        "first": "James",
                        "middle": [],
                        "last": "Fan",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Gondek",
                        "suffix": ""
                    },
                    {
                        "first": "Aditya",
                        "middle": [],
                        "last": "Kalyanpur",
                        "suffix": ""
                    },
                    {
                        "first": "Adam",
                        "middle": [],
                        "last": "Lally",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [
                            "William"
                        ],
                        "last": "Murdock",
                        "suffix": ""
                    },
                    {
                        "first": "Eric",
                        "middle": [],
                        "last": "Nyberg",
                        "suffix": ""
                    },
                    {
                        "first": "John",
                        "middle": [
                            "M"
                        ],
                        "last": "Prager",
                        "suffix": ""
                    },
                    {
                        "first": "Nico",
                        "middle": [],
                        "last": "Schlaefer",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "A"
                        ],
                        "last": "Welty",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "AI Magazine",
                "volume": "31",
                "issue": "3",
                "pages": "59--79",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "David A. Ferrucci, Eric W. Brown, Jennifer Chu- Carroll, James Fan, David Gondek, Aditya Kalyan- pur, Adam Lally, J. William Murdock, Eric Nyberg, John M. Prager, Nico Schlaefer, and Christopher A. Welty. 2010. Building watson: An overview of the deepqa project. AI Magazine, 31(3):59-79.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "An enhanced convolutional neural network model for answer selection",
                "authors": [
                    {
                        "first": "Jiahui",
                        "middle": [],
                        "last": "Guo",
                        "suffix": ""
                    },
                    {
                        "first": "Bin",
                        "middle": [],
                        "last": "Yue",
                        "suffix": ""
                    },
                    {
                        "first": "Guandong",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Zhenglu",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Jin-Mao",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the 26th International Conference on World Wide Web Companion",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jiahui Guo, Bin Yue, Guandong Xu, Zhenglu Yang, and Jin-Mao Wei. 2017. An enhanced convolu- tional neural network model for answer selection. In Proceedings of the 26th International Confer- ence on World Wide Web Companion, WWW '17",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "International World Wide Web Conferences Steering Committee",
                "authors": [],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "789--790",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Companion, pages 789-790, Republic and Canton of Geneva, Switzerland. International World Wide Web Conferences Steering Committee.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Pairwise word interaction modeling with deep neural networks for semantic similarity measurement",
                "authors": [
                    {
                        "first": "Hua",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Jimmy",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "volume": "",
                "issue": "",
                "pages": "937--948",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Hua He and Jimmy Lin. 2016. Pairwise word inter- action modeling with deep neural networks for se- mantic similarity measurement. In Proceedings of the 2016 Conference of the North American Chap- ter of the Association for Computational Linguis- tics: Human Language Technologies, pages 937- 948, San Diego, California. Association for Com- putational Linguistics.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Question answering with lcc's chaucer at trec 2006",
                "authors": [
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Hickl",
                        "suffix": ""
                    },
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Williams",
                        "suffix": ""
                    },
                    {
                        "first": "Jeremy",
                        "middle": [],
                        "last": "Bensley",
                        "suffix": ""
                    },
                    {
                        "first": "Kirk",
                        "middle": [],
                        "last": "Roberts",
                        "suffix": ""
                    },
                    {
                        "first": "Ying",
                        "middle": [],
                        "last": "Shi",
                        "suffix": ""
                    },
                    {
                        "first": "Bryan",
                        "middle": [],
                        "last": "Rink",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Proceedings of the Text REtrieval Conference",
                "volume": "",
                "issue": "",
                "pages": "283--292",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Andrew Hickl, John Williams, Jeremy Bensley, Kirk Roberts, Ying Shi, and Bryan Rink. 2007. Ques- tion answering with lcc's chaucer at trec 2006. In Proceedings of the Text REtrieval Conference, pages 283-292.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "A convolutional neural network for modelling sentences",
                "authors": [
                    {
                        "first": "Nal",
                        "middle": [],
                        "last": "Kalchbrenner",
                        "suffix": ""
                    },
                    {
                        "first": "Edward",
                        "middle": [],
                        "last": "Grefenstette",
                        "suffix": ""
                    },
                    {
                        "first": "Phil",
                        "middle": [],
                        "last": "Blunsom",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics",
                "volume": "1",
                "issue": "",
                "pages": "655--665",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Nal Kalchbrenner, Edward Grefenstette, and Phil Blun- som. 2014. A convolutional neural network for modelling sentences. In Proceedings of the 52nd Annual Meeting of the Association for Computa- tional Linguistics (Volume 1: Long Papers), pages 655-665, Baltimore, Maryland. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Convolutional neural networks for sentence classification",
                "authors": [
                    {
                        "first": "Yoon",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "volume": "",
                "issue": "",
                "pages": "1746--1751",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yoon Kim. 2014. Convolutional neural networks for sentence classification. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1746-1751, Doha, Qatar. Association for Computational Lin- guistics.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Adam: A method for stochastic optimization",
                "authors": [
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Diederik",
                        "suffix": ""
                    },
                    {
                        "first": "Jimmy",
                        "middle": [],
                        "last": "Kingma",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Ba",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of the 3rd International Conference on Learning Representations (ICLR)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Diederik P. Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. In Proceed- ings of the 3rd International Conference on Learn- ing Representations (ICLR).",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Imagenet classification with deep convolutional neural networks",
                "authors": [
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Krizhevsky",
                        "suffix": ""
                    },
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    },
                    {
                        "first": "Geoffrey",
                        "middle": [
                            "E"
                        ],
                        "last": "Hinton",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Advances in neural information processing systems",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin- ton. 2012. Imagenet classification with deep con- volutional neural networks. In Advances in neural information processing systems.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Question analysis: How watson reads a clue",
                "authors": [
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Lally",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Prager",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Mc Mccord",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Boguraev",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Patwardhan",
                        "suffix": ""
                    },
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Fan",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Fodor",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Chu-Carroll",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "IBM Journal of Research and Development",
                "volume": "56",
                "issue": "3.4",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "A. Lally, JM Prager, MC McCord, BK Boguraev, S. Patwardhan, J. Fan, P. Fodor, and J. Chu-Carroll. 2012. Question analysis: How watson reads a clue. IBM Journal of Research and Development, 56(3.4).",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Learning question classifiers: the role of semantic information",
                "authors": [
                    {
                        "first": "Xin",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Roth",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Natural Language Engineering",
                "volume": "12",
                "issue": "3",
                "pages": "229--249",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xin Li and Dan Roth. 2006. Learning question clas- sifiers: the role of semantic information. Natural Language Engineering, 12(3):229-249.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Neural variational inference for text processing",
                "authors": [
                    {
                        "first": "Yishu",
                        "middle": [],
                        "last": "Miao",
                        "suffix": ""
                    },
                    {
                        "first": "Lei",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Phil",
                        "middle": [],
                        "last": "Blunsom",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "International Conference on Machine Learning",
                "volume": "",
                "issue": "",
                "pages": "1727--1736",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yishu Miao, Lei Yu, and Phil Blunsom. 2016. Neu- ral variational inference for text processing. In In- ternational Conference on Machine Learning, pages 1727-1736.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "A decomposable attention model for natural language inference",
                "authors": [
                    {
                        "first": "Ankur",
                        "middle": [],
                        "last": "Parikh",
                        "suffix": ""
                    },
                    {
                        "first": "Oscar",
                        "middle": [],
                        "last": "T\u00e4ckstr\u00f6m",
                        "suffix": ""
                    },
                    {
                        "first": "Dipanjan",
                        "middle": [],
                        "last": "Das",
                        "suffix": ""
                    },
                    {
                        "first": "Jakob",
                        "middle": [],
                        "last": "Uszkoreit",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proc. of EMNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ankur Parikh, Oscar T\u00e4ckstr\u00f6m, Dipanjan Das, and Jakob Uszkoreit. 2016. A decomposable attention model for natural language inference. In Proc. of EMNLP.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Noisecontrastive estimation for answer selection with deep neural networks",
                "authors": [
                    {
                        "first": "Jinfeng",
                        "middle": [],
                        "last": "Rao",
                        "suffix": ""
                    },
                    {
                        "first": "Hua",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Jimmy",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the 25th ACM International on Conference on Information and Knowledge Management, CIKM '16",
                "volume": "",
                "issue": "",
                "pages": "1913--1916",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jinfeng Rao, Hua He, and Jimmy Lin. 2016. Noise- contrastive estimation for answer selection with deep neural networks. In Proceedings of the 25th ACM International on Conference on Information and Knowledge Management, CIKM '16, pages 1913-1916, New York, NY, USA. ACM.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Experiments with convolutional neural network models for answer selection",
                "authors": [
                    {
                        "first": "Jinfeng",
                        "middle": [],
                        "last": "Rao",
                        "suffix": ""
                    },
                    {
                        "first": "Hua",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Jimmy",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval",
                "volume": "",
                "issue": "",
                "pages": "1217--1220",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jinfeng Rao, Hua He, and Jimmy Lin. 2017. Experi- ments with convolutional neural network models for answer selection. In Proceedings of the 40th Inter- national ACM SIGIR Conference on Research and Development in Information Retrieval, pages 1217- 1220. ACM.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Attentive pooling networks",
                "authors": [
                    {
                        "first": "Ming",
                        "middle": [],
                        "last": "Cicero Dos Santos",
                        "suffix": ""
                    },
                    {
                        "first": "Bing",
                        "middle": [],
                        "last": "Tan",
                        "suffix": ""
                    },
                    {
                        "first": "Bowen",
                        "middle": [],
                        "last": "Xiang",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1602.03609"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Cicero dos Santos, Ming Tan, Bing Xiang, and Bowen Zhou. 2016. Attentive pooling networks. arXiv preprint arXiv:1602.03609.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Bidirectional recurrent neural networks",
                "authors": [
                    {
                        "first": "Mike",
                        "middle": [],
                        "last": "Schuster",
                        "suffix": ""
                    },
                    {
                        "first": "Kuldip K",
                        "middle": [],
                        "last": "Paliwal",
                        "suffix": ""
                    }
                ],
                "year": 1997,
                "venue": "IEEE Transactions on Signal Processing",
                "volume": "",
                "issue": "11",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mike Schuster and Kuldip K Paliwal. 1997. Bidirec- tional recurrent neural networks. IEEE Transactions on Signal Processing, 45(11).",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Exploring the effectiveness of convolutional neural networks for answer selection in end-to-end question answering",
                "authors": [
                    {
                        "first": "Royal",
                        "middle": [],
                        "last": "Sequiera",
                        "suffix": ""
                    },
                    {
                        "first": "Gaurav",
                        "middle": [],
                        "last": "Baruah",
                        "suffix": ""
                    },
                    {
                        "first": "Zhucheng",
                        "middle": [],
                        "last": "Tu",
                        "suffix": ""
                    },
                    {
                        "first": "Salman",
                        "middle": [],
                        "last": "Mohammed",
                        "suffix": ""
                    },
                    {
                        "first": "Jinfeng",
                        "middle": [],
                        "last": "Rao",
                        "suffix": ""
                    },
                    {
                        "first": "Haotian",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Jimmy",
                        "middle": [
                            "J"
                        ],
                        "last": "Lin",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "SIGIR 2017 Workshop on Neural Information Retrieval",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Royal Sequiera, Gaurav Baruah, Zhucheng Tu, Salman Mohammed, Jinfeng Rao, Haotian Zhang, and Jimmy J. Lin. 2017. Exploring the effectiveness of convolutional neural networks for answer selection in end-to-end question answering. In SIGIR 2017 Workshop on Neural Information Retrieval (Neu- IR'17).",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Inter-weighted alignment network for sentence pair modeling",
                "authors": [
                    {
                        "first": "Gehui",
                        "middle": [],
                        "last": "Shen",
                        "suffix": ""
                    },
                    {
                        "first": "Yunlun",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Zhi-Hong",
                        "middle": [],
                        "last": "Deng",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "1179--1189",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Gehui Shen, Yunlun Yang, and Zhi-Hong Deng. 2017. Inter-weighted alignment network for sentence pair modeling. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Pro- cessing, pages 1179-1189, Copenhagen, Denmark. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Convolutional neural networks vs. convolution kernels: Feature engineering for answer sentence reranking",
                "authors": [
                    {
                        "first": "Kateryna",
                        "middle": [],
                        "last": "Tymoshenko",
                        "suffix": ""
                    },
                    {
                        "first": "Daniele",
                        "middle": [],
                        "last": "Bonadiman",
                        "suffix": ""
                    },
                    {
                        "first": "Alessandro",
                        "middle": [],
                        "last": "Moschitti",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "volume": "",
                "issue": "",
                "pages": "1268--1278",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kateryna Tymoshenko, Daniele Bonadiman, and Alessandro Moschitti. 2016. Convolutional neu- ral networks vs. convolution kernels: Feature en- gineering for answer sentence reranking. In Pro- ceedings of the 2016 Conference of the North Amer- ican Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1268-1278, San Diego, California. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Assessing the impact of syntactic and semantic structures for answer passages reranking",
                "authors": [
                    {
                        "first": "Kateryna",
                        "middle": [],
                        "last": "Tymoshenko",
                        "suffix": ""
                    },
                    {
                        "first": "Alessandro",
                        "middle": [],
                        "last": "Moschitti",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, CIKM '15",
                "volume": "",
                "issue": "",
                "pages": "1451--1460",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kateryna Tymoshenko and Alessandro Moschitti. 2015. Assessing the impact of syntactic and seman- tic structures for answer passages reranking. In Pro- ceedings of the 24th ACM International on Confer- ence on Information and Knowledge Management, CIKM '15, pages 1451-1460, New York, NY, USA. ACM.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Encoding semantic resources in syntactic structures for passage reranking",
                "authors": [
                    {
                        "first": "Kateryna",
                        "middle": [],
                        "last": "Tymoshenko",
                        "suffix": ""
                    },
                    {
                        "first": "Alessandro",
                        "middle": [],
                        "last": "Moschitti",
                        "suffix": ""
                    },
                    {
                        "first": "Aliaksei",
                        "middle": [],
                        "last": "Severyn",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "664--672",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kateryna Tymoshenko, Alessandro Moschitti, and Ali- aksei Severyn. 2014. Encoding semantic resources in syntactic structures for passage reranking. In Pro- ceedings of the 14th Conference of the European Chapter of the Association for Computational Lin- guistics, pages 664-672, Gothenburg, Sweden. As- sociation for Computational Linguistics.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Inner attention based recurrent neural networks for answer selection",
                "authors": [
                    {
                        "first": "Bingning",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Kang",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Jun",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "ACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Bingning Wang, Kang Liu, and Jun Zhao. 2016a. Inner attention based recurrent neural networks for answer selection. In ACL.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "What is the Jeopardy model? a quasi-synchronous grammar for QA",
                "authors": [
                    {
                        "first": "Mengqiu",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Noah",
                        "middle": [
                            "A"
                        ],
                        "last": "Smith",
                        "suffix": ""
                    },
                    {
                        "first": "Teruko",
                        "middle": [],
                        "last": "Mitamura",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)",
                "volume": "",
                "issue": "",
                "pages": "22--32",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mengqiu Wang, Noah A. Smith, and Teruko Mita- mura. 2007. What is the Jeopardy model? a quasi-synchronous grammar for QA. In Proceed- ings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Com- putational Natural Language Learning (EMNLP- CoNLL), pages 22-32, Prague, Czech Republic. As- sociation for Computational Linguistics.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "A compareaggregate model for matching text sequences",
                "authors": [
                    {
                        "first": "Shuohang",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Jing",
                        "middle": [],
                        "last": "Jiang",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proc. of ICLR",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Shuohang Wang and Jing Jiang. 2017. A compare- aggregate model for matching text sequences. In Proc. of ICLR.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Bilateral multi-perspective matching for natural language sentences",
                "authors": [
                    {
                        "first": "Zhiguo",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Wael",
                        "middle": [],
                        "last": "Hamza",
                        "suffix": ""
                    },
                    {
                        "first": "Radu",
                        "middle": [],
                        "last": "Florian",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of IJCAI",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zhiguo Wang, Wael Hamza, and Radu Florian. 2017. Bilateral multi-perspective matching for natural lan- guage sentences. In Proceedings of IJCAI.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "Sentence similarity learning by lexical decomposition and composition",
                "authors": [
                    {
                        "first": "Zhiguo",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Haitao",
                        "middle": [],
                        "last": "Mi",
                        "suffix": ""
                    },
                    {
                        "first": "Abraham",
                        "middle": [],
                        "last": "Ittycheriah",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers",
                "volume": "",
                "issue": "",
                "pages": "1340--1349",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zhiguo Wang, Haitao Mi, and Abraham Ittycheriah. 2016b. Sentence similarity learning by lexical de- composition and composition. In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1340-1349. The COLING 2016 Organizing Committee.",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "Wikiqa: A challenge dataset for open-domain question answering",
                "authors": [
                    {
                        "first": "Yi",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Wen-Tau",
                        "middle": [],
                        "last": "Yih",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [],
                        "last": "Meek",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "2013--2018",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yi Yang, Wen-tau Yih, and Christopher Meek. 2015. Wikiqa: A challenge dataset for open-domain ques- tion answering. In Proceedings of the 2015 Con- ference on Empirical Methods in Natural Language Processing, pages 2013-2018, Lisbon, Portugal. As- sociation for Computational Linguistics.",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "Question answering using enhanced lexical semantic models",
                "authors": [
                    {
                        "first": "Wen-Tau",
                        "middle": [],
                        "last": "Yih",
                        "suffix": ""
                    },
                    {
                        "first": "Ming-Wei",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [],
                        "last": "Meek",
                        "suffix": ""
                    },
                    {
                        "first": "Andrzej",
                        "middle": [],
                        "last": "Pastusiak",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics",
                "volume": "1",
                "issue": "",
                "pages": "1744--1753",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Wen-tau Yih, Ming-Wei Chang, Christopher Meek, and Andrzej Pastusiak. 2013. Question answering us- ing enhanced lexical semantic models. In Proceed- ings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa- pers), pages 1744-1753, Sofia, Bulgaria. Associa- tion for Computational Linguistics.",
                "links": null
            },
            "BIBREF34": {
                "ref_id": "b34",
                "title": "Abcnn: Attention-based convolutional neural network for modeling sentence pairs",
                "authors": [
                    {
                        "first": "Wenpeng",
                        "middle": [],
                        "last": "Yin",
                        "suffix": ""
                    },
                    {
                        "first": "Hinrich",
                        "middle": [],
                        "last": "Schutze",
                        "suffix": ""
                    },
                    {
                        "first": "Bing",
                        "middle": [],
                        "last": "Xiang",
                        "suffix": ""
                    },
                    {
                        "first": "Bowen",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Transactions of the Association for Computational Linguistics",
                "volume": "4",
                "issue": "",
                "pages": "259--272",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Wenpeng Yin, Hinrich Schutze, Bing Xiang, and Bowen Zhou. 2016. Abcnn: Attention-based convo- lutional neural network for modeling sentence pairs. Transactions of the Association for Computational Linguistics, 4:259-272.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 1: The S&M CNN model. Diagram from Severyn and Moschitti (2016).",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "abbr",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "Figure 2: Our model with word and semantic overlap vectors. The convolution of size 3 is not padded, and the filters are 4.",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table><tr><td>Cat.</td><td>Named Entity Type</td></tr><tr><td>HUM LOC NUM ENTY DESC ABBR</td><td>Person Loc, Gpe Date, Time, Percent, Quantity, Ordinal, Cardinal Norp, Org, Facility, Product, Event, Work of art, Law, Language Norp, Org, Facility, Product, Event, Work of art, Law, Language, Date, Time, Percent, Quantity, Ordinal, Car-dinal Norp, Org, Facility, Product, Event, Work of art, Law, Language</td></tr></table>",
                "type_str": "table",
                "text": "Mapping between question categoriesand OntoNotes entity types.",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table><tr><td>only correct or incorrect answer sentence candi-</td></tr><tr><td>dates. The resulting dev. and test sets contain</td></tr><tr><td>respectively 65 and 68 questions. This setting</td></tr><tr><td>follows a standard in recent work on TREC-QA</td></tr><tr><td>which is referred to as TrecQA Clean (See Shen</td></tr><tr><td>et al. (2017)).</td></tr><tr><td>Results. Table 2 contains results from previ-ous work, and the performance of our models.</td></tr><tr><td>CNN W O is our variant of the S&amp;M model. It has comparable performance in terms of MAP,</td></tr><tr><td>but it is 2.4% points higher in MRR. Our model</td></tr><tr><td>CNN W O+SO that integrates the semantic over-lap improves over CNN W O by 1.44% points in MAP, and 0.67% points in MRR. It approaches</td></tr><tr><td>the model by Rao et al. (2016), which uses a</td></tr><tr><td>triplet ranking loss, and several strategies to build</td></tr></table>",
                "type_str": "table",
                "text": "MAP and MRR (%) on the TrecQA Clean dataset.",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table/>",
                "type_str": "table",
                "text": "MAP and MRR (%) on the WikiQA dataset.",
                "html": null,
                "num": null
            },
            "TABREF4": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Table 3 contains the results on Wik-iQA. Again the MAP score is comparable with the S&M model, while the MRR is slightly higher. Our model CNN W O+SO improves over CNN W O by 2.71% points in MAP, and 2.56% points in MRR, with a higher margin with respect to TrecQA. Interestingly, our approach improves upon the Tymoshenko et al. (",
                "html": null,
                "num": null
            },
            "TABREF5": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Aliaksei Severyn and Alessandro Moschitti. 2012. Structural relationships for large-scale learning of answer re-ranking. In Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '12, pages 741-750, New York, NY, USA. ACM. Aliaksei Severyn and Alessandro Moschitti. 2015. Learning to rank short text pairs with convolutional deep neural networks. In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 373-382. ACM. Aliaksei Severyn and Alessandro Moschitti. 2016. Modeling relational information in question-answer pairs with convolutional neural networks. CoRR, abs/1604.01178. Aliaksei Severyn, Massimo Nicosia, and Alessandro Moschitti. 2013a. Building structures from classifiers for passage reranking. In Proceedings of the 22nd ACM international conference on Conference on information &#38; knowledge management, CIKM '13, pages 969-978, New York, NY, USA. ACM. Aliaksei Severyn, Massimo Nicosia, and Alessandro Moschitti. 2013b. Learning adaptable patterns for passage reranking. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 75-83, Sofia, Bulgaria. Association for Computational Linguistics.",
                "html": null,
                "num": null
            }
        }
    }
}