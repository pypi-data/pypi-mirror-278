{
    "paper_id": "E06-1015",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T11:59:50.891445Z"
    },
    "title": "Making Tree Kernels practical for Natural Language Learning",
    "authors": [
        {
            "first": "Alessandro",
            "middle": [],
            "last": "Moschitti",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Rome \"Tor Vergata",
                "location": {
                    "settlement": "Rome",
                    "country": "Italy"
                }
            },
            "email": ""
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "In recent years tree kernels have been proposed for the automatic learning of natural language applications. Unfortunately, they show (a) an inherent super linear complexity and (b) a lower accuracy than traditional attribute/value methods.\nIn this paper, we show that tree kernels are very helpful in the processing of natural language as (a) we provide a simple algorithm to compute tree kernels in linear average running time and (b) our study on the classification properties of diverse tree kernels show that kernel combinations always improve the traditional methods. Experiments with Support Vector Machines on the predicate argument classification task provide empirical support to our thesis.",
    "pdf_parse": {
        "paper_id": "E06-1015",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "In recent years tree kernels have been proposed for the automatic learning of natural language applications. Unfortunately, they show (a) an inherent super linear complexity and (b) a lower accuracy than traditional attribute/value methods.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            },
            {
                "text": "In this paper, we show that tree kernels are very helpful in the processing of natural language as (a) we provide a simple algorithm to compute tree kernels in linear average running time and (b) our study on the classification properties of diverse tree kernels show that kernel combinations always improve the traditional methods. Experiments with Support Vector Machines on the predicate argument classification task provide empirical support to our thesis.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "In recent years tree kernels have been shown to be interesting approaches for the modeling of syntactic information in natural language tasks, e.g. syntactic parsing (Collins and Duffy, 2002) , relation extraction (Zelenko et al., 2003) , Named Entity recognition (Cumby and Roth, 2003; Culotta and Sorensen, 2004) and Semantic Parsing (Moschitti, 2004) .",
                "cite_spans": [
                    {
                        "start": 166,
                        "end": 191,
                        "text": "(Collins and Duffy, 2002)",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 214,
                        "end": 236,
                        "text": "(Zelenko et al., 2003)",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 264,
                        "end": 286,
                        "text": "(Cumby and Roth, 2003;",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 287,
                        "end": 314,
                        "text": "Culotta and Sorensen, 2004)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 336,
                        "end": 353,
                        "text": "(Moschitti, 2004)",
                        "ref_id": "BIBREF12"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The main tree kernel advantage is the possibility to generate a high number of syntactic features and let the learning algorithm to select those most relevant for a specific application. In contrast, their major drawback are (a) the computational time complexity which is superlinear in the number of tree nodes and (b) the accuracy that they produce is often lower than the one provided by linear models on manually designed features.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "To solve problem (a), a linear complexity algorithm for the subtree (ST) kernel computation, was designed in (Vishwanathan and Smola, 2002) . Unfortunately, the ST set is rather poorer than the one generated by the subset tree (SST) kernel designed in (Collins and Duffy, 2002) . Intuitively, an ST rooted in a node n of the target tree always contains all n's descendants until the leaves. This does not hold for the SSTs whose leaves can be internal nodes.",
                "cite_spans": [
                    {
                        "start": 109,
                        "end": 139,
                        "text": "(Vishwanathan and Smola, 2002)",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 252,
                        "end": 277,
                        "text": "(Collins and Duffy, 2002)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "To solve the problem (b), a study on different tree substructure spaces should be carried out to derive the tree kernel that provide the highest accuracy. On the one hand, SSTs provide learning algorithms with richer information which may be critical to capture syntactic properties of parse trees as shown, for example, in (Zelenko et al., 2003; Moschitti, 2004) . On the other hand, if the SST space contains too many irrelevant features, overfitting may occur and decrease the classification accuracy (Cumby and Roth, 2003) . As a consequence, the fewer features of the ST approach may be more appropriate.",
                "cite_spans": [
                    {
                        "start": 324,
                        "end": 346,
                        "text": "(Zelenko et al., 2003;",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 347,
                        "end": 363,
                        "text": "Moschitti, 2004)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 504,
                        "end": 526,
                        "text": "(Cumby and Roth, 2003)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this paper, we aim to solve the above problems. We present (a) an algorithm for the evaluation of the ST and SST kernels which runs in linear average time and (b) a study of the impact of diverse tree kernels on the accuracy of Support Vector Machines (SVMs).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Our fast algorithm computes the kernels between two syntactic parse trees in O(m + n) average time, where m and n are the number of nodes in the two trees. This low complexity allows SVMs to carry out experiments on hundreds of thousands of training instances since it is not higher than the complexity of the polynomial ker-nel, widely used on large experimentation e.g. (Pradhan et al., 2004) . To confirm such hypothesis, we measured the impact of the algorithm on the time required by SVMs for the learning of about 122,774 predicate argument examples annotated in PropBank (Kingsbury and Palmer, 2002) and 37,948 instances annotated in FrameNet (Fillmore, 1982) .",
                "cite_spans": [
                    {
                        "start": 372,
                        "end": 394,
                        "text": "(Pradhan et al., 2004)",
                        "ref_id": null
                    },
                    {
                        "start": 578,
                        "end": 606,
                        "text": "(Kingsbury and Palmer, 2002)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 650,
                        "end": 666,
                        "text": "(Fillmore, 1982)",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Regarding the classification properties, we studied the argument labeling accuracy of ST and SST kernels and their combinations with the standard features (Gildea and Jurafsky, 2002) . The results show that, on both PropBank and FrameNet datasets, the SST-based kernel, i.e. the richest in terms of substructures, produces the highest SVM accuracy. When SSTs are combined with the manual designed features, we always obtain the best figure classifier. This suggests that the many fragments included in the SST space are relevant and, since their manual design may be problematic (requiring a higher programming effort and deeper knowledge of the linguistic phenomenon), tree kernels provide a remarkable help in feature engineering.",
                "cite_spans": [
                    {
                        "start": 155,
                        "end": 182,
                        "text": "(Gildea and Jurafsky, 2002)",
                        "ref_id": "BIBREF5"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In the remainder of this paper, Section 2 describes the parse tree kernels and our fast algorithm. Section 3 introduces the predicate argument classification problem and its solution. Section 4 shows the comparative performance in term of the execution time and accuracy. Finally, Section 5 discusses the related work whereas Section 6 summarizes the conclusions.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The kernels that we consider represent trees in terms of their substructures (fragments). These latter define feature spaces which, in turn, are mapped into vector spaces, e.g. n . The associated kernel function measures the similarity between two trees by counting the number of their common fragments. More precisely, a kernel function detects if a tree subpart (common to both trees) belongs to the feature space that we intend to generate. For such purpose, the fragment types need to be described. We consider two important characterizations: the subtrees (STs) and the subset trees (SSTs).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Fast Parse Tree Kernels",
                "sec_num": "2"
            },
            {
                "text": "In our study, we consider syntactic parse trees, consequently, each node with its children is associated with a grammar production rule, where the symbol at left-hand side corresponds to the parent node and the symbols at right-hand side are associated with its children. The terminal symbols of the grammar are always associated with the leaves of the tree. For example, Figure 1 illustrates the syntactic parse of the sentence \"Mary brought a cat to school\". We define as a subtree (ST) any node of a tree along with all its descendants. For example, the line in Figure 1 circles the subtree rooted in the NP node. A subset tree (SST) is a more general structure. The difference with the subtrees is that the leaves can be associated with non-terminal symbols. The SSTs satisfy the constraint that they are generated by applying the same grammatical rule set which generated the original tree. For example, [S [N VP]] is a SST of the tree in Figure 1 which has two non-terminal symbols, N and VP, as leaves. Given a syntactic tree we can use as feature representation the set of all its STs or SSTs. For example, Figure 2 shows the parse tree of the sentence \"Mary brought a cat\" together with its 6 STs, whereas Figure 3 shows 10 SSTs (out of 17) of the subtree of Figure 2 rooted in VP. The high different number of substructures gives an intuitive quantification of the different information level between the two tree-based representations.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 379,
                        "end": 380,
                        "text": "1",
                        "ref_id": null
                    },
                    {
                        "start": 572,
                        "end": 573,
                        "text": "1",
                        "ref_id": null
                    },
                    {
                        "start": 951,
                        "end": 952,
                        "text": "1",
                        "ref_id": null
                    },
                    {
                        "start": 1122,
                        "end": 1123,
                        "text": "2",
                        "ref_id": null
                    },
                    {
                        "start": 1222,
                        "end": 1223,
                        "text": "3",
                        "ref_id": null
                    },
                    {
                        "start": 1275,
                        "end": 1276,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Subtrees and Subset Trees",
                "sec_num": "2.1"
            },
            {
                "text": "The main idea of tree kernels is to compute the number of the common substructures between two trees T 1 and T 2 without explicitly considering the whole fragment space. For this purpose, we slightly modified the kernel function proposed in (Collins and Duffy, 2002) by introducing a parameter \u03c3 which enables the ST or the SST evaluation.",
                "cite_spans": [
                    {
                        "start": 241,
                        "end": 266,
                        "text": "(Collins and Duffy, 2002)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Tree Kernel Functions",
                "sec_num": "2.2"
            },
            {
                "text": "Given the set of fragments {f 1 , f 2 , ..} = F, we defined the indicator function I i (n) which is equal 1 if the target f i is rooted at node n and 0 otherwise. We define",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Tree Kernel Functions",
                "sec_num": "2.2"
            },
            {
                "text": "K(T 1 , T 2 ) = n 1 \u2208N T 1 n 2 \u2208N T 2 \u2206(n 1 , n 2 ) (1)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Tree Kernel Functions",
                "sec_num": "2.2"
            },
            {
                "text": "where N T 1 and N T 2 are the sets of the T 1 's and T 2 's nodes, respectively and",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Tree Kernel Functions",
                "sec_num": "2.2"
            },
            {
                "text": "\u2206(n 1 , n 2 ) = |F| i=1 I i (n 1 )I i (n 2",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Tree Kernel Functions",
                "sec_num": "2.2"
            },
            {
                "text": "). This latter is equal to the number of common fragments rooted in the n 1 and n 2 nodes. We can compute \u2206 as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Tree Kernel Functions",
                "sec_num": "2.2"
            },
            {
                "text": "1. if the productions at n 1 and n 2 are different then \u2206(n 1 , n 2 ) = 0; 2. if the productions at n 1 and n 2 are the same, and n 1 and n 2 have only leaf children (i.e. they are pre-terminals symbols) then \u2206(n 1 , n 2 ) = 1; 3. if the productions at n 1 and n 2 are the same, and n 1 and n 2 are not pre-terminals then",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Tree Kernel Functions",
                "sec_num": "2.2"
            },
            {
                "text": "\u2206(n 1 , n 2 ) = nc(n 1 ) j=1 (\u03c3 + \u2206(c j n 1 , c j n 2 )) (2)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Tree Kernel Functions",
                "sec_num": "2.2"
            },
            {
                "text": "where \u03c3 \u2208 {0, 1}, nc(n 1 ) is the number of the children of n 1 and c j n is the j-th child of the node n. Note that, since the productions are the same,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Tree Kernel Functions",
                "sec_num": "2.2"
            },
            {
                "text": "nc(n 1 ) = nc(n 2 ). When \u03c3 = 0, \u2206(n 1 , n 2 ) is equal 1 only if \u2200j \u2206(c j n 1 , c j n 2 ) = 1, i.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Tree Kernel Functions",
                "sec_num": "2.2"
            },
            {
                "text": "e. all the productions associated with the children are identical. By recursively applying this property, it follows that the subtrees in n 1 and n 2 are identical. Thus, Eq. 1 evaluates the subtree (ST) kernel. When \u03c3 = 1, \u2206(n 1 , n 2 ) evaluates the number of SSTs common to n 1 and n 2 as proved in (Collins and Duffy, 2002) .",
                "cite_spans": [
                    {
                        "start": 302,
                        "end": 327,
                        "text": "(Collins and Duffy, 2002)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Tree Kernel Functions",
                "sec_num": "2.2"
            },
            {
                "text": "Additionally, we study some variations of the above kernels which include the leaves in the fragment space. For this purpose, it is enough to add the condition: 0. if n 1 and n 2 are leaves and their associated symbols are equal then \u2206(n 1 , n 2 ) = 1, to the recursive rule set for the \u2206 evaluation (Zhang and Lee, 2003) . We will refer to such extended kernels as ST+bow and SST+bow (bag-ofwords). Moreover, we add the decay factor \u03bb by modifying steps (2) and (3) as follows 1 :   2 ",
                "cite_spans": [
                    {
                        "start": 300,
                        "end": 321,
                        "text": "(Zhang and Lee, 2003)",
                        "ref_id": "BIBREF18"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 455,
                        "end": 458,
                        "text": "(2)",
                        "ref_id": "TABREF2"
                    },
                    {
                        "start": 463,
                        "end": 485,
                        "text": "(3) as follows 1 :   2",
                        "ref_id": "TABREF3"
                    }
                ],
                "eq_spans": [],
                "section": "The Tree Kernel Functions",
                "sec_num": "2.2"
            },
            {
                "text": ". \u2206(n 1 , n 2 ) = \u03bb, 3. \u2206(n 1 , n 2 ) = \u03bb nc(n 1 ) j=1 (\u03c3 + \u2206(c j n 1 , c j n 2 )). The computational complexity of Eq. 1 is O(|N T 1 | \u00d7 |N T 2 |).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Tree Kernel Functions",
                "sec_num": "2.2"
            },
            {
                "text": "We will refer to this basic implementation as the Quadratic Tree Kernel (QTK). However, as observed in (Collins and Duffy, 2002) this worst case is quite unlikely for the syntactic trees of natural language sentences, thus, we can design algorithms that run in linear time on average. if (production of(n1) > production of(n2)) then n2 = extract(L2); else if (production of(n1) < production of(n2)) then n1 = extract(L1); else while (production of(n1) == production of(n2)) while (production of(n1) == production of(n2)) add( n1, n2 ",
                "cite_spans": [
                    {
                        "start": 103,
                        "end": 128,
                        "text": "(Collins and Duffy, 2002)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Tree Kernel Functions",
                "sec_num": "2.2"
            },
            {
                "text": "To compute the kernels defined in the previous section, we sum the \u2206 function for each pair n 1 , n 2 \u2208 N T 1 \u00d7 N T 2 (Eq. 1). When the productions associated with n 1 and n 2 are different, we can avoid to evaluate \u2206(n 1 , n 2 ) since it is 0. Thus, we look for a node pair set",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Fast Tree Kernel (FTK)",
                "sec_num": "2.3"
            },
            {
                "text": "N p ={ n 1 , n 2 \u2208 N T 1 \u00d7 N T 2 : p(n 1 ) = p(n 2 )}",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Fast Tree Kernel (FTK)",
                "sec_num": "2.3"
            },
            {
                "text": ", where p(n) returns the production rule associated with n.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Fast Tree Kernel (FTK)",
                "sec_num": "2.3"
            },
            {
                "text": "To efficiently build N p , we (i) extract the L 1 and L 2 lists of the production rules from T 1 and T 2 , (ii) sort them in the alphanumeric order and (iii) scan them to find the node pairs n 1 , n 2 such that",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Fast Tree Kernel (FTK)",
                "sec_num": "2.3"
            },
            {
                "text": "(p(n 1 ) = p(n 2 )) \u2208 L 1 \u2229L 2 . Step (iii) may require only O(|N T 1 | + |N T 2 |) time, but, if p(n 1 ) appears r 1 times in T 1 and p(n 2 ) is repeated r 2 times in",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Fast Tree Kernel (FTK)",
                "sec_num": "2.3"
            },
            {
                "text": "T 2 , we need to consider r 1 \u00d7 r 2 pairs. The formal algorithm is given in Table 1 .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 82,
                        "end": 83,
                        "text": "1",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "A Fast Tree Kernel (FTK)",
                "sec_num": "2.3"
            },
            {
                "text": "Note that: (a) The list sorting can be done only once at the data preparation time (i.e. before training) in",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Fast Tree Kernel (FTK)",
                "sec_num": "2.3"
            },
            {
                "text": "O(|N T 1 | \u00d7 log(|N T 1 |)). (b)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Fast Tree Kernel (FTK)",
                "sec_num": "2.3"
            },
            {
                "text": "The algorithm shows that the worst case occurs when the parse trees are both generated using only one production rule, i.e. the two internal while cycles carry out",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Fast Tree Kernel (FTK)",
                "sec_num": "2.3"
            },
            {
                "text": "|N T 1 | \u00d7 |N T 2 | iterations.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Fast Tree Kernel (FTK)",
                "sec_num": "2.3"
            },
            {
                "text": "In contrast, two identical parse trees may generate a linear number of non-null pairs if there are few groups of nodes associated with the same production rule. (c) Such approach is perfectly compatible with the dynamic programming algorithm which computes \u2206. In fact, the only difference with the original approach is that the matrix entries corresponding to pairs of different production rules are not considered. Since such entries contain null values they do not affect the application of the original dynamic programming. Moreover, the order of the pair evaluation can be established at run time, starting from the root nodes towards the children.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Fast Tree Kernel (FTK)",
                "sec_num": "2.3"
            },
            {
                "text": "An interesting application of the SST kernel is the classification of the predicate argument structures defined in PropBank (Kingsbury and Palmer, 2002) or FrameNet (Fillmore, 1982) . Figure 4 shows the parse tree of the sentence: \"Mary brought a cat to school\" along with the pred-icate argument annotation proposed in the Prop-Bank project. Only verbs are considered as predicates whereas arguments are labeled sequentially from ARG0 to ARG9. Also in FrameNet predicate/argument information is described but for this purpose richer semantic structures called Frames are used. The Frames are schematic representations of situations involving various participants, properties and roles in which a word may be typically used. Frame elements or semantic roles are arguments of predicates called target words. For example the following sentence is annotated according to the AR-REST frame:",
                "cite_spans": [
                    {
                        "start": 124,
                        "end": 152,
                        "text": "(Kingsbury and Palmer, 2002)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 165,
                        "end": 181,
                        "text": "(Fillmore, 1982)",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 191,
                        "end": 192,
                        "text": "4",
                        "ref_id": "FIGREF2"
                    }
                ],
                "eq_spans": [],
                "section": "A Semantic Application of Parse Tree Kernels",
                "sec_num": "3"
            },
            {
                "text": "[ T ime One Saturday night] [ Authorities police in Brooklyn ] [ T arget apprehended ] [ Suspect sixteen teenagers].",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Semantic Application of Parse Tree Kernels",
                "sec_num": "3"
            },
            {
                "text": "The roles Suspect and Authorities are specific to the frame.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Semantic Application of Parse Tree Kernels",
                "sec_num": "3"
            },
            {
                "text": "The common approach to learn the classification of predicate arguments relates to the extraction of features from the syntactic parse tree of the target sentence. In (Gildea and Jurafsky, 2002) seven different features2 , which aim to capture the relation between the predicate and its arguments, were proposed. For example, the Parse Tree Path of the pair brought, ARG1 in the syntactic tree of Figure 4 is V \u2191 VP \u2193 NP. It encodes the dependency between the predicate and the argument as a sequence of nonterminal labels linked by direction symbols (up or down).",
                "cite_spans": [
                    {
                        "start": 166,
                        "end": 193,
                        "text": "(Gildea and Jurafsky, 2002)",
                        "ref_id": "BIBREF5"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 403,
                        "end": 404,
                        "text": "4",
                        "ref_id": "FIGREF2"
                    }
                ],
                "eq_spans": [],
                "section": "A Semantic Application of Parse Tree Kernels",
                "sec_num": "3"
            },
            {
                "text": "An alternative tree kernel representation, proposed in (Moschitti, 2004) , is the selection of the minimal tree subset that includes a predicate with only one of its arguments. For example, in Figure 4 , the substructures inside the three frames are the semantic/syntactic structures associated with the three arguments of the verb to bring, i.e. S ARG0 , S ARG1 and S ARGM .",
                "cite_spans": [
                    {
                        "start": 55,
                        "end": 72,
                        "text": "(Moschitti, 2004)",
                        "ref_id": "BIBREF12"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 200,
                        "end": 201,
                        "text": "4",
                        "ref_id": "FIGREF2"
                    }
                ],
                "eq_spans": [],
                "section": "A Semantic Application of Parse Tree Kernels",
                "sec_num": "3"
            },
            {
                "text": "Given a feature representation of predicate ar-guments, we can build an individual ONE-vs-ALL (OVA) classifier C i for each argument i. As a final decision of the multiclassifier, we select the argument type ARG t associated with the maximum value among the scores provided by the C i , i.e. t = argmax i\u2208S score(C i ), where S is the set of argument types. We adopted the OVA approach as it is simple and effective as showed in (Pradhan et al., 2004) . Note that the representation in Figure 4 is quite intuitive and, to conceive it, the designer requires much less linguistic knowledge about semantic roles than those necessary to define relevant features manually. To understand such point, we should make a step back before Gildea and Jurafsky defined the first set of features for Semantic Role Labeling (SRL). The idea that syntax may have been useful to derive semantic information was already inspired by linguists, but from a machine learning point of view, to decide which tree fragments may have been useful for semantic role labeling was not an easy task. In principle, the designer should have had to select and experiment all possible tree subparts. This is exactly what the tree kernels can automatically do: the designer just need to roughly select the interesting whole subtree (correlated with the linguistic phenomenon) and the tree kernel will generate all possible syntactic features from it. The task of selecting the most relevant substructures is carried out by the kernel machines themselves.",
                "cite_spans": [
                    {
                        "start": 429,
                        "end": 451,
                        "text": "(Pradhan et al., 2004)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 493,
                        "end": 494,
                        "text": "4",
                        "ref_id": "FIGREF2"
                    }
                ],
                "eq_spans": [],
                "section": "A Semantic Application of Parse Tree Kernels",
                "sec_num": "3"
            },
            {
                "text": "The aim of the experiments is twofold. On the one hand, we show that the FTK running time is linear on the average case and is much faster than QTK. This is accomplished by measuring the learning time and the average kernel computation time. On the other hand, we study the impact of the different tree based kernels on the predicate argument classification accuracy.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Experiments",
                "sec_num": "4"
            },
            {
                "text": "We used two different corpora: PropBank (www.cis.upenn.edu/\u223cace) along with Pen-nTree bank 2 (Marcus et al., 1993) and FrameNet.",
                "cite_spans": [
                    {
                        "start": 93,
                        "end": 114,
                        "text": "(Marcus et al., 1993)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Set-up",
                "sec_num": "4.1"
            },
            {
                "text": "PropBank contains about 53,700 sentences and a fixed split between training and testing which has been used in other researches, e.g. (Gildea and Palmer, 2002; Pradhan et al., 2004) . In this split, sections from 02 to 21 are used for training, section 23 for testing and sections 1 and 22 as developing set. We considered a total of 122,774 and 7,359 arguments (from ARG0 to ARG9, ARGA and ARGM) in training and testing, respectively. Their tree structures were extracted from the Penn Treebank. It should be noted that the main contribution to the global accuracy is given by ARG0, ARG1 and ARGM.",
                "cite_spans": [
                    {
                        "start": 134,
                        "end": 159,
                        "text": "(Gildea and Palmer, 2002;",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 160,
                        "end": 181,
                        "text": "Pradhan et al., 2004)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Set-up",
                "sec_num": "4.1"
            },
            {
                "text": "From the FrameNet corpus (http://www.icsi .berkeley.edu/\u223cframenet), we extracted all 24,558 sentences of the 40 Frames selected for the Automatic Labeling of Semantic Roles task of Senseval 3 (www.senseval.org). We mapped together the semantic roles having the same name and we considered only the 18 most frequent roles associated with verbal predicates, for a total of 37,948 arguments. We randomly selected 30% of sentences for testing and 70% for training. Additionally, 30% of training was used as a validationset. Note that, since the FrameNet data does not include deep syntactic tree annotation, we processed the FrameNet data with Collins' parser (Collins, 1997) , consequently, the experiments on FrameNet relate to automatic syntactic parse trees.",
                "cite_spans": [
                    {
                        "start": 656,
                        "end": 671,
                        "text": "(Collins, 1997)",
                        "ref_id": "BIBREF1"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Set-up",
                "sec_num": "4.1"
            },
            {
                "text": "The classifier evaluations were carried out with the SVM-light-TK software available at http://ai-nlp.info.uniroma2.it/moschitti/ which encodes ST and SST kernels in the SVMlight software (Joachims, 1999) . We used the default linear (Linear) and polynomial (Poly) kernels for the evaluations with the standard features defined in (Gildea and Jurafsky, 2002) . We adopted the default regularization parameter (i.e., the average of 1/|| x||) and we tried a few cost-factor values (i.e., j \u2208 {1, 3, 7, 10, 30, 100}) to adjust the rate between Precision and Recall on the validation-set.",
                "cite_spans": [
                    {
                        "start": 188,
                        "end": 204,
                        "text": "(Joachims, 1999)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 331,
                        "end": 358,
                        "text": "(Gildea and Jurafsky, 2002)",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 479,
                        "end": 513,
                        "text": "(i.e., j \u2208 {1, 3, 7, 10, 30, 100})",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Set-up",
                "sec_num": "4.1"
            },
            {
                "text": "For the ST and SST kernels, we derived that the best \u03bb (see Section 2.2) were 1 and 0.4, respectively. The classification performance was evaluated using the F 1 measure3 for the single arguments and the accuracy for the final multiclassifier. This latter choice allows us to compare our results with previous literature work, e.g. (Gildea and Jurafsky, 2002; Pradhan et al., 2004) .",
                "cite_spans": [
                    {
                        "start": 332,
                        "end": 359,
                        "text": "(Gildea and Jurafsky, 2002;",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 360,
                        "end": 381,
                        "text": "Pradhan et al., 2004)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Set-up",
                "sec_num": "4.1"
            },
            {
                "text": "In this section we compare our Fast Tree Kernel (FTK) approach with the Quadratic Tree Kernel (QTK) algorithm. The latter refers to the naive evaluation of Eq. 1 as presented in (Collins and Duffy, 2002) .",
                "cite_spans": [
                    {
                        "start": 178,
                        "end": 203,
                        "text": "(Collins and Duffy, 2002)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Time Complexity Experiments",
                "sec_num": "4.2"
            },
            {
                "text": "Figure 5 shows the learning time4 of the SVMs using QTK and FTK (over the SST structures) for the classification of one large argument (i.e. ARG0), according to different percentages of training data. We note that, with 70% of the training data, FTK is about 10 times faster than QTK. With all the training data FTK terminated in 6 hours whereas QTK required more than 1 week. The above results are quite interesting because they show that (1) we can use tree kernels with SVMs on huge training sets, e.g. on 122,774 instances and (2) the time needed to converge is approximately the one required by SVMs when using polynomial kernel. This latter shows the minimal complexity needed to work in the dual space.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "5",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Time Complexity Experiments",
                "sec_num": "4.2"
            },
            {
                "text": "To study the FTK running time, we extracted from PennTree bank the first 500 trees5 containing exactly n nodes, then, we evaluated all 25,000 possible tree pairs. Each point of the Figure 6 shows the average computation time on all the tree pairs of a fixed size n.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 188,
                        "end": 189,
                        "text": "6",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Time Complexity Experiments",
                "sec_num": "4.2"
            },
            {
                "text": "In the figures, the trend lines which best interpolates the experimental values are also shown. It clearly appears that the training time is quadratic as SVMs have quadratic learning time complexity (see Figure 5 ) whereas the FTK running time has a linear behavior (Figure 6 ). The QTK algorithm shows a quadratic running time complexity, as expected.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 211,
                        "end": 212,
                        "text": "5",
                        "ref_id": null
                    },
                    {
                        "start": 274,
                        "end": 275,
                        "text": "6",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Time Complexity Experiments",
                "sec_num": "4.2"
            },
            {
                "text": "In these experiments, we investigate which kernel is the most accurate for the predicate argument classification.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Accuracy of the Tree Kernels",
                "sec_num": "4.3"
            },
            {
                "text": "First, we run ST, SST, ST+bow, SST+bow, Linear and Poly kernels over different training-set size of PropBank. Figure 7 shows the learning curves associated with the above kernels for the SVMbased multiclassifier. We note that (a) SSTs have a higher accuracy than STs, (b) bow does not improve either ST or SST kernels and (c) in the final part of the plot SST shows a higher gradient than ST, Linear and Poly. This latter produces the best accuracy 90.5% in line with the literature findings using standard features and polynomial SVMs, e.g. 87.1%6 in (Pradhan et al., 2004) .",
                "cite_spans": [
                    {
                        "start": 552,
                        "end": 574,
                        "text": "(Pradhan et al., 2004)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 117,
                        "end": 118,
                        "text": "7",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Accuracy of the Tree Kernels",
                "sec_num": "4.3"
            },
            {
                "text": "Second, in tables 2 and 3, we report the results using all available training data, on PropBank and FrameNet test sets, respectively. Each row of the two tables shows the F 1 measure of the individual classifiers using different kernels whereas the last column illustrates the global accuracy of the multiclassifier.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Accuracy of the Tree Kernels",
                "sec_num": "4.3"
            },
            {
                "text": "We note that, the F 1 of the single arguments across the different kernels follows the same behavior of the global multiclassifier accuracy. On FrameNet, the bow impact on the ST and SST accuracy is higher than on PropBank as it produces an improvement of about 1.5%. This suggests that (1) to detect semantic roles, lexical information is very important, (2) bow give a higher contribution as errors in POS-tagging make the word + POS fragments less reliable and (3) as the FrameNet trees are obtained with the Collins' syntactic parser, tree kernels seem robust to incorrect parse trees.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Accuracy of the Tree Kernels",
                "sec_num": "4.3"
            },
            {
                "text": "Third, we point out that the polynomial kernel on flat features is more accurate than tree kernels but the design of such effective features required noticeable knowledge and effort (Gildea and Jurafsky, 2002) . On the contrary, the choice of subtrees suitable to syntactically characterize a target phenomenon seems a easier task (see Section 3 for the predicate argument case). Moreover, by combining polynomial and SST kernels, we can improve the classification accuracy (Moschitti, 2004), i.e. tree kernels provide the learning algorithm with many relevant fragments which hardly can be designed by hand. In fact, as many predicate argument structures are quite large (up to 100 nodes) they contain many fragments. Finally, to study the combined kernels, we applied the K 1 + \u03b3K 2 formula, where K 1 is either the Linear or the Poly kernel and K 2 is the ST or the SST kernel. Table 4 shows the results of four kernel combinations. We note that, (a) STs and SSTs improve Poly (about 0.5 and 2 percent points on PropBank and FrameNet, respectively) and (b) the linear kernel, which uses fewer features than Poly, is more enhanced by the SSTs than STs (for example on PropBank we have 89.4% and 88.6% vs. 87.6%), i.e. Linear takes advantage by the richer feature set of the SSTs. It should be noted that our results of kernel combinations on FrameNet are in contrast with (Moschitti, 2004) , where no improvement was obtained. Our explanation is that, thanks to the fast evaluation of FTK, we could carry out an adequate parameterization.",
                "cite_spans": [
                    {
                        "start": 182,
                        "end": 209,
                        "text": "(Gildea and Jurafsky, 2002)",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 1374,
                        "end": 1391,
                        "text": "(Moschitti, 2004)",
                        "ref_id": "BIBREF12"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 887,
                        "end": 888,
                        "text": "4",
                        "ref_id": "TABREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Accuracy of the Tree Kernels",
                "sec_num": "4.3"
            },
            {
                "text": "Recently, several tree kernels have been designed.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "5"
            },
            {
                "text": "In the following, we highlight their differences and properties.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "5"
            },
            {
                "text": "In (Collins and Duffy, 2002) , the SST tree kernel was experimented with the Voted Perceptron for the parse-tree reranking task. The combination with the original PCFG model improved the syntactic parsing. Additionally, it was alluded that the average execution time depends on the number of repeated productions.",
                "cite_spans": [
                    {
                        "start": 3,
                        "end": 28,
                        "text": "(Collins and Duffy, 2002)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "5"
            },
            {
                "text": "In (Vishwanathan and Smola, 2002) , a linear complexity algorithm for the computation of the ST kernel is provided (in the worst case). The main idea is the use of the suffix trees to store partial matches for the evaluation of the string kernel (Lodhi et al., 2000) . This can be used to compute the ST fragments once the tree is converted into a string. To our knowledge, ours is the first application of the ST kernel for a natural language task.",
                "cite_spans": [
                    {
                        "start": 3,
                        "end": 33,
                        "text": "(Vishwanathan and Smola, 2002)",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 246,
                        "end": 266,
                        "text": "(Lodhi et al., 2000)",
                        "ref_id": "BIBREF10"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "5"
            },
            {
                "text": "In (Kazama and Torisawa, 2005) , an interesting algorithm that speeds up the average running time is presented. Such algorithm looks for node pairs that have in common a large number of trees (malicious nodes) and applies a transformation to the trees rooted in such nodes to make faster the kernel computation. The results show an increase of the speed similar to the one produced by our method.",
                "cite_spans": [
                    {
                        "start": 3,
                        "end": 30,
                        "text": "(Kazama and Torisawa, 2005)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "5"
            },
            {
                "text": "In (Zelenko et al., 2003) , two kernels over syntactic shallow parser structures were devised for the extraction of linguistic relations, e.g. personaffiliation. To measure the similarity between two nodes, the contiguous string kernel and the sparse string kernel (Lodhi et al., 2000) were used. In (Culotta and Sorensen, 2004 ) such kernels were slightly generalized by providing a matching function for the node pairs. The time complexity for their computation limited the experiments on data set of just 200 news items. Moreover, we note that the above tree kernels are not convolution kernels as those proposed in this article.",
                "cite_spans": [
                    {
                        "start": 3,
                        "end": 25,
                        "text": "(Zelenko et al., 2003)",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 265,
                        "end": 285,
                        "text": "(Lodhi et al., 2000)",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 300,
                        "end": 327,
                        "text": "(Culotta and Sorensen, 2004",
                        "ref_id": "BIBREF2"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "5"
            },
            {
                "text": "In (Shen et al., 2003) , a tree-kernel based on Lexicalized Tree Adjoining Grammar (LTAG) for the parse-reranking task was proposed. Since QTK was used for the kernel computation, the high learning complexity forced the authors to train different SVMs on different slices of training data. Our FTK, adapted for the LTAG tree kernel, would have allowed SVMs to be trained on the whole data.",
                "cite_spans": [
                    {
                        "start": 3,
                        "end": 22,
                        "text": "(Shen et al., 2003)",
                        "ref_id": "BIBREF14"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "5"
            },
            {
                "text": "In (Cumby and Roth, 2003) , a feature description language was used to extract structural features from the syntactic shallow parse trees associated with named entities. The experiments on the named entity categorization showed that when the description language selects an adequate set of tree fragments the Voted Perceptron algorithm increases its classification accuracy. The explanation was that the complete tree fragment set contains many irrelevant features and may cause overfitting.",
                "cite_spans": [
                    {
                        "start": 3,
                        "end": 25,
                        "text": "(Cumby and Roth, 2003)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "5"
            },
            {
                "text": "In this paper, we have shown that tree kernels can effectively be adopted in practical natural language applications. The main arguments against their use are their efficiency and accuracy lower than traditional feature based approaches. We have shown that a fast algorithm (FTK) can evaluate tree kernels in a linear average running time and also that the overall converging time required by SVMs is compatible with very large data sets. Regarding the accuracy, the experiments with Support Vector Machines on the PropBank and FrameNet predicate argument structures show that: (a) the richer the kernel is in term of substructures (e.g. SST), the higher the accuracy is, (b) tree kernels are effective also in case of automatic parse trees and (c) as kernel combinations always improve traditional feature models, the best approach is to combine scalar-based and structured based kernels.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusions",
                "sec_num": "6"
            },
            {
                "text": "To have a similarity score between 0 and 1, we also apply the normalization in the kernel space, i.e. K (T1, T2) = K(T 1 ,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "T 2 ) \u221a K(T 1 ,T 1 )\u00d7K(T 2 ,T 2 ).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Namely, they are Phrase Type, Parse Tree Path, Predicate Word, Head Word, Governing Category, Position and Voice.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "F1 assigns equal importance to Precision P and Recall R, i.e. f1 = 2P \u00d7R P +R .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "We run the experiments on a Pentium 4, 2GHz, with 1 Gb ram.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "We measured also the computation time for the incomplete trees associated with the predicate argument structures (see Section 3); we obtained the same results.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "6 The small difference (2.4%) is mainly due to the different treatment of ARGMs: we built a single ARGM class for all subclasses, e.g. ARGM-LOC and ARGM-TMP, whereas in(Pradhan et al., 2004), the ARGMs, were evaluated separately.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "I would like to thank the AI group at the University of Rome \"Tor Vergata\". Many thanks to the EACL 2006 anonymous reviewers, Roberto Basili and Giorgio Satta who provided me with valuable suggestions. This research is partially supported by the Presto Space EU Project#: FP6-507336.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgments",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron",
                "authors": [
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Collins",
                        "suffix": ""
                    },
                    {
                        "first": "Nigel",
                        "middle": [],
                        "last": "Duffy",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Michael Collins and Nigel Duffy. 2002. New ranking al- gorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron. In ACL02.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Three generative, lexicalized models for statistical parsing",
                "authors": [
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Collins",
                        "suffix": ""
                    }
                ],
                "year": 1997,
                "venue": "proceedings of the ACL97",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Michael Collins. 1997. Three generative, lexicalized mod- els for statistical parsing. In proceedings of the ACL97, Madrid, Spain.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Dependency tree kernels for relation extraction",
                "authors": [
                    {
                        "first": "Aron",
                        "middle": [],
                        "last": "Culotta",
                        "suffix": ""
                    },
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Sorensen",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "proceedings of ACL04",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Aron Culotta and Jeffrey Sorensen. 2004. Dependency tree kernels for relation extraction. In proceedings of ACL04, Barcelona, Spain.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Kernel methods for relational learning",
                "authors": [
                    {
                        "first": "Chad",
                        "middle": [],
                        "last": "Cumby",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Roth",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "proceedings of ICML 2003. Washington",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chad Cumby and Dan Roth. 2003. Kernel methods for rela- tional learning. In proceedings of ICML 2003. Washing- ton, US.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Frame semantics",
                "authors": [
                    {
                        "first": "Charles",
                        "middle": [
                            "J"
                        ],
                        "last": "Fillmore",
                        "suffix": ""
                    }
                ],
                "year": 1982,
                "venue": "Linguistics in the Morning Calm",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Charles J. Fillmore. 1982. Frame semantics. In Linguistics in the Morning Calm.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Automatic labeling of semantic roles",
                "authors": [
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Gildea",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Jurafsky",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Computational Linguistic",
                "volume": "28",
                "issue": "3",
                "pages": "496--530",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistic, 28(3):496-530.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "The necessity of parsing for predicate argument recognition",
                "authors": [
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Gildea",
                        "suffix": ""
                    },
                    {
                        "first": "Martha",
                        "middle": [],
                        "last": "Palmer",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "proceedings of ACL02",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Daniel Gildea and Martha Palmer. 2002. The necessity of parsing for predicate argument recognition. In proceed- ings of ACL02, Philadelphia, PA.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Making large-scale SVM learning practical",
                "authors": [
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Joachims",
                        "suffix": ""
                    }
                ],
                "year": 1999,
                "venue": "Advances in Kernel Methods -Support Vector Learning",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "T. Joachims. 1999. Making large-scale SVM learning prac- tical. In B. Sch\u00f6lkopf, C. Burges, and A. Smola, editors, Advances in Kernel Methods -Support Vector Learning.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Speeding up training with tree kernels for node relation labeling",
                "authors": [
                    {
                        "first": "Junichi",
                        "middle": [],
                        "last": "Kazama",
                        "suffix": ""
                    },
                    {
                        "first": "Kentaro",
                        "middle": [],
                        "last": "Torisawa",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "proceedings of EMNLP 2005",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Junichi Kazama and Kentaro Torisawa. 2005. Speeding up training with tree kernels for node relation labeling. In proceedings of EMNLP 2005, Toronto, Canada.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "From Treebank to PropBank",
                "authors": [
                    {
                        "first": "Paul",
                        "middle": [],
                        "last": "Kingsbury",
                        "suffix": ""
                    },
                    {
                        "first": "Martha",
                        "middle": [],
                        "last": "Palmer",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "proceedings of LREC-2002",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Paul Kingsbury and Martha Palmer. 2002. From Treebank to PropBank. In proceedings of LREC-2002, Spain.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Text classification using string kernels",
                "authors": [
                    {
                        "first": "Huma",
                        "middle": [],
                        "last": "Lodhi",
                        "suffix": ""
                    },
                    {
                        "first": "Craig",
                        "middle": [],
                        "last": "Saunders",
                        "suffix": ""
                    },
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Shawe-Taylor",
                        "suffix": ""
                    },
                    {
                        "first": "Nello",
                        "middle": [],
                        "last": "Cristianini",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [],
                        "last": "Watkins",
                        "suffix": ""
                    }
                ],
                "year": 2000,
                "venue": "NIPS02",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello Cristianini, and Christopher Watkins. 2000. Text clas- sification using string kernels. In NIPS02, Vancouver, Canada.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Building a large annotated corpus of english: The Penn Treebank",
                "authors": [
                    {
                        "first": "M",
                        "middle": [
                            "P"
                        ],
                        "last": "Marcus",
                        "suffix": ""
                    },
                    {
                        "first": "B",
                        "middle": [],
                        "last": "Santorini",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [
                            "A"
                        ],
                        "last": "Marcinkiewicz",
                        "suffix": ""
                    }
                ],
                "year": 1993,
                "venue": "Computational Linguistics",
                "volume": "19",
                "issue": "",
                "pages": "313--330",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993. Building a large annotated corpus of english: The Penn Treebank. Computational Linguistics, 19:313-330.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "A study on convolution kernels for shallow semantic parsing",
                "authors": [
                    {
                        "first": "Alessandro",
                        "middle": [],
                        "last": "Moschitti",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "proceedings ACL04",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alessandro Moschitti. 2004. A study on convolution ker- nels for shallow semantic parsing. In proceedings ACL04, Barcelona, Spain.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Support vector learning for semantic argument classification",
                "authors": [
                    {
                        "first": "Kadri",
                        "middle": [],
                        "last": "Sameer Pradhan",
                        "suffix": ""
                    },
                    {
                        "first": "Valeri",
                        "middle": [],
                        "last": "Hacioglu",
                        "suffix": ""
                    },
                    {
                        "first": "Wayne",
                        "middle": [],
                        "last": "Krugler",
                        "suffix": ""
                    },
                    {
                        "first": "James",
                        "middle": [
                            "H"
                        ],
                        "last": "Ward",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Martin",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Jurafsky",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Machine Learning Journal",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, Wayne Ward, James H. Martin, and Daniel Jurafsky. 2005. Sup- port vector learning for semantic argument classification. Machine Learning Journal.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Using LTAG based features in parse reranking",
                "authors": [
                    {
                        "first": "Libin",
                        "middle": [],
                        "last": "Shen",
                        "suffix": ""
                    },
                    {
                        "first": "Anoop",
                        "middle": [],
                        "last": "Sarkar",
                        "suffix": ""
                    },
                    {
                        "first": "Aravind",
                        "middle": [],
                        "last": "Joshi",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "proceedings of EMNLP 2003",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Libin Shen, Anoop Sarkar, and Aravind Joshi. 2003. Using LTAG based features in parse reranking. In proceedings of EMNLP 2003, Sapporo, Japan.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Max-margin parsing",
                "authors": [
                    {
                        "first": "Ben",
                        "middle": [],
                        "last": "Taskar",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Klein",
                        "suffix": ""
                    },
                    {
                        "first": "Mike",
                        "middle": [],
                        "last": "Collins",
                        "suffix": ""
                    },
                    {
                        "first": "Daphne",
                        "middle": [],
                        "last": "Koller",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "proceedings of EMNLP 2004",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ben Taskar, Dan Klein, Mike Collins, Daphne Koller, and Christopher Manning. 2004. Max-margin parsing. In proceedings of EMNLP 2004 Barcelona, Spain.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Fast kernels on strings and trees",
                "authors": [
                    {
                        "first": "S",
                        "middle": [
                            "V N"
                        ],
                        "last": "Vishwanathan",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [
                            "J"
                        ],
                        "last": "Smola",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "proceedings of Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "S.V.N. Vishwanathan and A.J. Smola. 2002. Fast kernels on strings and trees. In proceedings of Neural Information Processing Systems.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Kernel methods for relation extraction",
                "authors": [
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Zelenko",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Aone",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Richardella",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Journal of Machine Learning Research",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "D. Zelenko, C. Aone, and A. Richardella. 2003. Ker- nel methods for relation extraction. Journal of Machine Learning Research.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Question classification using support vector machines",
                "authors": [
                    {
                        "first": "Dell",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Wee",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Lee",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "proceedings of SI-GIR'03",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dell Zhang and Wee Sun Lee. 2003. Question classifica- tion using support vector machines. In proceedings of SI- GIR'03, ACM Press.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 1: A syntactic parse tree.",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Figure2: A syntactic parse tree with its subtrees (STs).",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "Figure 4: Tree substructure space for predicate argument classification.",
                "uris": null,
                "fig_num": "4",
                "type_str": "figure"
            },
            "FIGREF3": {
                "num": null,
                "text": "Figure 5: ARG0 classifier learning time according to different training percentages.",
                "uris": null,
                "fig_num": "67",
                "type_str": "figure"
            },
            "TABREF1": {
                "content": "<table><tr><td>, Np);</td></tr><tr><td>n2=get next elem(L2); /*get the head element</td></tr><tr><td>and move the pointer to the next element*/</td></tr><tr><td>end</td></tr><tr><td>n1 = extract(L1);</td></tr><tr><td>reset(L2); /*set the pointer at the first element*/</td></tr><tr><td>end</td></tr><tr><td>end</td></tr><tr><td>return Np ;</td></tr><tr><td>end</td></tr></table>",
                "type_str": "table",
                "text": "Pseudo-code for fast evaluation of the node pair sets used in the fast Tree Kernel.",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table><tr><td>ARGs</td><td>ST</td><td>SST</td><td>ST+bow</td><td>SST+bow</td><td>Linear</td><td>P oly</td></tr><tr><td>ARG0</td><td colspan=\"2\">86.5 88.0</td><td>86.9</td><td>88.4</td><td colspan=\"2\">88.6 90.6</td></tr><tr><td>ARG1</td><td colspan=\"2\">83.1 87.4</td><td>82.8</td><td>86.7</td><td colspan=\"2\">85.9 90.8</td></tr><tr><td>ARG2</td><td colspan=\"2\">58.0 67.6</td><td>58.9</td><td>66.7</td><td colspan=\"2\">65.5 80.4</td></tr><tr><td>ARG3</td><td colspan=\"2\">35.7 37.5</td><td>39.3</td><td>41.2</td><td colspan=\"2\">51.9 60.4</td></tr><tr><td>ARG4</td><td colspan=\"2\">62.7 65.6</td><td>63.3</td><td>63.9</td><td colspan=\"2\">66.2 70.0</td></tr><tr><td colspan=\"3\">ARGM 92.0 94.2</td><td>92.0</td><td>93.7</td><td colspan=\"2\">94.9 95.3</td></tr><tr><td>Acc.</td><td colspan=\"2\">84.6 87.7</td><td>84.8</td><td>87.5</td><td colspan=\"2\">87.6 90.7</td></tr><tr><td>Roles</td><td>ST</td><td>SST</td><td>ST+bow</td><td>SST+bow</td><td>Linear</td><td>P oly</td></tr><tr><td>agent</td><td colspan=\"2\">86.9 87.8</td><td>89.2</td><td>90.2</td><td colspan=\"2\">89.8 91.7</td></tr><tr><td>theme</td><td colspan=\"2\">76.1 79.2</td><td>78.5</td><td>80.7</td><td colspan=\"2\">82.9 90.4</td></tr><tr><td>goal</td><td colspan=\"2\">77.9 78.9</td><td>78.2</td><td>80.1</td><td colspan=\"2\">80.2 85.8</td></tr><tr><td>path</td><td colspan=\"2\">82.8 84.4</td><td>83.7</td><td>85.1</td><td colspan=\"2\">81.3 85.5</td></tr><tr><td>manner</td><td colspan=\"2\">79.9 82.0</td><td>81.3</td><td>82.5</td><td colspan=\"2\">70.8 80.5</td></tr><tr><td>source</td><td colspan=\"2\">85.6 87.7</td><td>86.9</td><td>87.8</td><td colspan=\"2\">86.5 89.8</td></tr><tr><td>time</td><td colspan=\"2\">76.3 78.3</td><td>77.0</td><td>79.1</td><td colspan=\"2\">61.8 68.3</td></tr><tr><td>reason</td><td colspan=\"2\">75.9 77.3</td><td>78.9</td><td>81.4</td><td colspan=\"2\">82.9 86.4</td></tr><tr><td>Acc.</td><td colspan=\"2\">80.0 81.2</td><td>81.3</td><td>82.9</td><td colspan=\"2\">82.3 85.6</td></tr><tr><td>18 roles</td><td/><td/><td/><td/><td/><td/></tr></table>",
                "type_str": "table",
                "text": "Evaluation of Kernels on PropBank.",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Evaluation of the Kernels on FrameNet semantic roles.",
                "html": null,
                "num": null
            },
            "TABREF4": {
                "content": "<table><tr><td>Corpus</td><td>Poly</td><td>ST+Linear</td><td>SST+Linear</td><td>ST+Poly</td><td>SST+Poly</td></tr><tr><td colspan=\"2\">PropBank 90.7</td><td>88.6</td><td>89.4</td><td>91.1</td><td>91.3</td></tr><tr><td colspan=\"2\">FrameNet 85.6</td><td>85.3</td><td>85.8</td><td>87.5</td><td>87.2</td></tr></table>",
                "type_str": "table",
                "text": "Multiclassifier accuracy using Kernel Combinations.",
                "html": null,
                "num": null
            }
        }
    }
}