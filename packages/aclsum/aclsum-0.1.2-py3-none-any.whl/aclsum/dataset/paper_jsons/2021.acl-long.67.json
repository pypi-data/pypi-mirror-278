{
    "paper_id": "2021",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:21:42.052456Z"
    },
    "title": "Bilingual Lexicon Induction via Unsupervised Bitext Construction and Word Alignment",
    "authors": [
        {
            "first": "Haoyue",
            "middle": [],
            "last": "Shi",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "",
            "middle": [],
            "last": "Tti-Chicago",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Luke",
            "middle": [],
            "last": "Zettlemoyer",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Sida",
            "middle": [
                "I"
            ],
            "last": "Wang",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Morgen",
            "middle": [],
            "last": "Guten",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Abend",
            "middle": [],
            "last": "Guten",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Katze",
            "middle": [],
            "last": "Das",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "",
            "middle": [],
            "last": "Danke",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "",
            "middle": [],
            "last": "Hallo",
            "suffix": "",
            "affiliation": {},
            "email": ""
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Bilingual lexicons map words in one language to their translations in another, and are typically induced by learning linear projections to align monolingual word embedding spaces. In this paper, we show it is possible to produce much higher quality lexicons with methods that combine (1) unsupervised bitext mining and (2) unsupervised word alignment. Directly applying a pipeline that uses recent algorithms for both subproblems significantly improves induced lexicon quality and further gains are possible by learning to filter the resulting lexical entries, with both unsupervised and semisupervised schemes. Our final model outperforms the state of the art on the BUCC 2020 shared task by 14 F 1 points averaged over 12 language pairs, while also providing a more interpretable approach that allows for rich reasoning of word meaning in context. Further analysis of our output and the standard reference lexicons suggests they are of comparable quality, and new benchmarks may be needed to measure further progress on this task. 1",
    "pdf_parse": {
        "paper_id": "2021",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Bilingual lexicons map words in one language to their translations in another, and are typically induced by learning linear projections to align monolingual word embedding spaces. In this paper, we show it is possible to produce much higher quality lexicons with methods that combine (1) unsupervised bitext mining and (2) unsupervised word alignment. Directly applying a pipeline that uses recent algorithms for both subproblems significantly improves induced lexicon quality and further gains are possible by learning to filter the resulting lexical entries, with both unsupervised and semisupervised schemes. Our final model outperforms the state of the art on the BUCC 2020 shared task by 14 F 1 points averaged over 12 language pairs, while also providing a more interpretable approach that allows for rich reasoning of word meaning in context. Further analysis of our output and the standard reference lexicons suggests they are of comparable quality, and new benchmarks may be needed to measure further progress on this task. 1",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Bilingual lexicons map words in one language to their translations in another, and can be automatically induced by learning linear projections to align monolingual word embedding spaces (Artetxe et al., 2016; Smith et al., 2017; Lample et al., 2018, inter alia) . Although very successful in practice, the linear nature of these methods encodes unrealistic simplifying assumptions (e.g. all translations of a word have similar embeddings). In this paper, we show it is possible to produce much higher quality lexicons without these restrictions by introducing new methods that combine (1) unsupervised bitext mining and (2) unsupervised word alignment.",
                "cite_spans": [
                    {
                        "start": 186,
                        "end": 208,
                        "text": "(Artetxe et al., 2016;",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 209,
                        "end": 228,
                        "text": "Smith et al., 2017;",
                        "ref_id": "BIBREF36"
                    },
                    {
                        "start": 229,
                        "end": 261,
                        "text": "Lample et al., 2018, inter alia)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "We show that simply pipelining recent algorithms for unsupervised bitext mining (Tran et al., 2020) and unsupervised word alignment (Sabet et al., 2020) significantly improves bilingual lexicon induction (BLI) quality, and that further gains are possible by learning to filter the resulting lexical entries. Improving on a recent method for doing BLI via unsupervised machine translation (Artetxe et al., 2019) , we show that unsupervised mining produces better bitext for lexicon induction than translation, especially for less frequent words.",
                "cite_spans": [
                    {
                        "start": 80,
                        "end": 99,
                        "text": "(Tran et al., 2020)",
                        "ref_id": "BIBREF37"
                    },
                    {
                        "start": 132,
                        "end": 152,
                        "text": "(Sabet et al., 2020)",
                        "ref_id": "BIBREF31"
                    },
                    {
                        "start": 388,
                        "end": 410,
                        "text": "(Artetxe et al., 2019)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "These core contributions are established by systematic experiments in the class of bitext construction and alignment methods (Figure 1 ). Our full induction algorithm filters the lexicon found via the initial unsupervised pipeline. The filtering can be either fully unsupervised or weakly-supervised: for the former, we filter using simple heuristics and global statistics; for the latter, we train a multi-layer perceptron (MLP) to predict the probability of a word pair being in the lexicon, where the features are global statistics of word alignments.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 133,
                        "end": 134,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In addition to BLI, our method can also be directly adapted to improve word alignment and reach competitive or better alignment accuracy than the state of the art on all investigated language pairs. We find that improved alignment in sentence representations (Tran et al., 2020) leads to better contextual word alignments using local similarity (Sabet et al., 2020) .",
                "cite_spans": [
                    {
                        "start": 259,
                        "end": 278,
                        "text": "(Tran et al., 2020)",
                        "ref_id": "BIBREF37"
                    },
                    {
                        "start": 345,
                        "end": 365,
                        "text": "(Sabet et al., 2020)",
                        "ref_id": "BIBREF31"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Our final BLI approach outperforms the previous state of the art on the BUCC 2020 shared task (Rapp et al., 2020) by 14 F 1 points averaged over 12 language pairs. Manual analysis shows that most of our false positives are due to the incompleteness of the reference and that our lexicon is comparable to the reference lexicon and the output of a supervised system. Because both of our key building blocks make use of the pretrainined contextual representations from mBART (Liu et al., Word Alignment Statistical Feature Extraction cooccurrence(good, guten) = 2 one-to-one align(good, guten) = 2 many-to-one align(good, guten) = 0 cosine_similarity(good, guten) = 0.8 inner_product(good, guten) = 1.8 count(good) = 2 count(guten) = 2",
                "cite_spans": [
                    {
                        "start": 94,
                        "end": 113,
                        "text": "(Rapp et al., 2020)",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 472,
                        "end": 519,
                        "text": "(Liu et al., Word Alignment Statistical Feature",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Multi-Layer Perceptron \ud835\udc43 good, guten = 0.95 2020) and CRISS (Tran et al., 2020) , we can also interpret these results as clear evidence that lexicon induction benefits from contextualized reasoning at the token level, in strong contrast to nearly all existing methods that learn linear projections on word types.",
                "cite_spans": [
                    {
                        "start": 60,
                        "end": 79,
                        "text": "(Tran et al., 2020)",
                        "ref_id": "BIBREF37"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Lexicon Induction",
                "sec_num": null
            },
            {
                "text": "Bilingual lexicon induction (BLI). The task of BLI aims to induce a bilingual lexicon (i.e., word translation) from comparable monolingual corpora (e.g., Wikipedia in different languages). Following Mikolov et al. (2013) , most methods train a linear projection to align two monolingual embedding spaces. For supervised BLI, a seed lexicon is used to learn the projection matrix (Artetxe et al., 2016; Smith et al., 2017; Joulin et al., 2018) . For unsupervised BLI, the projection matrix is typically found by an iterative procedure such as adversarial learning (Lample et al., 2018; Zhang et al., 2017) , or iterative refinement initialized by a statistical heuristics (Hoshen and Wolf, 2018; Artetxe et al., 2018) . Artetxe et al. (2019) show strong gains over previous works by word aligning bitext generated with unsupervised machine translation. We show that retrieval-based bitext mining and contextual word alignment achieves even better performance.",
                "cite_spans": [
                    {
                        "start": 199,
                        "end": 220,
                        "text": "Mikolov et al. (2013)",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 379,
                        "end": 401,
                        "text": "(Artetxe et al., 2016;",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 402,
                        "end": 421,
                        "text": "Smith et al., 2017;",
                        "ref_id": "BIBREF36"
                    },
                    {
                        "start": 422,
                        "end": 442,
                        "text": "Joulin et al., 2018)",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 563,
                        "end": 584,
                        "text": "(Lample et al., 2018;",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 585,
                        "end": 604,
                        "text": "Zhang et al., 2017)",
                        "ref_id": "BIBREF39"
                    },
                    {
                        "start": 671,
                        "end": 694,
                        "text": "(Hoshen and Wolf, 2018;",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 695,
                        "end": 716,
                        "text": "Artetxe et al., 2018)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 719,
                        "end": 740,
                        "text": "Artetxe et al. (2019)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "Word alignment. Word alignment is a fundamental problem in statistical machine translation, of which the goal is to align words that are translations of each in within parallel sentences (Brown et al., 1993) . Most methods assume parallel sentences for training data (Och and Ney, 2003; Dyer et al., 2013; Peter et al., 2017, inter alia) . In contrast, Sabet et al. (2020) propose SimAlign, which does not train on parallel sentences but instead aligns words that have the most similar pre-trained multilingual representations (Devlin et al., 2019; Conneau et al., 2019) . SimAlign achieves competitive or superior performance than conventional alignment methods despite not using parallel sentences, and provides one of the baseline components for our work. We also present a simple yet effective method to improve performance over SimAlign (Section 5).",
                "cite_spans": [
                    {
                        "start": 187,
                        "end": 207,
                        "text": "(Brown et al., 1993)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 267,
                        "end": 286,
                        "text": "(Och and Ney, 2003;",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 287,
                        "end": 305,
                        "text": "Dyer et al., 2013;",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 306,
                        "end": 337,
                        "text": "Peter et al., 2017, inter alia)",
                        "ref_id": null
                    },
                    {
                        "start": 353,
                        "end": 372,
                        "text": "Sabet et al. (2020)",
                        "ref_id": "BIBREF31"
                    },
                    {
                        "start": 527,
                        "end": 548,
                        "text": "(Devlin et al., 2019;",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 549,
                        "end": 570,
                        "text": "Conneau et al., 2019)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "Bitext mining/parallel corpus mining. Bitext mining has been a long studied task (Resnik, 1999; Shi et al., 2006; Abdul-Rauf and Schwenk, 2009, inter alia) . Most methods train neural multilingual encoders on bitext, which are then used with efficent nearest neighbor search to expand the training set (Espana-Bonet et al., 2017; Schwenk, 2018; Guo et al., 2018; Artetxe and Schwenk, 2019a, inter alia) . Recent work has also shown that unsupervised mining is possible (Tran et al., 2020; Keung et al., 2020) . We use CRISS (Tran et al., 2020) 2 as one of our component models.",
                "cite_spans": [
                    {
                        "start": 81,
                        "end": 95,
                        "text": "(Resnik, 1999;",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 96,
                        "end": 113,
                        "text": "Shi et al., 2006;",
                        "ref_id": "BIBREF35"
                    },
                    {
                        "start": 114,
                        "end": 155,
                        "text": "Abdul-Rauf and Schwenk, 2009, inter alia)",
                        "ref_id": null
                    },
                    {
                        "start": 302,
                        "end": 329,
                        "text": "(Espana-Bonet et al., 2017;",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 330,
                        "end": 344,
                        "text": "Schwenk, 2018;",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 345,
                        "end": 362,
                        "text": "Guo et al., 2018;",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 363,
                        "end": 402,
                        "text": "Artetxe and Schwenk, 2019a, inter alia)",
                        "ref_id": null
                    },
                    {
                        "start": 469,
                        "end": 488,
                        "text": "(Tran et al., 2020;",
                        "ref_id": "BIBREF37"
                    },
                    {
                        "start": 489,
                        "end": 508,
                        "text": "Keung et al., 2020)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 524,
                        "end": 543,
                        "text": "(Tran et al., 2020)",
                        "ref_id": "BIBREF37"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "We build on unsupervised methods for word alignment and bitext construction, as reviewed below.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baseline Components",
                "sec_num": "3"
            },
            {
                "text": "SimAlign (Sabet et al., 2020) is an unsupervised word aligner based on the similarity of contextualized token embeddings. Given a pair of parallel sentences, SimAlign computes embeddings using pretrained multilingual language models such as mBERT and XLM-R, and forms a matrix whose entries are the cosine similarities between every source token vector and every target token vector.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Unsupervised Word Alignment",
                "sec_num": "3.1"
            },
            {
                "text": "Based on the similarity matrix, the argmax algorithm aligns the positions that are the simultaneous column-wise and row-wise maxima. To increase recall, Sabet et al. (2020) also propose itermax, which applies argmax iteratively while excluding previously aligned positions.",
                "cite_spans": [
                    {
                        "start": 153,
                        "end": 172,
                        "text": "Sabet et al. (2020)",
                        "ref_id": "BIBREF31"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Unsupervised Word Alignment",
                "sec_num": "3.1"
            },
            {
                "text": "We consider two methods for bitext construction: unsupervised machine translation (generation; Artetxe et al., 2019, Section 3.2 ) and bitext retrieval (retrieval; Tran et al., 2020, Section 3.2) .",
                "cite_spans": [
                    {
                        "start": 95,
                        "end": 128,
                        "text": "Artetxe et al., 2019, Section 3.2",
                        "ref_id": null
                    },
                    {
                        "start": 164,
                        "end": 195,
                        "text": "Tran et al., 2020, Section 3.2)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Unsupervised Bitext Construction",
                "sec_num": "3.2"
            },
            {
                "text": "Generation Artetxe et al. (2019) train an unsupervised machine translation model with monolingual corpora, generate bitext with the obtained model, and further use the generated bitext to induce bilingual lexicons. We replace their statistical unsupervised translation model with CRISS, a recent high quality unsupervised machine translation model which is expected to produce much higher quality bitext (i.e., translations). For each sentence in the two monolingual corpora, we generate a translation to the other language using beam search or nucleus sampling (Holtzman et al., 2020) .",
                "cite_spans": [
                    {
                        "start": 11,
                        "end": 32,
                        "text": "Artetxe et al. (2019)",
                        "ref_id": null
                    },
                    {
                        "start": 562,
                        "end": 585,
                        "text": "(Holtzman et al., 2020)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Unsupervised Bitext Construction",
                "sec_num": "3.2"
            },
            {
                "text": "Retrieval Tran et al. (2020) show that the CRISS encoder module provides as a high-quality sentence encoder for cross-lingual retrieval: they take the average across the contextualized embeddings of tokens as sentence representation, perform nearest neighbor search with FAISS (Johnson et al., 2019) ,3 and mine bitext using the margin-based max-score method (Artetxe and Schwenk, 2019a). 4 The score between sentence representations s and t is defined by score(s, t)",
                "cite_spans": [
                    {
                        "start": 10,
                        "end": 28,
                        "text": "Tran et al. (2020)",
                        "ref_id": "BIBREF37"
                    },
                    {
                        "start": 277,
                        "end": 299,
                        "text": "(Johnson et al., 2019)",
                        "ref_id": "BIBREF18"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Unsupervised Bitext Construction",
                "sec_num": "3.2"
            },
            {
                "text": "(1) = cos (s, t) ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Unsupervised Bitext Construction",
                "sec_num": "3.2"
            },
            {
                "text": "We align the constructed bitext with CRISS-based SimAlign, and propose to use smoothed matched ratio for a pair of bilingual word type s, t \u03c1(s, t) = mat(s, t) coc(s, t) + \u03bb as the metric to induce lexicon, where mat(s, t) and coc(s, t) denote the one-to-one matching count (e.g., guten-good; Figure 1 ) and co-occurrence count of s, t appearing in a sentence pair respectively, and \u03bb is a non-negative smoothing term. 5During inference, we predict the target word t with the highest \u03c1(s, t) for each source word s. Like most previous work (Artetxe et al., 2016; Smith et al., 2017; Lample et al., 2018, inter alia) , this method translates each source word to exactly one target word.",
                "cite_spans": [
                    {
                        "start": 540,
                        "end": 562,
                        "text": "(Artetxe et al., 2016;",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 563,
                        "end": 582,
                        "text": "Smith et al., 2017;",
                        "ref_id": "BIBREF36"
                    },
                    {
                        "start": 583,
                        "end": 615,
                        "text": "Lample et al., 2018, inter alia)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 300,
                        "end": 301,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Fully Unsupervised Induction",
                "sec_num": "4.1"
            },
            {
                "text": "We also propose a weakly supervised method, which assumes access to a seed lexicon. This lexicon is used to train a classifier to further filter the potential lexical entries.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Weakly Supervised Induction",
                "sec_num": "4.2"
            },
            {
                "text": "For a pair of word type s, t , our classifier uses the following global features:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Weakly Supervised Induction",
                "sec_num": "4.2"
            },
            {
                "text": "\u2022 Count of alignment: we consider both one-toone alignment (Section 4.1) and many-to-one alignment (e.g., danke-you and danke-thank; Figure 1 ) of s and t separately as two features, since the task of lexicon induction is arguably biased toward one-to-one alignment.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 140,
                        "end": 141,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Weakly Supervised Induction",
                "sec_num": "4.2"
            },
            {
                "text": "\u2022 Count of co-occurrence used in Section 4.1.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Weakly Supervised Induction",
                "sec_num": "4.2"
            },
            {
                "text": "\u2022 The count of s in the source language and t in the target language. 6",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Weakly Supervised Induction",
                "sec_num": "4.2"
            },
            {
                "text": "\u2022 Non-contextualized word similarity: we feed the word type itself into CRISS, use the average pooling of the output subword embeddings, and consider both cosine similarity and dot-product similarity as features.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Weakly Supervised Induction",
                "sec_num": "4.2"
            },
            {
                "text": "For a counting feature c, we take log (c + \u03b8 c ), where \u03b8 consists of learnable parameters. There are 7 features in total, which is denoted by x s,t \u2208 R 7 .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Weakly Supervised Induction",
                "sec_num": "4.2"
            },
            {
                "text": "We compute the probability of a pair of words s, t being in the induced lexicon P \u0398 (s, t) 7 by a ReLU activated multi-layer perceptron (MLP):",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Weakly Supervised Induction",
                "sec_num": "4.2"
            },
            {
                "text": "\u0125 s,t = ReLU W 1 x s,t + b 1 P \u0398 (s, t) = \u03c3 w 2 \u2022 \u0125 s,t + b 2 ,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Weakly Supervised Induction",
                "sec_num": "4.2"
            },
            {
                "text": "where \u03c3(\u2022) denotes the sigmoid function, and",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Weakly Supervised Induction",
                "sec_num": "4.2"
            },
            {
                "text": "\u0398 = {W 1 , b 1 , w 2 , b 2 } denotes the learnable parame- ters of the model.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Weakly Supervised Induction",
                "sec_num": "4.2"
            },
            {
                "text": "Recall that we are able to access a seed lexicon, which consists of pairs of word translations. In the training stage, we seek to maximize the log likelihood:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Weakly Supervised Induction",
                "sec_num": "4.2"
            },
            {
                "text": "\u0398 * = arg max \u0398 s,t \u2208D + log P \u0398 (s, t) + s ,t \u2208D - log 1 -P \u0398 (s , t ) ,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Weakly Supervised Induction",
                "sec_num": "4.2"
            },
            {
                "text": "where D + and D -denotes the positive training set (i.e., the seed lexicon) and the negative training set respectively. We construct the negative training set by extracting all bilingual word pairs that cooccurred but are not in the seed word pairs. We tune two hyperparameters \u03b4 and n to maximize the F 1 score on the seed lexicon and use them for inference, where \u03b4 denotes the prediction threshold and n denotes the maximum number of translations for each source word, following Laville et al. (2020) who estimate these hyperparameters based on heuristics. The inference algorithm is summarized in Algorithm 1.",
                "cite_spans": [
                    {
                        "start": 482,
                        "end": 503,
                        "text": "Laville et al. (2020)",
                        "ref_id": "BIBREF24"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Weakly Supervised Induction",
                "sec_num": "4.2"
            },
            {
                "text": "The idea of using an MLP to induce lexicon with weak supervision (Section 4.2) can be directly extended to word alignment. Let B = { S i , T i } N i=1 6 SimAlign sometimes mistakenly align rare words to punctuation, and such features can help exclude such pairs. 7 Not to be confused with joint probability.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Extension to Word Alignment",
                "sec_num": "5"
            },
            {
                "text": "Algorithm 1: Inference algorithm for weakly-supervised lexicon induction. Input: Thresholds \u03b4, n, Model parameters \u0398, source words S Output: Induced lexicon L L \u2190 \u2205 for s \u2208 S do ( s, t 1 , . . . , s, t k ) \u2190 bilingual word pairs sorted by the descending order of",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Extension to Word Alignment",
                "sec_num": "5"
            },
            {
                "text": "P \u0398 (s, t i ) k = max{j | P \u0398 (s, t j ) \u2265 \u03b4, j \u2208 [k]} m = min(n, k ) L \u2190 L \u222a { s, t 1 , . . . , s, t m } end",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Extension to Word Alignment",
                "sec_num": "5"
            },
            {
                "text": "denote the constructed bitext in Section 3.2, where N denotes the number of sentence pairs, and S i and T i denote a pair of sentences in the source and target language respectively. In a pair of bitext S, T , S = s 1 , . . . , s s and T = t 1 , . . . , t s denote sentences consist of word tokens s i or t i .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Extension to Word Alignment",
                "sec_num": "5"
            },
            {
                "text": "For a pair of bitext, SimAlign with a specified inference algorithm produces word alignment A = { a i , b i } i , denoting that the word tokens s a i and t b i are aligned. Sabet et al. (2020) has proposed different algorithms to induce alignment from the same similarity matrix, and the best method varies across language pairs. In this work, we consider the relatively conservative (i.e., having higher precision) argmax and the higher recall itermax algorithm (Sabet et al., 2020) , and denote the alignments by A argmax and A itermax respectively.",
                "cite_spans": [
                    {
                        "start": 173,
                        "end": 192,
                        "text": "Sabet et al. (2020)",
                        "ref_id": "BIBREF31"
                    },
                    {
                        "start": 463,
                        "end": 483,
                        "text": "(Sabet et al., 2020)",
                        "ref_id": "BIBREF31"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Extension to Word Alignment",
                "sec_num": "5"
            },
            {
                "text": "We substitute the non-contextualized word similarity feature (Section 4.2) with contextualized word similarity where the corresponding word embedding is computed by averaging the final-layer contextualized subword embeddings of CRISS. The cosine similarities and dot-products of these embeddings are included as features.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Extension to Word Alignment",
                "sec_num": "5"
            },
            {
                "text": "Instead of the binary classification in Section 4.2, we do ternary classification for word alignments. For a pair of word tokens s i , t j , the gold label y s i ,t j is defined as",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Extension to Word Alignment",
                "sec_num": "5"
            },
            {
                "text": "1[ i, j \u2208 A argmax ] + 1[ i, j \u2208 A itermax ].",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Extension to Word Alignment",
                "sec_num": "5"
            },
            {
                "text": "Intuitively, the labels 0 and 2 represents confident alignment or non-alignment by both methods, while the label 1 models the potential alignment.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Extension to Word Alignment",
                "sec_num": "5"
            },
            {
                "text": "The MLP takes the features x s i ,t j \u2208 R 7 of the word token pair, and compute the probability of each label y by \u0125",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Extension to Word Alignment",
                "sec_num": "5"
            },
            {
                "text": "= ReLU W 1 x s i ,t j + b 1 g = W 2 \u2022 \u0125 + b 2 P \u03a6 (y | s i , t j , S, T ) = exp (g y ) y exp g y , where \u03a6 = {W 1 W 2 , b 1 , b 2 }.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Extension to Word Alignment",
                "sec_num": "5"
            },
            {
                "text": "On the training stage, we maximize the log-likelihood of groundtruth labels:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Extension to Word Alignment",
                "sec_num": "5"
            },
            {
                "text": "\u03a6 * = arg max \u03a6 S,T \u2208B s i \u2208S t j \u2208T log P \u03a6 (y s i ,t j | s i , t j , S, T ).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Extension to Word Alignment",
                "sec_num": "5"
            },
            {
                "text": "On the inference stage, we keep all word token pairs s i , t j that have",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Extension to Word Alignment",
                "sec_num": "5"
            },
            {
                "text": "E P [y] := y y \u2022 P (y | s i , t j , S, T ) > 1",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Extension to Word Alignment",
                "sec_num": "5"
            },
            {
                "text": "as the prediction.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Extension to Word Alignment",
                "sec_num": "5"
            },
            {
                "text": "Throughout our experiments, we use a two-layer perceptron with the hidden size of 8 for both lexicon induction and word alignment. We optimize all of our models using Adam (Kingma and Ba, 2015) with the initial learning rate 5 \u00d7 10 -4 . For our bitext construction methods, we retrieve the best matching sentence or translate the sentences in the source language Wikipedia; for baseline models, we use their default settings.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Setup and Baselines",
                "sec_num": "6"
            },
            {
                "text": "For evaluation, we use the BUCC 2020 BLI shared task dataset (Rapp et al., 2020) and metric (F 1 ). Like most recent work, this evaluation is based on MUSE (Lample et al., 2018) . 8 We primarily report the BUCC evaluation because it considers recall in addition to precision. However, because most recent work only evaluates on precision, we include those evaluations in Appendix D.",
                "cite_spans": [
                    {
                        "start": 61,
                        "end": 80,
                        "text": "(Rapp et al., 2020)",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 156,
                        "end": 177,
                        "text": "(Lample et al., 2018)",
                        "ref_id": "BIBREF23"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Setup and Baselines",
                "sec_num": "6"
            },
            {
                "text": "We compare the following baselines:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Setup and Baselines",
                "sec_num": "6"
            },
            {
                "text": "BUCC. Best results from the BUCC 2020 (Rapp et al., 2020) for each language pairs, we take the maximum F 1 score between the best closed-track results (Severini et al., 2020; Laville et al., 2020) and open-track ones (Severini et al., 2020) . Our method would be considered open track since the pretrained models used a much larger data set (Common Crawl 25) than the BUCC 2020 closedtrack (Wikipedia or Wacky; Baroni et al., 2009) .",
                "cite_spans": [
                    {
                        "start": 38,
                        "end": 57,
                        "text": "(Rapp et al., 2020)",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 151,
                        "end": 174,
                        "text": "(Severini et al., 2020;",
                        "ref_id": "BIBREF34"
                    },
                    {
                        "start": 175,
                        "end": 196,
                        "text": "Laville et al., 2020)",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 217,
                        "end": 240,
                        "text": "(Severini et al., 2020)",
                        "ref_id": "BIBREF34"
                    },
                    {
                        "start": 411,
                        "end": 431,
                        "text": "Baroni et al., 2009)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Setup and Baselines",
                "sec_num": "6"
            },
            {
                "text": "VECMAP. Popular and robust method for aligning monolingual word embeddings via a linear projection and extracting lexicons. Here, we use the standard implementation9 with FastText vectors (Bojanowski et al., 2017) 10 trained on the union of Wikipedia and Common Crawl corpus for each language. 11 We include both supervised and unsupervised versions.",
                "cite_spans": [
                    {
                        "start": 188,
                        "end": 213,
                        "text": "(Bojanowski et al., 2017)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Setup and Baselines",
                "sec_num": "6"
            },
            {
                "text": "WM. WikiMatrix (Schwenk et al., 2019) 12 is a dataset of mined bitext. The mining method LASER (Artetxe and Schwenk, 2019b ) is trained on real bitext and then used to mine more bitext from the Wikipedia corpora to get the WikiMatrix dataset. We test our lexicon induction method with WikiMatrix bitext as the input and compare to our methods that do not use bitext supervision.",
                "cite_spans": [
                    {
                        "start": 15,
                        "end": 37,
                        "text": "(Schwenk et al., 2019)",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 95,
                        "end": 122,
                        "text": "(Artetxe and Schwenk, 2019b",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Setup and Baselines",
                "sec_num": "6"
            },
            {
                "text": "We evaluate bidirectional translations from beam search (GEN; Section 3.2), bidirectional translations from nucleus sampling (GEN-N; Holtzman et al., 2020), 13 and retrieval (RTV; Section 3.2). In addition, it is natural to concatenate the global statistical features (Section 4.2) from both GEN and RTV and we refer to this approach by GEN-RTV.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Main Results",
                "sec_num": "7.1"
            },
            {
                "text": "Our main results are presented in Table 1 . All of our models (GEN, GEN-N, RTV, GEN-RTV) outperform the previous state of the art (BUCC) by a significant margin on all language pairs. Surprisingly, RTV and GEN-RTV even outperform WikiMatrix by average F 1 score, indicating that we do not need bitext supervision to obtain high-quality lexicons.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 40,
                        "end": 41,
                        "text": "1",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Main Results",
                "sec_num": "7.1"
            },
            {
                "text": "Bitext quality. Since RTV achieves surprisingly high performance, we are interested in how much the quality of bitext affects the lexicon induction performance. We divide all retrieved bitexts with score (Eq. 1) larger than 1 equally into five sections with respect to the score, and compare the lexicon Table 2 : F 1 scores (\u00d7100) on the test set of the BUCC 2020 shared task (Rapp et al., 2020) . We use the weakly supervised algorithm (Section 4.2). The best number in each row is bolded. RTV-1 is the same as RTV in Table 1 .",
                "cite_spans": [
                    {
                        "start": 377,
                        "end": 396,
                        "text": "(Rapp et al., 2020)",
                        "ref_id": "BIBREF29"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 310,
                        "end": 311,
                        "text": "2",
                        "ref_id": null
                    },
                    {
                        "start": 526,
                        "end": 527,
                        "text": "1",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Automatic Analysis",
                "sec_num": "7.2"
            },
            {
                "text": "induction performance (Table 2 ). In the table, RTV-1 refers to the bitext of the highest quality and RTV-5 refers to the ones of the lowest quality, in terms of the margin score (Eq 1). 14 We also add a random pseudo bitext baseline (Random), where all the bitext are randomly sampled from each language pair, as well as using all retrieved sentence pairs that have scores larger than 1 (RTV-ALL).",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 29,
                        "end": 30,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Automatic Analysis",
                "sec_num": "7.2"
            },
            {
                "text": "In general, the lexicon induction performance of RTV correlates well with the quality of bitext. Even using the bitext of the lowest quality (RTV-5), it is still able to induce reasonably good bilingual lexicon, outperforming the best numbers reported by BUCC 2020 participants (Table 1 ) on average. However, RTV achieves poor performance with random bitext (Table 2 ), indicating that it is only robust to a reasonable level of noise. While this is a lowerbound on bitext quality, even random bitext does not lead to 0 F 1 since the model may align any 14 See Appendix C for examples from each tier. co-occurrences of correct word pairs even when they appear in unrelated sentences.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 285,
                        "end": 286,
                        "text": "1",
                        "ref_id": "TABREF1"
                    },
                    {
                        "start": 366,
                        "end": 367,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Automatic Analysis",
                "sec_num": "7.2"
            },
            {
                "text": "Word alignment quality. We compare the lexicon induction performance using the same set of constructed bitext (RTV) and different word aligners (Table 3 ). According to Sabet et al. ( 2020), SimAlign outperforms fast align in terms of word alignment. We observe that such a trend translates to resulting lexicon induction performance well: a significantly better word aligner can usually lead to a better induced lexicon.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 151,
                        "end": 152,
                        "text": "3",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Automatic Analysis",
                "sec_num": "7.2"
            },
            {
                "text": "Bitext quantity. We investigate how the BLI performance changes when the quantity of bitext changes (Figure 2 ). We use CRISS with nucleus sampling (GEN-N) to create different amount of bitext of the same quality. We find that with only 1% of the bitext (160K sentence pairs on average) used by GEN-N, our weakly-supervised framework outperforms the previous state of the art (BUCC; Table 1 ). The model reaches its best performance using 20% of the bitext (3.2M sentence pairs on average) and then drops slightly with even more bitext. This is likely because more bitext introduces more candidates word pairs.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 108,
                        "end": 109,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    },
                    {
                        "start": 389,
                        "end": 390,
                        "text": "1",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Automatic Analysis",
                "sec_num": "7.2"
            },
            {
                "text": "Dependence on word frequency of GEN vs. RTV.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Automatic Analysis",
                "sec_num": "7.2"
            },
            {
                "text": "We observe that retrieval-based bitext construction (RTV) works significantly better than generationbased ones (GEN and GEN-N) , in terms of lexicon induction performance (Table 1 ). To further investigate the source of such difference, we compare the performance of the RTV and GEN as a function of source word frequency or target word frequency, where the word frequency are computed from the lower-cased Wikipedia corpus. In 11 of 12 language pairs except de-fr. In 6 of 12 language pairs, GEN does better than RTV for high frequency source words. As more lower frequency words are included, GEN eventually does worse than RTV. This helps explain why the combined model GEN-RTV is even better since GEN can have an edge in high frequency words over RTV. The trend that F 1 (RTV) -F 1 (GEN) increases as more lower frequency words are included seems true for all language pairs (Appendix A).",
                "cite_spans": [
                    {
                        "start": 111,
                        "end": 126,
                        "text": "(GEN and GEN-N)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 178,
                        "end": 179,
                        "text": "1",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Automatic Analysis",
                "sec_num": "7.2"
            },
            {
                "text": "On average and for the majority of language pairs, both methods do better on low-frequency source words than high-frequency ones (Figure 3a ), which is consistent with the findings by BUCC 2020 participants (Rapp et al., 2020) .",
                "cite_spans": [
                    {
                        "start": 207,
                        "end": 226,
                        "text": "(Rapp et al., 2020)",
                        "ref_id": "BIBREF29"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 137,
                        "end": 139,
                        "text": "3a",
                        "ref_id": "FIGREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Automatic Analysis",
                "sec_num": "7.2"
            },
            {
                "text": "VECMAP. While BLI through bitext construction and word alignment clearly achieves superior performance than that through vector rotation (Table 1), we further show that the gap is larger on low-frequency words (Figure 3 ).",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 218,
                        "end": 219,
                        "text": "3",
                        "ref_id": "FIGREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Automatic Analysis",
                "sec_num": "7.2"
            },
            {
                "text": "Following the advice of Kementchedjhieva et al. (2019) that some care is needed due to the incompleteness and biases of the evaluation, we perform manual analysis of selected results. For Chinese-English translations, we uniformly sample 20 wrong lexicon entries according to the evaluation for both GEN-RTV and weakly-supervised VECMAP. Our judgments of these samples are shown in Table 4 . For GEN-RTV, 18/20 of these sampled errors are actually acceptable translations, whereas for VECMAP, only 11/20 are acceptable. This indicates that the improvement in quality may be partly limited by the incompleteness of the reference lexicon and the ground truth performance of our method might be even better. The same analysis for English-Chinese is in Appendix B. Furthermore, we randomly sample 200 source words from the MUSE zh-en test set, and compare the quality between MUSE translation and those predicted by GEN-RTV. This comparison is MUSE-favored since only MUSE source words are included. Concretely, we take the union of word pairs, construct the new ground-truth by manual judgments (i.e., removing unacceptable pairs), and evaluate the F 1 score against the constructed ground-truth (Table 5 ). The overall gap of 3 F 1 means that a higher quality benchmark is necessary to resolve further improvements over GEN-RTV. The word pairs and judgments are included in the supplementary material (Section F).",
                "cite_spans": [
                    {
                        "start": 24,
                        "end": 54,
                        "text": "Kementchedjhieva et al. (2019)",
                        "ref_id": "BIBREF20"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 388,
                        "end": 389,
                        "text": "4",
                        "ref_id": "TABREF3"
                    },
                    {
                        "start": 1200,
                        "end": 1201,
                        "text": "5",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Ground-truth Analysis",
                "sec_num": "7.3"
            },
            {
                "text": "We evaluate different word alignment methods (Table 6 ) on existing word alignment datasets, 15 15 http://www-i6.informatik.rwth-aachen. de/goldAlignment (de-en); https://web.eecs.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 52,
                        "end": 53,
                        "text": "6",
                        "ref_id": "TABREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Word Alignment Results",
                "sec_num": "8"
            },
            {
                "text": "de-en en-fr en-hi ro-en following Sabet et al. (2020) . We investigate four language pairs: German-English (de-en), English-French (en-fr), English-Hindi (en-hi) and Romanian-English (ro-en). We find that the CRISS-based SimAlign already achieves competitive performance with the state-of-the-art method (Garg et al., 2019) which requires real bitext for training. By ensembling the argmax and itermax CRISS-based SimAlign results (Section 5), we set the new state of the art of word alignment without using any bitext supervision. However, by substituting the CRISS-based SimAlign in the BLI pipeline with our aligner, we obtain an average F 1 score of 73.0 for GEN-RTV, which does not improve over the result of 73.3 achieved by CRISS-based SimAlign (Table 1), indicating that further effort is required to take the advantage of the improved word aligner.",
                "cite_spans": [
                    {
                        "start": 34,
                        "end": 53,
                        "text": "Sabet et al. (2020)",
                        "ref_id": "BIBREF31"
                    },
                    {
                        "start": 304,
                        "end": 323,
                        "text": "(Garg et al., 2019)",
                        "ref_id": "BIBREF14"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model",
                "sec_num": null
            },
            {
                "text": "GIZA++ \u2020 0.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model",
                "sec_num": null
            },
            {
                "text": "We present a direct and effective framework for BLI with unsupervised bitext mining and word alignment, which sets a new state of the art on the task. From the perspective of pretrained multilingual models (Conneau et al., 2019; Liu et al., 2020; Tran et al., 2020, inter alia) , our work shows that they have successfully captured information about word translation that can be extracted using similarity based alignment and refinement. Although BLI is only about word types, it strongly benefits from contextualized reasoning at the token level.",
                "cite_spans": [
                    {
                        "start": 206,
                        "end": 228,
                        "text": "(Conneau et al., 2019;",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 229,
                        "end": 246,
                        "text": "Liu et al., 2020;",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 247,
                        "end": 277,
                        "text": "Tran et al., 2020, inter alia)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discussion",
                "sec_num": "9"
            },
            {
                "text": "umich.edu/ \u02dcmihalcea/wpt (en-fr and ro-en); https: //web.eecs.umich.edu/ \u02dcmihalcea/wpt05 (enhi)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discussion",
                "sec_num": "9"
            },
            {
                "text": "A Language-Specific Analysis While Figure 3 shows the average trend of F 1 scores with respect to the portion of source words or target words kept, we present such plots for each language pair in Figure 4 and 5. The trend of each separate method is inconsistent, which is consistent to the findings by BUCC 2020 participants (Rapp et al., 2020) . However, the conclusion that RTV gains more from low-frequency words still holds for most language pairs. B Acceptability Judgments for en \u2192 zh We present error analysis for the induced lexicon for English to Chinese translations (Table 7 ) using the same method as Table 4 . In this direction, many of the unacceptable cases are copying English words as their Chinese translations, which is also observed by Rapp et al. (2020) . This is due to an idiosyncrasy of the evaluation data where many English words are considered acceptable Chinese translations of the same words.",
                "cite_spans": [
                    {
                        "start": 325,
                        "end": 344,
                        "text": "(Rapp et al., 2020)",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 756,
                        "end": 774,
                        "text": "Rapp et al. (2020)",
                        "ref_id": "BIBREF29"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 42,
                        "end": 43,
                        "text": "3",
                        "ref_id": "FIGREF2"
                    },
                    {
                        "start": 203,
                        "end": 204,
                        "text": "4",
                        "ref_id": null
                    },
                    {
                        "start": 584,
                        "end": 585,
                        "text": "7",
                        "ref_id": "TABREF5"
                    },
                    {
                        "start": 619,
                        "end": 620,
                        "text": "4",
                        "ref_id": "TABREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Appendices",
                "sec_num": null
            },
            {
                "text": "We show examples of mined bitext with different quality (Table 8 ), where the mined bitexts are di-vided into 5 sections with respect to the similaritybased margin score (Eq 1). The Chinese sentences are automatically converted to traditional Chinese alphabets using chinese converter,16 to keep consistent with the MUSE dataset. Based on our knowledge about these languages, we see that the RTV-1 mostly consists of correct translations. While the other sections of bitext are of less quality, sentences within a pair are highly related or can be even partially aligned; therefore our bitext mining and alignment framework can still extract high-quality lexicon from such imperfect bitext.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 63,
                        "end": 64,
                        "text": "8",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "C Examples for Bitext in Different Sections",
                "sec_num": null
            },
            {
                "text": "Precision@1 (P@1) is a widely applied metric to evaluate bilingual lexicon induction (Smith et al., 2017; Lample et al., 2018; Artetxe et al., 2019, inter alia) , therefore we compare our models with existing approaches in terms of P@1 as well (Table 9 ). Our fully unsupervised method with retrieval-based bitext outperforms the previous state of the art (Artetxe et al., 2019) by 4.1 average P@1, and achieve competitive or superior performance on all investigated language pairs.",
                "cite_spans": [
                    {
                        "start": 85,
                        "end": 105,
                        "text": "(Smith et al., 2017;",
                        "ref_id": "BIBREF36"
                    },
                    {
                        "start": 106,
                        "end": 126,
                        "text": "Lample et al., 2018;",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 127,
                        "end": 160,
                        "text": "Artetxe et al., 2019, inter alia)",
                        "ref_id": null
                    },
                    {
                        "start": 356,
                        "end": 378,
                        "text": "(Artetxe et al., 2019)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 251,
                        "end": 252,
                        "text": "9",
                        "ref_id": "TABREF6"
                    }
                ],
                "eq_spans": [],
                "section": "D Results: P@1 on the MUSE Dataset",
                "sec_num": null
            },
            {
                "text": "To understand the remaining errors, we randomly sampled 400 word pairs from the induced lexicon and compare them to ground truth as and Google Translate via =googletranslate(A1, \"zh\", \"en\"). All error cases are included in Table 10. In overall precision, our induced lexicon is comparable to the output of Google translate API where there are 17 errors for GEN-RTV 14 errors for Google and 4 common errors. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E Error analysis",
                "sec_num": null
            },
            {
                "text": "https://github.com/pytorch/fairseq/ tree/master/examples/criss",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://github.com/facebookresearch/ faiss",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "We used max-score(Artetxe and Schwenk, 2019a) as it strongly outperforms the other methods they proposed.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "We use \u03bb = 20. This reduces the effect of noisy alignment: the most extreme case is that both mat(s, t) and coc(s, t) are 1, but it is probably not desirable despite the high matched ratio of 1.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://github.com/facebookresearch/ MUSE",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://github.com/artetxem/VecMap",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://github.com/facebookresearch/ fastText",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://github.com/facebookresearch/ fastText/blob/master/docs/crawl-vectors. md; that is, our VECMAP baselines have the same data availability with our main results.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://github.com/facebookresearch/ LASER/tree/master/tasks/WikiMatrix",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "We sample from the smallest word set whose cumulative probability mass exceeds 0.5 for next words.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://pypi.org/project/ chinese-converter/",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "We thank Chau Tran for help with pretrained CRISS models, as well as Mikel Artetxe, Kevin Gimpel, Karen Livescu, Jiayuan Mao and anonymous reviewers for their valuable feedback on this work.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgment",
                "sec_num": null
            },
            {
                "text": "RTV-1",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "annex",
                "sec_num": null
            },
            {
                "text": "Cold climates may present special challenges. \u5f88\u986f\u7136,\u66fe\u7d93\u5728\u67d0\u500b\u5834\u5408\u9054\u6210\u4e86\u5176\u6240\u4e0d\u77e5\u9053\u7684\u67d0\u7a2e\u5354\u8b70\u3002I thought they'd come to some kind of an agreement.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "\u5bd2\u51b7\u6c23\u5019\u53ef\u80fd\u6703\u5e36\u4f86\u7279\u6b8a\u6311\u6230\u3002",
                "sec_num": null
            },
            {
                "text": "The plotline is somewhat different from the first series.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "\u5287\u60c5\u767c\u5c55\u9806\u5e8f\u8207\u539f\u4f5c\u6f2b\u756b\u6709\u4e9b\u4e0d\u540c\u3002",
                "sec_num": null
            },
            {
                "text": "He also made sketches and paintings.zh-en \u6b64\u7bc0\u76ee\u88ab\u6279\u8a55\u70ba\u5ba3\u63da\u507d\u79d1\u5b78\u548c\u91ce\u53f2\u3002 The book was criticized for misrepresenting nutritional science.RTV-2",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "\u4ed6\u4e5f\u5275\u4f5c\u904e\u6cb9\u756b\u548c\u58c1\u756b\u3002",
                "sec_num": null
            },
            {
                "text": "Kawagoe Sports Park Athletics Stadium \u4ed6\u662f\u5979\u7684\u795e\u8056\u91ab\u5e2b\u548c\u4fdd\u8b77\u8005\u3002 He's her protector and her provider. \u5176\u5f8c\u4ee55,000\u82f1\u938a\u8f49\u6703\u5230\u76e7\u9813\u3002 He later returned to Morton for \u00a315,000.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "\u5a01\u85cd\u753a\u9ad4\u80b2\u904b\u52d5\u5834",
                "sec_num": null
            },
            {
                "text": "Lawrence and Joanna are the play's two major characters.zh-en \u4e00\u822c\u4e0a\u6c92\u6709\u6703\u54e1\u52a0\u5165\u5230\u6bcd\u653f\u9ee8\u3002 Voters do not register as members of political parties.RTV-3",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "\u6eec\u751f\u548c\u963f\u5bf6\u662f\u5c0f\u8aaa\u7684\u5169\u500b\u4e3b\u8981\u4eba\u7269\u3002",
                "sec_num": null
            },
            {
                "text": "He was formerly an editor of \"The New York Times Book Review\" . 48V\u5fae\u6df7\u7cfb\u7d71\u4e3b\u8981\u7531\u4ee5\u4e0b\u7d44\u4ef6\u69cb\u6210:The M120 mortar system consists of the following major components: \u5176\u5f8c\u4ee55,000\u82f1\u938a\u8f49\u6703\u5230\u76e7\u9813\u3002 He later returned to Morton for \u00a315,000.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "\u66fe\u4efb\u300a\u7d10\u7d04\u6642\u5831\u300b\u66f8\u8a55\u4eba\u3002",
                "sec_num": null
            },
            {
                "text": "and arrived at Hobart Town on 8 November.zh-en 1261\u5e74,\u62c9\u4e01\u5e1d\u570b\u88ab\u63a8\u7ffb,\u6771\u7f85\u99ac\u5e1d\u570b\u5fa9\u570b\u3002 The Byzantine Empire was fully reestablished in 1261.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "2\u670825\u65e5\u5f9e\u9999\u6e2f\u62b5\u9054\u6c55\u982d",
                "sec_num": null
            },
            {
                "text": "\u800c\u9019\u6b21\u822a\u884c\u4e5f\u8b49\u660e\u4ed6\u7684\u6307\u8cac\u662f\u6b63\u78ba\u7684\u3002 This proved that he was clearly innocent of the charges.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "RTV-4",
                "sec_num": null
            },
            {
                "text": "A cut-down version was made available for downloading.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "\u4e26\u5df2\u7d93\u653e\u51fa\u622a\u9762\u548c\u8a66\u7528\u7248\u3002",
                "sec_num": null
            },
            {
                "text": "It consists of 21 large gears and a 13 meters pendulum.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "\u5b83\u91cd370\u514b,\u7531\u4e00\u6839\u628a\u548c\u4e5d\u6839\u7d22\u7d44\u6210\u3002",
                "sec_num": null
            },
            {
                "text": "Still, the German performance was not flawless.zh-en \u6b64\u8981\u585e\u4e5f\u7528\u4ee5\u93ae\u58d3\u7684\u90e8\u843d\u3002 that were used by nomads in the region.RTV-5",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "\u6d3e\u8def\u5728\u968a\u4e2d\u7684\u5275\u9020\u529b\u53ef\u8b02\u7121\u51fa\u5176\u53f3,\u529f\u4e0d\u53ef\u62b9\u3002",
                "sec_num": null
            },
            {
                "text": "In those 18 games, the visiting team won only three times.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "\u4e0d\u904e,\u901931\u6b21\u51fa\u5834\u53ea\u670911\u6b21\u662f\u9996\u767c\u3002",
                "sec_num": null
            },
            {
                "text": "He was born in Frewsburg, New York, USA. 2014\u5e747\u670814\u65e5,\u7d44\u5718\u6210\u70ba\u4e00\u54e1\u3002 Roy joined the group on 4/18/98.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "\u751f\u65bc\u7f8e\u570b\u7d10\u7d04\u5dde\u5e03\u9b6f\u514b\u6797\u3002",
                "sec_num": null
            },
            {
                "text": "Far above, the lonely hawk floating.de-en Von 1988 bis 1991 lebte er in Venedig.From 1988-1991 he lived in Venice.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "\u76fe\u4e0a\u6709\u5954\u8d70\u4e2d\u7684\u7345\u5b50\u3002",
                "sec_num": null
            },
            {
                "text": "Der Film beginnt mit folgendem Zitat:The movie begins with the following statement: Geschichte von Saint Vincent und den Grenadinen History of Saint Vincent and the Grenadines Die Spuren des Kriegs sind noch allgegenw\u00e4rtig. Some signs of the people are still there. Saint-Paul (Savoie) Saint-Paul, Savoie de-en Nanderbarsche sind nicht brutpflegend.Oxpeckers are fairly gregarious.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "RTV-1",
                "sec_num": null
            },
            {
                "text": "Dort begegnet sie Raymond und seiner Tochter Sarah.There she meets Sara and her husband. Armansperg wurde zum Premierminister ernannt.Mansur was appointed the prime minister. Diese Arbeit wird von den M\u00e4nnchen ausgef\u00fchrt.Parental care is performed by males.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "RTV-2",
                "sec_num": null
            },
            {
                "text": "House of Limburg-Stirum de-en Es gibt mehrere Anbieter der Komponenten.There are several components to the site.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "August von Limburg-Stirum",
                "sec_num": null
            },
            {
                "text": "Doch dann werden sie von Piraten angegriffen. They are attacked by Saracen pirates. Wird nicht die tiefste -also meist 6.The shortest, probably five. Ihre Bl\u00fcte hatte sie zwischen 1976 und 1981.The crop trebled between 1955 and 1996. Er brachte Reliquien von der Hl.Eulogies were given by the Rev.de-en Gespielt wird meistens Mitte Juni.It is played principally on weekends.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "RTV-3",
                "sec_num": null
            },
            {
                "text": "Schuppiger Schlangenstern Plains garter snake Das Artwork stammt von Dave Field.The artwork is by Mike Egan. Ammonolyse ist eine der Hydrolyse analoge Reaktion, Hydroxylation is an oxidative process. Die Pellenz gliedert sich wie folgt:The Pellenz is divided as follows:de-en Auch Nicolau war praktizierender Katholik. Cassar was a practicing Roman Catholic.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "RTV-4",
                "sec_num": null
            },
            {
                "text": "Im Jahr 2018 lag die Mitgliederzahl bei 350. The membership in 2017 numbered around 1,000. Er tr\u00e4gt die Fahrgestellnummer TNT 102.It carries the registration number AWK 230. Als Moderator war Benjamin Jaworskyj angereist.Dmitry Nagiev appeared as the presenter. Benachbarte Naturr\u00e4ume und Landschaften sind:Neighboring hydrographic watersheds are:Table 8 : Examples of bitext in different sections (Section 7.2). We see that tier 1 has majority parallel sentences whereas lower tiers have mostly similar but not parallel sentences.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 353,
                        "end": 354,
                        "text": "8",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "RTV-5",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "On the use of comparable corpora to improve SMT performance",
                "authors": [
                    {
                        "first": "Sadaf",
                        "middle": [],
                        "last": "Abdul",
                        "suffix": ""
                    },
                    {
                        "first": "-Rauf",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Holger",
                        "middle": [],
                        "last": "Schwenk",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proc. of EACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sadaf Abdul-Rauf and Holger Schwenk. 2009. On the use of comparable corpora to improve SMT perfor- mance. In Proc. of EACL.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Learning principled bilingual mappings of word embeddings while preserving monolingual invariance",
                "authors": [
                    {
                        "first": "Mikel",
                        "middle": [],
                        "last": "Artetxe",
                        "suffix": ""
                    },
                    {
                        "first": "Gorka",
                        "middle": [],
                        "last": "Labaka",
                        "suffix": ""
                    },
                    {
                        "first": "Eneko",
                        "middle": [],
                        "last": "Agirre",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proc. of EMNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2016. Learning principled bilingual mappings of word em- beddings while preserving monolingual invariance. In Proc. of EMNLP.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings",
                "authors": [
                    {
                        "first": "Mikel",
                        "middle": [],
                        "last": "Artetxe",
                        "suffix": ""
                    },
                    {
                        "first": "Gorka",
                        "middle": [],
                        "last": "Labaka",
                        "suffix": ""
                    },
                    {
                        "first": "Eneko",
                        "middle": [],
                        "last": "Agirre",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proc. of ACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2018. A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings. In Proc. of ACL.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Bilingual lexicon induction through unsupervised machine translation",
                "authors": [
                    {
                        "first": "Mikel",
                        "middle": [],
                        "last": "Artetxe",
                        "suffix": ""
                    },
                    {
                        "first": "Gorka",
                        "middle": [],
                        "last": "Labaka",
                        "suffix": ""
                    },
                    {
                        "first": "Eneko",
                        "middle": [],
                        "last": "Agirre",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proc. of ACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2019. Bilingual lexicon induction through unsupervised machine translation. In Proc. of ACL.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Marginbased parallel corpus mining with multilingual sentence embeddings",
                "authors": [
                    {
                        "first": "Mikel",
                        "middle": [],
                        "last": "Artetxe",
                        "suffix": ""
                    },
                    {
                        "first": "Holger",
                        "middle": [],
                        "last": "Schwenk",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proc. of ACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mikel Artetxe and Holger Schwenk. 2019a. Margin- based parallel corpus mining with multilingual sen- tence embeddings. In Proc. of ACL.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Massively multilingual sentence embeddings for zeroshot cross-lingual transfer and beyond",
                "authors": [
                    {
                        "first": "Mikel",
                        "middle": [],
                        "last": "Artetxe",
                        "suffix": ""
                    },
                    {
                        "first": "Holger",
                        "middle": [],
                        "last": "Schwenk",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "TACL",
                "volume": "7",
                "issue": "",
                "pages": "597--610",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mikel Artetxe and Holger Schwenk. 2019b. Mas- sively multilingual sentence embeddings for zero- shot cross-lingual transfer and beyond. TACL, 7:597-610.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "The wacky wide web: a collection of very large linguistically processed webcrawled corpora. Language resources and evaluation",
                "authors": [
                    {
                        "first": "Marco",
                        "middle": [],
                        "last": "Baroni",
                        "suffix": ""
                    },
                    {
                        "first": "Silvia",
                        "middle": [],
                        "last": "Bernardini",
                        "suffix": ""
                    },
                    {
                        "first": "Adriano",
                        "middle": [],
                        "last": "Ferraresi",
                        "suffix": ""
                    },
                    {
                        "first": "Eros",
                        "middle": [],
                        "last": "Zanchetta",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "",
                "volume": "43",
                "issue": "",
                "pages": "209--226",
                "other_ids": {
                    "DOI": [
                        "10.1007/s10579-009-9081-4.pdf&casa_token=LvikP7HOIrkAAAAA:mf54fH6Gy1ptageR0eiMJ41i-xgKvbc0kZAfnfYWGEm3ixhNyhpbUSTyZQ7YGTojKo62_tZXRcuGgE_Zgb0"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The wacky wide web: a collection of very large linguistically processed web- crawled corpora. Language resources and evalua- tion, 43(3):209-226.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Enriching word vectors with subword information",
                "authors": [
                    {
                        "first": "Piotr",
                        "middle": [],
                        "last": "Bojanowski",
                        "suffix": ""
                    },
                    {
                        "first": "Edouard",
                        "middle": [],
                        "last": "Grave",
                        "suffix": ""
                    },
                    {
                        "first": "Armand",
                        "middle": [],
                        "last": "Joulin",
                        "suffix": ""
                    },
                    {
                        "first": "Tomas",
                        "middle": [],
                        "last": "Mikolov",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "TACL",
                "volume": "5",
                "issue": "",
                "pages": "135--146",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching word vectors with subword information. TACL, 5:135-146.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "The mathematics of statistical machine translation: Parameter estimation",
                "authors": [
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Peter",
                        "suffix": ""
                    },
                    {
                        "first": "Stephen",
                        "middle": [
                            "A"
                        ],
                        "last": "Brown",
                        "suffix": ""
                    },
                    {
                        "first": "Vincent",
                        "middle": [
                            "J"
                        ],
                        "last": "Della Pietra",
                        "suffix": ""
                    },
                    {
                        "first": "Robert",
                        "middle": [
                            "L"
                        ],
                        "last": "Della Pietra",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Mercer",
                        "suffix": ""
                    }
                ],
                "year": 1993,
                "venue": "Computational Linguistics",
                "volume": "19",
                "issue": "2",
                "pages": "263--311",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The math- ematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263- 311.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Unsupervised cross-lingual representation learning at scale",
                "authors": [
                    {
                        "first": "Alexis",
                        "middle": [],
                        "last": "Conneau",
                        "suffix": ""
                    },
                    {
                        "first": "Kartikay",
                        "middle": [],
                        "last": "Khandelwal",
                        "suffix": ""
                    },
                    {
                        "first": "Naman",
                        "middle": [],
                        "last": "Goyal",
                        "suffix": ""
                    },
                    {
                        "first": "Vishrav",
                        "middle": [],
                        "last": "Chaudhary",
                        "suffix": ""
                    },
                    {
                        "first": "Guillaume",
                        "middle": [],
                        "last": "Wenzek",
                        "suffix": ""
                    },
                    {
                        "first": "Francisco",
                        "middle": [],
                        "last": "Guzm\u00e1n",
                        "suffix": ""
                    },
                    {
                        "first": "Edouard",
                        "middle": [],
                        "last": "Grave",
                        "suffix": ""
                    },
                    {
                        "first": "Myle",
                        "middle": [],
                        "last": "Ott",
                        "suffix": ""
                    },
                    {
                        "first": "Luke",
                        "middle": [],
                        "last": "Zettlemoyer",
                        "suffix": ""
                    },
                    {
                        "first": "Veselin",
                        "middle": [],
                        "last": "Stoyanov",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1911.02116"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettle- moyer, and Veselin Stoyanov. 2019. Unsupervised cross-lingual representation learning at scale. arXiv preprint arXiv:1911.02116.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
                "authors": [
                    {
                        "first": "Jacob",
                        "middle": [],
                        "last": "Devlin",
                        "suffix": ""
                    },
                    {
                        "first": "Ming-Wei",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "Kenton",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Kristina",
                        "middle": [],
                        "last": "Toutanova",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proc. of NAACL-HLT",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proc. of NAACL-HLT.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Improving zero-shot learning by mitigating the hubness problem",
                "authors": [
                    {
                        "first": "Georgiana",
                        "middle": [],
                        "last": "Dinu",
                        "suffix": ""
                    },
                    {
                        "first": "Angeliki",
                        "middle": [],
                        "last": "Lazaridou",
                        "suffix": ""
                    },
                    {
                        "first": "Marco",
                        "middle": [],
                        "last": "Baroni",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proc. of ICLR",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Georgiana Dinu, Angeliki Lazaridou, and Marco Ba- roni. 2015. Improving zero-shot learning by mitigat- ing the hubness problem. In Proc. of ICLR.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "A simple, fast, and effective reparameterization of IBM model 2",
                "authors": [
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Dyer",
                        "suffix": ""
                    },
                    {
                        "first": "Victor",
                        "middle": [],
                        "last": "Chahuneau",
                        "suffix": ""
                    },
                    {
                        "first": "Noah",
                        "middle": [
                            "A"
                        ],
                        "last": "Smith",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proc. of NAACL-HLT",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chris Dyer, Victor Chahuneau, and Noah A. Smith. 2013. A simple, fast, and effective reparameteriza- tion of IBM model 2. In Proc. of NAACL-HLT.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "An empirical analysis of nmt-derived interlingual embeddings and their use in parallel sentence identification",
                "authors": [
                    {
                        "first": "Cristina",
                        "middle": [],
                        "last": "Espana-Bonet",
                        "suffix": ""
                    },
                    {
                        "first": "Ad\u00e1m",
                        "middle": [
                            "Csaba"
                        ],
                        "last": "Varga",
                        "suffix": ""
                    },
                    {
                        "first": "Alberto",
                        "middle": [],
                        "last": "Barr\u00f3n-Cedeno",
                        "suffix": ""
                    },
                    {
                        "first": "Josef",
                        "middle": [],
                        "last": "Van Genabith",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "IEEE Journal of Selected Topics in Signal Processing",
                "volume": "11",
                "issue": "8",
                "pages": "1340--1350",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Cristina Espana-Bonet, Ad\u00e1m Csaba Varga, Alberto Barr\u00f3n-Cedeno, and Josef van Genabith. 2017. An empirical analysis of nmt-derived interlingual em- beddings and their use in parallel sentence identifi- cation. IEEE Journal of Selected Topics in Signal Processing, 11(8):1340-1350.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Jointly learning to align and translate with transformer models",
                "authors": [
                    {
                        "first": "Sarthak",
                        "middle": [],
                        "last": "Garg",
                        "suffix": ""
                    },
                    {
                        "first": "Stephan",
                        "middle": [],
                        "last": "Peitz",
                        "suffix": ""
                    },
                    {
                        "first": "Udhyakumar",
                        "middle": [],
                        "last": "Nallasamy",
                        "suffix": ""
                    },
                    {
                        "first": "Matthias",
                        "middle": [],
                        "last": "Paulik",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proc. of EMNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sarthak Garg, Stephan Peitz, Udhyakumar Nallasamy, and Matthias Paulik. 2019. Jointly learning to align and translate with transformer models. In Proc. of EMNLP.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Effective parallel corpus mining using bilingual sentence embeddings",
                "authors": [
                    {
                        "first": "Mandy",
                        "middle": [],
                        "last": "Guo",
                        "suffix": ""
                    },
                    {
                        "first": "Qinlan",
                        "middle": [],
                        "last": "Shen",
                        "suffix": ""
                    },
                    {
                        "first": "Yinfei",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Heming",
                        "middle": [],
                        "last": "Ge",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Cer",
                        "suffix": ""
                    },
                    {
                        "first": "Gustavo",
                        "middle": [],
                        "last": "Hernandez Abrego",
                        "suffix": ""
                    },
                    {
                        "first": "Keith",
                        "middle": [],
                        "last": "Stevens",
                        "suffix": ""
                    },
                    {
                        "first": "Noah",
                        "middle": [],
                        "last": "Constant",
                        "suffix": ""
                    },
                    {
                        "first": "Yun-Hsuan",
                        "middle": [],
                        "last": "Sung",
                        "suffix": ""
                    },
                    {
                        "first": "Brian",
                        "middle": [],
                        "last": "Strope",
                        "suffix": ""
                    },
                    {
                        "first": "Ray",
                        "middle": [],
                        "last": "Kurzweil",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proc. of WMT",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mandy Guo, Qinlan Shen, Yinfei Yang, Heming Ge, Daniel Cer, Gustavo Hernandez Abrego, Keith Stevens, Noah Constant, Yun-Hsuan Sung, Brian Strope, and Ray Kurzweil. 2018. Effective parallel corpus mining using bilingual sentence embeddings. In Proc. of WMT.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "The curious case of neural text degeneration",
                "authors": [
                    {
                        "first": "Ari",
                        "middle": [],
                        "last": "Holtzman",
                        "suffix": ""
                    },
                    {
                        "first": "Jan",
                        "middle": [],
                        "last": "Buys",
                        "suffix": ""
                    },
                    {
                        "first": "Li",
                        "middle": [],
                        "last": "Du",
                        "suffix": ""
                    },
                    {
                        "first": "Maxwell",
                        "middle": [],
                        "last": "Forbes",
                        "suffix": ""
                    },
                    {
                        "first": "Yejin",
                        "middle": [],
                        "last": "Choi",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proc. of ICLR",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The curious case of neural text degeneration. In Proc. of ICLR.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Non-adversarial unsupervised word translation",
                "authors": [
                    {
                        "first": "Yedid",
                        "middle": [],
                        "last": "Hoshen",
                        "suffix": ""
                    },
                    {
                        "first": "Lior",
                        "middle": [],
                        "last": "Wolf",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proc. of EMNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yedid Hoshen and Lior Wolf. 2018. Non-adversarial unsupervised word translation. In Proc. of EMNLP.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Billion-scale similarity search with gpus",
                "authors": [
                    {
                        "first": "Jeff",
                        "middle": [],
                        "last": "Johnson",
                        "suffix": ""
                    },
                    {
                        "first": "Matthijs",
                        "middle": [],
                        "last": "Douze",
                        "suffix": ""
                    },
                    {
                        "first": "Herv\u00e9",
                        "middle": [],
                        "last": "J\u00e9gou",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "IEEE Trans. on Big Data",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jeff Johnson, Matthijs Douze, and Herv\u00e9 J\u00e9gou. 2019. Billion-scale similarity search with gpus. IEEE Trans. on Big Data.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Loss in translation: Learning bilingual word mapping with a retrieval criterion",
                "authors": [
                    {
                        "first": "Armand",
                        "middle": [],
                        "last": "Joulin",
                        "suffix": ""
                    },
                    {
                        "first": "Piotr",
                        "middle": [],
                        "last": "Bojanowski",
                        "suffix": ""
                    },
                    {
                        "first": "Tomas",
                        "middle": [],
                        "last": "Mikolov",
                        "suffix": ""
                    },
                    {
                        "first": "Herve",
                        "middle": [],
                        "last": "Jegou",
                        "suffix": ""
                    },
                    {
                        "first": "Edouard",
                        "middle": [],
                        "last": "Grave",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proc. of EMNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Armand Joulin, Piotr Bojanowski, Tomas Mikolov, Herve Jegou, and Edouard Grave. 2018. Loss in translation: Learning bilingual word mapping with a retrieval criterion. In Proc. of EMNLP.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Lost in evaluation: Misleading benchmarks for bilingual dictionary induction",
                "authors": [
                    {
                        "first": "Yova",
                        "middle": [],
                        "last": "Kementchedjhieva",
                        "suffix": ""
                    },
                    {
                        "first": "Mareike",
                        "middle": [],
                        "last": "Hartmann",
                        "suffix": ""
                    },
                    {
                        "first": "Anders",
                        "middle": [],
                        "last": "S\u00f8gaard",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proc. of EMNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yova Kementchedjhieva, Mareike Hartmann, and An- ders S\u00f8gaard. 2019. Lost in evaluation: Misleading benchmarks for bilingual dictionary induction. In Proc. of EMNLP.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Unsupervised bitext mining and translation via self-trained contextual embeddings",
                "authors": [
                    {
                        "first": "Phillip",
                        "middle": [],
                        "last": "Keung",
                        "suffix": ""
                    },
                    {
                        "first": "Julian",
                        "middle": [],
                        "last": "Salazar",
                        "suffix": ""
                    },
                    {
                        "first": "Yichao",
                        "middle": [],
                        "last": "Lu",
                        "suffix": ""
                    },
                    {
                        "first": "Noah",
                        "middle": [
                            "A"
                        ],
                        "last": "Smith",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "TACL",
                "volume": "8",
                "issue": "",
                "pages": "828--841",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Phillip Keung, Julian Salazar, Yichao Lu, and Noah A. Smith. 2020. Unsupervised bitext mining and translation via self-trained contextual embeddings. TACL, 8:828-841.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Adam: A method for stochastic optimization",
                "authors": [
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Diederik",
                        "suffix": ""
                    },
                    {
                        "first": "Jimmy",
                        "middle": [],
                        "last": "Kingma",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Ba",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proc. of ICLR",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In Proc. of ICLR.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Word translation without parallel data",
                "authors": [
                    {
                        "first": "Guillaume",
                        "middle": [],
                        "last": "Lample",
                        "suffix": ""
                    },
                    {
                        "first": "Alexis",
                        "middle": [],
                        "last": "Conneau",
                        "suffix": ""
                    },
                    {
                        "first": "Marc'aurelio",
                        "middle": [],
                        "last": "Ranzato",
                        "suffix": ""
                    },
                    {
                        "first": "Ludovic",
                        "middle": [],
                        "last": "Denoyer",
                        "suffix": ""
                    },
                    {
                        "first": "Herv\u00e9",
                        "middle": [],
                        "last": "J\u00e9gou",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proc. of ICLR",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Guillaume Lample, Alexis Conneau, Marc'Aurelio Ranzato, Ludovic Denoyer, and Herv\u00e9 J\u00e9gou. 2018. Word translation without parallel data. In Proc. of ICLR.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "TALN/LS2N participation at the BUCC shared task: Bilingual dictionary induction from comparable corpora",
                "authors": [
                    {
                        "first": "Martin",
                        "middle": [],
                        "last": "Laville",
                        "suffix": ""
                    },
                    {
                        "first": "Amir",
                        "middle": [],
                        "last": "Hazem",
                        "suffix": ""
                    },
                    {
                        "first": "Emmanuel",
                        "middle": [],
                        "last": "Morin",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proc. of Workshop on Building and Using Comparable Corpora",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Martin Laville, Amir Hazem, and Emmanuel Morin. 2020. TALN/LS2N participation at the BUCC shared task: Bilingual dictionary induction from comparable corpora. In Proc. of Workshop on Build- ing and Using Comparable Corpora.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Multilingual denoising pre-training for neural machine translation",
                "authors": [
                    {
                        "first": "Yinhan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Jiatao",
                        "middle": [],
                        "last": "Gu",
                        "suffix": ""
                    },
                    {
                        "first": "Naman",
                        "middle": [],
                        "last": "Goyal",
                        "suffix": ""
                    },
                    {
                        "first": "Xian",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Sergey",
                        "middle": [],
                        "last": "Edunov",
                        "suffix": ""
                    },
                    {
                        "first": "Marjan",
                        "middle": [],
                        "last": "Ghazvininejad",
                        "suffix": ""
                    },
                    {
                        "first": "Mike",
                        "middle": [],
                        "last": "Lewis",
                        "suffix": ""
                    },
                    {
                        "first": "Luke",
                        "middle": [],
                        "last": "Zettlemoyer",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2001.08210"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. 2020. Multilingual denoising pre-training for neural machine translation. arXiv preprint arXiv:2001.08210.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Exploiting similarities among languages for machine translation",
                "authors": [
                    {
                        "first": "Tomas",
                        "middle": [],
                        "last": "Mikolov",
                        "suffix": ""
                    },
                    {
                        "first": "Quoc",
                        "middle": [
                            "V"
                        ],
                        "last": "Le",
                        "suffix": ""
                    },
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tomas Mikolov, Quoc V. Le, and Ilya Sutskever. 2013. Exploiting similarities among languages for ma- chine translation.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "A systematic comparison of various statistical alignment models",
                "authors": [
                    {
                        "first": "Josef",
                        "middle": [],
                        "last": "Franz",
                        "suffix": ""
                    },
                    {
                        "first": "Hermann",
                        "middle": [],
                        "last": "Och",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Ney",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Computational Linguistics",
                "volume": "29",
                "issue": "1",
                "pages": "19--51",
                "other_ids": {
                    "DOI": [
                        "10.1162/089120103321337421"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19-51.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Generating alignments using target foresight in attention-based neural machine translation",
                "authors": [
                    {
                        "first": "Jan-Thorsten",
                        "middle": [],
                        "last": "Peter",
                        "suffix": ""
                    },
                    {
                        "first": "Arne",
                        "middle": [],
                        "last": "Nix",
                        "suffix": ""
                    },
                    {
                        "first": "Hermann",
                        "middle": [],
                        "last": "Ney",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "The Prague Bulletin of Mathematical Linguistics",
                "volume": "108",
                "issue": "1",
                "pages": "27--36",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jan-Thorsten Peter, Arne Nix, and Hermann Ney. 2017. Generating alignments using target fore- sight in attention-based neural machine translation. The Prague Bulletin of Mathematical Linguistics, 108(1):27-36.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Overview of the fourth BUCC shared task: Bilingual dictionary induction from comparable corpora",
                "authors": [
                    {
                        "first": "Reinhard",
                        "middle": [],
                        "last": "Rapp",
                        "suffix": ""
                    },
                    {
                        "first": "Pierre",
                        "middle": [],
                        "last": "Zweigenbaum",
                        "suffix": ""
                    },
                    {
                        "first": "Serge",
                        "middle": [],
                        "last": "Sharoff",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proc. of Workshop on Building and Using Comparable Corpora",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Reinhard Rapp, Pierre Zweigenbaum, and Serge Sharoff. 2020. Overview of the fourth BUCC shared task: Bilingual dictionary induction from compara- ble corpora. In Proc. of Workshop on Building and Using Comparable Corpora.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "Mining the web for bilingual text",
                "authors": [
                    {
                        "first": "Philip",
                        "middle": [],
                        "last": "Resnik",
                        "suffix": ""
                    }
                ],
                "year": 1999,
                "venue": "Proc. of ACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Philip Resnik. 1999. Mining the web for bilingual text. In Proc. of ACL.",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "SimAlign: High quality word alignments without parallel training data using static and contextualized embeddings",
                "authors": [
                    {
                        "first": "Jalili",
                        "middle": [],
                        "last": "Masoud",
                        "suffix": ""
                    },
                    {
                        "first": "Philipp",
                        "middle": [],
                        "last": "Sabet",
                        "suffix": ""
                    },
                    {
                        "first": "Hinrich",
                        "middle": [],
                        "last": "Dufter",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Schutze",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Findings of EMNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Masoud Jalili Sabet, Philipp Dufter, and Hinrich Schutze. 2020. SimAlign: High quality word align- ments without parallel training data using static and contextualized embeddings. In Findings of EMNLP.",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "Filtering and mining parallel data in a joint multilingual space",
                "authors": [
                    {
                        "first": "Holger",
                        "middle": [],
                        "last": "Schwenk",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proc. of ACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Holger Schwenk. 2018. Filtering and mining parallel data in a joint multilingual space. In Proc. of ACL.",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "Wikimatrix: Mining 135M parallel sentences in 1620 language pairs from Wikipedia",
                "authors": [
                    {
                        "first": "Holger",
                        "middle": [],
                        "last": "Schwenk",
                        "suffix": ""
                    },
                    {
                        "first": "Vishrav",
                        "middle": [],
                        "last": "Chaudhary",
                        "suffix": ""
                    },
                    {
                        "first": "Shuo",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Hongyu",
                        "middle": [],
                        "last": "Gong",
                        "suffix": ""
                    },
                    {
                        "first": "Francisco",
                        "middle": [],
                        "last": "Guzm\u00e1n",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1907.05791"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Holger Schwenk, Vishrav Chaudhary, Shuo Sun, Hongyu Gong, and Francisco Guzm\u00e1n. 2019. Wiki- matrix: Mining 135M parallel sentences in 1620 language pairs from Wikipedia. arXiv preprint arXiv:1907.05791.",
                "links": null
            },
            "BIBREF34": {
                "ref_id": "b34",
                "title": "LMU bilingual dictionary induction system with word surface similarity scores for BUCC 2020",
                "authors": [
                    {
                        "first": "Silvia",
                        "middle": [],
                        "last": "Severini",
                        "suffix": ""
                    },
                    {
                        "first": "Viktor",
                        "middle": [],
                        "last": "Hangya",
                        "suffix": ""
                    },
                    {
                        "first": "Alexander",
                        "middle": [],
                        "last": "Fraser",
                        "suffix": ""
                    },
                    {
                        "first": "Hinrich",
                        "middle": [],
                        "last": "Sch\u00fctze",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proc. of Workshop on Building and Using Comparable Corpora",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Silvia Severini, Viktor Hangya, Alexander Fraser, and Hinrich Sch\u00fctze. 2020. LMU bilingual dictionary induction system with word surface similarity scores for BUCC 2020. In Proc. of Workshop on Building and Using Comparable Corpora.",
                "links": null
            },
            "BIBREF35": {
                "ref_id": "b35",
                "title": "A DOM tree alignment model for mining parallel data from the web",
                "authors": [
                    {
                        "first": "Lei",
                        "middle": [],
                        "last": "Shi",
                        "suffix": ""
                    },
                    {
                        "first": "Cheng",
                        "middle": [],
                        "last": "Niu",
                        "suffix": ""
                    },
                    {
                        "first": "Ming",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Jianfeng",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Proc. of ACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Lei Shi, Cheng Niu, Ming Zhou, and Jianfeng Gao. 2006. A DOM tree alignment model for mining par- allel data from the web. In Proc. of ACL.",
                "links": null
            },
            "BIBREF36": {
                "ref_id": "b36",
                "title": "Offline bilingual word vectors, orthogonal transformations and the inverted softmax",
                "authors": [
                    {
                        "first": "L",
                        "middle": [],
                        "last": "Samuel",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Smith",
                        "suffix": ""
                    },
                    {
                        "first": "Steven",
                        "middle": [],
                        "last": "Hp Turban",
                        "suffix": ""
                    },
                    {
                        "first": "Nils",
                        "middle": [
                            "Y"
                        ],
                        "last": "Hamblin",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Hammerla",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proc. of ICLR",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Samuel L Smith, David HP Turban, Steven Hamblin, and Nils Y Hammerla. 2017. Offline bilingual word vectors, orthogonal transformations and the inverted softmax. In Proc. of ICLR.",
                "links": null
            },
            "BIBREF37": {
                "ref_id": "b37",
                "title": "Cross-lingual retrieval for iterative self-supervised training",
                "authors": [
                    {
                        "first": "Chau",
                        "middle": [],
                        "last": "Tran",
                        "suffix": ""
                    },
                    {
                        "first": "Yuqing",
                        "middle": [],
                        "last": "Tang",
                        "suffix": ""
                    },
                    {
                        "first": "Xian",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Jiatao",
                        "middle": [],
                        "last": "Gu",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proc. of NeurIPS",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chau Tran, Yuqing Tang, Xian Li, and Jiatao Gu. 2020. Cross-lingual retrieval for iterative self-supervised training. In Proc. of NeurIPS.",
                "links": null
            },
            "BIBREF38": {
                "ref_id": "b38",
                "title": "Adding interpretable attention to neural translation models improves word alignment",
                "authors": [
                    {
                        "first": "Thomas",
                        "middle": [],
                        "last": "Zenkel",
                        "suffix": ""
                    },
                    {
                        "first": "Joern",
                        "middle": [],
                        "last": "Wuebker",
                        "suffix": ""
                    },
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Denero",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1901.11359"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Thomas Zenkel, Joern Wuebker, and John DeNero. 2019. Adding interpretable attention to neural trans- lation models improves word alignment. arXiv preprint arXiv:1901.11359.",
                "links": null
            },
            "BIBREF39": {
                "ref_id": "b39",
                "title": "Adversarial training for unsupervised bilingual lexicon induction",
                "authors": [
                    {
                        "first": "Meng",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Huanbo",
                        "middle": [],
                        "last": "Luan",
                        "suffix": ""
                    },
                    {
                        "first": "Maosong",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proc. of ACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Meng Zhang, Yang Liu, Huanbo Luan, and Maosong Sun. 2017. Adversarial training for unsupervised bilingual lexicon induction. In Proc. of ACL.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 1: Overview of the proposed retrieval-based supervised BLI framework. Best viewed in color.",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Figure2: F 1 scores (\u00d7100) on the BUCC 2020 test set, produced by our weakly-supervised framework using different amount of bitext generated by CRISS with nucleus sampling. 100% is the same as GEN-N in Table 1. For less than 100%, we uniformly sample the corresponding amount of bitext; for greater, we generate multiple translations for each source sentence.",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "Figure3: Average F 1 scores (\u00d7100) with our weaklysupervised framework across the 12 language pairs (Table 1) on the filtered BUCC 2020 test set. Results on entries with (a) the k% most frequent source words, and (b) the k% most frequent target words.",
                "uris": null,
                "fig_num": "3",
                "type_str": "figure"
            },
            "TABREF1": {
                "content": "<table><tr><td>Language</td><td/><td/><td colspan=\"3\">Weakly-Supervised</td><td/><td/><td colspan=\"2\">Unsupervised</td></tr><tr><td>Pair</td><td>BUCC</td><td colspan=\"8\">VECMAP WM GEN GEN-N RTV GEN-RTV VECMAP GEN RTV</td></tr><tr><td>de-en</td><td>61.5</td><td>37.1</td><td colspan=\"2\">71.6 70.2</td><td>67.7</td><td>73.0</td><td>74.2</td><td>22.1</td><td>62.6 66.8</td></tr><tr><td>de-fr</td><td>76.8</td><td>43.2</td><td colspan=\"2\">79.8 79.1</td><td>79.2</td><td>78.9</td><td>83.2</td><td>27.1</td><td>79.4 80.3</td></tr><tr><td>en-de</td><td>54.5</td><td>33.2</td><td colspan=\"2\">62.1 62.7</td><td>59.3</td><td>64.4</td><td>66.0</td><td>33.7</td><td>51.0 56.2</td></tr><tr><td>en-es</td><td>62.6</td><td>45.3</td><td colspan=\"2\">71.8 73.7</td><td>69.6</td><td>77.0</td><td>75.3</td><td>44.1</td><td>60.2 65.6</td></tr><tr><td>en-fr</td><td>65.1</td><td>45.4</td><td colspan=\"2\">74.4 73.1</td><td>69.9</td><td>73.4</td><td>76.3</td><td>44.8</td><td>61.9 66.3</td></tr><tr><td>en-ru</td><td>41.4</td><td>29.2</td><td colspan=\"2\">54.4 43.5</td><td>37.9</td><td>53.1</td><td>53.1</td><td>24.6</td><td>28.4 45.4</td></tr><tr><td>en-zh</td><td>49.5</td><td>31.0</td><td colspan=\"2\">67.7 64.3</td><td>56.8</td><td>69.9</td><td>68.3</td><td>12.8</td><td>51.5 51.7</td></tr><tr><td>es-en</td><td>71.1</td><td>55.5</td><td colspan=\"2\">82.3 80.3</td><td>75.8</td><td>82.8</td><td>82.6</td><td>52.4</td><td>71.4 76.4</td></tr><tr><td>fr-de</td><td>71.0</td><td>46.2</td><td colspan=\"2\">82.1 80.0</td><td>78.7</td><td>80.9</td><td>81.7</td><td>46.0</td><td>76.4 77.3</td></tr><tr><td>fr-en</td><td>53.7</td><td>51.5</td><td colspan=\"2\">80.3 79.7</td><td>76.1</td><td>80.0</td><td>83.2</td><td>50.4</td><td>72.7 75.9</td></tr><tr><td>ru-en</td><td>57.1</td><td>44.8</td><td colspan=\"2\">72.7 61.1</td><td>59.2</td><td>72.7</td><td>72.9</td><td>42.1</td><td>51.8 68.0</td></tr><tr><td>zh-en</td><td>36.9</td><td>36.1</td><td colspan=\"2\">64.1 52.6</td><td>50.6</td><td>62.5</td><td>62.5</td><td>34.4</td><td>34.3 48.1</td></tr><tr><td>average</td><td>58.4</td><td>41.5</td><td colspan=\"2\">72.0 68.4</td><td>65.1</td><td>72.4</td><td>73.3</td><td>36.2</td><td>58.5 64.8</td></tr><tr><td/><td/><td/><td colspan=\"4\">Bitext Quality: High \u2192 Low</td><td/><td/></tr><tr><td/><td colspan=\"8\">Lang. RTV-1 RTV-2 RTV-3 RTV-4 RTV-5 Random RTV-ALL</td></tr><tr><td/><td>de-en</td><td>73.0</td><td>67.9</td><td>65.8</td><td>64.5</td><td>63.1</td><td>37.8</td><td>70.9</td></tr><tr><td/><td>de-fr</td><td>78.9</td><td>74.2</td><td>70.8</td><td>69.5</td><td>67.3</td><td>60.6</td><td>79.4</td></tr><tr><td/><td>en-de</td><td>64.4</td><td>59.7</td><td>58.1</td><td>56.6</td><td>57.2</td><td>36.5</td><td>62.5</td></tr><tr><td/><td>en-es</td><td>77.0</td><td>76.5</td><td>73.7</td><td>68.4</td><td>66.1</td><td>43.3</td><td>75.3</td></tr><tr><td/><td>en-fr</td><td>73.4</td><td>70.5</td><td>67.9</td><td>65.7</td><td>65.5</td><td>47.8</td><td>68.3</td></tr><tr><td/><td>en-ru</td><td>53.1</td><td>48.0</td><td>44.2</td><td>40.8</td><td>41.0</td><td>15.0</td><td>51.3</td></tr><tr><td/><td>en-zh</td><td>69.9</td><td>59.6</td><td>66.1</td><td>60.1</td><td>61.3</td><td>48.2</td><td>67.6</td></tr><tr><td/><td>es-en</td><td>82.8</td><td>82.4</td><td>79.6</td><td>74.2</td><td>72.3</td><td>44.4</td><td>81.1</td></tr><tr><td/><td>fr-de</td><td>80.9</td><td>76.9</td><td>73.2</td><td>74.7</td><td>74.5</td><td>64.7</td><td>79.1</td></tr><tr><td/><td>fr-en</td><td>80.0</td><td>79.0</td><td>74.2</td><td>72.6</td><td>71.6</td><td>50.1</td><td>79.4</td></tr><tr><td/><td>ru-en</td><td>72.7</td><td>66.8</td><td>60.5</td><td>55.8</td><td>54.0</td><td>14.7</td><td>71.0</td></tr><tr><td/><td>zh-en</td><td>62.5</td><td>58.0</td><td>54.1</td><td>50.9</td><td>49.3</td><td>13.6</td><td>61.3</td></tr><tr><td/><td>avg.</td><td>72.4</td><td>68.3</td><td>65.7</td><td>62.8</td><td>61.9</td><td>39.7</td><td>70.6</td></tr></table>",
                "type_str": "table",
                "text": "F 1 scores (\u00d7100) on the BUCC 2020 test set(Rapp et al., 2020). The best number in each row is bolded.",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table><tr><td/><td>GEN-RTV</td><td/><td>VECMAP</td></tr><tr><td>\u5009\u5eab</td><td colspan=\"2\">depot \u7533\u660e</td><td>endorsing</td></tr><tr><td>\u6d6a\u8cbb</td><td colspan=\"3\">wasting \u689d\u4ef6 preconditions ?</td></tr><tr><td>\u80cc\u9762</td><td colspan=\"2\">reverse \u79fb\u52d5</td><td>moving</td></tr><tr><td>\u5634\u5df4</td><td colspan=\"2\">mouths \u5929\u6d25</td><td>shanghai</td></tr><tr><td>\u53ef\u7b11</td><td colspan=\"2\">laughable \u500b\u6848</td><td>cases</td></tr><tr><td>\u96b1\u85cf</td><td colspan=\"2\">conceal \u767e\u5408</td><td>peony</td></tr><tr><td>\u8654\u8aa0</td><td colspan=\"2\">devout \u7533\u5831</td><td>filing</td></tr><tr><td>\u7d14\u6de8</td><td colspan=\"2\">purified ? \u8eca\u5ec2</td><td>carriages</td></tr><tr><td>\u622a\u6b62</td><td colspan=\"2\">deadline \u6d77\u8349</td><td>seaweed</td></tr><tr><td>\u5c0d\u5916</td><td colspan=\"2\">foreign ? \u5c65\u6b77</td><td>r\u00e9sum\u00e9</td></tr><tr><td>\u937e</td><td colspan=\"2\">clocks \u6536\u5bb9\u6240</td><td>asylums</td></tr><tr><td>\u52aa\u529b</td><td colspan=\"3\">effort \u958b\u5e55 soft-opened</td></tr><tr><td>\u8266</td><td colspan=\"2\">ships \u6709\u5f62</td><td>intangible</td></tr><tr><td>\u5dde</td><td colspan=\"2\">states \u5c0f\u5200</td><td>penknife</td></tr><tr><td>\u53d7\u50b7</td><td colspan=\"2\">wounded \u9ed1\u5c71</td><td>carpathian</td></tr><tr><td>\u6ed1\u52d5</td><td colspan=\"2\">sliding \u8c61\u5fb5</td><td>symbolise</td></tr><tr><td colspan=\"3\">\u6bd2\u7406\u5b78 toxicology \u7cbe\u83ef</td><td>fluff-free</td></tr><tr><td colspan=\"4\">\u63a8\u7ffb overthrown \u540c\u8b00 conspirator</td></tr><tr><td>\u7a7f</td><td colspan=\"2\">wore \u7c4c\u78bc</td><td>bargaining</td></tr><tr><td>\u79ae\u8c8c</td><td colspan=\"2\">courteous \u522e\u5200</td><td>rollers</td></tr><tr><td/><td colspan=\"3\">Data Source Precision Recall</td><td>F1</td></tr><tr><td/><td>MUSE</td><td>93.4</td><td>78.8 85.5</td></tr><tr><td/><td>GEN-RTV</td><td>96.6</td><td>71.9 82.5</td></tr><tr><td colspan=\"4\">Table 5: Comparison of Chinese-English lexicons</td></tr><tr><td colspan=\"4\">against manually labeled ground truth. The best num-</td></tr><tr><td colspan=\"3\">ber in each column is bolded.</td></tr></table>",
                "type_str": "table",
                "text": "Manually labeled acceptability judgments for random 20 error cases made by GEN-RTV (left) and VECMAP (right). and denote acceptable and unacceptable translation respectively. ? denotes word pairs that may be acceptable in rare or specific contexts.",
                "html": null,
                "num": null
            },
            "TABREF4": {
                "content": "<table><tr><td/><td>22</td><td>0.09</td><td>0.52</td><td>0.32</td></tr><tr><td>fast align  \u2020</td><td>0.30</td><td>0.16</td><td>0.62</td><td>0.32</td></tr><tr><td>Garg et al. (2019)</td><td>0.16</td><td>0.05</td><td>N/A</td><td>0.23</td></tr><tr><td>Zenkel et al. (2019)</td><td>0.21</td><td>0.10</td><td>N/A</td><td>0.28</td></tr><tr><td colspan=\"2\">SimAlign (Sabet et al., 2020)</td><td/><td/><td/></tr><tr><td>XLM-R-argmax  \u2020</td><td>0.19</td><td>0.07</td><td>0.39</td><td>0.29</td></tr><tr><td>mBART-argmax</td><td>0.20</td><td>0.09</td><td>0.45</td><td>0.29</td></tr><tr><td>CRISS-argmax  *</td><td>0.17</td><td>0.05</td><td>0.32</td><td>0.25</td></tr><tr><td>CRISS-itermax  *</td><td>0.18</td><td>0.08</td><td>0.30</td><td>0.23</td></tr><tr><td>MLP (ours)  *</td><td>0.15</td><td>0.04</td><td>0.28</td><td>0.22</td></tr></table>",
                "type_str": "table",
                "text": "Average error rate (AER) for word alignment (lower is better). The best numbers in each column are bolded. Models in the top section require ground-truth bitext, while those in the bottom section do not. * : models that involve unsupervised bitext construction. \u2020: results copied fromSabet et al. (2020).",
                "html": null,
                "num": null
            },
            "TABREF5": {
                "content": "<table><tr><td colspan=\"2\">GEN-RTV</td><td colspan=\"2\">VECMAP</td></tr><tr><td colspan=\"3\">southwestern \u897f\u5357\u90e8 spiritism</td><td>\u6276\u7b95</td></tr><tr><td>subject</td><td>\u8a71\u984c danny</td><td/><td>john</td></tr><tr><td colspan=\"2\">screenwriter \u5287\u4f5c\u5bb6 ? hubbard</td><td/><td>\u5a01\u5ec9\u65af</td></tr><tr><td>preschool</td><td>\u5b78\u9f61\u524d swizz</td><td/><td>incredible</td></tr><tr><td colspan=\"2\">palestine palestine viewing</td><td/><td>\u89c0\u8cde ?</td></tr><tr><td colspan=\"3\">strengthening \u5f37\u5316 prohibition</td><td>\u7981\u4ee4</td></tr><tr><td>zero</td><td>0 tons</td><td/><td>\u6eff\u8f09</td></tr><tr><td colspan=\"2\">insurance \u4fdd\u96aa\u516c\u53f8 pascal</td><td/><td>\u5e15\u65af\u5361</td></tr><tr><td>lines</td><td>\u7dda\u8def claudia</td><td/><td>christina</td></tr><tr><td>suburban</td><td>\u5e02\u90ca massive</td><td/><td>\u5de8\u5927</td></tr><tr><td>honorable</td><td>\u5c0a\u8cb4 ? equity</td><td/><td>\u4f30\u503c</td></tr><tr><td>placement</td><td>\u7f6e\u5165 sandy</td><td/><td>\u6c99\u8cea</td></tr><tr><td>lesotho</td><td>\u840a\u7d22\u6258 fwd</td><td/><td>\u4e0d\u904e\u5f8c</td></tr><tr><td>shanxi</td><td>shanxi taillight</td><td/><td>\u715e\u8eca\u71c8 ?</td></tr><tr><td>registration</td><td colspan=\"3\">\u6ce8\u518a horoscope \u751f\u8fb0\u516b\u5b57</td></tr><tr><td>protestors</td><td>\u6297\u8b70\u8005 busan</td><td/><td>\u4ec1\u5ddd</td></tr><tr><td>shovel</td><td>\u5277 hiding</td><td/><td>\u8eb2\u85cf</td></tr><tr><td>side</td><td>\u4e00\u65b9 entry</td><td/><td>\u95dc\u6642</td></tr><tr><td>turbulence</td><td colspan=\"2\">\u6e4d\u6d41 weekends</td><td>\u96d9\u4f11\u65e5 ?</td></tr><tr><td>omnibus</td><td colspan=\"2\">omnibus flagbearer</td><td>\u638c\u65d7</td></tr></table>",
                "type_str": "table",
                "text": "Manually labeled acceptability judgments for random 20 error cases in English to Chinese translation made by GEN-RTV and VECMAP.",
                "html": null,
                "num": null
            },
            "TABREF6": {
                "content": "<table><tr><td/><td colspan=\"2\">en-es</td><td colspan=\"2\">en-fr</td><td colspan=\"2\">en-de</td><td colspan=\"2\">en-ru</td><td>avg.</td></tr><tr><td/><td>\u2192</td><td>\u2190</td><td>\u2192</td><td>\u2190</td><td>\u2192</td><td>\u2190</td><td>\u2192</td><td>\u2190</td></tr><tr><td>Nearest neighbor  \u2020</td><td colspan=\"2\">81.9 82.8</td><td colspan=\"2\">81.6 81.7</td><td colspan=\"2\">73.3 72.3</td><td colspan=\"2\">44.3 65.6</td><td>72.9</td></tr><tr><td>Inv. nearest neighbor (Dinu et al., 2015)  \u2020</td><td colspan=\"2\">80.6 77.6</td><td colspan=\"2\">81.3 79.0</td><td colspan=\"2\">69.8 69.7</td><td colspan=\"2\">43.7 54.1</td><td>69.5</td></tr><tr><td>Inv. softmax (Smith et al., 2017)  \u2020</td><td colspan=\"2\">81.7 82.7</td><td colspan=\"2\">81.7 81.7</td><td colspan=\"2\">73.5 72.3</td><td colspan=\"2\">44.4 65.5</td><td>72.9</td></tr><tr><td>CSLS (Lample et al., 2018)  \u2020</td><td colspan=\"2\">82.5 84.7</td><td colspan=\"2\">83.3 83.4</td><td colspan=\"2\">75.6 75.3</td><td colspan=\"2\">47.4 67.2</td><td>74.9</td></tr><tr><td>Artetxe et al. (2019)  \u2020</td><td colspan=\"2\">87.0 87.9</td><td colspan=\"2\">86.0 86.2</td><td colspan=\"2\">81.9 80.2</td><td colspan=\"2\">50.4 71.3</td><td>78.9</td></tr><tr><td>RTV (ours)</td><td colspan=\"2\">89.9 93.5</td><td colspan=\"2\">84.5 89.5</td><td colspan=\"2\">83.0 88.6</td><td colspan=\"2\">54.5 80.7</td><td>83.0</td></tr><tr><td>GEN (ours)</td><td colspan=\"2\">81.5 88.7</td><td colspan=\"2\">81.6 88.6</td><td colspan=\"2\">78.9 83.7</td><td colspan=\"2\">35.4 68.2</td><td>75.8</td></tr><tr><td>src</td><td>GEN-RTV</td><td/><td colspan=\"3\">Google Trans.</td><td/><td/></tr><tr><td>\u7de8\u5287</td><td>writers</td><td/><td colspan=\"2\">&lt; screenwriter</td><td/><td/><td/></tr><tr><td>\u53ef\u7b11</td><td>laughing</td><td/><td colspan=\"2\">&lt; ridiculous</td><td/><td/><td/></tr><tr><td>\u6975\u6b0a</td><td colspan=\"2\">authoritarian</td><td colspan=\"2\">&lt; Totalitarian</td><td/><td/><td/></tr><tr><td>\u62bc\u97fb</td><td>couplets</td><td/><td colspan=\"2\">&lt; rhyme</td><td/><td/><td/></tr><tr><td>\u70d9\u5370</td><td>tattooed</td><td/><td colspan=\"2\">&lt; brand</td><td/><td/><td/></tr><tr><td>\u696d\u4e3b</td><td colspan=\"2\">homeowners</td><td colspan=\"2\">&lt; owner</td><td/><td/><td/></tr><tr><td>\u5b89\u5a1c</td><td>grande</td><td/><td colspan=\"2\">&lt; Anna</td><td/><td/><td/></tr><tr><td>\u5305\u982d</td><td>header</td><td/><td colspan=\"2\">&lt; Baotou</td><td/><td/><td/></tr><tr><td>\u7de8\u8f2f</td><td>editorial</td><td/><td>&lt; edit</td><td/><td/><td/><td/></tr><tr><td>\u9663\u98a8</td><td>winds</td><td/><td>&lt; gust</td><td/><td/><td/><td/></tr><tr><td>\u706b\u67f4</td><td>firewood</td><td/><td colspan=\"2\">&lt; matches</td><td/><td/><td/></tr><tr><td>\u76c3</td><td>bowl</td><td/><td>&lt; cup</td><td/><td/><td/><td/></tr><tr><td colspan=\"2\">\u6b66\u58eb\u9053 samurai</td><td/><td colspan=\"2\">&lt; Bushido</td><td/><td/><td/></tr><tr><td>\u8a69\u53e5</td><td>poem</td><td/><td colspan=\"2\">&lt; verse</td><td/><td/><td/></tr><tr><td>\u809a\u81cd</td><td>belly</td><td/><td colspan=\"2\">&lt; belly button</td><td/><td/><td/></tr><tr><td colspan=\"2\">\u73fe\u4ee3\u5316 modern</td><td/><td colspan=\"3\">&lt; modernization</td><td/><td/></tr><tr><td>\u611f\u5192</td><td>flu</td><td/><td>&lt; cold</td><td/><td/><td/><td/></tr><tr><td>\u5354\u5546</td><td>negotiate</td><td/><td colspan=\"2\">&gt; Consult</td><td/><td/><td/></tr><tr><td>\u7d0d\u7c73</td><td colspan=\"2\">nanometer</td><td colspan=\"2\">&gt; Nano</td><td/><td/><td/></tr><tr><td colspan=\"2\">\u985e\u4eba\u733f apes</td><td/><td colspan=\"2\">&gt; Anthropoid</td><td/><td/><td/></tr><tr><td>\u914d\u4ef6</td><td colspan=\"2\">accessories</td><td colspan=\"2\">&gt; Fitting</td><td/><td/><td/></tr><tr><td>\u532f</td><td colspan=\"2\">aggregated</td><td colspan=\"2\">&gt; exchange</td><td/><td/><td/></tr><tr><td>\u8cb8\u65b9</td><td>lenders</td><td/><td colspan=\"2\">&gt; Credit</td><td/><td/><td/></tr><tr><td>\u9006\u5dee</td><td>deficit</td><td/><td colspan=\"2\">&gt; Trade deficit</td><td/><td/><td/></tr><tr><td>\u5982\u679c</td><td>if</td><td/><td colspan=\"2\">&gt; in case</td><td/><td/><td/></tr><tr><td>\u9644\u4ef6</td><td colspan=\"2\">accessories</td><td colspan=\"2\">&gt; annex</td><td/><td/><td/></tr><tr><td>\u5be6\u7fd2</td><td>internship</td><td/><td colspan=\"2\">&gt; practice</td><td/><td/><td/></tr><tr><td>\u52a0\u5195</td><td>crowned</td><td/><td colspan=\"2\">&gt; Crown</td><td/><td/><td/></tr><tr><td>\u52a9\u7406</td><td>assistant</td><td/><td colspan=\"3\">&gt; assistant Manager</td><td/><td/></tr><tr><td colspan=\"5\">\u89aa\u548c\u6027 agreeableness &gt; Affinity</td><td/><td/><td/></tr><tr><td>\u570b\u571f</td><td>homeland</td><td/><td>&gt; land</td><td/><td/><td/><td/></tr><tr><td>\u904e\u5883</td><td>crossings</td><td/><td colspan=\"2\">Transit</td><td/><td/><td/></tr><tr><td>\u74b0\u6d41</td><td colspan=\"2\">circulation</td><td colspan=\"3\">Circumfluence</td><td/><td/></tr><tr><td>\u7f8a\u7fa4</td><td>sheep</td><td/><td colspan=\"2\">Herd</td><td/><td/><td/></tr></table>",
                "type_str": "table",
                "text": "P@1 of our lexicon inducer and previous methods on the standard MUSE test set(Lample et al., 2018), where the best number in each column is bolded. The first section consists of vector rotation-based methods, whileArtetxe et al. (2019)  conduct unsupervised machine translation and word alignment to induce bilingual lexicons. All methods are tested in the fully unsupervised setting. \u2020: numbers copied fromArtetxe et al. (2019).",
                "html": null,
                "num": null
            },
            "TABREF7": {
                "content": "<table/>",
                "type_str": "table",
                "text": "all errors cases among 400 random outputs of GEN-RTV compared to both our judgement and Google translate for reference. >: GEN-RTV unacceptable while Google Trans acceptable. <: GEN-RTV acceptable while Google Trans unacceptable. : both unacceptable.",
                "html": null,
                "num": null
            }
        }
    }
}