{
    "paper_id": "P01-1026",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:08:40.280959Z"
    },
    "title": "Organizing Encyclopedic Knowledge based on the Web and its Application to Question Answering",
    "authors": [
        {
            "first": "Atsushi",
            "middle": [],
            "last": "Fujii",
            "suffix": "",
            "affiliation": {},
            "email": "fujii@ulis.ac.jp"
        },
        {
            "first": "Tetsuya",
            "middle": [],
            "last": "Ishikawa",
            "suffix": "",
            "affiliation": {},
            "email": "ishikawa@ulis.ac.jp"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "We propose a method to generate large-scale encyclopedic knowledge, which is valuable for much NLP research, based on the Web. We first search the Web for pages containing a term in question. Then we use linguistic patterns and HTML structures to extract text fragments describing the term. Finally, we organize extracted term descriptions based on word senses and domains. In addition, we apply an automatically generated encyclopedia to a question answering system targeting the Japanese Information-Technology Engineers Examination.",
    "pdf_parse": {
        "paper_id": "P01-1026",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "We propose a method to generate large-scale encyclopedic knowledge, which is valuable for much NLP research, based on the Web. We first search the Web for pages containing a term in question. Then we use linguistic patterns and HTML structures to extract text fragments describing the term. Finally, we organize extracted term descriptions based on word senses and domains. In addition, we apply an automatically generated encyclopedia to a question answering system targeting the Japanese Information-Technology Engineers Examination.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Reflecting the growth in utilization of the World Wide Web, a number of Web-based language processing methods have been proposed within the natural language processing (NLP), information retrieval (IR) and artificial intelligence (AI) communities. A sample of these includes methods to extract linguistic resources (Fujii and Ishikawa, 2000; Resnik, 1999; Soderland, 1997) , retrieve useful information in response to user queries (Etzioni, 1997; McCallum et al., 1999) and mine/discover knowledge latent in the Web (Inokuchi et al., 1999) .",
                "cite_spans": [
                    {
                        "start": 315,
                        "end": 341,
                        "text": "(Fujii and Ishikawa, 2000;",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 342,
                        "end": 355,
                        "text": "Resnik, 1999;",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 356,
                        "end": 372,
                        "text": "Soderland, 1997)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 431,
                        "end": 446,
                        "text": "(Etzioni, 1997;",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 447,
                        "end": 469,
                        "text": "McCallum et al., 1999)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 516,
                        "end": 539,
                        "text": "(Inokuchi et al., 1999)",
                        "ref_id": "BIBREF10"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this paper, mainly from an NLP point of view, we explore a method to produce linguistic resources. Specifically, we enhance the method proposed by Fujii and Ishikawa (2000) , which extracts encyclopedic knowledge (i.e., term descriptions) from the Web.",
                "cite_spans": [
                    {
                        "start": 150,
                        "end": 175,
                        "text": "Fujii and Ishikawa (2000)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In brief, their method searches the Web for pages containing a term in question, and uses linguistic expressions and HTML layouts to extract fragments describing the term. They also use a language model to discard non-linguistic fragments. In addition, a clustering method is used to divide descriptions into a specific number of groups.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "On the one hand, their method is expected to enhance existing encyclopedias, where vocabulary size is relatively limited, and therefore the quantity problems has been resolved.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "On the other hand, encyclopedias extracted from the Web are not comparable with existing ones in terms of quality. In hand-crafted encyclopedias, term descriptions are carefully organized based on domains and word senses, which are especially effective for human usage. However, the output of Fujii's method is simply a set of unorganized term descriptions. Although clustering is optionally performed, resultant clusters are not necessarily related to explicit criteria, such as word senses and domains.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "To sum up, our belief is that by combining extraction and organization methods, we can enhance both quantity and quality of Web-based encyclopedias.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Motivated by this background, we introduce an organization model to Fujii's method and reformalize the whole framework. In other words, our proposed method is not only extraction but generation of encyclopedic knowledge.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Section 2 explains the overall design of our encyclopedia generation system, and Section 3 elaborates on our organization model. Section 4 then explores a method for applying our resultant encyclopedia to NLP research, specifically, question answering. Section 5 performs a number of experiments to evaluate our methods.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Figure 1 depicts the overall design of our system, which generates an encyclopedia for input terms.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Overview",
                "sec_num": "2.1"
            },
            {
                "text": "Our system, which is currently implemented for Japanese, consists of three modules: \"retrieval,\" \"extraction\" and \"organization,\" among which the organization module is newly introduced in this paper. In principle, the remaining two modules (\"retrieval\" and \"extraction\") are the same as proposed by Fujii and Ishikawa (2000) .",
                "cite_spans": [
                    {
                        "start": 300,
                        "end": 325,
                        "text": "Fujii and Ishikawa (2000)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overview",
                "sec_num": "2.1"
            },
            {
                "text": "In Figure 1 , terms can be submitted either on-line or off-line. A reasonable method is that while the system periodically updates the encyclopedia off-line, terms unindexed in the encyclopedia are dynamically processed in real-time usage. In either case, our system processes input terms one by one.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 10,
                        "end": 11,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Overview",
                "sec_num": "2.1"
            },
            {
                "text": "We briefly explain each module in the following three sections, respectively. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overview",
                "sec_num": "2.1"
            },
            {
                "text": "The retrieval module searches the Web for pages containing an input term, for which existing Web search engines can be used, and those with broad coverage are desirable.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Retrieval",
                "sec_num": "2.2"
            },
            {
                "text": "However, search engines performing query expansion are not always desirable, because they usually retrieve a number of pages which do not contain an input keyword. Since the extraction module (see Section 2.3) analyzes the usage of the input term in retrieved pages, pages not containing the term are of no use for our purpose.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Retrieval",
                "sec_num": "2.2"
            },
            {
                "text": "Thus, we use as the retrieval module \"Google,\" which is one of the major search engines and does not conduct query expansion 1 .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Retrieval",
                "sec_num": "2.2"
            },
            {
                "text": "In the extraction module, given Web pages containing an input term, newline codes, redundant white spaces and HTML tags that are not used in the following processes are discarded to standardize the page format.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Extraction",
                "sec_num": "2.3"
            },
            {
                "text": "Second, we approximately identify a region describing the term in the page, for which two rules are used.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Extraction",
                "sec_num": "2.3"
            },
            {
                "text": "1 http://www.google.com/",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Extraction",
                "sec_num": "2.3"
            },
            {
                "text": "The first rule is based on Japanese linguistic patterns typically used for term descriptions, such as \"X toha Y dearu (X is Y).\" Following the method proposed by Fujii and Ishikawa (2000) , we semi-automatically produced 20 patterns based on the Japanese CD-ROM World Encyclopedia (Heibonsha, 1998) , which includes approximately 80,000 entries related to various fields. It is expected that a region including the sentence that matched with one of those patterns can be a term description.",
                "cite_spans": [
                    {
                        "start": 162,
                        "end": 187,
                        "text": "Fujii and Ishikawa (2000)",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 281,
                        "end": 298,
                        "text": "(Heibonsha, 1998)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Extraction",
                "sec_num": "2.3"
            },
            {
                "text": "The second rule is based on HTML layout. In a typical case, a term in question is highlighted as a heading with tags such as <DT>, <B> and <Hx> (\"x\" denotes a digit), followed by its description. In some cases, terms are marked with the anchor <A> tag, providing hyperlinks to pages where they are described.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Extraction",
                "sec_num": "2.3"
            },
            {
                "text": "Finally, based on the region briefly identified by the above method, we extract a page fragment as a term description. Since term descriptions usually consist of a logical segment (such as a paragraph) rather than a single sentence, we extract a fragment that matched with one of the following patterns, which are sorted according to preference in descending order:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Extraction",
                "sec_num": "2.3"
            },
            {
                "text": "1. description tagged with <DD> in the case where the term is tagged with <DT>2 , 2. paragraph tagged with <P>, 3. itemization tagged with <UL>, 4. N sentences, where we empirically set N = 3.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Extraction",
                "sec_num": "2.3"
            },
            {
                "text": "As discussed in Section 1, organizing information extracted from the Web is crucial in our framework. For this purpose, we classify extracted term descriptions based on word senses and domains. Although a number of methods have been proposed to generate word senses (for example, one based on the vector space model (Sch\u00fctze, 1998)) , it is still difficult to accurately identify word senses without explicit dictionaries that define sense candidates.",
                "cite_spans": [
                    {
                        "start": 316,
                        "end": 332,
                        "text": "(Sch\u00fctze, 1998))",
                        "ref_id": "BIBREF20"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Organization",
                "sec_num": "2.4"
            },
            {
                "text": "In addition, since word senses are often associated with domains (Yarowsky, 1995) , word senses can be consequently distinguished by way of determining the domain of each description. For example, different senses for \"pipeline (processing method/transportation pipe)\" are associated with the computer and construction domains (fields), respectively.",
                "cite_spans": [
                    {
                        "start": 65,
                        "end": 81,
                        "text": "(Yarowsky, 1995)",
                        "ref_id": "BIBREF23"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Organization",
                "sec_num": "2.4"
            },
            {
                "text": "To sum up, the organization module classifies term descriptions based on domains, for which we use domain and description models. In Section 3, we elaborate on our organization model.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Organization",
                "sec_num": "2.4"
            },
            {
                "text": "Given one or more (in most cases more than one) descriptions for a single input term, the organization module selects appropriate description(s) for each domain related to the term.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overview",
                "sec_num": "3.1"
            },
            {
                "text": "We do not need all the extracted descriptions as final outputs, because they are usually similar to one another, and thus are redundant.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overview",
                "sec_num": "3.1"
            },
            {
                "text": "For the moment, we assume that we know a priori which domains are related to the input term.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overview",
                "sec_num": "3.1"
            },
            {
                "text": "From the viewpoint of probability theory, our task here is to select descriptions with greater probability for given domains. The probability for description d given domain c, P (d|c), is commonly transformed as in Equation ( 1), through use of the Bayesian theorem.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overview",
                "sec_num": "3.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "P (d|c) = P (c|d) \u2022 P (d) P (c)",
                        "eq_num": "(1)"
                    }
                ],
                "section": "Overview",
                "sec_num": "3.1"
            },
            {
                "text": "In practice, P (c) can be omitted because this factor is a constant, and thus does not affect the relative probability for different descriptions.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overview",
                "sec_num": "3.1"
            },
            {
                "text": "In Equation ( 1), P (c|d) models a probability that d corresponds to domain c. P (d) models a probability that d can be a description for the term in question, disregarding the domain. We shall call them domain and description models, respectively.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overview",
                "sec_num": "3.1"
            },
            {
                "text": "To sum up, in principle we select d's that are strongly associated with a specific domain, and are likely to be descriptions themselves.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overview",
                "sec_num": "3.1"
            },
            {
                "text": "Extracted descriptions are not linguistically understandable in the case where the extraction process is unsuccessful and retrieved pages inherently contain non-linguistic information (such as special characters and e-mail addresses).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overview",
                "sec_num": "3.1"
            },
            {
                "text": "To resolve this problem, Fujii and Ishikawa ( 2000) used a language model to filter out descriptions with low perplexity. However, in this paper we integrated a description model, which is practically the same as a language model, with an organization model. The new framework is more understandable with respect to probability theory.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overview",
                "sec_num": "3.1"
            },
            {
                "text": "In practice, we first use Equation ( 1) to compute P (d|c) for all the c's predefined in the domain model. Then we discard such c's whose P (d|c) is below a specific threshold. As a result, for the input term, related domains and descriptions are simultaneously selected. Thus, we do not have to know a priori which domains are related to each term.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overview",
                "sec_num": "3.1"
            },
            {
                "text": "In the following two sections, we explain methods to realize the domain and description models, respectively.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overview",
                "sec_num": "3.1"
            },
            {
                "text": "The domain model quantifies the extent to which description d is associated with domain c, which is fundamentally a categorization task. Among a number of existing categorization methods, we experimentally used one proposed by Iwayama and Tokunaga (1994) , which formulates P (c|d) as in Equation (2).",
                "cite_spans": [
                    {
                        "start": 227,
                        "end": 254,
                        "text": "Iwayama and Tokunaga (1994)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Domain Model",
                "sec_num": "3.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "P (c|d) = P (c) \u2022 t P (t|c) \u2022 P (t|d) P (t)",
                        "eq_num": "(2)"
                    }
                ],
                "section": "Domain Model",
                "sec_num": "3.2"
            },
            {
                "text": "Here, P (t|d), P (t|c) and P (t) denote probabilities that word t appears in d, c and all the domains, respectively. We regard P (c) as a constant. While P (t|d) is simply a relative frequency of t in d, we need predefined domains to compute P (t|c) and P (t). For this purpose, the use of large-scale corpora annotated with domains is desirable. However, since those resources are prohibitively expensive, we used the \"Nova\" dictionary for Japanese/English machine translation systems3 , which includes approximately one million entries related to 19 technical fields as listed below: aeronautics, biotechnology, business, chemistry, computers, construction, defense, ecology, electricity, energy, finance, law, mathematics, mechanics, medicine, metals, oceanography, plants, trade.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Domain Model",
                "sec_num": "3.2"
            },
            {
                "text": "We extracted words from dictionary entries to estimate P (t|c) and P (t), which are relative frequencies of t in c and all the domains, respectively. We used the ChaSen morphological analyzer (Matsumoto et al., 1997) to extract words from Japanese entries. We also used English entries because Japanese descriptions often contain English words.",
                "cite_spans": [
                    {
                        "start": 192,
                        "end": 216,
                        "text": "(Matsumoto et al., 1997)",
                        "ref_id": "BIBREF12"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Domain Model",
                "sec_num": "3.2"
            },
            {
                "text": "It may be argued that statistics extracted from dictionaries are unreliable, because word frequencies in real word usage are missing. However, words that are representative for a domain tend to be frequently used in compound word entries associated with the domain, and thus our method is a practical approximation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Domain Model",
                "sec_num": "3.2"
            },
            {
                "text": "The description model quantifies the extent to which a given page fragment is feasible as a description for the input term. In principle, we decompose the description model into language and quality properties, as shown in Equation (3).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Description Model",
                "sec_num": "3.3"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "P (d) = P L (d) \u2022 P Q (d)",
                        "eq_num": "(3)"
                    }
                ],
                "section": "Description Model",
                "sec_num": "3.3"
            },
            {
                "text": "Here, P L (d) and P Q (d) denote language and quality models, respectively.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Description Model",
                "sec_num": "3.3"
            },
            {
                "text": "It is expected that the quality model discards incorrect or misleading information contained in Web pages. For this purpose, a number of quality rating methods for Web pages (Amento et al., 2000; Zhu and Gauch, 2000) can be used. However, since Google (i.e., the search engine used in our system) rates the quality of pages based on hyperlink information, and selectively retrieves those with higher quality (Brin and Page, 1998) , we tentatively regarded P Q (d) as a constant. Thus, in practice the description model is approximated solely with the language model as in Equation (4).",
                "cite_spans": [
                    {
                        "start": 174,
                        "end": 195,
                        "text": "(Amento et al., 2000;",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 196,
                        "end": 216,
                        "text": "Zhu and Gauch, 2000)",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 408,
                        "end": 429,
                        "text": "(Brin and Page, 1998)",
                        "ref_id": "BIBREF2"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Description Model",
                "sec_num": "3.3"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "P (d) \u2248 P L (d)",
                        "eq_num": "(4)"
                    }
                ],
                "section": "Description Model",
                "sec_num": "3.3"
            },
            {
                "text": "Statistical approaches to language modeling have been used in much NLP research, such as machine translation (Brown et al., 1993) and speech recognition (Bahl et al., 1983) . Our model is almost the same as existing models, but is different in two respects.",
                "cite_spans": [
                    {
                        "start": 109,
                        "end": 129,
                        "text": "(Brown et al., 1993)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 153,
                        "end": 172,
                        "text": "(Bahl et al., 1983)",
                        "ref_id": "BIBREF1"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Description Model",
                "sec_num": "3.3"
            },
            {
                "text": "First, while general language models quantify the extent to which a given word sequence is linguistically acceptable, our model also quantifies the extent to which the input is acceptable as a term description. Thus, we trained the model based on an existing machine readable encyclopedia.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Description Model",
                "sec_num": "3.3"
            },
            {
                "text": "We used the ChaSen morphological analyzer to segment the Japanese CD-ROM World Encyclopedia (Heibonsha, 1998) into words (we replaced headwords with a common symbol), and then used the CMU-Cambridge toolkit (Clarkson and Rosenfeld, 1997) to model a word-based trigram.",
                "cite_spans": [
                    {
                        "start": 92,
                        "end": 109,
                        "text": "(Heibonsha, 1998)",
                        "ref_id": null
                    },
                    {
                        "start": 207,
                        "end": 237,
                        "text": "(Clarkson and Rosenfeld, 1997)",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Description Model",
                "sec_num": "3.3"
            },
            {
                "text": "Consequently, descriptions in which word sequences are more similar to those in the World Encyclopedia are assigned greater probability scores through our language model. Second, P (d), which is a product of probabilities for N -grams in d, is quite sensitive to the length of d. In the cases of machine translation and speech recognition, this problem is less crucial because multiple candidates compared based on the language model are almost equivalent in terms of length.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Description Model",
                "sec_num": "3.3"
            },
            {
                "text": "However, since in our case length of descriptions are significantly different, shorter descriptions are more likely to be selected, regardless of the quality. To avoid this problem, we normalize P (d) by the number of words contained in d.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Description Model",
                "sec_num": "3.3"
            },
            {
                "text": "Encyclopedias generated through our Web-based method can be used in a number of applications, including human usage, thesaurus production (Hearst, 1992; Nakamura and Nagao, 1988) and natural language understanding in general.",
                "cite_spans": [
                    {
                        "start": 138,
                        "end": 152,
                        "text": "(Hearst, 1992;",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 153,
                        "end": 178,
                        "text": "Nakamura and Nagao, 1988)",
                        "ref_id": "BIBREF15"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overview",
                "sec_num": "4.1"
            },
            {
                "text": "Among the above applications, natural language understanding (NLU) is the most challenging from a scientific point of view. Current practical NLU research includes dialogue, information extraction and question answering, among which we focus solely on question answering (QA) in this paper.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overview",
                "sec_num": "4.1"
            },
            {
                "text": "A straightforward application is to answer interrogative questions like \"What is X?\" in which a QA system searches the encyclopedia database for one or more descriptions related to X (this application is also effective for dialog systems).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overview",
                "sec_num": "4.1"
            },
            {
                "text": "In general, the performance of QA systems are evaluated based on coverage and accuracy. Coverage is the ratio between the number of questions answered (disregarding their correctness) and the total number of questions. Accuracy is the ratio between the number of correct answers and the total number of answers made by the system.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overview",
                "sec_num": "4.1"
            },
            {
                "text": "While coverage can be estimated objectively and systematically, estimating accuracy relies on human subjects (because there is no absolute description for term X), and thus is expensive.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overview",
                "sec_num": "4.1"
            },
            {
                "text": "In view of this problem, we targeted Information Technology Engineers Examinations4 , which are biannual (spring and autumn) examinations necessary for candidates to qualify to be IT engineers in Japan.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overview",
                "sec_num": "4.1"
            },
            {
                "text": "Among a number of classes, we focused on the \"Class II\" examination, which requires fundamental and general knowledge related to information technology. Approximately half of questions are associated with IT technical terms.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overview",
                "sec_num": "4.1"
            },
            {
                "text": "Since past examinations and answers are open to the public, we can evaluate the performance of our QA system with minimal cost.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overview",
                "sec_num": "4.1"
            },
            {
                "text": "The Class II examination consists of quadruple-choice questions, among which technical term questions can be subdivided into two types.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Analyzing IT Engineers Examinations",
                "sec_num": "4.2"
            },
            {
                "text": "In the first type of question, examinees choose the most appropriate description for a given technical term, such as \"memory interleave\" and \"router.\"",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Analyzing IT Engineers Examinations",
                "sec_num": "4.2"
            },
            {
                "text": "In the second type of question, examinees choose the most appropriate term for a given question, for which we show examples collected from the examination in the autumn of 1999 (translated into English by one of the authors) as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Analyzing IT Engineers Examinations",
                "sec_num": "4.2"
            },
            {
                "text": "1. Which data structure is most appropriate for FIFO (First-In First-Out)? In the autumn of 1999, out of 80 questions, the number of the first and second types were 22 and 18, respectively.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Analyzing IT Engineers Examinations",
                "sec_num": "4.2"
            },
            {
                "text": "For the first type of question, human examinees would search their knowledge base (i.e., memory) for the description of a given term, and compare that description with four candidates. Then they would choose the candidate that is most similar to the description.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementing a QA system",
                "sec_num": "4.3"
            },
            {
                "text": "For the second type of question, human examinees would search their knowledge base for the description of each of four candidate terms. Then they would choose the candidate term whose description is most similar to the question description.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementing a QA system",
                "sec_num": "4.3"
            },
            {
                "text": "The mechanism of our QA system is analogous to the above human methods. However, unlike human examinees, our system uses an encyclopedia generated from the Web as a knowledge base.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementing a QA system",
                "sec_num": "4.3"
            },
            {
                "text": "In addition, our system selectively uses term descriptions categorized into domains related to information technology. In other words, the description of \"pipeline (transportation pipe)\" is irrelevant or misleading to answer questions associated with \"pipeline (processing method).\"",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementing a QA system",
                "sec_num": "4.3"
            },
            {
                "text": "To compute the similarity between two descriptions, we used techniques developed in IR research, in which the similarity between a user query and each document in a collection is usually quantified based on word frequencies. In our case, a question and four possible answers correspond to query and document collection, respectively. We used a probabilistic method (Robertson and Walker, 1994) , which is one of the major IR methods.",
                "cite_spans": [
                    {
                        "start": 365,
                        "end": 393,
                        "text": "(Robertson and Walker, 1994)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementing a QA system",
                "sec_num": "4.3"
            },
            {
                "text": "To sum up, given a question, its type and four choices, our QA system chooses one of four candidates as the answer, in which the resolution algorithm varies depending on the question type.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementing a QA system",
                "sec_num": "4.3"
            },
            {
                "text": "Motivated partially by the TREC-8 QA collection (Voorhees and Tice, 2000) , question answering has of late become one of the major topics within the NLP/IR communities.",
                "cite_spans": [
                    {
                        "start": 48,
                        "end": 73,
                        "text": "(Voorhees and Tice, 2000)",
                        "ref_id": "BIBREF22"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "4.4"
            },
            {
                "text": "In fact, a number of QA systems targeting the TREC QA collection have recently been proposed (Harabagiu et al., 2000; Moldovan and Harabagiu, 2000; Prager et al., 2000) . Those systems are commonly termed \"open-domain\" systems, because questions expressed in natural language are not necessarily limited to explicit axes, including who, what, when, where, how and why.",
                "cite_spans": [
                    {
                        "start": 93,
                        "end": 117,
                        "text": "(Harabagiu et al., 2000;",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 118,
                        "end": 147,
                        "text": "Moldovan and Harabagiu, 2000;",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 148,
                        "end": 168,
                        "text": "Prager et al., 2000)",
                        "ref_id": "BIBREF17"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "4.4"
            },
            {
                "text": "However, Moldovan and Harabagiu (2000) found that each of the TREC questions can be recast as either a single axis or a combination of axes. They also found that out of the 200 TREC questions, 64 questions (approximately one third) were associated with the what axis, for which the Web-based encyclopedia is expected to improve the quality of answers.",
                "cite_spans": [
                    {
                        "start": 9,
                        "end": 38,
                        "text": "Moldovan and Harabagiu (2000)",
                        "ref_id": "BIBREF14"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "4.4"
            },
            {
                "text": "Although Harabagiu et al. (2000) proposed a knowledge-based QA system, most existing systems rely on conventional IR and shallow NLP methods. The use of encyclopedic knowledge for QA systems, as we demonstrated, needs to be further explored.",
                "cite_spans": [
                    {
                        "start": 9,
                        "end": 32,
                        "text": "Harabagiu et al. (2000)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "4.4"
            },
            {
                "text": "We conducted a number of experiments to investigate the effectiveness of our methods.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Methodology",
                "sec_num": "5.1"
            },
            {
                "text": "First, we generated an encyclopedia by way of our Web-based method (see Sections 2 and 3), and evaluated the quality of the encyclopedia itself.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Methodology",
                "sec_num": "5.1"
            },
            {
                "text": "Second, we applied the generated encyclopedia to our QA system (see Section 4), and evaluated its performance. The second experiment can be seen as a task-oriented evaluation for our encyclopedia generation method.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Methodology",
                "sec_num": "5.1"
            },
            {
                "text": "In the first experiment, we collected 96 terms from technical term questions in the Class II examination (the autumn of 1999). We used as test inputs those 96 terms and generated an encyclopedia, which was used in the second experiment.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Methodology",
                "sec_num": "5.1"
            },
            {
                "text": "For all the 96 test terms, Google (see Section 2.2) retrieved a positive number of pages, and the average number of pages for one term was 196,503. Since Google practically outputs contents of the top 1,000 pages, the remaining pages were not used in our experiments.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Methodology",
                "sec_num": "5.1"
            },
            {
                "text": "In the following two sections, we explain the first and second experiments, respectively.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Methodology",
                "sec_num": "5.1"
            },
            {
                "text": "For each test term, our method first computed P (d|c) using Equation (1) and discarded domains whose P (d|c) was below 0.05. Then, for each remaining domain, descriptions with higher P (d|c) were selected as the final outputs.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluating Encyclopedia Generation",
                "sec_num": "5.2"
            },
            {
                "text": "We selected the top three (not one) descriptions for each domain, because reading a couple of descriptions, which are short paragraphs, is not laborious for human users in real-world usage. As a result, at least one description was generated for 85 test terms, disregarding the correctness. The number of resultant descriptions was 326 (3.8 per term). We analyzed those descriptions from different perspectives.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluating Encyclopedia Generation",
                "sec_num": "5.2"
            },
            {
                "text": "First, we analyzed the distribution of the Google ranks for the Web pages from which the top three de-scriptions were eventually retained. Figure 2 shows the result, where we have combined the pages in groups of 50, so that the leftmost bar, for example, denotes the number of used pages whose original Google ranks ranged from 1 to 50.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 146,
                        "end": 147,
                        "text": "2",
                        "ref_id": "FIGREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Evaluating Encyclopedia Generation",
                "sec_num": "5.2"
            },
            {
                "text": "Although the first group includes the largest number of pages, other groups are also related to a relatively large number of pages. In other words, our method exploited a number of low ranking pages, which are not browsed or utilized by most Web users. Second, we analyzed the distribution of domains assigned to the 326 resultant descriptions. Figure 3 shows the result, in which, as expected, most descriptions were associated with the computer domain.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 352,
                        "end": 353,
                        "text": "3",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Evaluating Encyclopedia Generation",
                "sec_num": "5.2"
            },
            {
                "text": "However, the law domain was unexpectedly associated with a relatively great number of descriptions. We manually analyzed the resultant descriptions and found that descriptions for which appropriate domains are not defined in our domain model, such as sports, tended to be categorized into the law domain.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluating Encyclopedia Generation",
                "sec_num": "5.2"
            },
            {
                "text": "computers (200), law (41) , electricity (28), plants (15), medicine (10), finance (8), mathematics (8), mechanics (5), biotechnology (4), construction (2), ecology (2), chemistry (1), energy (1), oceanography (1) Figure 3 : Distribution of domains related to the 326 resultant descriptions.",
                "cite_spans": [
                    {
                        "start": 10,
                        "end": 25,
                        "text": "(200), law (41)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 220,
                        "end": 221,
                        "text": "3",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Evaluating Encyclopedia Generation",
                "sec_num": "5.2"
            },
            {
                "text": "Third, we evaluated the accuracy of our method, that is, the quality of an encyclopedia our method generated. For this purpose, each of the resultant descriptions was judged as to whether or not it is a correct description for a term in question. Each domain assigned to descriptions was also judged correct or incorrect.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluating Encyclopedia Generation",
                "sec_num": "5.2"
            },
            {
                "text": "We analyzed the result on a description-bydescription basis, that is, all the generated descriptions were considered independent of one another. The ratio of correct descriptions, disregarding the domain correctness, was 58.0% (189/326), and the ratio of correct descriptions categorized into the correct domain was 47.9% (156/326).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluating Encyclopedia Generation",
                "sec_num": "5.2"
            },
            {
                "text": "However, since all the test terms are inherently related to the IT field, we focused solely on descriptions categorized into the computer domain. In this case, the ratio of correct descriptions, disregarding the domain correctness, was 62.0% (124/200), and the ratio of correct descriptions categorized into the correct domain was 61.5% (123/200).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluating Encyclopedia Generation",
                "sec_num": "5.2"
            },
            {
                "text": "In addition, we analyzed the result on a term-byterm basis, because reading only a couple of descriptions is not crucial. In other words, we evaluated each term (not description), and in the case where at least one correct description categorized into the correct domain was generated for a term in question, we judged it correct. The ratio of correct terms was 89.4% (76/85), and in the case where we focused solely on the computer domain, the ratio was 84.8% (67/79).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluating Encyclopedia Generation",
                "sec_num": "5.2"
            },
            {
                "text": "In other words, by reading a couple of descriptions (3.8 descriptions per term), human users can obtain knowledge of approximately 90% of input terms.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluating Encyclopedia Generation",
                "sec_num": "5.2"
            },
            {
                "text": "Finally, we compared the resultant descriptions with an existing dictionary. For this purpose, we used the \"Nichigai\" computer dictionary (Nichigai Associates, 1996) , which lists approximately 30,000 Japanese technical terms related to the computer field, and contains descriptions for 13,588 terms. In the Nichigai dictionary, 42 out of the 96 test terms were described. Our method, which generated correct descriptions associated with the computer domain for 67 input terms, enhanced the Nichigai dictionary in terms of quantity.",
                "cite_spans": [
                    {
                        "start": 148,
                        "end": 165,
                        "text": "Associates, 1996)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluating Encyclopedia Generation",
                "sec_num": "5.2"
            },
            {
                "text": "These results indicate that our method for generating encyclopedias is of operational quality.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluating Encyclopedia Generation",
                "sec_num": "5.2"
            },
            {
                "text": "We used as test inputs 40 questions, which are related to technical terms collected from the Class II examination in the autumn of 1999.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluating Question Answering",
                "sec_num": "5.3"
            },
            {
                "text": "The objective here is not only to evaluate the performance of our QA system itself, but also to evaluate the quality of the encyclopedia generated by our method.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluating Question Answering",
                "sec_num": "5.3"
            },
            {
                "text": "Thus, as performed in the first experiment (Section 5.2), we used the Nichigai computer dictionary as a baseline encyclopedia. We compared the following three different resources as a knowledge base:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluating Question Answering",
                "sec_num": "5.3"
            },
            {
                "text": "\u2022 the Nichigai dictionary (\"Nichigai\"),",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluating Question Answering",
                "sec_num": "5.3"
            },
            {
                "text": "\u2022 the descriptions generated in the first experiment (\"Web\"),",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluating Question Answering",
                "sec_num": "5.3"
            },
            {
                "text": "\u2022 combination of both resources (\"Nichigai + Web\").",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluating Question Answering",
                "sec_num": "5.3"
            },
            {
                "text": "Table 1 shows the result of our comparative experiment, in which \"C\" and \"A\" denote coverage and accuracy, respectively, for variations of our QA system.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "1",
                        "ref_id": "TABREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Evaluating Question Answering",
                "sec_num": "5.3"
            },
            {
                "text": "Since all the questions we used are quadruplechoice, in case the system cannot answer the question, random choice can be performed to improve the coverage to 100%. Thus, for each knowledge resource we compared cases without/with random choice, which are denoted \"w/o Random\" and \"w/ Random\" in Table 1, respectively. In the case where random choice was not performed, the Web-based encyclopedia noticeably improved the coverage for the Nichigai dictionary, but decreased the accuracy. However, by combining both resources, the accuracy was noticeably improved, and the coverage was comparable with that for the Nichigai dictionary.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluating Question Answering",
                "sec_num": "5.3"
            },
            {
                "text": "On the other hand, in the case where random choice was performed, the Nichigai dictionary and the Webbased encyclopedia were comparable in terms of both the coverage and accuracy. Additionally, by combining both resources, the accuracy was further improved.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluating Question Answering",
                "sec_num": "5.3"
            },
            {
                "text": "We also investigated the performance of our QA system where descriptions related to the computer domain are solely used. However, coverage/accuracy did not significantly change, because as shown in Figure 3 , most of the descriptions were inherently related to the computer domain.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 205,
                        "end": 206,
                        "text": "3",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Evaluating Question Answering",
                "sec_num": "5.3"
            },
            {
                "text": "The World Wide Web has been an unprecedentedly enormous information source, from which a number of language processing methods have been explored to extract, retrieve and discover various types of information.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "6"
            },
            {
                "text": "In this paper, we aimed at generating encyclopedic knowledge, which is valuable for many applications including human usage and natural language understanding. For this purpose, we reformalized an existing Web-based extraction method, and proposed a new statistical organization model to improve the quality of extracted data.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "6"
            },
            {
                "text": "Given a term for which encyclopedic knowledge (i.e., descriptions) is to be generated, our method sequentially performs a) retrieval of Web pages contain-ing the term, b) extraction of page fragments describing the term, and c) organizing extracted descriptions based on domains (and consequently word senses).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "6"
            },
            {
                "text": "In addition, we proposed a question answering system, which answers interrogative questions associated with what, by using a Web-based encyclopedia as a knowledge base. For the purpose of evaluation, we used as test inputs technical terms collected from the Class II IT engineers examination, and found that the encyclopedia generated through our method was of operational quality and quantity.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "6"
            },
            {
                "text": "We also used test questions from the Class II examination, and evaluated the Web-based encyclopedia in terms of question answering. We found that our Webbased encyclopedia improved the system coverage obtained solely with an existing dictionary. In addition, when we used both resources, the performance was further improved.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "6"
            },
            {
                "text": "Future work would include generating information associated with more complex interrogations, such as ones related to how and why, so as to enhance Webbased natural language understanding.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "6"
            },
            {
                "text": "<DT> and <DD> are inherently provided to describe terms in HTML.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Produced by NOVA, Inc.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Japan Information-Technology Engineers Examination Center. http://www.jitec.jipdec.or.jp/",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "The authors would like to thank NOVA, Inc. for their support with the Nova dictionary and Katunobu Itou (The National Institute of Advanced Industrial Science and Technology, Japan) for his insightful comments on this paper.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgments",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Does \"authority\" mean quality? predicting expert quality ratings of Web documents",
                "authors": [
                    {
                        "first": "Brian",
                        "middle": [],
                        "last": "Amento",
                        "suffix": ""
                    },
                    {
                        "first": "Loren",
                        "middle": [],
                        "last": "Terveen",
                        "suffix": ""
                    },
                    {
                        "first": "Will",
                        "middle": [],
                        "last": "Hill",
                        "suffix": ""
                    }
                ],
                "year": 2000,
                "venue": "Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                "volume": "",
                "issue": "",
                "pages": "296--303",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Brian Amento, Loren Terveen, and Will Hill. 2000. Does \"authority\" mean quality? predicting expert quality ratings of Web documents. In Proceedings of the 23rd Annual International ACM SIGIR Con- ference on Research and Development in Informa- tion Retrieval, pages 296-303.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "A maximum linklihood approach to continuous speech recognition",
                "authors": [
                    {
                        "first": "",
                        "middle": [
                            "R"
                        ],
                        "last": "Lalit",
                        "suffix": ""
                    },
                    {
                        "first": "Frederick",
                        "middle": [],
                        "last": "Bahl",
                        "suffix": ""
                    },
                    {
                        "first": "Robert",
                        "middle": [
                            "L"
                        ],
                        "last": "Jelinek",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Mercer",
                        "suffix": ""
                    }
                ],
                "year": 1983,
                "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "volume": "5",
                "issue": "2",
                "pages": "179--190",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Lalit. R. Bahl, Frederick Jelinek, and Robert L. Mer- cer. 1983. A maximum linklihood approach to continuous speech recognition. IEEE Transac- tions on Pattern Analysis and Machine Intelligence, 5(2):179-190.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "The anatomy of a large-scale hypertextual Web search engine",
                "authors": [
                    {
                        "first": "Sergey",
                        "middle": [],
                        "last": "Brin",
                        "suffix": ""
                    },
                    {
                        "first": "Lawrence",
                        "middle": [],
                        "last": "Page",
                        "suffix": ""
                    }
                ],
                "year": 1998,
                "venue": "Computer Networks",
                "volume": "30",
                "issue": "1-7",
                "pages": "107--117",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sergey Brin and Lawrence Page. 1998. The anatomy of a large-scale hypertextual Web search engine. Computer Networks, 30(1-7):107-117.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "The mathematics of statistical machine translation: Parameter estimation",
                "authors": [
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Peter",
                        "suffix": ""
                    },
                    {
                        "first": "Stephen",
                        "middle": [
                            "A"
                        ],
                        "last": "Brown",
                        "suffix": ""
                    },
                    {
                        "first": "Vincent",
                        "middle": [
                            "J"
                        ],
                        "last": "Della Pietra",
                        "suffix": ""
                    },
                    {
                        "first": "Robert",
                        "middle": [
                            "L"
                        ],
                        "last": "Della Pietra",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Mercer",
                        "suffix": ""
                    }
                ],
                "year": 1993,
                "venue": "Computational Linguistics",
                "volume": "19",
                "issue": "2",
                "pages": "263--311",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Pa- rameter estimation. Computational Linguistics, 19(2):263-311.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Statistical language modeling using the CMU-Cambridge toolkit",
                "authors": [
                    {
                        "first": "Philip",
                        "middle": [],
                        "last": "Clarkson",
                        "suffix": ""
                    },
                    {
                        "first": "Ronald",
                        "middle": [],
                        "last": "Rosenfeld",
                        "suffix": ""
                    }
                ],
                "year": 1997,
                "venue": "Proceedings of EuroSpeech'97",
                "volume": "",
                "issue": "",
                "pages": "2707--2710",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Philip Clarkson and Ronald Rosenfeld. 1997. Statisti- cal language modeling using the CMU-Cambridge toolkit. In Proceedings of EuroSpeech'97, pages 2707-2710.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Moving up the information food chain",
                "authors": [
                    {
                        "first": "Oren",
                        "middle": [],
                        "last": "Etzioni",
                        "suffix": ""
                    }
                ],
                "year": 1997,
                "venue": "AI Magazine",
                "volume": "18",
                "issue": "2",
                "pages": "11--18",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Oren Etzioni. 1997. Moving up the information food chain. AI Magazine, 18(2):11-18.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Utilizing the World Wide Web as an encyclopedia: Extracting term descriptions from semi-structured texts",
                "authors": [
                    {
                        "first": "Atsushi",
                        "middle": [],
                        "last": "Fujii",
                        "suffix": ""
                    },
                    {
                        "first": "Tetsuya",
                        "middle": [],
                        "last": "Ishikawa",
                        "suffix": ""
                    }
                ],
                "year": 2000,
                "venue": "Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "488--495",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Atsushi Fujii and Tetsuya Ishikawa. 2000. Utilizing the World Wide Web as an encyclopedia: Extract- ing term descriptions from semi-structured texts. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, pages 488-495.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Experiments with open-domain textual question answering",
                "authors": [
                    {
                        "first": "Sanda",
                        "middle": [
                            "M"
                        ],
                        "last": "Harabagiu",
                        "suffix": ""
                    },
                    {
                        "first": "Marius",
                        "middle": [
                            "A"
                        ],
                        "last": "Pas",
                        "suffix": ""
                    },
                    {
                        "first": "Steven",
                        "middle": [
                            "J"
                        ],
                        "last": "Maiorano",
                        "suffix": ""
                    }
                ],
                "year": 2000,
                "venue": "Proceedings of the 18th International Conference on Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "292--298",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sanda M. Harabagiu, Marius A. Pas \u00b8ca, and Steven J. Maiorano. 2000. Experiments with open-domain textual question answering. In Proceedings of the 18th International Conference on Computational Linguistics, pages 292-298.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Automatic acquisition of hyponyms from large text corpora",
                "authors": [
                    {
                        "first": "Marti",
                        "middle": [
                            "A"
                        ],
                        "last": "Hearst",
                        "suffix": ""
                    }
                ],
                "year": 1992,
                "venue": "Proceedings of the 14th International Conference on Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "539--545",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Marti A. Hearst. 1992. Automatic acquisition of hy- ponyms from large text corpora. In Proceedings of the 14th International Conference on Computa- tional Linguistics, pages 539-545.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "CD-ROM World Encyclopedia",
                "authors": [],
                "year": 1998,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Hitachi Digital Heibonsha. 1998. CD-ROM World Encyclopedia. (In Japanese).",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Basket analysis for graph structured data",
                "authors": [
                    {
                        "first": "Akihiro",
                        "middle": [],
                        "last": "Inokuchi",
                        "suffix": ""
                    },
                    {
                        "first": "Takashi",
                        "middle": [],
                        "last": "Washio",
                        "suffix": ""
                    },
                    {
                        "first": "Hiroshi",
                        "middle": [],
                        "last": "Motoda",
                        "suffix": ""
                    },
                    {
                        "first": "Kouhei",
                        "middle": [],
                        "last": "Kumasawa",
                        "suffix": ""
                    },
                    {
                        "first": "Naohide",
                        "middle": [],
                        "last": "Arai",
                        "suffix": ""
                    }
                ],
                "year": 1999,
                "venue": "Proceedings of the 3rd Pacific-Asia Conference on Knowledge Discovery and Data Mining",
                "volume": "",
                "issue": "",
                "pages": "420--431",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Akihiro Inokuchi, Takashi Washio, Hiroshi Motoda, Kouhei Kumasawa, and Naohide Arai. 1999. Bas- ket analysis for graph structured data. In Proceed- ings of the 3rd Pacific-Asia Conference on Knowl- edge Discovery and Data Mining, pages 420-431.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "A probabilistic model for text categorization: Based on a single random variable with multiple values",
                "authors": [
                    {
                        "first": "Makoto",
                        "middle": [],
                        "last": "Iwayama",
                        "suffix": ""
                    },
                    {
                        "first": "Takenobu",
                        "middle": [],
                        "last": "Tokunaga",
                        "suffix": ""
                    }
                ],
                "year": 1994,
                "venue": "Proceedings of the 4th Conference on Applied Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "162--167",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Makoto Iwayama and Takenobu Tokunaga. 1994. A probabilistic model for text categorization: Based on a single random variable with multiple values. In Proceedings of the 4th Conference on Applied Nat- ural Language Processing, pages 162-167.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Japanese morphological analysis system ChaSen manual",
                "authors": [
                    {
                        "first": "Yuji",
                        "middle": [],
                        "last": "Matsumoto",
                        "suffix": ""
                    },
                    {
                        "first": "Akira",
                        "middle": [],
                        "last": "Kitauchi",
                        "suffix": ""
                    },
                    {
                        "first": "Tatsuo",
                        "middle": [],
                        "last": "Yamashita",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshitaka",
                        "middle": [],
                        "last": "Hirano",
                        "suffix": ""
                    },
                    {
                        "first": "Osamu",
                        "middle": [],
                        "last": "Imaichi",
                        "suffix": ""
                    },
                    {
                        "first": "Tomoaki",
                        "middle": [],
                        "last": "Imamura",
                        "suffix": ""
                    }
                ],
                "year": 1997,
                "venue": "NAIST",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita, Yoshitaka Hirano, Osamu Imaichi, and Tomoaki Imamura. 1997. Japanese morphological analysis system ChaSen manual. Technical Report NAIST- IS-TR97007, NAIST. (In Japanese).",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "A machine learning approach to building domain-specific search engines",
                "authors": [
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Mccallum",
                        "suffix": ""
                    },
                    {
                        "first": "Kamal",
                        "middle": [],
                        "last": "Nigam",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Rennie",
                        "suffix": ""
                    },
                    {
                        "first": "Kristie",
                        "middle": [],
                        "last": "Seymore",
                        "suffix": ""
                    }
                ],
                "year": 1999,
                "venue": "Proceedings of the 16th International Joint Conference on Artificial Intelligence",
                "volume": "",
                "issue": "",
                "pages": "662--667",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Andrew McCallum, Kamal Nigam, Jason Rennie, and Kristie Seymore. 1999. A machine learning ap- proach to building domain-specific search engines. In Proceedings of the 16th International Joint Con- ference on Artificial Intelligence, pages 662-667.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "The structure and performance of an open-domain question answering system",
                "authors": [
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Moldovan",
                        "suffix": ""
                    },
                    {
                        "first": "Sanda",
                        "middle": [],
                        "last": "Harabagiu",
                        "suffix": ""
                    }
                ],
                "year": 2000,
                "venue": "Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "563--570",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dan Moldovan and Sanda Harabagiu. 2000. The structure and performance of an open-domain ques- tion answering system. In Proceedings of the 38th Annual Meeting of the Association for Computa- tional Linguistics, pages 563-570.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Extraction of semantic information from an ordinary English dictionary and its evaluation",
                "authors": [
                    {
                        "first": "Jun'ichi",
                        "middle": [],
                        "last": "Nakamura",
                        "suffix": ""
                    },
                    {
                        "first": "Makoto",
                        "middle": [],
                        "last": "Nagao",
                        "suffix": ""
                    }
                ],
                "year": 1988,
                "venue": "Proceedings of the 10th International Conference on Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "459--464",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jun'ichi Nakamura and Makoto Nagao. 1988. Extrac- tion of semantic information from an ordinary En- glish dictionary and its evaluation. In Proceedings of the 10th International Conference on Computa- tional Linguistics, pages 459-464.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "English-Japanese computer terminology dictionary",
                "authors": [
                    {
                        "first": "Nichigai",
                        "middle": [],
                        "last": "Associates",
                        "suffix": ""
                    }
                ],
                "year": 1996,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Nichigai Associates. 1996. English-Japanese com- puter terminology dictionary. (In Japanese).",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Question-answering by predictive annotation",
                "authors": [
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Prager",
                        "suffix": ""
                    },
                    {
                        "first": "Eric",
                        "middle": [],
                        "last": "Brown",
                        "suffix": ""
                    },
                    {
                        "first": "Anni",
                        "middle": [],
                        "last": "Coden",
                        "suffix": ""
                    }
                ],
                "year": 2000,
                "venue": "Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                "volume": "",
                "issue": "",
                "pages": "184--191",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "John Prager, Eric Brown, and Anni Coden. 2000. Question-answering by predictive annotation. In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 184-191.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Mining the Web for bilingual texts",
                "authors": [
                    {
                        "first": "Philip",
                        "middle": [],
                        "last": "Resnik",
                        "suffix": ""
                    }
                ],
                "year": 1999,
                "venue": "Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "527--534",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Philip Resnik. 1999. Mining the Web for bilingual texts. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 527-534.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval",
                "authors": [
                    {
                        "first": "S",
                        "middle": [
                            "E"
                        ],
                        "last": "Robertson",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Walker",
                        "suffix": ""
                    }
                ],
                "year": 1994,
                "venue": "Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                "volume": "",
                "issue": "",
                "pages": "232--241",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "S. E. Robertson and S. Walker. 1994. Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval. In Proceedings of the 17th Annual International ACM SIGIR Confer- ence on Research and Development in Information Retrieval, pages 232-241.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Automatic word sense discrimination",
                "authors": [
                    {
                        "first": "Hinrich",
                        "middle": [],
                        "last": "Sch\u00fctze",
                        "suffix": ""
                    }
                ],
                "year": 1998,
                "venue": "Computational Linguistics",
                "volume": "24",
                "issue": "1",
                "pages": "97--123",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Hinrich Sch\u00fctze. 1998. Automatic word sense dis- crimination. Computational Linguistics, 24(1):97- 123.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Learning to extract textbased information from the World Wide Web",
                "authors": [
                    {
                        "first": "Stephen",
                        "middle": [],
                        "last": "Soderland",
                        "suffix": ""
                    }
                ],
                "year": 1997,
                "venue": "Proceedings of 3rd International Conference on Knowledge Discovery and Data Mining",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Stephen Soderland. 1997. Learning to extract text- based information from the World Wide Web. In Proceedings of 3rd International Conference on Knowledge Discovery and Data Mining.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Building a question answering test collection",
                "authors": [
                    {
                        "first": "Ellen",
                        "middle": [
                            "M"
                        ],
                        "last": "Voorhees",
                        "suffix": ""
                    },
                    {
                        "first": "Dawn",
                        "middle": [
                            "M"
                        ],
                        "last": "Tice",
                        "suffix": ""
                    }
                ],
                "year": 2000,
                "venue": "Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                "volume": "",
                "issue": "",
                "pages": "200--207",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ellen M. Voorhees and Dawn M. Tice. 2000. Building a question answering test collection. In Proceed- ings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Infor- mation Retrieval, pages 200-207.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Unsupervised word sense disambiguation rivaling supervised methods",
                "authors": [
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Yarowsky",
                        "suffix": ""
                    }
                ],
                "year": 1995,
                "venue": "Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "189--196",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "David Yarowsky. 1995. Unsupervised word sense dis- ambiguation rivaling supervised methods. In Pro- ceedings of the 33rd Annual Meeting of the Associa- tion for Computational Linguistics, pages 189-196.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Incorporating quality metrics in centralized/distributed information retrieval on the World Wide Web",
                "authors": [
                    {
                        "first": "Xiaolan",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    },
                    {
                        "first": "Susan",
                        "middle": [],
                        "last": "Gauch",
                        "suffix": ""
                    }
                ],
                "year": 2000,
                "venue": "Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                "volume": "",
                "issue": "",
                "pages": "288--295",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xiaolan Zhu and Susan Gauch. 2000. Incorporating quality metrics in centralized/distributed informa- tion retrieval on the World Wide Web. In Proceed- ings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Infor- mation Retrieval, pages 288-295.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 1: The overall design of our Web-based encyclopedia generation system.",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "a) binary trees, b) queues, c) stacks, d) heaps 2. Choose the LAN access method in which multiple terminals transmit data simultaneously and thus they potentially collide. a) ATM, b) CSM/CD, c) FDDI, d) token ring",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "Figure 2: Distribution of rankings for original pages in Google.",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table><tr><td/><td colspan=\"4\">w/o Random w/ Random</td></tr><tr><td>Resource</td><td>C</td><td>A</td><td>C</td><td>A</td></tr><tr><td>Nichigai</td><td colspan=\"4\">50.0 65.0 100 45.0</td></tr><tr><td>Web</td><td colspan=\"4\">92.5 48.6 100 46.9</td></tr><tr><td colspan=\"5\">Nichigai + Web 95.0 63.2 100 61.3</td></tr></table>",
                "type_str": "table",
                "text": "Coverage and accuracy (%) for different question answering methods.",
                "html": null,
                "num": null
            }
        }
    }
}