{
    "paper_id": "2020",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T11:58:31.002791Z"
    },
    "title": "HyperCore: Hyperbolic and Co-graph Representation for Automatic ICD Coding",
    "authors": [
        {
            "first": "Pengfei",
            "middle": [],
            "last": "Cao",
            "suffix": "",
            "affiliation": {
                "laboratory": "National Laboratory of Pattern Recognition",
                "institution": "Chinese Academy of Sciences",
                "location": {
                    "postCode": "100190",
                    "settlement": "Beijing",
                    "country": "China"
                }
            },
            "email": "pengfei.cao@nlpr.ia.ac.cn"
        },
        {
            "first": "Yubo",
            "middle": [],
            "last": "Chen",
            "suffix": "",
            "affiliation": {
                "laboratory": "National Laboratory of Pattern Recognition",
                "institution": "Chinese Academy of Sciences",
                "location": {
                    "postCode": "100190",
                    "settlement": "Beijing",
                    "country": "China"
                }
            },
            "email": "yubo.chen@nlpr.ia.ac.cn"
        },
        {
            "first": "Kang",
            "middle": [],
            "last": "Liu",
            "suffix": "",
            "affiliation": {
                "laboratory": "National Laboratory of Pattern Recognition",
                "institution": "Chinese Academy of Sciences",
                "location": {
                    "postCode": "100190",
                    "settlement": "Beijing",
                    "country": "China"
                }
            },
            "email": "kliu@nlpr.ia.ac.cn"
        },
        {
            "first": "Jun",
            "middle": [],
            "last": "Zhao",
            "suffix": "",
            "affiliation": {
                "laboratory": "National Laboratory of Pattern Recognition",
                "institution": "Chinese Academy of Sciences",
                "location": {
                    "postCode": "100190",
                    "settlement": "Beijing",
                    "country": "China"
                }
            },
            "email": "jzhao@nlpr.ia.ac.cn"
        },
        {
            "first": "Shengping",
            "middle": [],
            "last": "Liu",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Beijing Unisound Information Technology Co",
                "location": {
                    "postCode": "100028",
                    "settlement": "Ltd, Beijing",
                    "country": "China"
                }
            },
            "email": "liushengping@unisound.com"
        },
        {
            "first": "Weifeng",
            "middle": [],
            "last": "Chong",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Beijing Unisound Information Technology Co",
                "location": {
                    "postCode": "100028",
                    "settlement": "Ltd, Beijing",
                    "country": "China"
                }
            },
            "email": "chongweifeng@unisound.com"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "The International Classification of Diseases (ICD) provides a standardized way for classifying diseases, which endows each disease with a unique code. ICD coding aims to assign proper ICD codes to a medical record. Since manual coding is very laborious and prone to errors, many methods have been proposed for the automatic ICD coding task. However, most of existing methods independently predict each code, ignoring two important characteristics: Code Hierarchy and Code Co-occurrence. In this paper, we propose a Hyperbolic and Co-graph Representation method (HyperCore) to address the above problem. Specifically, we propose a hyperbolic representation method to leverage the code hierarchy. Moreover, we propose a graph convolutional network to utilize the code co-occurrence. Experimental results on two widely used datasets demonstrate that our proposed model outperforms previous state-ofthe-art methods.",
    "pdf_parse": {
        "paper_id": "2020",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "The International Classification of Diseases (ICD) provides a standardized way for classifying diseases, which endows each disease with a unique code. ICD coding aims to assign proper ICD codes to a medical record. Since manual coding is very laborious and prone to errors, many methods have been proposed for the automatic ICD coding task. However, most of existing methods independently predict each code, ignoring two important characteristics: Code Hierarchy and Code Co-occurrence. In this paper, we propose a Hyperbolic and Co-graph Representation method (HyperCore) to address the above problem. Specifically, we propose a hyperbolic representation method to leverage the code hierarchy. Moreover, we propose a graph convolutional network to utilize the code co-occurrence. Experimental results on two widely used datasets demonstrate that our proposed model outperforms previous state-ofthe-art methods.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "The International Classification of Diseases (ICD) is a healthcare classification system supported by the World Health Organization, which provides a unique code for each disease, symptom, sign and so on. ICD codes have been widely used for analyzing clinical data and monitoring health issues (Choi et al., 2016; Avati et al., 2018) . Due to the importance of ICD codes, ICD coding -which assigns proper ICD codes to a medical record -has drawn much attention. The task of ICD coding is usually undertaken by professional coders according to doctors' diagnosis descriptions in the form of free texts. However, manual coding is very expensive, time-consuming and error-prone. The cost incurred by coding errors and the financial investment spent on improving coding quality are estimated to be $25 billion per year in the US (Lang, 2007) . Two main reasons can account for this. First, only the people who have medical expert knowledge and specialized ICD coding skills can handle the task. However, it is hard to train such an eligible ICD coder. Second, it is difficult to correctly assign proper codes to the input document even for professional coders, because one document can be assigned multiple ICD codes and the number of codes in the taxonomy of ICD is large. For example, there are over 15,000 and 60,000 codes respectively in the ninth version (ICD-9) and the tenth version (ICD-10) of ICD taxonomies.",
                "cite_spans": [
                    {
                        "start": 294,
                        "end": 313,
                        "text": "(Choi et al., 2016;",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 314,
                        "end": 333,
                        "text": "Avati et al., 2018)",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 825,
                        "end": 837,
                        "text": "(Lang, 2007)",
                        "ref_id": "BIBREF14"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "To reduce human labor and coding errors, many methods have been carefully designed for automatic ICD coding (Perotte et al., 2013; Mullenbach et al., 2018) . For example in Figure 1 , given the clinical text of a patient, the ICD coding model needs to automatically predict the corresponding ICD codes. The automatic ICD coding task can be modeled as a multi-label classification task since each clinical text is usually accompanied by mul- tiple codes. Most of the previous methods handle each code in isolation and convert the multi-label problem into a set of binary classification problems to predict whether each code of interest presents or not (Mullenbach et al., 2018; Rios and Kavuluru, 2018) . Though effective, they ignore two important characteristics: Code Hierarchy and Code Co-occurrence, which can be leveraged to improve coding accuracy. In the following, we will introduce the two characteristics and the reasons why they are critical for the automatic ICD coding.",
                "cite_spans": [
                    {
                        "start": 108,
                        "end": 130,
                        "text": "(Perotte et al., 2013;",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 131,
                        "end": 155,
                        "text": "Mullenbach et al., 2018)",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 651,
                        "end": 676,
                        "text": "(Mullenbach et al., 2018;",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 677,
                        "end": 701,
                        "text": "Rios and Kavuluru, 2018)",
                        "ref_id": "BIBREF24"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 180,
                        "end": 181,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Code Hierarchy: Based on ICD taxonomy, ICD codes are organized under a tree-like hierarchical structure as shown in Figure 2 , which indicates the parent-child and sibling relations between codes. In the hierarchical structure, the upper level nodes represent more generic disease categories and the lower level nodes represent more specific diseases. The code hierarchy can capture the mutual exclusion of some codes. If code X and Y are both children of Z (i.e., X and Y are the siblings), it is unlikely to simultaneously assign X and Y to a patient in general (Xie and Xing, 2018) . For example in Figure 2 , if code \"464.00 (acute laryngitis without mention of obstruction)\" is assigned to a patient, it is unlikely to assign the code \"464.01 (acute laryngitis with obstruction)\" to the patient at the same time. If automatic ICD coding models ignore such a characteristic, they are prone to giving inconsistent predictions. Thus, a challenging problem is how to model the code hierarchy and use it to capture the mutual exclusion of codes.",
                "cite_spans": [
                    {
                        "start": 564,
                        "end": 584,
                        "text": "(Xie and Xing, 2018)",
                        "ref_id": "BIBREF28"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 123,
                        "end": 124,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    },
                    {
                        "start": 609,
                        "end": 610,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Code Co-occurrence: Since some diseases are concurrent or have a causal relationship with each other, their codes usually co-occur in the clinical text, such as \"997.91 (hypertension)\" and \"429.9 (heart disease)\". In this paper, we call such characteristic code co-occurrence which can capture the correlations of codes. The code co-occurrence can be utilized to correctly predict some codes which are difficult to predict by only using the clinical text itself. For example in Figure 1 , the code of \"acute respiratory failure\" can be easily inferred via capturing apparent clues (i.e., the green bold words) from the text. Although there are also a few clues to infer the code of \"acidosis\", they are very obscure, let alone predict the code of \"acidosis\" by only using these obscure clues. Fortunately, there is a strong association between these two diseases: one of the main causes of \"acidosis\" is \"acute respiratory failure\". This prior knowledge can be captured via the fact that the codes of the two diseases usually co-occur in clinical texts. By considering the correlation, the automatic ICD coding model can better exploit obscure clues to predict the code of \"acidosis\". Therefore, another problem is how to leverage code co-occurrence for ICD coding.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 485,
                        "end": 486,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this paper, we propose a novel method termed as Hyperbolic and Co-graph Representation method (HyperCore) to address above problems. Since the tree-likeness properties of the hyperbolic space make it more suitable for representing symbolic data with hierarchical structures than the Euclidean space (Nickel and Kiela, 2017) , we propose a hyperbolic representation learning method to learn the Code Hierarchy. Meanwhile, the graph has been proved effective in modeling data correlation and the graph convolutional network (GCN) enables to efficiently learn node representation (Kipf and Welling, 2016) . Thus, we devise a code co-occurrence graph (co-graph) for capturing Code Co-occurrence and exploit the GCN to learn the code representation in the co-graph.",
                "cite_spans": [
                    {
                        "start": 302,
                        "end": 326,
                        "text": "(Nickel and Kiela, 2017)",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 580,
                        "end": 604,
                        "text": "(Kipf and Welling, 2016)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The contributions of this paper are threefold. Firstly, to our best knowledge, this is the first work to propose a hyperbolic representation method to leverage the code hierarchy for automatic ICD coding. Secondly, this is also the first work to utilize a GCN to exploit code co-occurrence correlation for automatic ICD coding. Thirdly, experiments on two widely used automatic ICD coding datasets show that our proposed model outperforms previous state-of-the-art methods.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Automatic ICD Coding. Automatic ICD coding is a challenging and important task in the medical informatics community, which has been studied with traditional machine learning methods (Larkey and Croft, 1996; Perotte et al., 2013) and neural network methods (Koopman et al., 2015; Rios and Kavuluru, 2018; Yu et al., 2019) . Given discharge 2018) propose an attention based convolutional neural network (CNN) model to capture important information for each code. Xie and Xing (2018) adopt tree long shortterm memory (LSTM) to utilize code descriptions. Though effective, they ignore the code hierarchy and code co-occurrence. Hyperbolic Representation. Hyperbolic space has been applied to modeling complex networks (Krioukov et al., 2010) . Recent research on representation learning demonstrates that the hyperbolic space is more suitable for representing symbolic data with hierarchical structures than the Euclidean space (Nickel and Kiela, 2017, 2018; Hamann, 2018) . In the field of natural language processing (NLP), the hyperbolic representation has been successfully applied to question answering (Tay et al., 2018) , machine translation (Gulcehre et al., 2018) and sentence representation (Dhingra et al., 2018) . To our knowledge, this is the first work to apply hyperbolic representation method to the automatic ICD coding task. Graph Convolutional Networks. GCN (Kipf and Welling, 2016) is a powerful neural network, which operates on graph data. It yields substantial improvements over various NLP tasks such as semantic role labeling (Marcheggiani and Titov, 2017) , multi-document summarization (Yasunaga et al., 2017) and machine translation (Bastings et al., 2017) . Veli\u010dkovi\u0107 et al. (2017) propose graph atten-tion networks (GAT) to summarize neighborhood features by using masked self-attentional layers. We are the first to capture the code co-occurrence characteristic via the GCN for the automatic ICD coding task.",
                "cite_spans": [
                    {
                        "start": 182,
                        "end": 206,
                        "text": "(Larkey and Croft, 1996;",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 207,
                        "end": 228,
                        "text": "Perotte et al., 2013)",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 256,
                        "end": 278,
                        "text": "(Koopman et al., 2015;",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 279,
                        "end": 303,
                        "text": "Rios and Kavuluru, 2018;",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 304,
                        "end": 320,
                        "text": "Yu et al., 2019)",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 461,
                        "end": 480,
                        "text": "Xie and Xing (2018)",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 714,
                        "end": 737,
                        "text": "(Krioukov et al., 2010)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 924,
                        "end": 935,
                        "text": "(Nickel and",
                        "ref_id": null
                    },
                    {
                        "start": 936,
                        "end": 954,
                        "text": "Kiela, 2017, 2018;",
                        "ref_id": null
                    },
                    {
                        "start": 955,
                        "end": 968,
                        "text": "Hamann, 2018)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 1104,
                        "end": 1122,
                        "text": "(Tay et al., 2018)",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 1145,
                        "end": 1168,
                        "text": "(Gulcehre et al., 2018)",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 1197,
                        "end": 1219,
                        "text": "(Dhingra et al., 2018)",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 1547,
                        "end": 1577,
                        "text": "(Marcheggiani and Titov, 2017)",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 1609,
                        "end": 1632,
                        "text": "(Yasunaga et al., 2017)",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 1657,
                        "end": 1680,
                        "text": "(Bastings et al., 2017)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 1683,
                        "end": 1707,
                        "text": "Veli\u010dkovi\u0107 et al. (2017)",
                        "ref_id": "BIBREF27"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "We propose a hyperbolic and co-graph representation (HyperCore) model for automatic ICD coding. Firstly, to capture the code hierarchy, we learn the code hyperbolic representations and measure the similarities between document and codes in the hyperbolic space. Secondly, to exploit code cooccurrence, we exploit the GCN to learn code cooccurrence representations and use them as query vectors to obtain code-aware document representations. Finally, the document-code similarity scores and code-aware document representations are then aggregated to predict the codes. Figure 3 shows the overall architecture of our proposed model.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 575,
                        "end": 576,
                        "text": "3",
                        "ref_id": "FIGREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Method",
                "sec_num": "3"
            },
            {
                "text": "We first map each word into a low dimensional word embedding space. The document can be denoted as X = {x 1 , x 2 , . . . , x N }, where N is the length of the document. Then, we exploit the CNN to encode the clinical text due to its high computational efficiency:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Convolution Neural Network Encoder",
                "sec_num": "3.1"
            },
            {
                "text": "hi = tanh(Wc * x i:i+k-1 + bc) (1)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Convolution Neural Network Encoder",
                "sec_num": "3.1"
            },
            {
                "text": "where W c is the convolutional filter. b c is the bias. k is the filter size. * is the convolution operator.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Convolution Neural Network Encoder",
                "sec_num": "3.1"
            },
            {
                "text": "After encoding by CNN, we obtain the document representation H = {h 1 , h 2 , . . . , h N }. Since we need to assign multiple codes for each document and different codes may focus on different sections of the document, we employ code-wise attention to learn relevant document representations for each code. We first generate the code vector for each code via averaging the word embeddings of its descriptor:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Code-wise Attention",
                "sec_num": "3.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "vi = 1 N d N d j=1 wj, i = 1, . . . , L",
                        "eq_num": "(2)"
                    }
                ],
                "section": "Code-wise Attention",
                "sec_num": "3.2"
            },
            {
                "text": "where v i is the code vector, N d is the length of the descriptor, w j is the embedding of j-th word in the descriptor, and L is the total number of codes in the dataset (Jouhet et al., 2012; Johnson et al., 2016) .",
                "cite_spans": [
                    {
                        "start": 170,
                        "end": 191,
                        "text": "(Jouhet et al., 2012;",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 192,
                        "end": 213,
                        "text": "Johnson et al., 2016)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Code-wise Attention",
                "sec_num": "3.2"
            },
            {
                "text": "The code vectors set is",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Code-wise Attention",
                "sec_num": "3.2"
            },
            {
                "text": "V = {v 1 , v 2 , . . . , v L }.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Code-wise Attention",
                "sec_num": "3.2"
            },
            {
                "text": "Then, we generate the code-wise attention vector via matrix-vector product:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Code-wise Attention",
                "sec_num": "3.2"
            },
            {
                "text": "\u03b1i = softmax(H T vi) (3)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Code-wise Attention",
                "sec_num": "3.2"
            },
            {
                "text": "Finally, we use the document representation H and attention vector \u03b1 i to generate the code-aware document representation:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Code-wise Attention",
                "sec_num": "3.2"
            },
            {
                "text": "ci = H\u03b1i (4)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Code-wise Attention",
                "sec_num": "3.2"
            },
            {
                "text": "We concatenate the c i (i = 1, . . . , L) to obtain the code-aware document representation, denoted as C = {c 1 , c 2 , . . . , c L } \u2208 R dc\u00d7L .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Code-wise Attention",
                "sec_num": "3.2"
            },
            {
                "text": "To capture the code hierarchy, we learn the code hyperbolic representations and measure the similarities between document and codes in the hyperbolic space. In this section, we propose a hyperbolic code embedder to obtain code hyperbolic representations, and we also propose a hyperbolic document projector to project the document representations from Euclidean space to hyperbolic space. We then compute the similarities between the document and codes in the hyperbolic space.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Document-Code Similarities in Hyperbolic Space",
                "sec_num": "3.3"
            },
            {
                "text": "Hyperbolic geometry is a non-Euclidean geometry which studies spaces of constant negative curvature. Our approach is based on the Poincar\u00e9 ball model (Nickel and Kiela, 2017) , which is a particular model of hyperbolic space and is wellsuited for gradient-based optimization. In particular, let ",
                "cite_spans": [
                    {
                        "start": 150,
                        "end": 174,
                        "text": "(Nickel and Kiela, 2017)",
                        "ref_id": "BIBREF20"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Hyperbolic Geometry",
                "sec_num": "3.3.1"
            },
            {
                "text": "B n = {x \u2208 R n | ||x|| < 1}",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Hyperbolic Geometry",
                "sec_num": "3.3.1"
            },
            {
                "text": "gx = 2 1 -||x|| 2 2 g E (5)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Hyperbolic Geometry",
                "sec_num": "3.3.1"
            },
            {
                "text": "where x \u2208 B n . g E denotes the Euclidean metric tensor. Furthermore, the distance between two points u, v \u2208 B n is given as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Hyperbolic Geometry",
                "sec_num": "3.3.1"
            },
            {
                "text": "d(u, v) = arcosh(1 + 2 ||u -v|| 2 (1 -||u|| 2 )(1 -||v|| 2 ) ) (6)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Hyperbolic Geometry",
                "sec_num": "3.3.1"
            },
            {
                "text": "where arcosh is the inverse hyperbolic cosine function, i.e., arcosh(x) = ln(x + (x 2 -1)). If we consider the origin O and two points u, v, when the two points moving towards the outside of the Poincar\u00e9 ball (i.e., ||u||, ||v|| \u2192 1), the distance",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Hyperbolic Geometry",
                "sec_num": "3.3.1"
            },
            {
                "text": "d(u, v) tends to d(u, O) + d(O, v).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Hyperbolic Geometry",
                "sec_num": "3.3.1"
            },
            {
                "text": "That is, the path between the two points converges to a path through the origin, which can be seen as a tree-like hierarchical structure.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Hyperbolic Geometry",
                "sec_num": "3.3.1"
            },
            {
                "text": "The tree-likeness of the hyperbolic space makes it natural to embed hierarchical structures. By embedding code hierarchy in the Poincar\u00e9 ball, the top codes are placed near the origin and bottom codes are near the boundary. The embedding norm represents depth in the hierarchy, and the distance between embeddings represents the similarity. Let D = {(l p , l q )} be the set of parent-child relations between code pairs. \u0398 = {\u03b8 i } T i=1 , \u03b8 i \u2208 B dp is the corresponding code embedding set, where T is the number of all ICD codes. In order to enforce related codes to be closer than unrelated codes, we minimize the following loss function to get the code hyperbolic representations when ||\u03b8 i || < 1(i = 1, . . . , L):",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Hyperbolic Code Embedder",
                "sec_num": "3.3.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "J (\u0398) = - (lp,lq )\u2208D log exp(-d(\u03b8p, \u03b8q)) l q \u2208N (lp) exp(-d(\u03b8p, \u03b8 q ))",
                        "eq_num": "(7)"
                    }
                ],
                "section": "Hyperbolic Code Embedder",
                "sec_num": "3.3.2"
            },
            {
                "text": "where N (l p ) = {l q |(l p , l q ) / \u2208 D} \u222a {l p } is the set of negative samples. The hyperbolic code representations in our work are denoted as \u0398 L = {\u03b8 i } L i=1 . d(\u2022) is the distance defined as Equation (6).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Hyperbolic Code Embedder",
                "sec_num": "3.3.2"
            },
            {
                "text": "To compute the similarities between document and codes in hyperbolic space, the code-aware document representations C = {c 1 , c 2 , . . . , c L } need to be projected into the hyperbolic space. We exploit the re-parameterization technique (Dhingra et al., 2018; L\u00f3pez et al., 2019) to implement it, which involves computing a direction vector r and a norm magnitude \u03b7. We use the c i as an example to illustrate the procedure:",
                "cite_spans": [
                    {
                        "start": 240,
                        "end": 262,
                        "text": "(Dhingra et al., 2018;",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 263,
                        "end": 282,
                        "text": "L\u00f3pez et al., 2019)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Hyperbolic Document Projector",
                "sec_num": "3.3.3"
            },
            {
                "text": "ri = \u03a6 dir (ci), ri = ri ||ri|| \u03b7 i = \u03a6norm(ci), \u03b7i = \u03c3(\u03b7 i ) (8)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Hyperbolic Document Projector",
                "sec_num": "3.3.3"
            },
            {
                "text": "where \u03a6 dir : R dc \u2192 R dp is the direction function. We parameterize it as a multi-layer perceptron (MLP). \u03a6 norm : R dc \u2192 R is the norm magnitude function. We use a linear layer to implement it. \u03c3 is the sigmoid function to ensure the resulting norm \u03b7 i \u2208 (0, 1). The re-parameterized document representation is defined as m i = \u03b7 i r i , which lies in hyperbolic space B dp .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Hyperbolic Document Projector",
                "sec_num": "3.3.3"
            },
            {
                "text": "The re-parameterization technique enables to project the code-aware document representation into the Poincar\u00e9 ball, which enables the avoidance of the stochastic Riemannian optimization method (Bonnabel, 2013) to learn the parameters in the hyperbolic space. Instead, we can exploit the deep learning optimization method to update the parameters in the entire model.",
                "cite_spans": [
                    {
                        "start": 193,
                        "end": 209,
                        "text": "(Bonnabel, 2013)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Hyperbolic Document Projector",
                "sec_num": "3.3.3"
            },
            {
                "text": "Since there doesn't exist a clear hyperbolic innerproduct, the cosine similarity is not appropriate to be the metric. In our work, we adopt the hyperbolic distance function to model the relationships between the document and codes. Since the hyperbolic document representation for each code has been obtained, we just need to compute the similarity with the corresponding hyperbolic code embedding:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Compute Document-Code Similarity",
                "sec_num": "3.3.4"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "scorei = d(mi, \u03b8i) S = [score1; score2; . . . ; scoreL]",
                        "eq_num": "(9)"
                    }
                ],
                "section": "Compute Document-Code Similarity",
                "sec_num": "3.3.4"
            },
            {
                "text": "where S \u2208 R L is the document-code similarity score.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Compute Document-Code Similarity",
                "sec_num": "3.3.4"
            },
            {
                "text": "[; ] is the concatenation operation. d(\u2022) is the distance function defined as Equation (6).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Compute Document-Code Similarity",
                "sec_num": "3.3.4"
            },
            {
                "text": "To exploit code co-occurrence, we exploit the graph to model code co-occurrence correlation, and then we use the GCN to learn code cooccurrence representations. In this section, we first construct the co-graph according to the statistics of the code cooccurrence in the training set, and then we exploit the GCN to encode the code co-occurrence correlation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Code-aware Document Representations via Graph Convolutional Network",
                "sec_num": "3.4"
            },
            {
                "text": "Given a graph with L nodes, we can represent the graph using a L \u00d7 L adjacency matrix A. To capture the co-occurrence correlations between codes, we build the code co-occurrence graph (co-graph), which utilizes the code co-occurrence matrix as the adjacency matrix. If the i-th code and the j-th code co-occur in the clinical text, there is an edge between them. Intuitively, if the i-th code co-appears with the j-th code more often than the k-th code, the probabilities of the i-th code and the j-th code should have stronger dependencies. Therefore, in our work, we use the co-appearing times between two codes as the connection weights in the adjacency matrix, which can represent the prior knowledge. For example, if the i-th code co-appears n times with the j-th code, we set A ij = n.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Code Co-graph Construction",
                "sec_num": "3.4.1"
            },
            {
                "text": "The inputs of GCN are initial representations of codes V which are obtained via Equation ( 2) and the adjacency matrix A. We use the standard convolution computation (Kipf and Welling, 2016) to encode code co-occurrence:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Code Co-occurrence Encoding via GCN",
                "sec_num": "3.4.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "H (l+1) = \u03c1( D-1 2 \u00c3 D-1 2 H (l) W (l) )",
                        "eq_num": "(10)"
                    }
                ],
                "section": "Code Co-occurrence Encoding via GCN",
                "sec_num": "3.4.2"
            },
            {
                "text": "where \u00c3 = A + I. I is the identity matrix, Dii = j \u00c3ij , H (l) \u2208 R L\u00d7dc and H (0) = V . \u03c1 is an activation function (e.g., ReLU). After co-occurrence correlation encoding via GCN, the code representations enable to capture the code co-occurrence correlations. Then, we use the codewise attention to obtain code-aware document representations, denoted as D = {d 1 , d 2 , . . . , d L }1 .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Code Co-occurrence Encoding via GCN",
                "sec_num": "3.4.2"
            },
            {
                "text": "After capturing the code hierarchy and code cooccurrence, we use an aggregation layer to fuse document-code similarity scores S and code-aware document representations D for enhancing representation with each other:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Aggregation Layer",
                "sec_num": "3.5"
            },
            {
                "text": "U = \u03bbWsS + D T W d (11)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Aggregation Layer",
                "sec_num": "3.5"
            },
            {
                "text": "where W s and W d are transformation matrixes. 1 : Comparison of our model and other baselines on the MIMIC-III dataset. We run our model 10 times and each time we use different random seeds for initialization. We report the mean \u00b1 standard deviation of each result.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 47,
                        "end": 48,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Aggregation Layer",
                "sec_num": "3.5"
            },
            {
                "text": "U = {u 1 , u 2 , . . . , u L } \u2208 R L are",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Aggregation Layer",
                "sec_num": "3.5"
            },
            {
                "text": "The prediction for each code is generated via:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training",
                "sec_num": "3.6"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "\u0177i = \u03c3(ui), i = 1, . . . , L",
                        "eq_num": "(12)"
                    }
                ],
                "section": "Training",
                "sec_num": "3.6"
            },
            {
                "text": "Our model is to be trained using a multi-label binary cross-entropy loss:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training",
                "sec_num": "3.6"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "L = L i=1 [-yilog( \u0177i) -(1 -yi)log(1 -\u0177i)]",
                        "eq_num": "(13)"
                    }
                ],
                "section": "Training",
                "sec_num": "3.6"
            },
            {
                "text": "where y i \u2208 {0, 1} is the ground truth for the i-th code.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training",
                "sec_num": "3.6"
            },
            {
                "text": "We evaluate our proposed model on two widely used datasets, including MIMIC-II (Jouhet et al., 2012) and MIMIC-III (Johnson et al., 2016) . Both datasets contain discharge summaries that are tagged by human coders with a set of ICD-9 codes. For MIMIC-III dataset, we use the same experimental setting as previous works (Shi et al., 2017; Mullenbach et al., 2018) . ",
                "cite_spans": [
                    {
                        "start": 79,
                        "end": 100,
                        "text": "(Jouhet et al., 2012)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 115,
                        "end": 137,
                        "text": "(Johnson et al., 2016)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 319,
                        "end": 337,
                        "text": "(Shi et al., 2017;",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 338,
                        "end": 362,
                        "text": "Mullenbach et al., 2018)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Datasets",
                "sec_num": "4.1"
            },
            {
                "text": "Following previous work (Mullenbach et al., 2018) , we use macro-averaged and micro-averaged F1, macro-averaged and micro-averaged AUC (area under the ROC, i.e., receiver operating characteristic curve) and Precision@N (P@N) as the metrics. The P@N indicates the proportion of the correctlypredicted labels in the top-N predicted labels.",
                "cite_spans": [
                    {
                        "start": 24,
                        "end": 49,
                        "text": "(Mullenbach et al., 2018)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Metrics and Parameter Settings",
                "sec_num": "4.2"
            },
            {
                "text": "Hyper-parameters are tuned on the development set by grid search. The word embedding size d e is 100. The convolution filter size is 10. The size of the filter output is 200. The dropout rate is 0.4. The \u03bb is 0.2. The batch size is 16. Adam (Kingma and Ba, 2014) is used for optimization with an initial learning rate 1e-4. We pre-train the word embeddings on the combination of training sets of MIMIC-II and MIMIC-III datasets by using word2vec toolkit (Mikolov et al., 2013) .",
                "cite_spans": [
                    {
                        "start": 454,
                        "end": 476,
                        "text": "(Mikolov et al., 2013)",
                        "ref_id": "BIBREF18"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Metrics and Parameter Settings",
                "sec_num": "4.2"
            },
            {
                "text": "SVM: A hierarchical support vector machine (SVM) is proposed by Perotte et al. (2013) to use the hierarchical nature of ICD codes, which is evaluated on the MIMIC-II dataset. C-MemNN: A condensed memory neural network is proposed by Prakash et al. (2017) to predict ICD codes on the MIMIC-III 50 dataset. C-LSTM-ATT: A character-aware LSTM based attention model is proposed by Shi et al. (2017) . It is also evaluated on the MIMIC-III 50 dataset. HA-GRU: A hierarchical attention gated recurrent unit model is proposed by Baumel et al. (2018) to predict ICD codes on the MIMIC-II dataset. CAML & DR-CAML: The convolutional attention network for multi-label classification (CAML) is proposed by Mullenbach et al. (2018) incorporates the code description. They achieve the state-of-the-art performance on the MIMIC-III and MIMIC-II datasets.",
                "cite_spans": [
                    {
                        "start": 64,
                        "end": 85,
                        "text": "Perotte et al. (2013)",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 233,
                        "end": 254,
                        "text": "Prakash et al. (2017)",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 377,
                        "end": 394,
                        "text": "Shi et al. (2017)",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 522,
                        "end": 542,
                        "text": "Baumel et al. (2018)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 694,
                        "end": 718,
                        "text": "Mullenbach et al. (2018)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baselines",
                "sec_num": "4.3"
            },
            {
                "text": "We repeat 10 times training and each time we use different random seeds for initialization. We report the mean \u00b1 standard deviation of each result. Table 1 and Table 2 show the results on the MIMIC-III and MIMIC-II datasets, respectively. Since some baselines are evaluated either on MIMIC-III or MIMIC-II, the baselines used for the two datasets are different. Overall, we observe that:",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 154,
                        "end": 155,
                        "text": "1",
                        "ref_id": null
                    },
                    {
                        "start": 166,
                        "end": 167,
                        "text": "2",
                        "ref_id": "TABREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Compared with State-of-the-art Methods",
                "sec_num": "4.4"
            },
            {
                "text": "(1) In Table 1 , our method HyperCore outperforms all the baselines on MIMIC-III dataset. For example, compared with the state-of-the-art model DR-CAML, our method achieves 2.2% and 3% improvements of Micro-F1 score on MIMIC-III full and MIMIC-III 50 respectively. It indicates that, as compared to neural network based models that handle each code in isolation, our method can better take advantage of the rich correlations among codes. In addition, the small standard deviations indicate that our model obtains stable good results.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 13,
                        "end": 14,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Compared with State-of-the-art Methods",
                "sec_num": "4.4"
            },
            {
                "text": "(2) As previous work (Mullenbach et al., 2018) , the Macro-F1 score of our method on MIMIC-III full is lower than that on the MIMIC-III 50. The reason is that MIMIC-III full has long-tail frequency distributions, and the Macro-F1 places more emphasis on rare code prediction. Therefore, it is difficult to achieve a high Macro-F1 score on MIMIC-III full. Nevertheless, our method still achieves the best result on the Macro-F1 metric. It indicates that our method is very effective.",
                "cite_spans": [
                    {
                        "start": 21,
                        "end": 46,
                        "text": "(Mullenbach et al., 2018)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Compared with State-of-the-art Methods",
                "sec_num": "4.4"
            },
            {
                "text": "(3) In Table 2 , our method HyperCore also achieves the best performance over all metrics on the MIMIC-II. Especially, compared with the stateof-the-art model DR-CAML, our method achieves 5.9% improvements of Macro-AUC, which indicates the effectiveness of our method.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 13,
                        "end": 14,
                        "text": "2",
                        "ref_id": "TABREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Compared with State-of-the-art Methods",
                "sec_num": "4.4"
            },
            {
                "text": "(4) As shown in Table 2 , the neural network based methods outperform the traditional model (SVM), which indicates the limitation of human-designed features and the advancement of neural networks for the automatic ICD coding.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 22,
                        "end": 23,
                        "text": "2",
                        "ref_id": "TABREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Compared with State-of-the-art Methods",
                "sec_num": "4.4"
            },
            {
                "text": "To investigate the effectiveness of the hyperbolic and co-graph representation, we conduct the ablation studies. The experimental results are listed in Table 3 . From the results, we can observe that:",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 158,
                        "end": 159,
                        "text": "3",
                        "ref_id": "TABREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Ablation Experiment",
                "sec_num": "4.5"
            },
            {
                "text": "(1) Effectiveness of Hyperbolic Representation. Compared with the model removed hyperbolic representation, the HyperCore improves the Micro-F1 score from 0.539 to 0.551 on MIMIC-III full dataset. It demonstrates the effectiveness of the hyperbolic representation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Ablation Experiment",
                "sec_num": "4.5"
            },
            {
                "text": "(2) Effectiveness of Co-graph Representation. Compared with the model removed the co-graph representation, the HyperCore model improves the performance, achieving 2.6% improvements of Micro-F1 score on the MIMIC-III 50 dataset. The great improvements indicate the co-graph representation is very effective.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Ablation Experiment",
                "sec_num": "4.5"
            },
            {
                "text": "(3) Effectiveness of Hyperbolic and Co-graph Representation. When we remove the hyperbolic and co-graph representation, the performance drops significantly. The Micro-F1 score drops from 0.477 to 0.439 on the MIMIC-II dataset. It indicates that simultaneously exploiting the hyperbolic and cograph representation is also very effective.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Ablation Experiment",
                "sec_num": "4.5"
            },
            {
                "text": "Since the dimensionality of the hperbolic code embeddings is very important for hyperbolic representation, we investigate its effect. The size of hyperbolic code embeddings is set 10, 20, 50, 70 and 100. Table 4 shows the results of our model on the MIMIC-III and MIMIC-II datasets. We have two important observations:",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 210,
                        "end": 211,
                        "text": "4",
                        "ref_id": "TABREF5"
                    }
                ],
                "eq_spans": [],
                "section": "The Analysis of Hyperbolic Code Embedding Dimension",
                "sec_num": "4.6.1"
            },
            {
                "text": "(1) The best hyperbolic code embedding dimensionality on MIMIC-III full is larger than it on MIMIC-III 50 and MIMIC-II. The reason may be that the number of codes in MIMIC-III full is more than other two datasets, which needs higherdimensional hyperbolic code embedding to represent the code hierarchy.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Analysis of Hyperbolic Code Embedding Dimension",
                "sec_num": "4.6.1"
            },
            {
                "text": "(2) The performance does not always improve when the hyperbolic code embedding size increases. We guess that low dimensional embeddings can capture the hierarchy and the network is prone to over-fitting when high dimensional hyperbolic code embeddings are used.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Analysis of Hyperbolic Code Embedding Dimension",
                "sec_num": "4.6.1"
            },
            {
                "text": "After embedding the ICD codes into the hyperbolic space, the top level codes will be placed near the origin and low level codes near the boundary, which can be reflected via their norms. Table 5 shows examples of ICD-9 codes and their hyperbolic norms. The first and second blocks list codes of \"Diseases of the Respiratory System\" and \"Diseases of the Digestive System\", respectively. As expected, the lower level codes have higher hyperbolic norms, and this approves that when the disease is more specific, the hyperbolic norm is larger. For example, code \"487.8 (influenza with other manifestations)\" has a higher norm than \"487 (influenza)\", and \"550.0 (inguinal hernia with gangrene)\" has a higher norm than \"550 (inguinal hernia)\". It indicates that the hyperbolic code embeddings can capture the code hierarchy.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 193,
                        "end": 194,
                        "text": "5",
                        "ref_id": "TABREF6"
                    }
                ],
                "eq_spans": [],
                "section": "The Hierarchy of Hyperbolic Code Embedding",
                "sec_num": "4.6.2"
            },
            {
                "text": "We give an example shown in Figure 4 to illustrate the visualization of code-wise attention and the effectiveness of hyperbolic and co-graph representation.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 35,
                        "end": 36,
                        "text": "4",
                        "ref_id": "FIGREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Case Study",
                "sec_num": "4.7"
            },
            {
                "text": "(1) Code-wise attention visualization: When the HyperCore model predicts the code \"518.81 (acute respiratory failure)\", it can assign larger weights to more informative words, like \"respiratory failure\" and \"chest tightness\". It shows the codes-wise attention enables to select the most informative words.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Case Study",
                "sec_num": "4.7"
            },
            {
                "text": "(2) The effectiveness of hyperbolic representations: Our proposed model and the CNN+Attention can both correctly predict the code \"518.81\". However, the CNN+Attention model gives contradictory predictions. Our proposed model can avoid the prediction contradictions by exploiting code hierarchy, which proves the effectiveness of hyperbolic representations.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Case Study",
                "sec_num": "4.7"
            },
            {
                "text": "(3) The effectiveness of co-graph representation: Although there is no very obvious clue to predict the code \"276.2 (acidosis)\", our model can exploit the co-occurrence between the code \"518.81\" and \"276.2\" to assist in inferring the code \"276.2\". It demonstrates the effectiveness of the co-graph representation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Case Study",
                "sec_num": "4.7"
            },
            {
                "text": "In this paper, we propose a novel hyperbolic and cograph representation framework for the automatic ICD coding task, which can jointly exploit code hierarchy and code co-occurrence. We exploit the hyperbolic representation learning method to leverage the code hierarchy in the hyperbolic space. Moreover, we use the graph convolutional network to capture the co-occurrence correlation. Experimental results on two widely used datasets indicate that our proposed model outperforms previous state-ofthe-art methods. We believe our method can also be applied to other tasks that need to exploit hierarchical label structure and label co-occurrence, such as fine-grained entity typing and hierarchical multi-label classification.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "5"
            },
            {
                "text": "C and D are both code-aware document representations, but D captures the code co-occurrence correlations.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "This work is supported by the Natural Key R&D Program of China (No.2017YFB1002101), the National Natural Science Foundation of China (No.61922085, No.61533018, No.61976211, No.61806201) and the Key Research Program of the Chinese Academy of Sciences (Grant NO. ZDBS-SSW-JSC006). This work is also supported by Beijing Academy of Artificial Intelligence (BAAI2019QN0301) and the CCF-Tencent Open Research Fund.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgments",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Improving palliative care with deep learning",
                "authors": [
                    {
                        "first": "Anand",
                        "middle": [],
                        "last": "Avati",
                        "suffix": ""
                    },
                    {
                        "first": "Kenneth",
                        "middle": [],
                        "last": "Jung",
                        "suffix": ""
                    },
                    {
                        "first": "Stephanie",
                        "middle": [],
                        "last": "Harman",
                        "suffix": ""
                    },
                    {
                        "first": "Lance",
                        "middle": [],
                        "last": "Downing",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Ng",
                        "suffix": ""
                    },
                    {
                        "first": "H",
                        "middle": [],
                        "last": "Nigam",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Shah",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "BMC medical informatics and decision making",
                "volume": "18",
                "issue": "4",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Anand Avati, Kenneth Jung, Stephanie Harman, Lance Downing, Andrew Ng, and Nigam H Shah. 2018. Improving palliative care with deep learn- ing. BMC medical informatics and decision making, 18(4):122.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Graph convolutional encoders for syntax-aware neural machine translation",
                "authors": [
                    {
                        "first": "Joost",
                        "middle": [],
                        "last": "Bastings",
                        "suffix": ""
                    },
                    {
                        "first": "Ivan",
                        "middle": [],
                        "last": "Titov",
                        "suffix": ""
                    },
                    {
                        "first": "Wilker",
                        "middle": [],
                        "last": "Aziz",
                        "suffix": ""
                    },
                    {
                        "first": "Diego",
                        "middle": [],
                        "last": "Marcheggiani",
                        "suffix": ""
                    },
                    {
                        "first": "Khalil",
                        "middle": [],
                        "last": "Simaan",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "1957--1967",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Joost Bastings, Ivan Titov, Wilker Aziz, Diego Marcheggiani, and Khalil Simaan. 2017. Graph convolutional encoders for syntax-aware neural ma- chine translation. In Proceedings of the 2017 Con- ference on Empirical Methods in Natural Language Processing, pages 1957-1967.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Multi-label classification of patient notes: case study on icd code assignment",
                "authors": [
                    {
                        "first": "Tal",
                        "middle": [],
                        "last": "Baumel",
                        "suffix": ""
                    },
                    {
                        "first": "Jumana",
                        "middle": [],
                        "last": "Nassour-Kassis",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Workshops at the Thirty-Second AAAI Conference on Artificial Intelligence",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tal Baumel, Jumana Nassour-Kassis, Raphael Co- hen, Michael Elhadad, and Noemie Elhadad. 2018. Multi-label classification of patient notes: case study on icd code assignment. In Workshops at the Thirty- Second AAAI Conference on Artificial Intelligence.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Stochastic gradient descent on riemannian manifolds",
                "authors": [
                    {
                        "first": "Silvere",
                        "middle": [],
                        "last": "Bonnabel",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "IEEE Transactions on Automatic Control",
                "volume": "58",
                "issue": "9",
                "pages": "2217--2229",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Silvere Bonnabel. 2013. Stochastic gradient descent on riemannian manifolds. IEEE Transactions on Auto- matic Control, 58(9):2217-2229.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Doctor ai: Predicting clinical events via recurrent neural networks",
                "authors": [
                    {
                        "first": "Edward",
                        "middle": [],
                        "last": "Choi",
                        "suffix": ""
                    },
                    {
                        "first": "Mohammad",
                        "middle": [
                            "Taha"
                        ],
                        "last": "Bahadori",
                        "suffix": ""
                    },
                    {
                        "first": "Andy",
                        "middle": [],
                        "last": "Schuetz",
                        "suffix": ""
                    },
                    {
                        "first": "Walter",
                        "middle": [
                            "F"
                        ],
                        "last": "Stewart",
                        "suffix": ""
                    },
                    {
                        "first": "Jimeng",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Machine Learning for Healthcare Conference",
                "volume": "",
                "issue": "",
                "pages": "301--318",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Edward Choi, Mohammad Taha Bahadori, Andy Schuetz, Walter F Stewart, and Jimeng Sun. 2016. Doctor ai: Predicting clinical events via recurrent neural networks. In Machine Learning for Health- care Conference, pages 301-318.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Embedding text in hyperbolic spaces",
                "authors": [
                    {
                        "first": "Bhuwan",
                        "middle": [],
                        "last": "Dhingra",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [],
                        "last": "Shallue",
                        "suffix": ""
                    },
                    {
                        "first": "Mohammad",
                        "middle": [],
                        "last": "Norouzi",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Dai",
                        "suffix": ""
                    },
                    {
                        "first": "George",
                        "middle": [],
                        "last": "Dahl",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the Twelfth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-12)",
                "volume": "",
                "issue": "",
                "pages": "59--69",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Bhuwan Dhingra, Christopher Shallue, Mohammad Norouzi, Andrew Dai, and George Dahl. 2018. Em- bedding text in hyperbolic spaces. In Proceedings of the Twelfth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-12), pages 59-69.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Hyperbolic attention networks",
                "authors": [
                    {
                        "first": "Caglar",
                        "middle": [],
                        "last": "Gulcehre",
                        "suffix": ""
                    },
                    {
                        "first": "Misha",
                        "middle": [],
                        "last": "Denil",
                        "suffix": ""
                    },
                    {
                        "first": "Mateusz",
                        "middle": [],
                        "last": "Malinowski",
                        "suffix": ""
                    },
                    {
                        "first": "Ali",
                        "middle": [],
                        "last": "Razavi",
                        "suffix": ""
                    },
                    {
                        "first": "Razvan",
                        "middle": [],
                        "last": "Pascanu",
                        "suffix": ""
                    },
                    {
                        "first": "Karl",
                        "middle": [
                            "Moritz"
                        ],
                        "last": "Hermann",
                        "suffix": ""
                    },
                    {
                        "first": "Peter",
                        "middle": [],
                        "last": "Battaglia",
                        "suffix": ""
                    },
                    {
                        "first": "Victor",
                        "middle": [],
                        "last": "Bapst",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Raposo",
                        "suffix": ""
                    },
                    {
                        "first": "Adam",
                        "middle": [],
                        "last": "Santoro",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of International Conference on Learning Representations",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Caglar Gulcehre, Misha Denil, Mateusz Malinowski, Ali Razavi, Razvan Pascanu, Karl Moritz Hermann, Peter Battaglia, Victor Bapst, David Raposo, Adam Santoro, et al. 2018. Hyperbolic attention net- works. In Proceedings of International Conference on Learning Representations.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "On the tree-likeness of hyperbolic spaces",
                "authors": [
                    {
                        "first": "Matthias",
                        "middle": [],
                        "last": "Hamann",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Mathematical Proceedings of the Cambridge Philosophical Society",
                "volume": "164",
                "issue": "",
                "pages": "345--361",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Matthias Hamann. 2018. On the tree-likeness of hy- perbolic spaces. In Mathematical Proceedings of the Cambridge Philosophical Society, volume 164, pages 345-361. Cambridge University Press.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Mimiciii, a freely accessible critical care database",
                "authors": [
                    {
                        "first": "E",
                        "middle": [
                            "W"
                        ],
                        "last": "Alistair",
                        "suffix": ""
                    },
                    {
                        "first": "Tom",
                        "middle": [
                            "J"
                        ],
                        "last": "Johnson",
                        "suffix": ""
                    },
                    {
                        "first": "Lu",
                        "middle": [],
                        "last": "Pollard",
                        "suffix": ""
                    },
                    {
                        "first": "H",
                        "middle": [],
                        "last": "Shen",
                        "suffix": ""
                    },
                    {
                        "first": "Mengling",
                        "middle": [],
                        "last": "Lehman Li-Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Mohammad",
                        "middle": [],
                        "last": "Feng",
                        "suffix": ""
                    },
                    {
                        "first": "Benjamin",
                        "middle": [],
                        "last": "Ghassemi",
                        "suffix": ""
                    },
                    {
                        "first": "Peter",
                        "middle": [],
                        "last": "Moody",
                        "suffix": ""
                    },
                    {
                        "first": "Leo",
                        "middle": [],
                        "last": "Szolovits",
                        "suffix": ""
                    },
                    {
                        "first": "Roger",
                        "middle": [
                            "G"
                        ],
                        "last": "Anthony Celi",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Mark",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Scientific data",
                "volume": "3",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alistair EW Johnson, Tom J Pollard, Lu Shen, H Lehman Li-wei, Mengling Feng, Moham- mad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. 2016. Mimic- iii, a freely accessible critical care database. Scien- tific data, 3:160035.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Automated classification of free-text pathology reports for registration of incident cases of cancer",
                "authors": [
                    {
                        "first": "Vianney",
                        "middle": [],
                        "last": "Jouhet",
                        "suffix": ""
                    },
                    {
                        "first": "Georges",
                        "middle": [],
                        "last": "Defossez",
                        "suffix": ""
                    },
                    {
                        "first": "Anita",
                        "middle": [],
                        "last": "Burgun",
                        "suffix": ""
                    },
                    {
                        "first": "Pierre",
                        "middle": [
                            "Le"
                        ],
                        "last": "Beux",
                        "suffix": ""
                    },
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Levillain",
                        "suffix": ""
                    },
                    {
                        "first": "Pierre",
                        "middle": [],
                        "last": "Ingrand",
                        "suffix": ""
                    },
                    {
                        "first": "Vincent",
                        "middle": [],
                        "last": "Claveau",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Methods of information in medicine",
                "volume": "51",
                "issue": "03",
                "pages": "242--251",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Vianney Jouhet, Georges Defossez, Anita Burgun, Pierre Le Beux, P Levillain, Pierre Ingrand, and Vincent Claveau. 2012. Automated classification of free-text pathology reports for registration of in- cident cases of cancer. Methods of information in medicine, 51(03):242-251.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Adam: A method for stochastic optimization",
                "authors": [
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Diederik",
                        "suffix": ""
                    },
                    {
                        "first": "Jimmy",
                        "middle": [],
                        "last": "Kingma",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Ba",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1412.6980"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Semisupervised classification with graph convolutional networks",
                "authors": [
                    {
                        "first": "N",
                        "middle": [],
                        "last": "Thomas",
                        "suffix": ""
                    },
                    {
                        "first": "Max",
                        "middle": [],
                        "last": "Kipf",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Welling",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of International Conference on Learning Representations",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Thomas N Kipf and Max Welling. 2016. Semi- supervised classification with graph convolutional networks. In Proceedings of International Confer- ence on Learning Representations.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Automatic icd-10 classification of cancers from free-text death certificates",
                "authors": [
                    {
                        "first": "Bevan",
                        "middle": [],
                        "last": "Koopman",
                        "suffix": ""
                    },
                    {
                        "first": "Guido",
                        "middle": [],
                        "last": "Zuccon",
                        "suffix": ""
                    },
                    {
                        "first": "Anthony",
                        "middle": [],
                        "last": "Nguyen",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "International journal of medical informatics",
                "volume": "84",
                "issue": "11",
                "pages": "956--965",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Bevan Koopman, Guido Zuccon, Anthony Nguyen, An- ton Bergheim, and Narelle Grayson. 2015. Auto- matic icd-10 classification of cancers from free-text death certificates. International journal of medical informatics, 84(11):956-965.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Hyperbolic geometry of complex networks",
                "authors": [
                    {
                        "first": "Dmitri",
                        "middle": [],
                        "last": "Krioukov",
                        "suffix": ""
                    },
                    {
                        "first": "Fragkiskos",
                        "middle": [],
                        "last": "Papadopoulos",
                        "suffix": ""
                    },
                    {
                        "first": "Maksim",
                        "middle": [],
                        "last": "Kitsak",
                        "suffix": ""
                    },
                    {
                        "first": "Amin",
                        "middle": [],
                        "last": "Vahdat",
                        "suffix": ""
                    },
                    {
                        "first": "Mari\u00e1n",
                        "middle": [],
                        "last": "Bogun\u00e1",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Physical Review E",
                "volume": "82",
                "issue": "3",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dmitri Krioukov, Fragkiskos Papadopoulos, Maksim Kitsak, Amin Vahdat, and Mari\u00e1n Bogun\u00e1. 2010. Hyperbolic geometry of complex networks. Physi- cal Review E, 82(3):036106.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Consultant report-natural language processing in the health care industry. Cincinnati Children's",
                "authors": [
                    {
                        "first": "Dee",
                        "middle": [],
                        "last": "Lang",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Hospital Medical Center",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dee Lang. 2007. Consultant report-natural language processing in the health care industry. Cincinnati Children's Hospital Medical Center, Winter, 6.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Combining classifiers in text categorization",
                "authors": [
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Leah",
                        "suffix": ""
                    },
                    {
                        "first": "W Bruce",
                        "middle": [],
                        "last": "Larkey",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Croft",
                        "suffix": ""
                    }
                ],
                "year": 1996,
                "venue": "SIGIR",
                "volume": "",
                "issue": "",
                "pages": "289--297",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Leah S Larkey and W Bruce Croft. 1996. Combining classifiers in text categorization. In SIGIR, pages 289-297. Citeseer.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Fine-grained entity typing in hyperbolic space",
                "authors": [
                    {
                        "first": "Federico",
                        "middle": [],
                        "last": "L\u00f3pez",
                        "suffix": ""
                    },
                    {
                        "first": "Benjamin",
                        "middle": [],
                        "last": "Heinzerling",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Strube",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)",
                "volume": "",
                "issue": "",
                "pages": "169--180",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Federico L\u00f3pez, Benjamin Heinzerling, and Michael Strube. 2019. Fine-grained entity typing in hyper- bolic space. In Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP- 2019), pages 169-180. Association for Computa- tional Linguistics.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Encoding sentences with graph convolutional networks for semantic role labeling",
                "authors": [
                    {
                        "first": "Diego",
                        "middle": [],
                        "last": "Marcheggiani",
                        "suffix": ""
                    },
                    {
                        "first": "Ivan",
                        "middle": [],
                        "last": "Titov",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "1506--1515",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Diego Marcheggiani and Ivan Titov. 2017. Encoding sentences with graph convolutional networks for se- mantic role labeling. In Proceedings of the 2017 Conference on Empirical Methods in Natural Lan- guage Processing, pages 1506-1515.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Efficient estimation of word representations in vector space",
                "authors": [
                    {
                        "first": "Tomas",
                        "middle": [],
                        "last": "Mikolov",
                        "suffix": ""
                    },
                    {
                        "first": "Kai",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Greg",
                        "middle": [],
                        "last": "Corrado",
                        "suffix": ""
                    },
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Dean",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1301.3781"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Tomas Mikolov, Kai Chen, Greg Corrado, and Jef- frey Dean. 2013. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Explainable prediction of medical codes from clinical text",
                "authors": [
                    {
                        "first": "James",
                        "middle": [],
                        "last": "Mullenbach",
                        "suffix": ""
                    },
                    {
                        "first": "Sarah",
                        "middle": [],
                        "last": "Wiegreffe",
                        "suffix": ""
                    },
                    {
                        "first": "Jon",
                        "middle": [],
                        "last": "Duke",
                        "suffix": ""
                    },
                    {
                        "first": "Jimeng",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Jacob",
                        "middle": [],
                        "last": "Eisenstein",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 2018 Conference of the North American Chapter",
                "volume": "",
                "issue": "",
                "pages": "1101--1111",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "James Mullenbach, Sarah Wiegreffe, Jon Duke, Jimeng Sun, and Jacob Eisenstein. 2018. Explainable pre- diction of medical codes from clinical text. In Pro- ceedings of the 2018 Conference of the North Amer- ican Chapter of the Association for Computational Linguistics: Human Language Technologies, Vol- ume 1 (Long Papers), pages 1101-1111.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Poincar\u00e9 embeddings for learning hierarchical representations",
                "authors": [
                    {
                        "first": "Maximillian",
                        "middle": [],
                        "last": "Nickel",
                        "suffix": ""
                    },
                    {
                        "first": "Douwe",
                        "middle": [],
                        "last": "Kiela",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Advances in neural information processing systems",
                "volume": "",
                "issue": "",
                "pages": "6338--6347",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Maximillian Nickel and Douwe Kiela. 2017. Poincar\u00e9 embeddings for learning hierarchical representa- tions. In Advances in neural information processing systems, pages 6338-6347.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Learning continuous hierarchies in the lorentz model of hyperbolic geometry",
                "authors": [
                    {
                        "first": "Maximillian",
                        "middle": [],
                        "last": "Nickel",
                        "suffix": ""
                    },
                    {
                        "first": "Douwe",
                        "middle": [],
                        "last": "Kiela",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "International Conference on Machine Learning",
                "volume": "",
                "issue": "",
                "pages": "3776--3785",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Maximillian Nickel and Douwe Kiela. 2018. Learning continuous hierarchies in the lorentz model of hy- perbolic geometry. In International Conference on Machine Learning, pages 3776-3785.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Diagnosis code assignment: models and evaluation metrics",
                "authors": [
                    {
                        "first": "Rimma",
                        "middle": [],
                        "last": "Adler Perotte",
                        "suffix": ""
                    },
                    {
                        "first": "Karthik",
                        "middle": [],
                        "last": "Pivovarov",
                        "suffix": ""
                    },
                    {
                        "first": "Nicole",
                        "middle": [],
                        "last": "Natarajan",
                        "suffix": ""
                    },
                    {
                        "first": "Frank",
                        "middle": [],
                        "last": "Weiskopf",
                        "suffix": ""
                    },
                    {
                        "first": "No\u00e9mie",
                        "middle": [],
                        "last": "Wood",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Elhadad",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Journal of the American Medical Informatics Association",
                "volume": "21",
                "issue": "2",
                "pages": "231--237",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Adler Perotte, Rimma Pivovarov, Karthik Natarajan, Nicole Weiskopf, Frank Wood, and No\u00e9mie Elhadad. 2013. Diagnosis code assignment: models and eval- uation metrics. Journal of the American Medical In- formatics Association, 21(2):231-237.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Condensed memory networks for clinical diagnostic inferencing",
                "authors": [
                    {
                        "first": "Aaditya",
                        "middle": [],
                        "last": "Prakash",
                        "suffix": ""
                    },
                    {
                        "first": "Siyuan",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Sadid",
                        "suffix": ""
                    },
                    {
                        "first": "Vivek",
                        "middle": [],
                        "last": "Hasan",
                        "suffix": ""
                    },
                    {
                        "first": "Kathy",
                        "middle": [],
                        "last": "Datla",
                        "suffix": ""
                    },
                    {
                        "first": "Ashequl",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Joey",
                        "middle": [],
                        "last": "Qadir",
                        "suffix": ""
                    },
                    {
                        "first": "Oladimeji",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Farri",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence",
                "volume": "",
                "issue": "",
                "pages": "3274--3280",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Aaditya Prakash, Siyuan Zhao, Sadid A Hasan, Vivek Datla, Kathy Lee, Ashequl Qadir, Joey Liu, and Oladimeji Farri. 2017. Condensed memory net- works for clinical diagnostic inferencing. In Pro- ceedings of the Thirty-First AAAI Conference on Ar- tificial Intelligence, pages 3274-3280. AAAI Press.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Fewshot and zero-shot multi-label learning for structured label spaces",
                "authors": [
                    {
                        "first": "Anthony",
                        "middle": [],
                        "last": "Rios",
                        "suffix": ""
                    },
                    {
                        "first": "Ramakanth",
                        "middle": [],
                        "last": "Kavuluru",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "3132--3142",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Anthony Rios and Ramakanth Kavuluru. 2018. Few- shot and zero-shot multi-label learning for structured label spaces. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Process- ing, pages 3132-3142.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Towards automated icd coding using deep learning",
                "authors": [
                    {
                        "first": "Haoran",
                        "middle": [],
                        "last": "Shi",
                        "suffix": ""
                    },
                    {
                        "first": "Pengtao",
                        "middle": [],
                        "last": "Xie",
                        "suffix": ""
                    },
                    {
                        "first": "Zhiting",
                        "middle": [],
                        "last": "Hu",
                        "suffix": ""
                    },
                    {
                        "first": "Ming",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Eric",
                        "middle": [
                            "P"
                        ],
                        "last": "Xing",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1711.04075"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Haoran Shi, Pengtao Xie, Zhiting Hu, Ming Zhang, and Eric P Xing. 2017. Towards automated icd coding using deep learning. arXiv preprint arXiv:1711.04075.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Hyperbolic representation learning for fast and efficient neural question answering",
                "authors": [
                    {
                        "first": "Yi",
                        "middle": [],
                        "last": "Tay",
                        "suffix": ""
                    },
                    {
                        "first": "Anh",
                        "middle": [],
                        "last": "Luu",
                        "suffix": ""
                    },
                    {
                        "first": "Siu",
                        "middle": [],
                        "last": "Tuan",
                        "suffix": ""
                    },
                    {
                        "first": "Hui",
                        "middle": [],
                        "last": "Cheung",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining",
                "volume": "",
                "issue": "",
                "pages": "583--591",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yi Tay, Luu Anh Tuan, and Siu Cheung Hui. 2018. Hyperbolic representation learning for fast and effi- cient neural question answering. In Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining, pages 583-591. ACM.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Graph attention networks",
                "authors": [
                    {
                        "first": "Petar",
                        "middle": [],
                        "last": "Veli\u010dkovi\u0107",
                        "suffix": ""
                    },
                    {
                        "first": "Guillem",
                        "middle": [],
                        "last": "Cucurull",
                        "suffix": ""
                    },
                    {
                        "first": "Arantxa",
                        "middle": [],
                        "last": "Casanova",
                        "suffix": ""
                    },
                    {
                        "first": "Adriana",
                        "middle": [],
                        "last": "Romero",
                        "suffix": ""
                    },
                    {
                        "first": "Pietro",
                        "middle": [],
                        "last": "Lio",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of International Conference on Learning Representations",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Petar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. 2017. Graph attention networks. In Proceedings of International Conference on Learning Represen- tations.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "A neural architecture for automated icd coding",
                "authors": [
                    {
                        "first": "Pengtao",
                        "middle": [],
                        "last": "Xie",
                        "suffix": ""
                    },
                    {
                        "first": "Eric",
                        "middle": [],
                        "last": "Xing",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
                "volume": "1",
                "issue": "",
                "pages": "1066--1076",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Pengtao Xie and Eric Xing. 2018. A neural architec- ture for automated icd coding. In Proceedings of the 56th Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers), pages 1066-1076.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Graph-based neural multi-document summarization",
                "authors": [
                    {
                        "first": "Michihiro",
                        "middle": [],
                        "last": "Yasunaga",
                        "suffix": ""
                    },
                    {
                        "first": "Rui",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Kshitijh",
                        "middle": [],
                        "last": "Meelu",
                        "suffix": ""
                    },
                    {
                        "first": "Ayush",
                        "middle": [],
                        "last": "Pareek",
                        "suffix": ""
                    },
                    {
                        "first": "Krishnan",
                        "middle": [],
                        "last": "Srinivasan",
                        "suffix": ""
                    },
                    {
                        "first": "Dragomir",
                        "middle": [],
                        "last": "Radev",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the 21st Conference on Computational Natural Language Learning",
                "volume": "",
                "issue": "",
                "pages": "452--462",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Michihiro Yasunaga, Rui Zhang, Kshitijh Meelu, Ayush Pareek, Krishnan Srinivasan, and Dragomir Radev. 2017. Graph-based neural multi-document summarization. In Proceedings of the 21st Confer- ence on Computational Natural Language Learning (CoNLL 2017), pages 452-462.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "Automatic icd code assignment of chinese clinical notes based on multilayer attention birnn",
                "authors": [
                    {
                        "first": "Ying",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Min",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Liangliang",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Zhihui",
                        "middle": [],
                        "last": "Fei",
                        "suffix": ""
                    },
                    {
                        "first": "Fang-Xiang",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Jianxin",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Journal of biomedical informatics",
                "volume": "91",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ying Yu, Min Li, Liangliang Liu, Zhihui Fei, Fang- Xiang Wu, and Jianxin Wang. 2019. Automatic icd code assignment of chinese clinical notes based on multilayer attention birnn. Journal of biomedical in- formatics, 91:103114.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 1: An example of automatic ICD coding task. The input and output of the automatic ICD coding model are clinical text and predicted ICD codes, respectively. For better understanding, we add the corresponding disease name for each code.",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Figure 2: An example of ICD-9 descriptors and the derived hierarchical structure.",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "Figure3: The architecture of Hyperbolic and Co-graph Representation method (HyperCore). In the Poincar\u00e9 ball B n , we show the embeded code hierarchy (i.e., tree-like hierarchical structure). The dots l i (i = 1, 2, 3) on the treelike hierarchical structure and triangles m i (i = 1, 2, 3) in the Poincar\u00e9 ball denote hyperbolic code embeddings and hyperbolic document representations, respectively.",
                "uris": null,
                "fig_num": "3",
                "type_str": "figure"
            },
            "FIGREF3": {
                "num": null,
                "text": "Figure 4: An example to illustrate the effectiveness of the proposed model. The green bold codes indicate they are highly correlated. The red bold codes denote there exists contradictions between them.",
                "uris": null,
                "fig_num": "4",
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table/>",
                "type_str": "table",
                "text": "be the open n-dimensional unit ball, where || \u2022 || denotes the Euclidean norm. The Poincar\u00e9 ball (B n , g x ) is defined by the Riemannian manifold, i.e., the open unit ball equipped with the Riemannian metric tensor:",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table><tr><td>Model</td><td colspan=\"2\">AUC Macro Micro</td><td>Macro</td><td>F1</td><td>Micro</td><td>P@8</td></tr><tr><td>SVM</td><td>-</td><td>-</td><td>-</td><td/><td>0.293</td><td>-</td></tr><tr><td>HA-GRU</td><td>-</td><td>-</td><td>-</td><td/><td>0.366</td><td>-</td></tr><tr><td>CAML</td><td>0.820</td><td>0.966</td><td>0.048</td><td/><td>0.442</td><td>0.523</td></tr><tr><td colspan=\"2\">DR-CAML 0.826</td><td>0.966</td><td>0.049</td><td/><td>0.457</td><td>0.515</td></tr><tr><td>HyperCore</td><td colspan=\"6\">0.885 \u00b10.001 \u00b10.004 \u00b10.002 \u00b10.003 \u00b10.003 0.971 0.070 0.477 0.537</td></tr></table>",
                "type_str": "table",
                "text": "Experimental results are shown in means \u00b1 standard deviations on the MIMIC-II dataset.",
                "html": null,
                "num": null
            },
            "TABREF4": {
                "content": "<table><tr><td>Models</td><td colspan=\"6\">MIMIC-III full Macro-F1 Micro-F1 Macro-F1 Micro-F1 Macro-F1 Micro-F1 MIMIC-III 50 MIMIC-II</td></tr><tr><td>HyperCore</td><td>0.090</td><td>0.551</td><td>0.609</td><td>0.663</td><td>0.070</td><td>0.477</td></tr><tr><td>w/o hyperbolic representation</td><td>0.081</td><td>0.539</td><td>0.576</td><td>0.645</td><td>0.062</td><td>0.464</td></tr><tr><td>w/o co-graph representation</td><td>0.085</td><td>0.541</td><td>0.582</td><td>0.637</td><td>0.055</td><td>0.453</td></tr><tr><td>w/o hyperbolic and co-graph representation</td><td>0.077</td><td>0.531</td><td>0.570</td><td>0.626</td><td>0.047</td><td>0.439</td></tr></table>",
                "type_str": "table",
                "text": ". DR-CAML is an extension of CAML which Ablation study by removing the main components, where \"w/o\" indicates without.",
                "html": null,
                "num": null
            },
            "TABREF5": {
                "content": "<table><tr><td>Dimensionality</td><td colspan=\"9\">MIMIC-III full Macro-F1 Micro-F1 P@8 Macro-F1 Micro-F1 P@5 Macro-F1 Micro-F1 P@8 MIMIC-III 50 MIMIC-II</td></tr><tr><td>10</td><td>0.083</td><td>0.539</td><td>0.701</td><td>0.593</td><td>0.651</td><td>0.619</td><td>0.064</td><td>0.463</td><td>0.528</td></tr><tr><td>20</td><td>0.085</td><td>0.542</td><td>0.704</td><td>0.598</td><td>0.656</td><td>0.625</td><td>0.066</td><td>0.471</td><td>0.532</td></tr><tr><td>50</td><td>0.087</td><td>0.547</td><td>0.708</td><td>0.609</td><td>0.663</td><td>0.632</td><td>0.070</td><td>0.477</td><td>0.537</td></tr><tr><td>70</td><td>0.090</td><td>0.551</td><td>0.722</td><td>0.605</td><td>0.660</td><td>0.627</td><td>0.065</td><td>0.473</td><td>0.534</td></tr><tr><td>100</td><td>0.083</td><td>0.548</td><td>0.710</td><td>0.602</td><td>0.659</td><td>0.625</td><td>0.064</td><td>0.473</td><td>0.530</td></tr><tr><td>ICD-9 code</td><td/><td/><td>Norm</td><td/><td/><td/><td/><td/><td/></tr><tr><td colspan=\"4\">460-519 (Diseases of the Respiratory System) 0.455</td><td/><td/><td/><td/><td/><td/></tr><tr><td colspan=\"3\">480-488 (Pneumonia and Influenza)</td><td>0.520</td><td/><td/><td/><td/><td/><td/></tr><tr><td>487 (Influenza)</td><td/><td/><td>0.568</td><td/><td/><td/><td/><td/><td/></tr><tr><td colspan=\"3\">487.8 (Influenza with other manifestations)</td><td>0.928</td><td/><td/><td/><td/><td/><td/></tr><tr><td colspan=\"3\">520-579 (Diseases of the Digestive System)</td><td>0.412</td><td/><td/><td/><td/><td/><td/></tr><tr><td colspan=\"3\">550-579 (Hernia of Abdominal Cavity)</td><td>0.472</td><td/><td/><td/><td/><td/><td/></tr><tr><td colspan=\"2\">550 (Inguinal hernia)</td><td/><td>0.590</td><td/><td/><td/><td/><td/><td/></tr><tr><td colspan=\"3\">550.0 (Inguinal hernia with gangrene)</td><td>0.902</td><td/><td/><td/><td/><td/><td/></tr></table>",
                "type_str": "table",
                "text": "Experimental results of HyperCore with different size of hyperbolic code embeddings.",
                "html": null,
                "num": null
            },
            "TABREF6": {
                "content": "<table/>",
                "type_str": "table",
                "text": "The first and second blocks list some codes and their hyperbolic norms of ''Diseases of the Respiratory System\" and \"Diseases of the Digestive System\", respectively. In each block, the disease becomes more specific from top to bottom. The norms of codes increase with the depth.",
                "html": null,
                "num": null
            }
        }
    }
}