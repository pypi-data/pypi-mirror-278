{
    "paper_id": "P11-2109",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:32:57.579451Z"
    },
    "title": "Generalized Interpolation in Decision Tree LM",
    "authors": [
        {
            "first": "Denis",
            "middle": [],
            "last": "Filimonov",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Mary",
            "middle": [],
            "last": "Harper",
            "suffix": "",
            "affiliation": {},
            "email": "mharper@umd.edu"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "In the face of sparsity, statistical models are often interpolated with lower order (backoff) models, particularly in Language Modeling. In this paper, we argue that there is a relation between the higher order and the backoff model that must be satisfied in order for the interpolation to be effective. We show that in n-gram models, the relation is trivially held, but in models that allow arbitrary clustering of context (such as decision tree models), this relation is generally not satisfied. Based on this insight, we also propose a generalization of linear interpolation which significantly improves the performance of a decision tree language model.",
    "pdf_parse": {
        "paper_id": "P11-2109",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "In the face of sparsity, statistical models are often interpolated with lower order (backoff) models, particularly in Language Modeling. In this paper, we argue that there is a relation between the higher order and the backoff model that must be satisfied in order for the interpolation to be effective. We show that in n-gram models, the relation is trivially held, but in models that allow arbitrary clustering of context (such as decision tree models), this relation is generally not satisfied. Based on this insight, we also propose a generalization of linear interpolation which significantly improves the performance of a decision tree language model.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "A prominent use case for Language Models (LMs) in NLP applications such as Automatic Speech Recognition (ASR) and Machine Translation (MT) is selection of the most fluent word sequence among multiple hypotheses. Statistical LMs formulate the problem as the computation of the model's probability to generate the word sequence w 1 w 2 . . . w m \u2261 w m 1 , assuming that higher probability corresponds to more fluent hypotheses. LMs are often represented in the following generative form:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "p(w m 1 ) = m i=1 p(w i |w i-1 1 )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In the following discussion, we will refer to the function p(w i |w i-1 1 ) as a language model.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Note the context space for this function, w i-1 1 is arbitrarily long, necessitating some independence assumption, which usually consists of reducing the relevant context to n -1 immediately preceding tokens:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "p(w i |w i-1 1 ) \u2248 p(w i |w i-1 i-n+1 )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "These distributions are typically estimated from observed counts of n-grams w i i-n+1 in the training data. The context space is still far too large; therefore, the models are recursively smoothed using lower order distributions. For instance, in a widely used n-gram LM, the probabilities are estimated as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "p(w i |w i-1 i-n+1 ) = \u03c1(w i |w i-1 i-n+1 ) + (1) \u03b3(w i-1 i-n+1 ) \u2022 p(w i |w i-1 i-n+2 )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "where \u03c1 is a discounted probability 1 .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In addition to n-gram models, there are many other ways to estimate probability distributions p(w i |w i-1 i-n+1 ); in this work, we are particularly interested in models involving decision trees (DTs). As in n-gram models, DT models also often utilize interpolation with lower order models; however, there are issues concerning the interpolation which arise from the fact that decision trees permit arbitrary clustering of context, and these issues are the main subject of this paper.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The vast context space in a language model mandates the use of context clustering in some form. In n-gram models, the clustering can be represented as a k-ary decision tree of depth n -1, where k is the size of the vocabulary. Note that this is a very constrained form of a decision tree, and is probably suboptimal. Indeed, it is likely that some of the clusters predict very similar distributions of words, and the model would benefit from merging them. Therefore, it is reasonable to believe that arbitrary (i.e., unconstrained) context clustering such as a decision tree should be able to outperform the n-gram model.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Decision Trees",
                "sec_num": "2"
            },
            {
                "text": "A decision tree provides us with a clustering function \u03a6(w i-1 i-n+1 ) \u2192 {\u03a6 1 , . . . , \u03a6 N }, where N is the number of clusters (leaves in the DT), and clusters \u03a6 k are disjoint subsets of the context space; the probability estimation is approximated as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Decision Trees",
                "sec_num": "2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "p(w i |w i-1 i-n+1 ) \u2248 p(w i |\u03a6(w i-1 i-n+1 ))",
                        "eq_num": "(2)"
                    }
                ],
                "section": "Decision Trees",
                "sec_num": "2"
            },
            {
                "text": "Methods of DT construction and probability estimation used in this work are based on (Filimonov and Harper, 2009) ; therefore, we refer the reader to that paper for details. Another advantage of using decision trees is the ease of adding parameters such as syntactic tags:",
                "cite_spans": [
                    {
                        "start": 85,
                        "end": 113,
                        "text": "(Filimonov and Harper, 2009)",
                        "ref_id": "BIBREF2"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Decision Trees",
                "sec_num": "2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "p(w m 1 ) = X t 1 ...tm p(w m 1 t m 1 ) = X t 1 ...tm m Y i=1 p(witi|w i-1 1 t i-1 1 ) \u2248 X t 1 ...tm m Y i=1 p(witi|\u03a6(w i-1 i-n+1 t i-1 i-n+1 ))",
                        "eq_num": "(3)"
                    }
                ],
                "section": "Decision Trees",
                "sec_num": "2"
            },
            {
                "text": "In this case, the decision tree would cluster the context space w i-1 i-n+1 t i-1 i-n+1 based on information theoretic metrics, without utilizing heuristics for which order the context attributes are to be backed off (cf. Eq. 1). In subsequent discussion, we will write equations for word models (Eq. 2), but they are equally applicable to joint models (Eq. 3) with trivial transformations.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Decision Trees",
                "sec_num": "2"
            },
            {
                "text": "Let us rewrite the interpolation Eq. 1 in a more generic way:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Backoff Property",
                "sec_num": "3"
            },
            {
                "text": "p(w i |w i-1 1 ) = \u03c1 n (w i |\u03a6 n (w i-1 1 )) + (4) \u03b3(\u03a6 n (w i-1 1 )) \u2022 p(w i |BO n-1 (w i-1 1 ))",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Backoff Property",
                "sec_num": "3"
            },
            {
                "text": "where, \u03c1 n is a discounted distribution, \u03a6 n is a clustering function of order n, and \u03b3(\u03a6 n (w i-1 1 )) is the backoff weight chosen to normalize the distribution. BO n-1 is the backoff clustering function of order n -1, representing a reduction of context size. In the case of an n-gram model, \u03a6 n (w i-1 1 ) is the set of word sequences where the last n -1 words are w i-1 i-n+1 , similarly, BO n-1 (w i-1 1 ) is the set of sequences ending with w i-1 i-n+2 . In the case of a decision tree model, the same backoff function is typically used, but the clustering function can be arbitrary.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Backoff Property",
                "sec_num": "3"
            },
            {
                "text": "The intuition behind Eq. 4 is that the backoff context BO n-1 (w i-1 1 ) allows for more robust (but less informed) probability estimation than the context cluster \u03a6 n (w i-1 1 ). More precisely:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Backoff Property",
                "sec_num": "3"
            },
            {
                "text": "\u2200 w i-1 1 ,W : W \u2208 \u03a6 n (w i-1 1 ) \u21d2 W \u2208 BO n-1 (w i-1 1 )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Backoff Property",
                "sec_num": "3"
            },
            {
                "text": "(5) that is, every word sequence W that belongs to a context cluster \u03a6 n (w i-1 1 ), belongs to the same backoff cluster BO n-1 (w i-1 1 ) (hence has the same backoff distribution). For n-gram models, Property 5 trivially holds since BO n-1 (w i-1 1 ) and \u03a6 n (w i-1 1 ) are defined as sets of sequences ending with w i-1 i-n+2 and w i-1 i-n+1 with the former clearly being a superset of the latter. However, when \u03a6 can be arbitrary, e.g., a decision tree, that is not necessarily so.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Backoff Property",
                "sec_num": "3"
            },
            {
                "text": "Let us consider what happens when we have two context sequences W and W that belong to the same cluster",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Backoff Property",
                "sec_num": "3"
            },
            {
                "text": "\u03a6 n (W ) = \u03a6 n (W ) but differ- ent backoff clusters BO n-1 (W ) = BO n-1 (W ).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Backoff Property",
                "sec_num": "3"
            },
            {
                "text": "For example: suppose we have \u03a6(w i-2 w i-1 ) = ({on}, {may,june}) and two corresponding backoff clusters: BO = ({may}) and BO = ({june}). Following on, the word may is likely to be a month rather than a modal verb, although the latter is more frequent and will dominate in BO . Therefore we have much less faith in p(w i |BO ) than in p(w i |BO ) and would like a much smaller weight \u03b3 assigned to BO , but it is not possible in the backoff scheme in Eq. 4, thus we will have to settle on a compromise value of \u03b3, resulting in suboptimal performance.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Backoff Property",
                "sec_num": "3"
            },
            {
                "text": "We would expect this effect to be more pronounced in higher order models, because viola-tions of Property 5 are less frequent in lower order models. Indeed, in a 2-gram model, the property is never violated since its backoff, unigram, contains the entire context in one cluster. The 3-gram example above, \u03a6(w i-2 w i-1 ) = ({on}, {may,june}), although illustrative, is not likely to occur because may in w i-1 position will likely be split from june very early on, since it is very informative about the following word. However, in a 4-gram model, \u03a6(w i-3 w i-2 w i-1 ) = ({on}, {may,june}, {<unk>}) is quite plausible.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Backoff Property",
                "sec_num": "3"
            },
            {
                "text": "Thus, arbitrary clustering (an advantage of DTs) leads to violation of Property 5, which, we argue, may lead to a degradation of performance if backoff interpolation Eq. 4 is used. In the next section, we generalize the interpolation scheme which, as we show in Section 6, allows us to find a better solution in the face of the violation of Property 5.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Backoff Property",
                "sec_num": "3"
            },
            {
                "text": "We use linear interpolation as the baseline, represented recursively, which is similar to Jelinek-Mercer smoothing for n-gram models (Jelinek and Mercer, 1980) :",
                "cite_spans": [
                    {
                        "start": 133,
                        "end": 159,
                        "text": "(Jelinek and Mercer, 1980)",
                        "ref_id": "BIBREF5"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Linear Interpolation",
                "sec_num": "4"
            },
            {
                "text": "pn(wi|w i-1 i-n+1 ) = \u03bbn(\u03c6n) \u2022 pn(wi|\u03c6n) + (6) (1 -\u03bbn(\u03c6n)) \u2022 pn-1(wi|w i-1 i-n+2 )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Linear Interpolation",
                "sec_num": "4"
            },
            {
                "text": "where \u03c6 n \u2261 \u03a6 n (w i-1 i-n+1 ), and \u03bb n (\u03c6 n ) \u2208 [0, 1] are assigned to each cluster and are optimized on a heldout set using EM. p n (w i |\u03c6 n ) is the probability distribution at the cluster \u03c6 n in the tree of order n. This interpolation method is particularly useful as, unlike count-based discounting methods (e.g., Kneser-Ney), it can be applied to already smooth distributions p n2 .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Linear Interpolation",
                "sec_num": "4"
            },
            {
                "text": "We can unwind the recursion in Eq. 6 and make substitutions:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generalized Interpolation",
                "sec_num": "5"
            },
            {
                "text": "\u03bb n (\u03c6 n ) \u2192 \u03bbn (\u03c6 n ) (1 -\u03bb n (\u03c6 n )) \u2022 \u03bb n-1 (\u03c6 n-1 ) \u2192 \u03bbn-1 (\u03c6 n-1 ) . . . pn (w i |w i-1 i-n+1 ) = n m=1 \u03bbm (\u03c6 m ) \u2022 p m (w i |\u03c6 m ) (7) n m=1 \u03bbm (\u03c6 m ) = 1",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generalized Interpolation",
                "sec_num": "5"
            },
            {
                "text": "Note that in this parameterization, the weight assigned to p n-1 (w i |\u03c6 n-1 ) is limited by (1-\u03bb n (\u03c6 n )), i.e., the weight assigned to the higher order model. Ideally we should be able to assign a different set of interpolation weights for every eligible combination of clusters \u03c6 n , \u03c6 n-1 , . . . , \u03c6 1 . However, not only is the number of such combinations extremely large, but many of them will not be observed in the training data, making parameter estimation cumbersome. Therefore, we propose the following parameterization for the interpolation of decision tree models:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generalized Interpolation",
                "sec_num": "5"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "pn (w i |w i-1 i-n+1 ) = n m=1 \u03bb m (\u03c6 m ) \u2022 p m (w i |\u03c6 m ) n m=1 \u03bb m (\u03c6 m )",
                        "eq_num": "(8)"
                    }
                ],
                "section": "Generalized Interpolation",
                "sec_num": "5"
            },
            {
                "text": "Note that this parameterization has the same number of parameters as in Eq. 7 (one per cluster in every tree), but the number of degrees of freedom is larger because the the parameters are not constrained to sum to 1, hence the denominator. In Eq. 8, there is no explicit distinction between higher order and backoff models. Indeed, it acknowledges that lower order models are not backoff models when Property 5 is not satisfied. However, it can be shown that Eq. 8 reduces to Eq. 6 if Property 5 holds. Therefore, the new parameterization can be thought of as a generalization of linear interpolation. Indeed, suppose we have the parameterization in Eq. 8 and Property 5. Let us transform this parameterization into Eq. 7 by induction. We define:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generalized Interpolation",
                "sec_num": "5"
            },
            {
                "text": "\u039b m \u2261 m k=1 \u03bb k ; \u039b m = \u03bb m + \u039b m-1",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generalized Interpolation",
                "sec_num": "5"
            },
            {
                "text": "where, due to space limitation, we redefine",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generalized Interpolation",
                "sec_num": "5"
            },
            {
                "text": "\u03bb m \u2261 \u03bb m (\u03c6 m ) and \u039b m \u2261 \u039b m (\u03c6 m ); \u03c6 m \u2261 \u03a6 m (w i-1",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generalized Interpolation",
                "sec_num": "5"
            },
            {
                "text": "1 ), i.e., the cluster of model order m, to which the sequence w i-1 1 belongs. The lowest order distribution p 1 is not interpolated with anything, hence:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generalized Interpolation",
                "sec_num": "5"
            },
            {
                "text": "\u039b 1 p1 (w i |\u03c6 1 ) = \u03bb 1 p 1 (w i |\u03c6 1 )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generalized Interpolation",
                "sec_num": "5"
            },
            {
                "text": "Now the induction step. From Property 5, it follows that \u03c6 m \u2282 \u03c6 m-1 , thus, for all sequences in \u2200 w 1 : Perplexity results on PTB WSJ section 23. Percentage numbers in parentheses denote the reduction of perplexity relative to the lower order model of the same type. \"Word-tree\" and \"syntactic\" refer to DT models estimated using words only (Eq. 2) and words and tags jointly (Eq. 3).",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 102,
                        "end": 103,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Generalized Interpolation",
                "sec_num": "5"
            },
            {
                "text": "\u03c6 m , we have the same distribution:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generalized Interpolation",
                "sec_num": "5"
            },
            {
                "text": "\u03bb m p m (w i |\u03c6 m ) + \u039b m-1 pm-1 (w i |\u03c6 m-1 ) = = \u039b m \u03bb m \u039b m p m (w i |\u03c6 m ) + \u039b m-1 \u039b m pm-1 (w i |\u03c6 m-1 ) = \u039b m \u03bbm p m (w i |\u03c6 m ) + (1 -\u03bbm )p m-1 (w i |\u03c6 m-1 ) = \u039b m pm (w i |\u03c6 m ) ; \u03bbm \u2261 \u03bb m \u039b m",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generalized Interpolation",
                "sec_num": "5"
            },
            {
                "text": "Note that the last transformation is because \u03c6 m \u2282 \u03c6 m-1 ; had it not been the case, pm would depend on the combination of \u03c6 m and \u03c6 m-1 and require multiple parameters to be represented on its entire domain w n 1 \u2208 \u03c6 m . After n iterations, we have: Thus, we have constructed pn (w i |\u03c6 n ) using the same recursive representation as in Eq. 6, which proves that the standard linear interpolation is a special case of the new interpolation scheme, which occurs when the backoff Property 5 holds.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generalized Interpolation",
                "sec_num": "5"
            },
            {
                "text": "Models are trained on 35M words of WSJ 94-96 from LDC2008T13. The text was converted into speech-like form, namely numbers and abbreviations were verbalized, text was downcased, punctuation was removed, and contractions and possessives were joined with the previous word (i.e., they 'll becomes they'll). For syntactic modeling, we used tags comprised of POS tags of the word and its head, as in (Filimonov and Harper, 2009) . Parsing of the text for tag extraction occurred after verbalization of numbers and abbreviations but before any further processing; we used an appropriately trained latent variable PCFG parser (Huang and Harper, 2009) . For reference, we include n-gram models with Jelinek-Mercer and modified interpolated KN discounting. All models use the same vocabulary of approximately 50k words. We implemented four decision tree models3 : two using the interpolation method of (Eq. 6) and two based on the generalized interpolation (Eq. 8). Parameters \u03bb were estimated using the L-BFGS to minimize the entropy on a heldout set. In order to eliminate the influence of all factors other than the interpolation, we used the same decision trees. The perplexity results on WSJ section 23 are presented in Table 1 . As we have predicted, the effect of the new interpolation becomes apparent at the 4-gram order, when Property 5 is most frequently violated. Note that we observe similar patterns for both word-tree and syntactic models, with syntactic models outperforming their word-tree counterparts.",
                "cite_spans": [
                    {
                        "start": 396,
                        "end": 424,
                        "text": "(Filimonov and Harper, 2009)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 620,
                        "end": 644,
                        "text": "(Huang and Harper, 2009)",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 1223,
                        "end": 1224,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Results and Discussion",
                "sec_num": "6"
            },
            {
                "text": "We believe that (Xu and Jelinek, 2004 ) also suffers from violation of Property 5, however, since they use a heuristic method4 to set backoff weights, it is difficult to ascertain the extent.",
                "cite_spans": [
                    {
                        "start": 16,
                        "end": 37,
                        "text": "(Xu and Jelinek, 2004",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results and Discussion",
                "sec_num": "6"
            },
            {
                "text": "The main contribution of this paper is the insight that in the standard recursive backoff there is an implied relation between the backoff and the higher order models, which is essential for adequate performance. When this relation is not satisfied other interpolation methods should be employed; hence, we propose a generalization of linear interpolation that significantly outperforms the standard form in such a scenario.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "7"
            },
            {
                "text": "We refer the reader to(Chen and Goodman, 1999) for a survey of the discounting methods for n-gram models.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "In decision trees, the distribution at a cluster (leaf) is often recursively interpolated with its parent node, e.g.(Bahl et al., 1990;Heeman, 1999;Filimonov and Harper, 2009).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "We refer the reader to(Filimonov and Harper, 2009) for details on the tree construction algorithm.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "The higher order model was discounted according to KN discounting, while the lower order model could be either a lower order DT (forest) model, or a standard n-gram model, with the former performing slightly better.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "A tree-based statistical language model for natural language speech recognition. Readings in speech recognition",
                "authors": [
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Lalit",
                        "suffix": ""
                    },
                    {
                        "first": "Peter",
                        "middle": [
                            "F"
                        ],
                        "last": "Bahl",
                        "suffix": ""
                    },
                    {
                        "first": "Peter",
                        "middle": [
                            "V"
                        ],
                        "last": "Brown",
                        "suffix": ""
                    },
                    {
                        "first": "Robert",
                        "middle": [
                            "L"
                        ],
                        "last": "De Souza",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Mercer",
                        "suffix": ""
                    }
                ],
                "year": 1990,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "507--514",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Lalit R. Bahl, Peter F. Brown, Peter V. de Souza, and Robert L. Mercer. 1990. A tree-based statistical lan- guage model for natural language speech recognition. Readings in speech recognition, pages 507-514.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "An empirical study of smoothing techniques for language modeling",
                "authors": [
                    {
                        "first": "Stanley",
                        "middle": [
                            "F"
                        ],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Joshua",
                        "middle": [],
                        "last": "Goodman",
                        "suffix": ""
                    }
                ],
                "year": 1999,
                "venue": "Computer Speech & Language",
                "volume": "13",
                "issue": "4",
                "pages": "359--393",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Stanley F. Chen and Joshua Goodman. 1999. An empir- ical study of smoothing techniques for language mod- eling. Computer Speech & Language, 13(4):359-393.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "A joint language model with fine-grain syntactic tags",
                "authors": [
                    {
                        "first": "Denis",
                        "middle": [],
                        "last": "Filimonov",
                        "suffix": ""
                    },
                    {
                        "first": "Mary",
                        "middle": [],
                        "last": "Harper",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of the EMNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Denis Filimonov and Mary Harper. 2009. A joint lan- guage model with fine-grain syntactic tags. In Pro- ceedings of the EMNLP.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "POS tags and decision trees for language modeling",
                "authors": [
                    {
                        "first": "Peter",
                        "middle": [
                            "A"
                        ],
                        "last": "Heeman",
                        "suffix": ""
                    }
                ],
                "year": 1999,
                "venue": "Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora",
                "volume": "",
                "issue": "",
                "pages": "129--137",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Peter A. Heeman. 1999. POS tags and decision trees for language modeling. In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 129-137.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Self-Training PCFG grammars with latent annotations across languages",
                "authors": [
                    {
                        "first": "Zhongqiang",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Mary",
                        "middle": [],
                        "last": "Harper",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of the EMNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zhongqiang Huang and Mary Harper. 2009. Self- Training PCFG grammars with latent annotations across languages. In Proceedings of the EMNLP 2009.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Interpolated estimation of markov source parameters from sparse data",
                "authors": [
                    {
                        "first": "Frederick",
                        "middle": [],
                        "last": "Jelinek",
                        "suffix": ""
                    },
                    {
                        "first": "Robert",
                        "middle": [
                            "L"
                        ],
                        "last": "Mercer",
                        "suffix": ""
                    }
                ],
                "year": 1980,
                "venue": "Proceedings of the Workshop on Pattern Recognition in Practice",
                "volume": "",
                "issue": "",
                "pages": "381--397",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Frederick Jelinek and Robert L. Mercer. 1980. Inter- polated estimation of markov source parameters from sparse data. In Proceedings of the Workshop on Pat- tern Recognition in Practice, pages 381-397.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Random forests in language modeling",
                "authors": [
                    {
                        "first": "Peng",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Frederick",
                        "middle": [],
                        "last": "Jelinek",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proceedings of the EMNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Peng Xu and Frederick Jelinek. 2004. Random forests in language modeling. In Proceedings of the EMNLP.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "\u03c6 m )p m (w i |\u03c6 m ) = \u039b n pn (w i |\u03c6 n ); (cf. Eq. 8)",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            }
        }
    }
}