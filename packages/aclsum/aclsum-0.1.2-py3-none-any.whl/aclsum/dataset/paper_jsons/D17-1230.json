{
    "paper_id": "D17-1230",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T11:59:37.177942Z"
    },
    "title": "Adversarial Learning for Neural Dialogue Generation",
    "authors": [
        {
            "first": "Jiwei",
            "middle": [],
            "last": "Li",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Stanford University",
                "location": {
                    "settlement": "Stanford",
                    "region": "CA",
                    "country": "USA"
                }
            },
            "email": "jiweil@stanford.edu"
        },
        {
            "first": "Will",
            "middle": [],
            "last": "Monroe",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Stanford University",
                "location": {
                    "settlement": "Stanford",
                    "region": "CA",
                    "country": "USA"
                }
            },
            "email": "wmonroe4@stanford.edu"
        },
        {
            "first": "Tianlin",
            "middle": [],
            "last": "Shi",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Stanford University",
                "location": {
                    "settlement": "Stanford",
                    "region": "CA",
                    "country": "USA"
                }
            },
            "email": "tianlins@stanford.edu"
        },
        {
            "first": "S\u00e9bastien",
            "middle": [],
            "last": "Jean",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "New York University",
                "location": {
                    "region": "NY",
                    "country": "USA"
                }
            },
            "email": "sebastien@cs.nyu.edu"
        },
        {
            "first": "Alan",
            "middle": [],
            "last": "Ritter",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Ohio State University",
                "location": {
                    "region": "OH",
                    "country": "USA"
                }
            },
            "email": ""
        },
        {
            "first": "Dan",
            "middle": [],
            "last": "Jurafsky",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Stanford University",
                "location": {
                    "settlement": "Stanford",
                    "region": "CA",
                    "country": "USA"
                }
            },
            "email": "jurafsky@stanford.edu"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "In this paper, drawing intuition from the Turing test, we propose using adversarial training for open-domain dialogue generation: the system is trained to produce sequences that are indistinguishable from human-generated dialogue utterances. We cast the task as a reinforcement learning (RL) problem where we jointly train two systems, a generative model to produce response sequences, and a discriminator-analagous to the human evaluator in the Turing test-to distinguish between the human-generated dialogues and the machine-generated ones. The outputs from the discriminator are then used as rewards for the generative model, pushing the system to generate dialogues that mostly resemble human dialogues.\nIn addition to adversarial training we describe a model for adversarial evaluation that uses success in fooling an adversary as a dialogue evaluation metric, while avoiding a number of potential pitfalls. Experimental results on several metrics, including adversarial evaluation, demonstrate that the adversarially-trained system generates higher-quality responses than previous baselines.",
    "pdf_parse": {
        "paper_id": "D17-1230",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "In this paper, drawing intuition from the Turing test, we propose using adversarial training for open-domain dialogue generation: the system is trained to produce sequences that are indistinguishable from human-generated dialogue utterances. We cast the task as a reinforcement learning (RL) problem where we jointly train two systems, a generative model to produce response sequences, and a discriminator-analagous to the human evaluator in the Turing test-to distinguish between the human-generated dialogues and the machine-generated ones. The outputs from the discriminator are then used as rewards for the generative model, pushing the system to generate dialogues that mostly resemble human dialogues.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            },
            {
                "text": "In addition to adversarial training we describe a model for adversarial evaluation that uses success in fooling an adversary as a dialogue evaluation metric, while avoiding a number of potential pitfalls. Experimental results on several metrics, including adversarial evaluation, demonstrate that the adversarially-trained system generates higher-quality responses than previous baselines.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Open domain dialogue generation (Ritter et al., 2011; Sordoni et al., 2015; Xu et al., 2016; Wen et al., 2016; Li et al., 2016b; Serban et al., 2016c Serban et al., , 2017) ) aims at generating meaningful and coherent dialogue responses given the dialogue history. Prior systems, e.g., phrase-based machine translation systems (Ritter et al., 2011; Sordoni et al., 2015) or end-to-end neural systems (Shang et al., 2015; Vinyals and Le, 2015; Li et al., 2016a; Yao et al., 2015; Luan et al., 2016) approximate such a goal by predicting the next dialogue utterance given the dialogue history using the maximum likelihood estimation (MLE) objective. Despite its success, this over-simplified training objective leads to problems: responses are dull, generic (Sordoni et al., 2015; Serban et al., 2016a; Li et al., 2016a) , repetitive, and short-sighted (Li et al., 2016d) .",
                "cite_spans": [
                    {
                        "start": 32,
                        "end": 53,
                        "text": "(Ritter et al., 2011;",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 54,
                        "end": 75,
                        "text": "Sordoni et al., 2015;",
                        "ref_id": "BIBREF37"
                    },
                    {
                        "start": 76,
                        "end": 92,
                        "text": "Xu et al., 2016;",
                        "ref_id": "BIBREF45"
                    },
                    {
                        "start": 93,
                        "end": 110,
                        "text": "Wen et al., 2016;",
                        "ref_id": "BIBREF42"
                    },
                    {
                        "start": 111,
                        "end": 128,
                        "text": "Li et al., 2016b;",
                        "ref_id": null
                    },
                    {
                        "start": 129,
                        "end": 149,
                        "text": "Serban et al., 2016c",
                        "ref_id": null
                    },
                    {
                        "start": 150,
                        "end": 174,
                        "text": "Serban et al., , 2017) )",
                        "ref_id": null
                    },
                    {
                        "start": 327,
                        "end": 348,
                        "text": "(Ritter et al., 2011;",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 349,
                        "end": 370,
                        "text": "Sordoni et al., 2015)",
                        "ref_id": "BIBREF37"
                    },
                    {
                        "start": 400,
                        "end": 420,
                        "text": "(Shang et al., 2015;",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 421,
                        "end": 442,
                        "text": "Vinyals and Le, 2015;",
                        "ref_id": "BIBREF41"
                    },
                    {
                        "start": 443,
                        "end": 460,
                        "text": "Li et al., 2016a;",
                        "ref_id": null
                    },
                    {
                        "start": 461,
                        "end": 478,
                        "text": "Yao et al., 2015;",
                        "ref_id": "BIBREF46"
                    },
                    {
                        "start": 479,
                        "end": 497,
                        "text": "Luan et al., 2016)",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 756,
                        "end": 778,
                        "text": "(Sordoni et al., 2015;",
                        "ref_id": "BIBREF37"
                    },
                    {
                        "start": 779,
                        "end": 800,
                        "text": "Serban et al., 2016a;",
                        "ref_id": null
                    },
                    {
                        "start": 801,
                        "end": 818,
                        "text": "Li et al., 2016a)",
                        "ref_id": null
                    },
                    {
                        "start": 851,
                        "end": 869,
                        "text": "(Li et al., 2016d)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Solutions to these problems require answering a few fundamental questions: what are the crucial aspects that characterize an ideal conversation, how can we quantitatively measure them, and how can we incorporate them into a machine learning system? For example, Li et al. (2016d) manually define three ideal dialogue properties (ease of answering, informativeness and coherence) and use a reinforcement-learning framework to train the model to generate highly rewarded responses. Yu et al. (2016b) use keyword retrieval confidence as a reward. However, it is widely acknowledged that manually defined reward functions can't possibly cover all crucial aspects and can lead to suboptimal generated utterances.",
                "cite_spans": [
                    {
                        "start": 262,
                        "end": 279,
                        "text": "Li et al. (2016d)",
                        "ref_id": null
                    },
                    {
                        "start": 480,
                        "end": 497,
                        "text": "Yu et al. (2016b)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "A good dialogue model should generate utterances indistinguishable from human dialogues. Such a goal suggests a training objective resembling the idea of the Turing test (Turing, 1950) . We borrow the idea of adversarial training (Goodfellow et al., 2014; Denton et al., 2015) in computer vision, in which we jointly train two models, a generator (a neural SEQ2SEQ model) that defines the probability of generating a dialogue sequence, and a discriminator that labels dialogues as human-generated or machine-generated. This discriminator is analogous to the evaluator in the Turing test. We cast the task as a reinforcement learning problem, in which the quality of machinegenerated utterances is measured by its ability to fool the discriminator into believing that it is a human-generated one. The output from the discriminator is used as a reward to the generator, pushing it to generate utterances indistinguishable from human-generated dialogues.",
                "cite_spans": [
                    {
                        "start": 170,
                        "end": 184,
                        "text": "(Turing, 1950)",
                        "ref_id": "BIBREF40"
                    },
                    {
                        "start": 230,
                        "end": 255,
                        "text": "(Goodfellow et al., 2014;",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 256,
                        "end": 276,
                        "text": "Denton et al., 2015)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The idea of a Turing test-employing an evaluator to distinguish machine-generated texts from human-generated ones-can be applied not only to training but also testing, where it goes by the name of adversarial evaluation. Adversarial evaluation was first employed in Bowman et al. (2016) to evaluate sentence generation quality, and preliminarily studied for dialogue generation by Kannan and Vinyals (2016) . In this paper, we discuss potential pitfalls of adversarial evaluations and necessary steps to avoid them and make evaluation reliable.",
                "cite_spans": [
                    {
                        "start": 266,
                        "end": 286,
                        "text": "Bowman et al. (2016)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 381,
                        "end": 406,
                        "text": "Kannan and Vinyals (2016)",
                        "ref_id": "BIBREF12"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Experimental results demonstrate that our approach produces more interactive, interesting, and non-repetitive responses than standard SEQ2SEQ models trained using the MLE objective function.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Dialogue generation Response generation for dialogue can be viewed as a source-to-target transduction problem. Ritter et al. (2011) frame the generation problem as a machine translation problem. Sordoni et al. (2015) improved Ritter et al.'s system by rescoring the outputs of a phrasal MT-based conversation system with a neural model incorporating prior context. Recent progress in SEQ2SEQ models have inspired several efforts (Vinyals and Le, 2015; Serban et al., 2016a,d; Luan et al., 2016) to build end-to-end conversational systems that first apply an encoder to map a message to a distributed vector representing its meaning and then generate a response from the vector.",
                "cite_spans": [
                    {
                        "start": 111,
                        "end": 131,
                        "text": "Ritter et al. (2011)",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 195,
                        "end": 216,
                        "text": "Sordoni et al. (2015)",
                        "ref_id": "BIBREF37"
                    },
                    {
                        "start": 429,
                        "end": 451,
                        "text": "(Vinyals and Le, 2015;",
                        "ref_id": "BIBREF41"
                    },
                    {
                        "start": 452,
                        "end": 475,
                        "text": "Serban et al., 2016a,d;",
                        "ref_id": null
                    },
                    {
                        "start": 476,
                        "end": 494,
                        "text": "Luan et al., 2016)",
                        "ref_id": "BIBREF22"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "Our work adapts the encoder-decoder model to RL training, and can thus be viewed as an extension of Li et al. (2016d) , but with more general RL rewards. Li et al. (2016d) simulate dialogues between two virtual agents, using policy gradient methods to reward sequences that display three useful conversational properties: informativity, coherence, and ease of answering. Our work is also related to recent efforts to integrate the SEQ2SEQ and reinforcement learning paradigms, drawing on the advantages of both (Wen et al., 2016) . For example, Su et al. (2016) combine reinforcement learning with neural generation on tasks with real users. Asghar et al. (2016) train an end-to-end RL dialogue model using human users.",
                "cite_spans": [
                    {
                        "start": 100,
                        "end": 117,
                        "text": "Li et al. (2016d)",
                        "ref_id": null
                    },
                    {
                        "start": 154,
                        "end": 171,
                        "text": "Li et al. (2016d)",
                        "ref_id": null
                    },
                    {
                        "start": 511,
                        "end": 529,
                        "text": "(Wen et al., 2016)",
                        "ref_id": "BIBREF42"
                    },
                    {
                        "start": 545,
                        "end": 561,
                        "text": "Su et al. (2016)",
                        "ref_id": "BIBREF38"
                    },
                    {
                        "start": 642,
                        "end": 662,
                        "text": "Asghar et al. (2016)",
                        "ref_id": "BIBREF1"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "Dialogue quality is traditionally evaluated (Sordoni et al., 2015, e.g.) using word-overlap metrics such as BLEU and METEOR scores used for machine translation. Some recent work (Liu et al., 2016) has started to look at more flexible and reliable evaluation metrics such as human-rating prediction (Lowe et al., 2017) and next utterance classification (Lowe et al., 2016) .",
                "cite_spans": [
                    {
                        "start": 44,
                        "end": 72,
                        "text": "(Sordoni et al., 2015, e.g.)",
                        "ref_id": null
                    },
                    {
                        "start": 178,
                        "end": 196,
                        "text": "(Liu et al., 2016)",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 298,
                        "end": 317,
                        "text": "(Lowe et al., 2017)",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 352,
                        "end": 371,
                        "text": "(Lowe et al., 2016)",
                        "ref_id": "BIBREF21"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "The idea of generative adversarial networks has enjoyed great success in computer vision (Radford et al., 2015; Chen et al., 2016a; Salimans et al., 2016) . Training is formalized as a game in which the generative model is trained to generate outputs to fool the discriminator; the technique has been successfully applied to image generation.",
                "cite_spans": [
                    {
                        "start": 89,
                        "end": 111,
                        "text": "(Radford et al., 2015;",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 112,
                        "end": 131,
                        "text": "Chen et al., 2016a;",
                        "ref_id": null
                    },
                    {
                        "start": 132,
                        "end": 154,
                        "text": "Salimans et al., 2016)",
                        "ref_id": "BIBREF27"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Adversarial networks",
                "sec_num": null
            },
            {
                "text": "However, to the best of our knowledge, this idea has not achieved comparable success in NLP. This is due to the fact that unlike in vision, text generation is discrete, which makes the error outputted from the discriminator hard to backpropagate to the generator. Some recent work has begun to address this issue: Lamb et al. (2016) propose providing the discriminator with the intermediate hidden vectors of the generator rather than its sequence outputs. Such a strategy makes the system differentiable and achieves promising results in tasks like character-level language modeling and handwriting generation. Yu et al. (2016a) use policy gradient reinforcement learning to backpropagate the error from the discriminator, showing improvement in multiple generation tasks such as poem generation, speech language generation and music generation. Outside of sequence generation, Chen et al. (2016b) apply the idea of adversarial training to sentiment analysis and Zhang et al. (2017) apply the idea to domain adaptation tasks.",
                "cite_spans": [
                    {
                        "start": 314,
                        "end": 332,
                        "text": "Lamb et al. (2016)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 612,
                        "end": 629,
                        "text": "Yu et al. (2016a)",
                        "ref_id": null
                    },
                    {
                        "start": 879,
                        "end": 898,
                        "text": "Chen et al. (2016b)",
                        "ref_id": null
                    },
                    {
                        "start": 964,
                        "end": 983,
                        "text": "Zhang et al. (2017)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Adversarial networks",
                "sec_num": null
            },
            {
                "text": "Our work is distantly related to recent work that formalizes sequence generation as an action-taking problem in reinforcement learning. Ranzato et al. (2016) train RNN decoders in a SEQ2SEQ model using policy gradient to obtain competitive machine translation results. Bahdanau et al. (2017) take this a step further by training an actor-critic RL model for machine translation. Also related is recent work (Shen et al., 2016; Wiseman and Rush, 2016) to address the issues of exposure bias and loss-evaluation mismatch in neural translation.",
                "cite_spans": [
                    {
                        "start": 136,
                        "end": 157,
                        "text": "Ranzato et al. (2016)",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 269,
                        "end": 291,
                        "text": "Bahdanau et al. (2017)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 407,
                        "end": 426,
                        "text": "(Shen et al., 2016;",
                        "ref_id": "BIBREF35"
                    },
                    {
                        "start": 427,
                        "end": 450,
                        "text": "Wiseman and Rush, 2016)",
                        "ref_id": "BIBREF44"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Adversarial networks",
                "sec_num": null
            },
            {
                "text": "In this section, we describe in detail the components of the proposed adversarial reinforcement learning model. The problem can be framed as follows: given a dialogue history x consisting of a sequence of dialogue utterances,1 the model needs to generate a response y = {y 1 , y 2 , ..., y T }. We view the process of sentence generation as a sequence of actions that are taken according to a policy defined by an encoder-decoder recurrent neural network.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Adversarial Training for Dialogue Generation",
                "sec_num": "3"
            },
            {
                "text": "The adversarial REINFORCE algorithm consists of two components: a generative model G and a discriminative model D.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Adversarial REINFORCE",
                "sec_num": "3.1"
            },
            {
                "text": "The generative model G defines the policy that generates a response y given dialogue history x. It takes a form similar to SEQ2SEQ models, which first map the source input to a vector representation using a recurrent net and then compute the probability of generating each token in the target using a softmax function.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generative model",
                "sec_num": null
            },
            {
                "text": "Discriminative model The discriminative model D is a binary classifier that takes as input a sequence of dialogue utterances {x, y} and outputs a label indicating whether the input is generated by humans or machines. The input dialogue is encoded into a vector representation using a hierarchical encoder (Li et al., 2015; Serban et al., 2016b ),2 which is then fed to a 2-class softmax function, returning the probability of the input dialogue episode being a machine-generated dialogue (denoted Q -({x, y})) or a human-generated dialogue (denoted Q + ({x, y})).",
                "cite_spans": [
                    {
                        "start": 305,
                        "end": 322,
                        "text": "(Li et al., 2015;",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 323,
                        "end": 343,
                        "text": "Serban et al., 2016b",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generative model",
                "sec_num": null
            },
            {
                "text": "The key idea of the system is to encourage the generator to generate utterances that are indistinguishable from human generated dialogues. We use policy gradient methods to achieve such a goal, in which the score of current utterances being human-generated ones assigned by the discriminator (i.e., Q + ({x, y})) is used as a reward for the generator, which is trained to maximize the expected reward of generated utterance(s) using the REINFORCE algorithm (Williams, 1992):",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Policy Gradient Training",
                "sec_num": null
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "J(\u03b8) = E y\u223cp(y|x) (Q + ({x, y})|\u03b8)",
                        "eq_num": "(1)"
                    }
                ],
                "section": "Policy Gradient Training",
                "sec_num": null
            },
            {
                "text": "Given the input dialogue history x, the bot generates a dialogue utterance y by sampling from the policy. The concatenation of the generated utterance y and the input x is fed to the discriminator. The gradient of (1) is approximated using the likelihood ratio trick (Williams, 1992; Glynn, 1990; Aleksandrov et al., 1968) :",
                "cite_spans": [
                    {
                        "start": 267,
                        "end": 283,
                        "text": "(Williams, 1992;",
                        "ref_id": "BIBREF43"
                    },
                    {
                        "start": 284,
                        "end": 296,
                        "text": "Glynn, 1990;",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 297,
                        "end": 322,
                        "text": "Aleksandrov et al., 1968)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Policy Gradient Training",
                "sec_num": null
            },
            {
                "text": "\u2207J(\u03b8) \u2248 [Q + ({x, y}) -b({x, y})] \u2207 log \u03c0(y|x) = [Q + ({x, y}) -b({x, y})] \u2207 t log p(y t |x, y 1:t-1 ) (2)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Policy Gradient Training",
                "sec_num": null
            },
            {
                "text": "where \u03c0 denotes the probability of the generated responses. b({x, y}) denotes the baseline value to reduce the variance of the estimate while keeping it unbiased. 3 The discriminator is simultaneously updated with the human generated dialogue that contains dialogue history x as a positive example and the machine-generated dialogue as a negative example.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Policy Gradient Training",
                "sec_num": null
            },
            {
                "text": "The REINFORCE algorithm described has the disadvantage that the expectation of the reward is approximated by only one sample, and the reward associated with this sample (i.e., [Q + ({x, y})b({x, y})] in Eq(2)) is used for all actions (the generation of each token) in the generated sequence. Suppose, for example, the input history is what's your name, the human-generated response is I am John, and the machine-generated response is I don't know. The vanilla REINFORCE model assigns the same negative reward to all tokens within the human-generated response (i.e., I, don't, know), whereas proper credit assignment in training would give separate rewards, most likely a neutral reward for the token I, and negative rewards to don't and know. We call this reward for every generation step, abbreviated REGS.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Reward for Every Generation Step (REGS)",
                "sec_num": "3.2"
            },
            {
                "text": "Rewards for intermediate steps or partially decoded sequences are thus necessary. Unfortunately, the discriminator is trained to assign scores to fully 3 Like Ranzato et al. ( 2016), we train another neural network model (the critic) to estimate the value (or future reward) of current state (i.e., the dialogue history) under the current policy \u03c0. The critic network takes as input the dialogue history, transforms it to a vector representation using a hierarchical network and maps the representation to a scalar. The network is optimized based on the mean squared loss between the estimated reward and the real reward. generated sequences, but not partially decoded ones. We propose two strategies for computing intermediate step rewards by (1) using Monte Carlo (MC) search and ( 2) training a discriminator that is able to assign rewards to partially decoded sequences.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Reward for Every Generation Step (REGS)",
                "sec_num": "3.2"
            },
            {
                "text": "In (1) Monte Carlo search, given a partially decoded s P , the model keeps sampling tokens from the distribution until the decoding finishes. Such a process is repeated N (set to 5) times and the N generated sequences will share a common prefix s P . These N sequences are fed to the discriminator, the average score of which is used as a reward for the s P . A similar strategy is adopted in Yu et al. (2016a) . The downside of MC is that it requires repeating the sampling process for each prefix of each sequence and is thus significantly time-consuming. 4In (2), we directly train a discriminator that is able to assign rewards to both fully and partially decoded sequences. We break the generated sequences into partial sequences, namely {y For each partially-generated sequence Y t = y 1:t , the discriminator gives a classification score",
                "cite_spans": [
                    {
                        "start": 393,
                        "end": 410,
                        "text": "Yu et al. (2016a)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Reward for Every Generation Step (REGS)",
                "sec_num": "3.2"
            },
            {
                "text": "Q + (x, Y t ).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Reward for Every Generation Step (REGS)",
                "sec_num": "3.2"
            },
            {
                "text": "We compute the baseline b(x, Y t ) using a similar model to the vanilla REINFORCE model. This yields the following gradient to update the generator: 3) with (2), we can see that the values for rewards and baselines are different among generated tokens in the same response.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Reward for Every Generation Step (REGS)",
                "sec_num": "3.2"
            },
            {
                "text": "\u2207J(\u03b8) \u2248 t (Q + (x, Y t ) -b(x, Y t )) \u2207 log p(y t |x, Y 1:t-1 ) (3) Comparing (",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Reward for Every Generation Step (REGS)",
                "sec_num": "3.2"
            },
            {
                "text": "Teacher Forcing Practically, we find that updating the generative model only using Eq. 1 leads to unstable training for both vanilla Reinforce and REGS, with the perplexity value skyrocketing after training the model for a few hours (even when the generator is initialized using a pre-trained SEQ2SEQ model). The reason this happens is that the generative model can only be indirectly exposed to the gold-standard target sequences through the reward passed back from the discriminator, and this reward is used to promote or discourage its (the generator's) own generated sequences. Such a training strategy is fragile: once the generator (accidentally) deteriorates in some training batches and the discriminator consequently does an extremely good job in recognizing sequences from the generator, the generator immediately gets lost. It knows that its generated sequences are bad based on the rewards outputted from the discriminator, but it does not know what sequences are good and how to push itself to generate these good sequences (the odds of generating a good response from random sampling are minute, due to the vast size of the space of possible sequences). Loss of the reward signal leads to a breakdown in the training process.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Reward for Every Generation Step (REGS)",
                "sec_num": "3.2"
            },
            {
                "text": "To alleviate this issue and give the generator more direct access to the gold-standard targets, we propose also feeding human generated responses to the generator for model updates. The most straightforward strategy is for the discriminator to automatically assign a reward of 1 (or other positive values) to the human generated responses and for the generator to use this reward to update itself on human generated examples. This can be seen as having a teacher intervene with the generator some fraction of the time and force it to generate the true responses, an approach that is similar to the professor-forcing algorithm of Lamb et al. (2016) .",
                "cite_spans": [
                    {
                        "start": 629,
                        "end": 647,
                        "text": "Lamb et al. (2016)",
                        "ref_id": "BIBREF13"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Reward for Every Generation Step (REGS)",
                "sec_num": "3.2"
            },
            {
                "text": "A closer look reveals that this modification is the same as the standard training of SEQ2SEQ mod- Compute Reward r for (X, \u0176 ) using D. . Update G on (X, \u0176 ) using reward r .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Reward for Every Generation Step (REGS)",
                "sec_num": "3.2"
            },
            {
                "text": "Teacher-Forcing: Update G on (X, Y ) . End End Figure 1 : A brief review of the proposed adversarial reinforcement algorithm for training the generator G and discriminator D. The reward r from the discriminator D can be computed using different strategies according to whether using REINFORCE or REGS. The update of the generator G on (X, \u0176 ) can be done by either using Eq.2 or Eq.3. D-steps is set to 5 and G-steps is set to 1. els, making the final training alternately update the SEQ2SEQ model using the adversarial objective and the MLE objective. One can think of the professor-forcing model as a regularizer to regulate the generator once it starts deviating from the training dataset.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 54,
                        "end": 55,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Reward for Every Generation Step (REGS)",
                "sec_num": "3.2"
            },
            {
                "text": "We also propose another workaround, in which the discriminator first assigns a reward to a human generated example using its own model, and the generator then updates itself using this reward on the human generated example only if the reward is larger than the baseline value. Such a strategy has the advantage that different weights for model updates are assigned to different human generated examples (in the form of different reward values produced by the generator) and that human generated examples are always associated with nonnegative weights.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Reward for Every Generation Step (REGS)",
                "sec_num": "3.2"
            },
            {
                "text": "A sketch of the proposed model is shown in Figure 1 .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 50,
                        "end": 51,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Reward for Every Generation Step (REGS)",
                "sec_num": "3.2"
            },
            {
                "text": "We first pre-train the generative model by predicting target sequences given the dialogue history. We trained a SEQ2SEQ model (Sutskever et al., 2014) with an attention mechanism (Bahdanau et al., 2015; Luong et al., 2015) on the OpenSubtitles dataset. We followed protocols recommended by Sutskever et al. (2014) , such as gradient clipping, mini-batch and learning rate decay. We also pre-train the discriminator. To generate negative examples, we decode part of the training data. Half of the negative examples are generated using beamsearch with mutual information reranking as described in Li et al. (2016a) , and the other half is generated from sampling.",
                "cite_spans": [
                    {
                        "start": 126,
                        "end": 150,
                        "text": "(Sutskever et al., 2014)",
                        "ref_id": "BIBREF39"
                    },
                    {
                        "start": 179,
                        "end": 202,
                        "text": "(Bahdanau et al., 2015;",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 203,
                        "end": 222,
                        "text": "Luong et al., 2015)",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 290,
                        "end": 313,
                        "text": "Sutskever et al. (2014)",
                        "ref_id": "BIBREF39"
                    },
                    {
                        "start": 595,
                        "end": 612,
                        "text": "Li et al. (2016a)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training Details",
                "sec_num": "3.3"
            },
            {
                "text": "For data processing, model training and decoding (both the proposed adversarial training model and the standard SEQ2SEQ models), we employ a few strategies that improve response quality, including: (2) Remove training examples with length of responses shorter than a threshold (set to 5). We find that this significantly improves the general response quality. 5 (2) Instead of using the same learning rate for all examples, using a weighted learning rate that considers the average tf-idf score for tokens within the response. Such a strategy decreases the influence from dull and generic utterances. 6 (3) Penalizing intra-sibling ranking when doing beam search decoding to promote N-best list diversity as described in Li et al. (2016c) . ( 4) Penalizing word types (stop words excluded) that have already been generated. Such a strategy dramatically decreases the rate of repetitive responses such as no. no. no. no. no. or contradictory responses such as I don't like oranges but i like oranges.",
                "cite_spans": [
                    {
                        "start": 721,
                        "end": 738,
                        "text": "Li et al. (2016c)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training Details",
                "sec_num": "3.3"
            },
            {
                "text": "In this section, we discuss strategies for successful adversarial evaluation. Note that the proposed adversarial training and adversarial evaluation are separate procedures. They are independent of each other and share no common parameters.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Adversarial Evaluation",
                "sec_num": "4"
            },
            {
                "text": "The idea of adversarial evaluation, first proposed by Bowman et al. (2016) , is to train a discriminant function to separate generated and true sentences, in an attempt to evaluate the model's sentence generation capability. The idea has been preliminarily studied by Kannan and Vinyals (2016) in the context of dialogue generation. Adversarial evaluation also resembles the idea of the Turing test, which 5 To compensate for the loss of short responses, one can train a separate model using short sequences. 6 We treat each sentence as a document. Stop words are removed. Learning rates are normalized within one batch. For example, suppose t1, t2, ..., ti, ... ,tN denote the tf-idf scores for sentences within current batch and lr denotes the original learning rate. The learning rate for sentence with index i is N \u2022 lr \u2022 t i i t i . To avoid exploding learning rates for sequences with extremely rare words, the tf-idf score of a sentence is capped at L times the minimum tf-idf score in the current batch. L is empirically chosen and is set to 3. requires a human evaluator to distinguish machinegenerated texts from human-generated ones. Since it is time-consuming and costly to ask a human to talk to a model and give judgements, we train a machine evaluator in place of the human evaluator to distinguish the human dialogues and machine dialogues, and we use it to measure the general quality of the generated responses.",
                "cite_spans": [
                    {
                        "start": 54,
                        "end": 74,
                        "text": "Bowman et al. (2016)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 268,
                        "end": 293,
                        "text": "Kannan and Vinyals (2016)",
                        "ref_id": "BIBREF12"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Adversarial Evaluation",
                "sec_num": "4"
            },
            {
                "text": "Adversarial evaluation involves both training and testing. At training time, the evaluator is trained to label dialogues as machine-generated (negative) or human-generated (positive). At test time, the trained evaluator is evaluated on a held-out dataset. If the human-generated dialogues and machinegenerated ones are indistinguishable, the model will achieve 50 percent accuracy at test time.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Adversarial Evaluation",
                "sec_num": "4"
            },
            {
                "text": "We define Adversarial Success (AdverSuc for short) to be the fraction of instances in which a model is capable of fooling the evaluator. AdverSuc is the difference between 1 and the accuracy achieved by the evaluator. Higher values of AdverSuc for a dialogue generation model are better.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Adversarial Success",
                "sec_num": "4.1"
            },
            {
                "text": "One caveat with the adversarial evaluation methods is that they are model-dependent. We approximate the human evaluator in the Turing test with an automatic evaluator and assume that the evaluator is perfect: low accuracy of the discriminator should indicate high quality of the responses, since we interpret this to mean the generated responses are indistinguishable from the human ones. Unfortunately, there is another factor that can lead to low discriminative accuracy: a poor discriminative model. Consider a discriminator that always gives random labels or always gives the same label. Such an evaluator always yields a high AdverSuc value of 0.5. Bowman et al. (2016) propose two different discriminator models separately using unigram features and neural features. It is hard to tell which feature set is more reliable. The standard strategy of testing the model on a held-out development set is not suited to this case, since a model that overfits the development set is necessarily superior.",
                "cite_spans": [
                    {
                        "start": 654,
                        "end": 674,
                        "text": "Bowman et al. (2016)",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Testing the Evaluator's Ability",
                "sec_num": "4.2"
            },
            {
                "text": "To deal with this issue, we propose setting up a few manually-invented situations to test the ability of the automatic evaluator. This is akin to setting up examinations to test the ability of the human evaluator in the Turing test. We report not only the AdverSuc values, but also the scores that the evalu-ator achieves in these manually-designed test cases, indicating how much we can trust the reported Ad-verSuc. We develop scenarios in which we know in advance how a perfect evaluator should behave, and then compare AdverSuc from a discriminative model with the gold-standard AdverSuc. Scenarios we design include:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Testing the Evaluator's Ability",
                "sec_num": "4.2"
            },
            {
                "text": "\u2022 ). The evaluator reliability error (ERE) is the average deviation of an evaluator's adversarial error from the gold-standard error in the above tasks, with equal weight for each task. The smaller the error, the more reliable the evaluator is.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Testing the Evaluator's Ability",
                "sec_num": "4.2"
            },
            {
                "text": "Evaluator reliability error uses scenarios constructed from human-generated dialogues to assess feature or hyper-parameter choice for the evaluator. Unfortunately, no machine-generated responses are involved in the ERE metric. The following example illustrates the serious weakness resulting from this strategy: as will be shown in the experiment section, when inputs are decoded using greedy or beam search models, most generation systems to date yield an adversarial success less than 10 percent (evaluator accuracy 90 percent). But when using sampling for decoding, the adversarial success skyrockets to around 40 percent,7 only 10 percent less than what's needed to pass the Turing test. A close look at the decoded sequences using sampling tells a different story: the responses from 5 and 6 .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 789,
                        "end": 790,
                        "text": "5",
                        "ref_id": null
                    },
                    {
                        "start": 795,
                        "end": 796,
                        "text": "6",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Machine-vs-Random Accuracy",
                "sec_num": "4.3"
            },
            {
                "text": "sampling are sometimes incoherent, irrelevant or even ungrammatical. We thus propose an additional sanity check, in which we report the accuracy of distinguishing between machine-generated responses and randomly sampled responses (machine-vs-random for short). This resembles the N-choose-1 metric described in Shao et al. (2017) . Higher accuracy indicates that the generated responses are distinguishable from randomly sampled human responses, indicating that the generative model is not fooling the generator simply by introducing randomness. As we will show in Sec. 5, using sampling results in high AdverSuc values but low machine-vs-random accuracy.",
                "cite_spans": [
                    {
                        "start": 311,
                        "end": 329,
                        "text": "Shao et al. (2017)",
                        "ref_id": "BIBREF34"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Machine-vs-Random Accuracy",
                "sec_num": "4.3"
            },
            {
                "text": "In this section, we detail experimental results on adversarial success and human evaluation. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Results",
                "sec_num": "5"
            },
            {
                "text": "ERE We first test adversarial evaluation models with different feature sets and model architectures for reliability, as measured by evaluator reliability error (ERE). We explore the following models: (1) SVM+Unigram: SVM using unigram features. 8 A 8 Trained using the SVM-Light package (Joachims, 2002) . multi-utterance dialogue (i.e., input messages and responses) is transformed to a unigram representation; (2) Concat Neural: a neural classification model with a softmax function that takes as input the concatenation of representations of constituent dialogues sentences; (3) Hierarchical Neural: a hierarchical encoder with a structure similar to the discriminator used in the reinforcement; and (4) SVM+Neural+multi-lex-features: a SVM model that uses the following features: unigrams, neural representations of dialogues obtained by the neural model trained using strategy (3),9 the forward likelihood log p(t|s) and backward likelihood p(s|t).",
                "cite_spans": [
                    {
                        "start": 287,
                        "end": 303,
                        "text": "(Joachims, 2002)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Adversarial Evaluation",
                "sec_num": "5.1"
            },
            {
                "text": "ERE scores obtained by different models are reported in Table 2 . As can be seen, the hierarchical neural evaluator (model 3) is more reliable than simply concatenating the sentence-level representations (model 2). Using the combination of neural features and lexicalized features yields the most reliable evaluator. For the rest of this section, we report results obtained by the Hierarchical Neural setting due to its end-to-end nature, despite its inferiority to SVM+Neural+multil-features.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 62,
                        "end": 63,
                        "text": "2",
                        "ref_id": "TABREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Adversarial Evaluation",
                "sec_num": "5.1"
            },
            {
                "text": "Table 3 presents AdverSuc values for different models, along with machine-vs-random accuracy described in Section 4.3. Higher values of Adver-Suc and machine-vs-random are better.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "3",
                        "ref_id": "TABREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Adversarial Evaluation",
                "sec_num": "5.1"
            },
            {
                "text": "Baselines we consider include standard SEQ2SEQ models using greedy decoding (MLEgreedy), beam-search (MLE+BS) and sampling, as well as the mutual information reranking model of Li et al. (2016a) with two algorithmic variations:",
                "cite_spans": [
                    {
                        "start": 177,
                        "end": 194,
                        "text": "Li et al. (2016a)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Adversarial Evaluation",
                "sec_num": "5.1"
            },
            {
                "text": "(1) MMI+p(t|s), in which a large N-best list is first Results are shown in Table 3 . What first stands out is decoding using sampling (as discussed in Section 4.3), achieving a significantly higher AdverSuc number than all the rest models. However, this does not indicate the superiority of the sampling decoding model, since the machine-vs-random accuracy is at the same time significantly lower. This means that sampled responses based on SEQ2SEQ models are not only hard for an evaluator to distinguish from real human responses, but also from randomly sampled responses. A similar, though much less extreme, effect is observed for MMI-p(t), which has an AdverSuc value slightly higher than Adver-Reinforce, but a significantly lower machine-vsrandom score.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 81,
                        "end": 82,
                        "text": "3",
                        "ref_id": "TABREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Adversarial Evaluation",
                "sec_num": "5.1"
            },
            {
                "text": "By comparing different baselines, we find that MMI+p(t|s) is better than MLE-greedy, which is in turn better than MLE+BS. This result is in line with human-evaluation results from Li et al. (2016a) . The two proposed adversarial algorithms achieve better performance than the baselines. We expect this to be the case, since the adversarial algorithms are trained on an objective function more similar to the evaluation metric (i.e., adversarial success). REGS performs slightly better than the vanilla RE-INFORCE algorithm.",
                "cite_spans": [
                    {
                        "start": 180,
                        "end": 197,
                        "text": "Li et al. (2016a)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Adversarial Evaluation",
                "sec_num": "5.1"
            },
            {
                "text": "For human evaluation, we follow protocols defined in Li et al. (2016d) , employing crowdsourced judges to evaluate a random sample of 200 items. We present both an input message and the generated outputs to 3 judges and ask them to decide which of the two outputs is better (single-turn general quality). Ties are permitted. Identical strings are assigned the same score. We also present the judges with multi-turn conversations simulated between the two agents. Each conversation consists 4 . We observe a significant quality improvement on both single-turn quality and multi-turn quality from the proposed adversarial model. It is worth noting that the reinforcement learning system described in Li et al. (2016d) , which simulates conversations between two bots and is trained based on manually designed reward functions, only improves multiturn dialogue quality, while the model described in this paper improves both single-turn and multiturn dialogue generation quality. This confirms that the reward adopted in adversarial training is more general, natural and effective in training dialogue systems.",
                "cite_spans": [
                    {
                        "start": 53,
                        "end": 70,
                        "text": "Li et al. (2016d)",
                        "ref_id": null
                    },
                    {
                        "start": 698,
                        "end": 715,
                        "text": "Li et al. (2016d)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 490,
                        "end": 491,
                        "text": "4",
                        "ref_id": "TABREF5"
                    }
                ],
                "eq_spans": [],
                "section": "Human Evaluation",
                "sec_num": "5.2"
            },
            {
                "text": "In this paper, drawing intuitions from the Turing test, we propose using an adversarial training approach for response generation. We cast the model in the framework of reinforcement learning and train a generator based on the signal from a discriminator to generate response sequences indistinguishable from human-generated dialogues. We observe clear performance improvements on multiple metrics from the adversarial training strategy. The adversarial training model should theoretically benefit a variety of generation tasks in NLP. Unfortunately, in preliminary experiments applying the same training paradigm to machine translation, we did not observe a clear performance boost. We conjecture that this is because the adversarial training strategy is more beneficial to tasks in which there is a big discrepancy between the distributions of the generated sequences and the reference target sequences. In other words, the adversarial approach is more beneficial on tasks in which entropy of the targets is high. Exploring this relationship further is a focus of our future work. Zhou Yu, Ziyu Xu, Alan W Black, and Alex I Rudnicky. 2016b . Strategy and policy learning for nontask-oriented conversational systems. In 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue. page 404.",
                "cite_spans": [
                    {
                        "start": 1083,
                        "end": 1141,
                        "text": "Zhou Yu, Ziyu Xu, Alan W Black, and Alex I Rudnicky. 2016b",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion and Future Work",
                "sec_num": "6"
            },
            {
                "text": "Yuan Zhang, Regina Barzilay, and Tommi Jaakkola. 2017.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion and Future Work",
                "sec_num": "6"
            },
            {
                "text": "Aspect-augmented adversarial networks for domain adaptation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion and Future Work",
                "sec_num": "6"
            },
            {
                "text": "arXiv preprint arXiv:1701.00188 .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion and Future Work",
                "sec_num": "6"
            },
            {
                "text": "We approximate the dialogue history using the concatenation of two preceding utterances. We found that using more",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "than 2 context utterances yields very tiny performance improvements for SEQ2SEQ models.2 To be specific, each utterance p or q is mapped to a vector representation hp or hq using LSTM(Hochreiter and Schmidhuber, 1997). Another LSTM is put on sentence level, mapping the context dialogue sequence to a single representation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Consider one target sequence with length 20, we need to sample",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "5*20=100 full sequences to get rewards for all intermediate steps. Training one batch with 128 examples roughly takes roughly 1 min on a single GPU, which is computationally intractable considering the size of the dialogue data we have. We thus parallelize the sampling processes, distributing jobs across 8 GPUs.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Similar results are also reported inKannan and Vinyals (2016).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "The representation before the softmax layer.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "Acknowledgements The authors thank Michel Galley, Bill Dolan, Chris Brockett, Jianfeng Gao and other members of the NLP group at Mi-crosoft Research, as well as Sumit Chopra and Marc'Aurelio Ranzato from Facebook AI Research for helpful discussions and comments. Jiwei Li is supported by a Facebook Fellowship, which we gratefully acknowledge. This work is also partially supported by the NSF under award IIS-1514268, and the DARPA Communicating with Computers (CwC) program under ARO prime contract no. W911NF-15-1-0462, IIS-1464128. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of DARPA, the NSF, or Facebook.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "acknowledgement",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Online sequence-to-sequence reinforcement learning for open-domain conversational agents",
                "authors": [
                    {
                        "first": "Nabiha",
                        "middle": [],
                        "last": "Asghar",
                        "suffix": ""
                    },
                    {
                        "first": "Pasca",
                        "middle": [],
                        "last": "Poupart",
                        "suffix": ""
                    },
                    {
                        "first": "Jiang",
                        "middle": [],
                        "last": "Xin",
                        "suffix": ""
                    },
                    {
                        "first": "Hang",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1612.03929"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Nabiha Asghar, Pasca Poupart, Jiang Xin, and Hang Li. 2016. Online sequence-to-sequence reinforcement learning for open-domain conversational agents. arXiv preprint arXiv:1612.03929 .",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "An actor-critic algorithm for sequence prediction",
                "authors": [
                    {
                        "first": "Dzmitry",
                        "middle": [],
                        "last": "Bahdanau",
                        "suffix": ""
                    },
                    {
                        "first": "Philemon",
                        "middle": [],
                        "last": "Brakel",
                        "suffix": ""
                    },
                    {
                        "first": "Kelvin",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Anirudh",
                        "middle": [],
                        "last": "Goyal",
                        "suffix": ""
                    },
                    {
                        "first": "Ryan",
                        "middle": [],
                        "last": "Lowe",
                        "suffix": ""
                    },
                    {
                        "first": "Joelle",
                        "middle": [],
                        "last": "Pineau",
                        "suffix": ""
                    },
                    {
                        "first": "Aaron",
                        "middle": [],
                        "last": "Courville",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron Courville, and Yoshua Bengio. 2017. An actor-critic algorithm for sequence prediction. ICLR .",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Neural machine translation by jointly learning to align and translate",
                "authors": [
                    {
                        "first": "Dzmitry",
                        "middle": [],
                        "last": "Bahdanau",
                        "suffix": ""
                    },
                    {
                        "first": "Kyunghyun",
                        "middle": [],
                        "last": "Cho",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proc. of ICLR",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2015. Neural machine translation by jointly learning to align and translate. In Proc. of ICLR.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Generating sentences from a continuous space",
                "authors": [
                    {
                        "first": "Luke",
                        "middle": [],
                        "last": "Samuel R Bowman",
                        "suffix": ""
                    },
                    {
                        "first": "Oriol",
                        "middle": [],
                        "last": "Vilnis",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [
                            "M"
                        ],
                        "last": "Vinyals",
                        "suffix": ""
                    },
                    {
                        "first": "Rafal",
                        "middle": [],
                        "last": "Dai",
                        "suffix": ""
                    },
                    {
                        "first": "Samy",
                        "middle": [],
                        "last": "Jozefowicz",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Samuel R Bowman, Luke Vilnis, Oriol Vinyals, An- drew M Dai, Rafal Jozefowicz, and Samy Ben- gio. 2016. Generating sentences from a continuous space. CoNLL .",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "2016a. Infogan: Interpretable representation learning by information maximizing generative adversarial nets",
                "authors": [
                    {
                        "first": "Xi",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Yan",
                        "middle": [],
                        "last": "Duan",
                        "suffix": ""
                    },
                    {
                        "first": "Rein",
                        "middle": [],
                        "last": "Houthooft",
                        "suffix": ""
                    },
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Schulman",
                        "suffix": ""
                    },
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    },
                    {
                        "first": "Pieter",
                        "middle": [],
                        "last": "Abbeel",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "Advances In Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "2172--2180",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. 2016a. Info- gan: Interpretable representation learning by infor- mation maximizing generative adversarial nets. In Advances In Neural Information Processing Systems. pages 2172-2180.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Adversarial deep averaging networks for cross-lingual sentiment classification",
                "authors": [
                    {
                        "first": "Xilun",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Ben",
                        "middle": [],
                        "last": "Athiwaratkun",
                        "suffix": ""
                    },
                    {
                        "first": "Yu",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Kilian",
                        "middle": [],
                        "last": "Weinberger",
                        "suffix": ""
                    },
                    {
                        "first": "Claire",
                        "middle": [],
                        "last": "Cardie",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1606.01614"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Xilun Chen, Ben Athiwaratkun, Yu Sun, Kilian Wein- berger, and Claire Cardie. 2016b. Adversarial deep averaging networks for cross-lingual sentiment clas- sification. arXiv preprint arXiv:1606.01614 .",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Deep generative image models using a? laplacian pyramid of adversarial networks",
                "authors": [
                    {
                        "first": "Soumith",
                        "middle": [],
                        "last": "Emily L Denton",
                        "suffix": ""
                    },
                    {
                        "first": "Rob",
                        "middle": [],
                        "last": "Chintala",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Fergus",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Advances in neural information processing systems",
                "volume": "",
                "issue": "",
                "pages": "1486--1494",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Emily L Denton, Soumith Chintala, Rob Fergus, et al. 2015. Deep generative image models using a? laplacian pyramid of adversarial networks. In Ad- vances in neural information processing systems. pages 1486-1494.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Likelihood ratio gradient estimation for stochastic systems",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Peter W Glynn",
                        "suffix": ""
                    }
                ],
                "year": 1990,
                "venue": "Communications of the ACM",
                "volume": "33",
                "issue": "10",
                "pages": "75--84",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Peter W Glynn. 1990. Likelihood ratio gradient estima- tion for stochastic systems. Communications of the ACM 33(10):75-84.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Generative adversarial nets",
                "authors": [
                    {
                        "first": "Ian",
                        "middle": [],
                        "last": "Goodfellow",
                        "suffix": ""
                    },
                    {
                        "first": "Jean",
                        "middle": [],
                        "last": "Pouget-Abadie",
                        "suffix": ""
                    },
                    {
                        "first": "Mehdi",
                        "middle": [],
                        "last": "Mirza",
                        "suffix": ""
                    },
                    {
                        "first": "Bing",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Warde-Farley",
                        "suffix": ""
                    },
                    {
                        "first": "Sherjil",
                        "middle": [],
                        "last": "Ozair",
                        "suffix": ""
                    },
                    {
                        "first": "Aaron",
                        "middle": [],
                        "last": "Courville",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "2672--2680",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative ad- versarial nets. In Advances in Neural Information Processing Systems. pages 2672-2680.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Long short-term memory",
                "authors": [
                    {
                        "first": "Sepp",
                        "middle": [],
                        "last": "Hochreiter",
                        "suffix": ""
                    },
                    {
                        "first": "J\u00fcrgen",
                        "middle": [],
                        "last": "Schmidhuber",
                        "suffix": ""
                    }
                ],
                "year": 1997,
                "venue": "Neural computation",
                "volume": "9",
                "issue": "8",
                "pages": "1735--1780",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural computation 9(8):1735- 1780.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Learning to classify text using support vector machines: Methods, theory and algorithms",
                "authors": [
                    {
                        "first": "Thorsten",
                        "middle": [],
                        "last": "Joachims",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Thorsten Joachims. 2002. Learning to classify text us- ing support vector machines: Methods, theory and algorithms. Kluwer Academic Publishers.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Adversarial evaluation of dialogue models",
                "authors": [
                    {
                        "first": "Anjuli",
                        "middle": [],
                        "last": "Kannan",
                        "suffix": ""
                    },
                    {
                        "first": "Oriol",
                        "middle": [],
                        "last": "Vinyals",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "NIPS 2016 Workshop on Adversarial Training",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Anjuli Kannan and Oriol Vinyals. 2016. Adversarial evaluation of dialogue models. In NIPS 2016 Work- shop on Adversarial Training.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Professor forcing: A new algorithm for training recurrent networks",
                "authors": [
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Lamb",
                        "suffix": ""
                    },
                    {
                        "first": "Anirudh",
                        "middle": [],
                        "last": "Goyal",
                        "suffix": ""
                    },
                    {
                        "first": "Ying",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Saizheng",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Aaron",
                        "middle": [],
                        "last": "Courville",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Advances In Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "4601--4609",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alex Lamb, Anirudh Goyal, Ying Zhang, Saizheng Zhang, Aaron Courville, and Yoshua Bengio. 2016. Professor forcing: A new algorithm for training re- current networks. In Advances In Neural Informa- tion Processing Systems. pages 4601-4609.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "2016a. A diversity-promoting objective function for neural conversation models",
                "authors": [
                    {
                        "first": "Jiwei",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Michel",
                        "middle": [],
                        "last": "Galley",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Brockett",
                        "suffix": ""
                    },
                    {
                        "first": "Jianfeng",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Bill",
                        "middle": [],
                        "last": "Dolan",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "Proc. of NAACL-HLT",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. 2016a. A diversity-promoting ob- jective function for neural conversation models. In Proc. of NAACL-HLT.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Georgios Spithourakis, Jianfeng Gao, and Bill Dolan. 2016b. A persona-based neural conversation model",
                "authors": [
                    {
                        "first": "Jiwei",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Michel",
                        "middle": [],
                        "last": "Galley",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Brockett",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics",
                "volume": "1",
                "issue": "",
                "pages": "994--1003",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jiwei Li, Michel Galley, Chris Brockett, Georgios Sp- ithourakis, Jianfeng Gao, and Bill Dolan. 2016b. A persona-based neural conversation model. In Pro- ceedings of the 54th Annual Meeting of the Asso- ciation for Computational Linguistics (Volume 1: Long Papers). Berlin, Germany, pages 994-1003. http://www.aclweb.org/anthology/P16-1094.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "A hierarchical neural autoencoder for paragraphs and documents",
                "authors": [
                    {
                        "first": "Jiwei",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Minh-Thang",
                        "middle": [],
                        "last": "Luong",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Jurafsky",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jiwei Li, Minh-Thang Luong, and Dan Jurafsky. 2015. A hierarchical neural autoencoder for paragraphs and documents. ACL .",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "A simple, fast diverse decoding algorithm for neural generation",
                "authors": [
                    {
                        "first": "Jiwei",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Will",
                        "middle": [],
                        "last": "Monroe",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Jurafsky",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1611.08562"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jiwei Li, Will Monroe, and Dan Jurafsky. 2016c. A simple, fast diverse decoding algorithm for neural generation. arXiv preprint arXiv:1611.08562 .",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Deep reinforcement learning for dialogue generation",
                "authors": [
                    {
                        "first": "Jiwei",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Will",
                        "middle": [],
                        "last": "Monroe",
                        "suffix": ""
                    },
                    {
                        "first": "Alan",
                        "middle": [],
                        "last": "Ritter",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Jurafsky",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jiwei Li, Will Monroe, Alan Ritter, and Dan Jurafsky. 2016d. Deep reinforcement learning for dialogue generation. EMNLP .",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation",
                "authors": [
                    {
                        "first": "Chia-Wei",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Ryan",
                        "middle": [],
                        "last": "Lowe",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Iulian V Serban",
                        "suffix": ""
                    },
                    {
                        "first": "Laurent",
                        "middle": [],
                        "last": "Noseworthy",
                        "suffix": ""
                    },
                    {
                        "first": "Joelle",
                        "middle": [],
                        "last": "Charlin",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Pineau",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chia-Wei Liu, Ryan Lowe, Iulian V Serban, Michael Noseworthy, Laurent Charlin, and Joelle Pineau. 2016. How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation met- rics for dialogue response generation. EMNLP .",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Towards an automatic turing test: Learning to evaluate dialogue responses",
                "authors": [
                    {
                        "first": "Ryan",
                        "middle": [],
                        "last": "Lowe",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Noseworthy",
                        "suffix": ""
                    },
                    {
                        "first": "Iulian",
                        "middle": [],
                        "last": "Serban",
                        "suffix": ""
                    },
                    {
                        "first": "Nicolas",
                        "middle": [],
                        "last": "Angelard-Gontier",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    },
                    {
                        "first": "Joelle",
                        "middle": [],
                        "last": "Pineau",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ryan Lowe, Michael Noseworthy, Iulian Serban, Nico- las Angelard-Gontier, Yoshua Bengio, and Joelle Pineau. 2017. Towards an automatic turing test: Learning to evaluate dialogue responses. ACL .",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "On the evaluation of dialogue systems with next utterance classification",
                "authors": [
                    {
                        "first": "Ryan",
                        "middle": [],
                        "last": "Lowe",
                        "suffix": ""
                    },
                    {
                        "first": "Mike",
                        "middle": [],
                        "last": "Iulian V Serban",
                        "suffix": ""
                    },
                    {
                        "first": "Laurent",
                        "middle": [],
                        "last": "Noseworthy",
                        "suffix": ""
                    },
                    {
                        "first": "Joelle",
                        "middle": [],
                        "last": "Charlin",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Pineau",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ryan Lowe, Iulian V Serban, Mike Noseworthy, Lau- rent Charlin, and Joelle Pineau. 2016. On the evalu- ation of dialogue systems with next utterance classi- fication. SIGDIAL .",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "LSTM based conversation models",
                "authors": [
                    {
                        "first": "Yi",
                        "middle": [],
                        "last": "Luan",
                        "suffix": ""
                    },
                    {
                        "first": "Yangfeng",
                        "middle": [],
                        "last": "Ji",
                        "suffix": ""
                    },
                    {
                        "first": "Mari",
                        "middle": [],
                        "last": "Ostendorf",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1603.09457"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yi Luan, Yangfeng Ji, and Mari Ostendorf. 2016. LSTM based conversation models. arXiv preprint arXiv:1603.09457 .",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Effective approaches to attentionbased neural machine translation",
                "authors": [
                    {
                        "first": "Minh-Thang",
                        "middle": [],
                        "last": "Luong",
                        "suffix": ""
                    },
                    {
                        "first": "Hieu",
                        "middle": [],
                        "last": "Pham",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Minh-Thang Luong, Hieu Pham, and Christopher D Manning. 2015. Effective approaches to attention- based neural machine translation. ACL .",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Unsupervised representation learning with deep convolutional generative adversarial networks",
                "authors": [
                    {
                        "first": "Alec",
                        "middle": [],
                        "last": "Radford",
                        "suffix": ""
                    },
                    {
                        "first": "Luke",
                        "middle": [],
                        "last": "Metz",
                        "suffix": ""
                    },
                    {
                        "first": "Soumith",
                        "middle": [],
                        "last": "Chintala",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1511.06434"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Alec Radford, Luke Metz, and Soumith Chintala. 2015. Unsupervised representation learning with deep con- volutional generative adversarial networks. arXiv preprint arXiv:1511.06434 .",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Sequence level training with recurrent neural networks",
                "authors": [
                    {
                        "first": "Aurelio",
                        "middle": [],
                        "last": "Marc",
                        "suffix": ""
                    },
                    {
                        "first": "Sumit",
                        "middle": [],
                        "last": "Ranzato",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Chopra",
                        "suffix": ""
                    },
                    {
                        "first": "Wojciech",
                        "middle": [],
                        "last": "Auli",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Zaremba",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Marc'Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. 2016. Sequence level train- ing with recurrent neural networks. ICLR .",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Data-driven response generation in social media",
                "authors": [
                    {
                        "first": "Alan",
                        "middle": [],
                        "last": "Ritter",
                        "suffix": ""
                    },
                    {
                        "first": "Colin",
                        "middle": [],
                        "last": "Cherry",
                        "suffix": ""
                    },
                    {
                        "first": "William",
                        "middle": [
                            "B"
                        ],
                        "last": "Dolan",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Proceedings of EMNLP 2011",
                "volume": "",
                "issue": "",
                "pages": "583--593",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alan Ritter, Colin Cherry, and William B Dolan. 2011. Data-driven response generation in social media. In Proceedings of EMNLP 2011. pages 583-593.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Improved techniques for training gans",
                "authors": [
                    {
                        "first": "Tim",
                        "middle": [],
                        "last": "Salimans",
                        "suffix": ""
                    },
                    {
                        "first": "Ian",
                        "middle": [],
                        "last": "Goodfellow",
                        "suffix": ""
                    },
                    {
                        "first": "Wojciech",
                        "middle": [],
                        "last": "Zaremba",
                        "suffix": ""
                    },
                    {
                        "first": "Vicki",
                        "middle": [],
                        "last": "Cheung",
                        "suffix": ""
                    },
                    {
                        "first": "Alec",
                        "middle": [],
                        "last": "Radford",
                        "suffix": ""
                    },
                    {
                        "first": "Xi",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "2226--2234",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. 2016. Improved techniques for training gans. In Advances in Neural Information Processing Systems. pages 2226-2234.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Building end-to-end dialogue systems using generative hierarchical neural network models",
                "authors": [
                    {
                        "first": "Alessandro",
                        "middle": [],
                        "last": "Iulian V Serban",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Sordoni",
                        "suffix": ""
                    },
                    {
                        "first": "Aaron",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    },
                    {
                        "first": "Joelle",
                        "middle": [],
                        "last": "Courville",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Pineau",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of AAAI",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Iulian V Serban, Alessandro Sordoni, Yoshua Bengio, Aaron Courville, and Joelle Pineau. 2016a. Build- ing end-to-end dialogue systems using generative hi- erarchical neural network models. In Proceedings of AAAI.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Building end-to-end dialogue systems using generative hierarchical neural network models",
                "authors": [
                    {
                        "first": "Alessandro",
                        "middle": [],
                        "last": "Iulian V Serban",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Sordoni",
                        "suffix": ""
                    },
                    {
                        "first": "Aaron",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    },
                    {
                        "first": "Joelle",
                        "middle": [],
                        "last": "Courville",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Pineau",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI-16)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Iulian V Serban, Alessandro Sordoni, Yoshua Bengio, Aaron Courville, and Joelle Pineau. 2016b. Build- ing end-to-end dialogue systems using generative hi- erarchical neural network models. In Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI-16).",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "Multiresolution recurrent neural networks: An application to dialogue response generation",
                "authors": [
                    {
                        "first": "Iulian",
                        "middle": [],
                        "last": "Vlad Serban",
                        "suffix": ""
                    },
                    {
                        "first": "Tim",
                        "middle": [],
                        "last": "Klinger",
                        "suffix": ""
                    },
                    {
                        "first": "Gerald",
                        "middle": [],
                        "last": "Tesauro",
                        "suffix": ""
                    },
                    {
                        "first": "Kartik",
                        "middle": [],
                        "last": "Talamadupula",
                        "suffix": ""
                    },
                    {
                        "first": "Bowen",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    },
                    {
                        "first": "Aaron",
                        "middle": [],
                        "last": "Courville",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1606.00776"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Iulian Vlad Serban, Tim Klinger, Gerald Tesauro, Kartik Talamadupula, Bowen Zhou, Yoshua Ben- gio, and Aaron Courville. 2016c. Multiresolu- tion recurrent neural networks: An application to dialogue response generation. arXiv preprint arXiv:1606.00776 .",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "Generative deep neural networks for dialogue: A short review",
                "authors": [
                    {
                        "first": "Iulian",
                        "middle": [],
                        "last": "Vlad Serban",
                        "suffix": ""
                    },
                    {
                        "first": "Ryan",
                        "middle": [],
                        "last": "Lowe",
                        "suffix": ""
                    },
                    {
                        "first": "Laurent",
                        "middle": [],
                        "last": "Charlin",
                        "suffix": ""
                    },
                    {
                        "first": "Joelle",
                        "middle": [],
                        "last": "Pineau",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Iulian Vlad Serban, Ryan Lowe, Laurent Charlin, and Joelle Pineau. 2016d. Generative deep neural net- works for dialogue: A short review .",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "A hierarchical latent variable encoder-decoder model for generating dialogues",
                "authors": [
                    {
                        "first": "Iulian",
                        "middle": [],
                        "last": "Vlad Serban",
                        "suffix": ""
                    },
                    {
                        "first": "Alessandro",
                        "middle": [],
                        "last": "Sordoni",
                        "suffix": ""
                    },
                    {
                        "first": "Ryan",
                        "middle": [],
                        "last": "Lowe",
                        "suffix": ""
                    },
                    {
                        "first": "Laurent",
                        "middle": [],
                        "last": "Charlin",
                        "suffix": ""
                    },
                    {
                        "first": "Joelle",
                        "middle": [],
                        "last": "Pineau",
                        "suffix": ""
                    },
                    {
                        "first": "Aaron",
                        "middle": [],
                        "last": "Courville",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe, Laurent Charlin, Joelle Pineau, Aaron Courville, and Yoshua Bengio. 2017. A hierarchical latent variable encoder-decoder model for generating dia- logues. AAAI .",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "Neural responding machine for short-text conversation",
                "authors": [
                    {
                        "first": "Lifeng",
                        "middle": [],
                        "last": "Shang",
                        "suffix": ""
                    },
                    {
                        "first": "Zhengdong",
                        "middle": [],
                        "last": "Lu",
                        "suffix": ""
                    },
                    {
                        "first": "Hang",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of ACL-IJCNLP",
                "volume": "",
                "issue": "",
                "pages": "1577--1586",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Lifeng Shang, Zhengdong Lu, and Hang Li. 2015. Neu- ral responding machine for short-text conversation. In Proceedings of ACL-IJCNLP. pages 1577-1586.",
                "links": null
            },
            "BIBREF34": {
                "ref_id": "b34",
                "title": "Generating long and diverse responses with neural conversational models",
                "authors": [
                    {
                        "first": "Louis",
                        "middle": [],
                        "last": "Shao",
                        "suffix": ""
                    },
                    {
                        "first": "Stephan",
                        "middle": [],
                        "last": "Gouws",
                        "suffix": ""
                    },
                    {
                        "first": "Denny",
                        "middle": [],
                        "last": "Britz",
                        "suffix": ""
                    },
                    {
                        "first": "Anna",
                        "middle": [],
                        "last": "Goldie",
                        "suffix": ""
                    },
                    {
                        "first": "Brian",
                        "middle": [],
                        "last": "Strope",
                        "suffix": ""
                    },
                    {
                        "first": "Ray",
                        "middle": [],
                        "last": "Kurzweil",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Louis Shao, Stephan Gouws, Denny Britz, Anna Goldie, Brian Strope, and Ray Kurzweil. 2017. Gen- erating long and diverse responses with neural con- versational models. ICLR .",
                "links": null
            },
            "BIBREF35": {
                "ref_id": "b35",
                "title": "Minimum risk training for neural machine translation",
                "authors": [
                    {
                        "first": "Shiqi",
                        "middle": [],
                        "last": "Shen",
                        "suffix": ""
                    },
                    {
                        "first": "Yong",
                        "middle": [],
                        "last": "Cheng",
                        "suffix": ""
                    },
                    {
                        "first": "Zhongjun",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Hua",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Maosong",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua Wu, Maosong Sun, and Yang Liu. 2016. Minimum risk training for neural machine translation. ACL .",
                "links": null
            },
            "BIBREF36": {
                "ref_id": "b36",
                "title": "Mastering the game of Go with deep neural networks and tree search",
                "authors": [
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Silver",
                        "suffix": ""
                    },
                    {
                        "first": "Aja",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [
                            "J"
                        ],
                        "last": "Maddison",
                        "suffix": ""
                    },
                    {
                        "first": "Arthur",
                        "middle": [],
                        "last": "Guez",
                        "suffix": ""
                    },
                    {
                        "first": "Laurent",
                        "middle": [],
                        "last": "Sifre",
                        "suffix": ""
                    },
                    {
                        "first": "George",
                        "middle": [],
                        "last": "Van Den",
                        "suffix": ""
                    },
                    {
                        "first": "Julian",
                        "middle": [],
                        "last": "Driessche",
                        "suffix": ""
                    },
                    {
                        "first": "Ioannis",
                        "middle": [],
                        "last": "Schrittwieser",
                        "suffix": ""
                    },
                    {
                        "first": "Veda",
                        "middle": [],
                        "last": "Antonoglou",
                        "suffix": ""
                    },
                    {
                        "first": "Marc",
                        "middle": [],
                        "last": "Panneershelvam",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Lanctot",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Nature",
                "volume": "529",
                "issue": "7587",
                "pages": "484--489",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Ju- lian Schrittwieser, Ioannis Antonoglou, Veda Pan- neershelvam, Marc Lanctot, et al. 2016. Mastering the game of Go with deep neural networks and tree search. Nature 529(7587):484-489.",
                "links": null
            },
            "BIBREF37": {
                "ref_id": "b37",
                "title": "A neural network approach to context-sensitive generation of conversational responses",
                "authors": [
                    {
                        "first": "Alessandro",
                        "middle": [],
                        "last": "Sordoni",
                        "suffix": ""
                    },
                    {
                        "first": "Michel",
                        "middle": [],
                        "last": "Galley",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Auli",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Brockett",
                        "suffix": ""
                    },
                    {
                        "first": "Yangfeng",
                        "middle": [],
                        "last": "Ji",
                        "suffix": ""
                    },
                    {
                        "first": "Meg",
                        "middle": [],
                        "last": "Mitchell",
                        "suffix": ""
                    },
                    {
                        "first": "Jian-Yun",
                        "middle": [],
                        "last": "Nie",
                        "suffix": ""
                    },
                    {
                        "first": "Jianfeng",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Bill",
                        "middle": [],
                        "last": "Dolan",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of NAACL-HLT",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alessandro Sordoni, Michel Galley, Michael Auli, Chris Brockett, Yangfeng Ji, Meg Mitchell, Jian- Yun Nie, Jianfeng Gao, and Bill Dolan. 2015. A neural network approach to context-sensitive gener- ation of conversational responses. In Proceedings of NAACL-HLT.",
                "links": null
            },
            "BIBREF38": {
                "ref_id": "b38",
                "title": "Continuously learning neural dialogue management",
                "authors": [
                    {
                        "first": "Pei-Hao",
                        "middle": [],
                        "last": "Su",
                        "suffix": ""
                    },
                    {
                        "first": "Milica",
                        "middle": [],
                        "last": "Gasic",
                        "suffix": ""
                    },
                    {
                        "first": "Nikola",
                        "middle": [],
                        "last": "Mrksic",
                        "suffix": ""
                    },
                    {
                        "first": "Lina",
                        "middle": [],
                        "last": "Rojas-Barahona",
                        "suffix": ""
                    },
                    {
                        "first": "Stefan",
                        "middle": [],
                        "last": "Ultes",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Vandyke",
                        "suffix": ""
                    },
                    {
                        "first": "Tsung-Hsien",
                        "middle": [],
                        "last": "Wen",
                        "suffix": ""
                    },
                    {
                        "first": "Steve",
                        "middle": [],
                        "last": "Young",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Pei-Hao Su, Milica Gasic, Nikola Mrksic, Lina Rojas- Barahona, Stefan Ultes, David Vandyke, Tsung- Hsien Wen, and Steve Young. 2016. Continuously learning neural dialogue management. arxiv .",
                "links": null
            },
            "BIBREF39": {
                "ref_id": "b39",
                "title": "Sequence to sequence learning with neural networks",
                "authors": [
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    },
                    {
                        "first": "Oriol",
                        "middle": [],
                        "last": "Vinyals",
                        "suffix": ""
                    },
                    {
                        "first": "Quoc V",
                        "middle": [],
                        "last": "Le",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Advances in neural information processing systems",
                "volume": "",
                "issue": "",
                "pages": "3104--3112",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. In Advances in neural information processing sys- tems. pages 3104-3112.",
                "links": null
            },
            "BIBREF40": {
                "ref_id": "b40",
                "title": "Computing machinery and intelligence",
                "authors": [
                    {
                        "first": "Alan",
                        "middle": [
                            "M"
                        ],
                        "last": "Turing",
                        "suffix": ""
                    }
                ],
                "year": 1950,
                "venue": "Mind",
                "volume": "59",
                "issue": "236",
                "pages": "433--460",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alan M Turing. 1950. Computing machinery and intel- ligence. Mind 59(236):433-460.",
                "links": null
            },
            "BIBREF41": {
                "ref_id": "b41",
                "title": "A neural conversational model",
                "authors": [
                    {
                        "first": "Oriol",
                        "middle": [],
                        "last": "Vinyals",
                        "suffix": ""
                    },
                    {
                        "first": "Quoc",
                        "middle": [],
                        "last": "Le",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of ICML Deep Learning Workshop",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Oriol Vinyals and Quoc Le. 2015. A neural conversa- tional model. In Proceedings of ICML Deep Learn- ing Workshop.",
                "links": null
            },
            "BIBREF42": {
                "ref_id": "b42",
                "title": "A networkbased end-to-end trainable task-oriented dialogue system",
                "authors": [
                    {
                        "first": "Tsung-Hsien",
                        "middle": [],
                        "last": "Wen",
                        "suffix": ""
                    },
                    {
                        "first": "Milica",
                        "middle": [],
                        "last": "Gasic",
                        "suffix": ""
                    },
                    {
                        "first": "Nikola",
                        "middle": [],
                        "last": "Mrksic",
                        "suffix": ""
                    },
                    {
                        "first": "Lina",
                        "middle": [
                            "M"
                        ],
                        "last": "Rojas-Barahona",
                        "suffix": ""
                    },
                    {
                        "first": "Pei-Hao",
                        "middle": [],
                        "last": "Su",
                        "suffix": ""
                    },
                    {
                        "first": "Stefan",
                        "middle": [],
                        "last": "Ultes",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Vandyke",
                        "suffix": ""
                    },
                    {
                        "first": "Steve",
                        "middle": [],
                        "last": "Young",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1604.04562"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Lina M Rojas-Barahona, Pei-Hao Su, Stefan Ultes, David Vandyke, and Steve Young. 2016. A network- based end-to-end trainable task-oriented dialogue system. arXiv preprint arXiv:1604.04562 .",
                "links": null
            },
            "BIBREF43": {
                "ref_id": "b43",
                "title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning",
                "authors": [
                    {
                        "first": "Williams",
                        "middle": [],
                        "last": "Ronald",
                        "suffix": ""
                    }
                ],
                "year": 1992,
                "venue": "Machine learning",
                "volume": "8",
                "issue": "3-4",
                "pages": "229--256",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ronald J Williams. 1992. Simple statistical gradient- following algorithms for connectionist reinforce- ment learning. Machine learning 8(3-4):229-256.",
                "links": null
            },
            "BIBREF44": {
                "ref_id": "b44",
                "title": "Sequence-to-sequence learning as beam-search optimization",
                "authors": [
                    {
                        "first": "Sam",
                        "middle": [],
                        "last": "Wiseman",
                        "suffix": ""
                    },
                    {
                        "first": "Alexander",
                        "middle": [
                            "M"
                        ],
                        "last": "Rush",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sam Wiseman and Alexander M Rush. 2016. Sequence-to-sequence learning as beam-search optimization. ACL .",
                "links": null
            },
            "BIBREF45": {
                "ref_id": "b45",
                "title": "Incorporating loose-structured knowledge into LSTM with recall gate for conversation modeling",
                "authors": [
                    {
                        "first": "Zhen",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Bingquan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Baoxun",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Chengjie",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaolong",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1605.05110"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Zhen Xu, Bingquan Liu, Baoxun Wang, Chengjie Sun, and Xiaolong Wang. 2016. Incorporating loose-structured knowledge into LSTM with recall gate for conversation modeling. arXiv preprint arXiv:1605.05110 .",
                "links": null
            },
            "BIBREF46": {
                "ref_id": "b46",
                "title": "Attention with intention for a neural network conversation model",
                "authors": [
                    {
                        "first": "Kaisheng",
                        "middle": [],
                        "last": "Yao",
                        "suffix": ""
                    },
                    {
                        "first": "Geoffrey",
                        "middle": [],
                        "last": "Zweig",
                        "suffix": ""
                    },
                    {
                        "first": "Baolin",
                        "middle": [],
                        "last": "Peng",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "NIPS workshop on Machine Learning for Spoken Language Understanding and Interaction",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kaisheng Yao, Geoffrey Zweig, and Baolin Peng. 2015. Attention with intention for a neural network conver- sation model. In NIPS workshop on Machine Learn- ing for Spoken Language Understanding and Inter- action.",
                "links": null
            },
            "BIBREF47": {
                "ref_id": "b47",
                "title": "Seqgan: sequence generative adversarial nets with policy gradient",
                "authors": [
                    {
                        "first": "Lantao",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Weinan",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Jun",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Yong",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1609.05473"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. 2016a. Seqgan: sequence generative adver- sarial nets with policy gradient. arXiv preprint arXiv:1609.05473 .",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "We use human-generated dialogues as both positive examples and negative examples. A perfect evaluator should give an AdverSuc of 0.5 (accuracy 50%), which is the goldstandard result. \u2022 We use machine-generated dialogues as both positive examples and negative examples. A perfect evaluator should give an AdverSuc of 0.5 (accuracy 50%). \u2022 We use original human-generated dialogues as positive examples and dialogues consisting of random utterances as negative examples. A perfect evaluator should give an AdverSuc of 0 (accuracy 100%). \u2022 We use original human-generated dialogues as positive examples and dialogues with responses replaced by the utterances immediately following the true responses. A perfect evaluation should give an AdverSuc of 0 (accuracy 100%",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "TABREF2": {
                "content": "<table><tr><td>Input</td><td>tell me ... how long have you had this falling sickness ?</td></tr><tr><td>Vanilla-MLE</td><td>i 'm not a doctor .</td></tr><tr><td>Vanilla-Sample</td><td>well everything you did was totally untrue .</td></tr><tr><td>REINFORCE</td><td>i don 't know how long it 's been .</td></tr><tr><td>REGS Monte Carlo</td><td>A few months, I guess .</td></tr><tr><td>Input</td><td>so i had the doctors test sammy 's response to conditioning .</td></tr><tr><td>Vanilla-MLE</td><td>sammy wrote the test sammy wrote the test .</td></tr><tr><td>Vanilla-Sample</td><td>objects pick objects up objects objects objects objects objects</td></tr><tr><td>REINFORCE</td><td>i 'm not sure that 's relevant .</td></tr><tr><td>REGS Monte Carlo</td><td>so he took the pills .</td></tr><tr><td>Input</td><td>what are you going to do ?</td></tr><tr><td>Vanilla-MLE</td><td>i 'm going to the movies .</td></tr><tr><td>Vanilla-Sample</td><td>get him outta here first !</td></tr><tr><td>REINFORCE</td><td>i 'm going to get you .</td></tr><tr><td>REGS Monte Carlo</td><td>i 'm going to get you a new car .</td></tr><tr><td>Input</td><td>they fear your power your intellect .</td></tr><tr><td>Vanilla-MLE</td><td>you 're the only one who knows what 's going on .</td></tr><tr><td>Vanilla-Sample</td><td>when they are conquered and you surrender they will control all of us .</td></tr><tr><td>REINFORCE</td><td>i 'm afraid i 'm not ready yet .</td></tr><tr><td>REGS Monte Carlo</td><td>i 'm not afraid of your power .</td></tr></table>",
                "type_str": "table",
                "text": "Sampled responses from different models. More in Appendix Tables",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table><tr><td>Setting</td><td>ERE</td></tr><tr><td>SVM+Unigram</td><td>0.232</td></tr><tr><td>Concat Neural</td><td>0.209</td></tr><tr><td>Hierarchical Neural</td><td>0.193</td></tr><tr><td colspan=\"2\">SVM+Neural+multil-features 0.152</td></tr></table>",
                "type_str": "table",
                "text": "ERE scores obtained by different models.",
                "html": null,
                "num": null
            },
            "TABREF4": {
                "content": "<table><tr><td>Model</td><td colspan=\"2\">AdverSuc machine-vs-random</td></tr><tr><td>MLE-BS</td><td>0.037</td><td>0.942</td></tr><tr><td>MLE-Greedy</td><td>0.049</td><td>0.945</td></tr><tr><td>MMI+p(t|s)</td><td>0.073</td><td>0.953</td></tr><tr><td>MMI-p(t)</td><td>0.090</td><td>0.880</td></tr><tr><td>Sampling</td><td>0.372</td><td>0.679</td></tr><tr><td>Adver-Reinforce</td><td>0.080</td><td>0.945</td></tr><tr><td>Adver-REGS</td><td>0.098</td><td>0.952</td></tr><tr><td colspan=\"3\">generated using a pre-trained SEQ2SEQ model and</td></tr><tr><td colspan=\"3\">then reranked by the backward probability p(s|t)</td></tr><tr><td colspan=\"3\">and (2) MMI-p(t), in which language model</td></tr><tr><td colspan=\"3\">probability is penalized during decoding.</td></tr></table>",
                "type_str": "table",
                "text": "AdverSuc and machine-vs-random scores achieved by different training/decoding strategies.",
                "html": null,
                "num": null
            },
            "TABREF5": {
                "content": "<table><tr><td>Setting</td><td colspan=\"2\">adver-win adver-lose</td><td>tie</td></tr><tr><td>single-turn</td><td>0.62</td><td>0.18</td><td>0.20</td></tr><tr><td>multi-turn</td><td>0.72</td><td>0.10</td><td>0.18</td></tr></table>",
                "type_str": "table",
                "text": "The gain from the proposed adversarial model over the mutual information system based on pairwise human judgments. of 3 turns. Results are presented in Table",
                "html": null,
                "num": null
            }
        }
    }
}