{
    "paper_id": "P15-1042",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:10:30.848416Z"
    },
    "title": "Learning Bilingual Sentiment Word Embeddings for Cross-language Sentiment Classification",
    "authors": [
        {
            "first": "Huiwei",
            "middle": [],
            "last": "Zhou",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Dalian University of Technology",
                "location": {
                    "settlement": "Dalian",
                    "country": "P.R. China"
                }
            },
            "email": "zhouhuiwei@dlut.edu.cn"
        },
        {
            "first": "Long",
            "middle": [],
            "last": "Chen",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Dalian University of Technology",
                "location": {
                    "settlement": "Dalian",
                    "country": "P.R. China"
                }
            },
            "email": ""
        },
        {
            "first": "Fulin",
            "middle": [],
            "last": "Shi",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Dalian University of Technology",
                "location": {
                    "settlement": "Dalian",
                    "country": "P.R. China"
                }
            },
            "email": ""
        },
        {
            "first": "Degen",
            "middle": [],
            "last": "Huang",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Dalian University of Technology",
                "location": {
                    "settlement": "Dalian",
                    "country": "P.R. China"
                }
            },
            "email": "huangdg@dlut.edu.cn"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "The sentiment classification performance relies on high-quality sentiment resources. However, these resources are imbalanced in different languages. Cross-language sentiment classification (CLSC) can leverage the rich resources in one language (source language) for sentiment classification in a resource-scarce language (target language). Bilingual embeddings could eliminate the semantic gap between two languages for CLSC, but ignore the sentiment information of text. This paper proposes an approach to learning bilingual sentiment word embeddings (BSWE) for English-Chinese CLSC. The proposed B-SWE incorporate sentiment information of text into bilingual embeddings. Furthermore, we can learn high-quality BSWE by simply employing labeled corpora and their translations, without relying on largescale parallel corpora. Experiments on NLP&CC 2013 CLSC dataset show that our approach outperforms the state-of-theart systems.",
    "pdf_parse": {
        "paper_id": "P15-1042",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "The sentiment classification performance relies on high-quality sentiment resources. However, these resources are imbalanced in different languages. Cross-language sentiment classification (CLSC) can leverage the rich resources in one language (source language) for sentiment classification in a resource-scarce language (target language). Bilingual embeddings could eliminate the semantic gap between two languages for CLSC, but ignore the sentiment information of text. This paper proposes an approach to learning bilingual sentiment word embeddings (BSWE) for English-Chinese CLSC. The proposed B-SWE incorporate sentiment information of text into bilingual embeddings. Furthermore, we can learn high-quality BSWE by simply employing labeled corpora and their translations, without relying on largescale parallel corpora. Experiments on NLP&CC 2013 CLSC dataset show that our approach outperforms the state-of-theart systems.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Sentiment classification is a task of predicting sentiment polarity of text, which has attracted considerable interest in the NLP field. To date, a number of corpus-based approaches (Pang et al., 2002; Pang and Lee, 2004; Kennedy and Inkpen, 2006) have been developed for sentiment classification. The approaches heavily rely on quality and quantity of the labeled corpora, which are considered as the most valuable resources in sentiment classification task. However, such sentiment resources are imbalanced in different languages. To leverage resources in the source language to improve the sentiment classification performance in the target language, cross-language sentiment classification (CLSC) approaches have been investigated.",
                "cite_spans": [
                    {
                        "start": 182,
                        "end": 201,
                        "text": "(Pang et al., 2002;",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 202,
                        "end": 221,
                        "text": "Pang and Lee, 2004;",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 222,
                        "end": 247,
                        "text": "Kennedy and Inkpen, 2006)",
                        "ref_id": "BIBREF15"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The traditional CLSC approaches employ machine translation (MT) systems to translate corpora in the source language into the target language, and train the sentiment classifiers in the target language (Banea et al., 2008) . Directly employing the translated resources for sentiment classification in the target language is simple and could get acceptable results. However, the gap between the source language and target language inevitably impacts the performance of sentiment classification. To improve the classification accuracy, multiview approaches have been proposed. In these approaches, the resources in the source language and their translations in the target language are both used to train sentiment classifiers in two independent views (Wan, 2009; Gui et al., 2013; Zhou et al., 2014a) . The final results are determined by ensemble classifiers in these two views to overcome the weakness of monolingual classifiers. However, learning language-specific classifiers in each view fails to capture the common sentiment information of two languages during training process.",
                "cite_spans": [
                    {
                        "start": 201,
                        "end": 221,
                        "text": "(Banea et al., 2008)",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 748,
                        "end": 759,
                        "text": "(Wan, 2009;",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 760,
                        "end": 777,
                        "text": "Gui et al., 2013;",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 778,
                        "end": 797,
                        "text": "Zhou et al., 2014a)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "With the revival of interest in deep learning (Hinton and Salakhutdinov, 2006) , shared deep representations (or embeddings) (Bengio et al., 2013) are employed for CLSC (Chandar A P et al., 2013) . Usually, paired sentences from parallel corpora are used to learn word embeddings across languages (Chandar A P et al., 2013; Chandar A P et al., 2014) , eliminating the need of MT systems. The learned bilingual embeddings could easily project the training data and test data into a common space, where training and testing are performed. However, high-quality bilingual embeddings rely on the large-scale task-related parallel corpora, which are not always readily available. Meanwhile, though semantic similarities across languages are captured during bilingual embedding learning process, sentiment information of text is ignored. That is, bilingual embeddings learned from unlabeled parallel corpora are not effective enough for CLSC because of a lack of explicit sentiment information. Tang and Wan (2014) first proposed a bilingual sentiment embedding model using the original training data and the corresponding translations through a linear mapping rather than deep learning technique.",
                "cite_spans": [
                    {
                        "start": 46,
                        "end": 78,
                        "text": "(Hinton and Salakhutdinov, 2006)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 125,
                        "end": 146,
                        "text": "(Bengio et al., 2013)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 169,
                        "end": 195,
                        "text": "(Chandar A P et al., 2013)",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 297,
                        "end": 323,
                        "text": "(Chandar A P et al., 2013;",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 324,
                        "end": 349,
                        "text": "Chandar A P et al., 2014)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 989,
                        "end": 1008,
                        "text": "Tang and Wan (2014)",
                        "ref_id": "BIBREF24"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "This paper proposes a denoising autoencoder based approach to learning bilingual sentiment word embeddings (BSWE) for CLSC, which incorporates sentiment polarities of text into the bilingual embeddings. The proposed approach learns BSWE with the original labeled documents and their translations instead of parallel corpora. The BSWE learning process consists of two phases: the unsupervised phase of semantic learning and the supervised phase of sentiment learning. In the unsupervised phase, sentiment words and their negation features are extracted from the source training data and their translations to represent paired documents. These features are used as inputs for a denoising autoencoder to learn the bilingual embeddings. In the supervised phase, sentiment polarity labels of documents are used to guide BSWE learning for incorporating sentiment information into the bilingual embeddings.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The learned BSWE are applied to project English training data and Chinese test data into a common space. In this space, a linear support vector machine (SVM) is used to perform training and testing. The experiments are carried on NLP&CC 2013 CLSC dataset, including book, DVD and music categories. Experimental results show that our approach achieves 80.68% average accuracy, which outperforms the state-of-the-art systems on this dataset. Although the BSWE are only evaluated on English-Chinese CLSC here, it can be popularized to many other languages.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The major contributions of this work can be summarized as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\u2022 We propose bilingual sentiment word embeddings (BSWE) for CLSC based on deep learning technique. Experimental results show that the proposed BSWE significantly outperform the bilingual embeddings by incorporating sentiment information.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\u2022 Instead of large-scale parallel corpora, only the labeled English corpora and Englishto-Chinese translations are required for B-SWE learning. It is proved that in spite of the small-scale of training set, our approach outperforms the state-of-the-art systems in NLP&CC 2013 CLSC share task.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\u2022 We employ sentiment words and their negation features rather than all words in documents to learn sentiment-specific embeddings, which significantly reduces the dimension of input vectors as well as improves sentiment classification performance.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this section, we review the literature related to this paper from two perspectives: cross-language sentiment classification and embedding learning for sentiment classification.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "The critical problem of CLSC is how to bridge the gap between the source language and target language. Machine translations or parallel corpora are usually employed to solve this problem. We present a brief review of CLSC from two aspects: machine translation based approaches and parallel corpora based approaches. Machine translation based approaches use MT systems to project training data into the target language or test data into the source language. Wan (2009) proposed a co-training approach for CLSC. The approach first translated Chinese test data into English, and English training data into Chinese. Then, they performed training and testing in two independent views: English view and Chinese view. Gui et al. (2013) combined self-training approach with co-training approach by estimating the confidence of each monolingual system. Li et al. (2013) selected the samples in the source language that were similar to those in the target language to decrease the gap between two languages. Zhou et al. (2014a) proposed a combination CLSC model, which adopted denoising autoencoders (Vincent et al., 2008) to enhance the robustness to translation errors of the input.",
                "cite_spans": [
                    {
                        "start": 457,
                        "end": 467,
                        "text": "Wan (2009)",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 711,
                        "end": 728,
                        "text": "Gui et al. (2013)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 844,
                        "end": 860,
                        "text": "Li et al. (2013)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 998,
                        "end": 1017,
                        "text": "Zhou et al. (2014a)",
                        "ref_id": null
                    },
                    {
                        "start": 1090,
                        "end": 1112,
                        "text": "(Vincent et al., 2008)",
                        "ref_id": "BIBREF25"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Cross-language Sentiment Classification (CLSC)",
                "sec_num": "2.1"
            },
            {
                "text": "Most recently, a number of studies adopt deep learning technique to learn bilingual representations with parallel corpora. Bilingual representations have been successfully applied in many NLP tasks, such as machine translation (Zou et al., 2013) , sentiment classification (Chandar A P et al., 2013; Zhou et al., 2014b) , text classification (Chandar A P et al., 2014) , etc. Chandar A P et al. (2013) learned bilingual representations with aligned sentences throughout two phases: the language-specific representation learning phase and the shared representation learning phase. In the language-specific representation learning phase, they applied autoencoders to obtain a language-specific representation for each entity in two languages respectively. In shared representation learning phase, pairs of parallel language-specific representations were passed to an autoencoder to learn bilingual representations. To joint language-specific representations and bilingual representations, Chandar A P et al. (2014) integrated the two learning phases into a unified process to learn bilingual embeddings. Zhou et al. (2014b) employed bilingual representations for English-Chinese CLSC. The work mentioned above employed aligned sentences in bilingual embedding learning process. However, in the sentiment classification process, only representations in the source language are used for training, and representations in the target language are used for testing, which ignores the interactions of semantic information between the source language and target language.",
                "cite_spans": [
                    {
                        "start": 227,
                        "end": 245,
                        "text": "(Zou et al., 2013)",
                        "ref_id": "BIBREF31"
                    },
                    {
                        "start": 273,
                        "end": 299,
                        "text": "(Chandar A P et al., 2013;",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 300,
                        "end": 319,
                        "text": "Zhou et al., 2014b)",
                        "ref_id": null
                    },
                    {
                        "start": 342,
                        "end": 368,
                        "text": "(Chandar A P et al., 2014)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 376,
                        "end": 401,
                        "text": "Chandar A P et al. (2013)",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 987,
                        "end": 1012,
                        "text": "Chandar A P et al. (2014)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 1102,
                        "end": 1121,
                        "text": "Zhou et al. (2014b)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Cross-language Sentiment Classification (CLSC)",
                "sec_num": "2.1"
            },
            {
                "text": "Bilingual embedding learning algorithms focus on capturing syntactic and semantic similarities across languages, but ignore sentiment information. To date, many embedding learning algorithms have been developed for sentiment classification problem by incorporating sentiment information into word embeddings. Maas et al. (2011) presented a probabilistic model that combined unsupervised and supervised techniques to learn word vectors, capturing semantic information as well as sentiment information. Wang et al. (2014) introduced sentiment labels into Neural Network Language Models (Bengio et al., 2003) to enhance sentiment expression ability of word vectors. Tang et al. (2014) theoretically and empirically analyzed the effects of the syntactic context and sentiment information in word vectors, and showed that the syntactic context and sentiment information were equally important to sentiment classification.",
                "cite_spans": [
                    {
                        "start": 309,
                        "end": 327,
                        "text": "Maas et al. (2011)",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 501,
                        "end": 519,
                        "text": "Wang et al. (2014)",
                        "ref_id": null
                    },
                    {
                        "start": 584,
                        "end": 605,
                        "text": "(Bengio et al., 2003)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 663,
                        "end": 681,
                        "text": "Tang et al. (2014)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Embedding Learning for Sentiment Classification",
                "sec_num": "2.2"
            },
            {
                "text": "Recent years have seen a surge of interest in word embeddings with deep learning technique (Bespalov et al., 2011; Glorot et al., 2011; Socher et al., 2011; Socher et al., 2012) , which have been empirically shown to preserve linguistic regularities (Mikolov et al., 2013) . Our work focuses on learning bilingual sentiment word embeddings (B-SWE) with deep learning technique. Unlike the work of Chandar A P et al. (2014) that adopted parallel corpora to learn bilingual embeddings, we only use training data and their translations to learn BSWE. More importantly, sentiment information is integrated into bilingual embeddings to improve their performance in CLSC.",
                "cite_spans": [
                    {
                        "start": 91,
                        "end": 114,
                        "text": "(Bespalov et al., 2011;",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 115,
                        "end": 135,
                        "text": "Glorot et al., 2011;",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 136,
                        "end": 156,
                        "text": "Socher et al., 2011;",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 157,
                        "end": 177,
                        "text": "Socher et al., 2012)",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 250,
                        "end": 272,
                        "text": "(Mikolov et al., 2013)",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 397,
                        "end": 422,
                        "text": "Chandar A P et al. (2014)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Embedding Learning for Sentiment Classification",
                "sec_num": "2.2"
            },
            {
                "text": "3 Bilingual Sentiment Word Embeddings (BSWE) for Cross-language Sentiment Classification",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Embedding Learning for Sentiment Classification",
                "sec_num": "2.2"
            },
            {
                "text": "It has been demonstrated that the denoising autoencoder could decrease the effects of translation errors on the performance of CLSC (Zhou et al., 2014a) . This paper proposes a deep learning based approach, which employs the denoising autoencoder to learn the bilingual embeddings for CLSC.",
                "cite_spans": [
                    {
                        "start": 132,
                        "end": 152,
                        "text": "(Zhou et al., 2014a)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Denoising Autoencoder",
                "sec_num": "3.1"
            },
            {
                "text": "A denoising autoencoder is the modification of an autoencoder. The autoencoder (Bengio et al., 2007) includes an encoder f \u03b8 and a decoder g \u03b8 . The encoder maps a d-dimensional input vector x \u2208 [0, 1] d to a hidden representation y \u2208 [0, 1] d through a deterministic mapping y = f \u03b8 (x) = \u03c3(Wx + b), parameterized by \u03b8 = {W, b}. W is a weight matrix, b is a bias term, and \u03c3(x) is the activation function. The decoder maps y back to a reconstructed vector x = g \u03b8 (y) = \u03c3(W T y + c), parameterized by \u03b8 = {W T , c}, where c is the bias term for reconstruction.",
                "cite_spans": [
                    {
                        "start": 79,
                        "end": 100,
                        "text": "(Bengio et al., 2007)",
                        "ref_id": "BIBREF2"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Denoising Autoencoder",
                "sec_num": "3.1"
            },
            {
                "text": "Through the process of encoding and decoding, the parameters \u03b8 and \u03b8 of the autoencoder will be trained by gradient descent to minimize the loss function. The sum of reconstruction crossentropies across the training set is usually used as the loss function:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Denoising Autoencoder",
                "sec_num": "3.1"
            },
            {
                "text": "l(x) = - d i=1 [x i log xi +(1-x i ) log(1-x i )] (1)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Denoising Autoencoder",
                "sec_num": "3.1"
            },
            {
                "text": "A denoising autoencoder enhances robustness to noises by corrupting the input x to a partially destroyed version x. The desired noise level of the input x can be changed by adjusting the destruction fraction \u03bd. For each input x, a fixed number \u03bdd (d is the dimension of x) of components are selected randomly, and their values are set to 0, while the others are left untouched. Like an autoencoder, the destroyed input x is mapped to a latent representation y = f \u03b8 (x) = \u03c3(Wx + b). Then y is mapped back to a reconstructed vector x through x = g \u03b8 (y) = \u03c3(W T y + c). The loss function of a denoising autoencoder is the same as that of an autoencoder. Minimizing the loss makes x close to the input x rather than x.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Denoising Autoencoder",
                "sec_num": "3.1"
            },
            {
                "text": "Our BSWE learning process can be divided into two phases: the unsupervised phase of semantic learning and the supervised phase of sentiment learning. In the unsupervised phase, a denoising autoencoder is employed to learn the bilingual embeddings. In the supervised phase, the sentiment information is incorporated into the bilingual embeddings based on sentiment labels of documents to obtain BSWE.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Denoising Autoencoder",
                "sec_num": "3.1"
            },
            {
                "text": "In the unsupervised phase, the English training documents and their Chinese translations are employed to learn the bilingual embeddings (Sentiment polarity labels of documents are not employed in this phase). Based on the English documents, 2,000 English sentiment words in MPQA subjectivity lexicon 1 are extracted by the Chisquare method (Galavotti et al., 2000) . Their corresponding Chinese translations are used as Chinese sentiment words. Besides, some sentiment words are often modified by negation words, which lead to inversion of their polarities. Therefore, negation features are introduced to each sentiment word to represent its negative form. We take into account 14 frequently-used negation words in English such as not and none; 5 negation words in Chinese such as \u00d8 (no/not) and vk (without). A sentiment word modified by these negation words in the window [-2, 2] is considered as its negative form in this paper, while sentiment word features remain the initial meaning. Negation features use binary expressions. If a sentiment word is not modified by negation words, the value of its negation features is set to 0. Thus, the sentiment words and their corresponding negation features in English and Chinese are adopted to represent the document pairs (x E , x C ).",
                "cite_spans": [
                    {
                        "start": 340,
                        "end": 364,
                        "text": "(Galavotti et al., 2000)",
                        "ref_id": "BIBREF10"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Unsupervised Phase of the Bilingual Embedding Learning",
                "sec_num": "3.2"
            },
            {
                "text": "We expect that pairs of documents could be forced to capture the common semantic information of two languages. To achieve this, a denoising 1 http://mpqa.cs.pitt.edu/lexicons/subj lexicon autoencoder is used to perform the reconstructions of paired documents in both English and Chinese. Figure 1 shows the framework of bilingual embedding learning. For the corrupted versions xE (x C ) of the initial input vector x E (x C ), we use the sigmoid function as the activation function to extract latent representations:",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 295,
                        "end": 296,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Unsupervised Phase of the Bilingual Embedding Learning",
                "sec_num": "3.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "E W E x \uf025 E y C x [ ] T E C W W \uff0c C x ) , ( C E l x x E x \u02c6E x ) ( E l x C W C x \uf025 C y C x \u02c6C x ) , ( E C l x x E x \u02c6E x ) ( C l x [ ] T E C W W \uff0c E x C x",
                        "eq_num": "("
                    }
                ],
                "section": "Unsupervised Phase of the Bilingual Embedding Learning",
                "sec_num": "3.2"
            },
            {
                "text": "y E = f \u03b8 (x E ) = \u03c3(W E xE + b)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Unsupervised Phase of the Bilingual Embedding Learning",
                "sec_num": "3.2"
            },
            {
                "text": "(2)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Unsupervised Phase of the Bilingual Embedding Learning",
                "sec_num": "3.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "y C = f \u03b8 (x C ) = \u03c3(W C xC + b)",
                        "eq_num": "(3)"
                    }
                ],
                "section": "Unsupervised Phase of the Bilingual Embedding Learning",
                "sec_num": "3.2"
            },
            {
                "text": "where W E and W C are the language-specific word representation matrices, corresponding to English and Chinese respectively. Notice that the bias b is shared to ensure that the produced representations in two languages are on the same scale.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Unsupervised Phase of the Bilingual Embedding Learning",
                "sec_num": "3.2"
            },
            {
                "text": "For the latent representations in either language, we would like two decoders to perform reconstructions in English and Chinese respectively. As shown in Figure 1 (a), for the latent representation y E in English, one decoder is used to map y E back to a reconstruction xE in English, and the other is used to map y E back to a reconstruction xC in Chinese such that:",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 161,
                        "end": 162,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Unsupervised Phase of the Bilingual Embedding Learning",
                "sec_num": "3.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "xE = g \u03b8 (y E ) = \u03c3(W T E y E + c E ) (4) xC = g \u03b8 (y E ) = \u03c3(W T C y E + c C )",
                        "eq_num": "(5)"
                    }
                ],
                "section": "Unsupervised Phase of the Bilingual Embedding Learning",
                "sec_num": "3.2"
            },
            {
                "text": "where c E and c C are the biases of the decoders in English and Chinese, respectively. Similarly, the same steps repeat for the latent representation y C in Chinese, which are shown in Figure 1(b) .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 192,
                        "end": 196,
                        "text": "1(b)",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Unsupervised Phase of the Bilingual Embedding Learning",
                "sec_num": "3.2"
            },
            {
                "text": "The encoder and decoder structures allow us to learn a mapping within and across languages. Specifically, for a given document pair (x E , x C ), we can learn bilingual embeddings to reconstruct x E from itself (loss l(x E )), reconstruc-",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Unsupervised Phase of the Bilingual Embedding Learning",
                "sec_num": "3.2"
            },
            {
                "text": "t x C from itself (loss l(x C )), construct x C from x E (loss l(x E , x C )), construct x E from x C (loss l(x C , x E )) and reconstruct the concatena- tion of x E and x C ([x E , x C ]) from itself (loss l([x E , x C ], [x E , xC ])",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Unsupervised Phase of the Bilingual Embedding Learning",
                "sec_num": "3.2"
            },
            {
                "text": "). The sum of 5 losses is used as the loss function of bilingual embeddings:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Unsupervised Phase of the Bilingual Embedding Learning",
                "sec_num": "3.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "L =l(x E ) + l(x C ) + l(x E , x C ) + l(x C , x E ) + l([x E , x C ], [x E , xC ])",
                        "eq_num": "(6)"
                    }
                ],
                "section": "Unsupervised Phase of the Bilingual Embedding Learning",
                "sec_num": "3.2"
            },
            {
                "text": "In the unsupervised phase, we have learned the bilingual embeddings, which could capture the semantic information within and across languages. However, the sentiment polarities of text are ignored in the unsupervised phase. Bilingual embeddings without sentiment information are not effective enough for sentiment classification task. This paper proposes an approach to learning B-SWE for CLSC, which introduces a supervised learning phase to incorporate sentiment information into the bilingual embeddings. The process of supervised phase is shown in Figure 2 . For paired documents [x E , x C ], the sigmoid function is adopted as the activation function to extract latent bilingual representations",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 559,
                        "end": 560,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Supervised Phase of Sentiment Learning",
                "sec_num": "3.3"
            },
            {
                "text": "y b = \u03c3([W E , W C ][x E , x C ] + b), where [W E , W C ] is the concatenation of W E and W C .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Supervised Phase of Sentiment Learning",
                "sec_num": "3.3"
            },
            {
                "text": "The latent bilingual representation y b is used to obtain the positive polarity probability p(s = 1|d; \u03be) of a document through a sigmoid function:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Supervised Phase of Sentiment Learning",
                "sec_num": "3.3"
            },
            {
                "text": "p(s = 1|d; \u03be) = \u03c3(\u03d5 T y b + b l ) (7)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Supervised Phase of Sentiment Learning",
                "sec_num": "3.3"
            },
            {
                "text": "where \u03d5 is the logistic regression weight vector and b l is the bias of logistic regression. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Supervised Phase of Sentiment Learning",
                "sec_num": "3.3"
            },
            {
                "text": "\u03be * = arg max \u03be i=1 log p(s i |d i ; \u03be) (8)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Supervised Phase of Sentiment Learning",
                "sec_num": "3.3"
            },
            {
                "text": "Through the supervised learning phase, [W E , W C ] is optimized by maximizing sentiment polarity probability. Thus, rich sentiment information is encoded into the bilingual embeddings.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Supervised Phase of Sentiment Learning",
                "sec_num": "3.3"
            },
            {
                "text": "The following experiments will prove that the proposed BSWE outperform the traditional bilingual embeddings significantly in CLSC.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Supervised Phase of Sentiment Learning",
                "sec_num": "3.3"
            },
            {
                "text": "Method (BDR)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Bilingual Document Representation",
                "sec_num": "3.4"
            },
            {
                "text": "Once we have learned BSWE [W E , W C ], whose columns are representations for sentiment words, we can use them to represent documents in two languages.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Bilingual Document Representation",
                "sec_num": "3.4"
            },
            {
                "text": "Given an English training document d E containing 2,000 sentiment word features s 1 , s 2 , \u2022 \u2022 \u2022 , s 2,000 and 2,000 corresponding negation features, we represent it as the TF-IDF weighted sum of BSWE:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Bilingual Document Representation",
                "sec_num": "3.4"
            },
            {
                "text": "\u03c6 d E = 4,000 i=1 T F -IDF (s i )W E.,s i (9)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Bilingual Document Representation",
                "sec_num": "3.4"
            },
            {
                "text": "Similarly, for its Chinese translation d C containing 2,000 sentiment word features t 1 , t 2 , \u2022 \u2022 \u2022 , t 2,000 and 2,000 corresponding negation features, we represent it as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Bilingual Document Representation",
                "sec_num": "3.4"
            },
            {
                "text": "\u03c6 d C = 4,000 j=1 T F -IDF (t j )W C.,t j (10)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Bilingual Document Representation",
                "sec_num": "3.4"
            },
            {
                "text": "We propose a bilingual document representation method (BDR) in this paper, which represents each document d i with the concatenation of its English and Chinese representations [\u03c6 d E , \u03c6 d C ]. B-DR is expected to enhance the ability of sentiment expression for further improving the classification performance. Such bilingual document representations are fed to a linear SVM to perform sentiment classification.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Bilingual Document Representation",
                "sec_num": "3.4"
            },
            {
                "text": "Data Set. The proposed approach is evaluated on NLP&CC 2013 CLSC dataset 2 3 . The dataset con-sists of product reviews on three categories: book, DVD, and music. Each category contains 4,000 English labeled data as training data (the ratio of the number of positive and negative samples is 1:1) and 4,000 Chinese unlabeled data as test data.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Settings",
                "sec_num": "4.1"
            },
            {
                "text": "Tools. In our experiments, Google Translate4 is adopted for both English-to-Chinese and Chineseto-English translation. ICTCLAS (Zhang et al., 2003) is used as Chinese word segmentation tool. A denoising autoencoder is developed based on Theano system (Bergstra et al., 2010) . BSWE are trained for 50 and 30 epochs in unsupervised phase and supervised phases respectively. SV M light (Joachims, 1999) is used to train linear SVM sentiment classifiers Evaluation Metric. The performance is evaluated by the classification accuracy for each category, and the average accuracy of three categories, respectively. The category accuracy is defined as:",
                "cite_spans": [
                    {
                        "start": 127,
                        "end": 147,
                        "text": "(Zhang et al., 2003)",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 251,
                        "end": 274,
                        "text": "(Bergstra et al., 2010)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 384,
                        "end": 400,
                        "text": "(Joachims, 1999)",
                        "ref_id": "BIBREF14"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Settings",
                "sec_num": "4.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "Accuracy c = #system correct c #system total c (",
                        "eq_num": "11"
                    }
                ],
                "section": "Experimental Settings",
                "sec_num": "4.1"
            },
            {
                "text": ")",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Settings",
                "sec_num": "4.1"
            },
            {
                "text": "where c is one of the three categories, and #system correct c and #system total c stand for the number of being correctly classified reviews and the number of total reviews in the category c, respectively. The average accuracy is shown as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Settings",
                "sec_num": "4.1"
            },
            {
                "text": "Average = 1 3 c Accuracy c (12)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Settings",
                "sec_num": "4.1"
            },
            {
                "text": "In this section, we evaluate the quality of BSWE for CLSC. The dimension of bilingual embeddings d is set to 50, and destruction fraction \u03bd is set to 0.2.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluations on BSWE",
                "sec_num": "4.2"
            },
            {
                "text": "We first compare our unsupervised bilingual embedding learning method with the parallel corpora based method. The parallel corpora based method uses the paired documents in the parallel corpus5 to learn bilingual embeddings, while our method only uses the English training documents and their Chinese translations (Sentiment polarity labels of documents are not employed here). The Boolean feature weight calculation method is adopted to represent documents for bilingual embedding learning and BDR is employed to represent training data and test data for sentiment classification. To represent the paired documents in the parallel corpus, 27,597 English words and 31,786 Chinese words are extracted for bilingual embedding learning. Our method only needs 2,000 English sentiment words, 2,000 Chinese sentiment words, and their negation features, which significantly reduces the dimension of input vectors. The average accuracies on NLP&CC 2013 test data of the two bilingual embedding learning methods are shown in Figure 3 . As can be seen from Figure 3 , when the corpus scales of the two methods are the same (4,000 paired documents), our method (75.09% average accuracy) surpasses the parallel corpora method (54.82% average accuracy) by about 20%. With the scale of the parallel corpora increasing, the performance of parallel corpora based method is steadily improved. However, the performance is not as good as our bilingual embedding learning method. Though the document number of the parallel corpus is up to 70,000 , the average accuracy is only 70.05%. It is proved that our method is more suitable for learning bilingual embeddings for cross-language sentiment classification than the parallel corpora based method.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 1023,
                        "end": 1024,
                        "text": "3",
                        "ref_id": "FIGREF3"
                    },
                    {
                        "start": 1054,
                        "end": 1055,
                        "text": "3",
                        "ref_id": "FIGREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Effects of Bilingual Embedding Learning Methods",
                "sec_num": null
            },
            {
                "text": "In this part, we compare the Boolean and TF-IDF feature weight calculation methods in bilingual embedding learning process.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Effects of Feature Weight in Bilingual Embeddings",
                "sec_num": null
            },
            {
                "text": "Table 1 shows the classification accuracy with the Boolean and TF-IDF methods. Generally, the TF-IDF method performs better than the Boolean method. The average accuracy of the TF-IDF method is 1.16% higher than the Boolean method, which illustrates that the TF-IDF method could reflect the latent contribution of sentiment words to each document effectively. The TF-IDF weight calculation method is exploited in the following experiments. Notice that sentiment information",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "1",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Effects of Feature Weight in Bilingual Embeddings",
                "sec_num": null
            },
            {
                "text": "is not yet introduced in the bilingual embeddings here.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Effects of Feature Weight in Bilingual Embeddings",
                "sec_num": null
            },
            {
                "text": "Incorporating sentiment information in the bilingual embeddings, the performance of bilingual embeddings (without sentiment information) and BSWE (with sentiment information) is compared in Figure 4 . As can be seen from Figure 4 , by encoding sentiment information in the bilingual embeddings, the performance in book, DVD and music categories significantly improves to 79.47%, 78.72% and 76.58% respectively (2.82% increase in book, 1.12% in DVD, and 2.08% in music). The average accuracy reaches 78.26%, which is 2.01% higher than that of the bilingual embeddings. The experimental results indicate the effectiveness of sentiment information in the bilingual embedding learning. The BSWE learning approach is employed for CLSC in the following experiments.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 197,
                        "end": 198,
                        "text": "4",
                        "ref_id": "FIGREF4"
                    },
                    {
                        "start": 228,
                        "end": 229,
                        "text": "4",
                        "ref_id": "FIGREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Effects of Sentiment Information in BSWE",
                "sec_num": null
            },
            {
                "text": "In this experiment, our bilingual document representation method (BDR) is compared with the following monolingual document representation methods.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Effects of Bilingual Document Representation Method",
                "sec_num": null
            },
            {
                "text": "En Figure 5 shows the average accuracy curves of different document representation methods with different destruction fraction \u03bd. We vary \u03bd from 0 to 0.9 with an interval of 0.1.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 10,
                        "end": 11,
                        "text": "5",
                        "ref_id": "FIGREF5"
                    }
                ],
                "eq_spans": [],
                "section": "Effects of Bilingual Document Representation Method",
                "sec_num": null
            },
            {
                "text": "From Figure 5 we can see that En-En, Cn-Cn, and En-Cn get similar results. BDR performs constantly better than the other representation methods throughout the interval [0, 0.9]. The absolute superiority of BDR benefits from the enhanced ability of sentiment expression.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 12,
                        "end": 13,
                        "text": "5",
                        "ref_id": "FIGREF5"
                    }
                ],
                "eq_spans": [],
                "section": "Effects of Bilingual Document Representation Method",
                "sec_num": null
            },
            {
                "text": "Meanwhile, when the input x is partially de-stroyed (\u03bd varies from 0.1 to 0.9), the performance of En-En, Cn-Cn and En-Cn remains stable, which illustrates the robustness of the denoising autoencoder to corrupting noises. In addition, the average accuracies of BDR in the interval \u03bd \u2208 [0.1, 0.9] are all higher than the average accuracy under the condition \u03bd = 0 (78.23%). Therefore, adding noises properly to the training data could improve the performance of BSWE for CLSC.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Effects of Bilingual Document Representation Method",
                "sec_num": null
            },
            {
                "text": "Figure 6 shows the relationship between accuracies and dimension d of BSWE as well as that between accuracies and destruction fraction \u03bd in autoencoders in different categories. Dimension of embeddings d varies from 50 to 500, and destruction fraction \u03bd varies from 0.1 to 0.9. As shown in Figure 6 , the average accuracies generally move upward as dimension of BSWE increasing. Generally, the average accuracies keep higher than 80% with \u03bd varying from 0.1 to 0.5 as well as dimension varying from 300 to 500. When \u03bd = 0.1 and d = 400, the average accuracy reaches the peak value 80.68% (category accuracy of 81.05% in book, 81.60% in DVD, and 79.40% in music). The experimental results show that in BSWE learning process, increasing the dimension of embeddings or properly adding noises to the training data helps improve the performance of CLSC. In this paper, we only evaluate BSWE when dimension d varies from 50 to 500. However, there is still space for further improvement if d continues to increase.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "6",
                        "ref_id": "FIGREF6"
                    },
                    {
                        "start": 297,
                        "end": 298,
                        "text": "6",
                        "ref_id": "FIGREF6"
                    }
                ],
                "eq_spans": [],
                "section": "Influences of Dimension d and Destruction Fraction \u03bd",
                "sec_num": "4.3"
            },
            {
                "text": "Table 2 shows comparisons of the performance between our approach and some state-of-the-art systems on NLP&CC 2013 CLSC dataset. Our approach achieves the best performance with an 80.68% average accuracy. Compared with the recent related work, our approach is more effective and suitable for eliminating the language gap. Gui et al. (2013; 2014) and Zhou et al. (2014a) adopted the multi-view approach to bridge the language gap. Gui et al. (2013) proposed a mixed CLSC model by combining co-training and transfer learning strategies. They achieved the highest accuracy of 78.89% in NLP&CC CLSC share task. Gui et al. (2014) further improved the accuracy to 80.10% by removing noise from the transferred samples to avoid negative transfers. Zhou et al. (2014a) built denoising autoencoders in two independent views to enhance the robustness to translation errors in the inputs and achieved 80.02% accuracy. The multi-view approach learns language-specific classifiers in each view during training process, which is difficult to capture the common sentiment information of the two languages. Our approach integrates the bilingual embedding learning into a unified process, and outperforms Chen et al. (2014) , Gui et al. (2013) , Gui et al. (2014) and Zhou et al. (2014a) by 3.59%, 1.79%, 0.58%, and 0.66% respectively. The superiority of our approach benefits from the unified bilingual embedding learning process and the integration of semantic and sentiment information.",
                "cite_spans": [
                    {
                        "start": 322,
                        "end": 339,
                        "text": "Gui et al. (2013;",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 340,
                        "end": 345,
                        "text": "2014)",
                        "ref_id": null
                    },
                    {
                        "start": 430,
                        "end": 447,
                        "text": "Gui et al. (2013)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 607,
                        "end": 624,
                        "text": "Gui et al. (2014)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 741,
                        "end": 760,
                        "text": "Zhou et al. (2014a)",
                        "ref_id": null
                    },
                    {
                        "start": 1188,
                        "end": 1206,
                        "text": "Chen et al. (2014)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 1209,
                        "end": 1226,
                        "text": "Gui et al. (2013)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 1229,
                        "end": 1246,
                        "text": "Gui et al. (2014)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 1251,
                        "end": 1270,
                        "text": "Zhou et al. (2014a)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "2",
                        "ref_id": "TABREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Comparison with Related Work",
                "sec_num": "4.4"
            },
            {
                "text": "This paper proposes an approach to learning B-SWE by incorporating sentiment information into the bilingual embeddings for CLSC. The proposed approach learns BSWE with the labeled documents and their translations rather than parallel corpora. In addition, BDR is proposed to enhance the sentiment expression ability which combines English and Chinese representations. Experiments on the NLP&CC 2013 CLSC dataset show that our approach outperforms the previous stateof-the-art systems as well as traditional bilingual embedding systems. The proposed BSWE are only evaluated on English-Chinese CLSC in this paper, but it can be popularized to other languages. Both semantic and sentiment information play an important role in sentiment classification. In the following work, we will further investigate the relationship between semantic and sentiment information for CLSC, and balance their functions to optimize their combination for CLSC.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion and Future Work",
                "sec_num": "5"
            },
            {
                "text": "http://tcci.ccf.org.cn/conference/2013/dldoc/evsam03.zip",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "http://tcci.ccf.org.cn/conference/2013/dldoc/evdata03.zip",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "http://translate.google.cn/",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "http://www.datatang.com/data/45485",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "We wish to thank the anonymous reviewers for their valuable comments. This research is supported by National Natural Science Foundation of China (Grant No. 61272375).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgments",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Multilingual Subjectivity Analysis Using Machine Translation",
                "authors": [
                    {
                        "first": "Carmen",
                        "middle": [],
                        "last": "Banea",
                        "suffix": ""
                    },
                    {
                        "first": "Rada",
                        "middle": [],
                        "last": "Mihalcea",
                        "suffix": ""
                    },
                    {
                        "first": "Janyce",
                        "middle": [],
                        "last": "Wiebe",
                        "suffix": ""
                    },
                    {
                        "first": "Samer",
                        "middle": [],
                        "last": "Hassan",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "127--135",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Carmen Banea, Rada Mihalcea, Janyce Wiebe and Samer Hassan. 2008. Multilingual Subjectivity Analysis Using Machine Translation. In Proceed- ings of the 2008 Conference on Empirical Method- s in Natural Language Processing, pages 127-135. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "A Neural Probabilistic Language Model",
                "authors": [
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    },
                    {
                        "first": "R\u00e9jean",
                        "middle": [],
                        "last": "Ducharme",
                        "suffix": ""
                    },
                    {
                        "first": "Pascal",
                        "middle": [],
                        "last": "Vincent",
                        "suffix": ""
                    },
                    {
                        "first": "Christian",
                        "middle": [],
                        "last": "Jauvin",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "The Journal of Machine Learning Research",
                "volume": "3",
                "issue": "",
                "pages": "1137--1155",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yoshua Bengio, R\u00e9jean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A Neural Probabilistic Lan- guage Model. The Journal of Machine Learning Re- search, vol 3: 1137-1155.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Greedy layer-wise training of deep networks",
                "authors": [
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    },
                    {
                        "first": "Pascal",
                        "middle": [],
                        "last": "Lamblin",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Popovici",
                        "suffix": ""
                    },
                    {
                        "first": "Hugo",
                        "middle": [],
                        "last": "Larochelle",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Proceedings of Advances in Neural Information Processing Systems 19 (NIPS 06)",
                "volume": "",
                "issue": "",
                "pages": "153--160",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle. 2007. Greedy layer-wise train- ing of deep networks. In Proceedings of Advances in Neural Information Processing Systems 19 (NIPS 06), pages 153-160. MIT Press.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Representation learning: A review and new perspectives",
                "authors": [
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    },
                    {
                        "first": "Aaron",
                        "middle": [],
                        "last": "Courville",
                        "suffix": ""
                    },
                    {
                        "first": "Pascal",
                        "middle": [],
                        "last": "Vincent",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "volume": "35",
                "issue": "8",
                "pages": "1798--1828",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yoshua Bengio, Aaron Courville, and Pascal Vincent. 2013. Representation learning: A review and new perspectives. IEEE Transactions on Pattern Anal- ysis and Machine Intelligence 35(8): 1798-1828. IEEE.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Theano: a CPU and GPU math expression compiler",
                "authors": [
                    {
                        "first": "James",
                        "middle": [],
                        "last": "Bergstra",
                        "suffix": ""
                    },
                    {
                        "first": "Olivier",
                        "middle": [],
                        "last": "Breuleux",
                        "suffix": ""
                    },
                    {
                        "first": "Frederic",
                        "middle": [],
                        "last": "Bastien",
                        "suffix": ""
                    },
                    {
                        "first": "Pascal",
                        "middle": [],
                        "last": "Lamblin",
                        "suffix": ""
                    },
                    {
                        "first": "Razvan",
                        "middle": [],
                        "last": "Pascanu",
                        "suffix": ""
                    },
                    {
                        "first": "Guillaume",
                        "middle": [],
                        "last": "Desjardins",
                        "suffix": ""
                    },
                    {
                        "first": "Joseph",
                        "middle": [],
                        "last": "Turian",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the Python for scientific computing conference",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "James Bergstra, Olivier Breuleux, Frederic Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Des- jardins, Joseph Turian, Yoshua Bengio. 2010. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for scientific comput- ing conference (SciPy).",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Sentiment classification based on supervised latent n-gram analysis",
                "authors": [
                    {
                        "first": "Dmitriy",
                        "middle": [],
                        "last": "Bespalov",
                        "suffix": ""
                    },
                    {
                        "first": "Bing",
                        "middle": [],
                        "last": "Bai",
                        "suffix": ""
                    },
                    {
                        "first": "Yanjun",
                        "middle": [],
                        "last": "Qi",
                        "suffix": ""
                    },
                    {
                        "first": "Ali",
                        "middle": [],
                        "last": "Shokoufandeh",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Proceedings of the Conference on Information and Knowledge Management",
                "volume": "",
                "issue": "",
                "pages": "375--382",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dmitriy Bespalov, Bing Bai, Yanjun Qi, and Ali Shok- oufandeh. 2011. Sentiment classification based on supervised latent n-gram analysis. In Proceedings of the Conference on Information and Knowledge Management, pages 375-382. ACM.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Multilingual deep learning",
                "authors": [
                    {
                        "first": "Sarath",
                        "middle": [],
                        "last": "Chandar",
                        "suffix": ""
                    },
                    {
                        "first": "A P",
                        "middle": [],
                        "last": "Mitesh",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Khapra",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Balaraman Ravindran, Vikas Raykar and Amrita Saha",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sarath Chandar A P, Mitesh M. Khapra, Balara- man Ravindran, Vikas Raykar and Amrita Saha. 2013. Multilingual deep learning. In Deep Learning Workshop at NIPS 2013.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "An autoencoder approach to learning bilingual word representations",
                "authors": [
                    {
                        "first": "Sarath",
                        "middle": [],
                        "last": "Chandar",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [
                            "P"
                        ],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Stanislas",
                        "middle": [],
                        "last": "Lauly",
                        "suffix": ""
                    },
                    {
                        "first": "Hugo",
                        "middle": [],
                        "last": "Larochelle",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Mitesh",
                        "suffix": ""
                    },
                    {
                        "first": "Balaraman",
                        "middle": [],
                        "last": "Khapra",
                        "suffix": ""
                    },
                    {
                        "first": "Vikas",
                        "middle": [],
                        "last": "Ravindran",
                        "suffix": ""
                    },
                    {
                        "first": "Amrita",
                        "middle": [],
                        "last": "Raykar",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Saha",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "1853--1861",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sarath Chandar A P, Stanislas Lauly, Hugo Larochelle, Mitesh M Khapra, Balaraman Ravindran, Vikas Raykar, and Amrita Saha. 2014. An autoencoder approach to learning bilingual word representations. In Advances in Neural Information Processing Systems, pages 1853-1861.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Cross-Language Sentiment Analysis Based on Parser (in Chinese)",
                "authors": [
                    {
                        "first": "Qiang",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Yanxiang",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Xule",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Songtao",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Min",
                        "middle": [],
                        "last": "Peng",
                        "suffix": ""
                    },
                    {
                        "first": "Fei",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Acta Scientiarum Naturalium Universitatis Pekinensis",
                "volume": "50",
                "issue": "1",
                "pages": "55--60",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Qiang Chen, Yanxiang He, Xule Liu, Songtao Sun, Min Peng, and Fei Li. 2014. Cross-Language Sen- timent Analysis Based on Parser (in Chinese). Acta Scientiarum Naturalium Universitatis Pekinensis, 50 (1): 55-60.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Reducing the Dimensionality of Data with Neural Networks",
                "authors": [
                    {
                        "first": "G",
                        "middle": [
                            "E"
                        ],
                        "last": "Hinton",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [
                            "R"
                        ],
                        "last": "Salakhutdinov",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Science",
                "volume": "313",
                "issue": "",
                "pages": "504--507",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "G. E. Hinton and R. R. Salakhutdinov. 2006. Reducing the Dimensionality of Data with Neural Networks. Science, vol 313: 504-507.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Feature Selection and Negative Evidence in Automated Text Categorization",
                "authors": [
                    {
                        "first": "Luigi",
                        "middle": [],
                        "last": "Galavotti",
                        "suffix": ""
                    },
                    {
                        "first": "Fabrizio",
                        "middle": [],
                        "last": "Sebastiani",
                        "suffix": ""
                    },
                    {
                        "first": "Maria",
                        "middle": [],
                        "last": "Simi",
                        "suffix": ""
                    }
                ],
                "year": 2000,
                "venue": "Proceedings of ECDL-00, 4th European Conference on Research and Advanced Technology for Digital Libraries",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Luigi Galavotti, Fabrizio Sebastiani, and Maria Sim- i. 2000. Feature Selection and Negative Evidence in Automated Text Categorization. In Proceedings of ECDL-00, 4th European Conference on Research and Advanced Technology for Digital Libraries.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Domain adaptation for large-scale sentiment classification: A deep learning approach",
                "authors": [
                    {
                        "first": "Xavier",
                        "middle": [],
                        "last": "Glorot",
                        "suffix": ""
                    },
                    {
                        "first": "Antoine",
                        "middle": [],
                        "last": "Bordes",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Proceedings of 28th International Conference on Machine Learning",
                "volume": "",
                "issue": "",
                "pages": "513--520",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xavier Glorot, Antoine Bordes, and Yoshua Bengio. 2011. Domain adaptation for large-scale sentiment classification: A deep learning approach. In Pro- ceedings of 28th International Conference on Ma- chine Learning, pages 513-520.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "A mixed model for cross lingual opinion analysis",
                "authors": [
                    {
                        "first": "Lin",
                        "middle": [],
                        "last": "Gui",
                        "suffix": ""
                    },
                    {
                        "first": "Ruifeng",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Jun",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Li",
                        "middle": [],
                        "last": "Yuan",
                        "suffix": ""
                    },
                    {
                        "first": "Yuanlin",
                        "middle": [],
                        "last": "Yao",
                        "suffix": ""
                    },
                    {
                        "first": "Jiyun",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Qiaoyun",
                        "middle": [],
                        "last": "Qiu",
                        "suffix": ""
                    },
                    {
                        "first": "Shuwei",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Kam-Fai",
                        "middle": [],
                        "last": "Wong",
                        "suffix": ""
                    },
                    {
                        "first": "Ricky",
                        "middle": [],
                        "last": "Cheung",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of Natural Language Processing and Chinese Computing",
                "volume": "",
                "issue": "",
                "pages": "93--104",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Lin Gui, Ruifeng Xu, Jun Xu, Li Yuan, Yuanlin Yao, Jiyun Zhou, Qiaoyun Qiu, Shuwei Wang, Kam- Fai Wong, and Ricky Cheung. 2013. A mixed mod- el for cross lingual opinion analysis. In Proceedings of Natural Language Processing and Chinese Com- puting, pages 93-104. Springer Verlag.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Cross-lingual Opinion Analysis via Negative Transfer Detection",
                "authors": [
                    {
                        "first": "Lin",
                        "middle": [],
                        "last": "Gui",
                        "suffix": ""
                    },
                    {
                        "first": "Ruifeng",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Qin",
                        "middle": [],
                        "last": "Lu",
                        "suffix": ""
                    },
                    {
                        "first": "Jun",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Jian",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Bin",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaolong",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers)",
                "volume": "",
                "issue": "",
                "pages": "860--865",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Lin Gui, Ruifeng Xu, Qin Lu, Jun Xu, Jian Xu, Bin Liu, and Xiaolong Wang. 2014. Cross-lingual Opinion Analysis via Negative Transfer Detection. In Pro- ceedings of the 52nd Annual Meeting of the Associ- ation for Computational Linguistics (Short Papers), pages 860-865. Association for Computational Lin- guistics.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Making large-Scale SVM Learning Practical",
                "authors": [
                    {
                        "first": "Thorsten",
                        "middle": [],
                        "last": "Joachims",
                        "suffix": ""
                    }
                ],
                "year": 1999,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Thorsten Joachims. 1999. Making large-Scale SVM Learning Practical. Universit\u00e4t Dortmund.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Sentiment classification of movie reviews using contextual valence shifters",
                "authors": [
                    {
                        "first": "Alistair",
                        "middle": [],
                        "last": "Kennedy",
                        "suffix": ""
                    },
                    {
                        "first": "Diana",
                        "middle": [],
                        "last": "Inkpen",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Computational intelligence",
                "volume": "22",
                "issue": "2",
                "pages": "110--125",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alistair Kennedy and Diana Inkpen. 2006. Sentiment classification of movie reviews using contextual va- lence shifters. Computational intelligence, 22(2): 110-125.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Active learning for cross-lingual sentiment classification",
                "authors": [
                    {
                        "first": "Shoushan",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Rong",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Huanhuan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Chu-Ren",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of Natural Language Processing and Chinese Computing",
                "volume": "",
                "issue": "",
                "pages": "236--246",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Shoushan Li, Rong Wang, Huanhuan Liu, and Chu- Ren Huang. 2013. Active learning for cross-lingual sentiment classification. In Proceedings of Natu- ral Language Processing and Chinese Computing, pages 236-246. Springer Verlag.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Learning Word Vectors for Sentiment Analysis",
                "authors": [
                    {
                        "first": "Andrew",
                        "middle": [
                            "L"
                        ],
                        "last": "Maas",
                        "suffix": ""
                    },
                    {
                        "first": "Raymond",
                        "middle": [
                            "E"
                        ],
                        "last": "Daly",
                        "suffix": ""
                    },
                    {
                        "first": "Peter",
                        "middle": [
                            "T"
                        ],
                        "last": "Pham",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [
                            "Y"
                        ],
                        "last": "Ng",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [],
                        "last": "Potts",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "142--150",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning Word Vectors for Sentiment Anal- ysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 142-150. Association for Computational Lin- guistics.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Linguistic regularities in continuous space word representations",
                "authors": [
                    {
                        "first": "Tomas",
                        "middle": [],
                        "last": "Mikolov",
                        "suffix": ""
                    },
                    {
                        "first": "Wen-Tau",
                        "middle": [],
                        "last": "Yih",
                        "suffix": ""
                    },
                    {
                        "first": "Geoffrey",
                        "middle": [],
                        "last": "Zweig",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of NAACL-HLT",
                "volume": "",
                "issue": "",
                "pages": "746--751",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013. Linguistic regularities in continuous space word representations. In Proceedings of NAACL- HLT, pages 746-751. Association for Computation- al Linguistics.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Thumbs up? Sentiment classification using machine learning techniques",
                "authors": [
                    {
                        "first": "Bo",
                        "middle": [],
                        "last": "Pang",
                        "suffix": ""
                    },
                    {
                        "first": "Lillian",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Shivakumar",
                        "middle": [],
                        "last": "Vaithyanathan",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "79--86",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Bo Pang, Lillian Lee and Shivakumar Vaithyanathan. 2002. Thumbs up? Sentiment classification using machine learning techniques. In Proceedings of the Conference on Empirical Methods in Natural Lan- guage Processing, pages 79-86. ACM.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "A sentimental education: sentiment analysis using subjectivity summarization based on minimum cuts",
                "authors": [
                    {
                        "first": "Bo",
                        "middle": [],
                        "last": "Pang",
                        "suffix": ""
                    },
                    {
                        "first": "Lillian",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "271--278",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Bo Pang and Lillian Lee. 2004. A sentimental educa- tion: sentiment analysis using subjectivity summa- rization based on minimum cuts. In Proceedings of the 42nd Annual Meeting on Association for Com- putational Linguistics, pages 271-278. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Parsing natural scenes and natural language with recursive neural networks",
                "authors": [
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    },
                    {
                        "first": "Cliff",
                        "middle": [],
                        "last": "Chiung-Yu Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [
                            "Y"
                        ],
                        "last": "Ng",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Proceedings of the International Conference on Machine Learning",
                "volume": "",
                "issue": "",
                "pages": "129--136",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Richard Socher, Cliff Chiung-Yu Lin, Andrew Y. Ng, and Christopher D. Manning. 2011. Parsing natu- ral scenes and natural language with recursive neu- ral networks. In Proceedings of the International Conference on Machine Learning, pages 129-136. Bellevue.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Semantic Compositionality through Recursive Matrix-Vector Spaces",
                "authors": [
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    },
                    {
                        "first": "Brody",
                        "middle": [],
                        "last": "Huval",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [
                            "Y"
                        ],
                        "last": "Ng",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "1201--1211",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Richard Socher, Brody Huval, Christopher D. Man- ning, and Andrew Y. Ng. 2012. Semantic Com- positionality through Recursive Matrix-Vector S- paces. In Proceedings of the Conference on Empiri- cal Methods in Natural Language Processing, pages 1201-1211. Association for Computational Linguis- tics.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Learning Sentiment-Specific Word Embedding for Twitter Sentiment Classification",
                "authors": [
                    {
                        "first": "Duyu",
                        "middle": [],
                        "last": "Tang",
                        "suffix": ""
                    },
                    {
                        "first": "Furu",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Nan",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Ming",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Ting",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Bing",
                        "middle": [],
                        "last": "Qin",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistic",
                "volume": "",
                "issue": "",
                "pages": "1555--1565",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, T- ing Liu, and Bing Qin. 2014. Learning Sentiment- Specific Word Embedding for Twitter Sentimen- t Classification. In Proceedings of the 52nd Annual Meeting of the Association for Computational Lin- guistic, pages 1555-1565. Association for Compu- tational Linguistics.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Learning Bilingual Embedding Model for Cross-language Sentiment Classification",
                "authors": [
                    {
                        "first": "Xuewei",
                        "middle": [],
                        "last": "Tang",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaojun",
                        "middle": [],
                        "last": "Wan",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of 2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)",
                "volume": "",
                "issue": "",
                "pages": "134--141",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xuewei Tang and Xiaojun Wan. 2014. Learn- ing Bilingual Embedding Model for Cross-language Sentiment Classification. In Proceedings of 2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Tech- nologies (IAT), pages 134-141. IEEE.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Extracting and composing robust features with denoising autoencoders",
                "authors": [
                    {
                        "first": "Pascal",
                        "middle": [],
                        "last": "Vincent",
                        "suffix": ""
                    },
                    {
                        "first": "Hugo",
                        "middle": [],
                        "last": "Larochelle",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    },
                    {
                        "first": "Pierre-Antoine",
                        "middle": [],
                        "last": "Manzagol",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Proceedings of the 25th international conference on Machine learning",
                "volume": "",
                "issue": "",
                "pages": "1096--1103",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. 2008. Extracting and composing robust features with denoising autoen- coders. In Proceedings of the 25th international conference on Machine learning, pages 1096-1103. ACM.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Co-training for cross-lingual sentiment classification",
                "authors": [
                    {
                        "first": "Xiaojun",
                        "middle": [],
                        "last": "Wan",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP",
                "volume": "",
                "issue": "",
                "pages": "235--243",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xiaojun Wan. 2009. Co-training for cross-lingual sen- timent classification. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natu- ral Language Processing of the AFNLP, pages 235- 243. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Word Vector Modeling for Sentiment Analysis of Product Reviews",
                "authors": [
                    {
                        "first": "Yuan",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Zhaohui",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Jie",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Zhicheng",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Yalou",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Dong",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of Natural Language Processing and Chinese Computing",
                "volume": "",
                "issue": "",
                "pages": "168--180",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yuan Wang, Zhaohui Li, Jie Liu, Zhicheng He, Yalou Huang, and Dong Li. 2014. Word Vec- tor Modeling for Sentiment Analysis of Product Re- views. In Proceedings of Natural Language Pro- cessing and Chinese Computing, pages 168-180. Springer Verlag.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "HHMM-based Chinese Lexical Analyzer ICTCLAS",
                "authors": [
                    {
                        "first": "Huaping",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Hongkui",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Deyi",
                        "middle": [],
                        "last": "Xiong",
                        "suffix": ""
                    },
                    {
                        "first": "Qun",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "2nd SIGHAN workshop affiliated with 41th ACL",
                "volume": "",
                "issue": "",
                "pages": "184--187",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Huaping Zhang, Hongkui Yu, Deyi Xiong, and Qun Li- u. 2003. HHMM-based Chinese Lexical Analyzer ICTCLAS. In 2nd SIGHAN workshop affiliated with 41th ACL, pages 184-187. Association for Compu- tational Linguistics.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Bridging the Language Gap: Learning Distributed Semantics for Cross-Lingual Sentiment Classification",
                "authors": [
                    {
                        "first": "Guangyou",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Tingting",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Jun",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of Natural Language Processing and Chinese Computing",
                "volume": "",
                "issue": "",
                "pages": "138--149",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Guangyou Zhou, Tingting He, and Jun Zhao. 2014b. Bridging the Language Gap: Learning Distribut- ed Semantics for Cross-Lingual Sentiment Classifi- cation. In Proceedings of Natural Language Pro- cessing and Chinese Computing, pages 138-149. Springer Verlag.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "2014a. Cross-lingual sentiment classification based on denoising autoencoder",
                "authors": [
                    {
                        "first": "Huiwei",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Long",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Degen",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "Proceedings of Natural Language Processing and Chinese Computing",
                "volume": "",
                "issue": "",
                "pages": "181--192",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Huiwei Zhou, Long Chen, and Degen Huang. 2014a. Cross-lingual sentiment classification based on de- noising autoencoder. In Proceedings of Natu- ral Language Processing and Chinese Computing, pages 181-192. Springer Verlag.",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "Bilingual Word Embedding for Phrase-Based Machine Translation",
                "authors": [
                    {
                        "first": "Will",
                        "middle": [
                            "Y"
                        ],
                        "last": "Zou",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Cer",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "1393--1398",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Will Y. Zou, Richard Socher, Daniel Cer, and Christo- pher D. Manning. 2013. Bilingual Word Embed- ding for Phrase-Based Machine Translation. In Pro- ceedings of the 2013 Conference on Empirical Meth- ods in Natural Language Processing, pages 1393- 1398.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 1: The framework of bilingual embedding learning.",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Figure 2: The supervised learning process.",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "The sentiment label s is a Boolean value representing sentiment polarity of a document: s = 0 represents negative polarity and s = 1 represents positive polarity. Parameter \u03be * = {[W E , W C ] * , b * , \u03d5 * , b * l } is learned by maximizing the objective function according to the sentiment polarity label s i of document d i :",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF3": {
                "num": null,
                "text": "Figure 3: Our unsupervised bilingual embedding learning method vs. Parallel corpora based method.",
                "uris": null,
                "fig_num": "3",
                "type_str": "figure"
            },
            "FIGREF4": {
                "num": null,
                "text": "Figure 4: Performance comparison of the bilingual embeddings and BSWE.",
                "uris": null,
                "fig_num": "4",
                "type_str": "figure"
            },
            "FIGREF5": {
                "num": null,
                "text": "Figure 5: Effects of bilingual document representation method (BDR).",
                "uris": null,
                "fig_num": "5",
                "type_str": "figure"
            },
            "FIGREF6": {
                "num": null,
                "text": "Figure 6: The relationship between accuracies and dimension d as well as that between accuracies and destruction fraction \u03bd.",
                "uris": null,
                "fig_num": "6",
                "type_str": "figure"
            },
            "TABREF1": {
                "content": "<table><tr><td>Category</td><td>book</td><td>DVD</td><td>music</td><td>Average</td></tr><tr><td>Boolean</td><td colspan=\"4\">76.22% 74.30% 74.75% 75.09%</td></tr><tr><td>TF-IDF</td><td colspan=\"4\">76.65% 77.60% 74.50% 76.25%</td></tr></table>",
                "type_str": "table",
                "text": "The classification accuracy with the Boolean and TF-IDF methods.",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table><tr><td/><td>0.8</td><td/><td/><td/><td/></tr><tr><td/><td>0.79</td><td/><td/><td/><td/></tr><tr><td/><td>0.78</td><td/><td/><td/><td/><td>En-En Cn-Cn</td></tr><tr><td/><td/><td/><td/><td/><td/><td>En-Cn</td></tr><tr><td>Average</td><td>0.76 0.77</td><td/><td/><td/><td/><td>BDR</td></tr><tr><td/><td>0.75</td><td/><td/><td/><td/></tr><tr><td/><td>0.74</td><td/><td/><td/><td/></tr><tr><td/><td>0.73 0</td><td>0.2</td><td>0.4</td><td>\u03bd</td><td>0.6</td><td>0.8</td></tr></table>",
                "type_str": "table",
                "text": "-En: This method represents training and test documents in English only with W E . English training documents and Chinese-to-English translations of test documents are both represented with W E . Cn-Cn: This method represents training and test documents in Chinese only with W C . English-to-Chinese translations of training documents and Chinese test documents are both represented with W C . En-Cn: This method represents English training documents with W E , while represents Chinese test documents with W C . Chandar A P et al. (2014) employed this method in their work. BDR: This method adopts our bilingual document representation method, which represents training and test documents with both W E and W C .",
                "html": null,
                "num": null
            },
            "TABREF4": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Performance comparisons on the NLP&CC 2013 CLSC dataset.",
                "html": null,
                "num": null
            }
        }
    }
}