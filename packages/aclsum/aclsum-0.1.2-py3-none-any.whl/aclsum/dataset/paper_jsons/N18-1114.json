{
    "paper_id": "N18-1114",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T11:57:24.127135Z"
    },
    "title": "Tensor Product Generation Networks for Deep NLP Modeling",
    "authors": [
        {
            "first": "Qiuyuan",
            "middle": [],
            "last": "Huang",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Microsoft Research",
                "location": {
                    "region": "WA",
                    "country": "USA"
                }
            },
            "email": ""
        },
        {
            "first": "Paul",
            "middle": [],
            "last": "Smolensky",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Microsoft Research",
                "location": {
                    "region": "WA",
                    "country": "USA"
                }
            },
            "email": ""
        },
        {
            "first": "Xiaodong",
            "middle": [],
            "last": "He",
            "suffix": "",
            "affiliation": {},
            "email": "xiaodong.he@jd.com"
        },
        {
            "first": "Li",
            "middle": [],
            "last": "Deng",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Microsoft Research",
                "location": {
                    "region": "WA",
                    "country": "USA"
                }
            },
            "email": "l.deng@ieee.org"
        },
        {
            "first": "Dapeng",
            "middle": [],
            "last": "Wu",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Florida",
                "location": {
                    "region": "FL",
                    "country": "USA"
                }
            },
            "email": "dpwu@ufl.edu"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "We present a new approach to the design of deep networks for natural language processing (NLP), based on the general technique of Tensor Product Representations (TPRs) for encoding and processing symbol structures in distributed neural networks. A network architecture -the Tensor Product Generation Network (TPGN) -is proposed which is capable in principle of carrying out TPR computation, but which uses unconstrained deep learning to design its internal representations. Instantiated in a model for image-caption generation, TPGN outperforms LSTM baselines when evaluated on the COCO dataset. The TPR-capable structure enables interpretation of internal representations and operations, which prove to contain considerable grammatical content. Our caption-generation model can be interpreted as generating sequences of grammatical categories and retrieving words by their categories from a plan encoded as a distributed representation.",
    "pdf_parse": {
        "paper_id": "N18-1114",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "We present a new approach to the design of deep networks for natural language processing (NLP), based on the general technique of Tensor Product Representations (TPRs) for encoding and processing symbol structures in distributed neural networks. A network architecture -the Tensor Product Generation Network (TPGN) -is proposed which is capable in principle of carrying out TPR computation, but which uses unconstrained deep learning to design its internal representations. Instantiated in a model for image-caption generation, TPGN outperforms LSTM baselines when evaluated on the COCO dataset. The TPR-capable structure enables interpretation of internal representations and operations, which prove to contain considerable grammatical content. Our caption-generation model can be interpreted as generating sequences of grammatical categories and retrieving words by their categories from a plan encoded as a distributed representation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "In this paper we introduce a new architecture for natural language processing (NLP). On what type of principles can a computational architecture be founded? It would seem a sound principle to require that the hypothesis space for learning which an architecture provides include network hypotheses that are independently known to be suitable for performing the target task. Our proposed architecture makes available to deep learning network configurations that perform natural language generation by use of Tensor Product Representations (TPRs) (Smolensky and Legendre, 2006) . Whether learning will create TPRs is unknown in advance, but what we can say with certainty is that the hypothesis space being searched during learn-ing includes TPRs as one appropriate solution to the problem.",
                "cite_spans": [
                    {
                        "start": 544,
                        "end": 574,
                        "text": "(Smolensky and Legendre, 2006)",
                        "ref_id": "BIBREF31"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "TPRs are a general method for generating vector-space embeddings of complex symbol structures. Prior work has proved that TPRs enable powerful symbol processing to be carried out using neural network computation (Smolensky, 2012) . This includes generating parse trees that conform to a grammar (Cho et al., 2017) , although incorporating such capabilities into deep learning networks such as those developed here remains for future work. The architecture presented here relies on simpler use of TPRs to generate sentences; grammars are not explicitly encoded here.",
                "cite_spans": [
                    {
                        "start": 212,
                        "end": 229,
                        "text": "(Smolensky, 2012)",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 295,
                        "end": 313,
                        "text": "(Cho et al., 2017)",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "We test the proposed architecture by applying it to image-caption generation (on the MS-COCO dataset, (COCO, 2017) ). The results improve upon a baseline deploying a state-of-the-art LSTM architecture (Vinyals et al., 2015) , and the TPR foundations of the architecture provide greater interpretability.",
                "cite_spans": [
                    {
                        "start": 102,
                        "end": 114,
                        "text": "(COCO, 2017)",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 201,
                        "end": 223,
                        "text": "(Vinyals et al., 2015)",
                        "ref_id": "BIBREF35"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Section 2 of the paper reviews TPR. Section 3 presents the proposed architecture, the Tensor Product Generation Network (TPGN). Section 4 describes the particular model we study for image captioning, and Section 5 presents the experimental results. Importantly, what the model has learned is interpreted in Section 5.3. Section 6 discusses the relation of the new model to previous work and Section 7 concludes.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The central idea of TPRs (Smolensky, 1990 ) can be appreciated by contrasting the TPR for a word string with a bag-of-words (BoW) vector-space embedding. In a BoW embedding, the vector that encodes Jay saw Kay is the same as the one that encodes Kay saw Jay: J + K + s where J, K, s are respectively the vector embeddings of the words Jay, Kay, saw.",
                "cite_spans": [
                    {
                        "start": 25,
                        "end": 41,
                        "text": "(Smolensky, 1990",
                        "ref_id": "BIBREF29"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Review of tensor product representation",
                "sec_num": "2"
            },
            {
                "text": "A TPR embedding that avoids this confusion starts by analyzing Jay saw Kay as the set {Jay/SUBJ, Kay/OBJ, saw/VERB}. (Other analyses are possible: see Section 3.) Next we choose an embedding in a vector space V F for Jay, Kay, saw as in the BoW case: J, K, s. Then comes the step unique to TPRs: we choose an embedding in a vector space V R for the roles SUBJ, OBJ, VERB: r SUBJ , r OBJ , r VERB . Crucially, r SUBJ = r OBJ . Finally, the TPR for Jay saw Kay is the following vector in",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Review of tensor product representation",
                "sec_num": "2"
            },
            {
                "text": "V F \u2297 V R : v Jay saw Kay = J \u2297 r SUBJ + K \u2297 r OBJ + s \u2297 r VERB",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Review of tensor product representation",
                "sec_num": "2"
            },
            {
                "text": "(1) Each word is tagged with the role it fills in the sentence; Jay and Kay fill different roles.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Review of tensor product representation",
                "sec_num": "2"
            },
            {
                "text": "This TPR avoids the BoW confusion:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Review of tensor product representation",
                "sec_num": "2"
            },
            {
                "text": "v Jay saw Kay = v Kay saw Jay because J \u2297 r SUBJ + K \u2297 r OBJ = J \u2297 r OBJ + K \u2297 r SUBJ .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Review of tensor product representation",
                "sec_num": "2"
            },
            {
                "text": "In the terminology of TPRs, in Jay saw Kay, Jay is the filler of the role SUBJ, and J \u2297 r SUBJ is the vector embedding of the filler/role binding Jay/SUBJ. In the vector space embedding, the binding operation is the tensor -or generalized outer -product \u2297; i.e., J \u2297 r SUBJ is a tensor with 2 indices defined by:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Review of tensor product representation",
                "sec_num": "2"
            },
            {
                "text": "[J \u2297 r SUBJ ] \u03d5\u03c1 \u2261 [J] \u03d5 [r SUBJ ] \u03c1 .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Review of tensor product representation",
                "sec_num": "2"
            },
            {
                "text": "The tensor product can be used recursively, which is essential for the TPR embedding of recursive structures such as trees and for the computation of recursive functions over TPRs. However, in the present context, recursion will not be required, in which case the tensor product can be regarded as simply the matrix outer product (which cannot be used recursively); we can regard J\u2297r SUBJ as the matrix product Jr SUBJ . Then Equation 1 becomes v Jay saw Kay = Jr SUBJ + Kr OBJ + sr VERB (2) Note that the set of matrices (or the set of tensors with any fixed number of indices) is a vector space; thus Jay saw Kay \u2192 v Jay saw Kay is a vector-space embedding of the symbol structures constituting sentences. Whether we regard v Jay saw Kay as a 2-index tensor or as a matrix, we can call it simply a 'vector' since it is an element of a vector space: in the context of TPRs, 'vector' is used in a general sense and should not be taken to imply a single-indexed array.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Review of tensor product representation",
                "sec_num": "2"
            },
            {
                "text": "Crucial to the computational power of TPRs and to the architecture we propose here is the notion of unbinding. Just as an outer product -the tensor product -can be used to bind the vector embedding a filler Jay to the vector embedding a role SUBJ, J \u2297 r SUBJ or Jr SUBJ , so an inner product can be used to take the vector embedding a structure and unbind a role contained within that structure, yielding the symbol that fills the role.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Review of tensor product representation",
                "sec_num": "2"
            },
            {
                "text": "In the simplest case of orthonormal role vectors r i , to unbind role SUBJ in Jay saw Kay we can compute the matrix-vector product: v Jay saw Kay r SUBJ = J (because r i r j = \u03b4 ij when the role vectors are orthonormal). A similar situation obtains when the role vectors are not orthonormal, provided they are not linearly dependent: for each role such as SUBJ there is an unbinding vector u SUBJ such that r i u j = \u03b4 ij so we get: v Jay saw Kay u SUBJ = J. A role vector such as r SUBJ and its unbinding vector u SUBJ are said to be duals of each other. (If R is the matrix in which each column is a role vector r j , then R is invertible when the role vectors are linearly independent; then the unbinding vectors u i are the rows of R -1 . When the r j are orthonormal, u i = r i . Replacing the matrix inverse with the pseudo-inverse allows approximate unbinding if the role vectors are linearly dependent.)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Review of tensor product representation",
                "sec_num": "2"
            },
            {
                "text": "We can now see how TPRs can be used to generate a sentence one word at a time. We start with the TPR for the sentence, e.g., v Jay saw Kay . From this vector we unbind the role of the first word, which is SUBJ: the embedding of the first word is thus v Jay saw Kay u SUBJ = J, the embedding of Jay. Next we take the TPR for the sentence and unbind the role of the second word, which is VERB: the embedding of the second word is then v Jay saw Kay u VERB = s, the embedding of saw. And so on.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Review of tensor product representation",
                "sec_num": "2"
            },
            {
                "text": "To accomplish this, we need two representations to generate the t th word: (i) the TPR of the sentence, S (or of the string of not-yet-produced words, S t ) and (ii) the unbinding vector for the t th word, u t . The architecture we propose will therefore be a recurrent network containing two subnetworks: (i) a subnet S hosting the representation S t , and a (ii) a subnet U hosting the unbinding vector u t . This is shown in Fig. 1 .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 433,
                        "end": 434,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Review of tensor product representation",
                "sec_num": "2"
            },
            {
                "text": "As Fig. 1 shows, the proposed Tensor Product Generation Network architecture (the dashed box labeled N ) is designed to support the technique for generation just described: the architecture is TPR-capable. There is a sentence-encoding subnetwork S which could host a TPR of the sentence to be generated, and an unbinding subnetwork U which could output a sequence of unbinding vectors u t ; at time t, the embedding f t of the word produced, x t , could then be extracted from S t via the matrix-vector product (shown in the figure by \"2 \u00d7\"): f t = S t u t . The lexical-decoding subnetwork L converts the embedding vector f t to the 1-hot vector x t corresponding to the word x t . Unlike some other work (Palangi et al., 2017) , TPGN is not constrained to literally learn TPRs. The representations that will actually be housed in S and U are determined by end-to-end deep learning on a task: the bubbles in Fig. 1 show what would be the meanings of S t , u t and f t if an actual TPR scheme were instantiated in the architecture. The learned representations S t will not be proven to literally be TPRs, but by analyzing the unbinding vectors u t the network learns, we will gain insight into the process by which the learned matrices S t give rise to the generated sentence.",
                "cite_spans": [
                    {
                        "start": 706,
                        "end": 728,
                        "text": "(Palangi et al., 2017)",
                        "ref_id": "BIBREF24"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 8,
                        "end": 9,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    },
                    {
                        "start": 914,
                        "end": 915,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "A TPR-capable generation architecture",
                "sec_num": "3"
            },
            {
                "text": "The task studied here is image captioning; Fig. 1 shows that the input to this TPGN model is an image, preprocessed by a CNN which produces the initial representation in S, S 0 . This vector S 0 drives the entire caption-generation process: it contains all the image-specific information for producing the caption. (We will call a caption a \"sentence\" even though it may in fact be just a noun phrase.)",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 48,
                        "end": 49,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "A TPR-capable generation architecture",
                "sec_num": "3"
            },
            {
                "text": "The two subnets S and U are mutuallyconnected LSTMs (Hochreiter and Schmidhuber, 1997) : see Fig. 2 . The internal hidden state of U, p t , is sent as input to S; U also produces output, the unbinding vector u t . The internal hidden state of S, S t , is sent as input to U, and also produced as output. As stated above, these two outputs are multiplied together to produce the embedding vector f t = S t u t of the output word x t . Furthermore, the 1-hot encoding x t of x t is fed back at the next time step to serve as input to both S and U.",
                "cite_spans": [
                    {
                        "start": 52,
                        "end": 86,
                        "text": "(Hochreiter and Schmidhuber, 1997)",
                        "ref_id": "BIBREF12"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 98,
                        "end": 99,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "A TPR-capable generation architecture",
                "sec_num": "3"
            },
            {
                "text": "What type of roles might the unbinding vectors be unbinding? A TPR for a caption could in principle be built upon positional roles, syntactic/semantic roles, or some combination of the two. In the caption a man standing in a room with a suitcase, the initial a and man might respectively occupy the positional roles of POS(ITION) 1 and POS 2 ; standing might occupy the syntactic role of VERB; in the role of SPATIAL-P(REPOSITION); while a room with a suitcase might fill a 5-role schema DET(ERMINER) 1 N(OUN) 1 P DET 2 N 2 . In fact we will provide evidence in Sec. 5.3.2 that our network learns just this kind of hybrid role decomposition; further evidence for these particular roles is presented elsewhere.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A TPR-capable generation architecture",
                "sec_num": "3"
            },
            {
                "text": "What form of information does the sentenceencoding subnetwork S need to encode in S? Continuing with the example of the previous paragraph, S needs to be some approximation to the TPR summing several filler/role binding matrices. In one of these bindings, a filler vector f a -which the lexical subnetwork L will map to the article a -is bound (via the outer product) to a role vector r POS 1 which is the dual of the first unbinding vector produced by the unbinding subnetwork U: u POS 1 . In the first iteration of generation the model computes S 1 u POS 1 = f a , which L then maps to a. Analogously, another binding approximately contained in S 2 is f man r POS 2 . There are corresponding approximate bindings for the remaining words of the caption; these employ syntactic/semantic roles. One example is f standing r V . At iteration 3, U decides the next word should be a verb, so it generates the unbinding vector u V which when multiplied by the current output of S, the matrix S 3 , yields a filler vector f standing which L maps to the output standing. S decided the caption should deploy standing as a verb and included in S an approximation to the binding f standing r V . It similarly decided the caption should deploy in as a spatial preposition, approximately including in S the binding f in r SPATIAL-P ; and so on for the other words in their respective roles in the caption.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A TPR-capable generation architecture",
                "sec_num": "3"
            },
            {
                "text": "As stated above, the unbinding subnetwork U and the sentence-encoding subnetwork S of Fig. 1 are each implemented as (1-layer, 1-directional) LSTMs (see Fig. 2 ); the lexical subnetwork L is implemented as a linear transformation followed by a softmax operation.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 91,
                        "end": 92,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    },
                    {
                        "start": 158,
                        "end": 159,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "System Description",
                "sec_num": "4"
            },
            {
                "text": "In the equations below, the LSTM variables internal to the S subnet are indexed by 1 (e.g., the forget-, input-, and output-gates are respectively f1 , \u00ee1 , \u00f41 ) while those of the unbinding subnet U are indexed by 2.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "System Description",
                "sec_num": "4"
            },
            {
                "text": "Thus the state updating equations for S are, for t = 1, \u2022 \u2022 \u2022 , T = caption length:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "System Description",
                "sec_num": "4"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "f1,t = \u03c3g(W 1,f pt-1 -D 1,f Wext-1 + U 1,f \u015ct-1) (3) \u00ee1,t = \u03c3g(W1,ipt-1 -D1,iWext-1 + U1,i \u015ct-1) (4) \u00f41,t = \u03c3g(W1,opt-1 -D1,oWext-1 + U1,o \u015ct-1) (5) g1,t = \u03c3 h (W1,cpt-1 -D1,cWext-1 + U1,c \u015ct-1) (6) c1,t = f1,t c1,t-1 + \u00ee1,t g1,t",
                        "eq_num": "(7)"
                    }
                ],
                "section": "System Description",
                "sec_num": "4"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "\u015ct = \u00f41,t \u03c3 h (c1,t)",
                        "eq_num": "(8)"
                    }
                ],
                "section": "System Description",
                "sec_num": "4"
            },
            {
                "text": "Here f1,t , \u00ee1,t , \u00f41,t ,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "System Description",
                "sec_num": "4"
            },
            {
                "text": "g 1,t , c 1,t , \u015ct \u2208 R d\u00d7d , p t \u2208 R d ; \u03c3 g (\u2022)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "System Description",
                "sec_num": "4"
            },
            {
                "text": "is the (element-wise) logistic sigmoid function; \u03c3 h (\u2022) is the hyperbolic tangent function; the operator denotes the Hadamard (element-wise) product; d\u00d7d) . For clarity, biases -included throughout the model -are omitted from all equations in this paper. The initial state \u015c0 is initialized by:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "System Description",
                "sec_num": "4"
            },
            {
                "text": "W 1,f , W 1,i , W 1,o , W 1,c \u2208 R (d\u00d7d)\u00d7d , D 1,f , D 1,i , D 1,o , D 1,c \u2208 R (d\u00d7d)\u00d7d , U 1,f , U 1,i , U 1,o , U 1,c \u2208 R (d\u00d7d)\u00d7(",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "System Description",
                "sec_num": "4"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "\u015c0 = C s (v -v)",
                        "eq_num": "(9)"
                    }
                ],
                "section": "System Description",
                "sec_num": "4"
            },
            {
                "text": "where v \u2208 R 2048 is the vector of visual features extracted from the current image by ResNet (Gan et al., 2017) and v is the mean of all such vectors; C s \u2208 R (d\u00d7d)\u00d72048 . On the output side, x t \u2208 R V is a 1-hot vector with dimension equal to the size of the caption vocabulary, V , and W e \u2208 R d\u00d7V is a word embedding matrix, the i-th column of which is the embedding vector of the i-th word in the vocabulary; it is obtained by the Stanford GLoVe algorithm with zero mean (Pennington et al., 2017) .",
                "cite_spans": [
                    {
                        "start": 93,
                        "end": 111,
                        "text": "(Gan et al., 2017)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 475,
                        "end": 500,
                        "text": "(Pennington et al., 2017)",
                        "ref_id": "BIBREF26"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "System Description",
                "sec_num": "4"
            },
            {
                "text": "x 0 is initialized as the one-hot vector corresponding to a \"start-of-sentence\" symbol.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "System Description",
                "sec_num": "4"
            },
            {
                "text": "For U in Fig. 1 , the state updating equations are:",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 14,
                        "end": 15,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "System Description",
                "sec_num": "4"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "f2,t = \u03c3g( \u015ct-1w2,f -D 2,f Wext-1 + U 2,f pt-1) (10) \u00ee2,t = \u03c3g( \u015ct-1w2,i -D2,iWext-1 + U2,ipt-1) (11) \u00f42,t = \u03c3g( \u015ct-1w2,o -D2,oWext-1 + U2,opt-1) (12) g2,t = \u03c3 h ( \u015ct-1w2,c -D2,cWext-1 + U2,cpt-1) (13) c2,t = f2,t c2,t-1 + \u00ee2,t g2,t",
                        "eq_num": "(14)"
                    }
                ],
                "section": "System Description",
                "sec_num": "4"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "pt = \u00f42,t \u03c3 h (c2,t)",
                        "eq_num": "(15)"
                    }
                ],
                "section": "System Description",
                "sec_num": "4"
            },
            {
                "text": "Here",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "System Description",
                "sec_num": "4"
            },
            {
                "text": "w 2,f , w 2,i , w 2,o , w 2,c \u2208 R d , D 2,f , D 2,i , D 2,o , D 2,c \u2208 R d\u00d7d , and U 2,f , U 2,i , U 2,o , U 2,c \u2208 R d\u00d7d .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "System Description",
                "sec_num": "4"
            },
            {
                "text": "The initial state p 0 is the zero vector.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "System Description",
                "sec_num": "4"
            },
            {
                "text": "The dimensionality of the crucial vectors shown in Fig. 1 , u t and f t , is increased from d\u00d71 to d 2 \u00d71 as follows. A block-diagonal d 2 \u00d7 d 2 matrix S t is created by placing d copies of the d \u00d7 d matrix \u015ct as blocks along the principal diagonal. This matrix is the output of the sentence-encoding subnetwork S. Now the 'filler vector' f t \u2208 R d 2 -'unbound' from the sentence representation S t with the 'unbinding vector' u t -is obtained by Eq. ( 16).",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 56,
                        "end": 57,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "System Description",
                "sec_num": "4"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "f t = S t u t (",
                        "eq_num": "16"
                    }
                ],
                "section": "System Description",
                "sec_num": "4"
            },
            {
                "text": ")",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "System Description",
                "sec_num": "4"
            },
            {
                "text": "Here u t \u2208 R d 2 , the output of the unbinding subnetwork U, is computed as in Eq. ( 17), where W u \u2208 R d 2 \u00d7d is U's output weight matrix.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "System Description",
                "sec_num": "4"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "u t = \u03c3 h (W u p t )",
                        "eq_num": "(17)"
                    }
                ],
                "section": "System Description",
                "sec_num": "4"
            },
            {
                "text": "Finally, the lexical subnetwork L produces a decoded word x t \u2208 R V by",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "System Description",
                "sec_num": "4"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "x t = \u03c3 s (W x f t )",
                        "eq_num": "(18)"
                    }
                ],
                "section": "System Description",
                "sec_num": "4"
            },
            {
                "text": "where \u03c3 s (\u2022) is the softmax function and W x \u2208 R V \u00d7d 2 is the overall output weight matrix. Since W x plays the role of a word de-embedding matrix, we can set",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "System Description",
                "sec_num": "4"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "W x = (W e )",
                        "eq_num": "(19)"
                    }
                ],
                "section": "System Description",
                "sec_num": "4"
            },
            {
                "text": "where W e is the word-embedding matrix. Since W e is pre-defined, we directly set W x by Eq. ( 19) without training L through Eq. ( 18). Note that S and U are learned jointly through end-to-end training as shown in Algorithm 1. Algorithm 1 End-to-end training of S and U Input: Image feature vector v (i) and corresponding caption end for 14: end for 5 Experimental results",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "System Description",
                "sec_num": "4"
            },
            {
                "text": "X (i) = [x (i) 1 , \u2022 \u2022 \u2022 , x (i) T ] (i = 1 , \u2022 \u2022 \u2022 , N ),",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "System Description",
                "sec_num": "4"
            },
            {
                "text": "To evaluate the performance of our proposed model, we use the COCO dataset (COCO, 2017). The COCO dataset contains 123,287 images, each of which is annotated with at least 5 captions. We use the same pre-defined splits as in (Karpathy and Fei-Fei, 2015; Gan et al., 2017) : 113,287 images for training, 5,000 images for validation, and 5,000 images for testing. We use the same vocabu-lary as that employed in (Gan et al., 2017) , which consists of 8,791 words.",
                "cite_spans": [
                    {
                        "start": 225,
                        "end": 253,
                        "text": "(Karpathy and Fei-Fei, 2015;",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 254,
                        "end": 271,
                        "text": "Gan et al., 2017)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 410,
                        "end": 428,
                        "text": "(Gan et al., 2017)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dataset",
                "sec_num": "5.1"
            },
            {
                "text": "For the CNN of Fig. 1 , we used ResNet-152 (He et al., 2016) , pretrained on the ImageNet dataset. The feature vector v has 2048 dimensions. Word embedding vectors in W e are downloaded from the web (Pennington et al., 2017) . The model is implemented in TensorFlow (Abadi et al., 2015) with the default settings for random initialization and optimization by backpropagation.",
                "cite_spans": [
                    {
                        "start": 43,
                        "end": 60,
                        "text": "(He et al., 2016)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 199,
                        "end": 224,
                        "text": "(Pennington et al., 2017)",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 266,
                        "end": 286,
                        "text": "(Abadi et al., 2015)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 20,
                        "end": 21,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Evaluation",
                "sec_num": "5.2"
            },
            {
                "text": "In our experiments, we choose d = 25 (where d is the dimension of vector p t ). The dimension of S t is 625 \u00d7 625 (while \u015ct is 25 \u00d7 25); the vocabulary size V = 8, 791; the dimension of u t and f t is d 2 = 625.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation",
                "sec_num": "5.2"
            },
            {
                "text": "The main evaluation results on the MS COCO dataset are reported in Table 5 .2. The widelyused BLEU (Papineni et al., 2002) , METEOR (Banerjee and Lavie, 2005) , and CIDEr (Vedantam et al., 2015) metrics are reported in our quantitative evaluation of the performance of the proposed model. In evaluation, our baseline is the widely used CNN-LSTM captioning method originally proposed in (Vinyals et al., 2015) . For comparison, we include results in that paper in the first line of Table 5 .2. We also re-implemented the model using the latest ResNet features and report the results in the second line of Table 5 .2. Our re-implementation of the CNN-LSTM method matches the performance reported in (Gan et al., 2017) , showing that the baseline is a state-of-theart implementation. For TPGN, we use parameter settings in a similar range to those in (Gan et al., 2017) . TPGN has comparable, although slightly Methods METEOR BLEU-1 BLEU-2 BLEU-3 BLEU-4 CIDEr NIC (Vinyals et al., 2015) It is worth mentioning that this paper is aimed at developing a Tensor Product Representation (TPR) inspired network to replace the core layers in an LSTM; therefore, it is directly comparable to an LSTM baseline. So in the experiments, we focus on comparison to a strong CNN-LSTM baseline. We acknowledge that more recent papers (Xu et al., 2017; Rennie et al., 2017; Yao et al., 2017; Lu et al., 2017; Gan et al., 2017) reported better performance on the task of image captioning. Performance improvements in these more recent models are mainly due to using better image features such as those obtained by Region-based Convolutional Neural Networks (R-CNN), or using reinforcement learning (RL) to directly optimize metrics such as CIDEr, or using more complex attention mechanisms (Gan et al., 2017) to provide a better context vector for caption generation, or using an ensemble of multiple LSTMs, among others. However, the LSTM is still playing a core role in these works and we believe improvement over the core LSTM, in both performance and interpretability, is still very valuable; that is why we compare the proposed TPGN with a state-of-the-art native LSTM (the second line of Table 5 .2).",
                "cite_spans": [
                    {
                        "start": 99,
                        "end": 122,
                        "text": "(Papineni et al., 2002)",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 132,
                        "end": 158,
                        "text": "(Banerjee and Lavie, 2005)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 171,
                        "end": 194,
                        "text": "(Vedantam et al., 2015)",
                        "ref_id": "BIBREF34"
                    },
                    {
                        "start": 386,
                        "end": 408,
                        "text": "(Vinyals et al., 2015)",
                        "ref_id": "BIBREF35"
                    },
                    {
                        "start": 697,
                        "end": 715,
                        "text": "(Gan et al., 2017)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 848,
                        "end": 866,
                        "text": "(Gan et al., 2017)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 961,
                        "end": 983,
                        "text": "(Vinyals et al., 2015)",
                        "ref_id": "BIBREF35"
                    },
                    {
                        "start": 1314,
                        "end": 1331,
                        "text": "(Xu et al., 2017;",
                        "ref_id": "BIBREF36"
                    },
                    {
                        "start": 1332,
                        "end": 1352,
                        "text": "Rennie et al., 2017;",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 1353,
                        "end": 1370,
                        "text": "Yao et al., 2017;",
                        "ref_id": "BIBREF37"
                    },
                    {
                        "start": 1371,
                        "end": 1387,
                        "text": "Lu et al., 2017;",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 1388,
                        "end": 1405,
                        "text": "Gan et al., 2017)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 1768,
                        "end": 1786,
                        "text": "(Gan et al., 2017)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 73,
                        "end": 74,
                        "text": "5",
                        "ref_id": null
                    },
                    {
                        "start": 487,
                        "end": 488,
                        "text": "5",
                        "ref_id": null
                    },
                    {
                        "start": 610,
                        "end": 611,
                        "text": "5",
                        "ref_id": null
                    },
                    {
                        "start": 2178,
                        "end": 2179,
                        "text": "5",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Evaluation",
                "sec_num": "5.2"
            },
            {
                "text": "To get a sense of how the sentence encodings S t learned by TPGN approximate TPRs, we now investigate the meaning of the role-unbinding vec-tor u t the model uses to unbind from S t -via Eq. ( 16) -the filler vector f t that produces -via Eq. ( 18) -the one-hot vector x t of the t th generated caption word. The meaning of an unbinding vector is the meaning of the role it unbinds. Interpreting the unbinding vectors reveals the meaning of the roles in a TPR that S approximates. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Interpretation of learned unbinding vectors",
                "sec_num": "5.3"
            },
            {
                "text": "We run the TPGN model with 5,000 test images as input, and obtain the unbinding vector u t used to generate each word x t in the caption of a test image. We plot 1,000 unbinding vectors u t , which correspond to the first 1,000 words in the resulting captions of these 5,000 test images. There are 17 parts of speech (POS) in these 1,000 words.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Visualization of u t",
                "sec_num": "5.3.1"
            },
            {
                "text": "The POS tags are obtained by the Stanford Parser (Manning, 2017) . We use the Embedding Projector in Tensor-Board (Google, 2017) to plot 1,000 unbinding vectors u t with a custom linear projection in Tensor-Board to reduce 625 dimensions of u t to 2 dimensions shown in Fig. 3 through Fig. 7 .",
                "cite_spans": [
                    {
                        "start": 49,
                        "end": 64,
                        "text": "(Manning, 2017)",
                        "ref_id": null
                    },
                    {
                        "start": 114,
                        "end": 128,
                        "text": "(Google, 2017)",
                        "ref_id": "BIBREF10"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 275,
                        "end": 276,
                        "text": "3",
                        "ref_id": "FIGREF3"
                    },
                    {
                        "start": 290,
                        "end": 291,
                        "text": "7",
                        "ref_id": "FIGREF7"
                    }
                ],
                "eq_spans": [],
                "section": "Visualization of u t",
                "sec_num": "5.3.1"
            },
            {
                "text": "Fig. 3 shows the unbinding vectors of 1000 words; different POS tags of words are represented by different colors. In fact, we can partition the 625-dim space of u t into 17 regions, each of which contains 76.3% words of the same type of POS on average; i.e., each region is dominated by words of one POS type. This clearly indicates that each unbinding vector contains important grammatical information about the word it generates. As examples, Fig. 4 to Fig. 7 show the distribution of the unbinding vectors of nouns, verbs, adjectives, and prepositions, respectively. Furthermore, we show that the subject and the object of a sentence can be distinguished based on u t in (Huang et al., 2018) . ",
                "cite_spans": [
                    {
                        "start": 675,
                        "end": 695,
                        "text": "(Huang et al., 2018)",
                        "ref_id": "BIBREF13"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 5,
                        "end": 6,
                        "text": "3",
                        "ref_id": "FIGREF3"
                    },
                    {
                        "start": 451,
                        "end": 452,
                        "text": "4",
                        "ref_id": "FIGREF4"
                    },
                    {
                        "start": 461,
                        "end": 462,
                        "text": "7",
                        "ref_id": "FIGREF7"
                    }
                ],
                "eq_spans": [],
                "section": "Visualization of u t",
                "sec_num": "5.3.1"
            },
            {
                "text": "Since the previous section indicates that there is a clustering structure for u t , in this section we partition u t into N u clusters and examine the grammar roles played by u t .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Clustering of u t",
                "sec_num": "5.3.2"
            },
            {
                "text": "First, we run the trained TPGN model on the 113,287 training images, obtaining the role- unbinding vector u t used to generate each word x t in the caption sentence. There are approximately 1.2 million u t vectors over all the training images. We apply the K-means clustering algorithm to these vectors to obtain N u clusters and the centroid \u00b5 i of each cluster i (i = 0, \u2022 \u2022 \u2022 , N u -1).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Clustering of u t",
                "sec_num": "5.3.2"
            },
            {
                "text": "Then, we run the TPGN model with 5,000 test images as input, and obtain the role vector u t of each word x t in the caption sentence of a test image. Using the nearest neighbor rule, we obtain the index i of the cluster that each u t is assigned to.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Clustering of u t",
                "sec_num": "5.3.2"
            },
            {
                "text": "The partitioning of the unbinding vectors u t into N u = 2 clusters exposes the most fundamental distinction made by the roles. We find that the vectors assigned to Cluster 1 generate words which are nouns, pronouns, indefinite and definite articles, and adjectives, while the vectors assigned to Cluster 0 generate verbs, prepositions, conjunctions, and adverbs. Thus Cluster 1 contains the noun-related words, Cluster 0 the verb-like words (verbs, prepositions and conjunctions are all potentially followed by noun-phrase complements, for example). Cross-cutting this distinction is another dimension, however: the initial word in a caption (always a determiner) is sometimes generated with a Cluster 1 unbinding vector, sometimes with a Cluster 0 vector. Outside the captioninitial position, exceptions to the nominal/verbal \u223c Cluster 1/0 generalization are rare, as attested by the high rates of conformity to the generalization shown in Table 5 .3.1. Table 5 .3.1 shows the likelihood of correctness of this 'N/V' generalization for the words in 5,000 sentences captioned for the 5,000 test images; N w is the number of words in the category, N r is the number of words conforming to the generalization, and P c = N r /N w is the proportion conforming. We use the Natural Language Toolkit (NLTK, 2017) to identify the part of speech of each word in the captions.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 948,
                        "end": 949,
                        "text": "5",
                        "ref_id": null
                    },
                    {
                        "start": 962,
                        "end": 963,
                        "text": "5",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Clustering of u t",
                "sec_num": "5.3.2"
            },
            {
                "text": "A similar analysis with N u = 10 clusters reveals the results shown in Table 5 .3.1; these results concern the first 100 captions, which were inspected manually to identify interpretable patterns. (More comprehensive results will be discussed elsewhere.)",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 77,
                        "end": 78,
                        "text": "5",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Clustering of u t",
                "sec_num": "5.3.2"
            },
            {
                "text": "The clusters can be interpreted as falling into 3 groups (see Table 5 .3.1). Clusters 2 and 3 are clearly positional roles: every initial word is generated by a role-unbinding vector from Cluster 2, and such vectors are not used elsewhere in the string. The same holds for Cluster 3 and the second caption word.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 68,
                        "end": 69,
                        "text": "5",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Clustering of u t",
                "sec_num": "5.3.2"
            },
            {
                "text": "For caption words after the second word, position is replaced by syntactic/semantic properties for interpretation purposes. The vector clusters aside from 2 and 3 generate words with a dominant grammatical category: for example, unbinding vectors assigned to the cluster 4 generate words that are 91% likely to be prepositions, and 72% likely to be spatial prepositions. Cluster 7 generates 88% nouns and 9% adjectives, with the remaining 3% scattered across other categories. As Table 5 .3.1 shows, clusters 1, 5, 7, 9 are primarily nominal, and 0, 4, 6, and 8 primarily verbal. (Only cluster 5 spans the N/V divide.)",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 486,
                        "end": 487,
                        "text": "5",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Clustering of u t",
                "sec_num": "5.3.2"
            },
            {
                "text": "This work follows a great deal of recent captiongeneration literature in exploiting end-to-end deep learning with a CNN image-analysis front end producing a distributed representation that is then used to drive a natural-language generation process, typically using RNNs (Mao et al., 2015; Vinyals et al., 2015; Devlin et al., 2015; Chen and Zitnick, 2015; Donahue et al., 2015; Karpathy and Fei-Fei, 2015; Kiros et al., 2014a,b; Xu et al., 2017; Rennie et al., 2017; Yao et al., 2017; Lu et al., 2017) . Our grammatical interpretation of the structural roles of words in sentences makes contact with other work that incorporates deep learning into grammatically-structured networks (Tai et al., 2015; Kumar et al., 2016; Kong et al., 2017; Andreas et al., 2015; Yogatama et al., 2016; Maillard et al., 2017; Socher et al., 2010; Pollack, 1990) . Here, the network is not itself structured to match the grammatical structure of sentences being processed; the structure is fixed, but is designed to support the learning of distributed representations that incorporate structure internal to the representations themselves -filler/role structure.",
                "cite_spans": [
                    {
                        "start": 271,
                        "end": 289,
                        "text": "(Mao et al., 2015;",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 290,
                        "end": 311,
                        "text": "Vinyals et al., 2015;",
                        "ref_id": "BIBREF35"
                    },
                    {
                        "start": 312,
                        "end": 332,
                        "text": "Devlin et al., 2015;",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 333,
                        "end": 356,
                        "text": "Chen and Zitnick, 2015;",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 357,
                        "end": 378,
                        "text": "Donahue et al., 2015;",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 379,
                        "end": 406,
                        "text": "Karpathy and Fei-Fei, 2015;",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 407,
                        "end": 429,
                        "text": "Kiros et al., 2014a,b;",
                        "ref_id": null
                    },
                    {
                        "start": 430,
                        "end": 446,
                        "text": "Xu et al., 2017;",
                        "ref_id": "BIBREF36"
                    },
                    {
                        "start": 447,
                        "end": 467,
                        "text": "Rennie et al., 2017;",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 468,
                        "end": 485,
                        "text": "Yao et al., 2017;",
                        "ref_id": "BIBREF37"
                    },
                    {
                        "start": 486,
                        "end": 502,
                        "text": "Lu et al., 2017)",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 683,
                        "end": 701,
                        "text": "(Tai et al., 2015;",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 702,
                        "end": 721,
                        "text": "Kumar et al., 2016;",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 722,
                        "end": 740,
                        "text": "Kong et al., 2017;",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 741,
                        "end": 762,
                        "text": "Andreas et al., 2015;",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 763,
                        "end": 785,
                        "text": "Yogatama et al., 2016;",
                        "ref_id": "BIBREF38"
                    },
                    {
                        "start": 786,
                        "end": 808,
                        "text": "Maillard et al., 2017;",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 809,
                        "end": 829,
                        "text": "Socher et al., 2010;",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 830,
                        "end": 844,
                        "text": "Pollack, 1990)",
                        "ref_id": "BIBREF27"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related work",
                "sec_num": "6"
            },
            {
                "text": "TPRs are also used in NLP in (Palangi et al., 2017) but there the representation of each individual input word is constrained to be a literal TPR filler/role binding. (The idea of using the outer product to construct internal representations was also explored in (Fukui et al., 2016) .) Here, by contrast, the learned representations are not themselves constrained, but the global structure of the network is designed to display the somewhat abstract property of being TPR-capable: the archi-tecture uses the TPR unbinding operation of the matrix-vector product to extract individual words for sequential output.",
                "cite_spans": [
                    {
                        "start": 29,
                        "end": 51,
                        "text": "(Palangi et al., 2017)",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 263,
                        "end": 283,
                        "text": "(Fukui et al., 2016)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related work",
                "sec_num": "6"
            },
            {
                "text": "Tensor Product Representation (TPR) (Smolensky, 1990 ) is a general technique for constructing vector embeddings of complex symbol structures in such a way that powerful symbolic functions can be computed using hand-designed neural network computation. Integrating TPR with deep learning is a largely open problem for which the work presented here proposes a general approach: design deep architectures that are TPRcapable -TPR computation is within the scope of the capabilities of the architecture in principle. For natural language generation, we proposed such an architecture, the Tensor Product Generation Network (TPGN): it embodies the TPR operation of unbinding which is used to extract particular symbols (e.g., words) from complex structures (e.g., sentences). The architecture can be interpreted as containing a part that encodes a sentence and a part that selects one structural role at a time to extract from the sentence. We applied the approach to image-caption generation, developing a TPGN model that was evaluated on the COCO dataset, on which it outperformed LSTM baselines on a range of standard metrics. Unlike standard LSTMs, however, the TPGN model admits a level of interpretability: we can see which roles are being unbound by the unbinding vectors generated internally within the model. We find such roles contain considerable grammatical information, enabling POS tag prediction for the words they generate and displaying clustering by POS.",
                "cite_spans": [
                    {
                        "start": 36,
                        "end": 52,
                        "text": "(Smolensky, 1990",
                        "ref_id": "BIBREF29"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "7"
            }
        ],
        "back_matter": [],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "TensorFlow: Largescale machine learning on heterogeneous systems",
                "authors": [
                    {
                        "first": "Mart\u00edn",
                        "middle": [],
                        "last": "Abadi",
                        "suffix": ""
                    },
                    {
                        "first": "Ashish",
                        "middle": [],
                        "last": "Agarwal",
                        "suffix": ""
                    },
                    {
                        "first": "Paul",
                        "middle": [],
                        "last": "Barham",
                        "suffix": ""
                    },
                    {
                        "first": "Eugene",
                        "middle": [],
                        "last": "Brevdo",
                        "suffix": ""
                    },
                    {
                        "first": "Zhifeng",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Craig",
                        "middle": [],
                        "last": "Citro",
                        "suffix": ""
                    },
                    {
                        "first": "Greg",
                        "middle": [
                            "S"
                        ],
                        "last": "Corrado",
                        "suffix": ""
                    },
                    {
                        "first": "Andy",
                        "middle": [],
                        "last": "Davis",
                        "suffix": ""
                    },
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Dean",
                        "suffix": ""
                    },
                    {
                        "first": "Matthieu",
                        "middle": [],
                        "last": "Devin",
                        "suffix": ""
                    },
                    {
                        "first": "Sanjay",
                        "middle": [],
                        "last": "Ghemawat",
                        "suffix": ""
                    },
                    {
                        "first": "Ian",
                        "middle": [],
                        "last": "Goodfellow",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Harp",
                        "suffix": ""
                    },
                    {
                        "first": "Geoffrey",
                        "middle": [],
                        "last": "Irving",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Isard",
                        "suffix": ""
                    },
                    {
                        "first": "Yangqing",
                        "middle": [],
                        "last": "Jia",
                        "suffix": ""
                    },
                    {
                        "first": "Rafal",
                        "middle": [],
                        "last": "Jozefowicz",
                        "suffix": ""
                    },
                    {
                        "first": "Lukasz",
                        "middle": [],
                        "last": "Kaiser",
                        "suffix": ""
                    },
                    {
                        "first": "Manjunath",
                        "middle": [],
                        "last": "Kudlur",
                        "suffix": ""
                    },
                    {
                        "first": "Josh",
                        "middle": [],
                        "last": "Levenberg",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Man\u00e9",
                        "suffix": ""
                    },
                    {
                        "first": "Rajat",
                        "middle": [],
                        "last": "Monga",
                        "suffix": ""
                    },
                    {
                        "first": "Sherry",
                        "middle": [],
                        "last": "Moore",
                        "suffix": ""
                    },
                    {
                        "first": "Derek",
                        "middle": [],
                        "last": "Murray",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Olah",
                        "suffix": ""
                    },
                    {
                        "first": "Mike",
                        "middle": [],
                        "last": "Schuster",
                        "suffix": ""
                    },
                    {
                        "first": "Jonathon",
                        "middle": [],
                        "last": "Shlens",
                        "suffix": ""
                    },
                    {
                        "first": "Benoit",
                        "middle": [],
                        "last": "Steiner",
                        "suffix": ""
                    },
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    },
                    {
                        "first": "Kunal",
                        "middle": [],
                        "last": "Talwar",
                        "suffix": ""
                    },
                    {
                        "first": "Paul",
                        "middle": [],
                        "last": "Tucker",
                        "suffix": ""
                    },
                    {
                        "first": "Vincent",
                        "middle": [],
                        "last": "Vanhoucke",
                        "suffix": ""
                    },
                    {
                        "first": "Vijay",
                        "middle": [],
                        "last": "Vasudevan",
                        "suffix": ""
                    },
                    {
                        "first": "Fernanda",
                        "middle": [],
                        "last": "Vi\u00e9gas",
                        "suffix": ""
                    },
                    {
                        "first": "Oriol",
                        "middle": [],
                        "last": "Vinyals",
                        "suffix": ""
                    },
                    {
                        "first": "Pete",
                        "middle": [],
                        "last": "Warden",
                        "suffix": ""
                    },
                    {
                        "first": "Martin",
                        "middle": [],
                        "last": "Wattenberg",
                        "suffix": ""
                    },
                    {
                        "first": "Martin",
                        "middle": [],
                        "last": "Wicke",
                        "suffix": ""
                    },
                    {
                        "first": "Yuan",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaoqiang",
                        "middle": [],
                        "last": "Zheng",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Cor- rado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Man\u00e9, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Tal- war, Paul Tucker, Vincent Vanhoucke, Vijay Va- sudevan, Fernanda Vi\u00e9gas, Oriol Vinyals, Pete War- den, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. 2015. TensorFlow: Large- scale machine learning on heterogeneous systems. Software available from tensorflow.org. https: //www.tensorflow.org/.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Deep compositional question answering with neural module networks",
                "authors": [
                    {
                        "first": "Jacob",
                        "middle": [],
                        "last": "Andreas",
                        "suffix": ""
                    },
                    {
                        "first": "Marcus",
                        "middle": [],
                        "last": "Rohrbach",
                        "suffix": ""
                    },
                    {
                        "first": "Trevor",
                        "middle": [],
                        "last": "Darrell",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Klein",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1511.027992"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. 2015. Deep compositional ques- tion answering with neural module networks. arxiv preprint. arXiv preprint arXiv:1511.02799 2.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments",
                "authors": [
                    {
                        "first": "Satanjeev",
                        "middle": [],
                        "last": "Banerjee",
                        "suffix": ""
                    },
                    {
                        "first": "Alon",
                        "middle": [],
                        "last": "Lavie",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of the ACL workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization. Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "65--72",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceed- ings of the ACL workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization. Association for Computational Lin- guistics, pages 65-72.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Mind's eye: A recurrent visual representation for image caption generation",
                "authors": [
                    {
                        "first": "Xinlei",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Lawrence",
                        "middle": [],
                        "last": "Zitnick",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": "",
                "issue": "",
                "pages": "2422--2431",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xinlei Chen and Lawrence Zitnick. 2015. Mind's eye: A recurrent visual representation for image caption generation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pages 2422-2431.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Incremental parsing in a continuous dynamical system: Sentence processing in Gradient Symbolic Computation",
                "authors": [
                    {
                        "first": "Matthew",
                        "middle": [],
                        "last": "Pyeong Whan Cho",
                        "suffix": ""
                    },
                    {
                        "first": "Paul",
                        "middle": [],
                        "last": "Goldrick",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Smolensky",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Linguistics Vanguard",
                "volume": "3",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.1515/lingvan-2016-0105"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Pyeong Whan Cho, Matthew Goldrick, and Paul Smolensky. 2017. Incremental parsing in a continu- ous dynamical system: Sentence processing in Gra- dient Symbolic Computation. Linguistics Vanguard 3. DOI:10.1515/lingvan-2016-0105.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Coco dataset for image captioning",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Coco",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "COCO. 2017. Coco dataset for image cap- tioning. http://mscoco.org/dataset/ #download.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Language models for image captioning: The quirks and what works",
                "authors": [
                    {
                        "first": "Jacob",
                        "middle": [],
                        "last": "Devlin",
                        "suffix": ""
                    },
                    {
                        "first": "Hao",
                        "middle": [],
                        "last": "Cheng",
                        "suffix": ""
                    },
                    {
                        "first": "Hao",
                        "middle": [],
                        "last": "Fang",
                        "suffix": ""
                    },
                    {
                        "first": "Saurabh",
                        "middle": [],
                        "last": "Gupta",
                        "suffix": ""
                    },
                    {
                        "first": "Li",
                        "middle": [],
                        "last": "Deng",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaodong",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Geoffrey",
                        "middle": [],
                        "last": "Zweig",
                        "suffix": ""
                    },
                    {
                        "first": "Margaret",
                        "middle": [],
                        "last": "Mitchell",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1505.01809"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jacob Devlin, Hao Cheng, Hao Fang, Saurabh Gupta, Li Deng, Xiaodong He, Geoffrey Zweig, and Mar- garet Mitchell. 2015. Language models for im- age captioning: The quirks and what works. arXiv preprint arXiv:1505.01809 .",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Long-term recurrent convolutional networks for visual recognition and description",
                "authors": [
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Donahue",
                        "suffix": ""
                    },
                    {
                        "first": "Lisa",
                        "middle": [
                            "Anne"
                        ],
                        "last": "Hendricks",
                        "suffix": ""
                    },
                    {
                        "first": "Sergio",
                        "middle": [],
                        "last": "Guadarrama",
                        "suffix": ""
                    },
                    {
                        "first": "Marcus",
                        "middle": [],
                        "last": "Rohrbach",
                        "suffix": ""
                    },
                    {
                        "first": "Subhashini",
                        "middle": [],
                        "last": "Venugopalan",
                        "suffix": ""
                    },
                    {
                        "first": "Kate",
                        "middle": [],
                        "last": "Saenko",
                        "suffix": ""
                    },
                    {
                        "first": "Trevor",
                        "middle": [],
                        "last": "Darrell",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
                "volume": "",
                "issue": "",
                "pages": "2625--2634",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadar- rama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, and Trevor Darrell. 2015. Long-term recurrent convolutional networks for visual recogni- tion and description. In Proceedings of the IEEE conference on computer vision and pattern recogni- tion. pages 2625-2634.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Multimodal compact bilinear pooling for visual question answering and visual grounding",
                "authors": [
                    {
                        "first": "Akira",
                        "middle": [],
                        "last": "Fukui",
                        "suffix": ""
                    },
                    {
                        "first": "Dong",
                        "middle": [
                            "Huk"
                        ],
                        "last": "Park",
                        "suffix": ""
                    },
                    {
                        "first": "Daylen",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Anna",
                        "middle": [],
                        "last": "Rohrbach",
                        "suffix": ""
                    },
                    {
                        "first": "Trevor",
                        "middle": [],
                        "last": "Darrell",
                        "suffix": ""
                    },
                    {
                        "first": "Marcus",
                        "middle": [],
                        "last": "Rohrbach",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1606.01847"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and Marcus Rohrbach. 2016. Multimodal compact bilinear pooling for visual question answering and visual grounding. arXiv preprint arXiv:1606.01847 .",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Semantic compositional networks for visual captioning",
                "authors": [
                    {
                        "first": "Zhe",
                        "middle": [],
                        "last": "Gan",
                        "suffix": ""
                    },
                    {
                        "first": "Chuang",
                        "middle": [],
                        "last": "Gan",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaodong",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Yunchen",
                        "middle": [],
                        "last": "Pu",
                        "suffix": ""
                    },
                    {
                        "first": "Kenneth",
                        "middle": [],
                        "last": "Tran",
                        "suffix": ""
                    },
                    {
                        "first": "Jianfeng",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Lawrence",
                        "middle": [],
                        "last": "Carin",
                        "suffix": ""
                    },
                    {
                        "first": "Li",
                        "middle": [],
                        "last": "Deng",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zhe Gan, Chuang Gan, Xiaodong He, Yunchen Pu, Kenneth Tran, Jianfeng Gao, Lawrence Carin, and Li Deng. 2017. Semantic compositional networks for visual captioning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recog- nition.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Embedding projector in tensorboard",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Google",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Google. 2017. Embedding projector in tensor- board. https://www.tensorflow.org/ programmers_guide/embedding.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Deep residual learning for image recognition",
                "authors": [
                    {
                        "first": "Kaiming",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Xiangyu",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Shaoqing",
                        "middle": [],
                        "last": "Ren",
                        "suffix": ""
                    },
                    {
                        "first": "Jian",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": "",
                "issue": "",
                "pages": "770--778",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recog- nition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pages 770-778.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Long short-term memory",
                "authors": [
                    {
                        "first": "Sepp",
                        "middle": [],
                        "last": "Hochreiter",
                        "suffix": ""
                    },
                    {
                        "first": "J\u00fcrgen",
                        "middle": [],
                        "last": "Schmidhuber",
                        "suffix": ""
                    }
                ],
                "year": 1997,
                "venue": "Neural computation",
                "volume": "9",
                "issue": "8",
                "pages": "1735--1780",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural computation 9(8):1735-1780.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Attentive tensor product learning for language generation and grammar parsing",
                "authors": [
                    {
                        "first": "Qiuyuan",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Li",
                        "middle": [],
                        "last": "Deng",
                        "suffix": ""
                    },
                    {
                        "first": "Dapeng",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Chang",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaodong",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1802.07089"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Qiuyuan Huang, Li Deng, Dapeng Wu, Chang Liu, and Xiaodong He. 2018. Attentive tensor product learn- ing for language generation and grammar parsing. arXiv preprint arXiv:1802.07089 .",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Deep visualsemantic alignments for generating image descriptions",
                "authors": [
                    {
                        "first": "Andrej",
                        "middle": [],
                        "last": "Karpathy",
                        "suffix": ""
                    },
                    {
                        "first": "Li",
                        "middle": [],
                        "last": "Fei-Fei",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": "",
                "issue": "",
                "pages": "3128--3137",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Andrej Karpathy and Li Fei-Fei. 2015. Deep visual- semantic alignments for generating image descrip- tions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pages 3128-3137.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Multimodal neural language models",
                "authors": [
                    {
                        "first": "Ryan",
                        "middle": [],
                        "last": "Kiros",
                        "suffix": ""
                    },
                    {
                        "first": "Ruslan",
                        "middle": [],
                        "last": "Salakhutdinov",
                        "suffix": ""
                    },
                    {
                        "first": "Rich",
                        "middle": [],
                        "last": "Zemel",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML-14)",
                "volume": "",
                "issue": "",
                "pages": "595--603",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ryan Kiros, Ruslan Salakhutdinov, and Rich Zemel. 2014a. Multimodal neural language models. In Proceedings of the 31st International Conference on Machine Learning (ICML-14). pages 595-603.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Unifying visual-semantic embeddings with multimodal neural language models",
                "authors": [
                    {
                        "first": "Ryan",
                        "middle": [],
                        "last": "Kiros",
                        "suffix": ""
                    },
                    {
                        "first": "Ruslan",
                        "middle": [],
                        "last": "Salakhutdinov",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [
                            "S"
                        ],
                        "last": "Zemel",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1411.2539"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ryan Kiros, Ruslan Salakhutdinov, and Richard S Zemel. 2014b. Unifying visual-semantic embed- dings with multimodal neural language models. arXiv preprint arXiv:1411.2539 .",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Dragnn: A transitionbased framework for dynamically connected neural networks",
                "authors": [
                    {
                        "first": "Lingpeng",
                        "middle": [],
                        "last": "Kong",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Alberti",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Andor",
                        "suffix": ""
                    },
                    {
                        "first": "Ivan",
                        "middle": [],
                        "last": "Bogatyy",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Weiss",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1703.04474"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Lingpeng Kong, Chris Alberti, Daniel Andor, Ivan Bo- gatyy, and David Weiss. 2017. Dragnn: A transition- based framework for dynamically connected neural networks. arXiv preprint arXiv:1703.04474 .",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Ask me anything: Dynamic memory networks for natural language processing",
                "authors": [
                    {
                        "first": "Ankit",
                        "middle": [],
                        "last": "Kumar",
                        "suffix": ""
                    },
                    {
                        "first": "Ozan",
                        "middle": [],
                        "last": "Irsoy",
                        "suffix": ""
                    },
                    {
                        "first": "Peter",
                        "middle": [],
                        "last": "Ondruska",
                        "suffix": ""
                    },
                    {
                        "first": "Mohit",
                        "middle": [],
                        "last": "Iyyer",
                        "suffix": ""
                    },
                    {
                        "first": "James",
                        "middle": [],
                        "last": "Bradbury",
                        "suffix": ""
                    },
                    {
                        "first": "Ishaan",
                        "middle": [],
                        "last": "Gulrajani",
                        "suffix": ""
                    },
                    {
                        "first": "Victor",
                        "middle": [],
                        "last": "Zhong",
                        "suffix": ""
                    },
                    {
                        "first": "Romain",
                        "middle": [],
                        "last": "Paulus",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "International Conference on Machine Learning",
                "volume": "",
                "issue": "",
                "pages": "1378--1387",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor Zhong, Romain Paulus, and Richard Socher. 2016. Ask me anything: Dynamic memory networks for natu- ral language processing. In International Confer- ence on Machine Learning. pages 1378-1387.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Knowing when to look: Adaptive attention via a visual sentinel for image captioning",
                "authors": [
                    {
                        "first": "Jiasen",
                        "middle": [],
                        "last": "Lu",
                        "suffix": ""
                    },
                    {
                        "first": "Caiming",
                        "middle": [],
                        "last": "Xiong",
                        "suffix": ""
                    },
                    {
                        "first": "Devi",
                        "middle": [],
                        "last": "Parikh",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": "6",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jiasen Lu, Caiming Xiong, Devi Parikh, and Richard Socher. 2017. Knowing when to look: Adaptive at- tention via a visual sentinel for image captioning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). volume 6.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Jointly learning sentence embeddings and syntax with unsupervised tree-lstms",
                "authors": [
                    {
                        "first": "Jean",
                        "middle": [],
                        "last": "Maillard",
                        "suffix": ""
                    },
                    {
                        "first": "Stephen",
                        "middle": [],
                        "last": "Clark",
                        "suffix": ""
                    },
                    {
                        "first": "Dani",
                        "middle": [],
                        "last": "Yogatama",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1705.09189"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jean Maillard, Stephen Clark, and Dani Yogatama. 2017. Jointly learning sentence embeddings and syntax with unsupervised tree-lstms. arXiv preprint arXiv:1705.09189 .",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Deep captioning with multimodal recurrent neural networks (m-rnn)",
                "authors": [
                    {
                        "first": "Junhua",
                        "middle": [],
                        "last": "Mao",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Yi",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Jiang",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Zhiheng",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Alan",
                        "middle": [],
                        "last": "Yuille",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of International Conference on Learning Representations",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhiheng Huang, and Alan Yuille. 2015. Deep captioning with multimodal recurrent neural networks (m-rnn). In Proceedings of International Conference on Learn- ing Representations.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Natural language toolkit (nltk)",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Nltk",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "NLTK. 2017. Natural language toolkit (nltk). http: //www.nltk.org.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Deep learning of grammaticallyinterpretable representations through questionanswering",
                "authors": [
                    {
                        "first": "Hamid",
                        "middle": [],
                        "last": "Palangi",
                        "suffix": ""
                    },
                    {
                        "first": "Paul",
                        "middle": [],
                        "last": "Smolensky",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaodong",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Li",
                        "middle": [],
                        "last": "Deng",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1705.08432"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Hamid Palangi, Paul Smolensky, Xiaodong He, and Li Deng. 2017. Deep learning of grammatically- interpretable representations through question- answering. arXiv preprint arXiv:1705.08432",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Bleu: a method for automatic evaluation of machine translation",
                "authors": [
                    {
                        "first": "Kishore",
                        "middle": [],
                        "last": "Papineni",
                        "suffix": ""
                    },
                    {
                        "first": "Salim",
                        "middle": [],
                        "last": "Roukos",
                        "suffix": ""
                    },
                    {
                        "first": "Todd",
                        "middle": [],
                        "last": "Ward",
                        "suffix": ""
                    },
                    {
                        "first": "Wei-Jing",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Proceedings of the 40th annual meeting on association for computational linguistics",
                "volume": "",
                "issue": "",
                "pages": "311--318",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic eval- uation of machine translation. In Proceedings of the 40th annual meeting on association for compu- tational linguistics. Association for Computational Linguistics, pages 311-318.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Stanford glove: Global vectors for word representation",
                "authors": [
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Pennington",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jeffrey Pennington, Richard Socher, and Christopher Manning. 2017. Stanford glove: Global vec- tors for word representation. https://nlp. stanford.edu/projects/glove/.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Recursive distributed representations",
                "authors": [
                    {
                        "first": "B",
                        "middle": [],
                        "last": "Jordan",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Pollack",
                        "suffix": ""
                    }
                ],
                "year": 1990,
                "venue": "Artificial Intelligence",
                "volume": "46",
                "issue": "1",
                "pages": "77--105",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jordan B Pollack. 1990. Recursive distributed repre- sentations. Artificial Intelligence 46(1):77-105.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Self-critical sequence training for image captioning",
                "authors": [
                    {
                        "first": "Steven",
                        "middle": [
                            "J"
                        ],
                        "last": "Rennie",
                        "suffix": ""
                    },
                    {
                        "first": "Etienne",
                        "middle": [],
                        "last": "Marcheret",
                        "suffix": ""
                    },
                    {
                        "first": "Youssef",
                        "middle": [],
                        "last": "Mroueh",
                        "suffix": ""
                    },
                    {
                        "first": "Jerret",
                        "middle": [],
                        "last": "Ross",
                        "suffix": ""
                    },
                    {
                        "first": "Vaibhava",
                        "middle": [],
                        "last": "Goel",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Steven J. Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. 2017. Self-critical sequence training for image captioning. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Tensor product variable binding and the representation of symbolic structures in connectionist systems",
                "authors": [
                    {
                        "first": "Paul",
                        "middle": [],
                        "last": "Smolensky",
                        "suffix": ""
                    }
                ],
                "year": 1990,
                "venue": "Artificial intelligence",
                "volume": "46",
                "issue": "1-2",
                "pages": "159--216",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Paul Smolensky. 1990. Tensor product variable bind- ing and the representation of symbolic structures in connectionist systems. Artificial intelligence 46(1- 2):159-216.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "Symbolic functions from neural computation",
                "authors": [
                    {
                        "first": "Paul",
                        "middle": [],
                        "last": "Smolensky",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Philosophical Transactions of the Royal Society -A: Mathematical, Physical and Engineering Sciences",
                "volume": "370",
                "issue": "",
                "pages": "3543--3569",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Paul Smolensky. 2012. Symbolic functions from neu- ral computation. Philosophical Transactions of the Royal Society -A: Mathematical, Physical and En- gineering Sciences 370:3543 -3569.",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "The harmonic mind: From neural computation to optimality-theoretic grammar",
                "authors": [
                    {
                        "first": "Paul",
                        "middle": [],
                        "last": "Smolensky",
                        "suffix": ""
                    },
                    {
                        "first": "G\u00e9raldine",
                        "middle": [],
                        "last": "Legendre",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Cognitive architecture",
                "volume": "1",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Paul Smolensky and G\u00e9raldine Legendre. 2006. The harmonic mind: From neural computation to optimality-theoretic grammar. Volume 1: Cognitive architecture. MIT Press.",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "Learning continuous phrase representations and syntactic parsing with recursive neural networks",
                "authors": [
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [
                            "Y"
                        ],
                        "last": "Ng",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop",
                "volume": "",
                "issue": "",
                "pages": "1--9",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Richard Socher, Christopher D Manning, and An- drew Y Ng. 2010. Learning continuous phrase representations and syntactic parsing with recursive neural networks. In Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop. pages 1-9.",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "Improved semantic representations from tree-structured long short-term memory networks",
                "authors": [
                    {
                        "first": "Kai",
                        "middle": [],
                        "last": "Sheng",
                        "suffix": ""
                    },
                    {
                        "first": "Tai",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1503.00075"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Kai Sheng Tai, Richard Socher, and Christopher D Manning. 2015. Improved semantic representations from tree-structured long short-term memory net- works. arXiv preprint arXiv:1503.00075 .",
                "links": null
            },
            "BIBREF34": {
                "ref_id": "b34",
                "title": "Cider: Consensus-based image description evaluation",
                "authors": [
                    {
                        "first": "Ramakrishna",
                        "middle": [],
                        "last": "Vedantam",
                        "suffix": ""
                    },
                    {
                        "first": "Lawrence",
                        "middle": [],
                        "last": "Zitnick",
                        "suffix": ""
                    },
                    {
                        "first": "Devi",
                        "middle": [],
                        "last": "Parikh",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": "",
                "issue": "",
                "pages": "4566--4575",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. 2015. Cider: Consensus-based image de- scription evaluation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recog- nition. pages 4566-4575.",
                "links": null
            },
            "BIBREF35": {
                "ref_id": "b35",
                "title": "Show and tell: A neural image caption generator",
                "authors": [
                    {
                        "first": "Oriol",
                        "middle": [],
                        "last": "Vinyals",
                        "suffix": ""
                    },
                    {
                        "first": "Alexander",
                        "middle": [],
                        "last": "Toshev",
                        "suffix": ""
                    },
                    {
                        "first": "Samy",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    },
                    {
                        "first": "Dumitru",
                        "middle": [],
                        "last": "Erhan",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": "",
                "issue": "",
                "pages": "3156--3164",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. 2015. Show and tell: A neural im- age caption generator. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recog- nition. pages 3156-3164. 1272",
                "links": null
            },
            "BIBREF36": {
                "ref_id": "b36",
                "title": "Image captioning with deep lstm based on sequential residual",
                "authors": [
                    {
                        "first": "Kaisheng",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Hanli",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Pengjie",
                        "middle": [],
                        "last": "Tang",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of IEEE International Conference on Multimedia and Expo (ICME)",
                "volume": "",
                "issue": "",
                "pages": "361--366",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kaisheng Xu, Hanli Wang, and Pengjie Tang. 2017. Image captioning with deep lstm based on sequen- tial residual. In Proceedings of IEEE International Conference on Multimedia and Expo (ICME). pages 361-366.",
                "links": null
            },
            "BIBREF37": {
                "ref_id": "b37",
                "title": "Boosting image captioning with attributes",
                "authors": [
                    {
                        "first": "Ting",
                        "middle": [],
                        "last": "Yao",
                        "suffix": ""
                    },
                    {
                        "first": "Yingwei",
                        "middle": [],
                        "last": "Pan",
                        "suffix": ""
                    },
                    {
                        "first": "Yehao",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Zhaofan",
                        "middle": [],
                        "last": "Qiu",
                        "suffix": ""
                    },
                    {
                        "first": "Tao",
                        "middle": [],
                        "last": "Mei",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of International Conference on Computer Vision",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ting Yao, Yingwei Pan, Yehao Li, Zhaofan Qiu, and Tao Mei. 2017. Boosting image captioning with at- tributes. In Proceedings of International Conference on Computer Vision.",
                "links": null
            },
            "BIBREF38": {
                "ref_id": "b38",
                "title": "Learning to compose words into sentences with reinforcement learning",
                "authors": [
                    {
                        "first": "Dani",
                        "middle": [],
                        "last": "Yogatama",
                        "suffix": ""
                    },
                    {
                        "first": "Phil",
                        "middle": [],
                        "last": "Blunsom",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Dyer",
                        "suffix": ""
                    },
                    {
                        "first": "Edward",
                        "middle": [],
                        "last": "Grefenstette",
                        "suffix": ""
                    },
                    {
                        "first": "Wang",
                        "middle": [],
                        "last": "Ling",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1611.09100"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Dani Yogatama, Phil Blunsom, Chris Dyer, Edward Grefenstette, and Wang Ling. 2016. Learning to compose words into sentences with reinforcement learning. arXiv preprint arXiv:1611.09100 .",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 1: Architecture of TPGN, a TPR-capable generation network. \"2 \u00d7\" denotes the matrix-vector product.",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Figure 2: The sentence-encoding subnet S and the unbinding subnet U are inter-connected LSTMs; v encodes the visual input while the x t encode the words of the output caption.",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "where N is the total number of samples.Output: W 1,f , W1,i, W1,o, W1,c, Cs, D 1,f , D1,i, D1,o, D1,c, U 1,f , U1,i, U1,o, U1,c, w 2,f , w2,i, w2,o, w2,c, D 2,f , D2,i, D2,o, D2,c, U 2,f , U2,i, U2,o, U2,c, Wu, Wx.1: Initialize S0 by (9); 2: Initialize x0 as the one-hot vector corresponding to the start-of-sentence symbol; 3: Initialize p0 as the zero vector; 4: Randomly initialize weights W 1,f , W1,i, W1,o, W1,c, Cs, D 1,f , D1,i, D1,o, D1,c, U",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF3": {
                "num": null,
                "text": "Figure 3: Unbinding vectors of 1000 words; different POS tags of words are represented by different colors.",
                "uris": null,
                "fig_num": "3",
                "type_str": "figure"
            },
            "FIGREF4": {
                "num": null,
                "text": "Figure 4: Unbinding vectors of 360 nouns in red and 640 words of other types of POS in grey.",
                "uris": null,
                "fig_num": "4",
                "type_str": "figure"
            },
            "FIGREF5": {
                "num": null,
                "text": "Figure 5: Unbinding vectors of 81 verbs in red and 919 words of other types of POS in grey.",
                "uris": null,
                "fig_num": "5",
                "type_str": "figure"
            },
            "FIGREF6": {
                "num": null,
                "text": "Figure 6: Unbinding vectors of 55 adjectives in red and 945 words of other types of POS in grey.",
                "uris": null,
                "fig_num": "6",
                "type_str": "figure"
            },
            "FIGREF7": {
                "num": null,
                "text": "Figure 7: Unbinding vectors of 169 prepositions in red and 831 words of other types of POS in grey.",
                "uris": null,
                "fig_num": "7",
                "type_str": "figure"
            },
            "TABREF1": {
                "content": "<table><tr><td/><td>0.237</td><td>0.666</td><td>0.461</td><td>0.329</td><td>0.246</td><td>0.855</td></tr><tr><td>CNN-LSTM</td><td>0.238</td><td>0.698</td><td>0.525</td><td>0.390</td><td>0.292</td><td>0.889</td></tr><tr><td>TPGN</td><td>0.243</td><td>0.709</td><td>0.539</td><td>0.406</td><td>0.305</td><td>0.909</td></tr><tr><td colspan=\"3\">more, parameters than the CNN-LSTM. The train-</td><td/><td/><td/><td/></tr><tr><td colspan=\"3\">ing time of TPGN is roughly 50% more than the</td><td/><td/><td/><td/></tr><tr><td colspan=\"3\">CNN-LSTM model. The weights in TPGN are</td><td/><td/><td/><td/></tr><tr><td colspan=\"3\">updated at every mini-batch; in the experiments,</td><td/><td/><td/><td/></tr><tr><td colspan=\"3\">we use a batch size of 64 images. As shown</td><td/><td/><td/><td/></tr><tr><td colspan=\"3\">in Table 5.2, compared to the CNN-LSTM base-</td><td/><td/><td/><td/></tr><tr><td colspan=\"3\">line, the proposed TPGN appreciably outperforms</td><td/><td/><td/><td/></tr><tr><td colspan=\"3\">the benchmark schemes in all metrics across the</td><td/><td/><td/><td/></tr><tr><td colspan=\"3\">board. The improvement in BLEU-n is greater for</td><td/><td/><td/><td/></tr><tr><td colspan=\"3\">greater n; TPGN particularly improves generation</td><td/><td/><td/><td/></tr><tr><td colspan=\"3\">of longer subsequences. The results attest to the</td><td/><td/><td/><td/></tr><tr><td colspan=\"2\">effectiveness of the TPGN architecture.</td><td/><td/><td/><td/><td/></tr></table>",
                "type_str": "table",
                "text": "Performance of the proposed TPGN model on the COCO dataset.",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table><tr><td colspan=\"2\">Category</td><td>Nw</td><td>Nr</td><td>Pc</td></tr><tr><td colspan=\"2\">Nouns Pronouns Indefinite articles Definite articles Adjectives Verbs Prepositions &amp; conjunctions Adverbs</td><td colspan=\"3\">16683 16115 0.969 462 442 0.957 7248 7107 0.981 797 762 0.956 2543 2237 0.880 3558 3409 0.958 8184 7859 0.960 13 8 0.615</td></tr><tr><td colspan=\"3\">ID Interpretation (proportion)</td><td/></tr><tr><td>2 3 1 5 7 9 0 4 6 8</td><td colspan=\"4\">Position 1 (1.00) Position 2 (1.00) Noun (0.54), Determiner (0.43) Determiner (0.50), Noun (0.19), Preposition (0.15) Noun (0.88), Adjective (0.09) Determiner (0.90), Noun (0.10) Preposition (0.64), . (0.16), V (0.14) Preposition: spatial (0.72) non-spatial (0.19) Preposition (0.59), . (0.14) Verb (0.37), Preposition (0.36), . (0.20)</td></tr></table>",
                "type_str": "table",
                "text": "Conformity to N/V generalization (N u = 2).",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Interpretation of unbinding clusters (N u = 10)",
                "html": null,
                "num": null
            }
        }
    }
}