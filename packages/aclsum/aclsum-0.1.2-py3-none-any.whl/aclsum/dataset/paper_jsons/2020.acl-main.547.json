{
    "paper_id": "2020",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:03:06.409122Z"
    },
    "title": "Neural Graph Matching Networks for Chinese Short Text Matching",
    "authors": [
        {
            "first": "Lu",
            "middle": [],
            "last": "Chen",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Jiao Tong University",
                "location": {
                    "settlement": "Shanghai",
                    "country": "China"
                }
            },
            "email": "chenlusz@sjtu.edu.cn"
        },
        {
            "first": "Yanbin",
            "middle": [],
            "last": "Zhao",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Jiao Tong University",
                "location": {
                    "settlement": "Shanghai",
                    "country": "China"
                }
            },
            "email": "zhaoyb@sjtu.edu.cn"
        },
        {
            "first": "Boer",
            "middle": [],
            "last": "Lv",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Jiao Tong University",
                "location": {
                    "settlement": "Shanghai",
                    "country": "China"
                }
            },
            "email": "boerlv@sjtu.edu.cn"
        },
        {
            "first": "Lesheng",
            "middle": [],
            "last": "Jin",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Jiao Tong University",
                "location": {
                    "settlement": "Shanghai",
                    "country": "China"
                }
            },
            "email": ""
        },
        {
            "first": "Zhi",
            "middle": [],
            "last": "Chen",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Jiao Tong University",
                "location": {
                    "settlement": "Shanghai",
                    "country": "China"
                }
            },
            "email": ""
        },
        {
            "first": "Su",
            "middle": [],
            "last": "Zhu",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Jiao Tong University",
                "location": {
                    "settlement": "Shanghai",
                    "country": "China"
                }
            },
            "email": ""
        },
        {
            "first": "Kai",
            "middle": [],
            "last": "Yu",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Jiao Tong University",
                "location": {
                    "settlement": "Shanghai",
                    "country": "China"
                }
            },
            "email": "kai.yu@sjtu.edu.cn"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Chinese short text matching usually employs word sequences rather than character sequences to get better performance. However, Chinese word segmentation can be erroneous, ambiguous or inconsistent, which consequently hurts the final matching performance. To address this problem, we propose neural graph matching networks, a novel sentence matching framework capable of dealing with multi-granular input information. Instead of a character sequence or a single word sequence, paired word lattices formed from multiple word segmentation hypotheses are used as input and the model learns a graph representation according to an attentive graph matching mechanism. Experiments on two Chinese datasets show that our models outperform the state-of-the-art short text matching models.",
    "pdf_parse": {
        "paper_id": "2020",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Chinese short text matching usually employs word sequences rather than character sequences to get better performance. However, Chinese word segmentation can be erroneous, ambiguous or inconsistent, which consequently hurts the final matching performance. To address this problem, we propose neural graph matching networks, a novel sentence matching framework capable of dealing with multi-granular input information. Instead of a character sequence or a single word sequence, paired word lattices formed from multiple word segmentation hypotheses are used as input and the model learns a graph representation according to an attentive graph matching mechanism. Experiments on two Chinese datasets show that our models outperform the state-of-the-art short text matching models.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Short text matching (STM) is a fundamental task of natural language processing (NLP). It is usually recognized as a paraphrase identification task or a sentence semantic matching task. Given a pair of sentences, a matching model is to predict their semantic similarity. It is widely used in question answer systems and dialogue systems (Gao et al., 2019; Yu et al., 2014) .",
                "cite_spans": [
                    {
                        "start": 336,
                        "end": 354,
                        "text": "(Gao et al., 2019;",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 355,
                        "end": 371,
                        "text": "Yu et al., 2014)",
                        "ref_id": "BIBREF29"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The recent years have seen advances in deep learning methods for text matching (Mueller and Thyagarajan, 2016; Gong et al., 2017; Chen et al., 2017; Lan and Xu, 2018) . However, almost all of these models are initially proposed for English text matching. Applying them for Chinese text matching, we have two choices. One is to take Chinese characters as the input of models. Another is first to segment each sentence into words, and then to take these words as input tokens. Although character-based models can overcome the * Kai Yu is the corresponding author. problem of data sparsity to some degree (Li et al., 2019) , the main drawback of these models is that explicit word information is not fully exploited, which can be potentially useful for semantic matching. However, word-based models often suffer some potential issues caused by word segmentation. As shown in Figure 1 , the character sequence \"\u5357 \u4eac \u5e02 \u957f \u6c5f \u5927 \u6865(South Capital City Long River Big Bridge)\" has two different meanings with different word segmentation. The first one refers to a bridge (Segment-1, Segment-2), and the other refers to a person (Segment-3). The ambiguity may be eliminated with more context. Additionally, the segmentation granularity of different tools is different. For example, \"\u957f\u6c5f\u5927 \u6865(Yangtze River Bridge)\" in Segment-1 is divided into two words \"\u957f\u6c5f(Yangtze River)\" and \"\u5927 \u6865(Bridge)\" in Segment-2. It has been shown that multi-granularity information is important for text matching (Lai et al., 2019) .",
                "cite_spans": [
                    {
                        "start": 79,
                        "end": 110,
                        "text": "(Mueller and Thyagarajan, 2016;",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 111,
                        "end": 129,
                        "text": "Gong et al., 2017;",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 130,
                        "end": 148,
                        "text": "Chen et al., 2017;",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 149,
                        "end": 166,
                        "text": "Lan and Xu, 2018)",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 602,
                        "end": 619,
                        "text": "(Li et al., 2019)",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 1473,
                        "end": 1491,
                        "text": "(Lai et al., 2019)",
                        "ref_id": "BIBREF14"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 879,
                        "end": 880,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Here we propose a neural graph matching method (GMN) for Chinese short text matching. Instead of segmenting each sentence into a word sequence, we keep all possible segmentation paths to form a word lattice graph, as shown in Figure 1 . GMN takes a pair of word lattice graphs as input and updates the representations of nodes according to the graph matching attention mechanism. Also, GMN can be combined with pre-trained language models, e.g. BERT (Devlin et al., 2019) . It can be regarded as a method to integrate word information in these pre-trained language models during the fine-tuning phase. The experiments on two Chinese Datasets show that our model outperforms not only previous state-of-the-art models but also the pre-trained model BERT as well as some variants of BERT.",
                "cite_spans": [
                    {
                        "start": 450,
                        "end": 471,
                        "text": "(Devlin et al., 2019)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 233,
                        "end": 234,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "First, we define the Chinese short text matching task in a formal way. Given two Chinese sentences",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Problem Statement",
                "sec_num": "2"
            },
            {
                "text": "S a = {c a 1 , c a 2 , \u2022 \u2022 \u2022 , c a ta } and S b = {c b 1 , c b 2 , \u2022 \u2022 \u2022 , c b t b }, the goal of a text matching model f (S a , S b",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Problem Statement",
                "sec_num": "2"
            },
            {
                "text": ") is to predict whether the semantic meaning of S a and S b is equal. Here, c a i and c b j represent the i-th and j-th Chinese character in the sentences respectively, and t a and t b denote the number of characters in the sentences.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Problem Statement",
                "sec_num": "2"
            },
            {
                "text": "In this paper, we propose a graph-based matching model. Instead of segmenting each sentence into a word sequence, we keep all possible segmentation paths to form a word lattice graph G = (V, E). V is the set of nodes and includes all character subsequences that match words in a lexicon D. E is the set of edges. If a node v i \u2208 V is adjacent to another node v j \u2208 V in the original sentence, then there is an edge e ij between them. N f w (v i ) denotes the set of all reachable nodes of node v i in its forward direction, while N bw (v i ) denotes the set of all reachable nodes of node v i in its backward direction.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Problem Statement",
                "sec_num": "2"
            },
            {
                "text": "With two graphs G a = (V a , E a ) and G b = (V b , E b ), our graph matching model is to predict their similarity, which indicates whether the original sentences S a and S b have the same meaning or not.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Problem Statement",
                "sec_num": "2"
            },
            {
                "text": "As shown in Figure 2 , our model consists of three components: a contextual node embedding module (BERT), a graph matching module, and a relation classifier.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 19,
                        "end": 20,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Proposed Framework",
                "sec_num": "3"
            },
            {
                "text": "For each node v i in graphs, its initial node embedding is the attentive pooling of contextual character representations. Concretely, we first concat the original character-level sentences to form a new sequence S = ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Contextual Node Embedding",
                "sec_num": "3.1"
            },
            {
                "text": "{[CLS], c a 1 , \u2022 \u2022 \u2022 , c a ta , [SEP], c b 1 , \u2022 \u2022 \u2022 , c b t b , [SEP]",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Contextual Node Embedding",
                "sec_num": "3.1"
            },
            {
                "text": "}, and then feed them to the BERT model to obtain the contextual representations for each charater",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Contextual Node Embedding",
                "sec_num": "3.1"
            },
            {
                "text": "c CLS , c a 1 , \u2022 \u2022 \u2022 , c a ta , c SEP , c b 1 , \u2022 \u2022 \u2022 , c b t b , c SEP . Assum- ing that the node v i consists of n i consecutive character tokens {c s i , c s i +1 , \u2022 \u2022 \u2022 , c s i +n i -1 } 1 ,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Contextual Node Embedding",
                "sec_num": "3.1"
            },
            {
                "text": "a feature-wised score vector \u00fbs i +k is calculated with a feed forward network (FNN) with two layers for each character c s i +k , i.e. \u00fbs i +k = FFN(c s i +k ), and then normalized with feature-wised multidimensional softmax. The corresponding character embedding c s i +k is weighted with the normalised scores u s i +k to obtain the initial node embedding v i = n-1 k=0 u s i +k c s i +k , where represents element-wise product of two vectors.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Contextual Node Embedding",
                "sec_num": "3.1"
            },
            {
                "text": "Our proposed neural graph matching module is based on graph neural networks (GNNs) (Scarselli et al., 2009) . GNNs are widely applied in various NLP tasks, such as text classification (Yao et al., 2019) , machine translation (Marcheggiani et al., 2018) , Chinese word segmentation (Yang et al., 2019) , Chinese named entity recognition (Zhang and Yang, 2018) , dialogue policy optimization (Chen et al., 2018c (Chen et al., , 2019 (Chen et al., , 2018b)) , and dialogue state tracking (Chen et al., 2020; Zhu et al., 2020) , etc. To the best of our knowledge, we are the first to introduce GNN in Chinese shot text matching.",
                "cite_spans": [
                    {
                        "start": 83,
                        "end": 107,
                        "text": "(Scarselli et al., 2009)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 184,
                        "end": 202,
                        "text": "(Yao et al., 2019)",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 225,
                        "end": 252,
                        "text": "(Marcheggiani et al., 2018)",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 281,
                        "end": 300,
                        "text": "(Yang et al., 2019)",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 336,
                        "end": 358,
                        "text": "(Zhang and Yang, 2018)",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 390,
                        "end": 409,
                        "text": "(Chen et al., 2018c",
                        "ref_id": null
                    },
                    {
                        "start": 410,
                        "end": 430,
                        "text": "(Chen et al., , 2019",
                        "ref_id": null
                    },
                    {
                        "start": 431,
                        "end": 454,
                        "text": "(Chen et al., , 2018b))",
                        "ref_id": null
                    },
                    {
                        "start": 485,
                        "end": 504,
                        "text": "(Chen et al., 2020;",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 505,
                        "end": 522,
                        "text": "Zhu et al., 2020)",
                        "ref_id": "BIBREF31"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Neural Graph Matching Module",
                "sec_num": "3.2"
            },
            {
                "text": "The neural graph matching module takes the contextual node embedding v i as the initial representation h 0 i for the node v i , then updates its representation from one step (or layer) to the next with two sub-steps: message propagation and representation updating.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Neural Graph Matching Module",
                "sec_num": "3.2"
            },
            {
                "text": "Without loss of generality, we will use nodes in G a to describe the update process of node representations, and the update process for nodes in G b is similar. Message Propagation At l-th step, each node v i in G a will not only aggregate messages m f w i and m bw i from its reachable nodes in two directions:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Neural Graph Matching Module",
                "sec_num": "3.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "m f w i = v j \u2208N f w (v i ) \u03b1 ij W f w h l-1 j , m bw i = v k \u2208N bw (v i ) \u03b1 ik W bw h l-1 k ,",
                        "eq_num": "(1)"
                    }
                ],
                "section": "Neural Graph Matching Module",
                "sec_num": "3.2"
            },
            {
                "text": "but also aggregate messages m b1 i and m b2 i from all nodes in graph G b ,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Neural Graph Matching Module",
                "sec_num": "3.2"
            },
            {
                "text": "m b1 i = vm\u2208V b \u03b1 im W f w h l-1 m , m b2 i = vq\u2208V b \u03b1 iq W bw h l-1 q .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Neural Graph Matching Module",
                "sec_num": "3.2"
            },
            {
                "text": "(2)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Neural Graph Matching Module",
                "sec_num": "3.2"
            },
            {
                "text": "Here \u03b1 ij , \u03b1 ik , \u03b1 im and \u03b1 iq are attention coefficients (Vaswani et al., 2017) . The parameters W f w and W bw as well as the parameters for attention coefficients are shared in Eq. ( 1) and Eq. ( 2). We define",
                "cite_spans": [
                    {
                        "start": 60,
                        "end": 82,
                        "text": "(Vaswani et al., 2017)",
                        "ref_id": "BIBREF24"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Neural Graph Matching Module",
                "sec_num": "3.2"
            },
            {
                "text": "m self i [m f w i , m bw i ] and m cross i [m b1 i , m b2 i ].",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Neural Graph Matching Module",
                "sec_num": "3.2"
            },
            {
                "text": "With this sharing mechanism, the model has a nice property that, when the two graphs are perfectly matched, we have",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Neural Graph Matching Module",
                "sec_num": "3.2"
            },
            {
                "text": "m self i \u2248 m cross i",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Neural Graph Matching Module",
                "sec_num": "3.2"
            },
            {
                "text": ". The reason why they are not exactly equal is that the node v i can only aggregate messages from its reachable nodes in graph G a , while it can aggregate messages from all nodes in G b . Representation Updating After aggregating messages, each node v i will update its representation from h l-1 i to h l i . Here we first compare two messages m self i and m cross i with multi-perspective cosine distance (Wang et al., 2017) , ",
                "cite_spans": [
                    {
                        "start": 407,
                        "end": 426,
                        "text": "(Wang et al., 2017)",
                        "ref_id": "BIBREF25"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Neural Graph Matching Module",
                "sec_num": "3.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "d k = cosine w cos k m self i , w cos k m cross i ,",
                        "eq_num": "(3)"
                    }
                ],
                "section": "Neural Graph Matching Module",
                "sec_num": "3.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "h l i = FFN m self i , d i ,",
                        "eq_num": "(4)"
                    }
                ],
                "section": "Neural Graph Matching Module",
                "sec_num": "3.2"
            },
            {
                "text": "where [\u2022, \u2022] denotes the concatation of two vectors,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Neural Graph Matching Module",
                "sec_num": "3.2"
            },
            {
                "text": "d i [d 1 , d 2 , \u2022 \u2022 \u2022 , d P ].",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Neural Graph Matching Module",
                "sec_num": "3.2"
            },
            {
                "text": "FFN is a feed forward network with two layers. After updating node representation L steps, we will obtain the graph-aware representation h L i for each node v i . h L i includes not only the information from its reachable nodes but also information of pairwise comparison with all nodes in another graph. The graph level representations g a and g b for two graphs G a and G b are computed by attentive pooling of representations of all nodes in each graph.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Neural Graph Matching Module",
                "sec_num": "3.2"
            },
            {
                "text": "With two graph level representations g a and g b , we can predict the similarity of two graphs or sentences, p = FFN g a , g b , g a g b , |g ag b | , (5) where p \u2208 [0, 1]. During the training phase, the training object is to minimize the binary crossentropy loss.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Relation Classifier",
                "sec_num": "3.3"
            },
            {
                "text": "Dataset We conduct experiments on two Chinese datasets for semantic textual similarity: LCQMC (Liu et al., 2018) and BQ (Chen et al., 2018a) . LCQMC is a large-scale open-domain corpus for question matching, while BQ is a domain-specific corpus for bank question matching. The sample in both datasets contains a pair of sentences and a binary label indicating whether the two sentences have the same meaning or share the same intention. All features of the two datasets are summarized in Table 1 . For each dataset, the accuracy (ACC) and F1 score are used as the evaluation metrics. Hyper-parameters The number of graph updating steps/layers L is 2 on both datasets. The dimension of node representation is 128. The dropout rate for all hidden layers is 0.2. The number of matching perspectives P is 20. Each model is trained by RMSProp with an initial learning rate of 0.0001 and a batch size of 32. We use the vocabulary provided by Song et al. (2018) to build the lattice.",
                "cite_spans": [
                    {
                        "start": 94,
                        "end": 112,
                        "text": "(Liu et al., 2018)",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 120,
                        "end": 140,
                        "text": "(Chen et al., 2018a)",
                        "ref_id": null
                    },
                    {
                        "start": 936,
                        "end": 954,
                        "text": "Song et al. (2018)",
                        "ref_id": "BIBREF22"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 494,
                        "end": 495,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Experimental Setup",
                "sec_num": "4.1"
            },
            {
                "text": "We compare our models with two types of baselines: basic neural models without pre-training and BERT-based models pre-trained on largescale corpora. The basic neural approaches also can be divided into two groups: representationbased models and interaction-based models. The representation-based models calculate the sentence representations independently and use the distance as the similarity score. Such models include Text-CNN (Kim, 2014) , BiLSTM (Graves and Schmidhuber, 2005) and Lattice-CNN (Lai et al., 2019) . Note that Lattice-CNN also takes word lattices as input. The interaction-based models consider the interaction between two sentences when calculating sentence representations, which include BiMPM (Wang et al., 2017) and ESIM (Chen et al., 2017) . ESIM has achieved state-of-the-art results on various matching tasks (Bowman et al., 2015; Chen and Wang, 2019; Williams et al., 2018) . For pre-trained models, we consider BERT and its several variants such as BERT-wmm (Cui et al., 2019) , BERT-wmm-ext (Cui et al., 2019) and ERNIE (Sun et al., 2019; Cui et al., 2019) . One common feature of these variants of BERT is that they all use word information during the pre-trained phase. We use GMN-BERT to denote our proposed model. We also employ a character-level transformer encoder instead of BERT as the contextual node embedding module described in Section 3.1, which is denoted as GMN. The comparison results are reported in Table 2 .",
                "cite_spans": [
                    {
                        "start": 431,
                        "end": 442,
                        "text": "(Kim, 2014)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 452,
                        "end": 482,
                        "text": "(Graves and Schmidhuber, 2005)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 499,
                        "end": 517,
                        "text": "(Lai et al., 2019)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 716,
                        "end": 735,
                        "text": "(Wang et al., 2017)",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 745,
                        "end": 764,
                        "text": "(Chen et al., 2017)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 836,
                        "end": 857,
                        "text": "(Bowman et al., 2015;",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 858,
                        "end": 878,
                        "text": "Chen and Wang, 2019;",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 879,
                        "end": 901,
                        "text": "Williams et al., 2018)",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 987,
                        "end": 1005,
                        "text": "(Cui et al., 2019)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 1021,
                        "end": 1039,
                        "text": "(Cui et al., 2019)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 1050,
                        "end": 1068,
                        "text": "(Sun et al., 2019;",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 1069,
                        "end": 1086,
                        "text": "Cui et al., 2019)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 1453,
                        "end": 1454,
                        "text": "2",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Main Results",
                "sec_num": "4.2"
            },
            {
                "text": "From the first part of the results, we can find that our GMN performs better than five baselines on both datasets. Also, the interaction-based models in general outperform the representation based models. Although Lattice-CNN2 also utilizes word lattices, it has no node-level comparison due to the limits of its structure, which causes significant performance degradation. As for interactionbased models, although they both use the multiperspective matching mechanism, GMN outperforms BiMPM and ESIM (char and word)3 , which indicates that the utilization of word lattice with our neural graph matching networks is powerful.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Main Results",
                "sec_num": "4.2"
            },
            {
                "text": "From the second part of Table 2 , we can find that the three variants of BERT (BERT-wwm, BERTwwn-ext, ERNIE)4 all outperform the original BERT, which indicates using word-level information during pre-training is important for Chinese matching tasks. Our model GMN-BERT performs better than all these BERT-based models. It shows that utilizing word information during the finetuning phase with GMN is an effective way to boost the performance of BERT for Chinese semantic matching. ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 30,
                        "end": 31,
                        "text": "2",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Main Results",
                "sec_num": "4.2"
            },
            {
                "text": "In this section, we investigate the effect of word segmentation on our model GMN. A word sequence can be regarded as a thin graph. Therefore, it can be used to replace the word lattice as the input of GMN. As shown in Figure 3 , we compare four models: Lattice is our GMN with word lattice as the input. PKU and JIEBA are similar to Lattice except that their input is word sequence produced by two word segmentation tools: Jieba5 and pkuseg (Luo et al., 2019) , while the input of JIEBA+PKU is a small lattice graph generated by merging two word segmentation results. We can find that lattice-based models (Lattice and JIEBA+PKU) performs much better then wordbased models (PKU and JIEBA). We can also find that the performance of PKU+JIEBA is very close to the performance of Lattice. The union of different word segmentation results can be regarded as a tiny lattice, which is usually the sub-graph of the overall lattice. Compared with the tiny graph, the overall lattice has more noisy nodes (i.e. invalid words in the corresponding sentence). Therefore We think it is reasonable that the performance of tiny lattice (PKU+JIEBA) is comparable to the performance of the overall lattice (Lattice). On",
                "cite_spans": [
                    {
                        "start": 441,
                        "end": 459,
                        "text": "(Luo et al., 2019)",
                        "ref_id": "BIBREF18"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 225,
                        "end": 226,
                        "text": "3",
                        "ref_id": "FIGREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Analysis",
                "sec_num": "4.3"
            },
            {
                "text": "In this paper, we propose a neural graph matching model for Chinese short text matching. It takes a pair of word lattices as input instead of word or character sequences. The utilization of word lattice can provide more multi-granularity information and avoid the error propagation issue of word segmentation. Additionally, our model and the pre-training model are complementary. It can be regarded as a flexible method to introduce word information into BERT during the fine-tuning phase. The experimental results show that our model outperforms the state-of-the-art text matching models as well as some BERT-based models.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "5"
            },
            {
                "text": "Here si denotes the index of the first character of vi in the sentence S a or S b . For brevity, the superscript of c s i +k is omitted.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "The results of Lattice-CNN is produced by the open source code https://github.com/Erutan-pku/LCN-for-Chinese-QA.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "3 The results of ESIM is produced by the open source code https://github.com/lanwuwei/SPM toolkit",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": ".4 The results of BERT-wwm, BERT-wwm-ext and ERNIE are taken from the paper(Cui et al., 2019).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://github.com/fxsjy/jieba the other hand, this indicates that our model has the ability to deal with the introduced noisy information in the lattice graph. In Figure4, we give two examples to show that word segmentation errors result in incorrect prediction of JIEBA, while Lattice can give the right answers.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "This work has been supported by the National Key Research and Development Program of China (Grant No. 2017YFB1002102) and Shanghai Jiao Tong University Scientific and Technological Innovation Funds (YG2020YQ01).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgments",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "A large annotated corpus for learning natural language inference",
                "authors": [
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Samuel",
                        "suffix": ""
                    },
                    {
                        "first": "Gabor",
                        "middle": [],
                        "last": "Bowman",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [],
                        "last": "Angeli",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Potts",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large anno- tated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "2018a. The bq corpus: A large-scale domain-specific chinese corpus for sentence semantic equivalence identification",
                "authors": [
                    {
                        "first": "Jing",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Qingcai",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Xin",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Haijun",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Daohe",
                        "middle": [],
                        "last": "Lu",
                        "suffix": ""
                    },
                    {
                        "first": "Buzhou",
                        "middle": [],
                        "last": "Tang",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "volume": "",
                "issue": "",
                "pages": "4946--4951",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jing Chen, Qingcai Chen, Xin Liu, Haijun Yang, Daohe Lu, and Buzhou Tang. 2018a. The bq cor- pus: A large-scale domain-specific chinese corpus for sentence semantic equivalence identification. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4946-4951.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Policy adaptation for deep reinforcement learning-based dialogue management",
                "authors": [
                    {
                        "first": "Lu",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Cheng",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "Zhi",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Bowen",
                        "middle": [],
                        "last": "Tan",
                        "suffix": ""
                    },
                    {
                        "first": "Milica",
                        "middle": [],
                        "last": "Ga\u0161i\u0107",
                        "suffix": ""
                    },
                    {
                        "first": "Kai",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP)",
                "volume": "",
                "issue": "",
                "pages": "6074--6078",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Lu Chen, Cheng Chang, Zhi Chen, Bowen Tan, Mil- ica Ga\u0161i\u0107, and Kai Yu. 2018b. Policy adaptation for deep reinforcement learning-based dialogue man- agement. In Proceedings of IEEE International Con- ference on Acoustics Speech and Signal Processing (ICASSP), pages 6074-6078. IEEE.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Agentgraph: Towards universal dialogue management with structured deep reinforcement learning",
                "authors": [
                    {
                        "first": "Lu",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Zhi",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Bowen",
                        "middle": [],
                        "last": "Tan",
                        "suffix": ""
                    },
                    {
                        "first": "Sishan",
                        "middle": [],
                        "last": "Long",
                        "suffix": ""
                    },
                    {
                        "first": "Milica",
                        "middle": [],
                        "last": "Gasic",
                        "suffix": ""
                    },
                    {
                        "first": "Kai",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing",
                "volume": "27",
                "issue": "9",
                "pages": "1378--1391",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Lu Chen, Zhi Chen, Bowen Tan, Sishan Long, Mil- ica Gasic, and Kai Yu. 2019. Agentgraph: To- wards universal dialogue management with struc- tured deep reinforcement learning. IEEE/ACM Transactions on Audio, Speech, and Language Pro- cessing, 27(9):1378-1391.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Schema-guided multi-domain dialogue state tracking with graph attention neural networks",
                "authors": [
                    {
                        "first": "Lu",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Boer",
                        "middle": [],
                        "last": "Lv",
                        "suffix": ""
                    },
                    {
                        "first": "Chi",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Su",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    },
                    {
                        "first": "Bowen",
                        "middle": [],
                        "last": "Tan",
                        "suffix": ""
                    },
                    {
                        "first": "Kai",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "The Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Lu Chen, Boer Lv, Chi Wang, Su Zhu, Bowen Tan, and Kai Yu. 2020. Schema-guided multi-domain di- alogue state tracking with graph attention neural net- works. In The Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI).",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Structured dialogue policy with graph neural networks",
                "authors": [
                    {
                        "first": "Lu",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Bowen",
                        "middle": [],
                        "last": "Tan",
                        "suffix": ""
                    },
                    {
                        "first": "Sishan",
                        "middle": [],
                        "last": "Long",
                        "suffix": ""
                    },
                    {
                        "first": "Kai",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 27th International Conference on Computational Linguistics (COLING)",
                "volume": "",
                "issue": "",
                "pages": "1257--1268",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Lu Chen, Bowen Tan, Sishan Long, and Kai Yu. 2018c. Structured dialogue policy with graph neu- ral networks. In Proceedings of the 27th Inter- national Conference on Computational Linguistics (COLING), pages 1257-1268.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Sequential matching model for end-to-end multi-turn response selection",
                "authors": [
                    {
                        "first": "Qian",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Wen",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
                "volume": "",
                "issue": "",
                "pages": "7350--7354",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Qian Chen and Wen Wang. 2019. Sequential match- ing model for end-to-end multi-turn response selec- tion. In ICASSP 2019-2019 IEEE International Con- ference on Acoustics, Speech and Signal Processing (ICASSP), pages 7350-7354. IEEE.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Enhanced lstm for natural language inference",
                "authors": [
                    {
                        "first": "Qian",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaodan",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    },
                    {
                        "first": "Zhen-Hua",
                        "middle": [],
                        "last": "Ling",
                        "suffix": ""
                    },
                    {
                        "first": "Si",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Hui",
                        "middle": [],
                        "last": "Jiang",
                        "suffix": ""
                    },
                    {
                        "first": "Diana",
                        "middle": [],
                        "last": "Inkpen",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
                "volume": "1",
                "issue": "",
                "pages": "1657--1668",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui Jiang, and Diana Inkpen. 2017. Enhanced lstm for natural language inference. In Proceedings of the 55th Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers), pages 1657-1668.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Pre-training with whole word masking for chinese bert",
                "authors": [
                    {
                        "first": "Yiming",
                        "middle": [],
                        "last": "Cui",
                        "suffix": ""
                    },
                    {
                        "first": "Wanxiang",
                        "middle": [],
                        "last": "Che",
                        "suffix": ""
                    },
                    {
                        "first": "Ting",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Bing",
                        "middle": [],
                        "last": "Qin",
                        "suffix": ""
                    },
                    {
                        "first": "Ziqing",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Shijin",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Guoping",
                        "middle": [],
                        "last": "Hu",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1906.08101"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing Yang, Shijin Wang, and Guoping Hu. 2019. Pre-training with whole word masking for chinese bert. arXiv preprint arXiv:1906.08101.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
                "authors": [
                    {
                        "first": "Jacob",
                        "middle": [],
                        "last": "Devlin",
                        "suffix": ""
                    },
                    {
                        "first": "Ming-Wei",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "Kenton",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Kristina",
                        "middle": [],
                        "last": "Toutanova",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "volume": "1",
                "issue": "",
                "pages": "4171--4186",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 1 (Long and Short Papers), pages 4171-4186.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Neural approaches to conversational ai",
                "authors": [
                    {
                        "first": "Jianfeng",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Michel",
                        "middle": [],
                        "last": "Galley",
                        "suffix": ""
                    },
                    {
                        "first": "Lihong",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Foundations and Trends R in Information Retrieval",
                "volume": "13",
                "issue": "2-3",
                "pages": "127--298",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jianfeng Gao, Michel Galley, Lihong Li, et al. 2019. Neural approaches to conversational ai. Founda- tions and Trends R in Information Retrieval, 13(2- 3):127-298.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Natural language inference over interaction space",
                "authors": [
                    {
                        "first": "Yichen",
                        "middle": [],
                        "last": "Gong",
                        "suffix": ""
                    },
                    {
                        "first": "Heng",
                        "middle": [],
                        "last": "Luo",
                        "suffix": ""
                    },
                    {
                        "first": "Jian",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1709.04348"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yichen Gong, Heng Luo, and Jian Zhang. 2017. Natu- ral language inference over interaction space. arXiv preprint arXiv:1709.04348.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures",
                "authors": [
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Graves",
                        "suffix": ""
                    },
                    {
                        "first": "J\u00fcrgen",
                        "middle": [],
                        "last": "Schmidhuber",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Neural networks",
                "volume": "18",
                "issue": "5-6",
                "pages": "602--610",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alex Graves and J\u00fcrgen Schmidhuber. 2005. Frame- wise phoneme classification with bidirectional lstm and other neural network architectures. Neural net- works, 18(5-6):602-610.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Convolutional neural networks for sentence classification",
                "authors": [
                    {
                        "first": "Yoon",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "volume": "",
                "issue": "",
                "pages": "1746--1751",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yoon Kim. 2014. Convolutional neural networks for sentence classification. In Proceedings of the 2014 Conference on Empirical Methods in Natural Lan- guage Processing (EMNLP), pages 1746-1751.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Lattice cnns for matching based chinese question answering",
                "authors": [
                    {
                        "first": "Yuxuan",
                        "middle": [],
                        "last": "Lai",
                        "suffix": ""
                    },
                    {
                        "first": "Yansong",
                        "middle": [],
                        "last": "Feng",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaohan",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Zheng",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Kun",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Dongyan",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1902.09087"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yuxuan Lai, Yansong Feng, Xiaohan Yu, Zheng Wang, Kun Xu, and Dongyan Zhao. 2019. Lattice cnns for matching based chinese question answering. arXiv preprint arXiv:1902.09087.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Neural network models for paraphrase identification, semantic textual similarity, natural language inference, and question answering",
                "authors": [
                    {
                        "first": "Wuwei",
                        "middle": [],
                        "last": "Lan",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 27th International Conference on Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "3890--3902",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Wuwei Lan and Wei Xu. 2018. Neural network models for paraphrase identification, semantic textual simi- larity, natural language inference, and question an- swering. In Proceedings of the 27th International Conference on Computational Linguistics, pages 3890-3902.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Is word segmentation necessary for deep learning of chinese representations?",
                "authors": [
                    {
                        "first": "Xiaoya",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Yuxian",
                        "middle": [],
                        "last": "Meng",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaofei",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Qinghong",
                        "middle": [],
                        "last": "Han",
                        "suffix": ""
                    },
                    {
                        "first": "Arianna",
                        "middle": [],
                        "last": "Yuan",
                        "suffix": ""
                    },
                    {
                        "first": "Jiwei",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "3242--3252",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xiaoya Li, Yuxian Meng, Xiaofei Sun, Qinghong Han, Arianna Yuan, and Jiwei Li. 2019. Is word segmen- tation necessary for deep learning of chinese repre- sentations? In Proceedings of the 57th Annual Meet- ing of the Association for Computational Linguistics, pages 3242-3252.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Lcqmc: A large-scale chinese question matching corpus",
                "authors": [
                    {
                        "first": "Xin",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Qingcai",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Chong",
                        "middle": [],
                        "last": "Deng",
                        "suffix": ""
                    },
                    {
                        "first": "Huajun",
                        "middle": [],
                        "last": "Zeng",
                        "suffix": ""
                    },
                    {
                        "first": "Jing",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Dongfang",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Buzhou",
                        "middle": [],
                        "last": "Tang",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 27th International Conference on Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "1952--1962",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xin Liu, Qingcai Chen, Chong Deng, Huajun Zeng, Jing Chen, Dongfang Li, and Buzhou Tang. 2018. Lcqmc: A large-scale chinese question matching corpus. In Proceedings of the 27th International Conference on Computational Linguistics, pages 1952-1962.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Pkuseg: A toolkit for multi-domain chinese word segmentation",
                "authors": [
                    {
                        "first": "Ruixuan",
                        "middle": [],
                        "last": "Luo",
                        "suffix": ""
                    },
                    {
                        "first": "Jingjing",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Yi",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Xuancheng",
                        "middle": [],
                        "last": "Ren",
                        "suffix": ""
                    },
                    {
                        "first": "Xu",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ruixuan Luo, Jingjing Xu, Yi Zhang, Xuancheng Ren, and Xu Sun. 2019. Pkuseg: A toolkit for multi-domain chinese word segmentation. CoRR, abs/1906.11455.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Exploiting semantics in neural machine translation with graph convolutional networks",
                "authors": [
                    {
                        "first": "Diego",
                        "middle": [],
                        "last": "Marcheggiani",
                        "suffix": ""
                    },
                    {
                        "first": "Joost",
                        "middle": [],
                        "last": "Bastings",
                        "suffix": ""
                    },
                    {
                        "first": "Ivan",
                        "middle": [],
                        "last": "Titov",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1804.08313"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Diego Marcheggiani, Joost Bastings, and Ivan Titov. 2018. Exploiting semantics in neural machine trans- lation with graph convolutional networks. arXiv preprint arXiv:1804.08313.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Siamese recurrent architectures for learning sentence similarity",
                "authors": [
                    {
                        "first": "Jonas",
                        "middle": [],
                        "last": "Mueller",
                        "suffix": ""
                    },
                    {
                        "first": "Aditya",
                        "middle": [],
                        "last": "Thyagarajan",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Thirtieth AAAI Conference on Artificial Intelligence",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jonas Mueller and Aditya Thyagarajan. 2016. Siamese recurrent architectures for learning sentence similar- ity. In Thirtieth AAAI Conference on Artificial Intel- ligence.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "The graph neural network model",
                "authors": [
                    {
                        "first": "Franco",
                        "middle": [],
                        "last": "Scarselli",
                        "suffix": ""
                    },
                    {
                        "first": "Marco",
                        "middle": [],
                        "last": "Gori",
                        "suffix": ""
                    },
                    {
                        "first": "Ah",
                        "middle": [],
                        "last": "Chung Tsoi",
                        "suffix": ""
                    },
                    {
                        "first": "Markus",
                        "middle": [],
                        "last": "Hagenbuchner",
                        "suffix": ""
                    },
                    {
                        "first": "Gabriele",
                        "middle": [],
                        "last": "Monfardini",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "IEEE Transactions on Neural Networks",
                "volume": "20",
                "issue": "1",
                "pages": "61--80",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. 2009. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61-80.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Directional skip-gram: Explicitly distinguishing left and right context for word embeddings",
                "authors": [
                    {
                        "first": "Yan",
                        "middle": [],
                        "last": "Song",
                        "suffix": ""
                    },
                    {
                        "first": "Shuming",
                        "middle": [],
                        "last": "Shi",
                        "suffix": ""
                    },
                    {
                        "first": "Jing",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Haisong",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "volume": "2",
                "issue": "",
                "pages": "175--180",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yan Song, Shuming Shi, Jing Li, and Haisong Zhang. 2018. Directional skip-gram: Explicitly distinguish- ing left and right context for word embeddings. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 175-180.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Ernie: Enhanced representation through knowledge integration",
                "authors": [
                    {
                        "first": "Yu",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Shuohuan",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Yukun",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Shikun",
                        "middle": [],
                        "last": "Feng",
                        "suffix": ""
                    },
                    {
                        "first": "Xuyi",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Han",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Xin",
                        "middle": [],
                        "last": "Tian",
                        "suffix": ""
                    },
                    {
                        "first": "Danxiang",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    },
                    {
                        "first": "Hua",
                        "middle": [],
                        "last": "Hao Tian",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1904.09223"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, and Hua Wu. 2019. Ernie: Enhanced rep- resentation through knowledge integration. arXiv preprint arXiv:1904.09223.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Attention is all you need",
                "authors": [
                    {
                        "first": "Ashish",
                        "middle": [],
                        "last": "Vaswani",
                        "suffix": ""
                    },
                    {
                        "first": "Noam",
                        "middle": [],
                        "last": "Shazeer",
                        "suffix": ""
                    },
                    {
                        "first": "Niki",
                        "middle": [],
                        "last": "Parmar",
                        "suffix": ""
                    },
                    {
                        "first": "Jakob",
                        "middle": [],
                        "last": "Uszkoreit",
                        "suffix": ""
                    },
                    {
                        "first": "Llion",
                        "middle": [],
                        "last": "Jones",
                        "suffix": ""
                    },
                    {
                        "first": "Aidan",
                        "middle": [
                            "N"
                        ],
                        "last": "Gomez",
                        "suffix": ""
                    },
                    {
                        "first": "\u0141ukasz",
                        "middle": [],
                        "last": "Kaiser",
                        "suffix": ""
                    },
                    {
                        "first": "Illia",
                        "middle": [],
                        "last": "Polosukhin",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Advances in neural information processing systems",
                "volume": "",
                "issue": "",
                "pages": "5998--6008",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information pro- cessing systems, pages 5998-6008.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Bilateral multi-perspective matching for natural language sentences",
                "authors": [
                    {
                        "first": "Zhiguo",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Wael",
                        "middle": [],
                        "last": "Hamza",
                        "suffix": ""
                    },
                    {
                        "first": "Radu",
                        "middle": [],
                        "last": "Florian",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the 26th International Joint Conference on Artificial Intelligence",
                "volume": "",
                "issue": "",
                "pages": "4144--4150",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zhiguo Wang, Wael Hamza, and Radu Florian. 2017. Bilateral multi-perspective matching for natural lan- guage sentences. In Proceedings of the 26th Inter- national Joint Conference on Artificial Intelligence, pages 4144-4150.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "A broad-coverage challenge corpus for sentence understanding through inference",
                "authors": [
                    {
                        "first": "Adina",
                        "middle": [],
                        "last": "Williams",
                        "suffix": ""
                    },
                    {
                        "first": "Nikita",
                        "middle": [],
                        "last": "Nangia",
                        "suffix": ""
                    },
                    {
                        "first": "Samuel",
                        "middle": [],
                        "last": "Bowman",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "volume": "1",
                "issue": "",
                "pages": "1112--1122",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sen- tence understanding through inference. In Proceed- ings of the 2018 Conference of the North American Chapter of the Association for Computational Lin- guistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112-1122. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Subword encoding in lattice lstm for chinese word segmentation",
                "authors": [
                    {
                        "first": "Jie",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Yue",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Shuailong",
                        "middle": [],
                        "last": "Liang",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 2019 Conference of the North American Chapter",
                "volume": "1",
                "issue": "",
                "pages": "2720--2725",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jie Yang, Yue Zhang, and Shuailong Liang. 2019. Sub- word encoding in lattice lstm for chinese word seg- mentation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2720-2725.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Graph convolutional networks for text classification",
                "authors": [
                    {
                        "first": "Liang",
                        "middle": [],
                        "last": "Yao",
                        "suffix": ""
                    },
                    {
                        "first": "Chengsheng",
                        "middle": [],
                        "last": "Mao",
                        "suffix": ""
                    },
                    {
                        "first": "Yuan",
                        "middle": [],
                        "last": "Luo",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of AAAI",
                "volume": "",
                "issue": "",
                "pages": "7370--7377",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Liang Yao, Chengsheng Mao, and Yuan Luo. 2019. Graph convolutional networks for text classification. In Proceedings of AAAI, pages 7370-7377.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Cognitive technology in task-oriented dialogue systems: Concepts, advances and future",
                "authors": [
                    {
                        "first": "Kai",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Lu",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Bo",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Kai",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Su",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "",
                "volume": "37",
                "issue": "",
                "pages": "1--17",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kai Yu, Lu Chen, Bo Chen, Kai Sun, and Su Zhu. 2014. Cognitive technology in task-oriented dialogue sys- tems: Concepts, advances and future. Chinese Jour- nal of Computers, 37(18):1-17.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "Chinese ner using lattice lstm",
                "authors": [
                    {
                        "first": "Yue",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Jie",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
                "volume": "1",
                "issue": "",
                "pages": "1554--1564",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yue Zhang and Jie Yang. 2018. Chinese ner using lat- tice lstm. In Proceedings of the 56th Annual Meet- ing of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1554-1564.",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "Efficient context and schema fusion networks for multidomain dialogue state tracking",
                "authors": [
                    {
                        "first": "Su",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    },
                    {
                        "first": "Jieyu",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Lu",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Kai",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2004.03386"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Su Zhu, Jieyu Li, Lu Chen, and Kai Yu. 2020. Effi- cient context and schema fusion networks for multi- domain dialogue state tracking. arXiv preprint arXiv:2004.03386.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 1: An example of the word segmentation and the corresponding word lattice",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Figure 2: Overview of our proposed framework",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "Figure 3: Performance (ACC) of GMN with different inputs on LCQMC dataset",
                "uris": null,
                "fig_num": "3",
                "type_str": "figure"
            },
            "FIGREF3": {
                "num": null,
                "text": "Figure 4: Examples of different prediction of Jieba and Lattice",
                "uris": null,
                "fig_num": "4",
                "type_str": "figure"
            },
            "TABREF1": {
                "content": "<table><tr><td>Models</td><td colspan=\"4\">BQ ACC. F1 ACC. F1 LCQMC</td></tr><tr><td>Text-CNN</td><td colspan=\"4\">68.5 69.2 72.8 75.7</td></tr><tr><td>BiLSTM</td><td colspan=\"4\">73.5 72.7 76.1 78.9</td></tr><tr><td>Lattice-CNN</td><td colspan=\"4\">78.2 78.3 82.1 82.4</td></tr><tr><td>BiMPM</td><td colspan=\"4\">81.9 81.7 83.3 84.9</td></tr><tr><td>ESIM-char</td><td colspan=\"4\">79.2 79.3 82.0 84.0</td></tr><tr><td>ESIM-word</td><td colspan=\"4\">81.9 81.9 82.6 84.5</td></tr><tr><td>GMN (Ours)</td><td colspan=\"4\">84.2 84.1 84.6 86.0</td></tr><tr><td>BERT</td><td colspan=\"4\">84.5 84.0 85.7 86.8</td></tr><tr><td>BERT-wwm</td><td>84.9</td><td>-</td><td>86.8</td><td>-</td></tr><tr><td>BERT-wwm-ext</td><td>84.8</td><td>-</td><td>86.6</td><td>-</td></tr><tr><td>ERNIE</td><td>84.6</td><td>-</td><td>87.0</td><td>-</td></tr><tr><td colspan=\"5\">GMN-BERT (Ours) 85.6 85.5 87.3 88.0</td></tr></table>",
                "type_str": "table",
                "text": "Performance of various models on LCQMC and BQ test datasets",
                "html": null,
                "num": null
            }
        }
    }
}