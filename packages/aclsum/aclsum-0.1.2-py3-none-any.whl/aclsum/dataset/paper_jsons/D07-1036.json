{
    "paper_id": "D07-1036",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:00:14.878808Z"
    },
    "title": "Improving Statistical Machine Translation Performance by Training Data Selection and Optimization",
    "authors": [
        {
            "first": "Yajuan",
            "middle": [],
            "last": "L\u00fc",
            "suffix": "",
            "affiliation": {},
            "email": "lvyajuan@ict.ac.cn"
        },
        {
            "first": "Jin",
            "middle": [],
            "last": "Huang",
            "suffix": "",
            "affiliation": {},
            "email": "huangjin@ict.ac.cn"
        },
        {
            "first": "Qun",
            "middle": [],
            "last": "Liu",
            "suffix": "",
            "affiliation": {},
            "email": "liuqun@ict.ac.cn"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Parallel corpus is an indispensable resource for translation model training in statistical machine translation (SMT). Instead of collecting more and more parallel training corpora, this paper aims to improve SMT performance by exploiting full potential of the existing parallel corpora. Two kinds of methods are proposed: offline data optimization and online model optimization. The offline method adapts the training data by redistributing the weight of each training sentence pairs. The online method adapts the translation model by redistributing the weight of each predefined submodels. Information retrieval model is used for the weighting scheme in both methods. Experimental results show that without using any additional resource, both methods can improve SMT performance significantly.",
    "pdf_parse": {
        "paper_id": "D07-1036",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Parallel corpus is an indispensable resource for translation model training in statistical machine translation (SMT). Instead of collecting more and more parallel training corpora, this paper aims to improve SMT performance by exploiting full potential of the existing parallel corpora. Two kinds of methods are proposed: offline data optimization and online model optimization. The offline method adapts the training data by redistributing the weight of each training sentence pairs. The online method adapts the translation model by redistributing the weight of each predefined submodels. Information retrieval model is used for the weighting scheme in both methods. Experimental results show that without using any additional resource, both methods can improve SMT performance significantly.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Statistical machine translation relies heavily on the available training data. Typically, the more data is used to estimate the parameters of the translation model, the better it can approximate the true translation probabilities, which will obviously lead to a higher translation performance. However, large corpora are not easily available. The collected corpora are usually from very different areas. For example, the parallel corpora provided by LDC come from quite different domains, such as Hongkong laws, Hangkong Hansards and Hongkong news. This results in the problem that a translation system trained on data from a particular domain(e.g. Hongkong Hansards) will perform poorly when translating text from a different domain(e.g. news articles). Our experiments also show that simply putting all these domain specific corpora together will not always improve translation quality. From another aspect, larger amount of training data also requires larger computational resources. With the increasing of training data, the improvement of translation quality will become smaller and smaller. Therefore, while keeping collecting more and more parallel corpora, it is also important to seek effective ways of making better use of available parallel training data.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "There are two cases when we train a SMT system. In one case, we know the target test set or target test domain, for example, when building a specific domain SMT system or when participating the NIST MT evaluation1 . In the other case, we are unaware of any information of the testing data. This paper presents two methods to exploit full potential of the available parallel corpora in the two cases. For the first case, we try to optimize the training data offline to make it match the test data better in domain, topic and style, thus improving the translation performance. For the second case, we first divide the training data into several domains and train submodels for each domain. Then, in the translation process, we try to optimize the predefined models according to the online input source sentence. Information retrieval model is used for similar sentences retrieval in both methods. Our preliminary experiments show that both methods can improve SMT performance without using any additional data.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The remainder of this paper is organized as follows: Section 2 describes the offline data selection and optimization method. Section 3 describes the online model optimization method. The evaluation and discussion are given in section 4. Related work is introduced before concluding.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In offline training data optimization, we assume that the target test data or target test domain is known before building the translation model. We first select sentences similar to the test text using information retrieval method to construct a small and adapted training data. Then the extracted similar subset is used to optimize the distribution of the whole training data. The adapted and the optimized training data will be used to train new translation models.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Offline training data optimization",
                "sec_num": "2"
            },
            {
                "text": "We use information retrieval method for similar data retrieval. The standard TF-IDF (Term Frequency and Inverse Document Frequency) term weighting scheme is used to measure the similarity between the test sentence and the training sentence.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Similar data selection using TF-IDF",
                "sec_num": "2.1"
            },
            {
                "text": "TF-IDF is a similarity measure widely used in information retrieval. Each document i is represented as a vector , is the size of the vocabulary.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Similar data selection using TF-IDF",
                "sec_num": "2.1"
            },
            {
                "text": "is calculate as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Similar data selection using TF-IDF",
                "sec_num": "2.1"
            },
            {
                "text": "D ) ,..., , ( 2 1 in i i w w w n ij w ) log( j ij ij idf tf w \u00d7 =",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Similar data selection using TF-IDF",
                "sec_num": "2.1"
            },
            {
                "text": "where, ij is the term frequency(TF) of the j-th word in the vocabulary in the document , i.e. the number of occurrences; tf i D j is the inverse document frequency(IDF) of the j-th word calculated as below:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Similar data selection using TF-IDF",
                "sec_num": "2.1"
            },
            {
                "text": "idf th term - j # # containing documents documents idf j = .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Similar data selection using TF-IDF",
                "sec_num": "2.1"
            },
            {
                "text": "The similarity between two documents is then defined as the cosine of the angle between the two vectors.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Similar data selection using TF-IDF",
                "sec_num": "2.1"
            },
            {
                "text": "We perform information retrieval using the Lemur toolkit2 . The source language part of the parallel training data is used as the document collection. Each sentence represents one document. Each sentence from the test data or test domain is used as one separate query. In the sentence retrieval process, both the query and the document are converted into vectors by assigning a term weight to each word. Then the cosine similarity is calculated proportional to the inner product of the two vectors. All retrieved sentences are ranked according to their similarity with the query. We pair each of the retrieved sentences with the corresponding target part and the top N most similar sentences pairs are put together to form an adapted parallel data. N ranges from one to several thousand in our experiments. Since Lemur toolkit gives the similarity score for each retrieved sentences, it is also possible to select the most similar sentences according to the similarity score.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Similar data selection using TF-IDF",
                "sec_num": "2.1"
            },
            {
                "text": "Note that the selected similar data can contain duplicate sentences as the top N retrieval results for different test sentences can contain the same training sentences. The duplicate sentences will force the translation probability towards the more often seen words. Intuitively, this could help. In experiment section, we will compare experimental results by keeping or removing duplicates to see how the duplicate sentences affect the translations.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Similar data selection using TF-IDF",
                "sec_num": "2.1"
            },
            {
                "text": "The selected subset contains the similar sentences with the test data or test domain. It matches the test data better in domain, topic and style. Hopefully, training translation model using this adapted parallel data may helpful for improving translation performance. In addition, the translation model trained using the selected subset is usually much smaller than that trained using the whole translation data. Limiting the size of translation model is very important for some real applications. Since SMT systems usually require large computation resource. The complexity of standard training and decoding algorithm depends mainly on the size of the parallel training data and the size of the translation model. Limiting the size of the training data with the similar translation performance would also reduce the memories and speed up the translations.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Similar data selection using TF-IDF",
                "sec_num": "2.1"
            },
            {
                "text": "In the information retrieval process, we only use the source language part for document indexing and query generating. It is easy to get source part of the test data. This is different from the common language model adaptation methods, which have to do at lease one pass machine translation to get the candidate English translation as query (Zhao 2004 , Zhang 2006 ). So our method has the advantage that it is independent from the quality of baseline translation system.",
                "cite_spans": [
                    {
                        "start": 341,
                        "end": 351,
                        "text": "(Zhao 2004",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 352,
                        "end": 364,
                        "text": ", Zhang 2006",
                        "ref_id": "BIBREF15"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Similar data selection using TF-IDF",
                "sec_num": "2.1"
            },
            {
                "text": "There are two factors on training data that influence the translation performance of SMT system: the scale and the quality. In some sense, we improve the quality of the training data by selecting the similar sentence to form an adapted training set. However, we also reduce the scale of the training data at the same time. Although this is helpful for some small device applications, it is also possible to induce the data sparseness problem. Here, we introduce a method to optimize between the scale and the quality of the training data.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training data optimization",
                "sec_num": "2.2"
            },
            {
                "text": "The basic idea is that we still use all the available training data; by redistributing the weight of each sentence pairs we adapt the whole training data to the test domain. In our experiments, we simply combine the selected small similar subset and the whole training data. The weights of each sentence pairs are changed accordingly. Figure 1 shows the procedure of the optimization.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 342,
                        "end": 343,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Training data optimization",
                "sec_num": "2.2"
            },
            {
                "text": "As can be seen, through the optimization, the weight of the similar sentence pairs are increased, while the general sentence pairs still have an ordinary weight. This make the translation model inclined to give higher probabilities to the adapted words, and at the same time avoid the data sparseness problem. Since we only change the weight of the sentence pairs, and no new training data is introduced, the translation model size trained on the optimized data will keep as the same as the original one. We use GIZA++ toolkit 3 for word align- It might be beneficial to investigate other sophisticated weighting schemes under the similar idea, such as to give more precise fractional weights to the sentences according the retrieval similarity scores.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Figure 1. Training data optimization",
                "sec_num": null
            },
            {
                "text": "In most circumstances, we don't know exactly the test data or the test domain when we train a machine translation system. This results in the fact that the performance of the translation system highly depends on the training data and the test data it is used in. To alleviate this blindfold status and maximize the potential of the available training corpora, we propose a novel online model optimization method.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Online model optimization",
                "sec_num": "3"
            },
            {
                "text": "The basic idea is that: several candidate translation models are prepared in training stage. In particularly, a general model is also prepared. Then, in the translation process, the similarity between the input sentence and the predefined models is calculated online to get the weights of each model. The optimized model is used to translate the input sentence.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Online model optimization",
                "sec_num": "3"
            },
            {
                "text": "There are two problems in the method: how to prepare submodels in training process and how to optimize the model weight online in translation process.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Online model optimization",
                "sec_num": "3"
            },
            {
                "text": "There are several ways to prepare submodels in training process. If the training data comes from very different sources, we can divide the data according to its origins. Otherwise, we can use clustering method to separate the training corpus into several classes. In addition, our offline data adaptation method can also be used for submodel preparation. For each candidate domain, we can use the source side of a small corpus as queries to extract a domain specific training set. In this case, a sentence pair in the training data may occur in several sub training data, but this doesn't matter. The general model is used when the online input is not similar to any prepared submodels. We can use all available training data to train the general model since generally larger data can get better model even there are some noises.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Prepare the submodels",
                "sec_num": "3.1"
            },
            {
                "text": "We also use TF-IDF information retrieval method for online model weighting. The procedure is as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Online model weighting",
                "sec_num": "3.2"
            },
            {
                "text": "For each input sentence:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Online model weighting",
                "sec_num": "3.2"
            },
            {
                "text": "1. Do IR on training data collection, using the input sentence as query.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Online model weighting",
                "sec_num": "3.2"
            },
            {
                "text": "2. Determine the weights of submodels according to the retrieved sentences.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Online model weighting",
                "sec_num": "3.2"
            },
            {
                "text": "3. Use the optimized model to translate the sentence.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Online model weighting",
                "sec_num": "3.2"
            },
            {
                "text": "The information retrieval process is the same as the offline data selection except that each retrieved sentence is attached with the sub-corpus information, i.e. it belongs to which sub-models in the training process.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Online model weighting",
                "sec_num": "3.2"
            },
            {
                "text": "With the sub-corpus information, we can calculate the weights of submodels. We get the top N most similar sentences, and then calculate proportions of each submodel's sentences. The proportion can be calculated use the count of the sentences or the similarity score of the sentences. The weight of each submodel can be determined according to the proportions.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Online model weighting",
                "sec_num": "3.2"
            },
            {
                "text": "Our optimized model is the log linear interpolation of the sub-models as follows: The online model optimization method makes it possible to select suitable models for each individual test sentence. Since the IR process is done on a fixed training data, the size of the index data is quite small compared with the web IR. The IR process will not take much time in the translation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Online model weighting",
                "sec_num": "3.2"
            },
            {
                "text": "We conduct our experiments on Chinese-to-English translation tasks. The baseline system is a variant of the phrase-base SMT system, implemented using log-linear translation model (He et al. 2006 ). The baseline SMT system is used in all experiments. The only difference between them is that they are trained on different parallel training data.",
                "cite_spans": [
                    {
                        "start": 179,
                        "end": 194,
                        "text": "(He et al. 2006",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental setting",
                "sec_num": "4.1"
            },
            {
                "text": "In training process, we use GIZA++4 toolkit for word alignment in both translation directions, and apply \"grow-diag-final\" method to refine it (Koehn et al., 2003) . We change the preprocess part of GIZA++ toolkit to make it accept the weighted training data. Then we use the same criterion as suggested in (Zens et al., 2002) to do phrase extraction. For the log-linear model training, we take minimum-error-rate training method as described in (Och, 2003) . The language model is trained using Xinhua portion of Gigaword with about 190M words. SRI Language Modeling toolkit5 is used to train a 4-gram model with modified Kneser-Ney smoothing (Chen and Goodman, 1998) . All experiments use the same language model. This ensures that any differences in performance are caused only by differences in the parallel training data.",
                "cite_spans": [
                    {
                        "start": 143,
                        "end": 163,
                        "text": "(Koehn et al., 2003)",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 307,
                        "end": 326,
                        "text": "(Zens et al., 2002)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 446,
                        "end": 457,
                        "text": "(Och, 2003)",
                        "ref_id": null
                    },
                    {
                        "start": 644,
                        "end": 668,
                        "text": "(Chen and Goodman, 1998)",
                        "ref_id": "BIBREF1"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental setting",
                "sec_num": "4.1"
            },
            {
                "text": "Our training data are from three LDC corpora as shown in Table 1 . We random select 200,000 sentence pairs from each corpus and combine them together as the baseline corpus, which includes 16M Chinese words and 19M English words in total. This is the usual case when we train a SMT system, i.e. we simply combine all corpora from different origins to get a larger training corpus.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 63,
                        "end": 64,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Experimental setting",
                "sec_num": "4.1"
            },
            {
                "text": "We use the 2002 NIST MT evaluation test data as our development set, and the 2005 NIST MT test data as the test set in offline data optimization experiments. In both data, each sentence has four human translations as references. The translation quality is evaluated by BLEU metric (Papineni et al., 2002) , as calculated by mteval-v11b.pl6 with case-sensitive matching of n-grams. From the results we can see that although the size of each sub training corpus is similar, the translation results from the corresponding system are quite different on the same test set. It seems that the FBIS corpus is much similar to the test set than the other two corpora. In fact, it is the case. The FBIS contains text mainly from China mainland news stories, while the 2005 NIST test set also include lots of China news text. The results illustrate the importance of selecting suitable training data.",
                "cite_spans": [
                    {
                        "start": 281,
                        "end": 304,
                        "text": "(Papineni et al., 2002)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental setting",
                "sec_num": "4.1"
            },
            {
                "text": "When combining all the sub corpora together, the baseline system gets a little better result than the sub systems. This indicates that larger data is useful even it includes some noise data. However, compared with the FBIS corpus, the baseline corpus contains three times larger data, while the improvement of translation result is not significant. This indicates that simply putting different corpora together is not a good way to make use of the available corpora.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Corpus",
                "sec_num": null
            },
            {
                "text": "We use baseline corpus as initial training corpus, and take Lemur toolkit to build document index on Chinese part of the corpus. The Chinese sentences in development set and test set are used as queries. For each query, N = 100, 200, 500, 1000, 2000 similar sentences are retrieved from the indexed collection. The extracted similar sentence pairs are used to train the new adapted translation models. Table 3 illustrates the results. We give the distinct pair numbers for each adapted set and compare the size of the translation models. To illustrate the effect of duplicate sentences, we also give the results with duplicates and without duplicates (distinct).",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 408,
                        "end": 409,
                        "text": "3",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Offline data optimization experiments",
                "sec_num": "4.3"
            },
            {
                "text": "Distinct pairs",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "System",
                "sec_num": null
            },
            {
                "text": "Baseline The results show that: 1. By using similar data selection, it is possible to use much smaller training data to get comparable or even better results than the baseline system. When N=200, using only 1/4 of the training data and 1/3 of the model size, the adapted translation model achieves comparable result with the baseline model. When N=500, the adapted model outperforms the baseline model with much less training data. The results indicate that relevant data is better data. The method is particular useful for SMT applications on small device.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "BLEU on distinct",
                "sec_num": null
            },
            {
                "text": "2. In general, using duplicate data achieves better results than using distinct data. This justifies our idea that give a higher weight to more similar data will benefit.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "BLEU on distinct",
                "sec_num": null
            },
            {
                "text": "3. With the increase of training data size, the translation performance tends to improve also. However, when the size of corpus achieves a certain scale, the performance may drop. This maybe because that with the increase of the data, noisy data may also be included. More and more included noises may destroy the data. It is necessary to use a development set to determine an optimal size of N.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "BLEU on distinct",
                "sec_num": null
            },
            {
                "text": "We combine each adapted data with the baseline corpus to get the optimized models. The results are shown in Table 4 . We also compare the adapted models (TopN) and the optimized models (TopN+) in the table.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 114,
                        "end": 115,
                        "text": "4",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "BLEU on distinct",
                "sec_num": null
            },
            {
                "text": "Without using any additional data, the optimized models achieve significant better results than the baseline model by redistributing the weight of training sentences. The optimized models also outperform adapted models when the size of the adapted data is small since they make use of all the available data which decrease the influence of data sparseness. However, with the increase of the adapted data, the performance of optimized models is similar to that of the adapted models.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "BLEU on distinct",
                "sec_num": null
            },
            {
                "text": "Distinct pairs The baseline translation results are shown in Table 5. We also give results on each sub test set (denotes as Xcorpus_part). Please note that the absolute BLEU scores are not comparable to the previous experiments since there is only one reference in this test set.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "System",
                "sec_num": null
            },
            {
                "text": "As expected, using the same domain data for training and testing achieves the best results as indicate by bold fonts. The results demonstrate again that relevant data is better data.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "BLEU on TopN+",
                "sec_num": null
            },
            {
                "text": "To test our online model optimization method, we divide the baseline corpus according to the origins of sub corpus. That is, the FBIS, HK_ Hansards and HK_News models are used as three submodels and the baseline model is used as general model. The four weighting schemes described in section 3.2 are used as online weighting schemes individually. The experimental results are shown in Table 6 . S_i indicates the system using weighting scheme i. Different weighting schemes don't show significant improvements from each other. However, all the four weighting schemes achieve better results than the baseline system. The improvements are shown not only on the whole test set but also on each part of the sub test set. The results justify the effectiveness of our online model optimization method.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 391,
                        "end": 392,
                        "text": "6",
                        "ref_id": "TABREF4"
                    }
                ],
                "eq_spans": [],
                "section": "BLEU on TopN+",
                "sec_num": null
            },
            {
                "text": "Most previous research on SMT training data is focused on parallel data collection. Some work tries to acquire parallel sentences from web (Nie et al. 1999; Resnik and Smith 2003; Chen et al. 2004 ). Others extract parallel sentences from comparable or non-parallel corpora (Munteanu and Marcu 2005, 2006) . These work aims to collect more parallel training corpora, while our work aims to make better use of existing parallel corpora.",
                "cite_spans": [
                    {
                        "start": 139,
                        "end": 156,
                        "text": "(Nie et al. 1999;",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 157,
                        "end": 179,
                        "text": "Resnik and Smith 2003;",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 180,
                        "end": 196,
                        "text": "Chen et al. 2004",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 274,
                        "end": 287,
                        "text": "(Munteanu and",
                        "ref_id": null
                    },
                    {
                        "start": 288,
                        "end": 305,
                        "text": "Marcu 2005, 2006)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related work",
                "sec_num": "5"
            },
            {
                "text": "Some research has been conducted on parallel data selection and adaptation. Eck et al. (2005) propose a method to select more informative sentences based on n-gram coverage. They use ngrams to estimate the importance of a sentence. The more previously unseen n-grams in the sentence the more important the sentence is. TF-IDF weighting scheme is also tried in their method, but didn't show improvements over n-grams. This method is independent of test data. Their goal is to decrease the amount of training data to make SMT system adaptable to small devices. Similar to our work, Hildebrand et al. (2005) also use information retrieval method for translation model adaptation. They select sentences similar to the test set from available in-of-domain and out-of-domain training data to form an adapted translation model. Different from their work, our method further use the small adapted data to optimize the distribution of the whole training data. It takes the full advantage of larger data and adapted data. In addition, we also propose an online translation model optimization method, which make it possible to select adapted translation model for each individual sentence.",
                "cite_spans": [
                    {
                        "start": 76,
                        "end": 93,
                        "text": "Eck et al. (2005)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 580,
                        "end": 604,
                        "text": "Hildebrand et al. (2005)",
                        "ref_id": "BIBREF13"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related work",
                "sec_num": "5"
            },
            {
                "text": "Since large scale monolingual corpora are easier to obtain than parallel corpora. There has some research on language model adaptation recent years. Zhao et al. (2004) and Eck et al.(2004) introduce information retrieval method for language model adaptation. Zhang et al.(2006) and Mauser et al.(2006) use adapted language model for SMT re-ranking. Since language model is built for target language in SMT, one pass translation is usually needed to generate n-best translation candidates in language model adaptation. Translation model adaptation doesn't need a pre-translation procedure. Comparatively, it is more direct. Language model adaptation and translation model adaptation are good complement to each other. It is possible that combine these two adaptation approaches could further improve machine translation performance.",
                "cite_spans": [
                    {
                        "start": 149,
                        "end": 167,
                        "text": "Zhao et al. (2004)",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 172,
                        "end": 188,
                        "text": "Eck et al.(2004)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 259,
                        "end": 277,
                        "text": "Zhang et al.(2006)",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 282,
                        "end": 301,
                        "text": "Mauser et al.(2006)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related work",
                "sec_num": "5"
            },
            {
                "text": "This paper presents two new methods to improve statistical machine translation performance by making better use of the available parallel training corpora. The offline data selection method adapts the training corpora to the test domain by retrieving similar sentence pairs and redistributing their weight in the training data. Experimental results show that the selected small subset achieves comparable or even better performance than the baseline system with much less training data. The optimized training data can further improve translation performance without using any additional resource. The online model optimization method adapts the translation model to the online test sentence by redistributing the weight of each predefined submodels. Preliminary results show the effectiveness of the method. Our work also demonstrates that in addition to larger training data, more relevant training data is also important for SMT model training.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion and future work",
                "sec_num": "6"
            },
            {
                "text": "In future work, we will improve our methods in several aspects. Currently, the similar sentence retrieval model and the weighting schemes are very simple. It might work better by trying other sophisticated similarity measure models or using some optimization algorithms to determine submodel's weights. Introducing language model optimization into our system might further improve translation performance.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion and future work",
                "sec_num": "6"
            },
            {
                "text": "http://www.nist.gov/speech/tests/mt/",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "http://www.cs.cmu.edu/~lemur/",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "http://www.fjoch.com/GIZA++.html",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "http://www.speech.sri.com/projects/srilm/",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "http://www.nist.gov/speech/tests/mt/resources/scoring.htm",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "This work was supported by National Natural Science Foundation of China, Contract No. 60603095 and 60573188.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgement",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Discovering Parallel Text from the World Wide Web",
                "authors": [
                    {
                        "first": "Jisong",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Rowena",
                        "middle": [],
                        "last": "Chau",
                        "suffix": ""
                    },
                    {
                        "first": "Chung-Hsing",
                        "middle": [],
                        "last": "Yeh",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "ACSW Frontiers",
                "volume": "",
                "issue": "",
                "pages": "157--161",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jisong Chen, Rowena Chau, Chung-Hsing Yeh 2004. Discovering Parallel Text from the World Wide Web. ACSW Frontiers 2004: 157-161",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "An Empirical Study of Smoothing Techniques for Language Modeling",
                "authors": [
                    {
                        "first": "Stanley",
                        "middle": [
                            "F"
                        ],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Joshua",
                        "middle": [],
                        "last": "Goodman",
                        "suffix": ""
                    }
                ],
                "year": 1998,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Stanley F. Chen and Joshua Goodman. 1998. An Em- pirical Study of Smoothing Techniques for Language Modeling. Technical Report TR-10-98, Harvard Uni- versity Center for Research in Computing Technol- ogy.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Language Model Adaptation for Statistical Machine Translation Based on Information Retrieval",
                "authors": [
                    {
                        "first": "Matthias",
                        "middle": [],
                        "last": "Eck",
                        "suffix": ""
                    },
                    {
                        "first": "Stephan",
                        "middle": [],
                        "last": "Vogel",
                        "suffix": ""
                    },
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Waibel",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proceedings of Fourth International Conference on Language Resources and Evaluation",
                "volume": "",
                "issue": "",
                "pages": "327--330",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Matthias Eck, Stephan Vogel, and Alex Waibel 2004. Language Model Adaptation for Statistical Machine Translation Based on Information Retrieval. Proceed- ings of Fourth International Conference on Language Resources and Evaluation:327-330",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Low cost portability for statistical machine translation based on n-gram coverage",
                "authors": [
                    {
                        "first": "Matthias",
                        "middle": [],
                        "last": "Eck",
                        "suffix": ""
                    },
                    {
                        "first": "Stephan",
                        "middle": [],
                        "last": "Vogel",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "227--234",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Matthias Eck, Stephan Vogel, Alex Waibel 2005. Low cost portability for statistical machine translation based on n-gram coverage. MT Summit X: 227-234.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "ICT System Description for the 2006 TC-STAR Run#2 SLT Evaluation",
                "authors": [
                    {
                        "first": "Zhongjun",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Deyi",
                        "middle": [],
                        "last": "Xiong",
                        "suffix": ""
                    },
                    {
                        "first": "Hongxu",
                        "middle": [],
                        "last": "Hou",
                        "suffix": ""
                    },
                    {
                        "first": "Qun",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Proceedings of TC-STAR Workshop on Speech-to-Speech Translation",
                "volume": "",
                "issue": "",
                "pages": "63--68",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zhongjun He, Yang Liu, Deyi Xiong, Hongxu Hou, and Qun Liu 2006. ICT System Description for the 2006 TC-STAR Run#2 SLT Evaluation. Proceedings of TC- STAR Workshop on Speech-to-Speech Translation: 63-68",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Statistical phrase-based translation",
                "authors": [
                    {
                        "first": "Philipp",
                        "middle": [],
                        "last": "Koehn",
                        "suffix": ""
                    },
                    {
                        "first": "Franz",
                        "middle": [
                            "J"
                        ],
                        "last": "Och",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Marcu",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Proceedings of HLT-NAACL",
                "volume": "",
                "issue": "",
                "pages": "127--133",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical phrase-based translation. Proceedings of HLT-NAACL 2003: 127-133.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "The RWTH Statistical Machine Translation System for the IWSLT 2006 Evaluation",
                "authors": [
                    {
                        "first": "Arne",
                        "middle": [],
                        "last": "Mauser",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Zens",
                        "suffix": ""
                    },
                    {
                        "first": "Evgeny",
                        "middle": [],
                        "last": "Matusov",
                        "suffix": ""
                    },
                    {
                        "first": "Sasa",
                        "middle": [],
                        "last": "Hasan",
                        "suffix": ""
                    },
                    {
                        "first": "Hermann",
                        "middle": [],
                        "last": "Ney",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Proceedings of International Workshop on Spoken Language Translation",
                "volume": "",
                "issue": "",
                "pages": "103--110",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Arne Mauser, Richard Zens, Evgeny Matusov, Sasa Hasan, Hermann Ney 2006. The RWTH Statistical Machine Translation System for the IWSLT 2006 Evaluation. Proceedings of International Workshop on Spoken Language Translation.:103-110",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Improving Machine Translation Performance by Exploiting Comparable Corpora",
                "authors": [
                    {
                        "first": "Stefan",
                        "middle": [],
                        "last": "Dragos",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Munteanu",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Marcu",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Computational Linguistics",
                "volume": "31",
                "issue": "4",
                "pages": "477--504",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dragos Stefan Munteanu and Daniel Marcu 2005. Im- proving Machine Translation Performance by Ex- ploiting Comparable Corpora. Computational Lin- guistics, 31 (4): 477-504",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Extracting Parallel Sub-Sentential Fragments from Comparable Corpora. ACL-2006",
                "authors": [
                    {
                        "first": "Stefan",
                        "middle": [],
                        "last": "Dragos",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Munteanu",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Marcu",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "81--88",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dragos Stefan Munteanu and Daniel Marcu 2006. Ex- tracting Parallel Sub-Sentential Fragments from Comparable Corpora. ACL-2006: 81-88",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Cross-Language Information Retrieval based on Parallel Texts and Automatic Mining of Parallel Texts in the Web",
                "authors": [
                    {
                        "first": "Jian-Yun",
                        "middle": [],
                        "last": "Nie",
                        "suffix": ""
                    },
                    {
                        "first": "Michel",
                        "middle": [],
                        "last": "Simard",
                        "suffix": ""
                    },
                    {
                        "first": "Pierre",
                        "middle": [],
                        "last": "Isabelle",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Durand",
                        "suffix": ""
                    }
                ],
                "year": 1999,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jian-Yun Nie, Michel Simard, Pierre Isabelle, Richard Durand 1999. Cross-Language Information Retrieval based on Parallel Texts and Automatic Mining of Parallel Texts in the Web. SIGIR-1999: 74-81",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Minimum Error Rate Training in Statistical Machine Translation",
                "authors": [
                    {
                        "first": "Josef",
                        "middle": [],
                        "last": "Franz",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Och",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "",
                "volume": "2003",
                "issue": "",
                "pages": "160--167",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Franz Josef Och 2003. Minimum Error Rate Training in Statistical Machine Translation. ACL-2003:160-167.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Bleu: a Method for Automatic Evaluation of Machine Translation",
                "authors": [
                    {
                        "first": "Kishore",
                        "middle": [],
                        "last": "Papineni",
                        "suffix": ""
                    },
                    {
                        "first": "Salim",
                        "middle": [],
                        "last": "Roukos",
                        "suffix": ""
                    },
                    {
                        "first": "Todd",
                        "middle": [],
                        "last": "Ward",
                        "suffix": ""
                    },
                    {
                        "first": "Wei-Jing",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a Method for Automatic Evaluation of Machine Translation. ACL-2002: 311- 318",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "The Web as a Parallel Corpus",
                "authors": [
                    {
                        "first": "Philip",
                        "middle": [],
                        "last": "Resnik",
                        "suffix": ""
                    },
                    {
                        "first": "Noah",
                        "middle": [
                            "A"
                        ],
                        "last": "Smith",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Computational Linguistics",
                "volume": "29",
                "issue": "3",
                "pages": "349--380",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Philip Resnik and Noah A. Smith 2003. The Web as a Parallel Corpus. Computational Linguistics 29(3): 349-380",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Adaptation of the Translation Model for Statistical Machine Translation based on Information Retrieval",
                "authors": [
                    {
                        "first": "Almut Silja",
                        "middle": [],
                        "last": "Hildebrand",
                        "suffix": ""
                    },
                    {
                        "first": "Matthias",
                        "middle": [],
                        "last": "Eck",
                        "suffix": ""
                    },
                    {
                        "first": "Stephan",
                        "middle": [],
                        "last": "Vogel",
                        "suffix": ""
                    },
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of EAMT 2005",
                "volume": "",
                "issue": "",
                "pages": "133--142",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Almut Silja Hildebrand, Matthias Eck, Stephan Vogel, and Alex Waibel 2005. Adaptation of the Translation Model for Statistical Machine Translation based on Information Retrieval. Proceedings of EAMT 2005: 133-142.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Phrase-Based Statistical Machine Translation. Annual German Conference on AI",
                "authors": [
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Zens",
                        "suffix": ""
                    },
                    {
                        "first": "Josef",
                        "middle": [],
                        "last": "Franz",
                        "suffix": ""
                    },
                    {
                        "first": "Hermann",
                        "middle": [],
                        "last": "Och",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Ney",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "",
                "volume": "2479",
                "issue": "",
                "pages": "18--32",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Richard Zens, Franz Josef Och, Hermann Ney 2002. Phrase-Based Statistical Machine Translation. An- nual German Conference on AI, KI 2002, Vol. LNAI 2479: 18-32",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Distributed Language Modeling for N-best List Re-ranking",
                "authors": [
                    {
                        "first": "Ying",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Almut Silja",
                        "middle": [],
                        "last": "Hildebrand",
                        "suffix": ""
                    },
                    {
                        "first": "Stephan",
                        "middle": [],
                        "last": "Vogel",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "",
                "volume": "2006",
                "issue": "",
                "pages": "216--223",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ying Zhang, Almut Silja Hildebrand, Stephan Vogel 2006. Distributed Language Modeling for N-best List Re-ranking. EMNLP-2006:216-223",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Language Model Adaptation for Statistical Machine Translation with structured query models",
                "authors": [
                    {
                        "first": "Bing",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Matthias",
                        "middle": [],
                        "last": "Eck",
                        "suffix": ""
                    },
                    {
                        "first": "Stephan",
                        "middle": [],
                        "last": "Vogel",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Bing Zhao, Matthias Eck, Stephan Vogel 2004. Lan- guage Model Adaptation for Statistical Machine Translation with structured query models. COLING- 2004",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "3 http://www.fjoch.com/GIZA++.html ment training in the training process. The input training file formats for GIZA++ is as follows: Each training sentence pair is stored in three lines. The first line is the number of times this sentence pair occurred. The second line is the source sentence where each token is replaced by its unique integer id and the third is the target sentence in the same format. To deal with our optimized training data, we only need to change the number of sentence pairs in the first line accordingly. This will not call for extra training time and memory for the whole training process.",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "the probability of general model, i p is the probability of submodel i. 0 p \u03b4 is the weight of general model. i\u03b4 is the weight of submodel i. Each model i is also implemented using log linear model in our SMT system. So after the log operation, the submodels are interpolated linearly.In our experiments, the interpolation factor i \u03b4 is determined using the following four simple weighting schemes: model i is the i-th submodel, . the proportion of model i in the retrieved results. We use count for proportion calculation. max_model is the submodel with the max proportion score.The training and translation procedure of online model optimization is illustrated in Figure2.",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "Figure 2. Online model optimization",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table><tr><td/><td colspan=\"2\">LDC No.</td><td>Description</td><td># sent. pairs</td></tr><tr><td>FBIS</td><td colspan=\"4\">LDC2003E14 FBIS Multilanguage Texts</td><td>200000</td></tr><tr><td colspan=\"5\">HK_Hansards LDC2004T08 Hong Kong Hansards Text</td><td>200000</td></tr><tr><td colspan=\"3\">HK_News LDC2004T08</td><td colspan=\"2\">Hong Kong News Text</td><td>200000</td></tr><tr><td>Baseline</td><td>-</td><td/><td>All above data</td><td>600000</td></tr><tr><td/><td colspan=\"4\">Table 1. Training corpora</td></tr><tr><td colspan=\"4\">4.2 Baseline experiments</td></tr><tr><td colspan=\"2\">System</td><td colspan=\"3\">BLEU on dev set BLEU on test set</td></tr><tr><td>FBIS</td><td/><td/><td>0.2614</td><td>0.2331</td></tr><tr><td colspan=\"2\">HK_Hansards</td><td/><td>0.1679</td><td>0.1624</td></tr><tr><td colspan=\"2\">HK_News</td><td/><td>0.1748</td><td>0.1608</td></tr><tr><td colspan=\"2\">Baseline</td><td/><td>0.2565</td><td>0.2363</td></tr></table>",
                "type_str": "table",
                "text": "We first train translation models on each sub training corpus and the baseline corpus. The development set is used to tune the feature weights. The results on test set are shown in Table2. Baseline results",
                "html": null,
                "num": null
            },
            "TABREF1": {
                "content": "<table><tr><td/><td>600000</td><td>2.41G</td><td>0.2363</td><td>0.2363</td></tr><tr><td>Top100</td><td>91804</td><td>0.43G</td><td>0.2306</td><td>0.2346</td></tr><tr><td>Top200</td><td>150619</td><td>0.73G</td><td>0.2360</td><td>0.2345</td></tr><tr><td>Top500</td><td>261003</td><td>1.28G</td><td>0.2415</td><td>0.2370</td></tr><tr><td colspan=\"2\">Top1000 357337</td><td>1.74G</td><td>0.2463</td><td>0.2376</td></tr><tr><td colspan=\"2\">Top2000 445890</td><td>2.11G</td><td>0.2351</td><td>0.2346</td></tr></table>",
                "type_str": "table",
                "text": "Offline data adaptation results",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table><tr><td>System Test data</td><td>FBIS</td><td>HK_ Hansards</td><td>HK_ News</td><td>Baseline</td></tr><tr><td>FBIS-part</td><td>0.1096</td><td>0.0687</td><td colspan=\"2\">0.0622 0.1030</td></tr><tr><td colspan=\"2\">HK_Hans_part 0.0726</td><td>0.0918</td><td colspan=\"2\">0.0846 0.0897</td></tr><tr><td colspan=\"2\">HK_News_part 0.0664</td><td>0.0801</td><td colspan=\"2\">0.0936 0.0870</td></tr><tr><td>MT05_part</td><td>0.1130</td><td>0.0805</td><td colspan=\"2\">0.0776 0.1116</td></tr><tr><td>Whole test set</td><td>0.0937</td><td>0.0799</td><td colspan=\"2\">0.0781 0.0993</td></tr><tr><td>System Test data</td><td>S_1</td><td>S_2</td><td>S_3</td><td>S_4</td></tr><tr><td>FBIS-part</td><td colspan=\"2\">0.1090 0.1090</td><td>0.1089</td><td>0.1089</td></tr><tr><td colspan=\"3\">HK_Hans_part 0.0906 0.0903</td><td>0.0902</td><td>0.0902</td></tr><tr><td colspan=\"3\">HK_News_part 0.0952 0.0950</td><td>0.0933</td><td>0.0934</td></tr><tr><td>MT05_part</td><td colspan=\"2\">0.1119 0.1123</td><td>0.1149</td><td>0.1151</td></tr><tr><td>Whole test set</td><td colspan=\"2\">0.1034 0.1034</td><td>0.1038</td><td>0.1038</td></tr></table>",
                "type_str": "table",
                "text": "Baseline results on new test set",
                "html": null,
                "num": null
            },
            "TABREF4": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Online model optimization results",
                "html": null,
                "num": null
            }
        }
    }
}