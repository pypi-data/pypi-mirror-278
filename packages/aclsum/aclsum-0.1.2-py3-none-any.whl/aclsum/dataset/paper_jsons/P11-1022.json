{
    "paper_id": "P11-1022",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T11:53:42.588232Z"
    },
    "title": "Goodness: A Method for Measuring Machine Translation Confidence",
    "authors": [
        {
            "first": "Nguyen",
            "middle": [],
            "last": "Bach",
            "suffix": "",
            "affiliation": {},
            "email": "nbach@cs.cmu.edu"
        },
        {
            "first": "Fei",
            "middle": [],
            "last": "Huang",
            "suffix": "",
            "affiliation": {},
            "email": "huangfe@us.ibm.com"
        },
        {
            "first": "Yaser",
            "middle": [],
            "last": "Al-Onaizan",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Ibm",
            "middle": [
                "T J"
            ],
            "last": "Watson",
            "suffix": "",
            "affiliation": {},
            "email": ""
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "State-of-the-art statistical machine translation (MT) systems have made significant progress towards producing user-acceptable translation output. However, there is still no efficient way for MT systems to inform users which words are likely translated correctly and how confident it is about the whole sentence. We propose a novel framework to predict wordlevel and sentence-level MT errors with a large number of novel features. Experimental results show that the MT error prediction accuracy is increased from 69.1 to 72.2 in F-score. The Pearson correlation between the proposed confidence measure and the human-targeted translation edit rate (HTER) is 0.6. Improvements between 0.4 and 0.9 TER reduction are obtained with the n-best list reranking task using the proposed confidence measure. Also, we present a visualization prototype of MT errors at the word and sentence levels with the objective to improve post-editor productivity.",
    "pdf_parse": {
        "paper_id": "P11-1022",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "State-of-the-art statistical machine translation (MT) systems have made significant progress towards producing user-acceptable translation output. However, there is still no efficient way for MT systems to inform users which words are likely translated correctly and how confident it is about the whole sentence. We propose a novel framework to predict wordlevel and sentence-level MT errors with a large number of novel features. Experimental results show that the MT error prediction accuracy is increased from 69.1 to 72.2 in F-score. The Pearson correlation between the proposed confidence measure and the human-targeted translation edit rate (HTER) is 0.6. Improvements between 0.4 and 0.9 TER reduction are obtained with the n-best list reranking task using the proposed confidence measure. Also, we present a visualization prototype of MT errors at the word and sentence levels with the objective to improve post-editor productivity.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "State-of-the-art Machine Translation (MT) systems are making progress to generate more usable translation outputs. In particular, statistical machine translation systems (Koehn et al., 2007; Bach et al., 2007; Shen et al., 2008) have advanced to a state that the translation quality for certain language pairs (e.g. Spanish-English, French-English, Iraqi-English) in certain domains (e.g. broadcasting news, force-protection, travel) is acceptable to users.",
                "cite_spans": [
                    {
                        "start": 170,
                        "end": 190,
                        "text": "(Koehn et al., 2007;",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 191,
                        "end": 209,
                        "text": "Bach et al., 2007;",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 210,
                        "end": 228,
                        "text": "Shen et al., 2008)",
                        "ref_id": "BIBREF20"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "However, a remaining open question is how to predict confidence scores for machine translated words and sentences. An MT system typically returns the best translation candidate from its search space, but still has no reliable way to inform users which word is likely to be correctly translated and how confident it is about the whole sentence. Such information is vital to realize the utility of machine translation in many areas. For example, a post-editor would like to quickly identify which sentences might be incorrectly translated and in need of correction. Other areas, such as cross-lingual question-answering, information extraction and retrieval, can also benefit from the confidence scores of MT output. Finally, even MT systems can leverage such information to do n-best list reranking, discriminative phrase table and rule filtering, and constraint decoding (Hildebrand and Vogel, 2008) .",
                "cite_spans": [
                    {
                        "start": 871,
                        "end": 899,
                        "text": "(Hildebrand and Vogel, 2008)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Numerous attempts have been made to tackle the confidence estimation problem. The work of Blatz et al. (2004) is perhaps the best known study of sentence and word level features and their impact on translation error prediction. Along this line of research, improvements can be obtained by incorporating more features as shown in (Quirk, 2004; Sanchis et al., 2007; Raybaud et al., 2009; Specia et al., 2009) . Soricut and Echihabi (2010) developed regression models which are used to predict the expected BLEU score of a given translation hypothesis. Improvement also can be obtained by using target part-of-speech and null dependency link in a MaxEnt classifier (Xiong et al., 2010) . Ueffing and Ney (2007) introduced word posterior probabilities (WPP) features and applied them in the n-best list reranking. From the usability point of view, back-translation is a tool to help users to assess the accuracy level of MT output (Bach et al., 2007) . Literally, it translates backward the MT output into the source language to see whether the output of backward translation matches the original source sentence.",
                "cite_spans": [
                    {
                        "start": 90,
                        "end": 109,
                        "text": "Blatz et al. (2004)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 329,
                        "end": 342,
                        "text": "(Quirk, 2004;",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 343,
                        "end": 364,
                        "text": "Sanchis et al., 2007;",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 365,
                        "end": 386,
                        "text": "Raybaud et al., 2009;",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 387,
                        "end": 407,
                        "text": "Specia et al., 2009)",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 410,
                        "end": 437,
                        "text": "Soricut and Echihabi (2010)",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 663,
                        "end": 683,
                        "text": "(Xiong et al., 2010)",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 686,
                        "end": 708,
                        "text": "Ueffing and Ney (2007)",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 928,
                        "end": 947,
                        "text": "(Bach et al., 2007)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "However, previous studies had a few shortcomings. First, source-side features were not extensively investigated. Blatz et al.(2004) only investigated source ngram frequency statistics and source language model features, while other work mainly focused on target side features. Second, previous work attempted to incorporate more features but faced scalability issues, i.e., to train many features we need many training examples and to train discriminatively we need to search through all possible translations of each training example. Another issue of previous work was that they are all trained with BLEU/TER score computing against the translation references which is different from predicting the human-targeted translation edit rate (HTER) which is crucial in post-editing applications (Snover et al., 2006; Papineni et al., 2002) . Finally, the backtranslation approach faces a serious issue when forward and backward translation models are symmetric. In this case, back-translation will not be very informative to indicate forward translation quality.",
                "cite_spans": [
                    {
                        "start": 113,
                        "end": 131,
                        "text": "Blatz et al.(2004)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 791,
                        "end": 812,
                        "text": "(Snover et al., 2006;",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 813,
                        "end": 835,
                        "text": "Papineni et al., 2002)",
                        "ref_id": "BIBREF15"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this paper, we predict error types of each word in the MT output with a confidence score, extend it to the sentence level, then apply it to n-best list reranking task to improve MT quality, and finally design a visualization prototype. We try to answer the following questions:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\u2022 Can we use a rich feature set such as sourceside information, alignment context, and dependency structures to improve error prediction performance?",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\u2022 Can we predict more translation error types i.e substitution, insertion, deletion and shift?",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\u2022 How good do our prediction methods correlate with human correction?",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\u2022 Do confidence measures help the MT system to select a better translation?",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\u2022 How confidence score can be presented to improve end-user perception?",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In Section 2, we describe the models and training method for the classifier. We describe novel features including source-side, alignment context, and dependency structures in Section 3. Experimental results and analysis are reported in Section 4. Section 5 and 6 present applications of confidence scores.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Confidence estimation can be viewed as a sequential labelling task in which the word sequence is MT output and word labels can be Bad/Good or Insertion/Substitution/Shif t/Good. We first estimate each individual word confidence and extend it to the whole sentence. Arabic text is fed into an Arabic-English SMT system and the English translation outputs are corrected by humans in two phases. In phase one, a bilingual speaker corrects the MT system translation output. In phase two, another bilingual speaker does quality checking for the correction done in phase one. If bad corrections were spotted, they correct them again. In this paper we use the final correction data from phase two as the reference thus HTER can be used as an evaluation metric. We have 75 thousand sentences with 2.4 million words in total from the human correction process described above.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Problem setting",
                "sec_num": "2.1"
            },
            {
                "text": "We obtain training labels for each word by performing TER alignment between MT output and the phasetwo human correction. From TER alignments we observed that out of total errors are 48% substitution, 28% deletion, 13% shift, and 11% insertion errors. Based on the alignment, each word produced by the MT system has a label: good, insertion, substitution and shift. Since a deletion error occurs when it only appears in the reference translation, not in the MT output, our model will not predict deletion errors in the MT output.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Problem setting",
                "sec_num": "2.1"
            },
            {
                "text": "In our problem, a training instance is a word from MT output, and its label when the MT sentence is aligned with the human correction. Given a training instance x, y is the true label of x; f stands for its feature vector f (x, y); and w is feature weight vector. We define a feature-rich classifier score(x, y) as follow score(x, y) = w.f (x, y)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word-level model",
                "sec_num": "2.2"
            },
            {
                "text": "(1)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word-level model",
                "sec_num": "2.2"
            },
            {
                "text": "To obtain the label, we choose the class with the highest score as the predicted label for that data instance.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word-level model",
                "sec_num": "2.2"
            },
            {
                "text": "To learn optimized weights, we use the Margin Infused Relaxed Algorithm or MIRA (Crammer and Singer, 2003; McDonald et al., 2005) which is an online learner closely related to both the support vector machine and perceptron learning framework. MIRA has been shown to provide state-of-the-art performance for sequential labelling task (Rozenfeld et al., 2006) , and is also able to provide an efficient mechanism to train and optimize MT systems with lots of features (Watanabe et al., 2007; Chiang et al., 2009) . In general, weights are updated at each step time t according to the following rule: ",
                "cite_spans": [
                    {
                        "start": 80,
                        "end": 106,
                        "text": "(Crammer and Singer, 2003;",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 107,
                        "end": 129,
                        "text": "McDonald et al., 2005)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 333,
                        "end": 357,
                        "text": "(Rozenfeld et al., 2006)",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 466,
                        "end": 489,
                        "text": "(Watanabe et al., 2007;",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 490,
                        "end": 510,
                        "text": "Chiang et al., 2009)",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word-level model",
                "sec_num": "2.2"
            },
            {
                "text": "w t+1 =",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word-level model",
                "sec_num": "2.2"
            },
            {
                "text": "where L(y, y ) is a measure of the loss of using y instead of the true label y. In this problem L(y, y ) is 0-1 loss function. More specifically, for each instance x i in the training data at a time t we find the label with the highest score:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word-level model",
                "sec_num": "2.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "y = arg max y score(x i , y)",
                        "eq_num": "(3)"
                    }
                ],
                "section": "Word-level model",
                "sec_num": "2.2"
            },
            {
                "text": "the weight vector is updated as follow",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word-level model",
                "sec_num": "2.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "w t+1 = w t + \u03c4 (f (x i , y) -f (x i , y ))",
                        "eq_num": "(4)"
                    }
                ],
                "section": "Word-level model",
                "sec_num": "2.2"
            },
            {
                "text": "\u03c4 can be interpreted as a step size; when \u03c4 is a large number we want to update our weights aggressively, otherwise weights are updated conservatively. (5) where C is a positive constant used to cap the maximum possible value of \u03c4 . In practice, a cut-off threshold n is the parameter which decides the number of features kept (whose occurrence is at least n) during training. Note that MIRA is sensitive to constant C, the cut-off feature threshold n, and the number of iterations. The final weight is typically normalized by the number of training iterations and the number of training instances. These parameters are tuned on a development set.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word-level model",
                "sec_num": "2.2"
            },
            {
                "text": "Given the feature sets and optimized weights, we use the Viterbi algorithm to find the best label sequence. To estimate the confidence of a sentence S we rely on the information from the forward-backward inference. One approach is to directly use the conditional probabilities of the whole sequence. However, this quantity is the confidence measure for the label sequence predicted by the classifier and it does not represent the goodness of the whole MT output. Another more appropriated method is to use the marginal probability of Good label which can be defined as follow:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Sentence-level model",
                "sec_num": "2.3"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "p(y i = Good|S) = \u03b1(y i |S)\u03b2(y i |S) j \u03b1(y j |S)\u03b2(y j |S)",
                        "eq_num": "(6)"
                    }
                ],
                "section": "Sentence-level model",
                "sec_num": "2.3"
            },
            {
                "text": "p(y i = Good|S) is the marginal probability of label Good at position i given the MT output sentence S. \u03b1(y i |S) and \u03b2(y i |S) are forward and backward values.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Sentence-level model",
                "sec_num": "2.3"
            },
            {
                "text": "Our confidence estimation for a sentence S of k words is defined as follow",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Sentence-level model",
                "sec_num": "2.3"
            },
            {
                "text": "goodness(S) = k i=1 p(y i = Good|S) k (7)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Sentence-level model",
                "sec_num": "2.3"
            },
            {
                "text": "goodness(S) is ranging between 0 and 1, where 0 is equivalent to an absolutely wrong translation and 1 is a perfect translation. Essentially, goodness(S) is the arithmetic mean which represents the goodness of translation per word in the whole sentence.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Sentence-level model",
                "sec_num": "2.3"
            },
            {
                "text": "Features are generated from feature types: abstract templates from which specific features are instantiated. Features sets are often parameterized in various ways.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Confidence Measure Features",
                "sec_num": "3"
            },
            {
                "text": "In this section, we describe three new feature sets introduced on top of our baseline classifier which has WPP and target POS features (Ueffing and Ney, 2007; Xiong et al., 2010) .",
                "cite_spans": [
                    {
                        "start": 135,
                        "end": 158,
                        "text": "(Ueffing and Ney, 2007;",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 159,
                        "end": 178,
                        "text": "Xiong et al., 2010)",
                        "ref_id": "BIBREF27"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Confidence Measure Features",
                "sec_num": "3"
            },
            {
                "text": "From MT decoder log, we can track which source phrases generate target phrases. Furthermore, one can infer the alignment between source and target words within the phrase pair using simple aligners such as IBM Model-1 alignment.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Source-side features",
                "sec_num": "3.1"
            },
            {
                "text": "Source phrase features: These features are designed to capture the likelihood that source phrase and target word co-occur with a given error label. The intuition behind them is that if a large percentage of the source phrase and target have often been seen together with the same label, then the produced target word should have this label in the future. Figure 1a illustrates this feature template where the first line is source POS tags, the second line is the Buckwalter romanized source Arabic sequence, and the third line is MT output. The source phrase feature is defined as follow f102(process) = 1 if source-phrase=\"hdhh alamlyt\" 0 otherwise",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 362,
                        "end": 364,
                        "text": "1a",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Source-side features",
                "sec_num": "3.1"
            },
            {
                "text": "Source POS: Source phrase features might be susceptible to sparseness issues. We can generalize source phrases based on their POS tags to reduce the number of parameters. For example, the example in Figure 1a is generalized as in Figure 1b and we have the following feature:",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 206,
                        "end": 208,
                        "text": "1a",
                        "ref_id": null
                    },
                    {
                        "start": 237,
                        "end": 239,
                        "text": "1b",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Source-side features",
                "sec_num": "3.1"
            },
            {
                "text": "f103(process) = 1 if source-POS=\" DT DTNN \" 0 otherwise",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Source-side features",
                "sec_num": "3.1"
            },
            {
                "text": "Source POS and phrase context features: This feature set allows us to look at the surrounding context of the source phrase. For example, in Figure 1c we have \"hdhh alamlyt\" generates \"process\". We also have other information such as on the right hand side the next two phrases are \"ayda\" and \"tshyr\" or the sequence of source target POS on the right hand side is \"RB VBP\". An example of this type of feature is",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 147,
                        "end": 149,
                        "text": "1c",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Source-side features",
                "sec_num": "3.1"
            },
            {
                "text": "f104(process) = 1 if source-POS-context=\" RB VBP \" 0 otherwise",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Source-side features",
                "sec_num": "3.1"
            },
            {
                "text": "The IBM Model-1 feature performed relatively well in comparison with the WPP feature as shown by Blatz et al. (2004) . In our work, we incorporate not only the IBM Model-1 feature but also the surrounding alignment context. The key intuition is that collocation is a reliable indicator for judging if a target word is generated by a particular source word (Huang, 2009) . Moreover, the IBM Model-1 feature was already used in several steps of a translation system such as word alignment, phrase extraction and scoring. Also the impact of this feature alone might fade away when the MT system is scaled up. We obtain word-to-word alignments by applying IBM Model-1 to bilingual phrase pairs that generated the MT output. The IBM Model-1 assumes one target word can only be aligned to one source word. Therefore, given a target word we can always identify which source word it is aligned to.",
                "cite_spans": [
                    {
                        "start": 97,
                        "end": 116,
                        "text": "Blatz et al. (2004)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 356,
                        "end": 369,
                        "text": "(Huang, 2009)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Alignment context features",
                "sec_num": "3.2"
            },
            {
                "text": "Source alignment context feature: We anchor the target word and derive context features surrounding its source word. For example, in Figure 2a and 2b we have an alignment between \"tshyr\" and \"refers\" The source contexts \"tshyr\" with a window of one word are \"ayda\" to the left and \"aly\" to the right.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 140,
                        "end": 142,
                        "text": "2a",
                        "ref_id": "FIGREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Alignment context features",
                "sec_num": "3.2"
            },
            {
                "text": "Target alignment context feature: Similar to source alignment context features, we anchor the source word and derive context features surrounding the aligned target word. Figure 2c shows a left target context feature of word \"refers\". Our features are derived from a window of four words.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 178,
                        "end": 180,
                        "text": "2c",
                        "ref_id": "FIGREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Alignment context features",
                "sec_num": "3.2"
            },
            {
                "text": "Combining alignment context with POS tags: Instead of using lexical context we have features to look at source and target POS alignment context. For instance, the feature in Figure 2d ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 181,
                        "end": 183,
                        "text": "2d",
                        "ref_id": "FIGREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Alignment context features",
                "sec_num": "3.2"
            },
            {
                "text": "The contextual and source information in the previous sections only take into account surface structures of source and target sentences. Meanwhile, dependency structures have been extensively used in various translation systems (Shen et al., 2008; Ma et al., 2008; Bach et al., 2009) . The adoption of dependency structures might enable the classifier to utilize deep structures to predict translation errors. Source and target structures are unlikely to be isomorphic as shown in Figure 3a . However, we expect some high-level linguistic structures are likely to transfer across certain language pairs. For example, prepositional phrases (PP) in Arabic and English are similar in a sense that PPs generally appear at the end of the sentence (after all the verbal arguments) and to a lesser extent at its beginning (Habash and Hu, 2009) . We use the Stanford parser to obtain dependency trees and POS tags (Marneffe et al., 2006) .",
                "cite_spans": [
                    {
                        "start": 228,
                        "end": 247,
                        "text": "(Shen et al., 2008;",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 248,
                        "end": 264,
                        "text": "Ma et al., 2008;",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 265,
                        "end": 283,
                        "text": "Bach et al., 2009)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 815,
                        "end": 836,
                        "text": "(Habash and Hu, 2009)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 906,
                        "end": 929,
                        "text": "(Marneffe et al., 2006)",
                        "ref_id": "BIBREF13"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 488,
                        "end": 490,
                        "text": "3a",
                        "ref_id": "FIGREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Source and target dependency structure features",
                "sec_num": "3.3"
            },
            {
                "text": "The motivation is to take advantage of the long distance dependency relations between source and target words. Given an alignment between a source word s i and a target word t j . A child-father agreement exists when s k is aligned to t l , where s k and t l are father of s i and t j in source and target dependency trees, respectively. Figure 3b illustrates that \"tshyr\" and \"refers\" have a child-father agreement.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 345,
                        "end": 347,
                        "text": "3b",
                        "ref_id": "FIGREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Child-Father agreement:",
                "sec_num": null
            },
            {
                "text": "To verify our intuition, we analysed 243K words of manual aligned Arabic-English bitext. We observed 29.2% words having child-father agreements. In term of structure types, we found 27.2% of copula verb and 30.2% prepositional structures, including object of a preposition, prepositional modifier, and prepositional complement, are having child-father agreements.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Child-Father agreement:",
                "sec_num": null
            },
            {
                "text": "Children agreement: In the child-father agreement feature we look up in the dependency tree, however, we also can look down to the dependency tree with a similar motivation. Essentially, given an alignment between a source word s i and a target word t j , how many children of s i and t j are aligned together? For example, \"tshyr\" and \"refers\" have 2 aligned children which are \"ayda-also\" and \"aly-to\" as shown in Figure 3c .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 423,
                        "end": 425,
                        "text": "3c",
                        "ref_id": "FIGREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Child-Father agreement:",
                "sec_num": null
            },
            {
                "text": "The SMT engine is a phrase-based system similar to the description in (Tillmann, 2006) , where various features are combined within a log-linear framework. These features include source-to-target phrase translation score, source-to-target and target-to-source wordto-word translation scores, language model score, distortion model scores and word count. The training data for these features are 7M Arabic-English sentence pairs, mostly newswire and UN corpora released by LDC. The parallel sentences have word alignment automatically generated with HMM and MaxEnt word aligner (Ge, 2004; Ittycheriah and Roukos, 2005) . Bilingual phrase translations are extracted from these word-aligned parallel corpora. The language model is a 5-gram model trained on roughly 3.5 billion English words.",
                "cite_spans": [
                    {
                        "start": 70,
                        "end": 86,
                        "text": "(Tillmann, 2006)",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 577,
                        "end": 587,
                        "text": "(Ge, 2004;",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 588,
                        "end": 617,
                        "text": "Ittycheriah and Roukos, 2005)",
                        "ref_id": "BIBREF10"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Arabic-English translation system",
                "sec_num": "4.1"
            },
            {
                "text": "Our training data contains 72k sentences Arabic-English machine translation with human corrections which include of 2.2M words in newswire and weblog domains. We have a development set of 2,707 sentences, 80K words (dev); an unseen test set of 2,707 sentences, 79K words (test). Feature selection and parameter tuning has been done on the development set in which we experimented values of C, n and iterations in range of [0.5:10], [1:5], and [50:200] respectively. The final MIRA classifier was trained by using pocket crf toolkit 1 with 100 iterations, hyper-parameter C was 5 and cut-off feature threshold n was 1.",
                "cite_spans": [
                    {
                        "start": 432,
                        "end": 451,
                        "text": "[1:5], and [50:200]",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Arabic-English translation system",
                "sec_num": "4.1"
            },
            {
                "text": "We use precision (P ), recall (R) and F-score (F ) to evaluate the classifier performance and they are com-1 http://pocket-crf-1.sourceforge.net/ puted as follow:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Arabic-English translation system",
                "sec_num": "4.1"
            },
            {
                "text": "P =",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Arabic-English translation system",
                "sec_num": "4.1"
            },
            {
                "text": "the number of correctly tagged labels the number of tagged labels R = the number of correctly tagged labels the number of reference labels",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Arabic-English translation system",
                "sec_num": "4.1"
            },
            {
                "text": "F = 2*P*R P+R (8)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Arabic-English translation system",
                "sec_num": "4.1"
            },
            {
                "text": "We designed our experiments to show the impact of each feature separately as well as their cumulative impact. We trained two types of classifiers to predict the error type of each word in MT output, namely Good/Bad with a binary classifier and Good/Insertion/Substitution/Shift with a 4-class classifier. Each classifier is trained with different feature sets as follow:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Contribution of feature sets",
                "sec_num": "4.2"
            },
            {
                "text": "\u2022 WPP: we reimplemented WPP calculation based on n-best lists as described in (Ueffing and Ney, 2007) .",
                "cite_spans": [
                    {
                        "start": 78,
                        "end": 101,
                        "text": "(Ueffing and Ney, 2007)",
                        "ref_id": "BIBREF25"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Contribution of feature sets",
                "sec_num": "4.2"
            },
            {
                "text": "\u2022 WPP + target POS: only WPP and target POS features are used. This is a similar feature set used by Xiong et al. (2010) .",
                "cite_spans": [
                    {
                        "start": 101,
                        "end": 120,
                        "text": "Xiong et al. (2010)",
                        "ref_id": "BIBREF27"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Contribution of feature sets",
                "sec_num": "4.2"
            },
            {
                "text": "\u2022 Our features: the classifier has source side, alignment context, and dependency structure features; WPP and target POS features are excluded.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Contribution of feature sets",
                "sec_num": "4.2"
            },
            {
                "text": "\u2022 WPP + our features: adding our features on top of WPP.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Contribution of feature sets",
                "sec_num": "4.2"
            },
            {
                "text": "\u2022 WPP + target POS + our features: using all features. To evaluate the effectiveness of each feature set, we apply them on two different baseline systems: using WPP and WPP+target POS, respectively. We augment each baseline with our feature sets separately. Table 1 shows the contribution in F-score of our proposed feature sets. Improvements are consistently obtained when combining the proposed features with baseline features. Experimental results also indicate that sourceside information, alignment context and dependency structures have unique and effective levers to improve the classifier performance. Among the three proposed feature sets, we observe the source side information contributes the most gain, which is followed by the alignment context and dependency structure features.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Contribution of feature sets",
                "sec_num": "4.2"
            },
            {
                "text": "We trained several classifiers with our proposed feature sets as well as baseline features. We compare their performances, including a naive baseline All-Good classifier, in which all words in the MT output are labelled as good translations. Figure 4 shows the performance of different classifiers trained with different feature sets on development and unseen test sets. On the unseen test set our proposed features outperform WPP and target POS features by 2.8 and 2.4 absolute F-score respectively. Improvements of our features are consistent in development and unseen sets as well as in binary and 4-class classifiers. We reach the best performance by combining our proposed features with WPP and target POS features. Experiments indicate that the gaps in Fscore between our best system with the naive All-Good system is 12.9 and 6.8 in binary and 4-class cases, respectively. Table 2 presents precision, recall, and Fscore of individual class of the best binary and 4-class classifiers. It shows that Good label is better predicted than other labels, meanwhile, Substitution is generally easier to predict than Insertion and Shif t.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 249,
                        "end": 250,
                        "text": "4",
                        "ref_id": "FIGREF4"
                    },
                    {
                        "start": 886,
                        "end": 887,
                        "text": "2",
                        "ref_id": "TABREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Performance of classifiers",
                "sec_num": "4.3"
            },
            {
                "text": "We estimate sentence level confidence score based on Equation 7. Figure 5 illustrates the correlation between our proposed goodness sentence level confidence score and the human-targeted translation edit rate (HTER). The Pearson correlation between goodness and HTER is 0.6, while the correlation of WPP and HTER is 0.52. This experiment shows that goodness has a large correlation with HTER. bars are thresholds used to visualize good and bad sentences respectively. We also experimented goodness computation in Equation 7 using geometric mean and harmonic mean; their Pearson correlation values are 0.5 and 0.35 respectively.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 72,
                        "end": 73,
                        "text": "5",
                        "ref_id": "FIGREF5"
                    }
                ],
                "eq_spans": [],
                "section": "Correlation between Goodness and HTER",
                "sec_num": "4.4"
            },
            {
                "text": "Experiments reporting in Section 4 indicate that the proposed confidence measure has a high correlation with HTER. However, it is not very clear if the core MT system can benefit from confidence measure by providing better translations. To investigate this question we present experimental results for the n-best list reranking task.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Improving MT quality with N-best list reranking",
                "sec_num": "5"
            },
            {
                "text": "The MT system generates top n hypotheses and for each hypothesis we compute sentence-level confidence scores. The best candidate is the hypothesis with highest confidence score. Table 3 shows the performance of reranking systems using goodness scores from our best classifier in various n-best sizes. We obtained 0.7 TER reduction and 0.4 BLEU point improvement on the development set with a 5-best list. On the unseen test, we obtained 0.6 TER reduction and 0.2 BLEU point improvement. Although, the improvement of BLEU score ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 184,
                        "end": 185,
                        "text": "3",
                        "ref_id": "TABREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Improving MT quality with N-best list reranking",
                "sec_num": "5"
            },
            {
                "text": "Besides the application of confidence score in the nbest list reranking task, we propose a method to visualize translation error using confidence scores. Our purpose is to visualize word and sentence-level confidence scores with the following objectives 1) easy for spotting translations errors; 2) simple and intuitive; and 3) helpful for post-editing productivity. We define three categories of translation quality (good/bad/decent) on both word and sentence level. On word level, the marginal probability of good label is used to visualize translation errors as follow: On sentence level, the goodness score is used as follow: Different font sizes and colors are used to catch the attention of post-editors whenever translation errors are likely to appear as shown in Table 4 . Colors are applied on word level, while font size is applied on both word and sentence level. The idea of using font size and colour to visualize translation confidence is similar to the idea of using tag/word cloud to describe the content of websites2 . The reason we are using big font size and red color is to attract post-editors' attention and help them find translation errors quickly. Figure 7 shows an example of visualizing confidence scores by font size and colours. It shows that \"not to deprive yourself \", displayed in big font and red color, is likely to be bad translations. Meanwhile, other words, such as \"you\", \"different\", \"from\", and \"assimilation\", displayed in small font and black color, are likely to be good translation. Medium font and orange color words are decent translations. MT output Source the poll also showed that most of the participants in the developing countries are ready to introduce qualitative changes in the pattern of their lives for the sake of reducing the effects of climate change.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 777,
                        "end": 778,
                        "text": "4",
                        "ref_id": "TABREF5"
                    },
                    {
                        "start": 1180,
                        "end": 1181,
                        "text": "7",
                        "ref_id": "FIGREF9"
                    }
                ],
                "eq_spans": [],
                "section": "Visualizing translation errors",
                "sec_num": "6"
            },
            {
                "text": "L i = \uf8f1 \uf8f2 \uf8f3 good if p(y i = Good|S) \u2265 0.8 bad if p(y i = Good|S) \u2264 0.45 decent otherwise",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Visualizing translation errors",
                "sec_num": "6"
            },
            {
                "text": "L S = \uf8f1 \uf8f2 \uf8f3 good if",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Visualizing translation errors",
                "sec_num": "6"
            },
            {
                "text": "We predict and visualize the survey also showed that most of the participants in developing countries are ready to introduce changes to the quality of their lifestyle in order to reduce the effects of climate change . ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Visualizing translation errors",
                "sec_num": "6"
            },
            {
                "text": "In this paper we proposed a method to predict confidence scores for machine translated words and sentences based on a feature-rich classifier using linguistic and context features. Our major contributions are three novel feature sets including source side information, alignment context, and dependency structures. Experimental results show that by combining the source side information, alignment context, and dependency structure features with word posterior probability and target POS context (Ueffing & Ney 2007; Xiong et al., 2010) , the MT error prediction accuracy is increased from 69.1 to 72.2 in F-score. Our framework is able to predict error types namely insertion, substitution and shift. The Pearson correlation with human judgement increases from 0.52 to 0.6. Furthermore, we show that the proposed confidence scores can help the MT system to select better translations and as a result improvements between 0.4 and 0.9 TER reduction are obtained. Finally, we demonstrate a prototype to visualize translation errors. This work can be expanded in several directions. First, we plan to apply confidence estimation to perform a second-pass constraint decoding. After the first pass decoding, our confidence estimation model can label which word is likely to be correctly translated. The second-pass decoding utilizes the confidence informa-tion to constrain the search space and hopefully can find a better hypothesis than in the first pass. This idea is very similar to the multi-pass decoding strategy employed by speech recognition engines. Moreover, we also intend to perform a user study on our visualization prototype to see if it increases the productivity of post-editors.",
                "cite_spans": [
                    {
                        "start": 496,
                        "end": 516,
                        "text": "(Ueffing & Ney 2007;",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 517,
                        "end": 536,
                        "text": "Xiong et al., 2010)",
                        "ref_id": "BIBREF27"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusions",
                "sec_num": "7"
            },
            {
                "text": "http://en.wikipedia.org/wiki/Tag cloud",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "We would like to thank Christoph Tillmann and the IBM machine translation team for their supports. Also, we would like to thank anonymous reviewers, Qin Gao, Joy Zhang, and Stephan Vogel for their helpful comments.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgements",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "The CMU TransTac 2007 Eyes-free and Hands-free Two-way Speech-to-Speech Translation System. In Proceedings of the IWSLT'07",
                "authors": [
                    {
                        "first": "Nguyen",
                        "middle": [],
                        "last": "Bach",
                        "suffix": ""
                    },
                    {
                        "first": "Matthias",
                        "middle": [],
                        "last": "Eck",
                        "suffix": ""
                    },
                    {
                        "first": "Paisarn",
                        "middle": [],
                        "last": "Charoenpornsawat",
                        "suffix": ""
                    },
                    {
                        "first": "Thilo",
                        "middle": [],
                        "last": "Khler",
                        "suffix": ""
                    },
                    {
                        "first": "Sebastian",
                        "middle": [],
                        "last": "Stker",
                        "suffix": ""
                    },
                    {
                        "first": "Thuylinh",
                        "middle": [],
                        "last": "Nguyen",
                        "suffix": ""
                    },
                    {
                        "first": "Roger",
                        "middle": [],
                        "last": "Hsiao",
                        "suffix": ""
                    },
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Waibel",
                        "suffix": ""
                    },
                    {
                        "first": "Stephan",
                        "middle": [],
                        "last": "Vogel",
                        "suffix": ""
                    },
                    {
                        "first": "Tanja",
                        "middle": [],
                        "last": "Schultz",
                        "suffix": ""
                    },
                    {
                        "first": "Alan",
                        "middle": [],
                        "last": "Black",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Nguyen Bach, Matthias Eck, Paisarn Charoenpornsawat, Thilo Khler, Sebastian Stker, ThuyLinh Nguyen, Roger Hsiao, Alex Waibel, Stephan Vogel, Tanja Schultz, and Alan Black. 2007. The CMU TransTac 2007 Eyes-free and Hands-free Two-way Speech-to-Speech Translation System. In Proceedings of the IWSLT'07, Trento, Italy.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Sourceside dependency tree reordering models with subtree movements and constraints",
                "authors": [
                    {
                        "first": "Nguyen",
                        "middle": [],
                        "last": "Bach",
                        "suffix": ""
                    },
                    {
                        "first": "Qin",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Stephan",
                        "middle": [],
                        "last": "Vogel",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of the",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Nguyen Bach, Qin Gao, and Stephan Vogel. 2009. Source- side dependency tree reordering models with subtree movements and constraints. In Proceedings of the",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Confidence estimation for machine translation",
                "authors": [
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Blatz",
                        "suffix": ""
                    },
                    {
                        "first": "Erin",
                        "middle": [],
                        "last": "Fitzgerald",
                        "suffix": ""
                    },
                    {
                        "first": "George",
                        "middle": [],
                        "last": "Foster",
                        "suffix": ""
                    },
                    {
                        "first": "Simona",
                        "middle": [],
                        "last": "Gandrabur",
                        "suffix": ""
                    },
                    {
                        "first": "Cyril",
                        "middle": [],
                        "last": "Goutte",
                        "suffix": ""
                    },
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Kulesza",
                        "suffix": ""
                    },
                    {
                        "first": "Alberto",
                        "middle": [],
                        "last": "Sanchis",
                        "suffix": ""
                    },
                    {
                        "first": "Nicola",
                        "middle": [],
                        "last": "Ueffing",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "The JHU Workshop Final Report",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "John Blatz, Erin Fitzgerald, George Foster, Simona Gan- drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and Nicola Ueffing. 2004. Confidence estimation for machine translation. In The JHU Workshop Final Report, Balti- more, Maryland, USA, April.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "001 new features for statistical machine translation",
                "authors": [
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Chiang",
                        "suffix": ""
                    },
                    {
                        "first": "Kevin",
                        "middle": [],
                        "last": "Knight",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of HLT-ACL",
                "volume": "11",
                "issue": "",
                "pages": "218--226",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "David Chiang, Kevin Knight, and Wei Wang. 2009. 11,001 new features for statistical machine translation. In Pro- ceedings of HLT-ACL, pages 218-226, Boulder, Colorado, June. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Ultraconservative online algorithms for multiclass problems",
                "authors": [
                    {
                        "first": "Koby",
                        "middle": [],
                        "last": "Crammer",
                        "suffix": ""
                    },
                    {
                        "first": "Yoram",
                        "middle": [],
                        "last": "Singer",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Journal of Machine Learning Research",
                "volume": "3",
                "issue": "",
                "pages": "951--991",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Koby Crammer and Yoram Singer. 2003. Ultraconservative online algorithms for multiclass problems. Journal of Ma- chine Learning Research, 3:951-991.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Max-posterior HMM alignment for machine translation",
                "authors": [
                    {
                        "first": "Niyu",
                        "middle": [],
                        "last": "Ge",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Presentation given at DARPA/TIDES NIST MT Evaluation workshop",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Niyu Ge. 2004. Max-posterior HMM alignment for machine translation. In Presentation given at DARPA/TIDES NIST MT Evaluation workshop.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Improving arabic-chinese statistical machine translation using english as pivot language",
                "authors": [
                    {
                        "first": "Nizar",
                        "middle": [],
                        "last": "Habash",
                        "suffix": ""
                    },
                    {
                        "first": "Jun",
                        "middle": [],
                        "last": "Hu",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of the 4th Workshop on Statistical Machine Translation",
                "volume": "",
                "issue": "",
                "pages": "173--181",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Nizar Habash and Jun Hu. 2009. Improving arabic-chinese statistical machine translation using english as pivot lan- guage. In Proceedings of the 4th Workshop on Statisti- cal Machine Translation, pages 173-181, Morristown, NJ, USA. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Combination of machine translation systems via hypothesis selection from combined n-best lists",
                "authors": [
                    {
                        "first": "Silja",
                        "middle": [],
                        "last": "Almut",
                        "suffix": ""
                    },
                    {
                        "first": "Stephan",
                        "middle": [],
                        "last": "Hildebrand",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Vogel",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Proceedings of the 8th Conference of the AMTA",
                "volume": "",
                "issue": "",
                "pages": "254--261",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Almut Silja Hildebrand and Stephan Vogel. 2008. Combi- nation of machine translation systems via hypothesis se- lection from combined n-best lists. In Proceedings of the 8th Conference of the AMTA, pages 254-261, Waikiki, Hawaii, October.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Confidence measure for word alignment",
                "authors": [
                    {
                        "first": "Fei",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of the ACL-IJCNLP '09",
                "volume": "",
                "issue": "",
                "pages": "932--940",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Fei Huang. 2009. Confidence measure for word align- ment. In Proceedings of the ACL-IJCNLP '09, pages 932-940, Morristown, NJ, USA. Association for Compu- tational Linguistics.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "A maximum entropy word aligner for arabic-english machine translation",
                "authors": [
                    {
                        "first": "Abraham",
                        "middle": [],
                        "last": "Ittycheriah",
                        "suffix": ""
                    },
                    {
                        "first": "Salim",
                        "middle": [],
                        "last": "Roukos",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of the HTL-EMNLP'05",
                "volume": "",
                "issue": "",
                "pages": "89--96",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Abraham Ittycheriah and Salim Roukos. 2005. A maximum entropy word aligner for arabic-english machine transla- tion. In Proceedings of the HTL-EMNLP'05, pages 89- 96, Morristown, NJ, USA. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Moses: Open source toolkit for statistical machine translation",
                "authors": [
                    {
                        "first": "Philipp",
                        "middle": [],
                        "last": "Koehn",
                        "suffix": ""
                    },
                    {
                        "first": "Hieu",
                        "middle": [],
                        "last": "Hoang",
                        "suffix": ""
                    },
                    {
                        "first": "Alexandra",
                        "middle": [],
                        "last": "Birch",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Callison-Burch",
                        "suffix": ""
                    },
                    {
                        "first": "Marcello",
                        "middle": [],
                        "last": "Federico",
                        "suffix": ""
                    },
                    {
                        "first": "Nicola",
                        "middle": [],
                        "last": "Bertoldi",
                        "suffix": ""
                    },
                    {
                        "first": "Brooke",
                        "middle": [],
                        "last": "Cowan",
                        "suffix": ""
                    },
                    {
                        "first": "Wade",
                        "middle": [],
                        "last": "Shen",
                        "suffix": ""
                    },
                    {
                        "first": "Christine",
                        "middle": [],
                        "last": "Moran",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Zens",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Dyer",
                        "suffix": ""
                    },
                    {
                        "first": "Ondrej",
                        "middle": [],
                        "last": "Bojar",
                        "suffix": ""
                    },
                    {
                        "first": "Alexandra",
                        "middle": [],
                        "last": "Constantin",
                        "suffix": ""
                    },
                    {
                        "first": "Evan",
                        "middle": [],
                        "last": "Herbst",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Proceedings of ACL'07",
                "volume": "",
                "issue": "",
                "pages": "177--180",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of ACL'07, pages 177-180, Prague, Czech Republic, June.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Improving word alignment using syntactic dependencies",
                "authors": [
                    {
                        "first": "Yanjun",
                        "middle": [],
                        "last": "Ma",
                        "suffix": ""
                    },
                    {
                        "first": "Sylwia",
                        "middle": [],
                        "last": "Ozdowska",
                        "suffix": ""
                    },
                    {
                        "first": "Yanli",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Andy",
                        "middle": [],
                        "last": "Way",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Proceedings of the ACL-08: HLT SSST-2",
                "volume": "",
                "issue": "",
                "pages": "69--77",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yanjun Ma, Sylwia Ozdowska, Yanli Sun, and Andy Way. 2008. Improving word alignment using syntactic depen- dencies. In Proceedings of the ACL-08: HLT SSST-2, pages 69-77, Columbus, OH.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Generating typed dependency parses from phrase structure parses",
                "authors": [
                    {
                        "first": "Marie-Catherine",
                        "middle": [],
                        "last": "Marneffe",
                        "suffix": ""
                    },
                    {
                        "first": "Bill",
                        "middle": [],
                        "last": "Maccartney",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Proceedings of LREC'06",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Marie-Catherine Marneffe, Bill MacCartney, and Christopher Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of LREC'06, Genoa, Italy.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Flexible text segmentation with structured multilabel classification",
                "authors": [
                    {
                        "first": "Ryan",
                        "middle": [],
                        "last": "Mcdonald",
                        "suffix": ""
                    },
                    {
                        "first": "Koby",
                        "middle": [],
                        "last": "Crammer",
                        "suffix": ""
                    },
                    {
                        "first": "Fernando",
                        "middle": [],
                        "last": "Pereira",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "987--994",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Flexible text segmentation with structured mul- tilabel classification. In Proceedings of Human Lan- guage Technology Conference and Conference on Empiri- cal Methods in Natural Language Processing, pages 987- 994, Vancouver, British Columbia, Canada, October. As- sociation for Computational Linguistics.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "BLEU: A method for automatic evaluation of machine translation",
                "authors": [
                    {
                        "first": "Kishore",
                        "middle": [],
                        "last": "Papineni",
                        "suffix": ""
                    },
                    {
                        "first": "Salim",
                        "middle": [],
                        "last": "Roukos",
                        "suffix": ""
                    },
                    {
                        "first": "Todd",
                        "middle": [],
                        "last": "Ward",
                        "suffix": ""
                    },
                    {
                        "first": "Wei-Jing",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Proceedings of ACL'02",
                "volume": "",
                "issue": "",
                "pages": "311--318",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In Proceedings of ACL'02, pages 311-318, Philadelphia, PA, July.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Training a sentence-level machine translation confidence measure",
                "authors": [
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Quirk",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proceedings of the 4th LREC",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chris Quirk. 2004. Training a sentence-level machine trans- lation confidence measure. In Proceedings of the 4th LREC.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Error detection for statistical machine translation using linguistic features",
                "authors": [
                    {
                        "first": "Caroline",
                        "middle": [],
                        "last": "Sylvain Raybaud",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Lavecchia",
                        "suffix": ""
                    },
                    {
                        "first": "Kamel",
                        "middle": [],
                        "last": "Langlois",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Smaili",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of the 13th EAMT",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sylvain Raybaud, Caroline Lavecchia, David Langlois, and Kamel Smaili. 2009. Error detection for statistical ma- chine translation using linguistic features. In Proceedings of the 13th EAMT, Barcelona, Spain, May.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "A systematic cross-comparison of sequence classifiers",
                "authors": [
                    {
                        "first": "Binyamin",
                        "middle": [],
                        "last": "Rozenfeld",
                        "suffix": ""
                    },
                    {
                        "first": "Ronen",
                        "middle": [],
                        "last": "Feldman",
                        "suffix": ""
                    },
                    {
                        "first": "Moshe",
                        "middle": [],
                        "last": "Fresko",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Proceedings of the SDM",
                "volume": "",
                "issue": "",
                "pages": "563--567",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Binyamin Rozenfeld, Ronen Feldman, and Moshe Fresko. 2006. A systematic cross-comparison of sequence clas- sifiers. In Proceedings of the SDM, pages 563-567, Bethesda, MD, USA, April.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Estimation of confidence measures for machine translation",
                "authors": [
                    {
                        "first": "Alberto",
                        "middle": [],
                        "last": "Sanchis",
                        "suffix": ""
                    },
                    {
                        "first": "Alfons",
                        "middle": [],
                        "last": "Juan",
                        "suffix": ""
                    },
                    {
                        "first": "Enrique",
                        "middle": [],
                        "last": "Vidal",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Proceedings of the MT Summit XI",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alberto Sanchis, Alfons Juan, and Enrique Vidal. 2007. Esti- mation of confidence measures for machine translation. In Proceedings of the MT Summit XI, Copenhagen, Denmark.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "A new string-to-dependency machine translation algorithm with a target dependency language model",
                "authors": [
                    {
                        "first": "Libin",
                        "middle": [],
                        "last": "Shen",
                        "suffix": ""
                    },
                    {
                        "first": "Jinxi",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Ralph",
                        "middle": [],
                        "last": "Weischedel",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Proceedings of ACL-08: HLT",
                "volume": "",
                "issue": "",
                "pages": "577--585",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new string-to-dependency machine translation algorithm with a target dependency language model. In Proceedings of ACL-08: HLT, pages 577-585, Columbus, Ohio, June. As- sociation for Computational Linguistics.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "A study of translation edit rate with targeted human annotation",
                "authors": [
                    {
                        "first": "Matthew",
                        "middle": [],
                        "last": "Snover",
                        "suffix": ""
                    },
                    {
                        "first": "Bonnie",
                        "middle": [],
                        "last": "Dorr",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Schwartz",
                        "suffix": ""
                    },
                    {
                        "first": "Linnea",
                        "middle": [],
                        "last": "Micciulla",
                        "suffix": ""
                    },
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Makhoul",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Proceedings of AMTA'06",
                "volume": "",
                "issue": "",
                "pages": "223--231",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of trans- lation edit rate with targeted human annotation. In Pro- ceedings of AMTA'06, pages 223-231, August.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Trustrank: Inducing trust in automatic translations via ranking",
                "authors": [
                    {
                        "first": "Radu",
                        "middle": [],
                        "last": "Soricut",
                        "suffix": ""
                    },
                    {
                        "first": "Abdessamad",
                        "middle": [],
                        "last": "Echihabi",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the 48th ACL",
                "volume": "",
                "issue": "",
                "pages": "612--621",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Radu Soricut and Abdessamad Echihabi. 2010. Trustrank: Inducing trust in automatic translations via ranking. In Proceedings of the 48th ACL, pages 612-621, Uppsala, Sweden, July. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Improving the confidence of machine translation quality estimates",
                "authors": [
                    {
                        "first": "Lucia",
                        "middle": [],
                        "last": "Specia",
                        "suffix": ""
                    },
                    {
                        "first": "Zhuoran",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Marco",
                        "middle": [],
                        "last": "Turchi",
                        "suffix": ""
                    },
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Shawe-Taylor",
                        "suffix": ""
                    },
                    {
                        "first": "Craig",
                        "middle": [],
                        "last": "Saunders",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of the MT Summit XII",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Lucia Specia, Zhuoran Wang, Marco Turchi, John Shawe- Taylor, and Craig Saunders. 2009. Improving the con- fidence of machine translation quality estimates. In Pro- ceedings of the MT Summit XII, Ottawa, Canada.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Efficient dynamic programming search algorithms for phrase-based SMT",
                "authors": [
                    {
                        "first": "Christoph",
                        "middle": [],
                        "last": "Tillmann",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Proceedings of the Workshop on Computationally Hard Problems and Joint Inference in Speech and Language Processing",
                "volume": "",
                "issue": "",
                "pages": "9--16",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Christoph Tillmann. 2006. Efficient dynamic programming search algorithms for phrase-based SMT. In Proceedings of the Workshop on Computationally Hard Problems and Joint Inference in Speech and Language Processing, pages 9-16, Morristown, NJ, USA. Association for Computa- tional Linguistics.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Word-level confidence estimation for machine translation",
                "authors": [
                    {
                        "first": "Nicola",
                        "middle": [],
                        "last": "Ueffing",
                        "suffix": ""
                    },
                    {
                        "first": "Hermann",
                        "middle": [],
                        "last": "Ney",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Computational Linguistics",
                "volume": "33",
                "issue": "1",
                "pages": "9--40",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Nicola Ueffing and Hermann Ney. 2007. Word-level confi- dence estimation for machine translation. Computational Linguistics, 33(1):9-40.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Online large-margin training for statistical machine translation",
                "authors": [
                    {
                        "first": "Taro",
                        "middle": [],
                        "last": "Watanabe",
                        "suffix": ""
                    },
                    {
                        "first": "Jun",
                        "middle": [],
                        "last": "Suzuki",
                        "suffix": ""
                    },
                    {
                        "first": "Hajime",
                        "middle": [],
                        "last": "Tsukada",
                        "suffix": ""
                    },
                    {
                        "first": "Hideki",
                        "middle": [],
                        "last": "Isozaki",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Proceedings of the EMNLP-CoNLL",
                "volume": "",
                "issue": "",
                "pages": "764--773",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki Isozaki. 2007. Online large-margin training for statisti- cal machine translation. In Proceedings of the EMNLP- CoNLL, pages 764-773, Prague, Czech Republic, June. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Error detection for statistical machine translation using linguistic features",
                "authors": [
                    {
                        "first": "Deyi",
                        "middle": [],
                        "last": "Xiong",
                        "suffix": ""
                    },
                    {
                        "first": "Min",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Haizhou",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the 48th ACL",
                "volume": "",
                "issue": "",
                "pages": "604--611",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Deyi Xiong, Min Zhang, and Haizhou Li. 2010. Error de- tection for statistical machine translation using linguistic features. In Proceedings of the 48th ACL, pages 604- 611, Uppsala, Sweden, July. Association for Computa- tional Linguistics.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "= max(0, \u03b1) \u03b1 = min C, L(y,y )-(score(xi,y)-score(xi,y )) ||f (xi,y)-f (xi,y )|| 2 2",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Figure 1: Source-side features.",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "Figure 2: Alignment context features.",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "FIGREF3": {
                "num": null,
                "text": "Figure 3: Dependency structures features.",
                "uris": null,
                "fig_num": "3",
                "type_str": "figure"
            },
            "FIGREF4": {
                "num": null,
                "text": "Figure 4: Performance of binary and 4-class classifiers trained with different feature sets on the development and unseen test sets.",
                "uris": null,
                "fig_num": "4",
                "type_str": "figure"
            },
            "FIGREF5": {
                "num": null,
                "text": "Figure 5: Correlation between Goodness and HTER.",
                "uris": null,
                "fig_num": "5",
                "type_str": "figure"
            },
            "FIGREF6": {
                "num": null,
                "text": "Figure 6: A comparison between reranking and oracle scores with different n-best size in TER metric on the development set.",
                "uris": null,
                "fig_num": "6",
                "type_str": "figure"
            },
            "FIGREF7": {
                "num": null,
                "text": "you totally different from zaid amr , and not to deprive yourself in a basement of imitation and assimilation . different from zaid and amr , so do not cram yourself in the tunnel of simulation , imitation and assimilation .",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF8": {
                "num": null,
                "text": "showed that most of the participants in the developing countries are ready to introduce qualitative changes in the pattern of their lives for the sake of reducing the effects of climate change.",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF9": {
                "num": null,
                "text": "Figure 7: MT errors visualization based on confidence scores.",
                "uris": null,
                "fig_num": "7",
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table><tr><td>s.t. score(x, y) \u2265 score(x, y ) + L(y, y )</td></tr></table>",
                "type_str": "table",
                "text": "arg min wt+1 ||w t+1 -w t ||",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table><tr><td/><td>Label</td><td>P</td><td>R</td><td>F</td></tr><tr><td>Binary</td><td>Good Bad</td><td colspan=\"3\">74.7 80.6 77.5 68 60.1 63.8</td></tr><tr><td/><td>Good</td><td>70.8</td><td>87</td><td>78.1</td></tr><tr><td>4-class</td><td colspan=\"4\">Insertion Substitution 57.8 44.9 50.5 37.5 16.9 23.3</td></tr><tr><td/><td>Shift</td><td colspan=\"3\">35.2 14.1 20.1</td></tr></table>",
                "type_str": "table",
                "text": "The black bar is the linear regression line. Blue and red Detailed performance in precision, recall and F-score of binary and 4-class classifiers with WPP+target POS+Our features on the unseen test set.",
                "html": null,
                "num": null
            },
            "TABREF4": {
                "content": "<table><tr><td/><td/><td>Dev</td><td>Test</td><td/></tr><tr><td/><td colspan=\"4\">TER BLEU TER BLEU</td></tr><tr><td colspan=\"2\">Baseline 49.9</td><td>31.0</td><td>50.2</td><td>30.6</td></tr><tr><td>2-best</td><td>49.5</td><td>31.4</td><td>49.9</td><td>30.8</td></tr><tr><td>5-best</td><td>49.2</td><td>31.4</td><td>49.6</td><td>30.8</td></tr><tr><td>10-best</td><td>49.2</td><td>31.2</td><td>49.5</td><td>30.8</td></tr><tr><td>20-best</td><td>49.1</td><td>31.0</td><td>49.3</td><td>30.7</td></tr><tr><td>30-best</td><td>49.0</td><td>31.0</td><td>49.3</td><td>30.6</td></tr><tr><td>40-best</td><td>49.0</td><td>31.0</td><td>49.4</td><td>30.5</td></tr><tr><td>50-best</td><td>49.1</td><td>30.9</td><td>49.4</td><td>30.5</td></tr><tr><td colspan=\"2\">100-best 49.0</td><td>30.9</td><td>49.3</td><td>30.5</td></tr><tr><td colspan=\"5\">is not obvious, TER reductions are consistent in both</td></tr><tr><td colspan=\"5\">development and unseen sets. Figure 6 shows the im-</td></tr><tr><td colspan=\"5\">provement of reranking with goodness score. Besides,</td></tr><tr><td colspan=\"5\">the figure illustrates the upper and lower bound perfor-</td></tr><tr><td colspan=\"5\">mances with TER metric in which the lower bound is</td></tr><tr><td colspan=\"5\">our baseline system and the upper bound is the best hy-</td></tr><tr><td colspan=\"5\">pothesis in a given n-best list. Oracle scores of each n-</td></tr><tr><td colspan=\"5\">best list are computed by choosing the translation can-</td></tr><tr><td colspan=\"3\">didate with lowest TER score.</td><td/><td/></tr></table>",
                "type_str": "table",
                "text": "Reranking performance with goodness score.",
                "html": null,
                "num": null
            },
            "TABREF5": {
                "content": "<table><tr><td/><td colspan=\"2\">goodness(S) \u2265 0.7</td></tr><tr><td>bad</td><td colspan=\"2\">if goodness(S) \u2264 0.5</td></tr><tr><td colspan=\"2\">decent otherwise</td><td/></tr><tr><td/><td colspan=\"2\">Choices Intention</td></tr><tr><td/><td>big</td><td>bad</td></tr><tr><td>Font size</td><td>small</td><td>good</td></tr><tr><td/><td>medium</td><td>decent</td></tr><tr><td/><td>red</td><td>bad</td></tr><tr><td>Colors</td><td>black</td><td>good</td></tr><tr><td/><td>orange</td><td>decent</td></tr></table>",
                "type_str": "table",
                "text": "Choices of layout",
                "html": null,
                "num": null
            }
        }
    }
}