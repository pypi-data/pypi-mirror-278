{
    "paper_id": "P19-1352",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:10:47.432537Z"
    },
    "title": "Shared-Private Bilingual Word Embeddings for Neural Machine Translation",
    "authors": [
        {
            "first": "Xuebo",
            "middle": [],
            "last": "Liu",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Derek",
            "middle": [
                "F"
            ],
            "last": "Wong",
            "suffix": "",
            "affiliation": {},
            "email": "derekfw@um.edu.mo"
        },
        {
            "first": "Yang",
            "middle": [],
            "last": "Liu",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Tsinghua University",
                "location": {
                    "settlement": "Beijing",
                    "country": "China"
                }
            },
            "email": ""
        },
        {
            "first": "Lidia",
            "middle": [
                "S"
            ],
            "last": "Chao",
            "suffix": "",
            "affiliation": {},
            "email": "lidiasc@um.edu.mo"
        },
        {
            "first": "Tong",
            "middle": [],
            "last": "Xiao",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Northeastern University",
                "location": {
                    "settlement": "Shenyang",
                    "country": "China"
                }
            },
            "email": "xiaotong@mail.neu.edu.cn"
        },
        {
            "first": "Jingbo",
            "middle": [],
            "last": "Zhu",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Northeastern University",
                "location": {
                    "settlement": "Shenyang",
                    "country": "China"
                }
            },
            "email": "zhujingbo@mail.neu.edu.cn"
        },
        {
            "first": "\u2020",
            "middle": [],
            "last": "Nlp",
            "suffix": "",
            "affiliation": {},
            "email": ""
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Word embedding is central to neural machine translation (NMT), which has attracted intensive research interest in recent years. In NMT, the source embedding plays the role of the entrance while the target embedding acts as the terminal. These layers occupy most of the model parameters for representation learning. Furthermore, they indirectly interface via a soft-attention mechanism, which makes them comparatively isolated. In this paper, we propose shared-private bilingual word embeddings, which give a closer relationship between the source and target embeddings, and which also reduce the number of model parameters. For similar source and target words, their embeddings tend to share a part of the features and they cooperatively learn these common representation units. Experiments on 5 language pairs belonging to 6 different language families and written in 5 different alphabets demonstrate that the proposed model provides a significant performance boost over the strong baselines with dramatically fewer model parameters.",
    "pdf_parse": {
        "paper_id": "P19-1352",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Word embedding is central to neural machine translation (NMT), which has attracted intensive research interest in recent years. In NMT, the source embedding plays the role of the entrance while the target embedding acts as the terminal. These layers occupy most of the model parameters for representation learning. Furthermore, they indirectly interface via a soft-attention mechanism, which makes them comparatively isolated. In this paper, we propose shared-private bilingual word embeddings, which give a closer relationship between the source and target embeddings, and which also reduce the number of model parameters. For similar source and target words, their embeddings tend to share a part of the features and they cooperatively learn these common representation units. Experiments on 5 language pairs belonging to 6 different language families and written in 5 different alphabets demonstrate that the proposed model provides a significant performance boost over the strong baselines with dramatically fewer model parameters.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "With the introduction of ever more powerful architectures, neural machine translation (NMT) has become the most promising machine translation method (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) . For word representation, different architecturesincluding, but not limited to, recurrence-based (Chen et al., 2018) , convolution-based (Gehring et al., 2017) and transformation-based (Vaswani et al., 2017) NMT models-have been taking advantage of the distributed word embeddings to capture the syntactic and semantic properties of words (Turian et al., 2010) .",
                "cite_spans": [
                    {
                        "start": 149,
                        "end": 181,
                        "text": "(Kalchbrenner and Blunsom, 2013;",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 182,
                        "end": 205,
                        "text": "Sutskever et al., 2014;",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 206,
                        "end": 228,
                        "text": "Bahdanau et al., 2015)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 327,
                        "end": 346,
                        "text": "(Chen et al., 2018)",
                        "ref_id": null
                    },
                    {
                        "start": 367,
                        "end": 389,
                        "text": "(Gehring et al., 2017)",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 415,
                        "end": 437,
                        "text": "(Vaswani et al., 2017)",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 569,
                        "end": 590,
                        "text": "(Turian et al., 2010)",
                        "ref_id": "BIBREF31"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Figure 1 : Comparison between (a) standard word embeddings and (b) shared-private word embeddings. In (a), the English word \"Long\" and the German word \"Lange\", which have similar lexical meanings, are represented by two private d-dimension vectors. While in (b), the two word embeddings are made up of two parts, indicating the shared (lined nodes) and the private (unlined nodes) features. This enables the two words to make use of common representation units, leading to a closer relationship between them.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "NMT usually utilizes three matrices to represent source embeddings, target input embeddings, and target output embeddings (also known as pre-softmax weight), respectively. These embeddings occupy most of the model parameters, which constrains the improvements of NMT because the recent methods become increasingly memory-hungry (Vaswani et al., 2017; Chen et al., 2018) . 1 Even though converting words into subword units (Sennrich et al., 2016b) , nearly 55% of model parameters are used for word representation in the Transformer model (Vaswani et al., 2017) .",
                "cite_spans": [
                    {
                        "start": 328,
                        "end": 350,
                        "text": "(Vaswani et al., 2017;",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 351,
                        "end": 369,
                        "text": "Chen et al., 2018)",
                        "ref_id": null
                    },
                    {
                        "start": 422,
                        "end": 446,
                        "text": "(Sennrich et al., 2016b)",
                        "ref_id": null
                    },
                    {
                        "start": 538,
                        "end": 560,
                        "text": "(Vaswani et al., 2017)",
                        "ref_id": "BIBREF32"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "To overcome this difficulty, several methods are proposed to reduce the parameters used for word representation of NMT. Press and Wolf (2017) propose two weight tying (WT) methods, called decoder WT and three-way WT, to substantially reduce the parameters of the word embeddings. Decoder WT ties the target input embedding and target output embedding, which has become the new de facto standard of practical NMT (Sen- Figure 2 : Shared-private bilingual word embeddings perform between the source and target words or sub-words (a) with similar lexical meaning, (b) with same word form, and (c) without any relationship. Different sharing mechanisms are adapted into different relationship categories. This strikes the right balance between capturing monolingual and bilingual characteristics. The closeness of relationship decides the portion of features to be used for sharing. Words with similar lexical meaning tend to share more features, followed by the words with the same word form, and then the unrelated words, as illustrated by the lined nodes. nrich et al., 2017) . Three-way WT uses only one matrix to represent the three word embeddings, where the source and target words that have the same word form tend to share a word vector. This method can also be adapted to sub-word NMT with a shared source-target sub-word vocabulary and it performs well in language pairs with many of the same characters, such as English-German and English-French (Vaswani et al., 2017) . Unfortunately, this method is not applicable to languages that are written in different alphabets, such as Chinese-English (Hassan et al., 2018) .",
                "cite_spans": [
                    {
                        "start": 115,
                        "end": 141,
                        "text": "NMT. Press and Wolf (2017)",
                        "ref_id": null
                    },
                    {
                        "start": 1055,
                        "end": 1074,
                        "text": "nrich et al., 2017)",
                        "ref_id": null
                    },
                    {
                        "start": 1454,
                        "end": 1476,
                        "text": "(Vaswani et al., 2017)",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 1602,
                        "end": 1623,
                        "text": "(Hassan et al., 2018)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 425,
                        "end": 426,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Another challenge facing the source and target word embeddings of NMT is the lack of interactions. This degrades the attention performance, leading to some unaligned translations that hurt the translation quality. Hence, Kuang et al. (2018) propose to bridge the source and target embeddings, which brings better attention to the related source and target words. Their method is applicable to any language pairs, providing a tight interaction between the source and target word pairs. However, their method requires additional components and model parameters.",
                "cite_spans": [
                    {
                        "start": 221,
                        "end": 240,
                        "text": "Kuang et al. (2018)",
                        "ref_id": "BIBREF17"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this work, we aim to enhance the word representations and the interactions between the source and target words, while using even fewer parameters. To this end, we present a languageindependent method, which is called sharedprivate bilingual word embeddings, to share a part of the embeddings of a pair of source and target words that have some common characteristics (i.e. similar words should have similar vectors). Figure 1 illustrates the difference between the standard word embeddings and shared-private word embeddings of NMT. In the proposed method, each source (or target) word is represented by a word embedding that consists of the shared features and the private features. The shared features can also be regarded as the prior alignments connecting the source and target words. The private features allow the words to better learn the monolingual characteristics. Meanwhile, the features shared by the source and target embeddings result in a significant reduction of the number of parameters used for word representations. The experimental results on 6 translation datasets of different scales show that our model with fewer parameters yields consistent improvements over the strong Transformer baselines.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 427,
                        "end": 428,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In monolingual vector space, similar words tend to have commonalities in the same dimensions of their word vectors (Mikolov et al., 2013) . These commonalities include: (1) a similar degree (value) of the same dimension and (2) a similar positive or negative correlation of the same dimension. Many previous works have noticed this phenomenon and have proposed to use shared vectors to represent similar words in monolingual vector space toward model compression (Li et al., 2016; Zhang et al., 2017b; Li et al., 2018) .",
                "cite_spans": [
                    {
                        "start": 115,
                        "end": 137,
                        "text": "(Mikolov et al., 2013)",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 463,
                        "end": 480,
                        "text": "(Li et al., 2016;",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 481,
                        "end": 501,
                        "text": "Zhang et al., 2017b;",
                        "ref_id": null
                    },
                    {
                        "start": 502,
                        "end": 518,
                        "text": "Li et al., 2018)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Approach",
                "sec_num": "2"
            },
            {
                "text": "Motivated by these works, in NMT, we assume that the source and target words that have similar characteristics should also have similar vectors. Hence, we propose to perform this sharing technique in bilingual vector space. More precisely, we share the features (dimensions) between the paired source and target embeddings (vectors). However, in contrast to the previous studies, we also model the private features of the word embedding to preserve the private characteristics of words for source and target languages. The private features allow the words to better learn the monolingual characteristics. Meanwhile, we also propose to adopt different sharing mechanisms among the word pairs, which will be described in the following sections.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Approach",
                "sec_num": "2"
            },
            {
                "text": "In the Transformer architecture, the shared features between the source and target embeddings always contribute to the calculation of the attention weight. 2 This results in paying more attention strength on the pair of related words. With the help of residual connections, the high-level representations can also benefit from the shared features of the topmost embedding layers. Both qualitative and quantitative analyses show the effectiveness on the translation tasks.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Approach",
                "sec_num": "2"
            },
            {
                "text": "Standard NMT jointly learns to translate and align, which has achieved remarkable results (Bahdanau et al., 2015) . In NMT, the intention is to identify the translation relationships between the source and target words. To simplify the model, we propose to divide the relationships into three main categories between a pair of source and target words:",
                "cite_spans": [
                    {
                        "start": 90,
                        "end": 113,
                        "text": "(Bahdanau et al., 2015)",
                        "ref_id": "BIBREF1"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Shared-Private Bilingual Word Embeddings",
                "sec_num": "2.1"
            },
            {
                "text": "(1) words with similar lexical meaning (abbreviated as lm), (2) words with same word form (abbreviated as wf), and (3) unrelated words (abbreviated as ur). Figure 2 shows some examples of these different relationship categories. The number of the shared features of the word embeddings is decided by their relationships. Before presenting the pairing process in detail, we first introduce the constraints to the proposed method for convenience:",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 163,
                        "end": 164,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Shared-Private Bilingual Word Embeddings",
                "sec_num": "2.1"
            },
            {
                "text": "\u2022 Each source word is only allowed to share the features with a single target word, and vice versa.3 ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Shared-Private Bilingual Word Embeddings",
                "sec_num": "2.1"
            },
            {
                "text": "\u2022 Each source word preferentially shares features with the target word that has similar lexical meaning, followed by the word with same word form, and then unrelated words.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Shared-Private Bilingual Word Embeddings",
                "sec_num": "2.1"
            },
            {
                "text": "As shown in Figure 2 (a), the English word \"Long\" and the German word \"Lange\", which have similar meaning, tend to share more common features of their embeddings. In our model, the source and target words with alignment links are regarded as parallel words that are the translation of each other. According to the word frequency, each source word x is paired with a target aligned word \u0177 that has the highest alignment probability among the candidates, and is computed as follows:",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 19,
                        "end": 20,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Words with Similar Lexical Meaning",
                "sec_num": "2.1.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "\u0177 = arg max y\u2208a(x) logA(y|x)",
                        "eq_num": "(1)"
                    }
                ],
                "section": "Words with Similar Lexical Meaning",
                "sec_num": "2.1.1"
            },
            {
                "text": "where a(\u2022) denotes the set of aligned candidates.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Words with Similar Lexical Meaning",
                "sec_num": "2.1.1"
            },
            {
                "text": "It is worth noting the target words that have been paired with the source words cannot be used as candidates. A(\u2022|\u2022) denotes the alignment probability. These can be obtained by either the intrinsic attention mechanism (Bahdanau et al., 2015) or unsupervised word aligner (Dyer et al., 2013) .",
                "cite_spans": [
                    {
                        "start": 218,
                        "end": 241,
                        "text": "(Bahdanau et al., 2015)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 271,
                        "end": 290,
                        "text": "(Dyer et al., 2013)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Words with Similar Lexical Meaning",
                "sec_num": "2.1.1"
            },
            {
                "text": "As shown in Figure 2 (b), the sub-word \"Ju@@\" simultaneously exists in English and German sentences. This kind of word tends to share a medium number of features of the word embeddings. Most of the time, the source and target words with the same word form also share similar lexical meaning. This category of words generally includes Arabic numbers, punctuations, named entities, cognates and loanwords. However, there are some bilingual homographs where the words in the source and target languages look the same but have completely different meanings. For example, the German word \"Gift\" means \"Poison\" in English. That is the reason we propose to first pair the words with similar lexical meaning instead of those words with same word forms. This might be the potential limitation of the three-way WT method (Press and Wolf, 2017), where words with the same word form indiscriminately share the same word embedding.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 19,
                        "end": 20,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Words with Same Word Form",
                "sec_num": "2.1.2"
            },
            {
                "text": "We regard source and target words that cannot be paired with each other as unrelated words. Figure 2(c) shows an example of a pair of unrelated words. This category is mainly composed of lowfrequency words, such as misspelled words, special characters, and foreign words. In standard NMT, the embeddings of low-frequency words are usually inadequately trained, resulting in a poor word representation. These words are often treated as noises and they are generally ignored",
                "cite_spans": [
                    {
                        "start": 92,
                        "end": 103,
                        "text": "Figure 2(c)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Unrelated Words",
                "sec_num": "2.1.3"
            },
            {
                "text": "E x \u2208 R 6\u00d75 Long Long (Lange)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Unrelated Words",
                "sec_num": "2.1.3"
            },
            {
                "text": "Italy Italy (Italien)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Unrelated Words",
                "sec_num": "2.1.3"
            },
            {
                "text": "E x lm \u2208 R 2\u00d75 S lm \u2208 R 2\u00d73 P x lm \u2208 R 2\u00d72 \u2295 \u2192 \u2295 Ju@@(",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Unrelated Words",
                "sec_num": "2.1.3"
            },
            {
                "text": "De@@( (Ju@@) Ju@@ De@@ (De@@) by the NMT systems (Feng et al., 2017) . Motivated by the frequency clustering methods proposed by Chen et al. (2016) where they cluster the words with similar frequency for training a hierarchical language model, in this work, we propose to use a small vector to model the possible features that might be shared between the source and target words which are unrelated but having similar word frequencies. In addition, it can be regarded as a way to improve the robustness of learning the embeddings of low-frequency words because of the noisy dimensions (Wang et al., 2018) .",
                "cite_spans": [
                    {
                        "start": 49,
                        "end": 68,
                        "text": "(Feng et al., 2017)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 129,
                        "end": 147,
                        "text": "Chen et al. (2016)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 585,
                        "end": 604,
                        "text": "(Wang et al., 2018)",
                        "ref_id": "BIBREF33"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Unrelated Words",
                "sec_num": "2.1.3"
            },
            {
                "text": "E x wf \u2208 R 2\u00d75 S wf \u2208 R 2\u00d72 P x wf \u2208 R 2\u00d73 \u2295 \u2192 \u2192 \u2295 Laden (Bericht) Sundial (Fiehlt) Laden Sundial E x ur \u2208 R 2\u00d75 S ur \u2208 R 2\u00d71 P x ur \u2208 R 2\u00d74 \u2295 \u2192",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Unrelated Words",
                "sec_num": "2.1.3"
            },
            {
                "text": "Before looking up embedding at each training step, the source and target embedding matrix are assembled by the sub-embedding matrices. As shown in Figure 3 , the source embedding E x \u2208 R |V |\u00d7d is computed as follows::",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 154,
                        "end": 155,
                        "text": "3",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Implementation",
                "sec_num": "2.2"
            },
            {
                "text": "E x = E x lm \u2295 E x wf \u2295 E x ur (2)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementation",
                "sec_num": "2.2"
            },
            {
                "text": "where \u2295 is the row concatenation operator. ) |\u00d7d represents the word embeddings of the source words belong to different categories, e.g. lm represents the words with similar lexical meaning. |V (\u2022) | denotes the vocabulary size of the corresponding category.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementation",
                "sec_num": "2.2"
            },
            {
                "text": "E x (\u2022) \u2208 R |V (\u2022",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementation",
                "sec_num": "2.2"
            },
            {
                "text": "The process of feature sharing is also implemented by matrix concatenation. For example, the embedding matrices of the source words with similar lexical meaning are computed as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementation",
                "sec_num": "2.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "E x lm = S lm \u2295P x lm (",
                        "eq_num": "3"
                    }
                ],
                "section": "Implementation",
                "sec_num": "2.2"
            },
            {
                "text": ")",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementation",
                "sec_num": "2.2"
            },
            {
                "text": "where \u2295 is the column concatenation operator. S lm \u2208 R |V lm |\u00d7\u03bb lm d represent the word embeddings of the shared features, where \u03bb lm denotes the proportion of the features for sharing in this relationship category. P x lm \u2208 R |V lm |\u00d7(1-\u03bb lm )d represent the word embeddings of the private features.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementation",
                "sec_num": "2.2"
            },
            {
                "text": "Similar to the target word embedding. These matrix concatenation operations, which have low computational complexity, are very cheap to the whole NMT computation process. We also empirically find both the training speed and decoding speed are not influenced with the introduction of the proposed method.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementation",
                "sec_num": "2.2"
            },
            {
                "text": "We carry out our experiments on the small-scale IWSLT'17 {Arabic (Ar), Japanese (Ja), Korean (Ko), Chinese (Zh)}-to-English (En) translation tasks, medium-scale NIST Chinese-English (Zh-En) translation task, and large-scale WMT'14 English-German (En-De) translation task.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "3"
            },
            {
                "text": "For the IWSLT {Ar, Ja, Ko, Zh}-to-En translation tasks, there are respectively 236K, 234K, 227K, and 235K sentence pairs in each training set. 4 The validation set is IWSLT17.TED.tst2014 and the test set is IWSLT17.TED.tst2015. For each language, we learn a BPE model with 16K merge operations (Sennrich et al., 2016b) .",
                "cite_spans": [
                    {
                        "start": 294,
                        "end": 318,
                        "text": "(Sennrich et al., 2016b)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "3"
            },
            {
                "text": "For the NIST Zh-En translation task, the training corpus consists of 1.25M sentence pairs with 27.9M Chinese words and 34.5M English words. We use the NIST MT06 dataset as the validation set and the test sets are the NIST MT02, MT03, MT04, MT05, MT08 datasets. To compare with the recent works, the vocabulary size is limited to 30K for both languages, covering 97.7% Chinese words and 99.3% English words, respectively. For the WMT En-De translation task, the training set contains 4.5M sentence pairs with 107M English words and 113M German words. We use the newstest13 and newstest14 as the validation set and test set, respectively. The joint BPE model is set to 32K merge operations.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "3"
            },
            {
                "text": "We implement all of the methods based on Transformer (Vaswani et al., 2017) using the base setting with the open-source toolkit thumt 5 (Zhang et al., 2017a) . There are six encoder and decoder layers in our models, while each layer employs eight parallel attention heads. The dimension of the word embedding and the high-level representation d model is 512, while that of the inner-FFN layer d ff is 2048. The Adam (Kingma and Ba, 2015) optimizer is used to update the model parameters with hyper-parameters \u03b2 1 = 0.9, \u03b2 2 = 0.98, \u03b5 = 10 -8 and a warm-up strategy with warmup steps = 4000 is adapted to the variable learning rate (Vaswani et al., 2017) . The dropout used in the residual connection, attention mech- 3 : Results on the IWSLT {Ar, Ja, Ko, Zh}-to-En translation tasks. These distant language pairs belonging to 5 different language families and written in 5 different alphabets.\"\u2191\" indicates the result is significantly better than that of the vanilla Transformer (p < 0.01).",
                "cite_spans": [
                    {
                        "start": 53,
                        "end": 75,
                        "text": "(Vaswani et al., 2017)",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 136,
                        "end": 157,
                        "text": "(Zhang et al., 2017a)",
                        "ref_id": null
                    },
                    {
                        "start": 416,
                        "end": 437,
                        "text": "(Kingma and Ba, 2015)",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 631,
                        "end": 653,
                        "text": "(Vaswani et al., 2017)",
                        "ref_id": "BIBREF32"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 717,
                        "end": 718,
                        "text": "3",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Setup",
                "sec_num": "3.1"
            },
            {
                "text": "anism, and feed-forward layer is set to 0.1. We employ uniform label smoothing with 0.1 uncertainty.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Setup",
                "sec_num": "3.1"
            },
            {
                "text": "During the training, each training batch contains nearly 25K source and target tokens. We evaluate the models every 2000 batches via the tokenized BLEU (Papineni et al., 2002) for early stopping. During the testing, we use the best single model for decoding with a beam of 4. The length penalty is tuned on the validation set, which is set to 0.6 for the English-German translation tasks, and 1.0 for others.",
                "cite_spans": [
                    {
                        "start": 152,
                        "end": 175,
                        "text": "(Papineni et al., 2002)",
                        "ref_id": "BIBREF25"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Setup",
                "sec_num": "3.1"
            },
            {
                "text": "We compare our proposed methods with the following related works:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Setup",
                "sec_num": "3.1"
            },
            {
                "text": "\u2022 Direct bridging (Kuang et al., 2018) : this method minimizes the word embedding loss between the transformations of the target words and their aligned source words by adding an auxiliary objective function. sent the target input embedding and target output embedding.",
                "cite_spans": [
                    {
                        "start": 18,
                        "end": 38,
                        "text": "(Kuang et al., 2018)",
                        "ref_id": "BIBREF17"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Setup",
                "sec_num": "3.1"
            },
            {
                "text": "\u2022 Three-way WT (Press and Wolf, 2017): this method is an extension of the decoder WT method that the source embedding and the two target embeddings are represented by one embedding matrix. This method cannot be applied to the language pairs with different alphabets, e.g. Zh-En.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Setup",
                "sec_num": "3.1"
            },
            {
                "text": "For the proposed model, we use an unsupervised word aligner fast-align6 (Dyer et al., 2013) to pair source and target words that have similar lexical meaning. We set the threshold of alignment probability to 0.05, i.e. only those words with an alignment probability over 0.05 can be paired as the words having similar lexical meaning. The sharing coefficient \u03bb = (\u03bb lm , \u03bb wf , \u03bb wf ) is set to (0.9,0.7,0.5), which is tuned on both the NIST Chinese-Enlgish task and the WMT English-German task.",
                "cite_spans": [
                    {
                        "start": 72,
                        "end": 91,
                        "text": "(Dyer et al., 2013)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Setup",
                "sec_num": "3.1"
            },
            {
                "text": "Table 1 reports the results on the NIST Chinese-English test sets. It is observed that the Transformer models significantly outperform SMT and RNNsearch models. Therefore, we decide to implement all of our experiments based on Transformer architecture. The direct bridging model can further improve the translation quality of the Transformer baseline. The decoder WT model improves the translation quality while reducing the number of parameters for the word representation. This improved performance happens because there are fewer model parameters, which prevents over-fitting (Press and Wolf, 2017) . Finally, the performance is further improved by the proposed method while using even fewer parameters than other models. Similar observations are obtained on the English-German translation task, as shown in Table 2. The improvement of the direct bridging model is reduced with the introduction of sub-word units since the attention distribution of the high-level representations becomes more confused. Although the two WT methods use fewer parameters, their translation quality degrades. We believe that sub-word NMT needs the well-trained embeddings to distinguish the homographs of subwords. In the proposed method, both the source and target embeddings benefit from the shared features, which leads to better word representations. Hence, it improves the quality of translation and also reduces the number of parameters.",
                "cite_spans": [
                    {
                        "start": 579,
                        "end": 601,
                        "text": "(Press and Wolf, 2017)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "1",
                        "ref_id": "TABREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Main Results",
                "sec_num": "3.2"
            },
            {
                "text": "Table 3 shows the results on the small-scale IWSLT translation tasks. We observe that the proposed method stays consistently better than the vanilla model on these distant language pairs. Although the Three-way WT method has been sufficiently validated on similar translation pairs at low-resource settings (Sennrich et al., 2016a) , it is not applicable to these distant language pairs. Instead, the proposed method is language-independent, making the WT methods more widely used.",
                "cite_spans": [
                    {
                        "start": 307,
                        "end": 331,
                        "text": "(Sennrich et al., 2016a)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "3",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Main Results",
                "sec_num": "3.2"
            },
            {
                "text": "The coefficient \u03bb = (\u03bb lm , \u03bb wf , \u03bb ur ) controls the proportion of the shared features. As shown in Table 4 , the decoder WT model can be seen as a kind of shared-private method where zero features are shared between the source and target word embeddings. For the proposed method, \u03bb = (0.5, 0.5, 0.5) and \u03bb = (1, 1, 1) are, respectively, used for sharing half and all features between the embeddings of all categories of words. This allows the model to significantly reduce the number of parameters and also improve the translation quality. For comparison purpose, we also consider sharing a large part of the features among the unrelated words by setting s 3 to 0.9, i.e. \u03bb = (0.5, 0.7, 0.9). We argue that it is hard for 1 Source mengmai xingzheng zhangguan bazhake biaoshi , dan shi gaishi jiu you shisan sangsheng . Reference mumbai municipal commissioner phatak claimed that 13 people were killed in the city alone . Vanilla bombay chief executive said that there were only 13 deaths in the city alone . Direct bridging bombay 's chief executive , said there were 13 dead in the city alone . Decoder WT chief executive of bombay , said that thirteen people had died in the city alone . Shared-private mumbai 's chief executive said 13 people were killed in the city alone .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 108,
                        "end": 109,
                        "text": "4",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Effect on Sharing Coefficients",
                "sec_num": "3.3"
            },
            {
                "text": "2 Source suoyi wo ye you liyou qu xiangxin ta de rensheng ye hen jingcai . Reference thus , i also have reason to believe that her life is also very wonderful . Vanilla so i have reason to believe her life is also very fantastic . Direct bridging so i had reason to believe her life was also brilliant . Decoder WT so , i have reasons to believe that she has a wonderful life . Shared-private so i also have reason to believe that her life is also wonderful . the model to learn an appropriate bilingual vector space in such a sharing setting.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Effect on Sharing Coefficients",
                "sec_num": "3.3"
            },
            {
                "text": "Finally, we propose to share more features between the more similar words by using s 1 = 0.9 and reduce the weight on the unrelated words, which is \u03bb = (0.9, 0.7, 0.5). This strikes the right balance between the translation quality and the number of model parameters. To investigate whether to share the features between unrelated words or not, we further conduct an experiment with the setting \u03bb = (0.9, 0.7, 0). The result confirms our assumption that a small number of shared features between unrelated words with similar word frequency achieve better model performance.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Effect on Sharing Coefficients",
                "sec_num": "3.3"
            },
            {
                "text": "Table 5 shows the performance of different word alignment thresholds. In the first row, we only pair the words whose alignment probability A(y|x) is above the threshold of 0.5 (see Equation 1 for more details). Under this circumstance, 4,869 words are categorized as parallel words that have similar lexical meaning. Based on these observations, we find that the alignment quality is not a key factor affecting the model performance. In contrast, pairing as many as similar words possible helps the model to better learn the bilingual vector space, which improves the translation performance. The following qualitative analyses support these observations either.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "5",
                        "ref_id": "TABREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Effect on Alignment Quality",
                "sec_num": "3.4"
            },
            {
                "text": "Table 6 shows two translation examples of the NIST Chinese-English translation task. To better understand the translations produced by these two models, we use layer-wise relevance propagation (LRP) (Ding et al., 2017) to produce the attention maps of the selected translations, as shown in Figure 4 and 5.",
                "cite_spans": [
                    {
                        "start": 199,
                        "end": 218,
                        "text": "(Ding et al., 2017)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "6",
                        "ref_id": "TABREF5"
                    },
                    {
                        "start": 298,
                        "end": 299,
                        "text": "4",
                        "ref_id": "FIGREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Analysis of the Translation Results",
                "sec_num": "3.5"
            },
            {
                "text": "In the first example, the Chinese word \"sangsheng\" is a low-frequency word and its ground truth is \"killed\". It is observed the inadequate representation of \"sangsheng\" leads to a decline in the translation quality of the vanilla, direct bridging, and decoder WT methods. In our proposed method, a part of the embedding of \"sangsheng\" is shared with that of \"killed\". These improved source representations help the model to generate better translations. Furthermore, as shown in Figure 4 , we observe that the proposed method has better long-distance reordering ability than the vanilla. We attribute this improvement to the shared features, which provide an alignment guidance for the attention mechanism.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 486,
                        "end": 487,
                        "text": "4",
                        "ref_id": "FIGREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Analysis of the Translation Results",
                "sec_num": "3.5"
            },
            {
                "text": "The second example implies that our proposed model is able to improve the adequacy of translation, as illustrated in Figure 5 . The Chinese word \"ye\" (also) appears twice in the source sentence, while only the proposed method can adequately translate both of them to the target word \"also\". This once again proves that the shared embeddings between the pair words,\"ye\" and \"also\" provide the attention model with a strong interaction between the words, leading to a more concentrated attention distribution and effectively alleviating the word omission problem.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 124,
                        "end": 125,
                        "text": "5",
                        "ref_id": "FIGREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Analysis of the Translation Results",
                "sec_num": "3.5"
            },
            {
                "text": "The proposed method has a limitation in that each word can only be paired with one corresponding word. However, synonym is a quite common phenomenon in natural language processing tasks. Qualitatively, we use principal component analysis (PCA) to visualize the learned embeddings of the vanilla model and the proposed method, as shown in Figure 6 . In the vanilla model, as shown in Figure 6 (a), only the similar monolingual embeddings are clustered, such as the English words \"died\" and \"killed\", and the Chinese words \"zhuxi\" (president) and \"zongtong\" (presi-dent). However, in the proposed method, no matter whether the similar source and target words are paired or not, they tend to cluster together; as shown in Figure 6 (b) and 6(c). In other words, the proposed method is able to handle the challenge of synonym. For example, both the Chinese words \"ye\" (paired with \"also\") and \"bing\" can be correctly translated to \"also\" and these three words tend to gather together in the vector space. This is similar to the Chinese word \"sangsheng\" (paired with \"killed\") and the English words \"died\" and \"killed\". Figure 6 (c) shows that the representations of the Chinese and English words which relate to \"president\" are very close.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 345,
                        "end": 346,
                        "text": "6",
                        "ref_id": "FIGREF4"
                    },
                    {
                        "start": 390,
                        "end": 391,
                        "text": "6",
                        "ref_id": "FIGREF4"
                    },
                    {
                        "start": 726,
                        "end": 727,
                        "text": "6",
                        "ref_id": "FIGREF4"
                    },
                    {
                        "start": 1121,
                        "end": 1122,
                        "text": "6",
                        "ref_id": "FIGREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Analysis of the Learned Embeddings",
                "sec_num": "3.6"
            },
            {
                "text": "Many previous works focus on improving the word representations of NMT by capturing the fine-grained (character) or coarse-grained (sub-word) monolingual characteristics, such as character-based NMT (Costa-Juss\u00e0 and Fonollosa, 2016; Ling et al., 2015; Cho et al., 2014; Chen et al., 2016) , sub-word NMT (Sennrich et al., 2016b; Johnson et al., 2017; Ataman and Federico, 2018) , and hybrid NMT (Luong and Manning, 2016) . They effectively consider and utilize the morphological information to enhance the word representations. Our work aims to enhance word representations through the bilingual features that are cooperatively learned by the source and target words.",
                "cite_spans": [
                    {
                        "start": 199,
                        "end": 232,
                        "text": "(Costa-Juss\u00e0 and Fonollosa, 2016;",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 233,
                        "end": 251,
                        "text": "Ling et al., 2015;",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 252,
                        "end": 269,
                        "text": "Cho et al., 2014;",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 270,
                        "end": 288,
                        "text": "Chen et al., 2016)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 304,
                        "end": 328,
                        "text": "(Sennrich et al., 2016b;",
                        "ref_id": null
                    },
                    {
                        "start": 329,
                        "end": 350,
                        "text": "Johnson et al., 2017;",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 351,
                        "end": 377,
                        "text": "Ataman and Federico, 2018)",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 395,
                        "end": 420,
                        "text": "(Luong and Manning, 2016)",
                        "ref_id": "BIBREF22"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "4"
            },
            {
                "text": "Recently, Gu et al. (2018) propose to use the pre-trained target (English) embeddings as a universal representation to improve the representation learning of the source (low-resource) languages.",
                "cite_spans": [
                    {
                        "start": 10,
                        "end": 26,
                        "text": "Gu et al. (2018)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "4"
            },
            {
                "text": "In our work, both the source and target embeddings can make use of the common representation unit, i.e. the source and target embedding help each other to learn a better representation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "4"
            },
            {
                "text": "The previously proposed methods have shown the effectiveness of integrating prior word alignments into the attention mechanism (Mi et al., 2016; Liu et al., 2016; Cheng et al., 2016; Feng et al., 2017) , leading to more accurate and adequate translation results with the assistance of prior guidance. We provide an alternative that integrates the prior alignments through the sharing of features, which can also leads to a reduction of model parameters. Kuang et al. (2018) propose to shorten the path length between the related source and target embeddings to enhance the embedding layer. We believe that the shared features can be seem as the zero distance between the paired word embeddings. Our proposed method also uses several ideas from the three-way WT method (Press and Wolf, 2017). Both of these methods are easy to implement and transparent to different NMT architectures. The main differences are: 1) we share a part of features instead of all features; 2) the words of different relationship categories are allowed to share with differently sized features; and (3) it is adaptable to any language pairs, making the WT methods more widely used.",
                "cite_spans": [
                    {
                        "start": 127,
                        "end": 144,
                        "text": "(Mi et al., 2016;",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 145,
                        "end": 162,
                        "text": "Liu et al., 2016;",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 163,
                        "end": 182,
                        "text": "Cheng et al., 2016;",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 183,
                        "end": 201,
                        "text": "Feng et al., 2017)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 454,
                        "end": 473,
                        "text": "Kuang et al. (2018)",
                        "ref_id": "BIBREF17"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "4"
            },
            {
                "text": "In this work, we propose a novel sharing technique to improve the learning of word embeddings for NMT. Each word embedding is composed of shared and private features. The shared features act as a prior alignment guidance for the attention model to improve the quality of attention. Meanwhile, the private features enable the words to better capture the monolingual characteristics, result in an improvement of the overall translation quality. According to the degree of relevance between a parallel word pair, the word pairs are categorized into three different groups and the number of shared features is different. Our experimental results show that the proposed method outperforms the strong Transformer baselines while using fewer model parameters.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "5"
            },
            {
                "text": "For the purpose of smoothing gradients, a very large batch size is needed during training.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Based on the dot-product attention mechanism, the attention weight between the source and target embeddings is the sum of the dot-product of their features.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "3 We investigate the effect of synonym in the experiment section.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://wit3.fbk.eu/mt.php?release= 2017-01-trnted",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://github.com/clab/fast_align",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "This work is supported in part by the National Natural Science Foundation of China (Nos. 61672555, 61876035, 61732005), the Joint Project of Macao Science and Technology Development Fund and National Natural Science Foundation of China (No. 045/2017/AFJ), the Multi-Year Research Grant from the University of Macau (No. MYRG2017-00087-FST). Yang Liu is supported by the National Key R&D Program of China (No. 2017YFB0202204), National Natural Science Foundation of China (No. 61761166008, No. 61432013), Beijing Advanced Innovation Center for Language Resources (No. TYR17002).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgements",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Compositional representation of morphologically-rich input for neural machine translation",
                "authors": [
                    {
                        "first": "Duygu",
                        "middle": [],
                        "last": "Ataman",
                        "suffix": ""
                    },
                    {
                        "first": "Marcello",
                        "middle": [],
                        "last": "Federico",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "ACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Duygu Ataman and Marcello Federico. 2018. Compo- sitional representation of morphologically-rich input for neural machine translation. In ACL 2018.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Neural machine translation by jointly learning to align and translate",
                "authors": [
                    {
                        "first": "Dzmitry",
                        "middle": [],
                        "last": "Bahdanau",
                        "suffix": ""
                    },
                    {
                        "first": "Kyunghyun",
                        "middle": [],
                        "last": "Cho",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2015. Neural machine translation by jointly learning to align and translate. In ICLR 2015.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "The best of both worlds: Combining recent advances in neural machine translation",
                "authors": [
                    {
                        "first": "Mia",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Chen",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Orhan",
                        "middle": [],
                        "last": "Firat",
                        "suffix": ""
                    },
                    {
                        "first": "Ankur",
                        "middle": [],
                        "last": "Bapna",
                        "suffix": ""
                    },
                    {
                        "first": "Melvin",
                        "middle": [],
                        "last": "Johnson",
                        "suffix": ""
                    },
                    {
                        "first": "Wolfgang",
                        "middle": [],
                        "last": "Macherey",
                        "suffix": ""
                    },
                    {
                        "first": "George",
                        "middle": [],
                        "last": "Foster",
                        "suffix": ""
                    },
                    {
                        "first": "Llion",
                        "middle": [],
                        "last": "Jones",
                        "suffix": ""
                    },
                    {
                        "first": "Niki",
                        "middle": [],
                        "last": "Parmar",
                        "suffix": ""
                    },
                    {
                        "first": "Mike",
                        "middle": [],
                        "last": "Schuster",
                        "suffix": ""
                    },
                    {
                        "first": "Zhifeng",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Yonghui",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Macduff",
                        "middle": [],
                        "last": "Hughes",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "ACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mia Xu Chen, Orhan Firat, Ankur Bapna, Melvin Johnson, Wolfgang Macherey, George Foster, Llion Jones, Niki Parmar, Mike Schuster, Zhifeng Chen, Yonghui Wu, and Macduff Hughes. 2018. The best of both worlds: Combining recent advances in neu- ral machine translation. In ACL 2018.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Strategies for training large vocabulary neural language models",
                "authors": [
                    {
                        "first": "Welin",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Grangier",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Auli",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Welin Chen, David Grangier, and Michael Auli. 2016. Strategies for training large vocabulary neural lan- guage models. In ACL 2016.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Agreement-based joint training for bidirectional attention-based neural machine translation",
                "authors": [
                    {
                        "first": "Yong",
                        "middle": [],
                        "last": "Cheng",
                        "suffix": ""
                    },
                    {
                        "first": "Shiqi",
                        "middle": [],
                        "last": "Shen",
                        "suffix": ""
                    },
                    {
                        "first": "Zhongjun",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Hua",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Maosong",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yong Cheng, Shiqi Shen, Zhongjun He, Wei He, Hua Wu, Maosong Sun, and Yang Liu. 2016. Agreement-based joint training for bidirectional attention-based neural machine translation. In IJ- CAI 2016.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation",
                "authors": [
                    {
                        "first": "Kyunghyun",
                        "middle": [],
                        "last": "Cho",
                        "suffix": ""
                    },
                    {
                        "first": "Bart",
                        "middle": [],
                        "last": "Van Merri\u00ebnboer",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [],
                        "last": "\u00b8aglar G\u00fclc \u00b8ehre",
                        "suffix": ""
                    },
                    {
                        "first": "Fethi",
                        "middle": [],
                        "last": "Bougares",
                        "suffix": ""
                    },
                    {
                        "first": "Holger",
                        "middle": [],
                        "last": "Schwenk",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "EMNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kyunghyun Cho, Bart Van Merri\u00ebnboer, C \u00b8aglar G\u00fclc \u00b8ehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representa- tions using RNN encoder-decoder for statistical ma- chine translation. In EMNLP 2014.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Character-based neural machine translation",
                "authors": [
                    {
                        "first": "Marta",
                        "middle": [
                            "R"
                        ],
                        "last": "Costa-Juss\u00e0",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [
                            "R"
                        ],
                        "last": "Jos\u00e9",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Fonollosa",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Marta R. Costa-Juss\u00e0 and Jos\u00e9 A. R. Fonollosa. 2016. Character-based neural machine translation. In ACL 2016.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Visualizing and understanding neural machine translation",
                "authors": [
                    {
                        "first": "Yanzhuo",
                        "middle": [],
                        "last": "Ding",
                        "suffix": ""
                    },
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Huanbo",
                        "middle": [],
                        "last": "Luan",
                        "suffix": ""
                    },
                    {
                        "first": "Maosong",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yanzhuo Ding, Yang Liu, Huanbo Luan, and Maosong Sun. 2017. Visualizing and understanding neural machine translation. In ACL 2017.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "A simple, fast, and effective reparameterization of ibm model 2",
                "authors": [
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Dyer",
                        "suffix": ""
                    },
                    {
                        "first": "Victor",
                        "middle": [],
                        "last": "Chahuneau",
                        "suffix": ""
                    },
                    {
                        "first": "Noah",
                        "middle": [
                            "A"
                        ],
                        "last": "Smith",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chris Dyer, Victor Chahuneau, and Noah A Smith. 2013. A simple, fast, and effective reparameteriza- tion of ibm model 2. In NAACL-HLT 2013.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Memory-augmented neural machine translation",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Feng",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Abel",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Y Feng, S Zhang, A Zhang, D Wang, and A Abel. 2017. Memory-augmented neural machine translation. In EMNLP 2017.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Convolutional sequence to sequence learning",
                "authors": [
                    {
                        "first": "Jonas",
                        "middle": [],
                        "last": "Gehring",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Auli",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Grangier",
                        "suffix": ""
                    },
                    {
                        "first": "Denis",
                        "middle": [],
                        "last": "Yarats",
                        "suffix": ""
                    },
                    {
                        "first": "Yann",
                        "middle": [
                            "N"
                        ],
                        "last": "Dauphin",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. 2017. Convolutional sequence to sequence learning. In ICML 2017.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Universal neural machine translation for extremely low resource languages",
                "authors": [
                    {
                        "first": "Jiatao",
                        "middle": [],
                        "last": "Gu",
                        "suffix": ""
                    },
                    {
                        "first": "Hany",
                        "middle": [],
                        "last": "Hassan",
                        "suffix": ""
                    },
                    {
                        "first": "Jacob",
                        "middle": [],
                        "last": "Devlin",
                        "suffix": ""
                    },
                    {
                        "first": "O K",
                        "middle": [],
                        "last": "Victor",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jiatao Gu, Hany Hassan, Jacob Devlin, and Victor O K Li. 2018. Universal neural machine translation for extremely low resource languages. In NAACL-HLT 2018.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Google's multilingual neural machine translation system: Enabling zero-shot translation",
                "authors": [
                    {
                        "first": "Melvin",
                        "middle": [],
                        "last": "Johnson",
                        "suffix": ""
                    },
                    {
                        "first": "Mike",
                        "middle": [],
                        "last": "Schuster",
                        "suffix": ""
                    },
                    {
                        "first": "V",
                        "middle": [],
                        "last": "Quoc",
                        "suffix": ""
                    },
                    {
                        "first": "Maxim",
                        "middle": [],
                        "last": "Le",
                        "suffix": ""
                    },
                    {
                        "first": "Yonghui",
                        "middle": [],
                        "last": "Krikun",
                        "suffix": ""
                    },
                    {
                        "first": "Zhifeng",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Nikhil",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Fernanda",
                        "middle": [],
                        "last": "Thorat",
                        "suffix": ""
                    },
                    {
                        "first": "Martin",
                        "middle": [],
                        "last": "Vi\u00e9gas",
                        "suffix": ""
                    },
                    {
                        "first": "Greg",
                        "middle": [],
                        "last": "Wattenberg",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Corrado",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Melvin Johnson, Mike Schuster, Quoc V Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Vi\u00e9gas, Martin Wattenberg, Greg Corrado, et al. 2017. Google's multilingual neural machine translation system: Enabling zero-shot translation. TACL.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Recurrent continuous translation models",
                "authors": [
                    {
                        "first": "Nal",
                        "middle": [],
                        "last": "Kalchbrenner",
                        "suffix": ""
                    },
                    {
                        "first": "Phil",
                        "middle": [],
                        "last": "Blunsom",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "EMNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent continuous translation models. In EMNLP 2013.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Adam: A method for stochastic optimization",
                "authors": [
                    {
                        "first": "Diederik",
                        "middle": [],
                        "last": "Kingma",
                        "suffix": ""
                    },
                    {
                        "first": "Jimmy",
                        "middle": [],
                        "last": "Ba",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "ICLR",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Diederik Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In ICLR 2015.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Statistical significance tests for machine translation evaluation",
                "authors": [
                    {
                        "first": "Philipp",
                        "middle": [],
                        "last": "Koehn",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "EMNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In EMNLP 2004.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Attention focusing for neural machine translation by bridging source and target embeddings",
                "authors": [
                    {
                        "first": "Shaohui",
                        "middle": [],
                        "last": "Kuang",
                        "suffix": ""
                    },
                    {
                        "first": "Junhui",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Ant\u00f3nio",
                        "middle": [],
                        "last": "Branco",
                        "suffix": ""
                    },
                    {
                        "first": "Weihua",
                        "middle": [],
                        "last": "Luo",
                        "suffix": ""
                    },
                    {
                        "first": "Deyi",
                        "middle": [],
                        "last": "Xiong",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "ACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Shaohui Kuang, Junhui Li, Ant\u00f3nio Branco, Weihua Luo, and Deyi Xiong. 2018. Attention focusing for neural machine translation by bridging source and target embeddings. In ACL 2018.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "2-component recurrent neural networks",
                "authors": [
                    {
                        "first": "Xiang",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Tao",
                        "middle": [],
                        "last": "Qin",
                        "suffix": ""
                    },
                    {
                        "first": "Jian",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Tie-Yan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xiang Li, Tao Qin, Jian Yang, and Tie-Yan Liu. 2016. 2-component recurrent neural networks. In NIPS 2016.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Slim embedding layers for recurrent neural language models",
                "authors": [
                    {
                        "first": "Zhongliang",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Raymond",
                        "middle": [],
                        "last": "Kulhanek",
                        "suffix": ""
                    },
                    {
                        "first": "Shaojun",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Yunxin",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Shuang",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zhongliang Li, Raymond Kulhanek, Shaojun Wang, Yunxin Zhao, and Shuang Wu. 2018. Slim embed- ding layers for recurrent neural language models. In AAAI 2018.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Character-based neural machine translation",
                "authors": [
                    {
                        "first": "Wang",
                        "middle": [],
                        "last": "Ling",
                        "suffix": ""
                    },
                    {
                        "first": "Isabel",
                        "middle": [],
                        "last": "Trancoso",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Dyer",
                        "suffix": ""
                    },
                    {
                        "first": "Alan",
                        "middle": [
                            "W"
                        ],
                        "last": "Black",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Wang Ling, Isabel Trancoso, Chris Dyer, and Alan W Black. 2015. Character-based neural machine trans- lation. arXiv.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Neural Machine Translation with Supervised Attention",
                "authors": [
                    {
                        "first": "Lemao",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Masao",
                        "middle": [],
                        "last": "Utiyama",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [
                            "M"
                        ],
                        "last": "Finch",
                        "suffix": ""
                    },
                    {
                        "first": "Eiichiro",
                        "middle": [],
                        "last": "Sumita",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Lemao Liu, Masao Utiyama, Andrew M Finch, and Ei- ichiro Sumita. 2016. Neural Machine Translation with Supervised Attention. In COLING 2016.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Achieving open vocabulary neural machine translation with hybrid word-character models",
                "authors": [
                    {
                        "first": "Minh-Thang",
                        "middle": [],
                        "last": "Luong",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Minh-Thang Luong and Christopher D Manning. 2016. Achieving open vocabulary neural machine trans- lation with hybrid word-character models. In ACL 2016.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Supervised attentions for neural machine translation",
                "authors": [
                    {
                        "first": "Haitao",
                        "middle": [],
                        "last": "Mi",
                        "suffix": ""
                    },
                    {
                        "first": "Zhiguo",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Abe",
                        "middle": [],
                        "last": "Ittycheriah",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Haitao Mi, Zhiguo Wang, and Abe Ittycheriah. 2016. Supervised attentions for neural machine translation. In EMNLP 2016.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Efficient estimation of word representations in vector space",
                "authors": [
                    {
                        "first": "Tomas",
                        "middle": [],
                        "last": "Mikolov",
                        "suffix": ""
                    },
                    {
                        "first": "Kai",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Greg",
                        "middle": [],
                        "last": "Corrado",
                        "suffix": ""
                    },
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Dean",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word represen- tations in vector space. In ICLR 2013.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Bleu: A method for automatic evaluation of machine translation",
                "authors": [
                    {
                        "first": "Kishore",
                        "middle": [],
                        "last": "Papineni",
                        "suffix": ""
                    },
                    {
                        "first": "Salim",
                        "middle": [],
                        "last": "Roukos",
                        "suffix": ""
                    },
                    {
                        "first": "Todd",
                        "middle": [],
                        "last": "Ward",
                        "suffix": ""
                    },
                    {
                        "first": "Wei-Jing",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "ACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: A method for automatic eval- uation of machine translation. In ACL 2002.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Using the Output Embedding to Improve Language Models",
                "authors": [],
                "year": 2017,
                "venue": "EACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ofir Press and Lior Wolf. 2017. Using the Output Em- bedding to Improve Language Models. In EACL 2017.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "The university of edinburgh's neural mt systems for wmt17",
                "authors": [
                    {
                        "first": "Rico",
                        "middle": [],
                        "last": "Sennrich",
                        "suffix": ""
                    },
                    {
                        "first": "Alexandra",
                        "middle": [],
                        "last": "Birch",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Currey",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Anna",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Germann",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Ulrich",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Haddow",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Barry",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Heafield",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Kenneth",
                        "suffix": ""
                    },
                    {
                        "first": "Antonio",
                        "middle": [],
                        "last": "Barone",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Valerio Miceli",
                        "suffix": ""
                    },
                    {
                        "first": "Philip",
                        "middle": [],
                        "last": "Williams",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "WMT@EMNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Rico Sennrich, Birch, Alexandra, Currey, Anna, Ger- mann, Ulrich, Haddow, Barry, Heafield, Kenneth, Barone, Antonio Valerio Miceli, and Williams, Philip. 2017. The university of edinburgh's neural mt systems for wmt17. In WMT@EMNLP 2017.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Edinburgh neural machine translation systems for WMT 16",
                "authors": [
                    {
                        "first": "Rico",
                        "middle": [],
                        "last": "Sennrich",
                        "suffix": ""
                    },
                    {
                        "first": "Barry",
                        "middle": [],
                        "last": "Haddow",
                        "suffix": ""
                    },
                    {
                        "first": "Alexandra",
                        "middle": [],
                        "last": "Birch",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016a. Edinburgh neural machine translation sys- tems for WMT 16. In ACL 2016.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Neural machine translation of rare words with subword units",
                "authors": [
                    {
                        "first": "Rico",
                        "middle": [],
                        "last": "Sennrich",
                        "suffix": ""
                    },
                    {
                        "first": "Barry",
                        "middle": [],
                        "last": "Haddow",
                        "suffix": ""
                    },
                    {
                        "first": "Alexandra",
                        "middle": [],
                        "last": "Birch",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016b. Neural machine translation of rare words with subword units. In ACL 2016.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "Sequence to sequence learning with neural networks",
                "authors": [
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    },
                    {
                        "first": "Oriol",
                        "middle": [],
                        "last": "Vinyals",
                        "suffix": ""
                    },
                    {
                        "first": "Quoc V",
                        "middle": [],
                        "last": "Le",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural net- works. In NIPS 2014.",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "Word representations: A simple and general method for semi-supervised learning",
                "authors": [
                    {
                        "first": "Lev-Arie",
                        "middle": [],
                        "last": "Joseph P Turian",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Ratinov",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "ACL 2010",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Joseph P Turian, Lev-Arie Ratinov, and Yoshua Ben- gio. 2010. Word representations: A simple and gen- eral method for semi-supervised learning. In ACL 2010.",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "Attention is all you need",
                "authors": [
                    {
                        "first": "Ashish",
                        "middle": [],
                        "last": "Vaswani",
                        "suffix": ""
                    },
                    {
                        "first": "Noam",
                        "middle": [],
                        "last": "Shazeer",
                        "suffix": ""
                    },
                    {
                        "first": "Niki",
                        "middle": [],
                        "last": "Parmar",
                        "suffix": ""
                    },
                    {
                        "first": "Jakob",
                        "middle": [],
                        "last": "Uszkoreit",
                        "suffix": ""
                    },
                    {
                        "first": "Llion",
                        "middle": [],
                        "last": "Jones",
                        "suffix": ""
                    },
                    {
                        "first": "Aidan",
                        "middle": [
                            "N"
                        ],
                        "last": "Gomez",
                        "suffix": ""
                    },
                    {
                        "first": "Lukasz",
                        "middle": [],
                        "last": "Kaiser",
                        "suffix": ""
                    },
                    {
                        "first": "Illia",
                        "middle": [],
                        "last": "Polosukhin",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In NIPS 2017.",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "Switchout: An efficient data augmentation algorithm for neural machine translation",
                "authors": [
                    {
                        "first": "Xinyi",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Hieu",
                        "middle": [],
                        "last": "Pham",
                        "suffix": ""
                    },
                    {
                        "first": "Zihang",
                        "middle": [],
                        "last": "Dai",
                        "suffix": ""
                    },
                    {
                        "first": "Graham",
                        "middle": [],
                        "last": "Neubig",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xinyi Wang, Hieu Pham, Zihang Dai, and Graham Neubig. 2018. Switchout: An efficient data aug- mentation algorithm for neural machine translation. In EMNLP 2018.",
                "links": null
            },
            "BIBREF34": {
                "ref_id": "b34",
                "title": "Thumt: An open source toolkit for neural machine translation",
                "authors": [
                    {
                        "first": "Jiacheng",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Yanzhuo",
                        "middle": [],
                        "last": "Ding",
                        "suffix": ""
                    },
                    {
                        "first": "Shiqi",
                        "middle": [],
                        "last": "Shen",
                        "suffix": ""
                    },
                    {
                        "first": "Yong",
                        "middle": [],
                        "last": "Cheng",
                        "suffix": ""
                    },
                    {
                        "first": "Maosong",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Huanbo",
                        "middle": [],
                        "last": "Luan",
                        "suffix": ""
                    },
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jiacheng Zhang, Yanzhuo Ding, Shiqi Shen, Yong Cheng, Maosong Sun, Huanbo Luan, and Yang Liu. 2017a. Thumt: An open source toolkit for neural machine translation. arXiv.",
                "links": null
            },
            "BIBREF35": {
                "ref_id": "b35",
                "title": "Towards compact and fast neural machine translation using a combined method",
                "authors": [
                    {
                        "first": "Xiaowei",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Feng",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Shuang",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Bo",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xiaowei Zhang, Wei Chen, Feng Wang, Shuang Xu, and Bo Xu. 2017b. Towards compact and fast neural machine translation using a combined method. In EMNLP 2017.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF1": {
                "num": null,
                "text": "Figure 3: The example of assembling the source word embedding matrix. The words in parentheses denote the paired words sharing features with them.",
                "uris": null,
                "fig_num": "3",
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "Figure 4: Long-distance reordering illustrated by the attention maps. The attention weights learned by the proposed shared-private model is more concentrated than that of the vanilla model.",
                "uris": null,
                "fig_num": "4",
                "type_str": "figure"
            },
            "FIGREF3": {
                "num": null,
                "text": "Figure5: Word omission problem illustrated by the attention maps. In the vanilla model, the third source word \"ye\" is not translated, while our shared-private model adequately translates it to give a better translation result.",
                "uris": null,
                "fig_num": "5",
                "type_str": "figure"
            },
            "FIGREF4": {
                "num": null,
                "text": "Figure 6: Visualization of the 2-dimensional PCA projection of the bilingual word embeddings of the two models. The blue words represent the Chinese embeddings while the red words represent the English embeddings. In (a), only the similar monolingual words are clustered together. While in (b) and (c), both the monolingual and bilingual words which have similar meanings are gathered together.",
                "uris": null,
                "fig_num": "6",
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table><tr><td>Architecture</td><td>Zh\u21d2En</td><td/><td>Params</td><td>Emb.</td><td>Red. Dev.</td><td>MT02</td><td>MT03</td><td>MT04</td><td>MT08</td><td>All</td></tr><tr><td>SMT*</td><td>-</td><td/><td>-</td><td>-</td><td>-34.00</td><td>35.81</td><td>34.70</td><td>37.15</td><td>25.28</td><td>33.39</td></tr><tr><td/><td>Vanilla</td><td/><td colspan=\"2\">74.8M 55.8M</td><td>0% 35.92</td><td>37.88</td><td>36.21</td><td>38.83</td><td>26.30</td><td>34.81</td></tr><tr><td>RNNsearch*</td><td colspan=\"2\">Source bridging Target bridging</td><td colspan=\"2\">78.5M 55.8M 76.6M 55.8M</td><td>0% 36.79 0% 36.69</td><td>38.71 39.04</td><td>37.24 37.63</td><td>40.28 40.41</td><td>27.40 27.98</td><td>35.91 36.27</td></tr><tr><td/><td colspan=\"2\">Direct bridging</td><td colspan=\"2\">78.9M 55.8M</td><td>0% 36.97</td><td>39.77</td><td>38.02</td><td>40.83</td><td>27.85</td><td>36.62</td></tr><tr><td/><td>Vanilla</td><td/><td colspan=\"2\">90.2M 46.1M</td><td>0% 41.37</td><td>42.53</td><td>40.25</td><td>43.58</td><td>32.89</td><td>40.33</td></tr><tr><td>Transformer</td><td colspan=\"2\">Direct bridging Decoder WT</td><td colspan=\"3\">90.5M 46.1M 74.9M 30.7M 33.4% 41.90 0% 41.67</td><td>42.89 43.02</td><td>41.34 41.89</td><td>43.56 43.87</td><td>32.69 32.62</td><td>40.54 40.82</td></tr><tr><td colspan=\"6\">Shared-private 62.8M 18.7M 59.En\u21d2De Params Emb. Red. BLEU</td><td/><td/><td/><td/></tr><tr><td>Vanilla</td><td>98.7M</td><td>54.5M</td><td>0%</td><td>27.62</td><td/><td/><td/><td/><td/></tr><tr><td>Direct bridging</td><td>98.9M</td><td>54.5M</td><td>0%</td><td>27.79</td><td/><td/><td/><td/><td/></tr><tr><td>Decoder WT</td><td>80.4M</td><td colspan=\"2\">36.2M 33.6%</td><td>27.51</td><td/><td/><td/><td/><td/></tr><tr><td>Three-way WT</td><td>63.1M</td><td colspan=\"2\">18.9M 65.3%</td><td>27.39</td><td/><td/><td/><td/><td/></tr><tr><td>Shared-private</td><td>65.0M</td><td colspan=\"2\">20.9M 63.1%</td><td>28.06  \u2021</td><td/><td/><td/><td/><td/></tr></table>",
                "type_str": "table",
                "text": "4% 42.57 \u2191 43.73 \u2191 41.99 \u2191 44.53 \u2191 33.81 \u21d1 41.61 \u21d1 Results on the NIST Chinese-English translation task. \"Params\" denotes the number of model parameters. \"Emb.\" represents the number of parameters used for word representation. \"Red.\" represents the reduction rate of the standard size. The results of SMT* and RNNsearch* are reported byKuang et al. (2018) with the same datasets and vocabulary settings. \"\u2191\" indicates the result is significantly better than that of the vanilla Transformer (p < 0.01), while \"\u21d1\" indicates the result is significantly better than that of all other Transformer models (p < 0.01). All significance tests are measured by paired bootstrap resampling(Koehn, 2004).",
                "html": null,
                "num": null
            },
            "TABREF1": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Results on the WMT English-German translation task. \" \u2021\" indicates the result is significantly better than the vanilla Transformer model (p < 0.05).",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table><tr><td/><td>Model</td><td>Emb.</td><td>Red. BLEU</td></tr><tr><td>Ar\u21d2 En</td><td colspan=\"2\">Vanilla Shared-private 11.8M 23.6M</td><td>0% 28.36 50% 29.71 \u2191</td></tr><tr><td>Ja\u21d2 En</td><td colspan=\"3\">Vanilla Shared-private 13.3M 48.0% 12.35 \u2191 25.6M 0% 10.94</td></tr><tr><td>Ko\u21d2 En</td><td colspan=\"3\">Vanilla Shared-private 13.2M 47.4% 17.84 \u2191 25.1M 0% 16.48</td></tr><tr><td>Zh\u21d2 En</td><td colspan=\"3\">Vanilla Shared-private 13.8M 49.6% 21.00 \u2191 27.4M 0% 19.36</td></tr></table>",
                "type_str": "table",
                "text": "",
                "html": null,
                "num": null
            },
            "TABREF4": {
                "content": "<table><tr><td colspan=\"4\">A(\u2022|\u2022) Lexical Form Unrelated Emb.</td><td>BLEU</td></tr><tr><td>0.5</td><td>4,869</td><td>309</td><td colspan=\"2\">24,822 22.0M 42.35</td></tr><tr><td>0.1</td><td>15,103</td><td>23</td><td colspan=\"2\">14,874 20.0M 42.53</td></tr><tr><td>0.05</td><td>21,172</td><td>11</td><td colspan=\"2\">8,817 18.7M 42.57</td></tr></table>",
                "type_str": "table",
                "text": "Effects on different alignment thresholds used for pairing the words with similar lexical meaning on the validation set of the NIST Chinese-English translation task.",
                "html": null,
                "num": null
            },
            "TABREF5": {
                "content": "<table><tr><td>mengmai xingzheng zhangguan &lt;unk&gt; biaoshi , dan shi gaishi jiu you shisan sangsheng . &lt;eos&gt;</td><td>mengmai xingzheng zhangguan &lt;unk&gt; biaoshi , dan shi gaishi jiu you shisan sangsheng . &lt;eos&gt;</td></tr><tr><td>bombay chief executive &lt;unk&gt; said that there were only 13 deaths in the city alone . &lt;eos&gt;</td><td>mumbai 's chief executive &lt;unk&gt; said 13 people were killed in the city alone . &lt;eos&gt;</td></tr><tr><td>(a) Vanilla</td><td>(b) Shared-private</td></tr></table>",
                "type_str": "table",
                "text": "Translation examples on MT08 test set. The first and second examples show the accuracy and adequacy of the proposed method, respectively. The bold words in each example are paired and will be discussed in the text.",
                "html": null,
                "num": null
            }
        }
    }
}