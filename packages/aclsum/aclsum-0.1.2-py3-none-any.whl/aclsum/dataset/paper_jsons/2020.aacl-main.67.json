{
    "paper_id": "2020",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:11:49.417824Z"
    },
    "title": "Neural RST-based Evaluation of Discourse Coherence",
    "authors": [
        {
            "first": "Grigorii",
            "middle": [],
            "last": "Guz",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of British",
                "location": {
                    "addrLine": "Columbia 1",
                    "settlement": "Inverted",
                    "region": "AI"
                }
            },
            "email": "g.guz@cs"
        },
        {
            "first": "Peyman",
            "middle": [],
            "last": "Bateni",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of British",
                "location": {
                    "addrLine": "Columbia 1",
                    "settlement": "Inverted",
                    "region": "AI"
                }
            },
            "email": "pbateni@cs"
        },
        {
            "first": "Darius",
            "middle": [],
            "last": "Muglich",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of British",
                "location": {
                    "addrLine": "Columbia 1",
                    "settlement": "Inverted",
                    "region": "AI"
                }
            },
            "email": "darius.muglich@alumni"
        },
        {
            "first": "Giuseppe",
            "middle": [],
            "last": "Carenini",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of British",
                "location": {
                    "addrLine": "Columbia 1",
                    "settlement": "Inverted",
                    "region": "AI"
                }
            },
            "email": "carenini@cs.ubc.ca"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "This paper evaluates the utility of Rhetorical Structure Theory (RST) trees and relations in discourse coherence evaluation. We show that incorporating silver-standard RST features can increase accuracy when classifying coherence. We demonstrate this through our tree-recursive neural model, namely RST-Recursive, which takes advantage of the text's RST features produced by a state of the art RST parser. We evaluate our approach on the Grammarly Corpus for Discourse Coherence (GCDC) and show that when ensembled with the current state of the art, we can achieve the new state of the art accuracy on this benchmark. Furthermore, when deployed alone, RST-Recursive achieves competitive accuracy while having 62% fewer parameters.",
    "pdf_parse": {
        "paper_id": "2020",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "This paper evaluates the utility of Rhetorical Structure Theory (RST) trees and relations in discourse coherence evaluation. We show that incorporating silver-standard RST features can increase accuracy when classifying coherence. We demonstrate this through our tree-recursive neural model, namely RST-Recursive, which takes advantage of the text's RST features produced by a state of the art RST parser. We evaluate our approach on the Grammarly Corpus for Discourse Coherence (GCDC) and show that when ensembled with the current state of the art, we can achieve the new state of the art accuracy on this benchmark. Furthermore, when deployed alone, RST-Recursive achieves competitive accuracy while having 62% fewer parameters.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Discourse coherence has been the subject of much research in Computational Linguistics thanks to its widespread applications (Lai and Tetreault, 2018) . Most current methods can be described as either stemming from explicit representations based on the Centering Theory (Grosz et al., 1994) , or deep learning approaches that learn without the use of hand-crafted linguistic features.",
                "cite_spans": [
                    {
                        "start": 125,
                        "end": 150,
                        "text": "(Lai and Tetreault, 2018)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 270,
                        "end": 290,
                        "text": "(Grosz et al., 1994)",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Our work explores a third research avenue based on the Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) . We hypothesize that texts of low/high coherence tend to adhere to different discourse structures. Thus, we pose that using even silver-standard RST features should help in separating coherent texts from incoherent ones. This stems from the definition of the coherence itselfas the writer of a document needs to follow specific rules for building a clear narrative or argument structure in which the role of each constituent of the document should be appropriate with respect * to its local and global context, and even existing discourse parsers should be able to predict a plausible structure that is consistent across all coherent documents. However, if a parser has difficulty interpreting a given document, it will be more likely to produce unrealistic trees with improbable patterns of discourse relations between constituents. This idea was first explored by Feng et al. (2014) , who followed an approach similar to Barzilay and Lapata (2008) by estimating entity transition likelihoods, but instead using discourse relations (predicted by a state of the art discourse parser (Feng and Hirst, 2014) ) that entities participate in as opposed to their grammatical roles. Their method achieved significant improvements in performance even when using silver-standard discourse trees, showing potential in the use of parsed RST features for classifying textual coherence.",
                "cite_spans": [
                    {
                        "start": 89,
                        "end": 114,
                        "text": "(Mann and Thompson, 1988)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 982,
                        "end": 1000,
                        "text": "Feng et al. (2014)",
                        "ref_id": null
                    },
                    {
                        "start": 1039,
                        "end": 1065,
                        "text": "Barzilay and Lapata (2008)",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 1199,
                        "end": 1221,
                        "text": "(Feng and Hirst, 2014)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Our work, however, is the first to develop and test a neural approach to leveraging RST discourse representations in coherence evaluation. Furthermore, Feng et al. (2014) only tested their proposal on the sentence permutation task, which involves ranking a sentence-permuted text against the original. As noted by Lai and Tetreault (2018) , this is not an accurate proxy for realistic coherence evaluation. We evaluate our method on their more realistic Grammarly Corpus Of Discourse Coherence (GCDC), where the model needs to classify a naturally produced text into one of three levels of coherence. Our contributions involve: (1) RST-Recursive, an RST-based neural tree-recursive method for coherence evaluation that achieves 2% below the state of the art performance on the GCDC while having 62% fewer parameters. (2) When ensembled with the current state of the art, namely Parseq (Lai and Tetreault, 2018) , we achieve a notable improvement over the plain ParSeq model. (3) We demonstrate the usefulness of silver-standard RST features in coherence classification, and establish our results as a lower-bound for performance improvements to be gained using RST features.",
                "cite_spans": [
                    {
                        "start": 139,
                        "end": 170,
                        "text": "Furthermore, Feng et al. (2014)",
                        "ref_id": null
                    },
                    {
                        "start": 314,
                        "end": 338,
                        "text": "Lai and Tetreault (2018)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 885,
                        "end": 910,
                        "text": "(Lai and Tetreault, 2018)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Authors contributed equally",
                "sec_num": null
            },
            {
                "text": "Centering Theory (Grosz et al., 1994) states that subsequent sentences in coherent texts are likely to continue to focus on the same entities (i.e., subjects, objects, etc.) as within the previous sentences. Building on top of this, Barzilay and Lapata (2008) were the first to propose the Entity-Grid model that constructs a two-dimensional array G n,m for a text of n sentences and m entities, which are used to estimate transition probabilities for entity occurrence patterns. More recently, Elsner and Charniak (2011) extended Entity-Grid using entity-specific features, while Tien Nguyen and Joty (2017) used a Convolutional Neural Network (CNN) on top of Entity-Grid to learn more hierarchical patterns.",
                "cite_spans": [
                    {
                        "start": 17,
                        "end": 37,
                        "text": "(Grosz et al., 1994)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 233,
                        "end": 259,
                        "text": "Barzilay and Lapata (2008)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Coherence Evaluation of Text",
                "sec_num": "2.1"
            },
            {
                "text": "On the other hand, feature-free deep neural techniques have dominated recent research. Li and Jurafsky (2017) ",
                "cite_spans": [
                    {
                        "start": 87,
                        "end": 109,
                        "text": "Li and Jurafsky (2017)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Coherence Evaluation of Text",
                "sec_num": "2.1"
            },
            {
                "text": "RST describes the structure of a text in the following way: first, the text is segmented into elementary discourse units (EDUs), which describe spans of text constituting clauses or clause-like units (Mann and Thompson, 1988) . Second, the EDUs are recursively structured into a tree hierarchy where each node defines an RST relation between the constituting sub-trees. The sub-tree with the central purpose is called the nucleus, and the one bearing secondary intent is called the satellite while a connective discourse relation is assigned to both. An example of a \"nucleus-satellite\" relation pairing is presented in Figure 1 where a claim is followed by the evidence for the claim; RST posits an \"Evidence\" relation between these two spans with the left sub-tree being the \"nucleus\" and the right sub-tree as \"satellite\".",
                "cite_spans": [
                    {
                        "start": 200,
                        "end": 225,
                        "text": "(Mann and Thompson, 1988)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 627,
                        "end": 628,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Rhetorical Structure Theory (RST)",
                "sec_num": "2.2"
            },
            {
                "text": "We parse silver-standard RST trees for documents using the CODRA (Joty et al., 2015) RST parser, which we then employ as input to our recursive neural model, RST-Recursive. The overall procedure for RST-Recursive is shown in Figure 1 . Given a document of n EDUs E 1:n with each EDU E i represented as a list of GloVe embeddings (Pennington et al., 2014) , we use an LSTM to process each E i , using the final hidden state as the EDU embedding e i = LSTM(E i ) for each leaf i of the document's RST tree. Afterwards, we apply a recursive LSTM architecture (Figure 2 ) that traverses the RST tree bottom-up. At each node s, we use the children's sub-tree embeddings [h l , c l , r l ] and [h r , c r , r r ] to form the node's sub-tree embedding:",
                "cite_spans": [
                    {
                        "start": 65,
                        "end": 84,
                        "text": "(Joty et al., 2015)",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 329,
                        "end": 354,
                        "text": "(Pennington et al., 2014)",
                        "ref_id": "BIBREF13"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 232,
                        "end": 233,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    },
                    {
                        "start": 564,
                        "end": 565,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "RST-Recursive",
                "sec_num": "3.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "[h s , c s ] = TreeLSTM([h l , c l , r l ], [h r , c r , r r ])",
                        "eq_num": "(1)"
                    }
                ],
                "section": "RST-Recursive",
                "sec_num": "3.1"
            },
            {
                "text": "where h l /c l and h r /c r are the LSTM hidden and cell states from the left and right sub-trees respectively. The relation embeddings of the children sub-trees, r l and r r , are learned vector embeddings for each of the 31 pre-defined relation labels in the form of \"[relation] [nucleus/satellite]\" (e.g., \"Evidence Satellite\" for the last EDU in Figure 1 ). At the root of the tree, the output hidden states from both children are concatenated into a single document embedding d = [h l , h r ]. As shown in Figure 3 , a fully connected layer is applied to this representation before using a Softmax function to obtain the coherence class probabilities.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 357,
                        "end": 358,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    },
                    {
                        "start": 518,
                        "end": 519,
                        "text": "3",
                        "ref_id": "FIGREF2"
                    }
                ],
                "eq_spans": [],
                "section": "RST-Recursive",
                "sec_num": "3.1"
            },
            {
                "text": "To evaluate if the addition of silver-standard RST features to existing methods can improve coherence evaluation, we ensemble RST-Recursive with the current state of the art coherence classifier: ParSeq. A deep learned non-linguistic classifier, ParSeq employs three layers of LSTMs that intend to capture coherence at different granularities. An overview of the ParSeq architecture is presented in Figure 4 . First, LSTM 1 (not shown) produces a single sentence embedding for each sentence in the text. Next, LSTM 2 generates paragraph embeddings using the corresponding sentence embeddings from LSTM 1 . Finally, LSTM 3 reads the paragraph embeddings, generating the final document embedding, which is passed to a fully connected layer to produce Softmax label probabilities.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 406,
                        "end": 407,
                        "text": "4",
                        "ref_id": "FIGREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Ensemble: ParSeq + RST-Recursive",
                "sec_num": "3.2"
            },
            {
                "text": "In this augmented variation of our model, we operate ParSeq on the document independently until a document level embedding d p is obtained at the highest-level LSTM. This document embedding is then concatenated to the RST-Recursive coherence embedding d = [h l , h r , d parseq ] in Figure 3 to produce class probabilities. Note that in this ensemble variation, we initialize tree leaves e 1:n with zero-vectors as opposed to EDU embeddings since ParSeq is sufficiently capable of capturing semantic information on its own, and early experiments using 5-fold cross-validation on the training set revealed model overfitting when training with EDU embeddings simultaneously.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Ensemble: ParSeq + RST-Recursive",
                "sec_num": "3.2"
            },
            {
                "text": "We evaluate RST-Recursive and Ensemble on the GCDC dataset (Lai and Tetreault, 2018) . This dataset consists of 4 separate sub-datasets: Clinton emails, Enron emails, Yahoo answers, and Yelp reviews, each containing 1000 documents for training and 200 documents for testing. Each document is assigned a discrete coherence label of incoherent (1), neutral (2), and coherent (3).",
                "cite_spans": [
                    {
                        "start": 59,
                        "end": 84,
                        "text": "(Lai and Tetreault, 2018)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dataset",
                "sec_num": "4.1"
            },
            {
                "text": "We parse RST trees for each example within the GCDC dataset using CODRA (Joty et al., 2015) . Due to CODRA's imperfect parsing of documents, RST trees could not be obtained for approximately 1.5%-2% of the documents, which were then excluded from the study. In addition, we re-evaluated ParSeq on only the RST-parsed portion of documents to assure consistent comparability of results. For more details, see Appendix A/B. Our code and dataset can be accessed below1 , and the access to the original GCDC corpus can be obtained here2 . We can share RST-parsings of GCDC examples with interested readers upon request once access to the GCDC dataset has also been obtained. ",
                "cite_spans": [
                    {
                        "start": 72,
                        "end": 91,
                        "text": "(Joty et al., 2015)",
                        "ref_id": "BIBREF5"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dataset",
                "sec_num": "4.1"
            },
            {
                "text": "We train all models with hyperparameter settings consistent with that of ParSeq reported by (Lai and Tetreault, 2018) . Specifically, we use a learning rate of 0.0001, hidden size of 100, relation embedding size of 50, and 300-dimensional pre-trained GloVe embeddings (Pennington et al., 2014) . We train with the Adam optimizer (Kingma and Ba, 2014) for 2 epochs. For every model/variation, the reported results represent the corresponding accuracies and F1 scores averaged over 1000 independent runs, each initialized with a different random seed.",
                "cite_spans": [
                    {
                        "start": 92,
                        "end": 117,
                        "text": "(Lai and Tetreault, 2018)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 268,
                        "end": 293,
                        "text": "(Pennington et al., 2014)",
                        "ref_id": "BIBREF13"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training",
                "sec_num": "4.2"
            },
            {
                "text": "Our full model incorporates the RST Tree (T) structure, nucleus/satellite properties (nuclearity) of subtrees (NS), RST specific connective relations (R), and EDU embeddings at leaves of the RST tree (E), as previously described in 3.1. Here, (T) defines the tree traversal operation and (NS) and (R) are learned vector embeddings for nuclearity and relations. We examine three ablations, each removing one of (NS), (R) and (E) from the model.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "RST-Recursive's Performance",
                "sec_num": "4.3"
            },
            {
                "text": "The results are provided in Tables 1 and 2 . As shown, the complete model is able to achieve a competitive overall accuracy and F1 at 53.04% and 44.30% respectively, which is close to the state of the art. Although this lags behind ParSeq by a noticeable 2% margin, RST-Recursive is able to achieve this performance with 62% fewer parameters (1,230k vs. 3,241k) , demonstrating the usefulness of linguistically-motivated features. Removing EDU embeddings reduces accuracy and F1 scores to 50.46% and 39.13%. This is still significantly better than the majority class baseline, signifying that even without any semantic infor- mation about the text and its contents, it is still possible to evaluate coherence using just the silverstandard RST features of the text. Removing RST relations and nuclearity, however, decreases performance substantially, dropping to the majority class level. This indicates that an RST tree structure alone (of the quality delivered by silver-standard parsers) is not sufficient to classify coherence. It must also be noted that since we employ silverstandard RST parsing as performed by CODRA (Joty et al., 2015) , the reported results act as a lower bound which we would expect to improve as parsing quality increases.",
                "cite_spans": [
                    {
                        "start": 342,
                        "end": 361,
                        "text": "(1,230k vs. 3,241k)",
                        "ref_id": null
                    },
                    {
                        "start": 1123,
                        "end": 1142,
                        "text": "(Joty et al., 2015)",
                        "ref_id": "BIBREF5"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 35,
                        "end": 36,
                        "text": "1",
                        "ref_id": "TABREF1"
                    },
                    {
                        "start": 41,
                        "end": 42,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "RST-Recursive's Performance",
                "sec_num": "4.3"
            },
            {
                "text": "We examine three variations of the Ensemble. The full model augments ParSeq with the text's RST tree, relations and nuclearity. This model is able to achieve the new state of the art performance, at 55.39% accuracy and 46.98% F1. Using final layer concatenation for ensembling is widely applicable to many other neural methods, and serves as a lower bound for the accuracy/F1 boost to be appreciated by incorporating RST features into the model. Removing the RST relations and/or nuclearity information completely eliminates the performance gain, which shows that the RST tree on its own is not sufficient as an RST source of information for distinguishing coherence, even when ensembled with ParSeq.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Ensemble's Performance",
                "sec_num": "4.4"
            },
            {
                "text": "As demonstrated in Figure 5 , coherence classifiers have difficulty predicting the neutral class (2), experiencing modal collapse towards the extreme ends in the best performing models. Early experiments using alternative objective functions such as the Ordinal Loss or Mean Squared Error resulted in a similar modal collapse or poor overall performance. We leave further exploration of this problem to future research. Furthermore, RST-Recursive shows a notably stronger recall on the coherent class (3) as compared to ParSeq. On the other hand, ParSeq has a higher recall/precision on class (1) and slightly higher precision on class (3). The Ensemble method, however, is able to take the best of both, achieving better recall, precision and F1 on both the incoherent and coherent classes as compared to ParSeq.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 26,
                        "end": 27,
                        "text": "5",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Classification Trends",
                "sec_num": "4.5"
            },
            {
                "text": "In this paper, we explore the usefulness of silverstandard parsed RST features in neural coherence classification. We propose two new methods, RST-Recursive and Ensemble. The former achieves reasonably good performance, only 2% short of state of the art, while more robust with 62% fewer parameters. The latter demonstrates the added advantage of RST features in improving classification accuracy of the existing state of the art methods by setting new state of the art performance with a modest but promising margin. This signifies that the document's rhetorical structure is an important aspect of its perceived clarity. Naturally, this improvement in performance is bounded by the quality of parsed RST features and could increase as better discourse parsers are developed.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusions and Future Work",
                "sec_num": "5"
            },
            {
                "text": "In the future, exploring other RST-based architectures for coherence classification, as well as better RST ensemble schemes and improving RST parsing can be avenues of potentially fruitful research. Additional research on multipronged approaches that draw from Centering Theory, RST and deep learning all together can also be of value.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusions and Future Work",
                "sec_num": "5"
            },
            {
                "text": "For model evaluation, we use the recently released Grammarly Corpus for Discourse Coherence (Lai and Tetreault, 2018) . GCDC consists of 4 sections -Clinton and Enron emails, as well as Yelp review and Yahoo answers, with 1000 training and 200 testing examples in each section. Each text is given a score from 1 (least coherent) to 3 (most coherent) by expert raters. GCDC's key advantage, compared to the ranking corpora used in the past (Prasad et al., 2008) , is that all the datapoints are human-labelled and not artificially permuted. Examples from the",
                "cite_spans": [
                    {
                        "start": 92,
                        "end": 117,
                        "text": "(Lai and Tetreault, 2018)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 439,
                        "end": 460,
                        "text": "(Prasad et al., 2008)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Appendices A Dataset Description",
                "sec_num": null
            },
            {
                "text": "Incoherent (1) For good Froyo, you just got to love some MoJo, yea baby yea! Creamy goodness with half the guilt of ice cream, a spread of tasty toppings, this in the TMP in definitely the place to be! They have little cups for sampling to find your favorite flavor. Great prices and with a yelping good 25% off discount just for \"checking in\" and half off Tuesdays with the FB word of the day, you just can't beat it! Perfect summer treat located in front of the TMP splash pad, you can soak up some sun and enjoy some fromazing yogurt in their outdoor sitting area! Go get you some Mojo froyo! Neutral (2) So Spintastic gets 5 stars because it's about as good as it gets for a laundromat, me thinks. Came here bc the dryer at my place was busted and waiting on the repairman. I found the people working the place extremely helpful. It was my first time there and she walked me through the steps of how to get a card, which machines to use, where I could buy the soap... only thing she didn't do was fold my dried laundry! Heh. Will remember this place for the future in the event that I need to get my clothes washed and ready. Free wi-fi and a soda machine is convenient. Oh and if you have a balance left on your card, you can redeem the card and any remaining balance if you like. dmo out Coherent (3) vet for almost 6 years. He is kind, compassionate and very loving and gentle with my dogs. All my dogs are shelter dogs and I am very picky about who cares for my animals. I walked in once with a dog I found running around the neighborhood and the staff could not find a chip so Dr. Besemer came out to help. He was busy but made time for me. He looked over the dog and could not find a chip, he also did a quick check on the dog and said that he appeared healthy. He didn't charge me for his time. This dog became my third adoped dog. Dr. Besemer is the best and I highly recommend him if you are looking for a vet. His staff is kind and compassionate. dataset are provided in Table 3 . When assigning the ranking to each text, the experts received the following instructions (Lai and Tetreault, 2018) : A text that is highly coherent (score 3) is easy to understand and easy to read. This usually means the text is well-organized, logically structured, and presents only information that supports the main idea. On the other hand, a text with low coherence (score 1) is difficult to understand. This may be because the text is not well organized, contains unrelated information that distracts from the main idea, or lacks transitions to connect the ideas in the text. Try to ignore the effects of grammar or spelling errors when assigning a coherence rating.",
                "cite_spans": [
                    {
                        "start": 2084,
                        "end": 2109,
                        "text": "(Lai and Tetreault, 2018)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 1991,
                        "end": 1992,
                        "text": "3",
                        "ref_id": "TABREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Coherence / Example",
                "sec_num": null
            },
            {
                "text": "We generated a discourse tree for each text in the GCDC dataset, utilizing the available CODRA discourse parser (Joty et al., 2015) . Early iterations resulted in up to 30% unsuccessful parsing rate on some sub-datasets. As a result, a punctuation fixing script was developed to fix minor punctuation problems without changing the text's structure or coherence. Post-fixing results lowered this RST parsing failure rate to reasonable margins in the 1% to 3% region (see Table 5 ). Note that all examples for which RST parsing was not successfully performed were excluded in our experiments. All baselines were re-evaluated using the RST-parsed set of examples.",
                "cite_spans": [
                    {
                        "start": 112,
                        "end": 131,
                        "text": "(Joty et al., 2015)",
                        "ref_id": "BIBREF5"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 476,
                        "end": 477,
                        "text": "5",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Coherence / Example",
                "sec_num": null
            },
            {
                "text": "While partial parsing of the dataset (see Appendix A) allows us to evaluate the accuracy of our models, it must be emphasized that as with the goal of this paper, we've used silver-standard RST parsing which lags well behind the human gold-standard. As shown in Table 4 , CODRA is far from reaching human-level accuracy in RST parsing. Additionally, since it was trained on RST-DT (Carlson et al., 2002) , it lacks out-of-domain adaptability, which becomes a bottle-neck in achieving substantial performance boost on badly structured domains of text such Yelp review. We again re-iterate the importance of RST parsing for RST-based coherence evaluation, and motivate future work in this area.",
                "cite_spans": [
                    {
                        "start": 381,
                        "end": 403,
                        "text": "(Carlson et al., 2002)",
                        "ref_id": "BIBREF1"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 268,
                        "end": 269,
                        "text": "4",
                        "ref_id": "TABREF4"
                    }
                ],
                "eq_spans": [],
                "section": "B CODRA Quality",
                "sec_num": null
            },
            {
                "text": "https://github.com/grig-guz/coherence-rst",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://github.com/aylai/GCDC-corpus",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "We believe that improvements in RST parsing will result in better accuracy for both future and existing RST-based coherence evaluation methods.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "annex",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Modeling local coherence: An entity-based approach",
                "authors": [
                    {
                        "first": "Regina",
                        "middle": [],
                        "last": "Barzilay",
                        "suffix": ""
                    },
                    {
                        "first": "Mirella",
                        "middle": [],
                        "last": "Lapata",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Computational Linguistics",
                "volume": "34",
                "issue": "1",
                "pages": "1--34",
                "other_ids": {
                    "DOI": [
                        "10.1162/coli.2008.34.1.1"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Regina Barzilay and Mirella Lapata. 2008. Modeling local coherence: An entity-based approach. Compu- tational Linguistics, 34(1):1-34.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Linguistic-Data Consortium, University of Pennsylvania. Micha Elsner and Eugene Charniak",
                "authors": [
                    {
                        "first": "Lynn",
                        "middle": [],
                        "last": "Carlson",
                        "suffix": ""
                    },
                    {
                        "first": "Mary",
                        "middle": [
                            "Ellen"
                        ],
                        "last": "Okurowski",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Marcy",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies",
                "volume": "",
                "issue": "",
                "pages": "125--129",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Lynn Carlson, Mary Ellen Okurowski, and Daniel Marcy. 2002. Rst discourse treebank. Linguistic- Data Consortium, University of Pennsylvania. Micha Elsner and Eugene Charniak. 2011. Extending the entity grid with entity-specific features. In Pro- ceedings of the 49th Annual Meeting of the Associ- ation for Computational Linguistics: Human Lan- guage Technologies, pages 125-129, Portland, Ore- gon, USA. Association for Computational Linguis- tics.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "A lineartime bottom-up discourse parser with constraints and post-editing",
                "authors": [
                    {
                        "first": "Vanessa",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Feng",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Graeme",
                        "middle": [],
                        "last": "Hirst",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics",
                "volume": "1",
                "issue": "",
                "pages": "511--521",
                "other_ids": {
                    "DOI": [
                        "10.3115/v1/P14-1048"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Vanessa Wei Feng and Graeme Hirst. 2014. A linear- time bottom-up discourse parser with constraints and post-editing. In Proceedings of the 52nd An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 511- 521, Baltimore, Maryland. Association for Compu- tational Linguistics.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "The impact of deep hierarchical discourse structures in the evaluation of text coherence",
                "authors": [
                    {
                        "first": "Vanessa",
                        "middle": [],
                        "last": "Wei Feng",
                        "suffix": ""
                    },
                    {
                        "first": "Ziheng",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Graeme",
                        "middle": [],
                        "last": "Hirst",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers",
                "volume": "",
                "issue": "",
                "pages": "940--949",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Vanessa Wei Feng, Ziheng Lin, and Graeme Hirst. 2014. The impact of deep hierarchical discourse structures in the evaluation of text coherence. In Pro- ceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Techni- cal Papers, pages 940-949, Dublin, Ireland. Dublin City University and Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Centering: A framework for modelling the coherence of discourse",
                "authors": [
                    {
                        "first": "Barbara",
                        "middle": [],
                        "last": "Grosz",
                        "suffix": ""
                    },
                    {
                        "first": "Aravind",
                        "middle": [],
                        "last": "Joshi",
                        "suffix": ""
                    },
                    {
                        "first": "Scott",
                        "middle": [],
                        "last": "Weinstein",
                        "suffix": ""
                    }
                ],
                "year": 1994,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Barbara Grosz, Aravind Joshi, and Scott Weinstein. 1994. Centering: A framework for modelling the coherence of discourse. Technical Reports (CIS).",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Codra: A novel discriminative framework for rhetorical analysis",
                "authors": [
                    {
                        "first": "Shafiq",
                        "middle": [],
                        "last": "Joty",
                        "suffix": ""
                    },
                    {
                        "first": "Giuseppe",
                        "middle": [],
                        "last": "Carenini",
                        "suffix": ""
                    },
                    {
                        "first": "Raymond",
                        "middle": [],
                        "last": "Ng",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Computational Linguistics",
                "volume": "41",
                "issue": "",
                "pages": "1--51",
                "other_ids": {
                    "DOI": [
                        "10.1162/COLI_a_00226"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Shafiq Joty, Giuseppe Carenini, and Raymond Ng. 2015. Codra: A novel discriminative framework for rhetorical analysis. Computational Linguistics, 41:1-51.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Adam: A method for stochastic optimization",
                "authors": [
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Diederik",
                        "suffix": ""
                    },
                    {
                        "first": "Jimmy",
                        "middle": [],
                        "last": "Kingma",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Ba",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "3rd International Conference for Learning Representations",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Diederik P. Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. Cite arxiv:1412.6980Comment: Published as a confer- ence paper at the 3rd International Conference for Learning Representations, San Diego, 2015.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Discourse coherence in the wild: A dataset, evaluation and methods",
                "authors": [
                    {
                        "first": "Alice",
                        "middle": [],
                        "last": "Lai",
                        "suffix": ""
                    },
                    {
                        "first": "Joel",
                        "middle": [
                            "R"
                        ],
                        "last": "Tetreault",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alice Lai and Joel R. Tetreault. 2018. Discourse coher- ence in the wild: A dataset, evaluation and methods. CoRR, abs/1805.04993.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Neural net models of open-domain discourse coherence",
                "authors": [
                    {
                        "first": "Jiwei",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Jurafsky",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "198--209",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/D17-1019"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jiwei Li and Dan Jurafsky. 2017. Neural net models of open-domain discourse coherence. In Proceed- ings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 198-209, Copenhagen, Denmark. Association for Computa- tional Linguistics.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Rethorical structure theory: Toward a functional theory of text organization",
                "authors": [
                    {
                        "first": "William",
                        "middle": [],
                        "last": "Mann",
                        "suffix": ""
                    },
                    {
                        "first": "Sandra",
                        "middle": [],
                        "last": "Thompson",
                        "suffix": ""
                    }
                ],
                "year": 1988,
                "venue": "Text",
                "volume": "8",
                "issue": "",
                "pages": "243--281",
                "other_ids": {
                    "DOI": [
                        "10.1515/text.1.1988.8.3.243"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "William Mann and Sandra Thompson. 1988. Rethori- cal structure theory: Toward a functional theory of text organization. Text, 8:243-281.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "A neural local coherence model for text quality assessment",
                "authors": [
                    {
                        "first": "Mohsen",
                        "middle": [],
                        "last": "Mesgar",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Strube",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "4328--4339",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/D18-1464"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Mohsen Mesgar and Michael Strube. 2018. A neu- ral local coherence model for text quality assess- ment. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4328-4339, Brussels, Belgium. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "A unified neural coherence model",
                "authors": [
                    {
                        "first": "Tasnim",
                        "middle": [],
                        "last": "Han Cheol Moon",
                        "suffix": ""
                    },
                    {
                        "first": "Shafiq",
                        "middle": [],
                        "last": "Mohiuddin",
                        "suffix": ""
                    },
                    {
                        "first": "Chi",
                        "middle": [],
                        "last": "Joty",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "volume": "",
                "issue": "",
                "pages": "2262--2272",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/D19-1231"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Han Cheol Moon, Tasnim Mohiuddin, Shafiq Joty, and Chi Xu. 2019. A unified neural coherence model. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan- guage Processing (EMNLP-IJCNLP), pages 2262- 2272, Hong Kong, China. Association for Computa- tional Linguistics.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "How much progress have we made on rst discourse parsing? a replication study of recent results on the rst-dt",
                "authors": [
                    {
                        "first": "Mathieu",
                        "middle": [],
                        "last": "Morey",
                        "suffix": ""
                    },
                    {
                        "first": "Philippe",
                        "middle": [],
                        "last": "Muller",
                        "suffix": ""
                    },
                    {
                        "first": "Nicholas",
                        "middle": [],
                        "last": "Asher",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "EMNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mathieu Morey, Philippe Muller, and Nicholas Asher. 2017. How much progress have we made on rst dis- course parsing? a replication study of recent results on the rst-dt. In EMNLP.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Glove: Global vectors for word representation",
                "authors": [
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Pennington",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "volume": "",
                "issue": "",
                "pages": "1532--1543",
                "other_ids": {
                    "DOI": [
                        "10.3115/v1/D14-1162"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word rep- resentation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Process- ing (EMNLP), pages 1532-1543, Doha, Qatar. Asso- ciation for Computational Linguistics.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Improved semantic representations from tree-structured long short-term memory networks",
                "authors": [
                    {
                        "first": "Kai",
                        "middle": [],
                        "last": "Sheng",
                        "suffix": ""
                    },
                    {
                        "first": "Tai",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kai Sheng Tai, Richard Socher, and Christopher D. Manning. 2015. Improved semantic representations from tree-structured long short-term memory net- works. CoRR, abs/1503.00075.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "A neural local coherence model",
                "authors": [
                    {
                        "first": "Tien",
                        "middle": [],
                        "last": "Dat",
                        "suffix": ""
                    },
                    {
                        "first": "Shafiq",
                        "middle": [],
                        "last": "Nguyen",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Joty",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
                "volume": "1",
                "issue": "",
                "pages": "1320--1330",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/P17-1121"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Dat Tien Nguyen and Shafiq Joty. 2017. A neural local coherence model. In Proceedings of the 55th Annual Meeting of the Association for Computational Lin- guistics (Volume 1: Long Papers), pages 1320-1330, Vancouver, Canada. Association for Computational Linguistics.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure1: Overview of RST-Recursive; EDU embeddings are generated for the leaf nodes using the EDU network. Subsequently, the RST tree is recursively traversed bottom-up using the RST network.",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Figure 2: Recursive LSTM architecture used in RST-Recursive adapted from (Tai et al., 2015).",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "Figure 3: Overview of the classification layer in RST-Recursive; At the root of the RST tree, children's hidden states are concatenated to form the document representation d = [h l , h r ] which is then transformed into a 3-dimensional vector of Softmax probabilities. next sentence given the current sentence and viceversa. Mesgar and Strube (2018) constructed a local coherence model that encodes patterns of changes on how adjacent sentences within the text are semantically related. Recently, Moon et al. (2019) used a multi-component model to capture both local and global coherence perturbations. Lai and Tetreault (2018) developed a hierarchical neural architecture named ParSeq with three stacked LSTM Networks, designed to encode the coherence at sentence, paragraph and document levels.",
                "uris": null,
                "fig_num": "3",
                "type_str": "figure"
            },
            "FIGREF3": {
                "num": null,
                "text": "Figure 4: The architectural overview of ParSeq; an illustration of ParSeq's structure, taken directly from the original paper (Lai and Tetreault, 2018).",
                "uris": null,
                "fig_num": "4",
                "type_str": "figure"
            },
            "TABREF1": {
                "content": "<table><tr><td>MODEL</td><td>T NS R E CLINTON</td><td>ENRON</td><td>YAHOO</td><td>YELP</td><td>AVERAGE</td></tr><tr><td>MAJORITY</td><td>55.33</td><td>44.39</td><td>38.02</td><td>54.82</td><td>48.14</td></tr><tr><td>RST-REC</td><td colspan=\"5\">55.33\u00b10.00 44.39\u00b10.00 38.02\u00b10.00 54.82\u00b10.00 48.14\u00b10.00</td></tr><tr><td>RST-REC</td><td colspan=\"5\">53.74\u00b10.14 44.67\u00b10.07 44.61\u00b10.09 53.76\u00b10.11 49.20\u00b10.07</td></tr><tr><td>RST-REC</td><td colspan=\"5\">54.07\u00b10.10 43.99\u00b10.07 49.39\u00b10.10 54.39\u00b10.12 50.46\u00b10.05</td></tr><tr><td>RST-REC</td><td colspan=\"5\">55.70\u00b10.08 53.86\u00b10.11 50.92\u00b10.13 51.70\u00b10.16 53.04\u00b10.09</td></tr><tr><td>PARSEQ</td><td colspan=\"5\">61.05\u00b10.13 54.23\u00b10.10 53.29\u00b10.14 51.76\u00b10.21 55.09\u00b10.09</td></tr><tr><td>ENSEMBLE</td><td colspan=\"5\">* 61.12\u00b10.13 54.20\u00b10.12 52.87\u00b10.16 51.52\u00b10.22 54.93\u00b10.10</td></tr><tr><td>ENSEMBLE</td><td colspan=\"5\">* 60.82\u00b10.13 54.01\u00b10.10 52.92\u00b10.15 51.63\u00b10.24 54.85\u00b10.10</td></tr><tr><td>ENSEMBLE</td><td colspan=\"5\">* 61.17\u00b10.12 53.99\u00b10.10 53.99\u00b10.14 52.40\u00b10.21 55.39\u00b10.09</td></tr><tr><td>MODEL</td><td>T NS R E CLINTON</td><td>ENRON</td><td>YAHOO</td><td>YELP</td><td>AVERAGE</td></tr><tr><td>MAJORITY</td><td>39.42</td><td>27.29</td><td>20.95</td><td>38.82</td><td>31.62</td></tr><tr><td>RST-REC</td><td colspan=\"5\">39.42\u00b10.00 27.29\u00b10.00 20.95\u00b10.00 38.82\u00b10.00 31.62\u00b10.00</td></tr><tr><td>RST-REC</td><td colspan=\"5\">39.20\u00b10.03 30.81\u00b10.16 35.67\u00b10.18 39.93\u00b10.08 36.40\u00b10.09</td></tr><tr><td>RST-REC</td><td colspan=\"5\">41.08\u00b10.07 31.21\u00b10.13 41.97\u00b10.14 42.27\u00b10.09 39.13\u00b10.08</td></tr><tr><td>RST-REC</td><td colspan=\"5\">45.90\u00b10.12 44.33\u00b10.16 43.85\u00b10.18 43.13\u00b10.10 44.30\u00b10.08</td></tr><tr><td>PARSEQ</td><td colspan=\"5\">52.12\u00b10.21 44.90\u00b10.15 46.22\u00b10.18 43.36\u00b10.09 46.65\u00b10.10</td></tr><tr><td>ENSEMBLE</td><td colspan=\"5\">* 52.35\u00b10.22 44.92\u00b10.16 45.48\u00b10.22 43.70\u00b10.11 46.61\u00b10.11</td></tr><tr><td>ENSEMBLE</td><td colspan=\"5\">* 51.90\u00b10.22 44.76\u00b10.14 45.48\u00b10.22 43.83\u00b10.13 46.49\u00b10.10</td></tr><tr><td>ENSEMBLE</td><td colspan=\"5\">* 52.42\u00b10.19 44.69\u00b10.15 46.88\u00b10.17 43.94\u00b10.09 46.98\u00b10.09</td></tr></table>",
                "type_str": "table",
                "text": "Overall and sub-dataset specific coherence classification accuracy on the GCDC dataset. Error boundaries describe 95% confidence intervals. Values in bold describe statistically significant state of the art performance. * indicates availability of EDU-level semantic information through the ensembling with ParSeq.",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table><tr><td>Parser</td><td colspan=\"4\">Structure Nuclearity Relation Full</td></tr><tr><td>CODRA</td><td>82.6</td><td>68.3</td><td>55.8</td><td>55.4</td></tr><tr><td>Human</td><td>88.3</td><td>77.3</td><td>65.4</td><td>64.7</td></tr></table>",
                "type_str": "table",
                "text": "Text examples of incoherent (class 1), neutral (class 2), and coherent (class 3) snippets from the Yelp subset of the GCDC dataset(Lai and Tetreault, 2018).",
                "html": null,
                "num": null
            },
            "TABREF4": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Micro-averaged F1 scores on the RST parsing of text by CODRA vs. Human Standard(Morey et al., 2017).",
                "html": null,
                "num": null
            }
        }
    }
}