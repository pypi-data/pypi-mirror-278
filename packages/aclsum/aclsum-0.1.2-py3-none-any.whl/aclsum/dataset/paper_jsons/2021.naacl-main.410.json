{
    "paper_id": "2021",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T11:54:49.588501Z"
    },
    "title": "Learning How to Ask: Querying LMs with Mixtures of Soft Prompts",
    "authors": [
        {
            "first": "Guanghui",
            "middle": [],
            "last": "Qin",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Johns Hopkins University",
                "location": {}
            },
            "email": ""
        },
        {
            "first": "Jason",
            "middle": [],
            "last": "Eisner",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Johns Hopkins University",
                "location": {}
            },
            "email": ""
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Natural-language prompts have recently been used to coax pretrained language models into performing other AI tasks, using a fill-in-theblank paradigm (Petroni et al., 2019) or a few-shot extrapolation paradigm (Brown et al., 2020). For example, language models retain factual knowledge from their training corpora that can be extracted by asking them to \"fill in the blank\" in a sentential prompt. However, where does this prompt come from? We explore the idea of learning prompts by gradient descent-either fine-tuning prompts taken from previous work, or starting from random initialization. Our prompts consist of \"soft words,\" i.e., continuous vectors that are not necessarily word type embeddings from the language model. Furthermore, for each task, we optimize a mixture of prompts, learning which prompts are most effective and how to ensemble them. Across multiple English LMs and tasks, our approach hugely outperforms previous methods, showing that the implicit factual knowledge in language models was previously underestimated. Moreover, this knowledge is cheap to elicit: random initialization is nearly as good as informed initialization.\nx performed until his death in y .",
    "pdf_parse": {
        "paper_id": "2021",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Natural-language prompts have recently been used to coax pretrained language models into performing other AI tasks, using a fill-in-theblank paradigm (Petroni et al., 2019) or a few-shot extrapolation paradigm (Brown et al., 2020). For example, language models retain factual knowledge from their training corpora that can be extracted by asking them to \"fill in the blank\" in a sentential prompt. However, where does this prompt come from? We explore the idea of learning prompts by gradient descent-either fine-tuning prompts taken from previous work, or starting from random initialization. Our prompts consist of \"soft words,\" i.e., continuous vectors that are not necessarily word type embeddings from the language model. Furthermore, for each task, we optimize a mixture of prompts, learning which prompts are most effective and how to ensemble them. Across multiple English LMs and tasks, our approach hugely outperforms previous methods, showing that the implicit factual knowledge in language models was previously underestimated. Moreover, this knowledge is cheap to elicit: random initialization is nearly as good as informed initialization.",
                "cite_spans": [
                    {
                        "start": 150,
                        "end": 172,
                        "text": "(Petroni et al., 2019)",
                        "ref_id": "BIBREF20"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            },
            {
                "text": "x performed until his death in y .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Pretrained language models, such as ELMo (Peters et al., 2018) , BERT (Devlin et al., 2019) , and BART (Lewis et al., 2020a) , have proved to provide useful representations for other NLP tasks. Recently, Petroni et al. (2019) and Jiang et al. (2020) demonstrated that language models (LMs) also contain factual and commonsense knowledge that can be elicited with a prompt. For example, to query the date-of- ,\" where we have filled the first blank with \"Mozart,\" and ask a cloze language model to fill in the second blank. The prompts used by Petroni et al. (2019) are manually created, while Jiang et al. (2020) use mining and paraphrasing based methods to automatically augment the prompt sets.",
                "cite_spans": [
                    {
                        "start": 41,
                        "end": 62,
                        "text": "(Peters et al., 2018)",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 70,
                        "end": 91,
                        "text": "(Devlin et al., 2019)",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 103,
                        "end": 124,
                        "text": "(Lewis et al., 2020a)",
                        "ref_id": null
                    },
                    {
                        "start": 204,
                        "end": 225,
                        "text": "Petroni et al. (2019)",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 230,
                        "end": 249,
                        "text": "Jiang et al. (2020)",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 543,
                        "end": 564,
                        "text": "Petroni et al. (2019)",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 593,
                        "end": 612,
                        "text": "Jiang et al. (2020)",
                        "ref_id": "BIBREF10"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Finding out what young children know is difficult because they can be very sensitive to the form of the question (Donaldson, 1978) . Opinion polling is also sensitive to question design (Broughton, 1995) . We observe that when we are querying an LM rather than a human, we have the opportunity to tune prompts using gradient descent-the workhorse of modern NLP-so that they better elicit the desired type of knowledge.",
                "cite_spans": [
                    {
                        "start": 113,
                        "end": 130,
                        "text": "(Donaldson, 1978)",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 186,
                        "end": 203,
                        "text": "(Broughton, 1995)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "A neural LM sees the prompt as a sequence of continuous word vectors (Baroni et al., 2014) . We tune in this continuous space, relaxing the constraint that the vectors be the embeddings of actual English words. Allowing \"soft prompts\" consisting of \"soft words\" is not only convenient for optimization, but is also more expressive. Soft prompts can emphasize particular words (by lengthening their vectors) or particular dimensions of those words. They can also adjust words that are misleading, ambiguous, or overly specific. Consider the following prompt for the relation date-of-death:",
                "cite_spans": [
                    {
                        "start": 69,
                        "end": 90,
                        "text": "(Baroni et al., 2014)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "This prompt may work for the male singer Cab Calloway, but if we want it to also work for the female painter Mary Cassatt, it might help to soften \"performed\" and \"his\" so that they do not insist on the wrong occupation and gender, and perhaps to soften \"until\" into a weaker connective (as Cassatt was in fact too blind to paint in her final years).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Another way to bridge between these cases is to have one prompt using \"performed\" and another using \"painted.\" In general, there may be many varied lexical patterns that signal a particular relation, and having more patterns will get better coverage (Hearst, 1992; Riloff and Jones, 1999) . We therefore propose to learn a mixture of soft prompts.",
                "cite_spans": [
                    {
                        "start": 250,
                        "end": 264,
                        "text": "(Hearst, 1992;",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 265,
                        "end": 288,
                        "text": "Riloff and Jones, 1999)",
                        "ref_id": "BIBREF23"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "We test the idea on several cloze language models, training prompts to complete factual and com-mon sense relations from 3 datasets. Comparing on held-out examples, our method dramatically outperforms previous work, even when initialized randomly. So when regarded as approximate knowledge bases, language models know more than we realized. We just had to find the right ways to ask.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Factual knowledge is traditionally extracted from large corpora using a pipeline of NLP tools (Surdeanu and Ji, 2014) , including entity extraction (Lample et al., 2016) , entity linking (Rao et al., 2013) and relation extraction (Sorokin and Gurevych, 2017) .",
                "cite_spans": [
                    {
                        "start": 94,
                        "end": 117,
                        "text": "(Surdeanu and Ji, 2014)",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 148,
                        "end": 169,
                        "text": "(Lample et al., 2016)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 187,
                        "end": 205,
                        "text": "(Rao et al., 2013)",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 230,
                        "end": 258,
                        "text": "(Sorokin and Gurevych, 2017)",
                        "ref_id": "BIBREF30"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "However, recent work has shown that simply training a system to complete sentences-language modeling-causes it to implicitly acquire nonlinguistic abilities from its training corpora (Rogers et al., 2020) , including factual knowledge (Petroni et al., 2019; Jiang et al., 2020) , common sense (Bisk et al., 2019) , reasoning (Talmor et al., 2020; Brown et al., 2020) , summarization (Radford et al., 2019) , and even arithmetic (Bouraoui et al., 2020) .",
                "cite_spans": [
                    {
                        "start": 183,
                        "end": 204,
                        "text": "(Rogers et al., 2020)",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 235,
                        "end": 257,
                        "text": "(Petroni et al., 2019;",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 258,
                        "end": 277,
                        "text": "Jiang et al., 2020)",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 293,
                        "end": 312,
                        "text": "(Bisk et al., 2019)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 325,
                        "end": 346,
                        "text": "(Talmor et al., 2020;",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 347,
                        "end": 366,
                        "text": "Brown et al., 2020)",
                        "ref_id": null
                    },
                    {
                        "start": 383,
                        "end": 405,
                        "text": "(Radford et al., 2019)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 428,
                        "end": 451,
                        "text": "(Bouraoui et al., 2020)",
                        "ref_id": "BIBREF2"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "Most of the previous work manually creates prompts to extract answers from the trained language model. We use LAMA (Petroni et al., 2019) as a baseline. Building on LAMA, the LM Prompt And Query Archive (LPAQA) method (Jiang et al., 2020) searches for new prompts by either mining a corpus or paraphrasing existing prompts. AutoPrompt (Shin et al., 2020) searches for improved prompts using a gradient signal, although its prompts are limited to sequences of actual (\"hard\") English words, unlike our method. We compare our novel soft prompts against all of these systems.",
                "cite_spans": [
                    {
                        "start": 115,
                        "end": 137,
                        "text": "(Petroni et al., 2019)",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 218,
                        "end": 238,
                        "text": "(Jiang et al., 2020)",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 335,
                        "end": 354,
                        "text": "(Shin et al., 2020)",
                        "ref_id": "BIBREF28"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "After we submitted the present paper in November 2020, two still unpublished manuscripts appeared on arXiv that also investigated soft prompts. Li and Liang (2021) considered the setting of generating text from a pretrained language model (GPT-2 or BART) conditioned on a textual prompt. To improve the results, they prepended a few taskspecific \"soft tokens\" to the prompt and tuned the embeddings of only these tokens (at all embedding layers). Liu et al. (2021) adopted a strategy similar to ours by tuning fill-in-the-blank prompts in a continuous space, testing on GPT-2 and BERT models, although they did not use the enhancements we proposed in \u00a7 \u00a73.2-3.4 below. Like our work, both these papers achieved strong gains.",
                "cite_spans": [
                    {
                        "start": 447,
                        "end": 464,
                        "text": "Liu et al. (2021)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "In other work, Bouraoui et al. (2020) mine prompts from a corpus, then fine-tune the whole language model so that it more accurately completes the prompts. Schick and Sch\u00fctze (2020a,b) are similar but fine-tune the language model differently for each prompt. Our method complements these by tuning the prompts themselves. \"Probing\" systems that ask what language models know about particular sentences (e.g., Eichler et al., 2019) usually use feedforward networks rather than further natural-language prompts. Yet Shin et al. (2020) show how to use naturallanguage prompts to ask about particular sentences. Our method could potentially be applied to those prompts, or to \"few-shot learning\" prompts that include input-output examples (Brown et al., 2020) .",
                "cite_spans": [
                    {
                        "start": 15,
                        "end": 37,
                        "text": "Bouraoui et al. (2020)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 156,
                        "end": 184,
                        "text": "Schick and Sch\u00fctze (2020a,b)",
                        "ref_id": null
                    },
                    {
                        "start": 409,
                        "end": 430,
                        "text": "Eichler et al., 2019)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 514,
                        "end": 532,
                        "text": "Shin et al. (2020)",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 735,
                        "end": 755,
                        "text": "(Brown et al., 2020)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "Our experiments will specifically aim at extracting relational knowledge from language models. We are given a fixed pretrained LM, a specific binary relation r such as date-of-death, and a training dataset E r consisting of known (x, y) pairs in r, such as (Mary Cassatt, 1926) . We will then train a system to predict y from x, and evaluate it on held-out (x, y) pairs of the same relation.",
                "cite_spans": [
                    {
                        "start": 257,
                        "end": 277,
                        "text": "(Mary Cassatt, 1926)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Method",
                "sec_num": "3"
            },
            {
                "text": "A prompt t is a sentence or phrase that includes two blanks, as illustrated in \u00a71. To pose the query, we fill the",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Method",
                "sec_num": "3"
            },
            {
                "text": "x blank with x: We can ask the LM for its probability distribution p LM (y | t, x) over single words that can now fill y . The correct answer would be 1926.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Method",
                "sec_num": "3"
            },
            {
                "text": "Suppose the LM identifies the word types with vectors in R d . We also allow t to be a soft prompt, in which the tokens can be arbitrary vectors in R d :",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Soft Prompts",
                "sec_num": "3.1"
            },
            {
                "text": "x v 1 v 2 v 3 v 4 v 5 y v 6",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Soft Prompts",
                "sec_num": "3.1"
            },
            {
                "text": "We can initialize these vectors to match those of a given hard prompt. (Each token of a hard prompt may be a word, subword, or punctuation mark, according to the tokenization procedure used by the LM.) However, we can then tune the vectors continuously. We do not change the number of vectors or their positions. For the prompt shown above, we have a 6d-dimensional search space.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Soft Prompts",
                "sec_num": "3.1"
            },
            {
                "text": "For each token i of a prompt, the vector v i enters into the LM's computations that complete the prompt. For example, a Transformer architecture computes successively deeper contextual embeddings of the token, v",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Deeply Perturbed Prompts",
                "sec_num": "3.2"
            },
            {
                "text": "( ) i : 0 \u2264 \u2264 L. Here v (0) i = v i and the embedding v ( )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Deeply Perturbed Prompts",
                "sec_num": "3.2"
            },
            {
                "text": "i at layer > 0 is computed from all tokens' embeddings v ( -1) j at the previous layer, using the LM's parameters.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Deeply Perturbed Prompts",
                "sec_num": "3.2"
            },
            {
                "text": "We can tune the prompt by additively perturbing each v ( ) i by a small vector \u2206 ( ) i before it is used in further computations. The \u2206 vectors for a given hard prompt are initialized to 0 and then tuned.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Deeply Perturbed Prompts",
                "sec_num": "3.2"
            },
            {
                "text": "Perturbing only layer 0 is equivalent to tuning v i directly as in \u00a73.1. However, if we are more aggressive and perturb all layers, we now have 6d \u2022 (L + 1) parameters to tune a 6-token prompt. The perturbations (\u2206 vectors) can be kept small through early stopping or some other form of regularization. Our intuition is that small perturbations will yield more \"familiar\" activation patterns that are similar to those that the LM was originally trained on. (Li and Liang (2021) tried a rather different approach to preventing overfitting when tuning all layers.)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Deeply Perturbed Prompts",
                "sec_num": "3.2"
            },
            {
                "text": "Given a set T r of soft prompts for relation r, we can define the ensemble predictive distribution",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Mixture Modeling",
                "sec_num": "3.3"
            },
            {
                "text": "p(y | x, r) = t\u2208Tr p(t | r) \u2022 p LM (y | t, x) (1)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Mixture Modeling",
                "sec_num": "3.3"
            },
            {
                "text": "where the learned mixture weights p(t | r) form a distribution over the soft prompts t \u2208 T r . Ensembling techniques other than mixture-of-experts could also be used, including product-of-experts (Jiang et al., 2020) .",
                "cite_spans": [
                    {
                        "start": 196,
                        "end": 216,
                        "text": "(Jiang et al., 2020)",
                        "ref_id": "BIBREF10"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Mixture Modeling",
                "sec_num": "3.3"
            },
            {
                "text": "As an extension, we can replace the mixture weights p(t | r) with p(t | r, x), to allow the model to select prompts that are appropriate for the given x. For example, a plural noun x might prefer prompts t that use a plural verb.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data-Dependent Mixture Modeling",
                "sec_num": "3.4"
            },
            {
                "text": "While we could directly build a neural softmax model for p(t | r, x), it seems useful to capture the intuition that t may work better if x is plausible in its",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data-Dependent Mixture Modeling",
                "sec_num": "3.4"
            },
            {
                "text": "x . Thus, we instead use Bayes' Theorem to write p(t | r, x) as proportional to p(t | r) \u2022 p(x | t, r)1/T , where we have included T to modulate the strength of the above intuition. 1 Here p(t | r) is still a learned distribution over prompts, and we use the fixed language model to estimate the second factor as y p LM (x, y | t) (dropping the dependence on r just as we did for the second factor of (1)). log T is tuned along with all other parameters.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data-Dependent Mixture Modeling",
                "sec_num": "3.4"
            },
            {
                "text": "Given an initial set of prompts T r , we jointly optimize the soft prompts t \u2208 T and their mixture weights p(t | r) (and log T in \u00a73.4) to minimize the log-loss of the predictive distribution (1):",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training Objective",
                "sec_num": "3.5"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "(x,y)\u2208Er -log t\u2208Tr p(y | t, x)",
                        "eq_num": "(2)"
                    }
                ],
                "section": "Training Objective",
                "sec_num": "3.5"
            },
            {
                "text": "This is a continuous and differentiable objective whose gradient can be computed by backpropagation. It can be locally minimized by gradient descent (using a softmax parameterization of the mixture weights). Equivalently, it can be locally minimized by the EM algorithm: the E step finds a posterior distribution over latent prompts for each (x, y) example, and the M step performs gradient descent to optimize the prompts in that mixture.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training Objective",
                "sec_num": "3.5"
            },
            {
                "text": "The relations we learn to predict are T-REx original (Elsahar et al., 2018) , T-REx extended (Shin et al., 2020) , Google-RE (Orr, 2013) , and ConceptNet (Speer et al., 2017) -or rather, the subsets that were used by the LAMA and AutoPrompt papers. See Appendix A for some statistics.",
                "cite_spans": [
                    {
                        "start": 53,
                        "end": 75,
                        "text": "(Elsahar et al., 2018)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 93,
                        "end": 112,
                        "text": "(Shin et al., 2020)",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 125,
                        "end": 136,
                        "text": "(Orr, 2013)",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 154,
                        "end": 174,
                        "text": "(Speer et al., 2017)",
                        "ref_id": "BIBREF31"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Relational Datasets",
                "sec_num": "4.1"
            },
            {
                "text": "Following Petroni et al. (2019) , we interrogate BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) . These are masked (cloze) language models. For variety, we also interrogate BART (Lewis et al., 2020a) , which conditions on the prompt with empty y and generates a copy where y has been filled in (by a single token). We constrain BART's decoding to ensure that its answer does take this form. Unlike BERT and RoBERTa, BART could be used to fill y with an arbitrarily long phrase, but we do not allow this because y in our datasets is always a single token.2 ",
                "cite_spans": [
                    {
                        "start": 10,
                        "end": 31,
                        "text": "Petroni et al. (2019)",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 54,
                        "end": 75,
                        "text": "(Devlin et al., 2019)",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 88,
                        "end": 106,
                        "text": "(Liu et al., 2019)",
                        "ref_id": null
                    },
                    {
                        "start": 189,
                        "end": 210,
                        "text": "(Lewis et al., 2020a)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Language Models",
                "sec_num": "4.2"
            },
            {
                "text": "For the two T-REx datasets, we inherit the trainingvalidation-test split from Shin et al. (2020) . For the other datasets, we split randomly in the ratio 80-10-10.3 Since all pairs (x, y) are distinct, there are no common triples among these three sets. Common x values are also rare because each dataset has at least 174 distinct x values. However, the number of distinct y values can be as small as 6. Thus, in another set of experiments (Appendix E), we used a more challenging split that ensures that there are no common y values among these three sets. This tests whether our model generalizes to unseen values.",
                "cite_spans": [
                    {
                        "start": 78,
                        "end": 96,
                        "text": "Shin et al. (2020)",
                        "ref_id": "BIBREF28"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dataset Splits",
                "sec_num": "4.3"
            },
            {
                "text": "For the T-REx and Google-RE datasets, we have four sources of initial prompts:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Prompts",
                "sec_num": "4.4"
            },
            {
                "text": "\u2022 (sin.) LAMA provides a single manually created hard prompt for each relation type r.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Prompts",
                "sec_num": "4.4"
            },
            {
                "text": "\u2022 (par.) LPAQA (Jiang et al., 2020) provides a set of 13-30 hard prompts for each r, which are paraphrases of the LAMA prompt.4 ",
                "cite_spans": [
                    {
                        "start": 15,
                        "end": 35,
                        "text": "(Jiang et al., 2020)",
                        "ref_id": "BIBREF10"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Prompts",
                "sec_num": "4.4"
            },
            {
                "text": "\u2022 (min.) LPAQA also provides a set of 6-29 hard prompts for each r, based on text mining.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Prompts",
                "sec_num": "4.4"
            },
            {
                "text": "\u2022 (ran.) For each (min.) prompt, we replace each word with a random vector, drawn from a Gaussian distribution fit to all of the LM's word embeddings. The number of words and the position of the blanks are preserved.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Prompts",
                "sec_num": "4.4"
            },
            {
                "text": "For the ConceptNet dataset, LAMA uses the gold Open Mind Common Sense (OMCS) dataset (Singh et al., 2002) . In this dataset, each example (x i , y i ) is equipped with its own prompt t i . (Each example is really a sentence with two substrings marked as x and y, which are removed to obtain t i .) These prompts are often overly specific: often y i can be predicted from (t i , x i ), or just from t i alone, but y j cannot be predicted from (t i , x j ). Thus, for each relation r, we use only the prompts that appear more than 10 times, resulting in 1-38 prompts.",
                "cite_spans": [
                    {
                        "start": 85,
                        "end": 105,
                        "text": "(Singh et al., 2002)",
                        "ref_id": "BIBREF29"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Prompts",
                "sec_num": "4.4"
            },
            {
                "text": "Statistics about the prompts are in Appendix B. We used only a single copy of each prompt, but a generalization would be to allow multiple slightly perturbed copies of each prompt, which could diverge and specialize during training (Rose, 1998) .",
                "cite_spans": [
                    {
                        "start": 232,
                        "end": 244,
                        "text": "(Rose, 1998)",
                        "ref_id": "BIBREF25"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Prompts",
                "sec_num": "4.4"
            },
            {
                "text": "We optimize equation ( 2) with the method introduced in \u00a73.5. We use the Adam optimizer (Kingma and Ba, 2015) with its default configuration. For gradient training, we set the batch size as 64, early-stop patience as 4, and test with the model that performs best on the dev set among 16 training epochs.",
                "cite_spans": [
                    {
                        "start": 88,
                        "end": 109,
                        "text": "(Kingma and Ba, 2015)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training",
                "sec_num": "4.5"
            },
            {
                "text": "Training is fast. Even for our largest model (BERT-large-cased) and largest dataset (T-REx extended), tuning a single prompt completes within a few minutes. With a mixture of prompts, training scales roughly linearly with the number of prompts. It is still presumably much cheaper in time and memory than fine-tuning the entire BERT model, which must back-propagate a much larger set of gradients.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training",
                "sec_num": "4.5"
            },
            {
                "text": "Our method outputs the most probable y given (r, x). Here and in the supplementary material, we report its average performance on all test examples, with precision-at-1 (P@1), precision-at-10 (P@10) and mean reciprocal rank (MRR) as metrics. We measure the improvement from tuning LAMA, LPAQA, and random prompts. We also compare with AutoPrompt. Baseline numbers come from prior papers or our reimplementations.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Metrics and Baselines",
                "sec_num": "4.6"
            },
            {
                "text": "Table 1 shows results on T-REx datasets obtained by querying three BERT-style models, with P@1 as the metric. Additional metrics and language models are shown in Tables 2 and 3 as well as Tables 5 and 6 in the supplementary material.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "1",
                        "ref_id": null
                    },
                    {
                        "start": 169,
                        "end": 170,
                        "text": "2",
                        "ref_id": "TABREF3"
                    },
                    {
                        "start": 175,
                        "end": 176,
                        "text": "3",
                        "ref_id": "TABREF4"
                    },
                    {
                        "start": 195,
                        "end": 196,
                        "text": "5",
                        "ref_id": null
                    },
                    {
                        "start": 201,
                        "end": 202,
                        "text": "6",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "4.7"
            },
            {
                "text": "We consistently get large improvements by tuning the initial prompts. Remarkably, our method beats all prior methods even when throwing away the words of their informed prompts in favor of random initial vectors. It simply finds a prompt that works well on the (x, y) training examples.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "4.7"
            },
            {
                "text": "We conduct an ablation study where we adjust only the mixture weights (which are initially uni- ) Soft (min., BEb) 50.7 ? (+16.6 ? ) 50.5 ? (+19.3 ? ) Soft (par., BEb) 48.4 (+12.8 ? ) 49.7 (+18.5 ? ) Soft (ran., BEb) 48.1 (+47.4) 50.6 (+49.8) LAMA (BEl) 28.9 \u2020 24.0 \u2020 LPAQA(BEl)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "4.7"
            },
            {
                "text": "39.4 \u2020 37.8 \u2020 Soft (sin., BEl) 51.1 (+22.2) 51.4 (+27.4) Soft (min., BEl) 51.6 (+12.2) 52.5 (+14.7) Soft (par., BEl) 51.1 (+11.7) 51.7 (+13.9) Soft (ran., BEl) 51.9 (+47.1) 51.9 (+50.5) AutoPrompt 40.0 -Soft (min., Rob) 40.6 ? (+39.4) -Table 1 : Results on T-REx datasets with P@1 as the metric. The \"Soft\" lines (our method) parenthetically show the improvement over the initial parameters (boldfaced if significant). In each subcolumn of comparable results, we boldface the best result along with all that are not significantly worse (sign test, p < 0.02).",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 242,
                        "end": 243,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "4.7"
            },
            {
                "text": "(We marked a boldface number with \"?\" if we lacked access to per-example output for one of the systems; differences from such systems were simply assumed to be significant.) \u2020 marks baseline results obtained from our reimplementations. In the Model column, BEb is BERT-base, BEl is BERT-large, Rob is RoBERTa-base. form) or only the word vectors in the prompts t. As Table 4 shows, each helps, but the major benefit comes from tuning the word vectors to get soft prompts. Appendix C visualizes a set of soft prompts, and Appendix D analyzes the mixture weights. We also experiment on a challenging setting where the y labels are distinct for training and test (Appendix E in the supplementary materials), and find that soft prompts still yield some benefits.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 373,
                        "end": 374,
                        "text": "4",
                        "ref_id": "TABREF5"
                    }
                ],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "4.7"
            },
            {
                "text": "The above results are for our basic method that tunes only the words of the prompt (i.e., layer 0). When we tune all layers-the \"deeply perturbed prompts\" of \u00a73.2-we typically obtain small additional gains, across various models and initializations, although tuning all layers does substantially hurt RoBERTa. These results are shown in Tables 5 and 6 in the supplementary material.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 344,
                        "end": 345,
                        "text": "5",
                        "ref_id": null
                    },
                    {
                        "start": 350,
                        "end": 351,
                        "text": "6",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "4.7"
            },
            {
                "text": "The tables show that the winning systemfor each combination of language model, T-REx dataset, and evaluation metric-always uses a mixture of soft prompts initialized to mined prompts. It always tunes all layers, except with RoBERTa.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "4.7"
            },
            {
                "text": "Finally, we also tried using data-dependent mix-Model P@1 P@10 MRR LAMA 9.7 \u2020 27.0 \u2020 15.6 \u2020 LPAQA 10.6 \u2020 23.7 \u2020 15.3 \u2020 Soft (sin.) 11.2 (+1.5) 33.5 (+ 6.5) 18.9 (+3.3) Soft (min.) 12.9 (+2.3) 34.7 (+11.0) 20.3 (+5.0) Soft (par.) 11.5 (+0.9) 31.4 (+ 7.7) 18.3 (+3.0) ture weights as in \u00a73.4. This had little effect, because training learned to discard the x information by setting the temperature parameter T high.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "4.7"
            },
            {
                "text": "Well-crafted natural language prompts are a powerful way to extract information from pretrained language models. In the case of cloze prompts used to query BERT and BART models for single-word answers, we have demonstrated startlingly large and consistent improvements from rapidly learning prompts that work-even though the resulting \"soft prompts\" are no longer natural language.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "5"
            },
            {
                "text": "Our code and data are available at https:// github.com/hiaoxui/soft-prompts.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "5"
            },
            {
                "text": "How about few-shot prediction with pretrained generative LMs? Here, Lewis et al. (2020b) show how to assemble a natural language prompt for input x from relevant input-output pairs (x i , y i ) selected by a trained retrieval model. Allowing fine-tuned soft string pairs is an intriguing future possibility for improving such methods without needing to fine-tune the entire language model.",
                "cite_spans": [
                    {
                        "start": 68,
                        "end": 88,
                        "text": "Lewis et al. (2020b)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "5"
            },
            {
                "text": "The statistics of the various relational databases are shown in Table 8 .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 70,
                        "end": 71,
                        "text": "8",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "A Statistics of Relational Databases",
                "sec_num": null
            },
            {
                "text": "Table 7 shows some statistics of the prompts we use to initialize the SoftPrompt model.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "7",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "B Statistics of the Initial Prompts",
                "sec_num": null
            },
            {
                "text": "Figure 1 shows what a mixture of soft prompts looks like when we tune only layer 0. The soft prompts are not too interpretable. The words closest to the tuned tokens (shown in blue) seem to be largely on the music topic. However, the soft templates do not seem to form meaningful phrases, nor is it obvious why they would prime for y to be an instrument when x is a musician.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "C Visualization of Soft Prompts",
                "sec_num": null
            },
            {
                "text": "For any given relation r, the entropy of the mixture weights is",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "D Entropy of the Mixture Model",
                "sec_num": null
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "H = t\u2208Tr p(t | r) \u2022 log 2 p(t | r)",
                        "eq_num": "(3)"
                    }
                ],
                "section": "D Entropy of the Mixture Model",
                "sec_num": null
            },
            {
                "text": "We then take 2 H \u2208 [1, |T r |] as a measure of the effective number of prompts that were retained. Table 10 shows some statistics of the effective number of prompts. In some cases, tuning the mixture weights essentially selected a single prompt, but on average, it settled on a mixture of several variant prompts (as illustrated by Figure 1 ).",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 105,
                        "end": 107,
                        "text": "10",
                        "ref_id": null
                    },
                    {
                        "start": 339,
                        "end": 340,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "D Entropy of the Mixture Model",
                "sec_num": null
            },
            {
                "text": "As described in \u00a74.3, we conducted an additional experiment to determine whether the prompts could generalize to novel y values. We conduct another experiment and ensure that there are no common y values among the train / dev / test sets. We use T-REx as the base relational database and split the datasets to make the ratio close to 80-10-10. The experiment results are shown in Table 9 . We can observe that our method again improves the results, just as in Tables 5 and 6 , which shows the generalizability of our method.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 386,
                        "end": 387,
                        "text": "9",
                        "ref_id": null
                    },
                    {
                        "start": 467,
                        "end": 468,
                        "text": "5",
                        "ref_id": null
                    },
                    {
                        "start": 473,
                        "end": 474,
                        "text": "6",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "E Challenging dataset with distinct y's",
                "sec_num": null
            },
            {
                "text": "[ ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E Challenging dataset with distinct y's",
                "sec_num": null
            },
            {
                "text": "Raising the temperature T increases the entropy of the mixture to get the benefits of ensembling; without T , the strong language model usually places almost all the weight on a single prompt.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Among other filters, the LAMA and AutoPrompt papers keep only the triples (r, x, y) such that y is a single token according to the language models used by LAMA. When working with BART, we further require y to be a single token according to BART's tokenization; thus, the BART results are not comparable with the other language models.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "The LAMA paper(Petroni et al., 2019) provided no split but used everything as test data for their zero-shot method.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "The LPAQA system combines their predictions via a learned weighted product of experts.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "We thank the anonymous reviewers for helpful comments. This work was supported by DARPA KAIROS and by the National Science Foundation under Grant No. 1718846. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes. The views and conclusions contained in this publication are those of the authors, and should not be interpreted as representing official policies nor endorsement by the funding agencies or by Microsoft (where Dr. Eisner is also a paid employee, in an arrangement that has been reviewed and approved by the Johns Hopkins University in accordance with its conflict of interest policies).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgments",
                "sec_num": null
            },
            {
                "text": "Figure 1 : Visualization of the LPAQA mining prompts for relation P1303 Instrument (i.e., x plays instrument y) from T-REx extended. We show the effect of tuning the layer-0 token embeddings (but not higher layers) on BERT-large-cased. The prompts are sorted in decreasing order by mixture weight. Each prompt's weight is shown at left; note that after the first 12 prompts, the remaining ones have negligible contribution. We show each soft prompt in blue, followed by the original (mined) prompt in red. To visualize the tuned vector v, we display the blue word w that maximizes p(w | v). The brightness of the blue word w and the original red word w 0 are respectively proportional to p(w | v) and p(w 0 | v). The red word has size 1, and the blue word has size ||v||/||v 0 ||, where v 0 is the original untuned vector (the embedding of w 0 ). In this example, the blue probabilities p(w | v) range from 6.5e-5 to 9.7e-5 (mean 8.6e-5 \u00b1 8.1e-6), the red probabilities p(w 0 | v) range from 7.7e-5 to 1.1e-4 (mean 9.5e-5 \u00b1 7.8e-6), and the relative magnitudes ||v||/||v 0 || vary from 1.00 to 1.49 (mean 1.12 \u00b1 0.13). ------\u2192 45.7 + 2.0 -----\u2192 47.7 59.5 +16.3 ? ------\u2192 75.8 + 3.2 -----\u2192 79.0 40.3 +15.9 ? ------\u2192 56.2 + 2.2 -----\u2192 58.4 Soft (min.) 34.1 +14.7 ?",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "annex",
                "sec_num": null
            },
            {
                "text": "------\u2192 48.8 + 1.9 -----\u2192 50.7 ? 62.0 +15.6 ? ------\u2192 79.6 + 1.1 -----\u2192 80.7 ? 43.6 +15.8 ? ------\u2192 59.4 + 1.7 -----\u2192 61.1 ? Soft (par.) 34.1 +12.8 ?------\u2192 46.9 + 1.5 -----\u2192 48.4 62.0 +16.8 ? ------\u2192 78.8 + 0.8 -----\u2192 79.6 43.6 +14.2 ? ------\u2192 57.8 + 1.3 -----\u2192 59.1 Soft (ran.) 0.7 +46.6 ------\u2192 47.3 + 0.8 -----\u2192 48.1 4.6 +74.0 ------\u2192 79.1 + 0.0 ----\u2192 79.1 2.3 +56.1 ------\u2192 58.4 + 0.5 -----\u2192 58.9 BEl LAMA 28.9 \u2020 57.7 \u2020 38.7 \u2020 LPAQA 39.4 \u2020 67.4 \u2020 49.1 \u2020 Soft (sin.) 28.9 +16.9-----\u2192 45.8 + 5.3 -----\u2192 51.1 57.7 +19.0 -----\u2192 76.7 + 4.4 -----\u2192 81.1 38.7 +17.8 -----\u2192 56.5 + 5.0 -----\u2192 61.5 Soft (min.) 39.4 +11.6 -----\u2192 51.0 + 0.6 -----\u2192 51.6 67.4 +14.0 -----\u2192 81.4 + 0.5 -----\u2192 81.9 49.1 +12.5 -----\u2192 61.6 + 0.5 -----\u2192 62.1 Soft (par.) 39.4-----\u2192 49.4 + 1.9 -----\u2192 51.3 8.0 +73.0 -----\u2192 81.0 + 0.7 -----\u2192 81.7 4.5 +55.9 -----\u2192 60.4 + 1.5 -----\u2192 61.9-----\u2192 39.9 5.7 +69.7 -----\u2192 75.4 2.9 +49.2 -----\u2192 52.1Table 5 : Experimental results on T-REx original datasets. In the LM column, BEb is BERT-base-cased, BEl is BERT-large-cased, BAb is BART-base-cased, BAl is BART-large-cased, Rob is RoBERTa-base, and Rol is RoBERTa-large. In the results block, \"init\" uses the initial untuned prompts; \"soft\" starts at \"init\" and tunes the prompts (layer 0) and mixture weights; and \"deep\" starts at \"init\" and tunes all the layers. Numbers above the arrows are the relative change in the performance. Within each block, we boldface the best system and all those that are not significantly worse (paired permutation test, p < 0.02). We also boldface the relative changes that are significantly different from 0. Other symbols are as in Table 1 . ------\u2192 48.6 + 1.0 -----\u2192 49.6 54.3 +23.3 ? ------\u2192 77.6 + 0.3 -----\u2192 77.9 35.8 +22.9 ? ------\u2192 58.7 + 0.6 -----\u2192 59.3 Soft (min.) 31.2 +19.0 ?",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 914,
                        "end": 915,
                        "text": "5",
                        "ref_id": null
                    },
                    {
                        "start": 1633,
                        "end": 1634,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "LM Method",
                "sec_num": null
            },
            {
                "text": "------\u2192 50.2 + 0.3 -----\u2192 50.5 ? 57.3 +21.9 ? ------\u2192 79.2 + 0.5 -----\u2192 79.7 ? 39.9 +20.2 ? ------\u2192 60.1 + 0.4 -----\u2192 60.5 ? Soft (par.) 31.2 +18.5 ?------\u2192 49.7 + 0.0 ----\u2192 49.7 57.3 +21.3 ? ------\u2192 78.6 + 0.6 -----\u2192 79.2 39.9 +19.6 ? ------\u2192 59.5 + 0.3 -----\u2192 59.8 Soft (ran.) 0.8 +46.3 ------\u2192 47.1 + 3.5 -----\u2192 50.6 4.0 +70.4 ------\u2192 74.4 + 4.9 -----\u2192 79.3 2.2 +54.3 ------\u2192 56.5 + 3.9 -----\u2192 60.4 BEl LAMA 24.0 \u2020 53.7 \u2020 34.1 \u2020 LPAQA 37.8 \u2020 64.4 \u2020 44.0 \u2020 Soft (sin.) 24.0 +26.2 -----\u2192 50.2 + 1.2 -----\u2192 51.4 53.7 +24.9 -----\u2192 78.6 + 0.9 -----\u2192 79.5 34.1 +25.9 -----\u2192 60.0 + 1.2 -----\u2192 61.2 Soft (min.) 37.8 +13.4 -----\u2192 51.2 + 1.3 -----\u2192 52.5 64.4 +15.1 -----\u2192 79.5 + 1.6 -----\u2192 81.1 44.0 +17.0 -----\u2192 61.0 + 1.4 -----\u2192 62.4 Soft (par.) 37.8 +12.5 -----\u2192 50.3 + 1.4 -----\u2192 51.7 64.4 +14.3 -----\u2192 78.7 + 2.1 -----\u2192 80.8 44.0 +16.1 -----\u2192 60.1 + 1.6 -----\u2192 61.7 Soft (ran.) 1.4 +46.1-----\u2192 47.5 + 4.4 -----\u2192 51.9 5.4 +68.9 -----\u2192 74.3 + 6.3 -----\u2192 80.6 5.7 +51.2 -----\u2192 56.9 + 5.0 -----\u2192 61.9 ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "LM Method",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Don't count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors",
                "authors": [
                    {
                        "first": "Marco",
                        "middle": [],
                        "last": "Baroni",
                        "suffix": ""
                    },
                    {
                        "first": "Georgiana",
                        "middle": [],
                        "last": "Dinu",
                        "suffix": ""
                    },
                    {
                        "first": "Germ\u00e1n",
                        "middle": [],
                        "last": "Kruszewski",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Association for Computational Linguistics (ACL)",
                "volume": "",
                "issue": "",
                "pages": "238--247",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Marco Baroni, Georgiana Dinu, and Germ\u00e1n Kruszewski. 2014. Don't count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors. In Associa- tion for Computational Linguistics (ACL), pages 238-247.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "PIQA: Reasoning about physical commonsense in natural language",
                "authors": [
                    {
                        "first": "Yonatan",
                        "middle": [],
                        "last": "Bisk",
                        "suffix": ""
                    },
                    {
                        "first": "Rowan",
                        "middle": [],
                        "last": "Zellers",
                        "suffix": ""
                    },
                    {
                        "first": "Le",
                        "middle": [],
                        "last": "Ronan",
                        "suffix": ""
                    },
                    {
                        "first": "Jianfeng",
                        "middle": [],
                        "last": "Bras",
                        "suffix": ""
                    },
                    {
                        "first": "Yejin",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Choi",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Association for the Advancement of Artificial Intelligence (AAAI)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. 2019. PIQA: Reasoning about physical commonsense in natural language. In Asso- ciation for the Advancement of Artificial Intelligence (AAAI).",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Inducing relational knowledge from BERT",
                "authors": [
                    {
                        "first": "Zied",
                        "middle": [],
                        "last": "Bouraoui",
                        "suffix": ""
                    },
                    {
                        "first": "Jose",
                        "middle": [],
                        "last": "Camacho-Collados",
                        "suffix": ""
                    },
                    {
                        "first": "Steven",
                        "middle": [],
                        "last": "Schockaert",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Association for the Advancement of Artificial Intelligence (AAAI)",
                "volume": "34",
                "issue": "",
                "pages": "7456--7463",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zied Bouraoui, Jose Camacho-Collados, and Steven Schockaert. 2020. Inducing relational knowledge from BERT. In Association for the Advancement of Artificial Intelligence (AAAI), volume 34, pages 7456-7463.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "The assumptions and theory of public opinion polling",
                "authors": [
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Broughton",
                        "suffix": ""
                    }
                ],
                "year": 1995,
                "venue": "Public Opinion Polling and Politics in Britain",
                "volume": "",
                "issue": "",
                "pages": "15--33",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "David Broughton. 1995. The assumptions and theory of public opinion polling. In Public Opinion Polling and Politics in Britain, pages 15-33. Springer.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
                "authors": [
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Devlin",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Toutanova",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "J. Devlin, M. Chang, K. Lee, and K. Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In North American Association for Computational Linguistics (NAACL).",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Children's Minds",
                "authors": [
                    {
                        "first": "M",
                        "middle": [
                            "C"
                        ],
                        "last": "Donaldson",
                        "suffix": ""
                    }
                ],
                "year": 1978,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "M. C. Donaldson. 1978. Children's Minds. W. W. Nor- ton.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "LINSPECTOR WEB: A multilingual probing suite for word representations",
                "authors": [
                    {
                        "first": "Max",
                        "middle": [],
                        "last": "Eichler",
                        "suffix": ""
                    },
                    {
                        "first": "G\u00f6zde",
                        "middle": [],
                        "last": "G\u00fcl \u015eahin",
                        "suffix": ""
                    },
                    {
                        "first": "Iryna",
                        "middle": [],
                        "last": "Gurevych",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations",
                "volume": "",
                "issue": "",
                "pages": "127--132",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/D19-3022"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Max Eichler, G\u00f6zde G\u00fcl \u015eahin, and Iryna Gurevych. 2019. LINSPECTOR WEB: A multilingual prob- ing suite for word representations. In Proceedings of the 2019 Conference on Empirical Methods in Nat- ural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations, pages 127-132.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "T-REx: A large scale alignment of natural language with knowledge base triples",
                "authors": [
                    {
                        "first": "Hady",
                        "middle": [],
                        "last": "Elsahar",
                        "suffix": ""
                    },
                    {
                        "first": "Pavlos",
                        "middle": [],
                        "last": "Vougiouklis",
                        "suffix": ""
                    },
                    {
                        "first": "Arslen",
                        "middle": [],
                        "last": "Remaci",
                        "suffix": ""
                    },
                    {
                        "first": "Christophe",
                        "middle": [],
                        "last": "Gravier",
                        "suffix": ""
                    },
                    {
                        "first": "Jonathon",
                        "middle": [],
                        "last": "Hare",
                        "suffix": ""
                    },
                    {
                        "first": "Elena",
                        "middle": [],
                        "last": "Simperl",
                        "suffix": ""
                    },
                    {
                        "first": "Frederique",
                        "middle": [],
                        "last": "Laforest",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Language Resources and Evaluation Conference (LREC)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Hady Elsahar, Pavlos Vougiouklis, Arslen Remaci, Christophe Gravier, Jonathon Hare, Elena Simperl, and Frederique Laforest. 2018. T-REx: A large scale alignment of natural language with knowledge base triples. In Language Resources and Evaluation Conference (LREC), page 5.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Automatic acquisition of hyponyms from large text corpora",
                "authors": [
                    {
                        "first": "M",
                        "middle": [
                            "A"
                        ],
                        "last": "Hearst",
                        "suffix": ""
                    }
                ],
                "year": 1992,
                "venue": "International Conference on Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "M. A. Hearst. 1992. Automatic acquisition of hy- ponyms from large text corpora. In International Conference on Computational Linguistics (COL- ING).",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "How can we know what language models know? Transactions of the",
                "authors": [
                    {
                        "first": "Zhengbao",
                        "middle": [],
                        "last": "Jiang",
                        "suffix": ""
                    },
                    {
                        "first": "Frank",
                        "middle": [
                            "F"
                        ],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Jun",
                        "middle": [],
                        "last": "Araki",
                        "suffix": ""
                    },
                    {
                        "first": "Graham",
                        "middle": [],
                        "last": "Neubig",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. 2020. How can we know what language models know? Transactions of the Association for Computational Linguistics (TACL).",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Adam: A method for stochastic optimization",
                "authors": [
                    {
                        "first": "D",
                        "middle": [
                            "P"
                        ],
                        "last": "Kingma",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [
                            "L"
                        ],
                        "last": "Ba",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "International Conference on Learning Representations (ICLR)",
                "volume": "",
                "issue": "",
                "pages": "1--15",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "D. P. Kingma and J. L. Ba. 2015. Adam: A method for stochastic optimization. In International Confer- ence on Learning Representations (ICLR), pages 1- 15.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Neural architectures for named entity recognition",
                "authors": [
                    {
                        "first": "Guillaume",
                        "middle": [],
                        "last": "Lample",
                        "suffix": ""
                    },
                    {
                        "first": "Miguel",
                        "middle": [],
                        "last": "Ballesteros",
                        "suffix": ""
                    },
                    {
                        "first": "Sandeep",
                        "middle": [],
                        "last": "Subramanian",
                        "suffix": ""
                    },
                    {
                        "first": "Kazuya",
                        "middle": [],
                        "last": "Kawakami",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Dyer",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "North American Association for Computational Linguistics and Human Language Technology (NAACL-HLT)",
                "volume": "",
                "issue": "",
                "pages": "260--270",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Guillaume Lample, Miguel Ballesteros, Sandeep Sub- ramanian, Kazuya Kawakami, and Chris Dyer. 2016. Neural architectures for named entity recog- nition. In North American Association for Computa- tional Linguistics and Human Language Technology (NAACL-HLT), pages 260-270.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension",
                "authors": [
                    {
                        "first": "Mike",
                        "middle": [],
                        "last": "Lewis",
                        "suffix": ""
                    },
                    {
                        "first": "Yinhan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Naman",
                        "middle": [],
                        "last": "Goyal ; Abdelrahman Mohamed",
                        "suffix": ""
                    },
                    {
                        "first": "Omer",
                        "middle": [],
                        "last": "Levy",
                        "suffix": ""
                    },
                    {
                        "first": "Ves",
                        "middle": [],
                        "last": "Stoyanov",
                        "suffix": ""
                    },
                    {
                        "first": "Luke",
                        "middle": [],
                        "last": "Zettlemoyer",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Association for Computational Linguistics (ACL)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mike Lewis, Yinhan Liu, Naman Goyal, Mar- jan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2020a. BART: Denoising sequence-to-sequence pre- training for natural language generation, translation, and comprehension. In Association for Computa- tional Linguistics (ACL).",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Sebastian Riedel, and Douwe Kiela. 2020b. Retrieval-augmented generation for knowledge-intensive NLP tasks",
                "authors": [
                    {
                        "first": "Patrick",
                        "middle": [],
                        "last": "Lewis",
                        "suffix": ""
                    },
                    {
                        "first": "Ethan",
                        "middle": [],
                        "last": "Perez",
                        "suffix": ""
                    },
                    {
                        "first": "Aleksandara",
                        "middle": [],
                        "last": "Piktus",
                        "suffix": ""
                    },
                    {
                        "first": "Fabio",
                        "middle": [],
                        "last": "Petroni",
                        "suffix": ""
                    },
                    {
                        "first": "Vladimir",
                        "middle": [],
                        "last": "Karpukhin",
                        "suffix": ""
                    },
                    {
                        "first": "Naman",
                        "middle": [],
                        "last": "Goyal",
                        "suffix": ""
                    },
                    {
                        "first": "Heinrich",
                        "middle": [],
                        "last": "K\u00fcttler",
                        "suffix": ""
                    },
                    {
                        "first": "Mike",
                        "middle": [],
                        "last": "Lewis",
                        "suffix": ""
                    },
                    {
                        "first": "Wen",
                        "middle": [],
                        "last": "Tau Yih",
                        "suffix": ""
                    },
                    {
                        "first": "Tim",
                        "middle": [],
                        "last": "Rockt\u00e4schel",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2005.11401"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen tau Yih, Tim Rockt\u00e4schel, Sebastian Riedel, and Douwe Kiela. 2020b. Retrieval-augmented generation for knowledge-intensive NLP tasks. arXiv preprint arXiv:2005.11401.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Prefixtuning: Optimizing continuous prompts for generation",
                "authors": [
                    {
                        "first": "Lisa",
                        "middle": [],
                        "last": "Xiang",
                        "suffix": ""
                    },
                    {
                        "first": "Percy",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Liang",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2101.00190"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Xiang Lisa Li and Percy Liang. 2021. Prefix- tuning: Optimizing continuous prompts for genera- tion. arXiv preprint arXiv:2101.00190.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "GPT understands, too",
                "authors": [
                    {
                        "first": "Xiao",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Yanan",
                        "middle": [],
                        "last": "Zheng",
                        "suffix": ""
                    },
                    {
                        "first": "Zhengxiao",
                        "middle": [],
                        "last": "Du",
                        "suffix": ""
                    },
                    {
                        "first": "Ming",
                        "middle": [],
                        "last": "Ding",
                        "suffix": ""
                    },
                    {
                        "first": "Yujie",
                        "middle": [],
                        "last": "Qian",
                        "suffix": ""
                    },
                    {
                        "first": "Zhilin",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Jie",
                        "middle": [],
                        "last": "Tang",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2103.10385"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. 2021. GPT understands, too. arXiv preprint arXiv:2103.10385.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "50,000 lessons on how to read: A relation extraction corpus",
                "authors": [
                    {
                        "first": "Dave",
                        "middle": [],
                        "last": "Orr",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dave Orr. 2013. 50,000 lessons on how to read: A re- lation extraction corpus. https://github. com/google-research-datasets/ relation-extraction-corpus.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Deep contextualized word representations",
                "authors": [
                    {
                        "first": "M",
                        "middle": [
                            "E"
                        ],
                        "last": "Peters",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Neumann",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Iyyer",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Gardner",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Clark",
                        "suffix": ""
                    },
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "L",
                        "middle": [
                            "S"
                        ],
                        "last": "Zettlemoyer",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. S. Zettlemoyer. 2018. Deep contextualized word representations. In North American Association for Computational Linguistics (NAACL).",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Language models as knowledge bases?",
                "authors": [
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Petroni",
                        "suffix": ""
                    },
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Rockt\u00e4schel",
                        "suffix": ""
                    },
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Lewis",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Bakhtin",
                        "suffix": ""
                    },
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [
                            "H"
                        ],
                        "last": "Miller",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Riedel",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "F. Petroni, T. Rockt\u00e4schel, P. Lewis, A. Bakhtin, Y. Wu, A. H. Miller, and S. Riedel. 2019. Language mod- els as knowledge bases? In Empirical Methods in Natural Language Processing (EMNLP).",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Language models are unsupervised multitask learners",
                "authors": [
                    {
                        "first": "Alec",
                        "middle": [],
                        "last": "Radford",
                        "suffix": ""
                    },
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Rewon",
                        "middle": [],
                        "last": "Child",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Luan",
                        "suffix": ""
                    },
                    {
                        "first": "Dario",
                        "middle": [],
                        "last": "Amodei",
                        "suffix": ""
                    },
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Entity linking: Finding extracted entities in a knowledge base",
                "authors": [
                    {
                        "first": "Delip",
                        "middle": [],
                        "last": "Rao",
                        "suffix": ""
                    },
                    {
                        "first": "Paul",
                        "middle": [],
                        "last": "Mcnamee",
                        "suffix": ""
                    },
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Dredze",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Multi-Source, Multilingual Information Extraction and Summarization",
                "volume": "",
                "issue": "",
                "pages": "93--115",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Delip Rao, Paul McNamee, and Mark Dredze. 2013. Entity linking: Finding extracted entities in a knowl- edge base. In Multi-Source, Multilingual Informa- tion Extraction and Summarization, pages 93-115. Springer.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Learning dictionaries for information extraction by multi-level bootstrapping",
                "authors": [
                    {
                        "first": "E",
                        "middle": [],
                        "last": "Riloff",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Jones",
                        "suffix": ""
                    }
                ],
                "year": 1999,
                "venue": "Association for the Advancement of Artificial Intelligence (AAAI)",
                "volume": "",
                "issue": "",
                "pages": "474--479",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "E. Riloff and R. Jones. 1999. Learning dictionaries for information extraction by multi-level bootstrapping. In Association for the Advancement of Artificial In- telligence (AAAI), pages 474-479.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "A primer in BERTology: What we know about how BERT works. Transactions of the Association for Computational Linguistics (TACL)",
                "authors": [
                    {
                        "first": "Anna",
                        "middle": [],
                        "last": "Rogers",
                        "suffix": ""
                    },
                    {
                        "first": "Olga",
                        "middle": [],
                        "last": "Kovaleva",
                        "suffix": ""
                    },
                    {
                        "first": "Anna",
                        "middle": [],
                        "last": "Rumshisky",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Anna Rogers, Olga Kovaleva, and Anna Rumshisky. 2020. A primer in BERTology: What we know about how BERT works. Transactions of the Associ- ation for Computational Linguistics (TACL).",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Deterministic annealing for clustering, compression, classification, regression, and related optimization problems",
                "authors": [
                    {
                        "first": "Kenneth",
                        "middle": [],
                        "last": "Rose",
                        "suffix": ""
                    }
                ],
                "year": 1998,
                "venue": "Proceedings of the IEEE",
                "volume": "80",
                "issue": "",
                "pages": "2210--2239",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kenneth Rose. 1998. Deterministic annealing for clus- tering, compression, classification, regression, and related optimization problems. Proceedings of the IEEE, 80:2210-2239.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Exploiting cloze questions for few-shot text classification and natural language inference",
                "authors": [
                    {
                        "first": "Timo",
                        "middle": [],
                        "last": "Schick",
                        "suffix": ""
                    },
                    {
                        "first": "Hinrich",
                        "middle": [],
                        "last": "Sch\u00fctze",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2001.07676"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Timo Schick and Hinrich Sch\u00fctze. 2020a. Exploit- ing cloze questions for few-shot text classification and natural language inference. arXiv preprint arXiv:2001.07676. Accepted to EACL 2021.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "It's not just size that matters: Small language models are also few-shot learners",
                "authors": [
                    {
                        "first": "Timo",
                        "middle": [],
                        "last": "Schick",
                        "suffix": ""
                    },
                    {
                        "first": "Hinrich",
                        "middle": [],
                        "last": "Sch\u00fctze",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2009.07118"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Timo Schick and Hinrich Sch\u00fctze. 2020b. It's not just size that matters: Small language mod- els are also few-shot learners. arXiv preprint arXiv:2009.07118.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "AutoPrompt: Eliciting knowledge from language models with automatically generated prompts",
                "authors": [
                    {
                        "first": "Taylor",
                        "middle": [],
                        "last": "Shin",
                        "suffix": ""
                    },
                    {
                        "first": "Yasaman",
                        "middle": [],
                        "last": "Razeghi",
                        "suffix": ""
                    },
                    {
                        "first": "Robert",
                        "middle": [
                            "L"
                        ],
                        "last": "Logan",
                        "suffix": ""
                    },
                    {
                        "first": "I",
                        "middle": [
                            "V"
                        ],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Eric",
                        "middle": [],
                        "last": "Wallace",
                        "suffix": ""
                    },
                    {
                        "first": "Sameer",
                        "middle": [],
                        "last": "Singh",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. 2020. AutoPrompt: Eliciting knowledge from language models with au- tomatically generated prompts. In Empirical Meth- ods in Natural Language Processing (EMNLP).",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Open Mind Common Sense: Knowledge acquisition from the general public",
                "authors": [
                    {
                        "first": "Push",
                        "middle": [],
                        "last": "Singh",
                        "suffix": ""
                    },
                    {
                        "first": "Thomas",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Erik",
                        "middle": [
                            "T"
                        ],
                        "last": "Mueller",
                        "suffix": ""
                    },
                    {
                        "first": "Grace",
                        "middle": [],
                        "last": "Lim",
                        "suffix": ""
                    },
                    {
                        "first": "Travell",
                        "middle": [],
                        "last": "Perkins",
                        "suffix": ""
                    },
                    {
                        "first": "Wan",
                        "middle": [
                            "Li"
                        ],
                        "last": "Zhu",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "On the Move to Meaningful Internet Systems 2002: CoopIS, DOA, and ODBASE",
                "volume": "2519",
                "issue": "",
                "pages": "1223--1237",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Push Singh, Thomas Lin, Erik T. Mueller, Grace Lim, Travell Perkins, and Wan Li Zhu. 2002. Open Mind Common Sense: Knowledge acquisition from the general public. In On the Move to Meaningful In- ternet Systems 2002: CoopIS, DOA, and ODBASE, volume 2519, pages 1223-1237. Springer.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "Contextaware representations for knowledge base relation extraction",
                "authors": [
                    {
                        "first": "Daniil",
                        "middle": [],
                        "last": "Sorokin",
                        "suffix": ""
                    },
                    {
                        "first": "Iryna",
                        "middle": [],
                        "last": "Gurevych",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Empirical Methods in Natural Language Processing (EMNLP)",
                "volume": "",
                "issue": "",
                "pages": "1784--1789",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Daniil Sorokin and Iryna Gurevych. 2017. Context- aware representations for knowledge base relation extraction. In Empirical Methods in Natural Lan- guage Processing (EMNLP), pages 1784-1789.",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "ConceptNet 5.5: An open multilingual graph of general knowledge",
                "authors": [
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Speer",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Chin",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Havasi",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Association for the Advancement of Artificial Intelligence (AAAI)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "R. Speer, J. Chin, and C. Havasi. 2017. ConceptNet 5.5: An open multilingual graph of general knowl- edge. In Association for the Advancement of Artifi- cial Intelligence (AAAI).",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "Overview of the English slot filling track at the TAC2014 knowledge base population evaluation",
                "authors": [
                    {
                        "first": "Mihai",
                        "middle": [],
                        "last": "Surdeanu",
                        "suffix": ""
                    },
                    {
                        "first": "Heng",
                        "middle": [],
                        "last": "Ji",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of the TAC-KBP 2014 Workshop",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mihai Surdeanu and Heng Ji. 2014. Overview of the English slot filling track at the TAC2014 knowledge base population evaluation. In Proceedings of the TAC-KBP 2014 Workshop.",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "oLMpics -On what language model pre-training captures",
                "authors": [
                    {
                        "first": "Alon",
                        "middle": [],
                        "last": "Talmor",
                        "suffix": ""
                    },
                    {
                        "first": "Yanal",
                        "middle": [],
                        "last": "Elazar",
                        "suffix": ""
                    },
                    {
                        "first": "Yoav",
                        "middle": [],
                        "last": "Goldberg",
                        "suffix": ""
                    },
                    {
                        "first": "Jonathan",
                        "middle": [],
                        "last": "Berant",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Transactions of the Association for Computational Linguistics",
                "volume": "8",
                "issue": "",
                "pages": "743--758",
                "other_ids": {
                    "DOI": [
                        "10.1162/tacl_a_00342"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Alon Talmor, Yanal Elazar, Yoav Goldberg, and Jonathan Berant. 2020. oLMpics -On what lan- guage model pre-training captures. Transactions of the Association for Computational Linguistics, 8:743-758.",
                "links": null
            }
        },
        "ref_entries": {
            "TABREF3": {
                "content": "<table><tr><td>Model</td><td>P@1</td><td>P@10</td><td>MRR</td></tr><tr><td colspan=\"2\">LAMA (BEb) 0.1  \u2020</td><td>2.6  \u2020</td><td>1.5  \u2020</td></tr><tr><td colspan=\"2\">LAMA (BEl) 0.1  \u2020</td><td>5.0  \u2020</td><td>1.9  \u2020</td></tr><tr><td colspan=\"4\">Soft (min.,BEb) 11.3(+11.2) 36.4(+33.8) 19.3(+17.8)</td></tr><tr><td colspan=\"4\">Soft (ran.,BEb) 11.8(+11.8) 34.8(+31.9) 19.8(+19.6)</td></tr><tr><td colspan=\"4\">Soft (min.,BEl) 12.8(+12.7) 37.0(+32.0) 20.9(+19.0)</td></tr><tr><td colspan=\"4\">Soft (ran.,BEl) 14.5(+14.5) 38.6(+34.2) 22.1(+21.9)</td></tr></table>",
                "type_str": "table",
                "text": "Results on Google-RE dataset obtained by querying the BERT-large-cased model.",
                "html": null,
                "num": null
            },
            "TABREF4": {
                "content": "<table><tr><td>Model</td><td colspan=\"3\">P@1 P@10 MRR</td></tr><tr><td>baseline</td><td>39.4</td><td>67.4</td><td>49.1</td></tr><tr><td colspan=\"2\">adjust mixture weights 40.0</td><td>69.1</td><td>53.3</td></tr><tr><td>adjust token vectors</td><td>50.7</td><td>80.7</td><td>61.1</td></tr><tr><td>adjust both</td><td>51.0</td><td>81.4</td><td>61.6</td></tr></table>",
                "type_str": "table",
                "text": "Results on ConceptNet (winner: random init).",
                "html": null,
                "num": null
            },
            "TABREF5": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Ablation experiments, conducted with the BERT-large model on the T-REx original dataset.",
                "html": null,
                "num": null
            }
        }
    }
}