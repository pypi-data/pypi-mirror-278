{
    "paper_id": "2021",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:35:12.824995Z"
    },
    "title": "L2C: Describing Visual Differences Needs Semantic Understanding of Individuals",
    "authors": [
        {
            "first": "An",
            "middle": [],
            "last": "Yan",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "UC Santa Cruz",
                "location": {
                    "postCode": "\u2663 UC",
                    "settlement": "Santa Barbara"
                }
            },
            "email": "ayan@ucsd.edu"
        },
        {
            "first": "\u2666",
            "middle": [],
            "last": "Xin",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "UC Santa Cruz",
                "location": {
                    "postCode": "\u2663 UC",
                    "settlement": "Santa Barbara"
                }
            },
            "email": ""
        },
        {
            "first": "Eric",
            "middle": [],
            "last": "Wang",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "UC Santa Cruz",
                "location": {
                    "postCode": "\u2663 UC",
                    "settlement": "Santa Barbara"
                }
            },
            "email": ""
        },
        {
            "first": "Tsu-Jui",
            "middle": [],
            "last": "Fu",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "UC Santa Cruz",
                "location": {
                    "postCode": "\u2663 UC",
                    "settlement": "Santa Barbara"
                }
            },
            "email": "tsu-juifu@cs.ucsb.edu"
        },
        {
            "first": "William",
            "middle": [],
            "last": "Yang",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "UC Santa Cruz",
                "location": {
                    "postCode": "\u2663 UC",
                    "settlement": "Santa Barbara"
                }
            },
            "email": "william@cs.ucsb.edu"
        },
        {
            "first": "San",
            "middle": [],
            "last": "Diego",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "UC Santa Cruz",
                "location": {
                    "postCode": "\u2663 UC",
                    "settlement": "Santa Barbara"
                }
            },
            "email": ""
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Recent advances in language and vision push forward the research of captioning a single image to describing visual differences between image pairs. Suppose there are two images, I 1 and I 2 , and the task is to generate a description W 1,2 comparing them, existing methods directly model \u27e8I 1 , I 2 \u27e9 \u2192 W 1,2 mapping without the semantic understanding of individuals. In this paper, we introduce a Learningto-Compare (L2C) model, which learns to understand the semantic structures of these two images and compare them while learning to describe each one. We demonstrate that L2C benefits from a comparison between explicit semantic representations and singleimage captions, and generalizes better on the new testing image pairs. It outperforms the baseline on both automatic evaluation and human evaluation for the Birds-to-Words dataset.",
    "pdf_parse": {
        "paper_id": "2021",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Recent advances in language and vision push forward the research of captioning a single image to describing visual differences between image pairs. Suppose there are two images, I 1 and I 2 , and the task is to generate a description W 1,2 comparing them, existing methods directly model \u27e8I 1 , I 2 \u27e9 \u2192 W 1,2 mapping without the semantic understanding of individuals. In this paper, we introduce a Learningto-Compare (L2C) model, which learns to understand the semantic structures of these two images and compare them while learning to describe each one. We demonstrate that L2C benefits from a comparison between explicit semantic representations and singleimage captions, and generalizes better on the new testing image pairs. It outperforms the baseline on both automatic evaluation and human evaluation for the Birds-to-Words dataset.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "The task of generating textual descriptions of images tests a machine's ability to understand visual data and interpret it in natural language. It is a fundamental research problem lying at the intersection of natural language processing, computer vision, and cognitive science. For example, single-image captioning (Farhadi et al., 2010; Kulkarni et al., 2013; Vinyals et al., 2015; Xu et al., 2015) has been extensively studied.",
                "cite_spans": [
                    {
                        "start": 316,
                        "end": 338,
                        "text": "(Farhadi et al., 2010;",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 339,
                        "end": 361,
                        "text": "Kulkarni et al., 2013;",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 362,
                        "end": 383,
                        "text": "Vinyals et al., 2015;",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 384,
                        "end": 400,
                        "text": "Xu et al., 2015)",
                        "ref_id": "BIBREF18"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Recently, a new intriguing task, visual comparison, along with several benchmarks (Jhamtani and Berg-Kirkpatrick, 2018; Tan et al., 2019; Park et al., 2019; Forbes et al., 2019) has drawn increasing attention in the community. To complete the task and generate comparative descriptions, a machine should understand the visual differences between a pair of images (see Figure 1 ). Previous methods (Jhamtani and Berg-Kirkpatrick, 2018) as the ResNet features (He et al., 2016) as a whole, and build end-to-end neural networks to predict the description of visual comparison directly. In contrast, humans can easily reason about the visual components of a single image and describe the visual differences between two images based on their semantic understanding of each one. Humans do not need to look at thousands of image pairs to describe the difference of new image pairs, as they can leverage their understanding of single images for visual comparison. Therefore, we believe that visual differences should be learned by understanding and comparing every single image's semantic representation. A most recent work (Zhang et al., 2020) conceptually supports this argument, where they show that low-level ResNet visual features lead to poor generalization in vision-and-language navigation, and high-level semantic segmentation helps the agent Image I 1",
                "cite_spans": [
                    {
                        "start": 82,
                        "end": 119,
                        "text": "(Jhamtani and Berg-Kirkpatrick, 2018;",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 120,
                        "end": 137,
                        "text": "Tan et al., 2019;",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 138,
                        "end": 156,
                        "text": "Park et al., 2019;",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 157,
                        "end": 177,
                        "text": "Forbes et al., 2019)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 397,
                        "end": 434,
                        "text": "(Jhamtani and Berg-Kirkpatrick, 2018)",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 458,
                        "end": 475,
                        "text": "(He et al., 2016)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 1116,
                        "end": 1136,
                        "text": "(Zhang et al., 2020)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 375,
                        "end": 376,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Image I 2 V 1 V 2 Relation- enhanced features V g 1 V g 2 LSTM h t",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "if 2-imgs:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Graph representation",
                "sec_num": null
            },
            {
                "text": "V g = V g 1 -V g 2 V g if 1 -im g if 2 -im g s",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Graph representation",
                "sec_num": null
            },
            {
                "text": "Visual comparisons generalize to unseen scenarios.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Semantic Pooling",
                "sec_num": null
            },
            {
                "text": "Motivated by humans, we propose a Learning-to-Compare (L2C) method that focuses on reasoning about the semantic structures of individual images and then compares the difference of the image pair. Our contributions are three-fold:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Single Image caption",
                "sec_num": null
            },
            {
                "text": "\u2022 We construct a structured image representation by leveraging image segmentation with a novel semantic pooling, and use graph convolutional networks to perform reasoning on these learned representations.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Single Image caption",
                "sec_num": null
            },
            {
                "text": "\u2022 We utilize single-image captioning data to boost semantic understanding of each image with its language counterpart.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Single Image caption",
                "sec_num": null
            },
            {
                "text": "\u2022 Our L2C model outperforms the baseline on both automatic evaluation and human evaluation, and generalizes better on the testing image pairs.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Single Image caption",
                "sec_num": null
            },
            {
                "text": "We present a novel framework in Figure 2 , which consists of three main components. First, a segmentation encoder is used to extract structured visual features with strong semantic priors. Then, a graph convolutional module performs reasoning on the learned semantic representations. To enhance the understanding of each image, we introduce a single-image captioning auxiliary loss to associate the single-image graph representation with the semantic meaning conveyed by its language counterpart. Finally, a decoder generates the visual descriptions comparing two images based on differences in graph representations. All parameters are shared for both images and both tasks.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 39,
                        "end": 40,
                        "text": "2",
                        "ref_id": "FIGREF2"
                    }
                ],
                "eq_spans": [],
                "section": "L2C Model",
                "sec_num": "2"
            },
            {
                "text": "To extract semantic visual features, we utilize pretrained fully convolutional networks (FCN) (Long et al., 2015) with ResNet-101 as the backbone. An image I is fed into the ResNet backbone to produce a feature map F \u2208 R D\u00d7H\u00d7W , which is then forwarded into an FCN head that generates a binary segmentation mask B for the bird class. However, the shapes of these masks are variable for each image, and simple pooling methods such as average pooling and max pooling would lose some information of spatial relations within the mask.",
                "cite_spans": [
                    {
                        "start": 94,
                        "end": 113,
                        "text": "(Long et al., 2015)",
                        "ref_id": "BIBREF10"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Semantic Representation Construction",
                "sec_num": "2.1"
            },
            {
                "text": "To address this issue and enable efficient aggregation over the area of interest (the masked area), we add a module after the ResNet to cluster each pixel within the mask into K classes. Feature map F is forwarded through this pooling module to obtain a confidence map C \u2208 R K\u00d7H\u00d7W , whose entry at each pixel is a K-dimensional vector that represents the probability distribution of K classes. Then a set of nodes",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Semantic Representation Construction",
                "sec_num": "2.1"
            },
            {
                "text": "V = {v 1 , ..., v K }, v k \u2208 R D is constructed as following: v k = i,j F \u2299 B \u2299 C k (1)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Semantic Representation Construction",
                "sec_num": "2.1"
            },
            {
                "text": "where i=1, ...H, j=1, ..., W,, C k is the k-th probability map and \u2299 denotes element-wise multiplication.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Semantic Representation Construction",
                "sec_num": "2.1"
            },
            {
                "text": "To enforce local smoothness, i.e., pixels in a neighborhood are more likely belong to one class, we employ total variation norm as a regularization term:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Semantic Representation Construction",
                "sec_num": "2.1"
            },
            {
                "text": "L T V = i,j |C i+1,j -Ci, j| + |C i,j+1 -Ci, j| (2)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Semantic Representation Construction",
                "sec_num": "2.1"
            },
            {
                "text": "Inspired by recent advances in visual reasoning and graph neural networks (Chen et al., 2018; Li et al., 2019) , we introduce a relational reasoning module to enhance the semantic representation of each image. A fully-connected visual semantic graph G = (V, E) is built, where V is the set of nodes, each containing a regional feature, and E is constructed by measuring the pairwise affinity between each two nodes v i , v j in a latent space.",
                "cite_spans": [
                    {
                        "start": 74,
                        "end": 93,
                        "text": "(Chen et al., 2018;",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 94,
                        "end": 110,
                        "text": "Li et al., 2019)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Comparative Relational Reasoning",
                "sec_num": "2.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "A(v i , v j ) = (W i v i ) T (W j v j )",
                        "eq_num": "(3)"
                    }
                ],
                "section": "Comparative Relational Reasoning",
                "sec_num": "2.2"
            },
            {
                "text": "where W i , W j are learnable matrices, and A is the constructed adjacency matrix.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Comparative Relational Reasoning",
                "sec_num": "2.2"
            },
            {
                "text": "We apply Graph Convolutional Networks (GCN) (Kipf and Welling, 2016) to perform reasoning on the graph. After the GCN module, the output",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Comparative Relational Reasoning",
                "sec_num": "2.2"
            },
            {
                "text": "V o = {v o 1 , ..., v o K }, v o k \u2208 R D will be a rela-",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Comparative Relational Reasoning",
                "sec_num": "2.2"
            },
            {
                "text": "tionship enhanced representation of a bird. For the visual comparison task, we compute the difference of each two visual nodes from two sets, denoted as",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Comparative Relational Reasoning",
                "sec_num": "2.2"
            },
            {
                "text": "V g dif f = {v o dif f,1 , ..., v o dif f,K }, v o dif f,k = v o k,1 -v o k,2 \u2208 R D .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Comparative Relational Reasoning",
                "sec_num": "2.2"
            },
            {
                "text": "After obtaining relation-enhanced semantic features, we use a Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) The LSTM takes V o or V o dif f with previous output word embedding y t-1 as input, updates the hidden state from h t-1 to h t , and predicts the word for the next time step. The generation process of bi-image comparison is learned by maximizing the log-likelihood of the predicted output sentence. The loss function is defined as follows:",
                "cite_spans": [
                    {
                        "start": 92,
                        "end": 126,
                        "text": "(Hochreiter and Schmidhuber, 1997)",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning to Compare while Learning to Describe",
                "sec_num": "2.3"
            },
            {
                "text": "L dif f = - t log P (y t |y 1\u2236t-1 , V o dif f ) (4)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning to Compare while Learning to Describe",
                "sec_num": "2.3"
            },
            {
                "text": "Similar loss is applied for learning single-image captioning:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning to Compare while Learning to Describe",
                "sec_num": "2.3"
            },
            {
                "text": "L single = - t log P (y t |y 1\u2236t-1 , V o ) (5)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning to Compare while Learning to Describe",
                "sec_num": "2.3"
            },
            {
                "text": "Overall, the model is optimized with a mixture of cross-entropy losses and total variation loss:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning to Compare while Learning to Describe",
                "sec_num": "2.3"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "L loss = L dif f + L single + \u03bbL T V (",
                        "eq_num": "6"
                    }
                ],
                "section": "Learning to Compare while Learning to Describe",
                "sec_num": "2.3"
            },
            {
                "text": ")",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning to Compare while Learning to Describe",
                "sec_num": "2.3"
            },
            {
                "text": "where \u03bb is an adaptive factor that weighs the total variation loss.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning to Compare while Learning to Describe",
                "sec_num": "2.3"
            },
            {
                "text": "3 Experiments Evaluation Metrics Performances are first evaluated on three automatic metrics1 : BLEU-4 (Papineni et al., 2002) , ROUGE-L (Lin, 2004) , and CIDEr-D (Vedantam et al., 2015) . Each generated description is compared to all five reference paragraphs. Note for this particular task, researchers observe that CIDEr-D is susceptible to common patterns in the data (See Table 1 for proof), and ROUGE-L is anecdotally correlated with higherquality descriptions (which is noted in previous work (Forbes et al., 2019) ). Hence we consider ROUGE-L as the major metric for evaluating performances. We then perform a human evaluation to further verify the performance.",
                "cite_spans": [
                    {
                        "start": 103,
                        "end": 126,
                        "text": "(Papineni et al., 2002)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 137,
                        "end": 148,
                        "text": "(Lin, 2004)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 163,
                        "end": 186,
                        "text": "(Vedantam et al., 2015)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 500,
                        "end": 521,
                        "text": "(Forbes et al., 2019)",
                        "ref_id": "BIBREF2"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 383,
                        "end": 384,
                        "text": "1",
                        "ref_id": "TABREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Learning to Compare while Learning to Describe",
                "sec_num": "2.3"
            },
            {
                "text": "Implementation Details We use Adam as the optimizer with an initial learning rate set to 1e-4. The pooling module to generate K classes is composed of two convolutional layers and batch normalization, with kernel sizes 3 and 1 respectively. We set K to 9 and \u03bb to 1. The dimension of graph representations is 512. The hidden size of the decoder is also 512. The batch sizes of B2W and CUB are 16 and 128. Following the advice from (Forbes et al., 2019) , we report the results using models with the highest ROUGE-L on the validation set, since it could correlate better with high-quality outputs for this task.",
                "cite_spans": [
                    {
                        "start": 431,
                        "end": 452,
                        "text": "(Forbes et al., 2019)",
                        "ref_id": "BIBREF2"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning to Compare while Learning to Describe",
                "sec_num": "2.3"
            },
            {
                "text": "As shown in ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Automatic Evaluation",
                "sec_num": "3.2"
            },
            {
                "text": "To fully evaluate our model, we conduct a pairwise human evaluation on Amazon Mechanical Turk with 100 image pairs randomly sampled from the test set, each sample was assigned to 5 workers to eliminate human variance. Following Wang et al. (2018) , for each image pair, workers are presented with two paragraphs from different models and asked to choose the better one based on text ",
                "cite_spans": [
                    {
                        "start": 228,
                        "end": 246,
                        "text": "Wang et al. (2018)",
                        "ref_id": "BIBREF17"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Human Evaluation",
                "sec_num": "3.3"
            },
            {
                "text": "Effect of Individual Components We perform ablation studies to show the effectiveness of semantic pooling, total variance loss, and graph reasoning, as shown in Table 3 . First, without semantic pooling, the model degrades to average pooling, and results show that semantic pooling can better preserve the spatial relations for the visual representations. Moreover, the total variation loss can further boost the performance by injecting the prior local smoothness. Finally, the results without GCN are lower than the full L2C model, indicating graph convolutions can efficiently modeling relations among visual regions.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 167,
                        "end": 168,
                        "text": "3",
                        "ref_id": "TABREF5"
                    }
                ],
                "eq_spans": [],
                "section": "Ablation Studies",
                "sec_num": "3.4"
            },
            {
                "text": "We analyze model performance under a varying number of K (K is the number of classes for confidence map C), as shown in Figure 3 . Empirically, we found the results are comparable when K is small. ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 127,
                        "end": 128,
                        "text": "3",
                        "ref_id": "FIGREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Sensitivity Test",
                "sec_num": null
            },
            {
                "text": "In this paper, we present a learning-to-compare framework for generating visual comparisons. Our segmentation encoder with semantic pooling and graph reasoning could construct structured image representations. We also show that learning to describe visual differences benefits from understanding the semantics of each image.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "4"
            },
            {
                "text": "https://www.nltk.org",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "We instruct the annotators to consider two perspectives, relevance (the text describes the context of two images) and expressiveness (grammatically and semantically correct).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "The research was partly sponsored by the U.S. Army Research Office and was accomplished under Contract Number W911NF19-D-0001 for the Institute for Collaborative Biotechnologies. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgments",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Iterative visual reasoning beyond convolutions",
                "authors": [
                    {
                        "first": "Xinlei",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Li-Jia",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Li",
                        "middle": [],
                        "last": "Fei-Fei",
                        "suffix": ""
                    },
                    {
                        "first": "Abhinav",
                        "middle": [],
                        "last": "Gupta",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": "",
                "issue": "",
                "pages": "7239--7248",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xinlei Chen, Li-Jia Li, Li Fei-Fei, and Abhinav Gupta. 2018. Iterative visual reasoning beyond convolu- tions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7239-7248.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Every picture tells a story: Generating sentences from images",
                "authors": [
                    {
                        "first": "Ali",
                        "middle": [],
                        "last": "Farhadi",
                        "suffix": ""
                    },
                    {
                        "first": "Mohsen",
                        "middle": [],
                        "last": "Hejrati",
                        "suffix": ""
                    },
                    {
                        "first": "Mohammad",
                        "middle": [
                            "Amin"
                        ],
                        "last": "Sadeghi",
                        "suffix": ""
                    },
                    {
                        "first": "Peter",
                        "middle": [],
                        "last": "Young",
                        "suffix": ""
                    },
                    {
                        "first": "Cyrus",
                        "middle": [],
                        "last": "Rashtchian",
                        "suffix": ""
                    },
                    {
                        "first": "Julia",
                        "middle": [],
                        "last": "Hockenmaier",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Forsyth",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "European conference on computer vision",
                "volume": "",
                "issue": "",
                "pages": "15--29",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ali Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi, Peter Young, Cyrus Rashtchian, Julia Hockenmaier, and David Forsyth. 2010. Every pic- ture tells a story: Generating sentences from images. In European conference on computer vision, pages 15-29. Springer.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Neural naturalist: Generating fine-grained image comparisons",
                "authors": [
                    {
                        "first": "Maxwell",
                        "middle": [],
                        "last": "Forbes",
                        "suffix": ""
                    },
                    {
                        "first": "Christine",
                        "middle": [],
                        "last": "Kaeser-Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Piyush",
                        "middle": [],
                        "last": "Sharma",
                        "suffix": ""
                    },
                    {
                        "first": "Serge",
                        "middle": [],
                        "last": "Belongie",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1909.04101"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Maxwell Forbes, Christine Kaeser-Chen, Piyush Sharma, and Serge Belongie. 2019. Neural natu- ralist: Generating fine-grained image comparisons. arXiv preprint arXiv:1909.04101.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Deep residual learning for image recognition",
                "authors": [
                    {
                        "first": "Kaiming",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Xiangyu",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Shaoqing",
                        "middle": [],
                        "last": "Ren",
                        "suffix": ""
                    },
                    {
                        "first": "Jian",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
                "volume": "",
                "issue": "",
                "pages": "770--778",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recog- nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770- 778.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Long short-term memory",
                "authors": [
                    {
                        "first": "Sepp",
                        "middle": [],
                        "last": "Hochreiter",
                        "suffix": ""
                    },
                    {
                        "first": "J\u00fcrgen",
                        "middle": [],
                        "last": "Schmidhuber",
                        "suffix": ""
                    }
                ],
                "year": 1997,
                "venue": "Neural computation",
                "volume": "9",
                "issue": "8",
                "pages": "1735--1780",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735-1780.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Learning to describe differences between pairs of similar images",
                "authors": [
                    {
                        "first": "Harsh",
                        "middle": [],
                        "last": "Jhamtani",
                        "suffix": ""
                    },
                    {
                        "first": "Taylor",
                        "middle": [],
                        "last": "Berg-Kirkpatrick",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1808.10584"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Harsh Jhamtani and Taylor Berg-Kirkpatrick. 2018. Learning to describe differences between pairs of similar images. arXiv preprint arXiv:1808.10584.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Semisupervised classification with graph convolutional networks",
                "authors": [
                    {
                        "first": "N",
                        "middle": [],
                        "last": "Thomas",
                        "suffix": ""
                    },
                    {
                        "first": "Max",
                        "middle": [],
                        "last": "Kipf",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Welling",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1609.02907"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Thomas N Kipf and Max Welling. 2016. Semi- supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Babytalk: Understanding and generating simple image descriptions",
                "authors": [
                    {
                        "first": "Girish",
                        "middle": [],
                        "last": "Kulkarni",
                        "suffix": ""
                    },
                    {
                        "first": "Visruth",
                        "middle": [],
                        "last": "Premraj",
                        "suffix": ""
                    },
                    {
                        "first": "Vicente",
                        "middle": [],
                        "last": "Ordonez",
                        "suffix": ""
                    },
                    {
                        "first": "Sagnik",
                        "middle": [],
                        "last": "Dhar",
                        "suffix": ""
                    },
                    {
                        "first": "Siming",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Yejin",
                        "middle": [],
                        "last": "Choi",
                        "suffix": ""
                    },
                    {
                        "first": "Alexander",
                        "middle": [
                            "C"
                        ],
                        "last": "Berg",
                        "suffix": ""
                    },
                    {
                        "first": "Tamara",
                        "middle": [
                            "L"
                        ],
                        "last": "Berg",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "volume": "35",
                "issue": "12",
                "pages": "2891--2903",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Girish Kulkarni, Visruth Premraj, Vicente Ordonez, Sagnik Dhar, Siming Li, Yejin Choi, Alexander C Berg, and Tamara L Berg. 2013. Babytalk: Under- standing and generating simple image descriptions. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(12):2891-2903.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Visual semantic reasoning for imagetext matching",
                "authors": [
                    {
                        "first": "Kunpeng",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Yulun",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Kai",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Yuanyuan",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Yun",
                        "middle": [],
                        "last": "Fu",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the IEEE International Conference on Computer Vision",
                "volume": "",
                "issue": "",
                "pages": "4654--4662",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kunpeng Li, Yulun Zhang, Kai Li, Yuanyuan Li, and Yun Fu. 2019. Visual semantic reasoning for image- text matching. In Proceedings of the IEEE Interna- tional Conference on Computer Vision, pages 4654- 4662.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "ROUGE: A package for automatic evaluation of summaries",
                "authors": [
                    {
                        "first": "Chin-Yew",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Text Summarization Branches Out",
                "volume": "",
                "issue": "",
                "pages": "74--81",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chin-Yew Lin. 2004. ROUGE: A package for auto- matic evaluation of summaries. In Text Summariza- tion Branches Out, pages 74-81, Barcelona, Spain. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Fully convolutional networks for semantic segmentation",
                "authors": [
                    {
                        "first": "Jonathan",
                        "middle": [],
                        "last": "Long",
                        "suffix": ""
                    },
                    {
                        "first": "Evan",
                        "middle": [],
                        "last": "Shelhamer",
                        "suffix": ""
                    },
                    {
                        "first": "Trevor",
                        "middle": [],
                        "last": "Darrell",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
                "volume": "",
                "issue": "",
                "pages": "3431--3440",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jonathan Long, Evan Shelhamer, and Trevor Darrell. 2015. Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE confer- ence on computer vision and pattern recognition, pages 3431-3440.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Bleu: a method for automatic evaluation of machine translation",
                "authors": [
                    {
                        "first": "Kishore",
                        "middle": [],
                        "last": "Papineni",
                        "suffix": ""
                    },
                    {
                        "first": "Salim",
                        "middle": [],
                        "last": "Roukos",
                        "suffix": ""
                    },
                    {
                        "first": "Todd",
                        "middle": [],
                        "last": "Ward",
                        "suffix": ""
                    },
                    {
                        "first": "Wei-Jing",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Proceedings of the 40th annual meeting on association for computational linguistics",
                "volume": "",
                "issue": "",
                "pages": "311--318",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic eval- uation of machine translation. In Proceedings of the 40th annual meeting on association for compu- tational linguistics, pages 311-318. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Robust change captioning",
                "authors": [
                    {
                        "first": "Dong",
                        "middle": [],
                        "last": "Huk",
                        "suffix": ""
                    },
                    {
                        "first": "Park",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Trevor",
                        "middle": [],
                        "last": "Darrell",
                        "suffix": ""
                    },
                    {
                        "first": "Anna",
                        "middle": [],
                        "last": "Rohrbach",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the IEEE International Conference on Computer Vision",
                "volume": "",
                "issue": "",
                "pages": "4624--4633",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dong Huk Park, Trevor Darrell, and Anna Rohrbach. 2019. Robust change captioning. In Proceedings of the IEEE International Conference on Computer Vision, pages 4624-4633.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Expressing visual relationships via language",
                "authors": [
                    {
                        "first": "Franck",
                        "middle": [],
                        "last": "Hao Tan",
                        "suffix": ""
                    },
                    {
                        "first": "Zhe",
                        "middle": [],
                        "last": "Dernoncourt",
                        "suffix": ""
                    },
                    {
                        "first": "Trung",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Mohit",
                        "middle": [],
                        "last": "Bui",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Bansal",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1906.07689"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Hao Tan, Franck Dernoncourt, Zhe Lin, Trung Bui, and Mohit Bansal. 2019. Expressing visual relationships via language. arXiv preprint arXiv:1906.07689.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Cider: Consensus-based image description evaluation",
                "authors": [
                    {
                        "first": "Ramakrishna",
                        "middle": [],
                        "last": "Vedantam",
                        "suffix": ""
                    },
                    {
                        "first": "Lawrence",
                        "middle": [],
                        "last": "Zitnick",
                        "suffix": ""
                    },
                    {
                        "first": "Devi",
                        "middle": [],
                        "last": "Parikh",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
                "volume": "",
                "issue": "",
                "pages": "4566--4575",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. 2015. Cider: Consensus-based image de- scription evaluation. In Proceedings of the IEEE conference on computer vision and pattern recogni- tion, pages 4566-4575.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Show and tell: A neural image caption generator",
                "authors": [
                    {
                        "first": "Oriol",
                        "middle": [],
                        "last": "Vinyals",
                        "suffix": ""
                    },
                    {
                        "first": "Alexander",
                        "middle": [],
                        "last": "Toshev",
                        "suffix": ""
                    },
                    {
                        "first": "Samy",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    },
                    {
                        "first": "Dumitru",
                        "middle": [],
                        "last": "Erhan",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
                "volume": "",
                "issue": "",
                "pages": "3156--3164",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. 2015. Show and tell: A neural im- age caption generator. In Proceedings of the IEEE conference on computer vision and pattern recogni- tion, pages 3156-3164.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "The caltech-ucsd birds-200-2011 dataset",
                "authors": [
                    {
                        "first": "Catherine",
                        "middle": [],
                        "last": "Wah",
                        "suffix": ""
                    },
                    {
                        "first": "Steve",
                        "middle": [],
                        "last": "Branson",
                        "suffix": ""
                    },
                    {
                        "first": "Peter",
                        "middle": [],
                        "last": "Welinder",
                        "suffix": ""
                    },
                    {
                        "first": "Pietro",
                        "middle": [],
                        "last": "Perona",
                        "suffix": ""
                    },
                    {
                        "first": "Serge",
                        "middle": [],
                        "last": "Belongie",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. 2011. The caltech-ucsd birds-200-2011 dataset.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "No metrics are perfect: Adversarial reward learning for visual storytelling",
                "authors": [
                    {
                        "first": "Xin",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Wenhu",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Yuan-Fang",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "William",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Wang",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
                "volume": "1",
                "issue": "",
                "pages": "899--909",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xin Wang, Wenhu Chen, Yuan-Fang Wang, and William Yang Wang. 2018. No metrics are perfect: Adversarial reward learning for visual storytelling. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 899-909.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Show, attend and tell: Neural image caption generation with visual attention",
                "authors": [
                    {
                        "first": "Kelvin",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Jimmy",
                        "middle": [],
                        "last": "Ba",
                        "suffix": ""
                    },
                    {
                        "first": "Ryan",
                        "middle": [],
                        "last": "Kiros",
                        "suffix": ""
                    },
                    {
                        "first": "Kyunghyun",
                        "middle": [],
                        "last": "Cho",
                        "suffix": ""
                    },
                    {
                        "first": "Aaron",
                        "middle": [],
                        "last": "Courville",
                        "suffix": ""
                    },
                    {
                        "first": "Ruslan",
                        "middle": [],
                        "last": "Salakhudinov",
                        "suffix": ""
                    },
                    {
                        "first": "Rich",
                        "middle": [],
                        "last": "Zemel",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "International conference on machine learning",
                "volume": "",
                "issue": "",
                "pages": "2048--2057",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. 2015. Show, attend and tell: Neural image caption generation with visual atten- tion. In International conference on machine learn- ing, pages 2048-2057.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Diagnosing the environment bias in vision-and-language navigation",
                "authors": [
                    {
                        "first": "Yubo",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Hao",
                        "middle": [],
                        "last": "Tan",
                        "suffix": ""
                    },
                    {
                        "first": "Mohit",
                        "middle": [],
                        "last": "Bansal",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2005.03086"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yubo Zhang, Hao Tan, and Mohit Bansal. 2020. Diag- nosing the environment bias in vision-and-language navigation. arXiv preprint arXiv:2005.03086.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 1: Overview of the visual comparison task and our motivation. The key is to understand both images and compare them. Explicit semantic structures can be compared between images and used to generate comparative descriptions aligned to the image saliency.",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "Figure 2: Our L2C model. It consists of a segmentation encoder, a graph convolutional module, and an LSTM decoder with an auxiliary loss for single-image captioning. Details are in Section 2.",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "FIGREF3": {
                "num": null,
                "text": "Figure 3: Sensitivity test on number of K chosen.",
                "uris": null,
                "fig_num": "3",
                "type_str": "figure"
            },
            "TABREF2": {
                "content": "<table><tr><td/><td/><td/><td>Validation</td><td/><td/><td>Test</td></tr><tr><td>Model</td><td/><td>BLEU-4 \u2191</td><td>ROUGE-L \u2191</td><td>CIDEr-D \u2191</td><td>BLEU-4 \u2191</td><td>ROUGE-L \u2191</td><td>CIDEr-D \u2191</td></tr><tr><td>Most Frequent</td><td/><td>20.0</td><td>31.0</td><td>42.0</td><td>20.0</td><td>30.0</td><td>43.0</td></tr><tr><td>Text-Only</td><td/><td>14.0</td><td>36.0</td><td>5.0</td><td>14.0</td><td>36.0</td><td>7.0</td></tr><tr><td>Neural Naturalist</td><td/><td>24.0</td><td>46.0</td><td>28.0</td><td>22.0</td><td>43.0</td><td>25.0</td></tr><tr><td>CNN+LSTM</td><td/><td>25.1</td><td>43.4</td><td>10.2</td><td>24.9</td><td>43.2</td><td>9.9</td></tr><tr><td>L2C [B2W]</td><td/><td>31.9</td><td>45.7</td><td>15.2</td><td>31.3</td><td>45.3</td><td>15.1</td></tr><tr><td colspan=\"2\">L2C [CUB+B2W]</td><td>32.3</td><td>46.2</td><td>16.4</td><td>31.8</td><td>45.6</td><td>16.3</td></tr><tr><td>Human</td><td/><td>26.0</td><td>47.0</td><td>39.0</td><td>27.0</td><td>47.0</td><td>42.0</td></tr><tr><td colspan=\"4\">Choice (%) L2C CNN+LSTM Tie</td><td/><td/><td/></tr><tr><td>Score</td><td>50.8</td><td>39.4</td><td>9.8</td><td/><td/><td/></tr></table>",
                "type_str": "table",
                "text": "Results for visual comparison on the Birds-to-Words dataset(Forbes et al., 2019). Most Frequent produces only the most observed description in the dataset: \"the two animals appear to be exactly the same\". Text-Only samples captions from the training data according to their empirical distribution. Neural Naturalist is a transformer model inForbes et al. (2019). CNN+LSTM is a commonly-used CNN encoder and LSTM decoder model.",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Human evaluation results. We present workers with two generations by L2C and CNN+LSTM for each image pair and let them choose the better one.",
                "html": null,
                "num": null
            },
            "TABREF4": {
                "content": "<table><tr><td>, first, L2C[B2W] (training</td></tr><tr><td>with visual comparison task only) outperforms</td></tr><tr><td>baseline methods on BLEU-4 and ROUGE-L. Pre-</td></tr><tr><td>vious approaches and architectures failed to bring</td></tr><tr><td>superior results by directly modeling the visual rela-</td></tr><tr><td>tionship on ResNet features. Second, joint learning</td></tr><tr><td>with a single-image caption L2C[B2W+CUB] can</td></tr><tr><td>help improve the ability of semantic understanding,</td></tr><tr><td>thus, the overall performance of the model. Finally,</td></tr><tr><td>our method also has a smaller gap between vali-</td></tr><tr><td>dation and test set compared to neural naturalist,</td></tr><tr><td>indicating its potential capability to generalize for</td></tr><tr><td>unseen samples.</td></tr></table>",
                "type_str": "table",
                "text": "",
                "html": null,
                "num": null
            },
            "TABREF5": {
                "content": "<table><tr><td>Validation</td></tr></table>",
                "type_str": "table",
                "text": "Ablation study on the B2W dataset. We individually remove Semantic Pooling, total variation (TV) loss, and GCN to test their effects.",
                "html": null,
                "num": null
            }
        }
    }
}