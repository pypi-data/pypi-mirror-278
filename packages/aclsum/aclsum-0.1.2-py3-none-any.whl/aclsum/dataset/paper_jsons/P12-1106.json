{
    "paper_id": "P12-1106",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T11:55:05.937595Z"
    },
    "title": "Combining Coherence Models and Machine Translation Evaluation Metrics for Summarization Evaluation",
    "authors": [
        {
            "first": "Ziheng",
            "middle": [],
            "last": "Lin",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "SAP Asia Pte Ltd",
                "location": {
                    "addrLine": "30 Pasir Panjang Road",
                    "postCode": "117440",
                    "country": "Singapore"
                }
            },
            "email": "ziheng.lin@sap.com"
        },
        {
            "first": "Chang",
            "middle": [],
            "last": "Liu",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "National University of Singapore",
                "location": {
                    "addrLine": "13 Computing Drive",
                    "postCode": "117417",
                    "country": "Singapore"
                }
            },
            "email": "liuchan1@comp.nus.edu.sg"
        },
        {
            "first": "Tou",
            "middle": [],
            "last": "Hwee",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "",
            "middle": [],
            "last": "Ng",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "National University of Singapore",
                "location": {
                    "addrLine": "13 Computing Drive",
                    "postCode": "117417",
                    "country": "Singapore"
                }
            },
            "email": ""
        },
        {
            "first": "Min-Yen",
            "middle": [],
            "last": "Kan",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "National University of Singapore",
                "location": {
                    "addrLine": "13 Computing Drive",
                    "postCode": "117417",
                    "country": "Singapore"
                }
            },
            "email": ""
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "An ideal summarization system should produce summaries that have high content coverage and linguistic quality. Many state-ofthe-art summarization systems focus on content coverage by extracting content-dense sentences from source articles. A current research focus is to process these sentences so that they read fluently as a whole. The current AE-SOP task encourages research on evaluating summaries on content, readability, and overall responsiveness. In this work, we adapt a machine translation metric to measure content coverage, apply an enhanced discourse coherence model to evaluate summary readability, and combine both in a trained regression model to evaluate overall responsiveness. The results show significantly improved performance over AESOP 2011 submitted metrics.",
    "pdf_parse": {
        "paper_id": "P12-1106",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "An ideal summarization system should produce summaries that have high content coverage and linguistic quality. Many state-ofthe-art summarization systems focus on content coverage by extracting content-dense sentences from source articles. A current research focus is to process these sentences so that they read fluently as a whole. The current AE-SOP task encourages research on evaluating summaries on content, readability, and overall responsiveness. In this work, we adapt a machine translation metric to measure content coverage, apply an enhanced discourse coherence model to evaluate summary readability, and combine both in a trained regression model to evaluate overall responsiveness. The results show significantly improved performance over AESOP 2011 submitted metrics.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Research and development on automatic and manual evaluation of summarization systems have been mainly focused on content coverage (Lin and Hovy, 2003; Nenkova and Passonneau, 2004; Hovy et al., 2006; Zhou et al., 2006) . However, users may still find it difficult to read such high-content coverage summaries as they lack fluency. To promote research on automatic evaluation of summary readability, the Text Analysis Conference (TAC) (Owczarzak and Dang, 2011) introduced a new subtask on readability to its Automatically Evaluating Summaries of Peers (AESOP) task.",
                "cite_spans": [
                    {
                        "start": 130,
                        "end": 150,
                        "text": "(Lin and Hovy, 2003;",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 151,
                        "end": 180,
                        "text": "Nenkova and Passonneau, 2004;",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 181,
                        "end": 199,
                        "text": "Hovy et al., 2006;",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 200,
                        "end": 218,
                        "text": "Zhou et al., 2006)",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 434,
                        "end": 460,
                        "text": "(Owczarzak and Dang, 2011)",
                        "ref_id": "BIBREF14"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Most of the state-of-the-art summarization systems (Ng et al., 2011; Zhang et al., 2011; Conroy et al., 2011) are extraction-based. They extract the most content-dense sentences from source articles. If no post-processing is performed to the generated summaries, the presentation of the extracted sentences may confuse readers. Knott (1996) argued that when the sentences of a text are randomly ordered, the text becomes difficult to understand, as its discourse structure is disturbed. Lin et al. (2011) validated this argument by using a trained model to differentiate an original text from a randomlyordered permutation of its sentences by looking at their discourse structures. This prior work leads us to believe that we can apply such discourse models to evaluate the readability of extract-based summaries. We will discuss the application of Lin et al.'s discourse coherence model to evaluate readability of machine generated summaries. We also introduce two new feature sources to enhance the model with hierarchical and Explicit/Non-Explicit information, and demonstrate that they improve the original model.",
                "cite_spans": [
                    {
                        "start": 51,
                        "end": 68,
                        "text": "(Ng et al., 2011;",
                        "ref_id": null
                    },
                    {
                        "start": 69,
                        "end": 88,
                        "text": "Zhang et al., 2011;",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 89,
                        "end": 109,
                        "text": "Conroy et al., 2011)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 328,
                        "end": 340,
                        "text": "Knott (1996)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 487,
                        "end": 504,
                        "text": "Lin et al. (2011)",
                        "ref_id": "BIBREF10"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "There are parallels between evaluations of machine translation (MT) and summarization with respect to textual content. For instance, the widely used ROUGE (Lin and Hovy, 2003) metrics are influenced by BLEU (Papineni et al., 2002) : both look at surface n-gram overlap for content coverage. Motivated by this, we will adapt a state-of-theart, linear programming-based MT evaluation metric, TESLA (Liu et al., 2010) , to evaluate the content coverage of summaries. TAC's overall responsiveness metric evaluates the quality of a summary with regard to both its content and readability. Given this, we combine our two component coherence and content models into an SVM-trained regression model as our surrogate to overall responsiveness. Our experiments show that the coherence model significantly outperforms all AESOP 2011 submissions on both initial and update tasks, while the adapted MT evaluation metric and the combined model significantly outperform all submissions on the initial task. To the best of our knowledge, this is the first work that applies a discourse coherence model to measure the readability of summaries in the AESOP task.",
                "cite_spans": [
                    {
                        "start": 155,
                        "end": 175,
                        "text": "(Lin and Hovy, 2003)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 207,
                        "end": 230,
                        "text": "(Papineni et al., 2002)",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 396,
                        "end": 414,
                        "text": "(Liu et al., 2010)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "2 Related Work Nenkova and Passonneau (2004) proposed a manual evaluation method that was based on the idea that there is no single best model summary for a collection of documents. Human annotators construct a pyramid to capture important Summarization Content Units (SCUs) and their weights, which is used to evaluate machine generated summaries. Lin and Hovy (2003) introduced an automatic summarization evaluation metric, called ROUGE, which was motivated by the MT evaluation metric, BLEU (Papineni et al., 2002) . It automatically determines the content quality of a summary by comparing it to the model summaries and counting the overlapping n-gram units. Two configurations -ROUGE-2, which counts bigram overlaps, and ROUGE-SU4, which counts unigram and bigram overlaps in a word window of four -have been found to correlate well with human evaluations. Hovy et al. (2006) pointed out that automated methods such as ROUGE, which match fixed length n-grams, face two problems of tuning the appropriate fragment lengths and matching them properly. They introduced an evaluation method that makes use of small units of content, called Basic Elements (BEs). Their method automatically segments a text into BEs, matches similar BEs, and finally scores them.",
                "cite_spans": [
                    {
                        "start": 15,
                        "end": 44,
                        "text": "Nenkova and Passonneau (2004)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 349,
                        "end": 368,
                        "text": "Lin and Hovy (2003)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 494,
                        "end": 517,
                        "text": "(Papineni et al., 2002)",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 862,
                        "end": 880,
                        "text": "Hovy et al. (2006)",
                        "ref_id": "BIBREF5"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Both ROUGE and BE have been implemented and included in the ROUGE/BE evaluation toolkit 1 , which has been used as the default evaluation tool in the summarization track in the Document Un-1 http://berouge.com/default.aspx derstanding Conference (DUC) and Text Analysis Conference (TAC). DUC and TAC also manually evaluated machine generated summaries by adopting the Pyramid method. Besides evaluating with ROUGE/BE and Pyramid, DUC and TAC also asked human judges to score every candidate summary with regard to its content, readability, and overall responsiveness.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "DUC and TAC defined linguistic quality to cover several aspects: grammaticality, non-redundancy, referential clarity, focus, and structure/coherence. Recently, Pitler et al. (2010) conducted experiments on various metrics designed to capture these aspects. Their experimental results on DUC 2006 and 2007 show that grammaticality can be measured by a set of syntactic features, while the last three aspects are best evaluated by local coherence. Conroy and Dang (2008) combined two manual linguistic scores -grammaticality and focus -with various ROUGE/BE metrics, and showed this helps better predict the responsiveness of the summarizers.",
                "cite_spans": [
                    {
                        "start": 160,
                        "end": 180,
                        "text": "Pitler et al. (2010)",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 287,
                        "end": 299,
                        "text": "DUC 2006 and",
                        "ref_id": null
                    },
                    {
                        "start": 300,
                        "end": 304,
                        "text": "2007",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 446,
                        "end": 468,
                        "text": "Conroy and Dang (2008)",
                        "ref_id": "BIBREF1"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Since 2009, TAC introduced the task of Automatically Evaluating Summaries of Peers (AESOP). AESOP 2009 and 2010 focused on two summary qualities: content and overall responsiveness. Summary content is measured by comparing the output of an automatic metric with the manual Pyramid score. Overall responsiveness measures a combination of content and linguistic quality. In AESOP 2011 (Owczarzak and Dang, 2011) , automatic metrics are also evaluated for their ability to assess summary readability, i.e., to measure how linguistically readable a machine generated summary is. Submitted metrics that perform consistently well on the three aspects include Giannakopoulos and Karkaletsis (2011), Conroy et al. (2011), and de Oliveira (2011) . Giannakopoulos and Karkaletsis (2011) created two character-based n-gram graph representations for both the model and candidate summaries, and applied graph matching algorithm to assess their similarity. Conroy et al. (2011) extended the model in (Conroy and Dang, 2008) to include shallow linguistic features such as term overlap, redundancy, and term and sentence entropy. de Oliveira (2011) modeled the similarity between the model and candidate summaries as a maximum bipartite matching problem, where the two summaries are represented as two sets of nodes and precision and recall are cal- culated from the matched edges. However, none of the AESOP metrics currently apply deep linguistic analysis, which includes discourse analysis. Motivated by the parallels between summarization and MT evaluation, we will adapt a state-ofthe-art MT evaluation metric to measure summary content quality. To apply deep linguistic analysis, we also enhance an existing discourse coherence model to evaluate summary readability. We focus on metrics that measure the average quality of machine summarizers, i.e., metrics that can rank a set of machine summarizers correctly (human summarizers are not included in the list).",
                "cite_spans": [
                    {
                        "start": 383,
                        "end": 409,
                        "text": "(Owczarzak and Dang, 2011)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 692,
                        "end": 717,
                        "text": "Conroy et al. (2011), and",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 718,
                        "end": 736,
                        "text": "de Oliveira (2011)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 739,
                        "end": 776,
                        "text": "Giannakopoulos and Karkaletsis (2011)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 943,
                        "end": 963,
                        "text": "Conroy et al. (2011)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 986,
                        "end": 1009,
                        "text": "(Conroy and Dang, 2008)",
                        "ref_id": "BIBREF1"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "TESLA (Liu et al., 2010 ) is an MT evaluation metric which extends BLEU by introducing a linear programming-based framework for improved matching. It also makes use of linguistic resources and considers both precision and recall.",
                "cite_spans": [
                    {
                        "start": 6,
                        "end": 23,
                        "text": "(Liu et al., 2010",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "TESLA-S: Evaluating Summary Content",
                "sec_num": "3"
            },
            {
                "text": "Figure 1 shows the matching of bags of n-grams (BNGs) that forms the core of the TESLA metric. The top row in Figure 1a represents the bag of ngrams (BNG) from the model summary, and the bottom row represents the BNG from the candidate summary. Each n-gram has a weight. The links between the n-grams represent the similarity score, which are constrained to be between 0 and 1. Mathematically, TESLA takes as input the following:",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "1",
                        "ref_id": null
                    },
                    {
                        "start": 117,
                        "end": 119,
                        "text": "1a",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "The Linear Programming Matching Framework",
                "sec_num": "3.1"
            },
            {
                "text": "1. The BNG of the model summary, X, and the BNG of the candidate summary, Y . The ith entry in X is x i and has weight x W i (analogously for y i and y W i ). 2. A similarity score s(x i , y j ) between all ngrams x i and y j .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Linear Programming Matching Framework",
                "sec_num": "3.1"
            },
            {
                "text": "The goal of the matching process is to align the two BNGs so as to maximize the overall similarity. The variables of the problem are the allocated weights for the edges,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Linear Programming Matching Framework",
                "sec_num": "3.1"
            },
            {
                "text": "w(x i , y j ) \u2200i, j TESLA maximizes i,j s(x i , y j )w(x i , y j ) subject to w(x i , y j ) \u2265 0 \u2200i, j j w(x i , y j ) \u2264 x W i \u2200i i w(x i , y j ) \u2264 y W j \u2200j",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Linear Programming Matching Framework",
                "sec_num": "3.1"
            },
            {
                "text": "This real-valued linear programming problem can be solved efficiently. The overall similarity S is the value of the objective function. Thus,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Linear Programming Matching Framework",
                "sec_num": "3.1"
            },
            {
                "text": "Precision = S j y W j Recall = S i x W i",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Linear Programming Matching Framework",
                "sec_num": "3.1"
            },
            {
                "text": "The final TESLA score is given by the F-measure:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Linear Programming Matching Framework",
                "sec_num": "3.1"
            },
            {
                "text": "F = Precision \u00d7 Recall \u03b1 \u00d7 Precision + (1 -\u03b1) \u00d7 Recall",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Linear Programming Matching Framework",
                "sec_num": "3.1"
            },
            {
                "text": "In this work, we set \u03b1 = 0.8, following (Liu et al., 2010) . The score places more importance on recall than precision. When multiple model summaries are provided, TESLA matches the candidate BNG with each of the model BNGs. The maximum score is taken as the combined score.",
                "cite_spans": [
                    {
                        "start": 40,
                        "end": 58,
                        "text": "(Liu et al., 2010)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Linear Programming Matching Framework",
                "sec_num": "3.1"
            },
            {
                "text": "We adapted TESLA for the nuances of summarization. Mimicking ROUGE-SU4, we construct one matching problem between the unigrams and one between skip bigrams with a window size of four. The two F scores are averaged to give the final score.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "TESLA-S: TESLA for Summarization",
                "sec_num": "3.2"
            },
            {
                "text": "The similarity score s(x i , y j ) is 1 if the word surface forms of x i and y j are identical, and 0 otherwise. TESLA has a more sophisticated similarity measure that focuses on awarding partial scores for synonyms and parts of speech (POS) matches. However, the majority of current state-of-the-art summarization systems are extraction-based systems, which do not generate new words. Although our simplistic similarity score may be problematic when evaluating abstract-based systems, the experimental results support our choice of the similarity function. This reflects a major difference between MT and summarization evaluation: while MT systems always generate new sentences, most summarization systems focus on locating existing salient sentences.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "TESLA-S: TESLA for Summarization",
                "sec_num": "3.2"
            },
            {
                "text": "Like in TESLA, function words (words in closed POS categories, such as prepositions and articles) have their weights reduced by a factor of 0.1, thus placing more emphasis on the content words. We found this useful empirically.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "TESLA-S: TESLA for Summarization",
                "sec_num": "3.2"
            },
            {
                "text": "Koehn (2004) introduced a bootstrap resampling method to compute statistical significance of the difference between two machine translation systems with regard to the BLEU score. We adapt this method to compute the difference between two evaluation metrics in summarization:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Significance Test",
                "sec_num": "3.3"
            },
            {
                "text": "1. Randomly choose n topics from the n given topics with replacement. 2. Summarize the topics with the list of machine summarizers.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Significance Test",
                "sec_num": "3.3"
            },
            {
                "text": "Step 2 with the two evaluation metrics under comparison. 4. Determine which metric gives a higher correlation score. 5. Repeat Step 1 -4 for 1,000 times.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluate the list of summaries from",
                "sec_num": "3."
            },
            {
                "text": "As we have 44 topics in TAC 2011 summarization track, n = 44. The percentage of times metric a gives higher correlation than metric b is said to be the significance level at which a outperforms b. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluate the list of summaries from",
                "sec_num": "3."
            },
            {
                "text": "We test TESLA-S on the AESOP 2011 content evaluation task, judging the metric fitness by comparing its correlations with human judgments for content. The results for the initial and update tasks are reported in Table 1 . We show the three baselines (ROUGE-2, ROUGE-SU4, and BE) and submitted metrics with correlations among the top three scores, which are underlined. This setting remains the same for the rest of the experiments. We use three correlation measures: Pearson's r, Spearman's \u03c1, and Kendall's \u03c4 , represented by P, S, and K, respectively. The ROUGE scores are the recall scores, as per convention. On the initial task, TESLA-S outperforms all metrics on all three correlation measures. On the update task, TESLA-S ranks second, first, and second on Pearson's r, Spearman's \u03c1, and Kendall's \u03c4 , respectively.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 217,
                        "end": 218,
                        "text": "1",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "3.4"
            },
            {
                "text": "To test how significant the differences are, we perform significance testing using Koehn's resampling method between TESLA-S and ROUGE-2/ROUGE-SU4, on which TESLA-S is based. The findings are:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "3.4"
            },
            {
                "text": "\u2022 Initial task: TESLA-S is better than ROUGE-2 at 99% significance level as measured by Pearson's r. \u2022 Update task: TESLA-S is better than ROUGE-SU4 at 95% significance level as measured by Pearson's r. \u2022 All other differences are statistically insignificant, including all correlations on Spearman's \u03c1 and Kendall's \u03c4 .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "3.4"
            },
            {
                "text": "The last point can be explained by the fact that Spearman's \u03c1 and Kendall's \u03c4 are sensitive to only the system rankings, whereas Pearson's r is sensitive to the magnitude of the differences as well, hence Pearson's r is in general a more sensitive measure.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "3.4"
            },
            {
                "text": "Intuitively, a readable text should also be coherent, and an incoherent text will result in low readability. Both readability and coherence indicate how fluent a text is. We thus hypothesize that a model that measures how coherent a text is can also measure its readability. Lin et al. (2011) introduced discourse role matrix to represent discourse coherence of a text. W first illustrate their model with an example, and then introduce two new feature sources. We then apply the models and evaluate summary readability.",
                "cite_spans": [
                    {
                        "start": 275,
                        "end": 292,
                        "text": "Lin et al. (2011)",
                        "ref_id": "BIBREF10"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "DICOMER: Evaluating Summary Readability",
                "sec_num": "4"
            },
            {
                "text": "First, a free text in Figure 2 is parsed by a discourse parser to derive its discourse relations, which are shown in Figure 3 transitions as features and their probabilities as values.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 29,
                        "end": 30,
                        "text": "2",
                        "ref_id": null
                    },
                    {
                        "start": 124,
                        "end": 125,
                        "text": "3",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Lin et al.'s Discourse Coherence Model",
                "sec_num": "4.1"
            },
            {
                "text": "We observe that there are two kinds of information in Figure 3 that are not captured by Lin et al.'s model. The first one is whether a relation is Explicit or Non-Explicit (Lin et al. (2010) termed Non-Explicit to include Implicit, AltLex, EntRel, and NoRel). Explicit relation and Non-Explicit relation have different distributions on each discourse relation (PDTB-Group, 2007) . Thus, adding this information may further improve the model. In addition to the set of the discourse roles of \"Relation type . Argument tag\", we introduce another set of \"Explicit/Non-Explicit . Relation type . Argument tag\". The cell C cananea,S 3 now contains Comp.Arg2, Temp.Arg1, Exp.Arg1, E.Comp.Arg2, E.Temp.Arg1, and N.Exp.Arg1 (E for Explicit and N for Non-Explicit).",
                "cite_spans": [
                    {
                        "start": 172,
                        "end": 190,
                        "text": "(Lin et al. (2010)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 360,
                        "end": 378,
                        "text": "(PDTB-Group, 2007)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 61,
                        "end": 62,
                        "text": "3",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Two New Feature Sources",
                "sec_num": "4.2"
            },
            {
                "text": "The other information that is not in the discourse role matrix is the discourse hierarchy structure, i.e., whether one relation is embedded within another relation. In Figure 3 , S 3.1 is Arg1 of Explicit Temporal, which is Arg2 of the higher relation Explicit Comparison as well as Arg1 of another higher relation Implicit Expansion. These dependencies are important for us to know how well-structured a summary is. It is represented by the multiple discourse roles in each cell of the matrix. For example, the multiple discourse roles in the cell C cananea,S 3 capture the three dependencies just mentioned. We introduce intra-cell bigrams as a new set of features to the original model: for a cell with multiple discourse roles, we sort them by their surface strings and multiply to obtain the bigrams. For instance, C cananea,S 3 will produce bigrams such as Comp.Arg2\u2194Exp.Arg1 and Comp.Arg2\u2194Temp.Arg1.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 175,
                        "end": 176,
                        "text": "3",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Two New Feature Sources",
                "sec_num": "4.2"
            },
            {
                "text": "When both the Explicit/Non-Explicit feature source and the intra-cell feature source are joined together, it also produces bigram features such as E.Comp.Arg2\u2194Temp.Arg1.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Two New Feature Sources",
                "sec_num": "4.2"
            },
            {
                "text": "Lin et al. ( 2011) used the SVM light (Joachims, 1999) package with the preference ranking configuration. To train the model, each source text and one of its permutations form a training pair, where the source text is given a rank of 1 and the permutation is given 0. In testing, the trained model predicts a real number score for each instance, and the instance with the higher score in a pair is said to be the source text.",
                "cite_spans": [
                    {
                        "start": 38,
                        "end": 54,
                        "text": "(Joachims, 1999)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Predicting Readability Scores",
                "sec_num": "4.3"
            },
            {
                "text": "In the TAC summarization track, human judges scored each model and candidate summary with a readability score from 1 to 5 (5 means most readable). Thus in our setting, instead of a pair of texts, the training input consists of a list of model and candidate summaries from each topic, with their annotated scores as the rankings. Given an unseen test summary, the trained model predicts a real number score. This score essentially is the readability ranking of the test summary. Such ranking can be evaluated by the ranking-based correlations of Spearman's \u03c1 and Kendall's \u03c4 . As Pearson's r measures linear correlation and we do not know whether the real number score follows a linear function, we take the logarithm of this score as the readability score for this instance.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Predicting Readability Scores",
                "sec_num": "4.3"
            },
            {
                "text": "We use the data from AESOP 2009 and 2010 as the training data, and test our metrics on AESOP 2011 data. To obtain the discourse relations of a summary, we use the discourse parser2 developed in Lin et al. (2010) .",
                "cite_spans": [
                    {
                        "start": 194,
                        "end": 211,
                        "text": "Lin et al. (2010)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Predicting Readability Scores",
                "sec_num": "4.3"
            },
            {
                "text": "Table 3 shows the resulting readability correlations. The last four rows show the correlation scores for our coherence model: LIN is the default model by (Lin et al., 2011) , LIN+C is LIN with the intra-cell feature class, LIN+E is enhanced with the Explicit/Non-Explicit feature class. We name the LIN model with both new feature sources (i.e., LIN+C+E) DICOMER -a DIscourse COherence Model for Evaluating Readability.",
                "cite_spans": [
                    {
                        "start": 154,
                        "end": 172,
                        "text": "(Lin et al., 2011)",
                        "ref_id": "BIBREF10"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "3",
                        "ref_id": "TABREF5"
                    }
                ],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "4.4"
            },
            {
                "text": "LIN outperforms all metrics on all correlations on both tasks. On the initial task, it outperforms the best scores by 3.62%, 16.20%, and 12.95% on Pearson, Spearman, and Kendall, respectively. Similar gaps (4.27%, 18.52%, and 13.96%) are observed on the update task. The results are much better on Spearman and Kendall. This is because LIN is trained with a ranking model, and both Spearman and Kendall are ranking-based correlations.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "4.4"
            },
            {
                "text": "Adding either intra-cell or Explicit/Non-Explicit features improves all correlation scores, with Explicit/Non-Explicit giving more pronounced improvements. When both new feature sources are in- corporated into the metric, we obtain the best results for all correlation scores: DICOMER outperforms LIN by 1.10%, 5.29%, and 3.95% on the initial task, and 2.50%, 4.74%, and 4.27% on the update task.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "4.4"
            },
            {
                "text": "Table 3 shows that summarization evaluation Metric 4 tops all other AESOP metrics, except in the case of Spearman's \u03c1 on the initial task. We compare our four models to this metric. The results of Koehn's significance test are reported in Table 4 , which demonstrates that all four models outperform Metric 4 significantly. In the last row, we see that when comparing DICOMER to LIN, DICOMER is significantly better on three correlation measures.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "3",
                        "ref_id": "TABREF5"
                    },
                    {
                        "start": 245,
                        "end": 246,
                        "text": "4",
                        "ref_id": "TABREF6"
                    }
                ],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "4.4"
            },
            {
                "text": "With TESLA-S measuring content coverage and DI-COMER measuring readability, it is feasible to combine them to predict the overall responsiveness of a summary. There exist many ways to combine two variables mathematically: we can combine them in a linear function or polynomial function, or in a way similar to how precision and recall are combined in F measure. We applied a machine learning approach to train a regression model for measuring responsiveness. The scores predicted by TESLA-S and DICOMER are used as two features. We use SVM light with the regression configuration, testing three kernels: linear function, polynomial function, and radial basis function. We called this model CREMER -a Combined REgression Model for Evaluating Responsiveness.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "CREMER: Evaluating Overall Responsiveness",
                "sec_num": "5"
            },
            {
                "text": "We train the regression model on AESOP 2009 and 2010 data sets, and test it on AESOP 2011. The DICOMER model that is trained in Section 4 is used to predict the readability scores on all AESOP 2009, 2010, and 2011 summaries. We apply TESLA-S to predict content scores on all AESOP 2009, 2010, and 2011 summaries.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "CREMER: Evaluating Overall Responsiveness",
                "sec_num": "5"
            },
            {
                "text": "The last three rows in Table 5 show the correlation scores of our regression model trained with SVM linear function (LF), polynomial function (PF), and radial basis function (RBF). PF performs better than LF, suggesting that content and readability scores should not be linearly combined. RBF gives better performances than both LF and PF, suggesting that RBF better models the way humans combine content and readability. On the initial task, the model trained with RBF outperforms all submitted metrics. It outperforms the best correlation scores by 1.71%, 3.86%, and 4.60% on Pearson, Spearman, and Kendall, respectively. All three regression models do not perform as well on the update task. Koehn's significance test shows that when trained with RBF, CREMER outperforms ROUGE-2 and ROUGE-SU4 on the initial task at a significance level of 99% for all three correlation measures.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 29,
                        "end": 30,
                        "text": "5",
                        "ref_id": "TABREF7"
                    }
                ],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "5.1"
            },
            {
                "text": "The intuition behind the combined regression model is that combining the readability and content scores will give an overall good responsiveness score. The function to combine them and their weights can be obtained by training. While the results showed that SVM radial basis kernel gave the best performances, this function may not truly mimic how human evaluates responsiveness. Human judges were told to rate summaries by their overall qualities. They may take into account other aspects besides content and readability. Given CREMER did not perform well on the update task, we hypothesize that human judgment of update summaries may involve more complicated rankings or factor in additional input that CREMER currently does not model. We plan to devise a better responsiveness metric in our future work, beyond using a simple combination.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discussion",
                "sec_num": "6"
            },
            {
                "text": "Figure 4 shows a complete picture of Pearson's r for all AESOP 2011 metrics and our three metrics on both initial and update tasks. We highlight our metrics with a circle on these curves. On the initial task, correlation scores for content are consistently higher than those for responsiveness with small gaps, whereas on the update task, they are almost overlapping. On the other hand, correlation scores for readability are much lower than those for content and responsiveness, with a gap of about 0.2. Comparing Figure 4a and 4b, evaluation metrics always correlate better on the initial task than on the update task. This suggests that there is much room for improvement for readability metrics, and metrics need to consider update information when evaluating update summarizers.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "4",
                        "ref_id": "FIGREF1"
                    },
                    {
                        "start": 522,
                        "end": 524,
                        "text": "4a",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Discussion",
                "sec_num": "6"
            },
            {
                "text": "We proposed TESLA-S by adapting an MT evaluation metric to measure summary content coverage, and introduced DICOMER by applying a dis- course coherence model with newly introduced features to evaluate summary readability. We combined these two metrics in the CREMER metric -an SVM-trained regression model -for automatic summarization overall responsiveness evaluation. Experimental results on AESOP 2011 show that DICOMER significantly outperforms all submitted metrics on both initial and update tasks with large gaps, while TESLA-S and CREMER significantly outperform all metrics on the initial task.3 ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "7"
            },
            {
                "text": "http://wing.comp.nus.edu.sg/ \u02dclinzihen/ parser/",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Our metrics are publicly available at http://wing. comp.nus.edu.sg/ \u02dclinzihen/summeval/.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "This research is supported by the Singapore National Research Foundation under its International Research Centre @ Singapore Funding Initiative and administered by the IDM Programme Office.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgments",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Modeling local coherence: An entity-based approach",
                "authors": [
                    {
                        "first": "Regina",
                        "middle": [],
                        "last": "Barzilay",
                        "suffix": ""
                    },
                    {
                        "first": "Mirella",
                        "middle": [],
                        "last": "Lapata",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Computational Linguistics",
                "volume": "34",
                "issue": "",
                "pages": "1--34",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Regina Barzilay and Mirella Lapata. 2008. Modeling local coherence: An entity-based approach. Computa- tional Linguistics, 34:1-34, March.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Mind the gap: Dangers of divorcing evaluations of summary content from linguistic quality",
                "authors": [
                    {
                        "first": "John",
                        "middle": [
                            "M"
                        ],
                        "last": "Conroy",
                        "suffix": ""
                    },
                    {
                        "first": "Hoa",
                        "middle": [
                            "Trang"
                        ],
                        "last": "Dang",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Proceedings of the 22nd International Conference on Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "John M. Conroy and Hoa Trang Dang. 2008. Mind the gap: Dangers of divorcing evaluations of summary content from linguistic quality. In Proceedings of the 22nd International Conference on Computational Lin- guistics (Coling 2008), Manchester, UK, August.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "CLASSY 2011 at TAC: Guided and multi-lingual summaries and evaluation metrics",
                "authors": [
                    {
                        "first": "John",
                        "middle": [
                            "M"
                        ],
                        "last": "Conroy",
                        "suffix": ""
                    },
                    {
                        "first": "Judith",
                        "middle": [
                            "D"
                        ],
                        "last": "Schlesinger",
                        "suffix": ""
                    },
                    {
                        "first": "Jeff",
                        "middle": [],
                        "last": "Kubina",
                        "suffix": ""
                    },
                    {
                        "first": "Peter",
                        "middle": [
                            "A"
                        ],
                        "last": "Rankel",
                        "suffix": ""
                    },
                    {
                        "first": "Dianne",
                        "middle": [
                            "P"
                        ],
                        "last": "O'leary",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Proceedings of the Text Analysis Conference 2011 (TAC 2011)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "John M. Conroy, Judith D. Schlesinger, Jeff Kubina, Peter A. Rankel, and Dianne P. O'Leary. 2011. CLASSY 2011 at TAC: Guided and multi-lingual sum- maries and evaluation metrics. In Proceedings of the Text Analysis Conference 2011 (TAC 2011), Gaithers- burg, Maryland, USA, November.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "CatolicaSC at TAC 2011",
                "authors": [
                    {
                        "first": "C",
                        "middle": [
                            "F"
                        ],
                        "last": "Paulo",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "De Oliveira",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Proceedings of the Text Analysis Conference",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Paulo C. F. de Oliveira. 2011. CatolicaSC at TAC 2011. In Proceedings of the Text Analysis Conference (TAC 2011), Gaithersburg, Maryland, USA, November.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "AutoSummENG and MeMoG in evaluating guided summaries",
                "authors": [
                    {
                        "first": "George",
                        "middle": [],
                        "last": "Giannakopoulos",
                        "suffix": ""
                    },
                    {
                        "first": "Vangelis",
                        "middle": [],
                        "last": "Karkaletsis",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Proceedings of the Text Analysis Conference (TAC 2011)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "George Giannakopoulos and Vangelis Karkaletsis. 2011. AutoSummENG and MeMoG in evaluating guided summaries. In Proceedings of the Text Analysis Con- ference (TAC 2011), Gaithersburg, Maryland, USA, November.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Automated summarization evaluation with basic elements",
                "authors": [
                    {
                        "first": "Eduard",
                        "middle": [],
                        "last": "Hovy",
                        "suffix": ""
                    },
                    {
                        "first": "Chin-Yew",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Liang",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Junichi",
                        "middle": [],
                        "last": "Fukumoto",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Proceedings of the Fifth Conference on Language Resources and Evaluation",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Eduard Hovy, Chin-Yew Lin, Liang Zhou, and Junichi Fukumoto. 2006. Automated summarization evalua- tion with basic elements. In Proceedings of the Fifth Conference on Language Resources and Evaluation (LREC 2006).",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Making large-scale support vector machine learning practical",
                "authors": [
                    {
                        "first": "Thorsten",
                        "middle": [],
                        "last": "Joachims",
                        "suffix": ""
                    }
                ],
                "year": 1999,
                "venue": "Advances in Kernel Methods -Support Vector Learning",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Thorsten Joachims. 1999. Making large-scale sup- port vector machine learning practical. In Bernhard Schlkopf, Christopher J. C. Burges, and Alexander J. Smola, editors, Advances in Kernel Methods -Support Vector Learning. MIT Press, Cambridge, MA, USA.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "A Data-Driven Methodology for Motivating a Set of Coherence Relations",
                "authors": [
                    {
                        "first": "Alistair",
                        "middle": [],
                        "last": "Knott",
                        "suffix": ""
                    }
                ],
                "year": 1996,
                "venue": "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alistair Knott. 1996. A Data-Driven Methodology for Motivating a Set of Coherence Relations. Ph.D. the- sis, Department of Artificial Intelligence, University of Edinburgh. Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP 2004).",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Automatic evaluation of summaries using n-gram co-occurrence statistics",
                "authors": [
                    {
                        "first": "Chin-Yew",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Eduard",
                        "middle": [],
                        "last": "Hovy",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology (NAACL 2003)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chin-Yew Lin and Eduard Hovy. 2003. Automatic evalu- ation of summaries using n-gram co-occurrence statis- tics. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Com- putational Linguistics on Human Language Technol- ogy (NAACL 2003), Morristown, NJ, USA.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "A PDTB-styled end-to-end discourse parser",
                "authors": [
                    {
                        "first": "Ziheng",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Hwee Tou",
                        "middle": [],
                        "last": "Ng",
                        "suffix": ""
                    },
                    {
                        "first": "Min-Yen",
                        "middle": [],
                        "last": "Kan",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "School of Computing, National University of Singapore",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2010. A PDTB-styled end-to-end discourse parser. Technical Report TRB8/10, School of Computing, National Uni- versity of Singapore, August.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Automatically evaluating text coherence using discourse relations",
                "authors": [
                    {
                        "first": "Ziheng",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Hwee Tou",
                        "middle": [],
                        "last": "Ng",
                        "suffix": ""
                    },
                    {
                        "first": "Min-Yen",
                        "middle": [],
                        "last": "Kan",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT 2011)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011. Au- tomatically evaluating text coherence using discourse relations. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Hu- man Language Technologies (ACL-HLT 2011), Port- land, Oregon, USA, June.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "TESLA: Translation evaluation of sentences with linear-programming-based analysis",
                "authors": [
                    {
                        "first": "Chang",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Dahlmeier",
                        "suffix": ""
                    },
                    {
                        "first": "Hwee Tou",
                        "middle": [],
                        "last": "Ng",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng. 2010. TESLA: Translation evaluation of sentences with linear-programming-based analysis. In Proceed- ings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, Uppsala, Sweden. As- sociation for Computational Linguistics.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Evaluating content selection in summarization: The pyramid method",
                "authors": [
                    {
                        "first": "Ani",
                        "middle": [],
                        "last": "Nenkova",
                        "suffix": ""
                    },
                    {
                        "first": "Rebecca",
                        "middle": [],
                        "last": "Passonneau",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proceedings of the 2004 Human Language Technology Conference / North American Chapter of the Association for Computational Linguistics Annual Meeting (HLT-NAACL 2004)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ani Nenkova and Rebecca Passonneau. 2004. Evaluat- ing content selection in summarization: The pyramid method. In Proceedings of the 2004 Human Language Technology Conference / North American Chapter of the Association for Computational Linguistics Annual Meeting (HLT-NAACL 2004), Boston, Massachusetts, USA, May.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "SWING: Exploiting category-specific information for guided summarization",
                "authors": [
                    {
                        "first": "Ping",
                        "middle": [],
                        "last": "Jun",
                        "suffix": ""
                    },
                    {
                        "first": "Praveen",
                        "middle": [],
                        "last": "Ng",
                        "suffix": ""
                    },
                    {
                        "first": "Ziheng",
                        "middle": [],
                        "last": "Bysani",
                        "suffix": ""
                    },
                    {
                        "first": "Min-Yen",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Chew",
                        "middle": [],
                        "last": "Kan",
                        "suffix": ""
                    },
                    {
                        "first": "Tan",
                        "middle": [],
                        "last": "Lim",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Proceedings of the Text Analysis Conference 2011 (TAC 2011)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jun Ping Ng, Praveen Bysani, Ziheng Lin, Min-Yen Kan, and Chew Lim Tan. 2011. SWING: Exploit- ing category-specific information for guided summa- rization. In Proceedings of the Text Analysis Confer- ence 2011 (TAC 2011), Gaithersburg, Maryland, USA, November.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Overview of the TAC 2011 summarization track: Guided task and AESOP task",
                "authors": [
                    {
                        "first": "Karolina",
                        "middle": [],
                        "last": "Owczarzak",
                        "suffix": ""
                    },
                    {
                        "first": "Hoa",
                        "middle": [
                            "Trang"
                        ],
                        "last": "Dang",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Proceedings of the Text Analysis Conference (TAC 2011)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Karolina Owczarzak and Hoa Trang Dang. 2011. Overview of the TAC 2011 summarization track: Guided task and AESOP task. In Proceedings of the Text Analysis Conference (TAC 2011), Gaithersburg, Maryland, USA, November.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "BLEU: a method for automatic evaluation of machine translation",
                "authors": [
                    {
                        "first": "Kishore",
                        "middle": [],
                        "last": "Papineni",
                        "suffix": ""
                    },
                    {
                        "first": "Salim",
                        "middle": [],
                        "last": "Roukos",
                        "suffix": ""
                    },
                    {
                        "first": "Todd",
                        "middle": [],
                        "last": "Ward",
                        "suffix": ""
                    },
                    {
                        "first": "Wei-Jing",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL 2002)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. BLEU: a method for automatic eval- uation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computa- tional Linguistics (ACL 2002), Stroudsburg, PA, USA.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "The Penn Discourse Treebank 2.0 Annotation Manual. The PDTB Research Group",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Pdtb-Group",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "PDTB-Group, 2007. The Penn Discourse Treebank 2.0 Annotation Manual. The PDTB Research Group.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Automatic evaluation of linguistic quality in multidocument summarization",
                "authors": [
                    {
                        "first": "Emily",
                        "middle": [],
                        "last": "Pitler",
                        "suffix": ""
                    },
                    {
                        "first": "Annie",
                        "middle": [],
                        "last": "Louis",
                        "suffix": ""
                    },
                    {
                        "first": "Ani",
                        "middle": [],
                        "last": "Nenkova",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL 2010)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Emily Pitler, Annie Louis, and Ani Nenkova. 2010. Automatic evaluation of linguistic quality in multi- document summarization. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL 2010), Stroudsburg, PA, USA.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Guided summarization with aspect recognition",
                "authors": [
                    {
                        "first": "Renxian",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "You",
                        "middle": [],
                        "last": "Ouyang",
                        "suffix": ""
                    },
                    {
                        "first": "Wenjie",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Proceedings of the Text Analysis Conference 2011 (TAC 2011)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Renxian Zhang, You Ouyang, and Wenjie Li. 2011. Guided summarization with aspect recognition. In Proceedings of the Text Analysis Conference 2011 (TAC 2011), Gaithersburg, Maryland, USA, Novem- ber.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Paraeval: Using paraphrases to evaluate summaries automatically",
                "authors": [
                    {
                        "first": "Liang",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Chin-Yew",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Dragos",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Stefan",
                        "middle": [],
                        "last": "Munteanu",
                        "suffix": ""
                    },
                    {
                        "first": "Eduard",
                        "middle": [],
                        "last": "Hovy",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL 2006)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Liang Zhou, Chin-Yew Lin, Dragos Stefan Munteanu, and Eduard Hovy. 2006. Paraeval: Using paraphrases to evaluate summaries automatically. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Com- putational Linguistics (HLT-NAACL 2006), Strouds- burg, PA, USA.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Evaluation metric values on the update task.",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Figure 4: Pearson's r for all AESOP 2011 submitted metrics and our proposed metrics. Our metrics are circled. Higher r value is better.",
                "uris": null,
                "fig_num": "4",
                "type_str": "figure"
            },
            "TABREF1": {
                "content": "<table><tr><td/><td/><td>Initial</td><td/><td/><td>Update</td></tr><tr><td/><td>P</td><td>S</td><td>K</td><td>P</td><td>S</td><td>K</td></tr><tr><td>R-2</td><td colspan=\"6\">0.9606 0.8943 0.7450 0.9029 0.8024 0.6323</td></tr><tr><td>R-SU4</td><td colspan=\"6\">0.9806 0.8935 0.7371 0.8847 0.8382 0.6654</td></tr><tr><td>BE</td><td colspan=\"6\">0.9388 0.9030 0.7456 0.9057 0.8385 0.6843</td></tr><tr><td>4</td><td colspan=\"6\">0.9672 0.9017 0.7351 0.8249 0.8035 0.6070</td></tr><tr><td>6</td><td colspan=\"6\">0.9678 0.8816 0.7229 0.9107 0.8370 0.6606</td></tr><tr><td>8</td><td colspan=\"6\">0.9555 0.8686 0.7024 0.8981 0.8251 0.6606</td></tr><tr><td>10</td><td colspan=\"6\">0.9501 0.8973 0.7550 0.7680 0.7149 0.5504</td></tr><tr><td>11</td><td colspan=\"6\">0.9617 0.8937 0.7450 0.9037 0.8018 0.6291</td></tr><tr><td>12</td><td colspan=\"6\">0.9739 0.8972 0.7466 0.8559 0.8249 0.6402</td></tr><tr><td>13</td><td colspan=\"6\">0.9648 0.9033 0.7582 0.8842 0.7961 0.6276</td></tr><tr><td>24</td><td colspan=\"6\">0.9509 0.8997 0.7535 0.8115 0.8199 0.6386</td></tr><tr><td colspan=\"7\">TESLA-S 0.9807 0.9173 0.7734 0.9072 0.8457 0.6811</td></tr></table>",
                "type_str": "table",
                "text": "Content correlation with human judgment on summarizer level. Top three scores among AE-SOP metrics are underlined. The TESLA-S score is bolded when it outperforms all others. ROUGE-2 is shortened to R-2 and ROUGE-SU4 to R-SU4.",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table><tr><td/><td colspan=\"2\">S 2 Comparison Implicit S 1 S 1</td><td colspan=\"3\">S 3.1 Explicit Comparison Explicit Temporal S 3.2</td><td>S 4.1 Implicit Expansion</td><td>S 4.2 Explicit Expansion S 4.3</td></tr><tr><td/><td colspan=\"7\">Figure 3: The discourse relations for Figure 2. Ar-</td></tr><tr><td/><td colspan=\"6\">rows are pointing from Arg2 to Arg1.</td></tr><tr><td>. Lin et al. observed that</td><td>S#</td><td colspan=\"2\">copper</td><td>cananea</td><td colspan=\"2\">Terms operat</td><td>depend</td><td>. . .</td></tr><tr><td>coherent texts preferentially follow certain relation</td><td>S 1</td><td>nil</td><td/><td>Comp.Arg1</td><td/><td>nil</td><td>Comp.Arg1</td></tr><tr><td>patterns. However, simply using such patterns to measure the coherence of a text can result in fea-</td><td>S 2</td><td colspan=\"2\">Comp.Arg2 Comp.Arg1</td><td colspan=\"3\">nil Comp.Arg2 Comp.Arg2 nil</td><td>nil</td></tr><tr><td>ture sparseness. To solve this problem, they expand the relation sequence into a discourse role matrix, as shown in Table 2. The matrix essentially cap-</td><td>S 3 S 4</td><td>nil nil</td><td/><td colspan=\"3\">Temp.Arg1 Temp.Arg1 Exp.Arg1 Exp.Arg1 Exp.Arg2 Exp.Arg1 Exp.Arg2</td><td>nil nil</td></tr><tr><td>tures term occurrences in the sentence-to-sentence</td><td/><td/><td/><td/><td/><td/></tr><tr><td>relation sequences. This model is motivated by</td><td/><td/><td/><td/><td/><td/></tr><tr><td>the entity-based model (Barzilay and Lapata, 2008)</td><td/><td/><td/><td/><td/><td/></tr><tr><td>which captures sentence-to-sentence entity transi-</td><td/><td/><td/><td/><td/><td/></tr><tr><td>tions. Next, the discourse role transition probabili-</td><td/><td/><td/><td/><td/><td/></tr><tr><td>ties of lengths 2 and 3 (e.g., Temp.Arg1\u2192Exp.Arg2</td><td/><td/><td/><td/><td/><td/></tr><tr><td>and Comp.Arg1\u2192nil\u2192Temp.Arg1) are calculated</td><td/><td/><td/><td/><td/><td/></tr><tr><td>with respect to the matrix. For example, the prob-</td><td/><td/><td/><td/><td/><td/></tr><tr><td>ability of Comp.Arg2\u2192Exp.Arg2 is 2/25 = 0.08 in</td><td/><td/><td/><td/><td/><td/></tr><tr><td>Table 2.</td><td/><td/><td/><td/><td/><td/></tr><tr><td>Lin et al. applied their model on the task of dis-</td><td/><td/><td/><td/><td/><td/></tr><tr><td>cerning an original text from a permuted ordering of</td><td/><td/><td/><td/><td/><td/></tr><tr><td>its sentences. They modeled it as a pairwise rank-</td><td/><td/><td/><td/><td/><td/></tr><tr><td>ing model (i.e., original vs. permuted), and trained a</td><td/><td/><td/><td/><td/><td/></tr><tr><td>SVM preference ranking model with discourse role</td><td/><td/><td/><td/><td/><td/></tr></table>",
                "type_str": "table",
                "text": "Japan normally depends heavily on the Highland Valley and Cananea mines as well as the Bougainville mine in Papua New Guinea. S 2 Recently, Japan has been buying copper elsewhere. S 3.1 But as Highland Valley and Cananea begin operating, S 3.2 they are expected to resume their roles as Japan's suppliers. S 4.1 According to Fred Demler, metals economist for Drexel Burnham Lambert, New York, S 4.2 \"Highland Valley has already started operating S 4.3 and Cananea is expected to do so soon.\"Figure2: A text with four sentences. S i.j means the jth clause in the ith sentence.",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Discourse role matrix fragment extracted from Figure 2 and 3. Rows correspond to sentences, columns to stemmed terms, and cells contain extracted discourse roles. Temporal, Contingency, Comparison, and Expansion are shortened to Temp, Cont, Comp, and Exp, respectively.",
                "html": null,
                "num": null
            },
            "TABREF5": {
                "content": "<table><tr><td/><td/><td/><td>Initial</td><td/><td/><td>Update</td></tr><tr><td/><td>vs.</td><td>P</td><td>S</td><td>K</td><td>P</td><td>S</td><td>K</td></tr><tr><td>LIN</td><td/><td>*</td><td colspan=\"5\">*  *   *  *   *  *   *  *   *  *</td></tr><tr><td>LIN+C LIN+E</td><td>4</td><td colspan=\"6\">*  *   *  *   *  *   *  *   *  *   *  *   *  *   *  *   *  *   *   *  *   *  *</td></tr><tr><td>DICOMER</td><td/><td colspan=\"6\">*  *   *  *   *  *   *  *   *  *   *  *</td></tr><tr><td colspan=\"2\">DICOMER LIN</td><td>-</td><td>*</td><td>*</td><td>*</td><td>-</td><td>-</td></tr></table>",
                "type_str": "table",
                "text": "Readability correlation with human judgment on summarizer level. Top three scores among AESOP metrics are underlined. Our score is bolded when it outperforms all AESOP metrics.",
                "html": null,
                "num": null
            },
            "TABREF6": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Koehn's significance test for readability. * * , * , and -indicate significance level >=99%, >=95%, and <95%, respectively.",
                "html": null,
                "num": null
            },
            "TABREF7": {
                "content": "<table><tr><td/><td/><td>Initial</td><td/><td/><td>Update</td></tr><tr><td/><td>P</td><td>S</td><td>K</td><td>P</td><td>S</td><td>K</td></tr><tr><td>R-2</td><td colspan=\"6\">0.9416 0.7897 0.6096 0.9169 0.8401 0.6778</td></tr><tr><td>R-SU4</td><td colspan=\"6\">0.9545 0.7902 0.6017 0.9123 0.8758 0.7065</td></tr><tr><td>BE</td><td colspan=\"6\">0.9155 0.7683 0.5673 0.8755 0.7964 0.6254</td></tr><tr><td>4</td><td colspan=\"6\">0.9498 0.8372 0.6662 0.8706 0.8674 0.7033</td></tr><tr><td>6</td><td colspan=\"6\">0.9512 0.7955 0.6112 0.9271 0.8769 0.7160</td></tr><tr><td>11</td><td colspan=\"6\">0.9427 0.7873 0.6064 0.9194 0.8432 0.6794</td></tr><tr><td>12</td><td colspan=\"6\">0.9469 0.8450 0.6746 0.8728 0.8611 0.6858</td></tr><tr><td>18</td><td colspan=\"6\">0.9480 0.8447 0.6715 0.8912 0.8377 0.6683</td></tr><tr><td>23</td><td colspan=\"6\">0.9317 0.7952 0.6080 0.9192 0.8664 0.6953</td></tr><tr><td>25</td><td colspan=\"6\">0.9512 0.7899 0.6033 0.9033 0.8139 0.6349</td></tr><tr><td>CREMER LF</td><td colspan=\"6\">0.9381 0.8346 0.6635 0.8280 0.6860 0.5173</td></tr><tr><td>CREMER P F</td><td colspan=\"6\">0.9621 0.8567 0.6921 0.8852 0.7863 0.6159</td></tr><tr><td colspan=\"7\">CREMER RBF 0.9716 0.8836 0.7206 0.9018 0.8285 0.6588</td></tr></table>",
                "type_str": "table",
                "text": "Responsiveness correlation with human judgment on summarizer level. Top three scores among AESOP metrics are underlined. CREMER score is bolded when it outperforms all AESOP metrics.",
                "html": null,
                "num": null
            }
        }
    }
}