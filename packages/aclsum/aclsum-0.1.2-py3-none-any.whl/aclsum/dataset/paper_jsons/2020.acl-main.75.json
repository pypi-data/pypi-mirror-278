{
    "paper_id": "2020",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T11:55:06.044042Z"
    },
    "title": "\"The Boating Store Had Its Best Sail Ever\": Pronunciation-attentive Contextualized Pun Recognition",
    "authors": [
        {
            "first": "Yichao",
            "middle": [],
            "last": "Zhou",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of California",
                "location": {
                    "settlement": "Los Angeles"
                }
            },
            "email": ""
        },
        {
            "first": "Jyun-Yu",
            "middle": [],
            "last": "Jiang",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of California",
                "location": {
                    "settlement": "Los Angeles"
                }
            },
            "email": ""
        },
        {
            "first": "Jieyu",
            "middle": [],
            "last": "Zhao",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of California",
                "location": {
                    "settlement": "Los Angeles"
                }
            },
            "email": "jyzhao@cs.ucla.edu"
        },
        {
            "first": "Kai-Wei",
            "middle": [],
            "last": "Chang",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of California",
                "location": {
                    "settlement": "Los Angeles"
                }
            },
            "email": "kwchang@cs.ucla.edu"
        },
        {
            "first": "Wei",
            "middle": [],
            "last": "Wang",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of California",
                "location": {
                    "settlement": "Los Angeles"
                }
            },
            "email": "weiwang@cs.ucla.edu"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Humor plays an important role in human languages and it is essential to model humor when building intelligence systems. Among different forms of humor, puns perform wordplay for humorous effects by employing words with double entendre and high phonetic similarity. However, identifying and modeling puns are challenging as puns usually involved implicit semantic or phonological tricks. In this paper, we propose Pronunciation-attentive Contextualized Pun Recognition (PCPR) to perceive human humor, detect if a sentence contains puns and locate them in the sentence. PCPR derives contextualized representation for each word in a sentence by capturing the association between the surrounding context and its corresponding phonetic symbols. Extensive experiments are conducted on two benchmark datasets. Results demonstrate that the proposed approach significantly outperforms the state-of-the-art methods in pun detection and location tasks. In-depth analyses verify the effectiveness and robustness of PCPR.",
    "pdf_parse": {
        "paper_id": "2020",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Humor plays an important role in human languages and it is essential to model humor when building intelligence systems. Among different forms of humor, puns perform wordplay for humorous effects by employing words with double entendre and high phonetic similarity. However, identifying and modeling puns are challenging as puns usually involved implicit semantic or phonological tricks. In this paper, we propose Pronunciation-attentive Contextualized Pun Recognition (PCPR) to perceive human humor, detect if a sentence contains puns and locate them in the sentence. PCPR derives contextualized representation for each word in a sentence by capturing the association between the surrounding context and its corresponding phonetic symbols. Extensive experiments are conducted on two benchmark datasets. Results demonstrate that the proposed approach significantly outperforms the state-of-the-art methods in pun detection and location tasks. In-depth analyses verify the effectiveness and robustness of PCPR.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "During the last decades, social media has promoted the creation of a vast amount of humorous web contents (Nijholt et al., 2017) . Automatic recognition of humor has become an important task in the area of figurative language processing, which can benefit various downstream NLP applications such as dialogue systems, sentiment analysis, and machine translation (Melby and Warner, 1995; Augello et al., 2008; Ghosh et al., 2015; Bertero and Fung, 2016; Blinov et al., 2019) . However, humor is one of the most complicated behaviors in natural language semantics and sometimes it is even difficult for humans to interpret. In most cases, understanding humor requires adequate background knowledge and a rich context. Puns are a form of humorous approaches using the different meanings of identical words or words with similar pronunciations to explain texts or utterances. There are two main types of puns. Homographic puns rely on multiple interpretations of the same word. As shown in Table 1 , the phrase all right means good condition or opposite to left; the word reaction means chemical change or action. The two meanings of the same expression are consistent with its context, which creates a humorous pun in both sentences when there is a clear contrast between two meanings. On the other hand, heterographic puns take advantage of phonologically same or similar words. For example, the word pairs sale and sail, weak and week in Table 1 have the same or similar pronunciations. The sentences are funny because both words fit the same context. Understanding puns is a big fish to fry for deep comprehension of complex semantics.",
                "cite_spans": [
                    {
                        "start": 106,
                        "end": 128,
                        "text": "(Nijholt et al., 2017)",
                        "ref_id": "BIBREF31"
                    },
                    {
                        "start": 362,
                        "end": 386,
                        "text": "(Melby and Warner, 1995;",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 387,
                        "end": 408,
                        "text": "Augello et al., 2008;",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 409,
                        "end": 428,
                        "text": "Ghosh et al., 2015;",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 429,
                        "end": 452,
                        "text": "Bertero and Fung, 2016;",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 453,
                        "end": 473,
                        "text": "Blinov et al., 2019)",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 992,
                        "end": 993,
                        "text": "1",
                        "ref_id": "TABREF0"
                    },
                    {
                        "start": 1443,
                        "end": 1444,
                        "text": "1",
                        "ref_id": "TABREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "These two forms of puns have been studied in literature from different angles. To recognize puns in a sentence, word sense disambiguation techniques (WSD) (Navigli, 2009) have been employed to identify the equitable intention of words in utterances (Pedersen, 2017) . External knowledge bases such as WordNet (Miller, 1998b) have been applied in determining word senses of pun words (Oele and Evang, 2017) . However, these methods cannot tackle heterographic puns with distinct word spellings and knowledge bases that only contain a limited vocabulary. To resolve the issues of sparseness and heterographics, the word embedding techniques (Mikolov et al., 2013; Pennington et al., 2014) provide flexible representations to model puns (Hurtado et al., 2017; Indurthi and Oota, 2017; Cai et al., 2018) . However, a word may have different meanings regarding its contexts. Especially, an infrequent meaning of the word might be utilized for creating a pun. Therefore, static word embeddings are insufficient to represent words. In addition, some puns are created by replacing a word with another word with the same or similar pronunciation as examples shown in Table 1 . Therefore, to recognize puns, it is essential to model the association between words in the sentence and the pronunciation of words. Despite existing approaches attempt to leverage phonological structures to understand puns (Doogan et al., 2017; Jaech et al., 2016) , there is a lack of a general framework to model these two types of signals in a whole.",
                "cite_spans": [
                    {
                        "start": 155,
                        "end": 170,
                        "text": "(Navigli, 2009)",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 249,
                        "end": 265,
                        "text": "(Pedersen, 2017)",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 309,
                        "end": 324,
                        "text": "(Miller, 1998b)",
                        "ref_id": null
                    },
                    {
                        "start": 383,
                        "end": 405,
                        "text": "(Oele and Evang, 2017)",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 639,
                        "end": 661,
                        "text": "(Mikolov et al., 2013;",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 662,
                        "end": 686,
                        "text": "Pennington et al., 2014)",
                        "ref_id": "BIBREF34"
                    },
                    {
                        "start": 734,
                        "end": 756,
                        "text": "(Hurtado et al., 2017;",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 757,
                        "end": 781,
                        "text": "Indurthi and Oota, 2017;",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 782,
                        "end": 799,
                        "text": "Cai et al., 2018)",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 1392,
                        "end": 1413,
                        "text": "(Doogan et al., 2017;",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 1414,
                        "end": 1433,
                        "text": "Jaech et al., 2016)",
                        "ref_id": "BIBREF15"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 1164,
                        "end": 1165,
                        "text": "1",
                        "ref_id": "TABREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this paper, we propose Pronunciation-attentive Contextualized Pun Recognition (PCPR) to jointly model the contextualized word embeddings and phonological word representations for pun recognition. To capture the phonological structures of words, we break each word into a sequence of phonemes as its pronunciation so that homophones can have similar phoneme sets. For instance, the phonemes of the word pun are {P, AH, N}. In PCPR, we construct a pronunciation attentive module to identify important phonemes of each word, which can be applied in other tasks related to phonology. We jointly encode the contextual and phonological features into a self-attentive embedding to tackle both pun detection and location tasks. We summarize our contributions as following.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\u2022 To the best of our knowledge, PCPR is the first work to jointly model contextualized word embeddings and pronunciation embeddings to recognize puns. Both contexts and phonological properties are beneficial to pun recognition. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Pun Recognition and Generation To recognize puns, Miller et al. (2017) summarize several systems for the SemEval 2017 tasks. To detect the pun, Pedersen (2017) supposes that if there is one pun in the sentence, when adopting different Word Sense Disambiguation (WSD) methods, the sense assigned to the sentence will be different. To locate the pun, based on the WSD results for pun detection, they choose the last word which changes the senses between different WSD runs. Even though this method can tackle both homographic and heterographic pun detection, it does not use any pretrained embedding model. Xiu et al. (2017) detect the pun in the sentence using similarity features which are calculated on sense vectors or cluster center vectors. To locate the pun, they use an unsupervised system by scoring each word in the sentence and choosing the word with the smallest score. However, this model exclusively relies on semantics to detect the heterographic puns but ignores the rich information embedded in the pronunciations. Doogan et al. ( 2017) leverage word embeddings as well as the phonetic information by concatenating pronunciation strings, but the concatenation has limited expression ability. They also mention that their systems suffer for short sentences as word embeddings do not have much context information. Besides, Zou and Lu (2019) jointly detect and locate the pun from a sequence labeling perspective by employing a new tagging schema. Diao et al. (2018) expand word embeddings using WordNet to settle the polysemy of homographic puns, following by a neural attention mechanism to extract the collocation to detect the homographic pun. However, all these methods only make use of limited context information. Other than the pun recognition, Yu et al. (2018) generate homographic puns without requiring any pun data for training. He et al. (2019) improve the homographic pun generation based on the \"local-global surprisal principle\" which posits that the pun word and the alternative word have a strong association with the distant and immediate context respectively. Pronunciation Embeddings Word embeddings assign each word with a vector so that words with similar semantic meanings are close in the embedding space. Most word embedding models only make use of text information and omitting the rich information contained in the pronunciation. How-ever, the pronunciation is also an important part of the language (Zhu et al., 2018) . Prior studies have demonstrated that the phonetic information can be used in speech recognition (Bengio and Heigold, 2014) , spell correction (Toutanova and Moore, 2002) and speech synthesis (Miller, 1998a) . By projecting to the embedding space, words sound alike are nearby to each other (Bengio and Heigold, 2014) . Furthermore, Kamper et al. (2016) make use of word pairs information to improve the acoustic word embedding. Zhu et al. (2018) show that combining the pronunciation with the writing texts can help to improve the performance of word embeddings. However, these pronunciation embeddings are word-level features, while in our approach, we make use of syllabic pronunciations which is phoneme-level and could help with the out-of-vocabulary (OOV) situation. Luo et al. (2019) also propose an adversarial generative network for pun generation, which does not require any pun corpus. Contextualized Word Embeddings Traditional word embeddings assign a fixed vector to one word even if the word has multiple meanings under different contexts (e.g., \"the river bank\" v.s. \"the commercial bank\"). McCann et al. (2017) combine the pivot word embeddings as well as the contextual embeddings generated by an encoder from a supervised neural machine translation task. Peters et al. (2017) enrich the word embeddings by the contextual information extracted from a bidirectional language model. (Devlin et al., 2018) learn the language embedding by stacking multiple transformer layers with masked language model objective which advances the state-of-the-art for many NLP tasks. Yang et al. (2019) enable learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and solve the problem of pretrain-finetune discrepancy.",
                "cite_spans": [
                    {
                        "start": 50,
                        "end": 70,
                        "text": "Miller et al. (2017)",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 144,
                        "end": 159,
                        "text": "Pedersen (2017)",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 605,
                        "end": 622,
                        "text": "Xiu et al. (2017)",
                        "ref_id": "BIBREF47"
                    },
                    {
                        "start": 1337,
                        "end": 1354,
                        "text": "Zou and Lu (2019)",
                        "ref_id": "BIBREF52"
                    },
                    {
                        "start": 1461,
                        "end": 1479,
                        "text": "Diao et al. (2018)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 1766,
                        "end": 1782,
                        "text": "Yu et al. (2018)",
                        "ref_id": "BIBREF50"
                    },
                    {
                        "start": 1854,
                        "end": 1870,
                        "text": "He et al. (2019)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 2441,
                        "end": 2459,
                        "text": "(Zhu et al., 2018)",
                        "ref_id": "BIBREF51"
                    },
                    {
                        "start": 2558,
                        "end": 2584,
                        "text": "(Bengio and Heigold, 2014)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 2604,
                        "end": 2631,
                        "text": "(Toutanova and Moore, 2002)",
                        "ref_id": "BIBREF42"
                    },
                    {
                        "start": 2653,
                        "end": 2668,
                        "text": "(Miller, 1998a)",
                        "ref_id": null
                    },
                    {
                        "start": 2752,
                        "end": 2778,
                        "text": "(Bengio and Heigold, 2014)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 2794,
                        "end": 2814,
                        "text": "Kamper et al. (2016)",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 2890,
                        "end": 2907,
                        "text": "Zhu et al. (2018)",
                        "ref_id": "BIBREF51"
                    },
                    {
                        "start": 3234,
                        "end": 3251,
                        "text": "Luo et al. (2019)",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 3568,
                        "end": 3588,
                        "text": "McCann et al. (2017)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 3735,
                        "end": 3755,
                        "text": "Peters et al. (2017)",
                        "ref_id": "BIBREF36"
                    },
                    {
                        "start": 3860,
                        "end": 3881,
                        "text": "(Devlin et al., 2018)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 4044,
                        "end": 4062,
                        "text": "Yang et al. (2019)",
                        "ref_id": "BIBREF49"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "In this section, we first formally define the problem and then introduce the proposed method, PCPR.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Pronunciation-attentive Contextualized Pun Recognition",
                "sec_num": "3"
            },
            {
                "text": "Suppose the input text consists of a sequence of N words {w 1 , w 2 , \u2022 \u2022 \u2022 , w N }. For each word w i with M i phonemes in its pronunciation, the phonemes are denoted as R(w",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Problem Statement",
                "sec_num": "3.1"
            },
            {
                "text": "i ) = {r i,1 , r i,2 , \u2022 \u2022 \u2022 , r i,M i },",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Problem Statement",
                "sec_num": "3.1"
            },
            {
                "text": "where r i,j is the j-th phoneme in the pronunciation of w i . These phonemes are given by a dictionary. In this paper, we aim to recognize potential puns in the text with two tasks, including pun detection and pun location, as described in the following. Task 1: Pun Detection. The pun detection task identifies whether a sentence contains a pun. Formally, the task is modeled as a classification problem with binary label y D . Task 2: Pun Location. Given a sentence containing at least a pun, the pun location task aims to unearth the pun word. More precisely, for each word w i , we would like to predict a binary label y L i that indicates if w i is a pun word.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Problem Statement",
                "sec_num": "3.1"
            },
            {
                "text": "In addition to independently solving the above two tasks, the ultimate goal of pun recognition is to build a pipeline from scratch to detect and then locate the puns in texts. Hence, we also evaluate the end-to-end performance by aggregating the solutions for two tasks.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Problem Statement",
                "sec_num": "3.1"
            },
            {
                "text": "Figure 1 shows the overall framework of the proposed Pronunciation-attentive Contextualized Pun Recognition (PCPR). For each word in the input text, we first derive two continuous vectors, including contextualized word embedding and pronunciation embedding, as representations in different aspects. Contextualized word embeddings derive appropriate word representations with consideration of context words and capture the accurate semantics in the text. To learn the phonological characteristics, each word is divided into phonemes while each phoneme is projected to a phoneme embedding space, thereby obtaining pronunciation embeddings with the attention mechanism (Bahdanau et al., 2015) . Finally, a self-attentive encoder blends contextualized word embeddings and pronunciation embeddings to capture the overall semantics for both pun detection and location.",
                "cite_spans": [
                    {
                        "start": 666,
                        "end": 689,
                        "text": "(Bahdanau et al., 2015)",
                        "ref_id": "BIBREF1"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Framework Overview",
                "sec_num": "3.2"
            },
            {
                "text": "The context is essential for interpreting a word in the text. Hence, we propose to apply contextualized word embeddings to derive word representations. In the framework of PCPR, any contextualized word embedding method, such as BERT (Devlin et al., 2018) , ELMo (Peters et al., 2018) , and XLNet (Yang et al., 2019) , can be utilized. Here, we choose BERT to derive contextualized word embeddings without loss of generality.",
                "cite_spans": [
                    {
                        "start": 233,
                        "end": 254,
                        "text": "(Devlin et al., 2018)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 262,
                        "end": 283,
                        "text": "(Peters et al., 2018)",
                        "ref_id": "BIBREF35"
                    },
                    {
                        "start": 296,
                        "end": 315,
                        "text": "(Yang et al., 2019)",
                        "ref_id": "BIBREF49"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Contextualized Word Embeddings",
                "sec_num": "3.3"
            },
            {
                "text": "Input Embeddings \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 E 1 E 2 E N \u2022 \u2022 \u2022 Input Words w 1 w 2 w N \u2022 \u2022 \u2022 r1,M 1 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 r1,1 r2,1 r2,M 2 rN,1 rN,M N u1,1 u1,M 1 u2,1 u2,M 2 uN,M N uN,1 Word Phonemes \u2022 \u2022 \u2022 T C 1 T C 2 T C N T P N T P 1 T P 2 Phoneme Embeddings Phonological Attention T C [CLS] E [CLS] \u2022 \u2022 \u2022 Joint Embeddings \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 T J 1 T J 2 T J N T J [CLS]",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Contextualized Word Embeddings",
                "sec_num": "3.3"
            },
            {
                "text": "Pun BERT deploys a multi-layer bidirectional encoder based on transformers with multi-head selfattention (Vaswani et al., 2017) to model words in the text after integrating both word and position embeddings (Sukhbaatar et al., 2015) . As a result, for each word, a representative contextualized embedding is derived by considering both the specific word and all contexts in the document. Here we denote T C i as the d C -dimensional contextualized word embedding for the word w i . In addition, BERT contains a special token [CLS] with an embedding vector in BERT to represent the semantics of the whole input text.",
                "cite_spans": [
                    {
                        "start": 105,
                        "end": 127,
                        "text": "(Vaswani et al., 2017)",
                        "ref_id": "BIBREF44"
                    },
                    {
                        "start": 207,
                        "end": 232,
                        "text": "(Sukhbaatar et al., 2015)",
                        "ref_id": "BIBREF41"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Contextualized Word Embeddings",
                "sec_num": "3.3"
            },
            {
                "text": "Detection Prediction \u0177D \u0177L",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Contextualized Word Embeddings",
                "sec_num": "3.3"
            },
            {
                "text": "To learn the phonological characteristics of words, PCPR models the word phonemes. For each phoneme r i,j of the word w i , we project r i,j to a d P -dimensional embedding space as a trainable vector u i,j to represent its phonological properties.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Pronunciation Embeddings",
                "sec_num": "3.4"
            },
            {
                "text": "Based on the phoneme embeddings of a word, we apply the attention mechanism (Bahdanau et al., 2015) to simultaneously identify important phonemes and derive the pronunciation embedding T P i . Specifically, the phoneme embeddings are transformed by a fully-connected hidden layer to measure the importance scores \u03b1 P i as follows:",
                "cite_spans": [
                    {
                        "start": 76,
                        "end": 99,
                        "text": "(Bahdanau et al., 2015)",
                        "ref_id": "BIBREF1"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Pronunciation Embeddings",
                "sec_num": "3.4"
            },
            {
                "text": "v i,j = tanh(F P (u i,j )), \u03b1 P i,j = v i,j v s k v i,k v s ,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Pronunciation Embeddings",
                "sec_num": "3.4"
            },
            {
                "text": "where F P (\u2022) is a fully-connected layer with d A outputs and d A is the attention size; v s is a d Adimensional context vector that estimates the importance score of each pronunciation embedding. Finally, the pronunciation embeddings T P i can be represented as the weighted combination of phoneme embeddings as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Pronunciation Embeddings",
                "sec_num": "3.4"
            },
            {
                "text": "T P i = j \u03b1 i,j u i,j .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Pronunciation Embeddings",
                "sec_num": "3.4"
            },
            {
                "text": "Moreover, we can further derive the joint embedding T J i to indicate both word semantics and phonological knowledge for the word w i by concatenating two different embeddings as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Pronunciation Embeddings",
                "sec_num": "3.4"
            },
            {
                "text": "T J i = T C i ; T P i .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Pronunciation Embeddings",
                "sec_num": "3.4"
            },
            {
                "text": "Note that the joint embeddings are d J -dimensional vectors, where d J = d C + d P .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Pronunciation Embeddings",
                "sec_num": "3.4"
            },
            {
                "text": "For the task of pun detection, understanding the meaning of input text is essential. Due to its advantages of interpretability over convolutional neural network (LeCun et al., 1995) and recurrent neural network (Schuster and Paliwal, 1997) , we deploy the self-attention mechanism (Vaswani et al., 2017) to capture the overall semantics represented in the joint embeddings. For each word w i , the self-attention mechanism estimates an importance vector \u03b1 S i :",
                "cite_spans": [
                    {
                        "start": 161,
                        "end": 181,
                        "text": "(LeCun et al., 1995)",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 211,
                        "end": 239,
                        "text": "(Schuster and Paliwal, 1997)",
                        "ref_id": "BIBREF39"
                    },
                    {
                        "start": 281,
                        "end": 303,
                        "text": "(Vaswani et al., 2017)",
                        "ref_id": "BIBREF44"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Pronunciation-attentive Contextualized Embedding with Self-attention",
                "sec_num": "3.5"
            },
            {
                "text": "F S (T ) = Softmax( T T \u221a d )T, \u03b1 S i = exp(F S (T J i )) j exp(F S (T J j ))",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Pronunciation-attentive Contextualized Embedding with Self-attention",
                "sec_num": "3.5"
            },
            {
                "text": ", where F S (\u2022) is the function to estimate the attention for queries, and d is a scaling factor to avoid extremely small gradients. Hence, the self-attentive embedding vector is computed by aggregating joint embeddings:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Pronunciation-attentive Contextualized Embedding with Self-attention",
                "sec_num": "3.5"
            },
            {
                "text": "T J [ATT] = i \u03b1 S i \u2022 T J i .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Pronunciation-attentive Contextualized Embedding with Self-attention",
                "sec_num": "3.5"
            },
            {
                "text": "Note that the knowledge of pronunciations is considered by the self-attentive encoder but not the contextualized word encoder. Finally, the pronunciation-attentive contextualized representation for the whole input text can be derived by concatenating the overall contextualized embedding and the self-attentive embedding:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Pronunciation-attentive Contextualized Embedding with Self-attention",
                "sec_num": "3.5"
            },
            {
                "text": "T J [CLS] = T C [CLS] ; T J [ATT] .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Pronunciation-attentive Contextualized Embedding with Self-attention",
                "sec_num": "3.5"
            },
            {
                "text": "Moreover, each word w i is benefited from the selfattentive encoder and is represented by a joint embedding:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Pronunciation-attentive Contextualized Embedding with Self-attention",
                "sec_num": "3.5"
            },
            {
                "text": "T J i,[ATT] = \u03b1 S i \u2022 T J i .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Pronunciation-attentive Contextualized Embedding with Self-attention",
                "sec_num": "3.5"
            },
            {
                "text": "Based on the joint embedding for each word and the pronunciation-attentive contextualized embedding for the whole input text, both tasks can be tackled with simple fully-connected layers. Pun Detection. Pun detection is modeled as a binary classification task. Given the overall embedding for the input text T J [CLS] , the prediction \u0177D is generated by a fully-connected layer and the softmax function:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Inference and Optimization",
                "sec_num": "3.6"
            },
            {
                "text": "\u0177D = argmax k\u2208{0,1} F D (T J [CLS] ) k ,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Inference and Optimization",
                "sec_num": "3.6"
            },
            {
                "text": "where F D (\u2022) derives the logits of two classes in binary classification. Pun Location. For each word w i , the corresponding self-attentive joint embedding T J i,[ATT] is applied as features for pun location. Similar to pun detection, the prediction \u0177L i is generated by:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Inference and Optimization",
                "sec_num": "3.6"
            },
            {
                "text": "\u0177L i = argmax k\u2208{0,1} F L (T J i,[ATT] ) k ,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Inference and Optimization",
                "sec_num": "3.6"
            },
            {
                "text": "where F L (\u2022) derives two logits for classifying if a word is a pun word. Since both tasks focus on binary classification, we optimize the model with cross-entropy loss.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Inference and Optimization",
                "sec_num": "3.6"
            },
            {
                "text": "In this section, we describe our experimental settings and explain the results and interpretations. We will verify some basic assumptions of this paper:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "4"
            },
            {
                "text": "(1) the contextualized word embeddings and pronunciation embeddings are both beneficial to the pun detection and location tasks; (2) the attention mechanism can improve the performance. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "4"
            },
            {
                "text": "Experimental Datasets. We conducted experiments on the SemEval 2017 shared task 7 dataset1 (SemEval) (Miller et al., 2017) and the Pun of The Day dataset (PTD) (Yang et al., 2015) .",
                "cite_spans": [
                    {
                        "start": 101,
                        "end": 122,
                        "text": "(Miller et al., 2017)",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 160,
                        "end": 179,
                        "text": "(Yang et al., 2015)",
                        "ref_id": "BIBREF48"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiment settings",
                "sec_num": "4.1"
            },
            {
                "text": "For pun detection, the SemEval dataset consists of 4, 030 and 2, 878 examples for pun detection and location while each example with a pun can be a homographic or heterographic pun. In contrast, the PTD dataset contains 4, 826 examples without labels of pun types. word embedding (Mikolov et al., 2018) trained on Wikipedia articles3 crawled in December, 2017. The PCPR is implemented in PyTorch while the fused Adam optimizer (Kingma and Ba, 2014) optimizes the parameters with an initial learning rate of 5 \u00d7 10 -5 . The dropout and batch size are set as 10 -1 and 32. We follow BERT (BASE) (Devlin et al., 2018) to use 12 Transformer layers and self-attention heads. To clarify, in PCPR, tokens and phonemes are independently processed, so the tokens processed with WordPiece tokenizer (Wu et al., 2016) in BERT are not required to line up with phonemes for computations. To deal with the out-of-vocabulary words, we use the output embeddings of the first WordPiece tokens as the representatives, which is consistent with many state-of-theart named entity recognition approaches (Devlin et al., 2018; Lee et al., 2019) . We also create a variant of PCPR called CPR by exploiting only the contextualized word encoder without considering phonemes to demonstrate the effectiveness of pronunciation embeddings.",
                "cite_spans": [
                    {
                        "start": 280,
                        "end": 302,
                        "text": "(Mikolov et al., 2018)",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 593,
                        "end": 614,
                        "text": "(Devlin et al., 2018)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 789,
                        "end": 806,
                        "text": "(Wu et al., 2016)",
                        "ref_id": "BIBREF46"
                    },
                    {
                        "start": 1082,
                        "end": 1103,
                        "text": "(Devlin et al., 2018;",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 1104,
                        "end": 1121,
                        "text": "Lee et al., 2019)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiment settings",
                "sec_num": "4.1"
            },
            {
                "text": "To tune the hyperparameters, we search the phoneme embedding size d P and the attention size d A from {8, 16, 32, 64, 128, 256, 512} as shown in Figure 2 . For the SemEval dataset, the best setting is (d P = 64, d A = 256) for the homographic puns while heterographic puns favor (d P = 64, d A = 32). For the PTD dataset, (d P = 64, d A = 32) can reach the best performance. Baseline Methods. We compare PCPR with several baseline methods.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 152,
                        "end": 153,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Experiment settings",
                "sec_num": "4.1"
            },
            {
                "text": "For the SemEval dataset, nine baseline methods are compared in the experiments, including Duluth (Pedersen, 2017) , JU CES NLP (Pramanick and Das, 2017), PunFields (Mikhalkova and Karyakin, 2017) , UWAV (Vadehra, 2017) , Fermi (Indurthi and Oota, 2017) , and UWaterloo (Vechtomova, 2017) . While most of them extract complicated linguistic features to train rule based and machine learning based classifiers. In addition to task participants, Sense (Cai et al., 2018) incorporates word sense representations into RNNs to tackle the homographic pun location task. The CRF (Zou and Lu, 2019) captures linguistic features such as POS tags, n-grams, and word suffix to model puns. Moreover, the Joint (Zou and Lu, 2019) jointly models two tasks with RNNs and a CRF tagger.",
                "cite_spans": [
                    {
                        "start": 97,
                        "end": 113,
                        "text": "(Pedersen, 2017)",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 164,
                        "end": 195,
                        "text": "(Mikhalkova and Karyakin, 2017)",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 203,
                        "end": 218,
                        "text": "(Vadehra, 2017)",
                        "ref_id": "BIBREF43"
                    },
                    {
                        "start": 227,
                        "end": 252,
                        "text": "(Indurthi and Oota, 2017)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 269,
                        "end": 287,
                        "text": "(Vechtomova, 2017)",
                        "ref_id": "BIBREF45"
                    },
                    {
                        "start": 449,
                        "end": 467,
                        "text": "(Cai et al., 2018)",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 571,
                        "end": 589,
                        "text": "(Zou and Lu, 2019)",
                        "ref_id": "BIBREF52"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiment settings",
                "sec_num": "4.1"
            },
            {
                "text": "For the PTD dataset, four baseline methods with reported performance are selected for comparisons. MCL (Mihalcea and Strapparava, 2005) exploits word representations with multiple stylistic features while HAE (Yang et al., 2015) applies a random forest model with Word2Vec and humancentric features. PAL (Chen and Lee, 2017) trains a convolutional neural network (CNN) to learn essential feature automatically. Based on existing CNN models, HUR (Chen and Soo, 2018) improves the performance by adjusting the filter size and adding a highway layer.",
                "cite_spans": [
                    {
                        "start": 103,
                        "end": 135,
                        "text": "(Mihalcea and Strapparava, 2005)",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 209,
                        "end": 228,
                        "text": "(Yang et al., 2015)",
                        "ref_id": "BIBREF48"
                    },
                    {
                        "start": 445,
                        "end": 465,
                        "text": "(Chen and Soo, 2018)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiment settings",
                "sec_num": "4.1"
            },
            {
                "text": "Pun Detection. Table 3 presents the pun detection performance of methods for both homographic and heterographic puns on the SemEval dataset while Table 4 shows the detection performance on the PTD dataset. For the SemEval dataset, compared to the nine baseline models, PCPR achieves the highest performance with 3.0% and 6.1% improvements of F 1 against the best among the baselines (i.e. Joint) for the homographic and heterographic datasets, respectively. For the PTD dataset, PCPR improves against HUR by 9.6%. Moreover, the variant CPR beats all of the baseline methods and shows the effectiveness of contextualized word embeddings. In addition, PCPR further improves the performances by 2.3% and 1.1% with the attentive pronunciation feature for detecting homographic and heterographic puns, respectively. An interesting observation is that pronunciation embeddings also facilitate homographic pun detection, implying the potential of pronunciation for enhancing general language modeling. Pun Location. Table 3 shows that the proposed PCPR model achieves highest F 1 -scores on both homographic and heterographic pun location tasks with 10.9% and 15.9% incredible increment against the best baseline method. The improvement is NLP 72.51 90.79 68.84 33.48 33.48 33.48 73.67 94.02 71.74 37.92 37.92 37.92 PunFields 79.93 73.37 67.82 32.79 32.79 32.79 75.80 59.40 57.47 much larger than that on pun detection task. We posit the reason is that predicting pun locations relies much more on the comparative relations among different tokens in one sentence. As a result, contextualized word embeddings acquire an enormous advantage. By applying the pronunciation-attentive representations, different words with similar pronunciations are linked, leading to a much better pinpoint of pun word for the heterographic dataset.",
                "cite_spans": [
                    {
                        "start": 1233,
                        "end": 1372,
                        "text": "NLP 72.51 90.79 68.84 33.48 33.48 33.48 73.67 94.02 71.74 37.92 37.92 37.92 PunFields 79.93 73.37 67.82 32.79 32.79 32.79 75.80 59.40 57.47",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 21,
                        "end": 22,
                        "text": "3",
                        "ref_id": "TABREF6"
                    },
                    {
                        "start": 152,
                        "end": 153,
                        "text": "4",
                        "ref_id": "TABREF7"
                    },
                    {
                        "start": 1015,
                        "end": 1016,
                        "text": "3",
                        "ref_id": "TABREF6"
                    }
                ],
                "eq_spans": [],
                "section": "Experimental Results",
                "sec_num": "4.2"
            },
            {
                "text": "We notice that some of the baseline models such as UWaterloo, UWAV and PunFields have poor performances. These methods consider the word position in a sentence or calculate the inverse document frequency of words. recognition is to establish a pipeline to detect and then locate puns. Table 5 shows the pipeline performances of PCPR and Joint, which is the only baseline with reported pipeline performance for recognizing the homographic and heterographic puns in the SemEval dataset. Joint achieves suboptimal performance and the authors of Joint attribute the performance drop to error propagation. In contrast, PCPR improves the F 1 -scores against Joint by 24.6% and 20.0% on two pun types.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 291,
                        "end": 292,
                        "text": "5",
                        "ref_id": "TABREF8"
                    }
                ],
                "eq_spans": [],
                "section": "Experimental Results",
                "sec_num": "4.2"
            },
            {
                "text": "Ablation Study. To better understand the effectiveness of each component in PCPR, we conduct an ablation study on the homographic puns of the SemEval dataset. Table 6 shows the results on taking out different features of PCPR, including pre-trained phoneme embeddings, the self-attentive encoder, and phonological attention. Note that we use the average pooling as an alternative when we remove the phonological attention module. As a result, we can see the drop after removing each of the three features. It shows that all these components are essential for PCPR to recognize puns. Attentive Weights Interpretation. Figure 3 illustrates the self-attention weights \u03b1 S i of three ex-A busy barber is quiet harried.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 165,
                        "end": 166,
                        "text": "6",
                        "ref_id": "TABREF9"
                    },
                    {
                        "start": 624,
                        "end": 625,
                        "text": "3",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Ablation Study and Analysis",
                "sec_num": "4.3"
            },
            {
                "text": "< l a t e x i t s h a 1 _ b a s e 6 4 = \" k f",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Ablation Study and Analysis",
                "sec_num": "4.3"
            },
            {
                "text": "B M Z z + J z k / u X 3 y V 8 s 7 k F S d R d l s = \" > A A A C C n i c b V C 7 T s N A E D y H V w g v A y X N Q Y R E Z d m h g D J A Q x k k 8 p C C F Z 3 P 6 + S U 8 9 n c n Z G s K D U N v 0 J D A U K 0 f A E d f 8 M l c Q E J I 6 0 0 m t n V 7 k 6 Q c q a 0 6 3 5 b p a X l l d W 1 8 n p l Y 3 N r e 8 f e 3 W u p J J M U m j T h i e w E R A F n A p q a a Q 6 d V A K J A w 7 t Y H g 1 8 d s P I B V L x K 3 O U / B j 0 h c s Y p R o I / X s w w s c Z C r H A Z E B S M w U v s 8 Y a D w g U j I I H Y x 7 d t V 1 3 C n w I v E K U k U F G j 3 7 6 y 5 M a B a D 0 J Q T p b q e m 2 p / R K R m l M O 4 c p c p S A k d k j 5 0 D R U k B u W P p q + M 8 b F R Q h w l 0 p T Q e K r + n h i R W K k 8 D k x n T P R A z X s T 8 T + v m + n o 3 B 8 x k W Y a B J 0 t i j K O d Y I n u e C Q S a C a 5 4 Y Q K p m 5 F V M T A q H a p F c x I X j z L y + S V s 3 x T p 3 a T a 1 a v y z i K K M D d I R O k I f O U B 1 d o w Z q I o o e 0 T N 6 R W / W k / V i v V s f s 9 a S V c z s o z + w P n 8 A S Q K Z W g = = < / l a t e x i t >",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Ablation Study and Analysis",
                "sec_num": "4.3"
            },
            {
                "text": "A busy barber is quiet harried.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Ablation Study and Analysis",
                "sec_num": "4.3"
            },
            {
                "text": "< l a t e x i t s h a 1 _ b a s e 6 4 = \" k f",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Ablation Study and Analysis",
                "sec_num": "4.3"
            },
            {
                "text": "B M Z z + J z k / u X 3 y V 8 s 7 k F S d R d l s = \" > A A A C C n i c b V C 7 T s N A E D y H V w g v A y X N Q Y R E Z d m h g D J A Q x k k 8 p C C F Z 3 P 6 + S U 8 9 n c n Z G s K D U N v 0 J D A U K 0 f A E d f 8 M l c Q E J I 6 0 0 m t n V 7 k 6 Q c q a 0 6 3 5 b p a X l l d W 1 8 n p l Y 3 N r e 8 f e 3 W u p J J M U m j T h i e w E R A F n A p q a a Q 6 d V A K J A w 7 t Y H g 1 8 d s P I B V L x K 3 O U / B j 0 h c s Y p R o I / X s w w s c Z C r H A Z E B S M w U v s 8 Y a D w g U j I I H Y x 7 d t V 1 3 C n w I v E K U k U F G j 3 7 6 y 5 M a B a D 0 J Q T p b q e m 2 p / R K R m l M O 4 c p c p S A k d k j 5 0 D R U k B u W P p q + M 8 b F R Q h w l 0 p T Q e K r + n h i R W K k 8 D k x n T P R A z X s T 8 T + v m + n o 3 B 8 x k W Y a B J 0 t i j K O d Y I n u e C Q S a C a 5 4 Y Q K p m 5 F V M T A q H a p F c x I X j z L y + S V s 3 x T p 3 a T a 1 a v y z i K K M D d I R O k I f O U B 1 d o w Z q I o o e 0 T N 6 R W / W k / V i v V s f s 9 a S V c z s o z + w P n 8 A S Q K Z W g = = < / l a t e x i t >",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Ablation Study and Analysis",
                "sec_num": "4.3"
            },
            {
                "text": "I phoned the zoo but the lion was busy. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Ablation Study and Analysis",
                "sec_num": "4.3"
            },
            {
                "text": "K Y W M g t S K C i j Q A m 1 2 A C L Q g n V s H 8 5 9 q t 3 Y K z Q 6 h Y H M T Q j 1 l W i I z h D J 7 W y x 9 c 0 7 m k F b Y o 9 o A 9 a 0 z D B C U v n 0 3 t m n W A H + V Y 2 5 + f 9 S d F F C G a Q I 7 M q t b J f j b b m S Q Q K u W T W 1 g M / x u a Q G R R c w i j T S C z E j P d Z F + o O F Y v A N o e T g 0 b 0 y C l t 2 t H G P Y V 0 o v 6 e G L L I 2 k E U u s 6 I Y c / O e 2 P x P 6 + e Y O e 8 O R Q q T h A U n y 7 q J J K i p u N 0 a F s Y 4 C g H D h g 3 w v 2 V 8 h 4 z j K P L M O N C C O Z P X o R K I R + c 5 g s 3 h V z x Y h Z H m h y Q Q 3 J C A n J G i u S K l E i Z c P J I n s k r e f O e v B f v 3 f u Y t q",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Ablation Study and Analysis",
                "sec_num": "4.3"
            },
            {
                "text": "a 8 2 c w + + V P e 5 w 8 v y 5 w N < / l a t e x i t > I phoned the zoo but the lion was busy. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Ablation Study and Analysis",
                "sec_num": "4.3"
            },
            {
                "text": "K Y W M g t S K C i j Q A m 1 2 A C L Q g n V s H 8 5 9 q t 3 Y K z Q 6 h Y H M T Q j 1 l W i I z h D J 7 W y x 9 c 0 7 m k F b Y o 9 o A 9 a 0 z D B C U v n 0 3 t m n W A H + V Y 2 5 + f 9 S d F F C G a Q I 7 M q t b J f j b b m S Q Q K u W T W 1 g M / x u a Q G R R c w i j T S C z E j P d Z F + o O F Y v A N o e T g 0 b 0 y C l t 2 t H G P Y V 0 o v 6 e G L L I 2 k E U u s 6 I Y c / O e 2 P x P 6 + e Y O e 8 O R Q q T h A U n y 7 q J J K i p u N 0 a F s Y 4 C g H D h g 3 w v 2 V 8 h 4 z j K P L M O N C C O Z P X o R K I R + c 5 g s 3 h V z x Y h Z H m h y Q Q 3 J C A n J G i u S K l E i Z c P J I n s k r e f O e v B f v 3 f u Y t q",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Ablation Study and Analysis",
                "sec_num": "4.3"
            },
            {
                "text": "a 8 2 c w + + V P e 5 w 8 v y 5 w N < / l a t e x i t >",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Ablation Study and Analysis",
                "sec_num": "4.3"
            },
            {
                "text": "The boating store had its best sail ever.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Ablation Study and Analysis",
                "sec_num": "4.3"
            },
            {
                "text": "< l a t e x i t s h a 1 _ b a s e 6 4 = \" x f u W q S e W P F Q y The boating store had its best sail ever.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Ablation Study and Analysis",
                "sec_num": "4.3"
            },
            {
                "text": "K R K t n y g U b E 7 U g A 4 = \" > A A A C E n i c b V C 7 T g M x E P S F V w i v A 0 o a i w g J m u g u F F B G 0 F A G K S 8 p i S K f s 0 m s + O y T v Y c U R f k G G n 6 F h g K E a K n o + B u c R w E J I 1 k a z + y u v R M l U l g M g m 8 v s 7 a + s b m V 3 c 7 t 7 O 7 t H / i H R z W r U 8 O h y r X U p h E x C 1 I o q K J A C Y 3 E A I s j C f V o e D v 1 6 w 9 g r N C q g q M E 2 j H r K 9 E T n K G T O v 5 F Z Q A 0 0 u 6 m + t S i N k A H r E s F W h q B R W q Z k B T c h E L H z w e F Y A a 6 S s I F y Z M F y h 3 / q 9 X V P I 1 B I Z f M 2 m Y Y J N g e M 4 O C S 5 j k W q m F h P E h 6 0 P T U c V i s O 3 x b K U J P X N K l / a 0 c U c h n a m / O 8 Y s t n Y U R",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Ablation Study and Analysis",
                "sec_num": "4.3"
            },
            {
                "text": "< l a t e x i t s h a 1 _ b a s e 6 4 = \" x f u W q S e W P F Q y amples from heterographic puns in the SemEval dataset. The word highlighted in the upper sentence (marked in pink) is a pun while we also color each word of the lower sentence in blue according to the magnitude of its attention weights. The deeper colors indicate higher attention weights. In the first example, busy has the largest weight because it has the most similar semantic meaning as harried. The barber also has relatively high weights. We suppose it is related to hairy which should be the other word of this double entendre. Similar, the zoo is corresponded to lion while phone and busy indicate line for the pun. Moreover, boating confirms sail while store supports sale. Interpreting the weights out of our self-attentive encoder explains the significance of each token when the model detects the pun in the context. The phonemes are essential in these cases because they strengthen the relationship among words with distant semantic meanings but similar phonological expressions.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Ablation Study and Analysis",
                "sec_num": "4.3"
            },
            {
                "text": "K R K t n y g U b E 7 U g A 4 = \" > A A A C E n i c b V C 7 T g M x E P S F V w i v A 0 o a i w g J m u g u F F B G 0 F A G K S 8 p i S K f s 0 m s + O y T v Y c U R f k G G n 6 F h g K E a K n o + B u c R w E J I 1 k a z + y u v R M l U l g M g m 8 v s 7 a + s b m V 3 c 7 t 7 O 7 t H / i H R z W r U 8 O h y r X U p h E x C 1 I o q K J A C Y 3 E A I s j C f V o e D v 1 6 w 9 g r N C q g q M E 2 j H r K 9 E T n K G T O v 5 F Z Q A 0 0 u 6 m + t S i N k A H r E s F W h q B R W q Z k B T c h E L H z w e F Y A a 6 S s I F y Z M F y h 3 / q 9 X V P I 1 B I Z f M 2 m Y Y J N g e M 4 O C S 5 j k W q m F h P E h 6 0 P T U c V i s O 3 x b K U J P X N K l / a 0 c U c h n a m / O 8 Y s t n Y U R",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Ablation Study and Analysis",
                "sec_num": "4.3"
            },
            {
                "text": "Sensitivity to Text Lengths. Figure 4 shows the performance of pun detection and location over different text lengths for homographic and heterographic puns in the SemEval dataset. For both tasks, the performance gets higher when the text lengths are longer because the context informa-tion is richer. Especially in the pun detection task, we observe that our model requires longer contexts (more than 20 words) to detect the homographic puns. However, shorter contexts (less than 10 words) are adequate for heterographic pun detection, which indicates the contribution from phonological features. In short, the results verify the importance of contextualized embeddings and pronunciation representations for pun recognition.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 36,
                        "end": 37,
                        "text": "4",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Ablation Study and Analysis",
                "sec_num": "4.3"
            },
            {
                "text": "Case Study and Error Analysis. Table 7 shows the results of a case study with the outputs of CPR and PCPR. In the first case, the heterographic pun comes from the words son and sun. CPR fails to recognize the pun word with limited context information while the phonological attention in PCPR helps to locate it. However, the pronunciation features in some cases can mislead the model to make wrong predictions. For example, patent in the second sentence is a homographic pun word and has several meanings, which can be found with the contextual features. Besides, the phonemes in lies are ubiquitous in many other words like laws, thereby confusing the model. In the last case, got is a widely used causative with dozens of meanings so that the word is hard to be recognized as a pun word with its contextual and phonological features.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 37,
                        "end": 38,
                        "text": "7",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Ablation Study and Analysis",
                "sec_num": "4.3"
            },
            {
                "text": "In this paper, we propose a novel approach, PCPR, for pun detection and location by leveraging a contextualized word encoder and modeling phonemes as word pronunciations. Moreover, we would love to apply the proposed model to other problems, such as general humor recognition, irony discovery, and sarcasm detection, as the future work.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusions",
                "sec_num": "5"
            },
            {
                "text": "http://alt.qcri.org/semeval2017/ task7/",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "http://svn.code.sf.net/p/cmusphinx/ code/trunk/cmudict/",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://dumps.wikimedia.org/enwiki/ latest/",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "We would like to thank the anonymous reviewers for their helpful comments.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgment",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Humorist bot: Bringing computational humour in a chat-bot system",
                "authors": [
                    {
                        "first": "Agnese",
                        "middle": [],
                        "last": "Augello",
                        "suffix": ""
                    },
                    {
                        "first": "Gaetano",
                        "middle": [],
                        "last": "Saccone",
                        "suffix": ""
                    },
                    {
                        "first": "Salvatore",
                        "middle": [],
                        "last": "Gaglio",
                        "suffix": ""
                    },
                    {
                        "first": "Giovanni",
                        "middle": [],
                        "last": "Pilato",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "CI-SIS 2008",
                "volume": "",
                "issue": "",
                "pages": "703--708",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Agnese Augello, Gaetano Saccone, Salvatore Gaglio, and Giovanni Pilato. 2008. Humorist bot: Bringing computational humour in a chat-bot system. In CI- SIS 2008, pages 703-708.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Neural machine translation by jointly learning to align and translate",
                "authors": [
                    {
                        "first": "Dzmitry",
                        "middle": [],
                        "last": "Bahdanau",
                        "suffix": ""
                    },
                    {
                        "first": "Kyunghyun",
                        "middle": [],
                        "last": "Cho",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2015. Neural machine translation by jointly learning to align and translate. In ICLR 2015.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Word embeddings for speech recognition",
                "authors": [
                    {
                        "first": "Samy",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    },
                    {
                        "first": "Georg",
                        "middle": [],
                        "last": "Heigold",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "ISCA",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Samy Bengio and Georg Heigold. 2014. Word embed- dings for speech recognition. In ISCA 2014.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Predicting humor response in dialogues from tv sitcoms",
                "authors": [
                    {
                        "first": "Dario",
                        "middle": [],
                        "last": "Bertero",
                        "suffix": ""
                    },
                    {
                        "first": "Pascale",
                        "middle": [],
                        "last": "Fung",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "ICASSP 2016",
                "volume": "",
                "issue": "",
                "pages": "5780--5784",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dario Bertero and Pascale Fung. 2016. Predicting humor response in dialogues from tv sitcoms. In ICASSP 2016, pages 5780-5784.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Large dataset and language model fun-tuning for humor recognition",
                "authors": [
                    {
                        "first": "Vladislav",
                        "middle": [],
                        "last": "Blinov",
                        "suffix": ""
                    },
                    {
                        "first": "Valeria",
                        "middle": [],
                        "last": "Bolotova-Baranova",
                        "suffix": ""
                    },
                    {
                        "first": "Pavel",
                        "middle": [],
                        "last": "Braslavski",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "ACL 2019",
                "volume": "",
                "issue": "",
                "pages": "4027--4032",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Vladislav Blinov, Valeria Bolotova-Baranova, and Pavel Braslavski. 2019. Large dataset and language model fun-tuning for humor recognition. In ACL 2019, pages 4027-4032.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Senseaware neural models for pun location in texts",
                "authors": [
                    {
                        "first": "Yitao",
                        "middle": [],
                        "last": "Cai",
                        "suffix": ""
                    },
                    {
                        "first": "Yin",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaojun",
                        "middle": [],
                        "last": "Wan",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "ACL 2018",
                "volume": "",
                "issue": "",
                "pages": "546--551",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yitao Cai, Yin Li, and Xiaojun Wan. 2018. Sense- aware neural models for pun location in texts. In ACL 2018, pages 546-551.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Predicting audience's laughter using convolutional neural network",
                "authors": [
                    {
                        "first": "Lei",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Chong",
                        "middle": [],
                        "last": "Min",
                        "suffix": ""
                    },
                    {
                        "first": "Lee",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1702.02584"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Lei Chen and Chong MIn Lee. 2017. Predicting audi- ence's laughter using convolutional neural network. arXiv preprint arXiv:1702.02584.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Humor recognition using deep learning",
                "authors": [
                    {
                        "first": "Peng-Yu",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Von-Wun",
                        "middle": [],
                        "last": "Soo",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "NAACL 2018",
                "volume": "",
                "issue": "",
                "pages": "113--117",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Peng-Yu Chen and Von-Wun Soo. 2018. Humor recog- nition using deep learning. In NAACL 2018, pages 113-117.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
                "authors": [
                    {
                        "first": "Jacob",
                        "middle": [],
                        "last": "Devlin",
                        "suffix": ""
                    },
                    {
                        "first": "Ming-Wei",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "Kenton",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Kristina",
                        "middle": [],
                        "last": "Toutanova",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1810.04805"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understand- ing. arXiv preprint arXiv:1810.04805.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Weca: A wordnet-encoded collocation-attention network for homographic pun recognition",
                "authors": [
                    {
                        "first": "Yufeng",
                        "middle": [],
                        "last": "Diao",
                        "suffix": ""
                    },
                    {
                        "first": "Hongfei",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Di",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Liang",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Kan",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Zhihao",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Jian",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Shaowu",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Bo",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Dongyu",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "EMNLP 2018",
                "volume": "",
                "issue": "",
                "pages": "2507--2516",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yufeng Diao, Hongfei Lin, Di Wu, Liang Yang, Kan Xu, Zhihao Yang, Jian Wang, Shaowu Zhang, Bo Xu, and Dongyu Zhang. 2018. Weca: A wordnet-encoded collocation-attention network for homographic pun recognition. In EMNLP 2018, pages 2507-2516.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Idiom savant at semeval-2017 task 7: Detection and interpretation of english puns",
                "authors": [
                    {
                        "first": "Aniruddha",
                        "middle": [],
                        "last": "Samuel Doogan",
                        "suffix": ""
                    },
                    {
                        "first": "Hanyang",
                        "middle": [],
                        "last": "Ghosh",
                        "suffix": ""
                    },
                    {
                        "first": "Tony",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Veale",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "SemEval-2017",
                "volume": "",
                "issue": "",
                "pages": "103--108",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Samuel Doogan, Aniruddha Ghosh, Hanyang Chen, and Tony Veale. 2017. Idiom savant at semeval- 2017 task 7: Detection and interpretation of english puns. In SemEval-2017, pages 103-108.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Semeval-2015 task 11: Sentiment analysis of figurative language in twitter",
                "authors": [
                    {
                        "first": "Aniruddha",
                        "middle": [],
                        "last": "Ghosh",
                        "suffix": ""
                    },
                    {
                        "first": "Guofu",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Tony",
                        "middle": [],
                        "last": "Veale",
                        "suffix": ""
                    },
                    {
                        "first": "Paolo",
                        "middle": [],
                        "last": "Rosso",
                        "suffix": ""
                    },
                    {
                        "first": "Ekaterina",
                        "middle": [],
                        "last": "Shutova",
                        "suffix": ""
                    },
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Barnden",
                        "suffix": ""
                    },
                    {
                        "first": "Antonio",
                        "middle": [],
                        "last": "Reyes",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Se-mEval 2015",
                "volume": "",
                "issue": "",
                "pages": "470--478",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Aniruddha Ghosh, Guofu Li, Tony Veale, Paolo Rosso, Ekaterina Shutova, John Barnden, and Antonio Reyes. 2015. Semeval-2015 task 11: Sentiment analysis of figurative language in twitter. In Se- mEval 2015, pages 470-478.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Pun generation with surprise",
                "authors": [
                    {
                        "first": "He",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Nanyun",
                        "middle": [],
                        "last": "Peng",
                        "suffix": ""
                    },
                    {
                        "first": "Percy",
                        "middle": [],
                        "last": "Liang",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "NAACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "He He, Nanyun Peng, and Percy Liang. 2019. Pun gen- eration with surprise. In NAACL 2019.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Elirf-upv at semeval-2017 task 7: Pun detection and interpretation",
                "authors": [
                    {
                        "first": "Llu\u00eds-F",
                        "middle": [],
                        "last": "Hurtado",
                        "suffix": ""
                    },
                    {
                        "first": "Encarna",
                        "middle": [],
                        "last": "Segarra",
                        "suffix": ""
                    },
                    {
                        "first": "Ferran",
                        "middle": [],
                        "last": "Pla",
                        "suffix": ""
                    },
                    {
                        "first": "Pascual",
                        "middle": [],
                        "last": "Carrasco",
                        "suffix": ""
                    },
                    {
                        "first": "Jos\u00e9-Angel",
                        "middle": [],
                        "last": "Gonz\u00e1lez",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "SemEval-2017",
                "volume": "",
                "issue": "",
                "pages": "440--443",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Llu\u00eds-F Hurtado, Encarna Segarra, Ferran Pla, Pascual Carrasco, and Jos\u00e9-Angel Gonz\u00e1lez. 2017. Elirf-upv at semeval-2017 task 7: Pun detection and interpre- tation. In SemEval-2017, pages 440-443.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Fermi at semeval-2017 task 7: Detection and interpretation of homographic puns in english language",
                "authors": [
                    {
                        "first": "Vijayasaradhi",
                        "middle": [],
                        "last": "Indurthi",
                        "suffix": ""
                    },
                    {
                        "first": "Subba",
                        "middle": [],
                        "last": "Reddy",
                        "suffix": ""
                    },
                    {
                        "first": "Oota",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "SemEval-2017",
                "volume": "",
                "issue": "",
                "pages": "457--460",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Vijayasaradhi Indurthi and Subba Reddy Oota. 2017. Fermi at semeval-2017 task 7: Detection and inter- pretation of homographic puns in english language. In SemEval-2017, pages 457-460.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Phonological pun-derstanding",
                "authors": [
                    {
                        "first": "Aaron",
                        "middle": [],
                        "last": "Jaech",
                        "suffix": ""
                    },
                    {
                        "first": "Rik",
                        "middle": [],
                        "last": "Koncel-Kedziorski",
                        "suffix": ""
                    },
                    {
                        "first": "Mari",
                        "middle": [],
                        "last": "Ostendorf",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "ACL 2016",
                "volume": "",
                "issue": "",
                "pages": "654--663",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Aaron Jaech, Rik Koncel-Kedziorski, and Mari Osten- dorf. 2016. Phonological pun-derstanding. In ACL 2016, pages 654-663.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Deep convolutional acoustic word embeddings using word-pair side information",
                "authors": [
                    {
                        "first": "Herman",
                        "middle": [],
                        "last": "Kamper",
                        "suffix": ""
                    },
                    {
                        "first": "Weiran",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Karen",
                        "middle": [],
                        "last": "Livescu",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "ICASSP 2016",
                "volume": "",
                "issue": "",
                "pages": "4950--4954",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Herman Kamper, Weiran Wang, and Karen Livescu. 2016. Deep convolutional acoustic word embed- dings using word-pair side information. In ICASSP 2016, pages 4950-4954. IEEE.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Adam: A method for stochastic optimization",
                "authors": [
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Diederik",
                        "suffix": ""
                    },
                    {
                        "first": "Jimmy",
                        "middle": [],
                        "last": "Kingma",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Ba",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1412.6980"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks",
                "authors": [
                    {
                        "first": "Yann",
                        "middle": [],
                        "last": "Lecun",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    }
                ],
                "year": 1995,
                "venue": "",
                "volume": "3361",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yann LeCun, Yoshua Bengio, et al. 1995. Convolu- tional networks for images, speech, and time series. The handbook of brain theory and neural networks, 3361(10):1995.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Biobert: pre-trained biomedical language representation model for biomedical text mining",
                "authors": [
                    {
                        "first": "Jinhyuk",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Wonjin",
                        "middle": [],
                        "last": "Yoon",
                        "suffix": ""
                    },
                    {
                        "first": "Sungdong",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "Donghyeon",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "Sunkyu",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "Chan",
                        "middle": [],
                        "last": "Ho",
                        "suffix": ""
                    },
                    {
                        "first": "So",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Jaewoo",
                        "middle": [],
                        "last": "Kang",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1901.08746"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. 2019. Biobert: pre-trained biomed- ical language representation model for biomedical text mining. arXiv preprint arXiv:1901.08746.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Pun-GAN: Generative adversarial network for pun generation",
                "authors": [
                    {
                        "first": "Fuli",
                        "middle": [],
                        "last": "Luo",
                        "suffix": ""
                    },
                    {
                        "first": "Shunyao",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Pengcheng",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Lei",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Baobao",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "Zhifang",
                        "middle": [],
                        "last": "Sui",
                        "suffix": ""
                    },
                    {
                        "first": "Sun",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Fuli Luo, Shunyao Li, Pengcheng Yang, Lei Li, Baobao Chang, Zhifang Sui, and Xu SUN. 2019. Pun-GAN: Generative adversarial network for pun generation. In Proceedings of the 2019 Conference on Empiri- cal Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP).",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Learned in translation: Contextualized word vectors",
                "authors": [
                    {
                        "first": "Bryan",
                        "middle": [],
                        "last": "Mccann",
                        "suffix": ""
                    },
                    {
                        "first": "James",
                        "middle": [],
                        "last": "Bradbury",
                        "suffix": ""
                    },
                    {
                        "first": "Caiming",
                        "middle": [],
                        "last": "Xiong",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "NeurIPS 2017",
                "volume": "",
                "issue": "",
                "pages": "6294--6305",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. 2017. Learned in translation: Con- textualized word vectors. In NeurIPS 2017, pages 6294-6305.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "The possibility of language: A discussion of the nature of language, with implications for human and machine translation",
                "authors": [
                    {
                        "first": "Alan",
                        "middle": [
                            "K"
                        ],
                        "last": "Melby",
                        "suffix": ""
                    },
                    {
                        "first": "Terry",
                        "middle": [],
                        "last": "Warner",
                        "suffix": ""
                    }
                ],
                "year": 1995,
                "venue": "",
                "volume": "14",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alan K Melby and Terry Warner. 1995. The possibility of language: A discussion of the nature of language, with implications for human and machine transla- tion, volume 14. John Benjamins Publishing.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Making computers laugh: Investigations in automatic humor recognition",
                "authors": [
                    {
                        "first": "Rada",
                        "middle": [],
                        "last": "Mihalcea",
                        "suffix": ""
                    },
                    {
                        "first": "Carlo",
                        "middle": [],
                        "last": "Strapparava",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "EMNLP 2005",
                "volume": "",
                "issue": "",
                "pages": "531--538",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Rada Mihalcea and Carlo Strapparava. 2005. Making computers laugh: Investigations in automatic humor recognition. In EMNLP 2005, pages 531-538.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Punfields at semeval-2017 task 7: Employing roget's thesaurus in automatic pun recognition and interpretation",
                "authors": [
                    {
                        "first": "Elena",
                        "middle": [],
                        "last": "Mikhalkova",
                        "suffix": ""
                    },
                    {
                        "first": "Yuri",
                        "middle": [],
                        "last": "Karyakin",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1707.05479"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Elena Mikhalkova and Yuri Karyakin. 2017. Pun- fields at semeval-2017 task 7: Employing roget's thesaurus in automatic pun recognition and interpre- tation. arXiv preprint arXiv:1707.05479.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Advances in pre-training distributed word representations",
                "authors": [
                    {
                        "first": "Tomas",
                        "middle": [],
                        "last": "Mikolov",
                        "suffix": ""
                    },
                    {
                        "first": "Edouard",
                        "middle": [],
                        "last": "Grave",
                        "suffix": ""
                    },
                    {
                        "first": "Piotr",
                        "middle": [],
                        "last": "Bojanowski",
                        "suffix": ""
                    },
                    {
                        "first": "Christian",
                        "middle": [],
                        "last": "Puhrsch",
                        "suffix": ""
                    },
                    {
                        "first": "Armand",
                        "middle": [],
                        "last": "Joulin",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "LREC",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tomas Mikolov, Edouard Grave, Piotr Bojanowski, Christian Puhrsch, and Armand Joulin. 2018. Ad- vances in pre-training distributed word representa- tions. In LREC 2018.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Distributed representations of words and phrases and their compositionality",
                "authors": [
                    {
                        "first": "Tomas",
                        "middle": [],
                        "last": "Mikolov",
                        "suffix": ""
                    },
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    },
                    {
                        "first": "Kai",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Greg",
                        "middle": [
                            "S"
                        ],
                        "last": "Corrado",
                        "suffix": ""
                    },
                    {
                        "first": "Jeff",
                        "middle": [],
                        "last": "Dean",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Advances in neural information processing systems",
                "volume": "",
                "issue": "",
                "pages": "3111--3119",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor- rado, and Jeff Dean. 2013. Distributed representa- tions of words and phrases and their compositional- ity. In Advances in neural information processing systems, pages 3111-3119.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Pronunciation modeling in speech synthesis",
                "authors": [
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Corey",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Miller",
                        "suffix": ""
                    }
                ],
                "year": 1998,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Corey Andrew Miller. 1998a. Pronunciation modeling in speech synthesis.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "WordNet: An electronic lexical database",
                "authors": [
                    {
                        "first": "George",
                        "middle": [],
                        "last": "Miller",
                        "suffix": ""
                    }
                ],
                "year": 1998,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "George Miller. 1998b. WordNet: An electronic lexical database. MIT press.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Semeval-2017 task 7: Detection and interpretation of english puns",
                "authors": [
                    {
                        "first": "Tristan",
                        "middle": [],
                        "last": "Miller",
                        "suffix": ""
                    },
                    {
                        "first": "Christian",
                        "middle": [],
                        "last": "Hempelmann",
                        "suffix": ""
                    },
                    {
                        "first": "Iryna",
                        "middle": [],
                        "last": "Gurevych",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "SemEval-2017",
                "volume": "",
                "issue": "",
                "pages": "58--68",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tristan Miller, Christian Hempelmann, and Iryna Gurevych. 2017. Semeval-2017 task 7: Detection and interpretation of english puns. In SemEval-2017, pages 58-68.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "Word sense disambiguation: A survey",
                "authors": [
                    {
                        "first": "Roberto",
                        "middle": [],
                        "last": "Navigli",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "ACM computing surveys (CSUR)",
                "volume": "41",
                "issue": "2",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Roberto Navigli. 2009. Word sense disambiguation: A survey. ACM computing surveys (CSUR), 41(2):10.",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "Humor in humancomputer interaction: a short survey",
                "authors": [
                    {
                        "first": "Anton",
                        "middle": [],
                        "last": "Nijholt",
                        "suffix": ""
                    },
                    {
                        "first": "I",
                        "middle": [],
                        "last": "Andreea",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Niculescu",
                        "suffix": ""
                    },
                    {
                        "first": "Alessandro",
                        "middle": [],
                        "last": "Valitutti",
                        "suffix": ""
                    },
                    {
                        "first": "Rafael",
                        "middle": [
                            "E"
                        ],
                        "last": "Banchs",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Anton Nijholt, Andreea I Niculescu, Valitutti Alessan- dro, and Rafael E Banchs. 2017. Humor in human- computer interaction: a short survey.",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "Buzzsaw at semeval-2017 task 7: Global vs. local context for interpreting and locating homographic english puns with sense embeddings",
                "authors": [
                    {
                        "first": "Dieke",
                        "middle": [],
                        "last": "Oele",
                        "suffix": ""
                    },
                    {
                        "first": "Kilian",
                        "middle": [],
                        "last": "Evang",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "SemEval-2017",
                "volume": "",
                "issue": "",
                "pages": "444--448",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dieke Oele and Kilian Evang. 2017. Buzzsaw at semeval-2017 task 7: Global vs. local context for interpreting and locating homographic english puns with sense embeddings. In SemEval-2017, pages 444-448.",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "Duluth at semeval-2017 task 7: Puns upon a midnight dreary, lexical semantics for the weak and weary",
                "authors": [
                    {
                        "first": "Ted",
                        "middle": [],
                        "last": "Pedersen",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1704.08388"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ted Pedersen. 2017. Duluth at semeval-2017 task 7: Puns upon a midnight dreary, lexical seman- tics for the weak and weary. arXiv preprint arXiv:1704.08388.",
                "links": null
            },
            "BIBREF34": {
                "ref_id": "b34",
                "title": "Glove: Global vectors for word representation",
                "authors": [
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Pennington",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "EMNLP 2014",
                "volume": "",
                "issue": "",
                "pages": "1532--1543",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word rep- resentation. In EMNLP 2014, pages 1532-1543.",
                "links": null
            },
            "BIBREF35": {
                "ref_id": "b35",
                "title": "Deep contextualized word representations",
                "authors": [
                    {
                        "first": "Matthew",
                        "middle": [],
                        "last": "Peters",
                        "suffix": ""
                    },
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Neumann",
                        "suffix": ""
                    },
                    {
                        "first": "Mohit",
                        "middle": [],
                        "last": "Iyyer",
                        "suffix": ""
                    },
                    {
                        "first": "Matt",
                        "middle": [],
                        "last": "Gardner",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [],
                        "last": "Clark",
                        "suffix": ""
                    },
                    {
                        "first": "Kenton",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Luke",
                        "middle": [],
                        "last": "Zettlemoyer",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "NAACL 2018",
                "volume": "",
                "issue": "",
                "pages": "2227--2237",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word repre- sentations. In NAACL 2018, pages 2227-2237.",
                "links": null
            },
            "BIBREF36": {
                "ref_id": "b36",
                "title": "Semi-supervised sequence tagging with bidirectional language models",
                "authors": [
                    {
                        "first": "Waleed",
                        "middle": [],
                        "last": "Matthew E Peters",
                        "suffix": ""
                    },
                    {
                        "first": "Chandra",
                        "middle": [],
                        "last": "Ammar",
                        "suffix": ""
                    },
                    {
                        "first": "Russell",
                        "middle": [],
                        "last": "Bhagavatula",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Power",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1705.00108"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Matthew E Peters, Waleed Ammar, Chandra Bhagavat- ula, and Russell Power. 2017. Semi-supervised se- quence tagging with bidirectional language models. arXiv preprint arXiv:1705.00108.",
                "links": null
            },
            "BIBREF37": {
                "ref_id": "b37",
                "title": "Evaluation: from precision, recall and f-measure to roc, informedness, markedness and correlation",
                "authors": [
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Martin",
                        "suffix": ""
                    },
                    {
                        "first": "Powers",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "David Martin Powers. 2011. Evaluation: from pre- cision, recall and f-measure to roc, informedness, markedness and correlation.",
                "links": null
            },
            "BIBREF38": {
                "ref_id": "b38",
                "title": "Ju cse nlp @ semeval 2017 task 7: Employing rules to detect and interpret english puns",
                "authors": [
                    {
                        "first": "Aniket",
                        "middle": [],
                        "last": "Pramanick",
                        "suffix": ""
                    },
                    {
                        "first": "Dipankar",
                        "middle": [],
                        "last": "Das",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "SemEval-2017",
                "volume": "",
                "issue": "",
                "pages": "432--435",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Aniket Pramanick and Dipankar Das. 2017. Ju cse nlp @ semeval 2017 task 7: Employing rules to detect and interpret english puns. In SemEval-2017, pages 432-435.",
                "links": null
            },
            "BIBREF39": {
                "ref_id": "b39",
                "title": "Bidirectional recurrent neural networks",
                "authors": [
                    {
                        "first": "Mike",
                        "middle": [],
                        "last": "Schuster",
                        "suffix": ""
                    },
                    {
                        "first": "Kuldip K",
                        "middle": [],
                        "last": "Paliwal",
                        "suffix": ""
                    }
                ],
                "year": 1997,
                "venue": "IEEE Transactions on Signal Processing",
                "volume": "45",
                "issue": "11",
                "pages": "2673--2681",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mike Schuster and Kuldip K Paliwal. 1997. Bidirec- tional recurrent neural networks. IEEE Transactions on Signal Processing, 45(11):2673-2681.",
                "links": null
            },
            "BIBREF40": {
                "ref_id": "b40",
                "title": "An introduction to information retrieval",
                "authors": [
                    {
                        "first": "Hinrich",
                        "middle": [],
                        "last": "Sch\u00fctze",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    },
                    {
                        "first": "Prabhakar",
                        "middle": [],
                        "last": "Raghavan",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Hinrich Sch\u00fctze, Christopher D Manning, and Prab- hakar Raghavan. 2007. An introduction to informa- tion retrieval. Cambridge University Press,.",
                "links": null
            },
            "BIBREF41": {
                "ref_id": "b41",
                "title": "End-to-end memory networks",
                "authors": [
                    {
                        "first": "Sainbayar",
                        "middle": [],
                        "last": "Sukhbaatar",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Weston",
                        "suffix": ""
                    },
                    {
                        "first": "Rob",
                        "middle": [],
                        "last": "Fergus",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Advances in neural information processing systems",
                "volume": "",
                "issue": "",
                "pages": "2440--2448",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. 2015. End-to-end memory networks. In Advances in neural information processing systems, pages 2440-2448.",
                "links": null
            },
            "BIBREF42": {
                "ref_id": "b42",
                "title": "Pronunciation modeling for improved spelling correction",
                "authors": [
                    {
                        "first": "Kristina",
                        "middle": [],
                        "last": "Toutanova",
                        "suffix": ""
                    },
                    {
                        "first": "Robert",
                        "middle": [
                            "C"
                        ],
                        "last": "Moore",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kristina Toutanova and Robert C Moore. 2002. Pro- nunciation modeling for improved spelling correc- tion.",
                "links": null
            },
            "BIBREF43": {
                "ref_id": "b43",
                "title": "Uwav at semeval-2017 task 7: Automated feature-based system for locating puns",
                "authors": [
                    {
                        "first": "Ankit",
                        "middle": [],
                        "last": "Vadehra",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "SemEval-2017",
                "volume": "",
                "issue": "",
                "pages": "449--452",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ankit Vadehra. 2017. Uwav at semeval-2017 task 7: Automated feature-based system for locating puns. In SemEval-2017, pages 449-452.",
                "links": null
            },
            "BIBREF44": {
                "ref_id": "b44",
                "title": "Attention is all you need",
                "authors": [
                    {
                        "first": "Ashish",
                        "middle": [],
                        "last": "Vaswani",
                        "suffix": ""
                    },
                    {
                        "first": "Noam",
                        "middle": [],
                        "last": "Shazeer",
                        "suffix": ""
                    },
                    {
                        "first": "Niki",
                        "middle": [],
                        "last": "Parmar",
                        "suffix": ""
                    },
                    {
                        "first": "Jakob",
                        "middle": [],
                        "last": "Uszkoreit",
                        "suffix": ""
                    },
                    {
                        "first": "Llion",
                        "middle": [],
                        "last": "Jones",
                        "suffix": ""
                    },
                    {
                        "first": "Aidan",
                        "middle": [
                            "N"
                        ],
                        "last": "Gomez",
                        "suffix": ""
                    },
                    {
                        "first": "\u0141ukasz",
                        "middle": [],
                        "last": "Kaiser",
                        "suffix": ""
                    },
                    {
                        "first": "Illia",
                        "middle": [],
                        "last": "Polosukhin",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Advances in neural information processing systems",
                "volume": "",
                "issue": "",
                "pages": "5998--6008",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information pro- cessing systems, pages 5998-6008.",
                "links": null
            },
            "BIBREF45": {
                "ref_id": "b45",
                "title": "Uwaterloo at semeval-2017 task 7: Locating the pun using syntactic characteristics and corpus-based metrics",
                "authors": [
                    {
                        "first": "Olga",
                        "middle": [],
                        "last": "Vechtomova",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "SemEval-2017",
                "volume": "",
                "issue": "",
                "pages": "421--425",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Olga Vechtomova. 2017. Uwaterloo at semeval-2017 task 7: Locating the pun using syntactic character- istics and corpus-based metrics. In SemEval-2017, pages 421-425.",
                "links": null
            },
            "BIBREF46": {
                "ref_id": "b46",
                "title": "Google's neural machine translation system: Bridging the gap between human and machine translation",
                "authors": [
                    {
                        "first": "Yonghui",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Mike",
                        "middle": [],
                        "last": "Schuster",
                        "suffix": ""
                    },
                    {
                        "first": "Zhifeng",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "V",
                        "middle": [],
                        "last": "Quoc",
                        "suffix": ""
                    },
                    {
                        "first": "Mohammad",
                        "middle": [],
                        "last": "Le",
                        "suffix": ""
                    },
                    {
                        "first": "Wolfgang",
                        "middle": [],
                        "last": "Norouzi",
                        "suffix": ""
                    },
                    {
                        "first": "Maxim",
                        "middle": [],
                        "last": "Macherey",
                        "suffix": ""
                    },
                    {
                        "first": "Yuan",
                        "middle": [],
                        "last": "Krikun",
                        "suffix": ""
                    },
                    {
                        "first": "Qin",
                        "middle": [],
                        "last": "Cao",
                        "suffix": ""
                    },
                    {
                        "first": "Klaus",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Macherey",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1609.08144"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google's neural machine translation system: Bridging the gap between hu- man and machine translation. arXiv preprint arXiv:1609.08144.",
                "links": null
            },
            "BIBREF47": {
                "ref_id": "b47",
                "title": "Ecnu at semeval-2017 task 7: Using supervised and unsupervised methods to detect and locate english puns",
                "authors": [
                    {
                        "first": "Yuhuan",
                        "middle": [],
                        "last": "Xiu",
                        "suffix": ""
                    },
                    {
                        "first": "Man",
                        "middle": [],
                        "last": "Lan",
                        "suffix": ""
                    },
                    {
                        "first": "Yuanbin",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "SemEval-2017",
                "volume": "",
                "issue": "",
                "pages": "453--456",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yuhuan Xiu, Man Lan, and Yuanbin Wu. 2017. Ecnu at semeval-2017 task 7: Using supervised and unsu- pervised methods to detect and locate english puns. In SemEval-2017, pages 453-456.",
                "links": null
            },
            "BIBREF48": {
                "ref_id": "b48",
                "title": "Humor recognition and humor anchor extraction",
                "authors": [
                    {
                        "first": "Diyi",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Alon",
                        "middle": [],
                        "last": "Lavie",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Dyer",
                        "suffix": ""
                    },
                    {
                        "first": "Eduard",
                        "middle": [],
                        "last": "Hovy",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "EMNLP 2015",
                "volume": "",
                "issue": "",
                "pages": "2367--2376",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Diyi Yang, Alon Lavie, Chris Dyer, and Eduard Hovy. 2015. Humor recognition and humor anchor extrac- tion. In EMNLP 2015, pages 2367-2376.",
                "links": null
            },
            "BIBREF49": {
                "ref_id": "b49",
                "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
                "authors": [
                    {
                        "first": "Zhilin",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Zihang",
                        "middle": [],
                        "last": "Dai",
                        "suffix": ""
                    },
                    {
                        "first": "Yiming",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Jaime",
                        "middle": [],
                        "last": "Carbonell",
                        "suffix": ""
                    },
                    {
                        "first": "Ruslan",
                        "middle": [],
                        "last": "Salakhutdinov",
                        "suffix": ""
                    },
                    {
                        "first": "Quoc V",
                        "middle": [],
                        "last": "Le",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1906.08237"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car- bonell, Ruslan Salakhutdinov, and Quoc V Le. 2019. Xlnet: Generalized autoregressive pretrain- ing for language understanding. arXiv preprint arXiv:1906.08237.",
                "links": null
            },
            "BIBREF50": {
                "ref_id": "b50",
                "title": "A neural approach to pun generation",
                "authors": [
                    {
                        "first": "Zhiwei",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Jiwei",
                        "middle": [],
                        "last": "Tan",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaojun",
                        "middle": [],
                        "last": "Wan",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "ACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zhiwei Yu, Jiwei Tan, and Xiaojun Wan. 2018. A neu- ral approach to pun generation. In ACL 2018.",
                "links": null
            },
            "BIBREF51": {
                "ref_id": "b51",
                "title": "Improve word embedding using both writing and pronunciation",
                "authors": [
                    {
                        "first": "Wenhao",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    },
                    {
                        "first": "Xin",
                        "middle": [],
                        "last": "Jin",
                        "suffix": ""
                    },
                    {
                        "first": "Jianyue",
                        "middle": [],
                        "last": "Ni",
                        "suffix": ""
                    },
                    {
                        "first": "Baogang",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Zhiguo",
                        "middle": [],
                        "last": "Lu",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "PloS one",
                "volume": "13",
                "issue": "12",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Wenhao Zhu, Xin Jin, Jianyue Ni, Baogang Wei, and Zhiguo Lu. 2018. Improve word embedding us- ing both writing and pronunciation. PloS one, 13(12):e0208785.",
                "links": null
            },
            "BIBREF52": {
                "ref_id": "b52",
                "title": "Joint detection and location of english puns",
                "authors": [
                    {
                        "first": "Yanyan",
                        "middle": [],
                        "last": "Zou",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Lu",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "NAACL 2019",
                "volume": "",
                "issue": "",
                "pages": "2117--2123",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yanyan Zou and Wei Lu. 2019. Joint detection and lo- cation of english puns. In NAACL 2019, pages 2117- 2123.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure1: The overall framework of PCPR. We leverage the self-attention mechanism to jointly model contextualized embeddings and phonological representations. PCPR can tackle both pun detection and pun location tasks.",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Figure 2: Pun location performance over different phoneme embedding sizes d P and attention sizes d A on the SemEval dataset.",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "l a t e x i t s h a 1 _ b a s e 6 4= \" o + 8 b h m V C o m + 0 4 a u G j u I 6 l K z h 2 v Y = \" > A A A C E H i c b Z C 7 T k J B E I b 3 4 A 3 x h l r a b C R G K 3 I O F l o S b b T D R C 4 J E L J n G W D D n t 2 T 3 T k a J D y C j a 9 i Y 6 E x t p Z 2 v o 3 L p V Bw k k 2 + / P 9 M Z u c P Y y k s + v 6 3 l 1 p a X l l d S 6 9 n N j a 3 t n e y u 3 s V q x P D o c y 1 1",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF3": {
                "num": null,
                "text": "l a t e x i t s h a 1 _ b a s e 6 4= \" o + 8 b h m V C o m + 0 4 a u G j u I 6 l K z h 2 v Y = \" > A A A C E H i c b Z C 7 T k J B E I b 3 4 A 3 x h l r a b C R G K 3 I O F l o S b b T D R C 4 J E L J n G W D D n t 2 T 3 T k a J D y C j a 9 i Y 6 E x t p Z 2 v o 3 L p V Bw k k 2 + / P 9 M Z u c P Y y k s + v 6 3 l 1 p a X l l d S 6 9 n N j a 3 t n e y u 3 s V q x P D o c y 1 1",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF4": {
                "num": null,
                "text": "6 4 y Z j i w y 9 5 U / M 9 r p t i 7 b o + F S l I E x e c P 9 V J J U d N p P r Q r D H C U I 0 c Y N 8 L 9 l f I B M 4 y j S z H n Q g i X V 1 4 l t W I h v C w U 7 4 v 5 0 s 0 i j i w 5 I a f k n I T k i p T I H S m T K u H k k T y T V / L m P X k v 3 r v 3 M S / N e I u e Y / I H 3 u c P B S 2 d D Q = = < / l a t e x i t >",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF5": {
                "num": null,
                "text": "Figure 3: Visualization of attention weights of each pun word (marked in pink) in the sentences. A deeper color indicates a higher attention weight.",
                "uris": null,
                "fig_num": "34",
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Homographic Puns 1. Did you hear about the guy whose whole left side was cut off? He's all right now. 2. I'd tell you a chemistry joke but I know I wouldn't get a reaction. Heterographic Puns 1. The boating store had its best sail (sale) ever. 2. I lift weights only on Saturday and Sunday because Monday to Friday are weak (week) days. Examples of homographic and heterographic puns.",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table><tr><td>Dataset</td><td colspan=\"2\">SemEval Homo Hetero</td><td>PTD</td></tr><tr><td>Examples w/ Puns</td><td colspan=\"3\">1,607 1,271 2,423</td></tr><tr><td>Examples w/o Puns</td><td>643</td><td>509</td><td>2,403</td></tr><tr><td>Total Examples</td><td colspan=\"3\">2,250 1,780 4,826</td></tr></table>",
                "type_str": "table",
                "text": "Data statistics. \"Homo\" and \"Hetero\" denote homographic and heterographic puns. Pun detection employs all of the examples in the two datasets while pun location only exploits the examples with puns in SemEval due to the limitation of annotations.",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table><tr><td>F1 score</td><td>0.88 0.90 0.92</td><td>4 8 16 32 64 128 Phoneme embedding size (dP) homographic heterographic</td><td>F1 score</td><td>0.86 0.88 0.90 0.92</td><td>16 32 64 128 256 512 Attention size (dA) homographic heterographic</td></tr><tr><td colspan=\"3\">(a) Phoneme emb. size dP</td><td/><td colspan=\"2\">(b) Attention size dA</td></tr><tr><td/><td/><td/><td/><td/><td>Evaluation Metrics. We adopt precision (P), re-</td></tr><tr><td/><td/><td/><td/><td/><td>call (R), and F 1 -score (Sch\u00fctze et al., 2007; Pow-</td></tr><tr><td/><td/><td/><td/><td/><td>ers, 2011) to compare the performance of PCPR</td></tr><tr><td/><td/><td/><td/><td/><td>with previous studies in both pun detection and</td></tr><tr><td/><td/><td/><td/><td/><td>location. More specifically, we apply 10-fold cross-</td></tr><tr><td/><td/><td/><td/><td/><td>validation to conduct evaluation. For each fold,</td></tr><tr><td/><td/><td/><td/><td/><td>we randomly select 10% of the instances from the</td></tr><tr><td/><td/><td/><td/><td/><td>training set for development. To conduct fair com-</td></tr><tr><td/><td/><td/><td/><td/><td>parisons, we strictly follow the experimental set-</td></tr><tr><td/><td/><td/><td/><td/><td>tings in previous studies (Zou and Lu, 2019; Cai</td></tr><tr><td/><td/><td/><td/><td/><td>et al., 2018) and include their reported numbers in</td></tr><tr><td/><td/><td/><td/><td/><td>the comparisons.</td></tr><tr><td/><td/><td/><td/><td/><td>Implementation Details. For data pre-processing,</td></tr><tr><td/><td/><td/><td/><td/><td>all of the numbers and punctuation marks are re-</td></tr><tr><td/><td/><td/><td/><td/><td>moved. The phonemes of each word are derived</td></tr><tr><td/><td/><td/><td/><td/><td>by the CMU Pronouncing Dictionary 2 . We initial-</td></tr><tr><td/><td/><td/><td/><td/><td>ize the phoneme embeddings by using the fastText</td></tr></table>",
                "type_str": "table",
                "text": "Table 2 further shows the data statistics. The two experimental datasets are the largest publicly available benchmarks that are used in the existing studies. SemEval-2017 dataset contains punning and non-punning jokes, aphorisms, and other short texts composed by professional humorists and online collections. Hence, we assume the genres of positive and negative examples should be identical or extremely similar.",
                "html": null,
                "num": null
            },
            "TABREF5": {
                "content": "<table><tr><td>Fermi</td><td colspan=\"4\">90.24 89.70 85.33 52.15 52.15 52.15</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>UWaterloo</td><td>-</td><td>-</td><td>-</td><td>65.26 65.21 65.23</td><td>-</td><td>-</td><td>-</td><td colspan=\"3\">79.73 79.54 79.64</td></tr><tr><td>Sense</td><td>-</td><td>-</td><td>-</td><td>81.50 74.70 78.00</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>CRF</td><td colspan=\"10\">87.21 64.09 73.89 86.31 55.32 67.43 89.56 70.94 79.17 88.46 62.76 73.42</td></tr><tr><td>Joint</td><td colspan=\"10\">91.25 93.28 92.19 83.55 77.10 80.19 86.67 93.08 89.76 81.41 77.50 79.40</td></tr><tr><td>CPR</td><td colspan=\"10\">91.42 94.21 92.79 88.80 85.65 87.20 93.35 95.04 94.19 92.31 88.24 90.23</td></tr><tr><td>PCPR</td><td colspan=\"10\">94.18 95.70 94.94 90.43 87.50 88.94 94.84 95.59 95.22 94.23 90.41 92.28</td></tr></table>",
                "type_str": "table",
                "text": "35.01 35.01 35.01 UWAV 68.38 47.23 46.71 34.10 34.10 34.10 65.23 41.78 42.53 42.80 42.80 42.80",
                "html": null,
                "num": null
            },
            "TABREF6": {
                "content": "<table><tr><td>Model</td><td>P</td><td>R</td><td>F 1</td></tr><tr><td>MCL</td><td colspan=\"3\">83.80 65.50 73.50</td></tr><tr><td>HAE</td><td colspan=\"3\">83.40 88.80 85.90</td></tr><tr><td>PAL</td><td colspan=\"3\">86.40 85.40 85.70</td></tr><tr><td>HUR</td><td colspan=\"3\">86.60 94.00 90.10</td></tr><tr><td>CPR</td><td colspan=\"3\">98.12 99.34 98.73</td></tr><tr><td colspan=\"4\">PCPR 98.44 99.13 98.79</td></tr></table>",
                "type_str": "table",
                "text": "Performance of detecting and locating puns on the SemEval dataset. All improvements of PCPR and CPR over baseline methods are statistically significant at a 95% confidence level in paired t-tests. Comparing to PCPR, CPR does not model word pronunciations. Results show that both PCPR and CPR outperform baselines. With modeling pronunciations, PCPR performs the best.",
                "html": null,
                "num": null
            },
            "TABREF7": {
                "content": "<table><tr><td>Model</td><td>Homographic Puns P R F 1</td><td>Heterographic Puns P R F 1</td></tr><tr><td colspan=\"3\">Joint 67.70 67.70 67.70 68.84 68.84 68.84</td></tr><tr><td>PCPR</td><td colspan=\"2\">87.21 81.72 84.38 85.16 80.15 82.58</td></tr></table>",
                "type_str": "table",
                "text": "Performance of pun detection on the PTD dataset.",
                "html": null,
                "num": null
            },
            "TABREF8": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Performance of pipeline recognition in the Se-mEval dastaset.",
                "html": null,
                "num": null
            },
            "TABREF9": {
                "content": "<table><tr><td>Model</td><td>P</td><td>R</td><td>F 1</td></tr><tr><td>PCPR</td><td colspan=\"3\">90.43 87.50 88.94</td></tr><tr><td colspan=\"4\">w/o Pre-trained Phoneme Emb. 89.37 85.65 87.47</td></tr><tr><td>w/o Self-attention Encoder</td><td colspan=\"3\">89.17 86.42 87.70</td></tr><tr><td>w/o Phonological Attention</td><td colspan=\"3\">89.56 87.35 88.44</td></tr></table>",
                "type_str": "table",
                "text": "We suppose such rulebased recognition techniques can hardly capture the deep semantic and syntactic properties of words. Pipeline Recognition. The ultimate goal of pun Ablation study on different features of PCPR for homographic pun detection on the SemEval dataset.",
                "html": null,
                "num": null
            }
        }
    }
}