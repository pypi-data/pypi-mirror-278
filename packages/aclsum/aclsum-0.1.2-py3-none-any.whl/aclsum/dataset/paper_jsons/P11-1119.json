{
    "paper_id": "P11-1119",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:08:09.695022Z"
    },
    "title": "An Affect-Enriched Dialogue Act Classification Model for Task-Oriented Dialogue",
    "authors": [
        {
            "first": "Kristy",
            "middle": [
                "Elizabeth"
            ],
            "last": "Boyer",
            "suffix": "",
            "affiliation": {},
            "email": "keboyer@ncsu.edu"
        },
        {
            "first": "Joseph",
            "middle": [
                "F"
            ],
            "last": "Grafsgaard",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Eun",
            "middle": [],
            "last": "Young",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Ha",
            "middle": [
                "Robert"
            ],
            "last": "Phillips",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "James",
            "middle": [
                "C"
            ],
            "last": "Lester",
            "suffix": "",
            "affiliation": {},
            "email": "lester@ncsu.edu"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Dialogue act classification is a central challenge for dialogue systems. Although the importance of emotion in human dialogue is widely recognized, most dialogue act classification models make limited or no use of affective channels in dialogue act classification. This paper presents a novel affect-enriched dialogue act classifier for task-oriented dialogue that models facial expressions of users, in particular, facial expressions related to confusion. The findings indicate that the affectenriched classifiers perform significantly better for distinguishing user requests for feedback and grounding dialogue acts within textual dialogue. The results point to ways in which dialogue systems can effectively leverage affective channels to improve dialogue act classification.",
    "pdf_parse": {
        "paper_id": "P11-1119",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Dialogue act classification is a central challenge for dialogue systems. Although the importance of emotion in human dialogue is widely recognized, most dialogue act classification models make limited or no use of affective channels in dialogue act classification. This paper presents a novel affect-enriched dialogue act classifier for task-oriented dialogue that models facial expressions of users, in particular, facial expressions related to confusion. The findings indicate that the affectenriched classifiers perform significantly better for distinguishing user requests for feedback and grounding dialogue acts within textual dialogue. The results point to ways in which dialogue systems can effectively leverage affective channels to improve dialogue act classification.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Dialogue systems aim to engage users in rich, adaptive natural language conversation. For these systems, understanding the role of a user's utterance in the broader context of the dialogue is a key challenge (Sridhar, Bangalore, & Narayanan, 2009) . Central to this endeavor is dialogue act classification, which categorizes the intention behind the user's move (e.g., asking a question, providing declarative information). Automatic dialogue act classification has been the focus of a large body of research, and a variety of approaches, including sequential models (Stolcke et al., 2000) , vector-based models (Sridhar, Bangalore, & Narayanan, 2009) , and most recently, featureenhanced latent semantic analysis (Di Eugenio, Xie, & Serafin, 2010) , have shown promise. These models may be further improved by leveraging regularities of the dialogue from both linguistic and extra-linguistic sources. Users' expressions of emotion are one such source.",
                "cite_spans": [
                    {
                        "start": 208,
                        "end": 247,
                        "text": "(Sridhar, Bangalore, & Narayanan, 2009)",
                        "ref_id": null
                    },
                    {
                        "start": 567,
                        "end": 589,
                        "text": "(Stolcke et al., 2000)",
                        "ref_id": "BIBREF39"
                    },
                    {
                        "start": 612,
                        "end": 651,
                        "text": "(Sridhar, Bangalore, & Narayanan, 2009)",
                        "ref_id": null
                    },
                    {
                        "start": 714,
                        "end": 748,
                        "text": "(Di Eugenio, Xie, & Serafin, 2010)",
                        "ref_id": "BIBREF13"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Human interaction has long been understood to include rich phenomena consisting of verbal and nonverbal cues, with facial expressions playing a vital role (Knapp & Hall, 2006; McNeill, 1992; Mehrabian, 2007; Russell, Bachorowski, & Fernandez-Dols, 2003; Schmidt & Cohn, 2001) . While the importance of emotional expressions in dialogue is widely recognized, the majority of dialogue act classification projects have focused either peripherally (or not at all) on emotion, such as by leveraging acoustic and prosodic features of spoken utterances to aid in online dialogue act classification (Sridhar, Bangalore, & Narayanan, 2009) . Other research on emotion in dialogue has involved detecting affect and adapting to it within a dialogue system (Forbes-Riley, Rotaru, Litman, & Tetreault, 2009; L\u00f3pez-C\u00f3zar, Silovsky, & Griol, 2010) , but this work has not explored leveraging affect information for automatic user dialogue act classification. Outside of dialogue, sentiment analysis within discourse is an active area of research (L\u00f3pez-C\u00f3zar et al., 2010) , but it is generally lim-ited to modeling textual features and not multimodal expressions of emotion such as facial actions. Such multimodal expressions have only just begun to be explored within corpus-based dialogue research (Calvo & D'Mello, 2010; Cavicchio, 2009) .",
                "cite_spans": [
                    {
                        "start": 155,
                        "end": 175,
                        "text": "(Knapp & Hall, 2006;",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 176,
                        "end": 190,
                        "text": "McNeill, 1992;",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 191,
                        "end": 207,
                        "text": "Mehrabian, 2007;",
                        "ref_id": "BIBREF31"
                    },
                    {
                        "start": 208,
                        "end": 253,
                        "text": "Russell, Bachorowski, & Fernandez-Dols, 2003;",
                        "ref_id": "BIBREF35"
                    },
                    {
                        "start": 254,
                        "end": 275,
                        "text": "Schmidt & Cohn, 2001)",
                        "ref_id": "BIBREF36"
                    },
                    {
                        "start": 591,
                        "end": 630,
                        "text": "(Sridhar, Bangalore, & Narayanan, 2009)",
                        "ref_id": null
                    },
                    {
                        "start": 745,
                        "end": 794,
                        "text": "(Forbes-Riley, Rotaru, Litman, & Tetreault, 2009;",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 795,
                        "end": 832,
                        "text": "L\u00f3pez-C\u00f3zar, Silovsky, & Griol, 2010)",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 1031,
                        "end": 1057,
                        "text": "(L\u00f3pez-C\u00f3zar et al., 2010)",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 1286,
                        "end": 1309,
                        "text": "(Calvo & D'Mello, 2010;",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 1310,
                        "end": 1326,
                        "text": "Cavicchio, 2009)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "This paper presents a novel affect-enriched dialogue act classification approach that leverages knowledge of users' facial expressions during computer-mediated textual human-human dialogue. Intuitively, the user's affective state is a promising source of information that may help to distinguish between particular dialogue acts (e.g., a confused user may be more likely to ask a question). We focus specifically on occurrences of students' confusion-related facial actions during taskoriented tutorial dialogue.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Confusion was selected as the focus of this work for several reasons. First, confusion is known to be prevalent within tutoring, and its implications for student learning are thought to run deep (Graesser, Lu, Olde, Cooper-Pye, & Whitten, 2005) . Second, while identifying the \"ground truth\" of emotion based on any external display by a user presents challenges, prior research has demonstrated a correlation between particular facial action units and confusion during learning (Craig, D'Mello, Witherspoon, Sullins, & Graesser, 2004; D'Mello, Craig, Sullins, & Graesser, 2006; McDaniel et al., 2007) . Finally, automatic facial action recognition technologies are developing rapidly, and confusion-related facial action events are among those that can be reliably recognized automatically (Bartlett et al., 2006; Cohn, Reed, Ambadar, Xiao, & Moriyama, 2004; Pantic & Bartlett, 2007; Zeng, Pantic, Roisman, & Huang, 2009) . This promising development bodes well for the feasibility of automatic real-time confusion detection within dialogue systems.",
                "cite_spans": [
                    {
                        "start": 195,
                        "end": 244,
                        "text": "(Graesser, Lu, Olde, Cooper-Pye, & Whitten, 2005)",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 479,
                        "end": 535,
                        "text": "(Craig, D'Mello, Witherspoon, Sullins, & Graesser, 2004;",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 536,
                        "end": 578,
                        "text": "D'Mello, Craig, Sullins, & Graesser, 2006;",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 579,
                        "end": 601,
                        "text": "McDaniel et al., 2007)",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 791,
                        "end": 814,
                        "text": "(Bartlett et al., 2006;",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 815,
                        "end": 859,
                        "text": "Cohn, Reed, Ambadar, Xiao, & Moriyama, 2004;",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 860,
                        "end": 884,
                        "text": "Pantic & Bartlett, 2007;",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 885,
                        "end": 922,
                        "text": "Zeng, Pantic, Roisman, & Huang, 2009)",
                        "ref_id": "BIBREF42"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Because of the importance of dialogue act classification within dialogue systems, it has been an active area of research for some time. Early work on automatic dialogue act classification modeled discourse structure with hidden Markov models, experimenting with lexical and prosodic features, and applying the dialogue act model as a constraint to aid in automatic speech recognition (Stolcke et al., 2000) . In contrast to this sequential modeling approach, which is best suited to offline processing, recent work has explored how lexical, syntactic, and prosodic features perform for online dialogue act tagging (when only partial dialogue sequences are available) within a maximum entropy framework (Sridhar, Bangalore, & Narayanan, 2009) . A recently proposed alternative approach involves treating dialogue utterances as documents within a latent semantic analysis framework, and applying feature enhancements that incorporate such information as speaker and utterance duration (Di Eugenio et al., 2010) . Of the approaches noted above, the modeling framework presented in this paper is most similar to the vector-based maximum entropy approach of Sridhar et al. (2009) . However, it takes a step beyond the previous work by including multimodal affective displays, specifically facial expressions, as features available to an affect-enriched dialogue act classification model.",
                "cite_spans": [
                    {
                        "start": 384,
                        "end": 406,
                        "text": "(Stolcke et al., 2000)",
                        "ref_id": "BIBREF39"
                    },
                    {
                        "start": 702,
                        "end": 741,
                        "text": "(Sridhar, Bangalore, & Narayanan, 2009)",
                        "ref_id": null
                    },
                    {
                        "start": 983,
                        "end": 1008,
                        "text": "(Di Eugenio et al., 2010)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 1153,
                        "end": 1174,
                        "text": "Sridhar et al. (2009)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dialogue Act Classification",
                "sec_num": "2.1"
            },
            {
                "text": "Detecting emotional states during spoken dialogue is an active area of research, much of which focuses on detecting frustration so that a user can be automatically transferred to a human dialogue agent (L\u00f3pez-C\u00f3zar et al., 2010) . Research on spoken dialogue has leveraged lexical features along with discourse cues and acoustic information to classify user emotion, sometimes at a coarse grain along a positive/negative axis (Lee & Narayanan, 2005) . Recent work on an affective companion agent has examined user emotion classification within conversational speech (Cavazza et al., 2010) . In contrast to that spoken dialogue research, the work in this paper is situated within textual dialogue, a widely used modality of communication for which a deeper understanding of user affect may substantially improve system performance.",
                "cite_spans": [
                    {
                        "start": 202,
                        "end": 228,
                        "text": "(L\u00f3pez-C\u00f3zar et al., 2010)",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 426,
                        "end": 449,
                        "text": "(Lee & Narayanan, 2005)",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 566,
                        "end": 588,
                        "text": "(Cavazza et al., 2010)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Detecting Emotions in Dialogue",
                "sec_num": "2.2"
            },
            {
                "text": "While many projects have focused on linguistic cues, recent work has begun to explore numerous channels for affect detection including facial actions, electrocardiograms, skin conductance, and posture sensors (Calvo & D'Mello, 2010) . A recent project in a map task domain investigates some of these sources of affect data within task-oriented dialogue (Cavicchio, 2009) . Like that work, the current project utilizes facial action tagging, for which promising automatic technologies exist (Bartlett et al., 2006; Pantic & Bartlett, 2007; Zeng, Pantic, Roisman, & Huang, 2009) . However, we leverage the recognized expressions of emotion for the task of dialogue act classification.",
                "cite_spans": [
                    {
                        "start": 209,
                        "end": 232,
                        "text": "(Calvo & D'Mello, 2010)",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 353,
                        "end": 370,
                        "text": "(Cavicchio, 2009)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 490,
                        "end": 513,
                        "text": "(Bartlett et al., 2006;",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 514,
                        "end": 538,
                        "text": "Pantic & Bartlett, 2007;",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 539,
                        "end": 576,
                        "text": "Zeng, Pantic, Roisman, & Huang, 2009)",
                        "ref_id": "BIBREF42"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Detecting Emotions in Dialogue",
                "sec_num": "2.2"
            },
            {
                "text": "Sets of emotion taxonomies for discourse and dialogue are often application-specific, for example, focusing on the frustration of users who are interacting with a spoken dialogue system (L\u00f3pez-C\u00f3zar et al., 2010) , or on uncertainty expressed by students while interacting with a tutor (Forbes-Riley, Rotaru, Litman, & Tetreault, 2007) . In contrast, the most widely utilized emotion frameworks are not application-specific; for example, Ekman's Facial Action Coding System (FACS) has been widely used as a rigorous technique for coding facial movements based on human facial anatomy (Ekman & Friesen, 1978) . Within this framework, facial movements are categorized into facial action units, which represent discrete movements of muscle groups. Additionally, facial action descriptors (for movements not derived from facial muscles) and movement and visibility codes are included. Ekman's basic emotions (Ekman, 1999) have been used in recent work on classifying emotion expressed within blog text (Das & Bandyopadhyay, 2009) , while other recent work (Nguyen, 2010 ) utilizes Russell's core affect model (Russell, 2003) for a similar task.",
                "cite_spans": [
                    {
                        "start": 186,
                        "end": 212,
                        "text": "(L\u00f3pez-C\u00f3zar et al., 2010)",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 286,
                        "end": 335,
                        "text": "(Forbes-Riley, Rotaru, Litman, & Tetreault, 2007)",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 584,
                        "end": 607,
                        "text": "(Ekman & Friesen, 1978)",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 904,
                        "end": 917,
                        "text": "(Ekman, 1999)",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 998,
                        "end": 1025,
                        "text": "(Das & Bandyopadhyay, 2009)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 1052,
                        "end": 1065,
                        "text": "(Nguyen, 2010",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 1105,
                        "end": 1120,
                        "text": "(Russell, 2003)",
                        "ref_id": "BIBREF34"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Categorizing Emotions within Dialogue and Discourse",
                "sec_num": "2.3"
            },
            {
                "text": "During tutorial dialogue, students may not frequently experience Ekman's basic emotions of happiness, sadness, anger, fear, surprise, and disgust. Instead, students appear to more frequently experience cognitive-affective states such as flow and confusion (Calvo & D'Mello, 2010) . Our work leverages Ekman's facial tagging scheme to identify a particular facial action unit, Action Unit 4 (AU4), that has been observed to correlate with confusion (Craig, D'Mello, Witherspoon, Sullins, & Graesser, 2004; D'Mello, Craig, Sullins, & Graesser, 2006; McDaniel et al., 2007) .",
                "cite_spans": [
                    {
                        "start": 256,
                        "end": 279,
                        "text": "(Calvo & D'Mello, 2010)",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 448,
                        "end": 504,
                        "text": "(Craig, D'Mello, Witherspoon, Sullins, & Graesser, 2004;",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 505,
                        "end": 547,
                        "text": "D'Mello, Craig, Sullins, & Graesser, 2006;",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 548,
                        "end": 570,
                        "text": "McDaniel et al., 2007)",
                        "ref_id": "BIBREF29"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Categorizing Emotions within Dialogue and Discourse",
                "sec_num": "2.3"
            },
            {
                "text": "Among the affective states that students experience during tutorial dialogue, confusion is prevalent, and its implications for student learning are signif-icant. Confusion is associated with cognitive disequilibrium, a state in which students' existing knowledge is inconsistent with a novel learning experience (Graesser, Lu, Olde, Cooper-Pye, & Whitten, 2005) . Students may express such confusion within dialogue as uncertainty, to which human tutors often adapt in a context-dependent fashion (Forbes-Riley et al., 2007) . Moreover, implementing adaptations to student uncertainty within a dialogue system can improve the effectiveness of the system (Forbes-Riley et al., 2009) .",
                "cite_spans": [
                    {
                        "start": 312,
                        "end": 361,
                        "text": "(Graesser, Lu, Olde, Cooper-Pye, & Whitten, 2005)",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 497,
                        "end": 524,
                        "text": "(Forbes-Riley et al., 2007)",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 654,
                        "end": 681,
                        "text": "(Forbes-Riley et al., 2009)",
                        "ref_id": "BIBREF21"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Importance of Confusion in Tutorial Dialogue",
                "sec_num": "2.4"
            },
            {
                "text": "For tutorial dialogue, the importance of understanding student utterances is paramount for a system to positively impact student learning (Dzikovska, Moore, Steinhauser, & Campbell, 2010) . The importance of frustration as a cognitive-affective state during learning suggests that the presence of student confusion may serve as a useful constraining feature for dialogue act classification of student utterances. This paper explores the use of facial expression features in this way.",
                "cite_spans": [
                    {
                        "start": 138,
                        "end": 187,
                        "text": "(Dzikovska, Moore, Steinhauser, & Campbell, 2010)",
                        "ref_id": "BIBREF14"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Importance of Confusion in Tutorial Dialogue",
                "sec_num": "2.4"
            },
            {
                "text": "The corpus was collected during a textual humanhuman tutorial dialogue study in the domain of introductory computer science (Boyer, Phillips, et al., 2010) . Students solved an introductory computer programming problem and carried on textual dialogue with tutors, who viewed a synchronized version of the students' problem-solving workspace. The original corpus consists of 48 dialogues, one per student. Each student interacted with one of two tutors. Facial videos of students were collected using built-in webcams, but were not shown to the tutors. Video quality was ranked based on factors such as obscured foreheads due to hats or hair, and improper camera position resulting in students' faces not being fully captured on the video. The highest-quality set contained 14 videos, and these videos were used in this analysis. They have a total running time of 11 hours and 55 minutes, and include dialogues with three female subjects and eleven male subjects.",
                "cite_spans": [
                    {
                        "start": 124,
                        "end": 155,
                        "text": "(Boyer, Phillips, et al., 2010)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Task-Oriented Dialogue Corpus",
                "sec_num": "3"
            },
            {
                "text": "The dialogue act annotation scheme (Table 1 ) was applied manually. The kappa statistic for interannotator agreement on a 10% subset of the corpus was \u03ba=0.80, indicating good reliability. ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 42,
                        "end": 43,
                        "text": "1",
                        "ref_id": "TABREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Dialogue act annotation",
                "sec_num": "3.1"
            },
            {
                "text": "The tutoring sessions were task-oriented, focusing on a computer programming exercise. The task had several subtasks consisting of programming modules to be implemented by the student. Each of those subtasks also had numerous fine-grained goals, and student task actions either contributed or did not contribute to the goals. Therefore, to obtain a rich representation of the task, a manual annotation along two dimensions was conducted (Boyer, Phillips, et al., 2010) . First, the subtask structure was annotated hierarchically, and then each task action was labeled for correctness according to the requirements of the assignment. Inter-annotator agreement was computed on 20% of the corpus at the leaves of the subtask tagging scheme, and re-sulted in a simple kappa of \u03ba=.56. However, the leaves of the annotation scheme feature an implicit ordering (subtasks were completed in order, and adjacent subtasks are semantically more similar than subtasks at a greater distance); therefore, a weighted kappa is also meaningful to consider for this annotation. The weighted kappa is \u03ba weighted =.80. An annotated excerpt of the corpus is displayed in Table 2 . ",
                "cite_spans": [
                    {
                        "start": 437,
                        "end": 468,
                        "text": "(Boyer, Phillips, et al., 2010)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 1155,
                        "end": 1156,
                        "text": "2",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Task action annotation",
                "sec_num": "3.2"
            },
            {
                "text": "In addition to the manually annotated dialogue and task features described above, syntactic features of each utterance were automatically extracted using the Stanford Parser (De Marneffe et al., 2006) .",
                "cite_spans": [
                    {
                        "start": 174,
                        "end": 200,
                        "text": "(De Marneffe et al., 2006)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Lexical and Syntactic Features",
                "sec_num": "3.3"
            },
            {
                "text": "From the phrase structure trees, we extracted the top-most syntactic node and its first two children.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Lexical and Syntactic Features",
                "sec_num": "3.3"
            },
            {
                "text": "In the case where an utterance consisted of more than one sentence, only the phrase structure tree of the first sentence was considered. Individual word tokens in the utterances were further processed with the Porter Stemmer (Porter, 1980) in the NLTK package (Loper & Bird, 2004) . Our prior work has shown that these lexical and syntactic features are highly predictive of dialogue acts during task-oriented tutorial dialogue (Boyer, Ha et al. 2010) .",
                "cite_spans": [
                    {
                        "start": 225,
                        "end": 239,
                        "text": "(Porter, 1980)",
                        "ref_id": null
                    },
                    {
                        "start": 260,
                        "end": 280,
                        "text": "(Loper & Bird, 2004)",
                        "ref_id": null
                    },
                    {
                        "start": 428,
                        "end": 451,
                        "text": "(Boyer, Ha et al. 2010)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Lexical and Syntactic Features",
                "sec_num": "3.3"
            },
            {
                "text": "An annotator who was certified in the Facial Action Coding System (FACS) (Ekman, Friesen, & Hager, 2002) tagged the video corpus consisting of fourteen dialogues. The FACS certification process requires annotators to pass a test designed to analyze their agreement with reference coders on a set of spontaneous facial expressions (Ekman & Rosenberg, 2005) . This annotator viewed the videos continuously and paused the playback whenever notable facial displays of Action Unit 4 (AU4: Brow Lowerer) were seen. This action unit was chosen for this study based on its correlations with confusion in prior research (Craig, D'Mello, Witherspoon, Sullins, & Graesser, 2004; D'Mello, Craig, Sullins, & Graesser, 2006; McDaniel et al., 2007) .",
                "cite_spans": [
                    {
                        "start": 73,
                        "end": 104,
                        "text": "(Ekman, Friesen, & Hager, 2002)",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 330,
                        "end": 355,
                        "text": "(Ekman & Rosenberg, 2005)",
                        "ref_id": null
                    },
                    {
                        "start": 611,
                        "end": 667,
                        "text": "(Craig, D'Mello, Witherspoon, Sullins, & Graesser, 2004;",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 668,
                        "end": 710,
                        "text": "D'Mello, Craig, Sullins, & Graesser, 2006;",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 711,
                        "end": 733,
                        "text": "McDaniel et al., 2007)",
                        "ref_id": "BIBREF29"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Facial Action Tagging",
                "sec_num": "4"
            },
            {
                "text": "To establish reliability of the annotation, a second FACS-certified annotator independently annotated 36% of the video corpus (5 of 14 dialogues), chosen randomly after stratification by gender and tutor. This annotator followed the same method as the first annotator, pausing the video at any point to tag facial action events. At any given time in the video, the coder was first identifying whether an action unit event existed, and then describing the facial movements that were present. The annotators also specified the beginning and ending time of each event. In this way, the action unit event tags spanned discrete durations of varying length, as specified by the coders. Because the two coders were not required to tag at the same point in time, but rather were permitted the freedom to stop the video at any point where they felt a notable facial action event occurred, calculating agreement between annotators required discretizing the continuous facial action time windows across the tutoring sessions. This discretization was performed at granularities of 1/4, 1/2, 3/4, and 1 second, and inter-rater reliability was calculated at each level of granularity (Table 3 ). Windows in which both annotators agreed that no facial action event was present were tagged by default as neutral. Figure 1 illustrates facial expressions that display facial Action Unit 4. Despite the fact that promising automatic approaches exist to identifying many facial action units (Bartlett et al., 2006; Cohn, Reed, Ambadar, Xiao, & Moriyama, 2004; Pantic & Bartlett, 2007; Zeng, Pantic, Roisman, & Huang, 2009) , manual annotation was selected for this project for two reasons. First, manual annotation is more robust than automatic recognition of facial action units, and manual annotation facilitated an exploratory, comprehensive view of student facial expressions during learning through task-oriented dialogue. Although a detailed discussion of the other emotions present in the corpus is beyond the scope of this paper, Figure 2 illustrates some other spontaneous student facial expressions that differ from those associated with confusion. ",
                "cite_spans": [
                    {
                        "start": 1471,
                        "end": 1494,
                        "text": "(Bartlett et al., 2006;",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 1495,
                        "end": 1539,
                        "text": "Cohn, Reed, Ambadar, Xiao, & Moriyama, 2004;",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 1540,
                        "end": 1564,
                        "text": "Pantic & Bartlett, 2007;",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 1565,
                        "end": 1602,
                        "text": "Zeng, Pantic, Roisman, & Huang, 2009)",
                        "ref_id": "BIBREF42"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 1177,
                        "end": 1178,
                        "text": "3",
                        "ref_id": "TABREF2"
                    },
                    {
                        "start": 1304,
                        "end": 1305,
                        "text": "1",
                        "ref_id": null
                    },
                    {
                        "start": 2025,
                        "end": 2026,
                        "text": "2",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Facial Action Tagging",
                "sec_num": "4"
            },
            {
                "text": "The goal of the modeling experiment was to determine whether the addition of confusion-related facial expression features significantly boosts dialogue act classification accuracy for student utterances.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Models",
                "sec_num": "5"
            },
            {
                "text": "We take a vector-based approach, in which the features consist of the following:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Features",
                "sec_num": "5.1"
            },
            {
                "text": "\u2022 Dialogue act features: Manually annotated dialogue act for the past three utterances. These features include tutor dialogue acts, annotated with a scheme analogous to that used to annotate student utterances (Boyer et al., 2009) . ",
                "cite_spans": [
                    {
                        "start": 210,
                        "end": 230,
                        "text": "(Boyer et al., 2009)",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Utterance Features",
                "sec_num": null
            },
            {
                "text": "A logistic regression approach was used to classify the dialogue acts based on the above feature vectors. The Weka machine learning toolkit (Hall et al., 2009) was used to learn the models and to first perform feature selection in a best-first search. Logistic regression is a generalized maximum likelihood model that discriminates between pairs of output values by calculating a feature weight vector over the predictors. The goal of this work is to explore the utility of confusion-related facial features in the context of particular dialogue act types. For this reason, a specialized classifier was learned by dialogue act.",
                "cite_spans": [
                    {
                        "start": 140,
                        "end": 159,
                        "text": "(Hall et al., 2009)",
                        "ref_id": "BIBREF24"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Modeling Approach",
                "sec_num": "5.2"
            },
            {
                "text": "The classification accuracy and kappa for each specialized classifier is displayed in Table 4 . Note that kappa statistics adjust for the accuracy that would be expected by majority-baseline chance; a kappa statistic of zero indicates that the classifier performed equal to chance, and a positive kappa statistic indicates that the classifier performed better than chance. A kappa of 1 constitutes perfect agreement. As the table illustrates, the feature selection chose to utilize the AU4 feature for every dialogue act except STATEMENT (S). When considering the accuracy of the model across the ten folds, two of the affect-enriched classifiers exhibited statistically significantly better performance. For GROUNDING (G) and REQUEST FOR FEEDBACK (RF), the facial expression features significantly improved the classification accuracy compared to a model that was learned without affective features.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 92,
                        "end": 93,
                        "text": "4",
                        "ref_id": "TABREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Classification Results",
                "sec_num": "5.3"
            },
            {
                "text": "Dialogue act classification is an essential task for dialogue systems, and it has been addressed with a variety of modeling approaches and feature sets. We have presented a novel approach that treats facial expressions of students as constraining features for an affect-enriched dialogue act classification model in task-oriented tutorial dialogue. The results suggest that knowledge of the student's confusion-related facial expressions can significantly enhance dialogue act classification for two types of dialogue acts, GROUNDING and REQUEST FOR FEEDBACK. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discussion",
                "sec_num": "6"
            },
            {
                "text": "Out of more than 1500 features available during feature selection, each of the specialized dialogue act classifiers selected between 30 and 50 features in each condition (with and without affect features). To gain insight into the specific features that were useful for classifying these dialogue acts, it is useful to examine which of the AU4 history features were chosen during feature selection.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Features Selected for Classification",
                "sec_num": "6.1"
            },
            {
                "text": "For GROUNDING, features that indicated the presence of absence of AU4 in the immediately preceding utterance, either at the 1 second or 5 second granularity, were selected. Absence of this confusion-related facial action unit was associated with a higher probability of a grounding act, such as an acknowledgement. This finding is consistent with our understanding of how students and tutors interacted in this corpus; when a student experienced confusion, she would be unlikely to then make a simple grounding dialogue move, but instead would tend to inspect her computer program, ask a question, or wait for the tutor to explain more.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Features Selected for Classification",
                "sec_num": "6.1"
            },
            {
                "text": "For REQUEST FOR FEEDBACK, the predictive features were presence or absence of AU4 within ten seconds of the longest available history (three turns in the past), as well as the presence of AU4 within five seconds of the current utterance (the utterance whose dialogue act is being classified). This finding suggests that there may be some lag between the student experiencing confusion and then choosing to make a request for feedback, and that the confusion-related facial expressions may re-emerge as the student is making a request for feedback, since the five-second window prior to the student sending the textual dialogue message would overlap with the student's construction of the message itself.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Features Selected for Classification",
                "sec_num": "6.1"
            },
            {
                "text": "Although the improvements seen with AU4 features for QUESTION, POSITIVE FEEDBACK, and EXTRA-DOMAIN acts were not statistically reliable, examining the AU4 features that were selected for classifying these moves points toward ways in which facial expressions may influence classification of these acts (Table 5 ). ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 308,
                        "end": 309,
                        "text": "5",
                        "ref_id": "TABREF5"
                    }
                ],
                "eq_spans": [],
                "section": "Features Selected for Classification",
                "sec_num": "6.1"
            },
            {
                "text": "The results presented here demonstrate that leveraging knowledge of user affect, in particular of spontaneous facial expressions, may improve the performance of dialogue act classification models. Perhaps most interestingly, displays of confusionrelated facial actions prior to a student dialogue move enabled an affect-enriched classifier to recognize requests for feedback with significantly greater accuracy than a classifier that did not have access to the facial action features. Feedback is known to be a key component of effective tutorial dialogue, through which tutors provide adaptive help (Shute, 2008) . Requesting feedback also seems to be an important behavior of students, characteristically engaged in more frequently by women than men, and more frequently by students with lower incoming knowledge than by students with higher incoming knowledge (Boyer, Vouk, & Lester, 2007) .",
                "cite_spans": [
                    {
                        "start": 600,
                        "end": 613,
                        "text": "(Shute, 2008)",
                        "ref_id": null
                    },
                    {
                        "start": 863,
                        "end": 892,
                        "text": "(Boyer, Vouk, & Lester, 2007)",
                        "ref_id": "BIBREF2"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implications",
                "sec_num": "6.2"
            },
            {
                "text": "The experiments reported here have several notable limitations. First, the time-consuming nature of manual facial action tagging restricted the number of dialogues that could be tagged. Although the highest quality videos were selected for annotation, other medium quality videos would have been sufficiently clear to permit tagging, which would have increased the sample size and likely revealed statistically significant trends. For example, the per-formance of the affect-enriched classifier was better for dialogue acts of interest such as positive feedback and questions, but this difference was not statistically reliable. An additional limitation stems from the more fundamental question of which affective states are indicated by particular external displays. The field is only just beginning to understand facial expressions during learning and to correlate these facial actions with emotions. Additional research into the \"ground truth\" of emotion expression will shed additional light on this area. Finally, the results of manual facial action annotation may constitute upper-bound findings for applying automatic facial expression analysis to dialogue act classification.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Limitations",
                "sec_num": "6.3"
            },
            {
                "text": "Emotion plays a vital role in human interactions. In particular, the role of facial expressions in humanhuman dialogue is widely recognized. Facial expressions offer a promising channel for understanding the emotions experienced by users of dialogue systems, particularly given the ubiquity of webcam technologies and the increasing number of dialogue systems that are deployed on webcamenabled devices. This paper has reported on a first step toward using knowledge of user facial expressions to improve a dialogue act classification model for tutorial dialogue, and the results demonstrate that facial expressions hold great promise for distinguishing the pedagogically relevant dialogue act REQUEST FOR FEEDBACK, and the conversational moves of GROUNDING.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusions and Future Work",
                "sec_num": "7"
            },
            {
                "text": "These early findings highlight the importance of future work in this area. Dialogue act classification models have not fully leveraged some of the techniques emerging from work on sentiment analysis. These approaches may prove particularly useful for identifying emotions in dialogue utterances. Another important direction for future work involves more fully exploring the ways in which affect expression differs between textual and spoken dialogue. Finally, as automatic facial tagging technologies mature, they may prove powerful enough to enable broadly deployed dialogue systems to feasibly leverage facial expression data in the near future.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusions and Future Work",
                "sec_num": "7"
            }
        ],
        "back_matter": [
            {
                "text": "This work is supported in part by the North Carolina State University Department of Computer Science and by the National Science Foundation through Grants REC-0632450, IIS-0812291, DRL-1007962 and the STARS Alliance Grant CNS-0739216. Any opinions, findings, conclusions, or recommendations expressed in this report are those of the participants, and do not necessarily represent the official views, opinions, or policy of the National Science Foundation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgments",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "When specialists and generalists work together: Overcoming domain dependence in sentiment tagging",
                "authors": [
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Andreevskaia",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Bergler",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics and Human Language Technologies (ACL HLT)",
                "volume": "",
                "issue": "",
                "pages": "290--298",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "A. Andreevskaia and S. Bergler. 2008. When specialists and generalists work together: Overcoming do- main dependence in sentiment tagging. Proceed- ings of the Annual Meeting of the Association for Computational Linguistics and Human Language Technologies (ACL HLT), 290-298.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Fully Automatic Facial Action Recognition in Spontaneous Behavior. 7th International Conference on Automatic Face and Gesture Recognition (FGR06)",
                "authors": [
                    {
                        "first": "M",
                        "middle": [
                            "S"
                        ],
                        "last": "Bartlett",
                        "suffix": ""
                    },
                    {
                        "first": "G",
                        "middle": [],
                        "last": "Littlewort",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Frank",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Lainscsek",
                        "suffix": ""
                    },
                    {
                        "first": "I",
                        "middle": [],
                        "last": "Fasel",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Movellan",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "223--230",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "M.S. Bartlett, G. Littlewort, M. Frank, C. Lainscsek, I. Fasel, and J. Movellan. 2006. Fully Automatic Facial Action Recognition in Spontaneous Behav- ior. 7th International Conference on Automatic Face and Gesture Recognition (FGR06), 223-230.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "The influence of learner characteristics on task-oriented tutorial dialogue",
                "authors": [
                    {
                        "first": "K",
                        "middle": [
                            "E"
                        ],
                        "last": "Boyer",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Vouk",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [
                            "C"
                        ],
                        "last": "Lester",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Proceedings of the International Conference on Artificial Intelligence in Education",
                "volume": "",
                "issue": "",
                "pages": "365--372",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "K.E. Boyer, M. Vouk, and J.C. Lester. 2007. The influ- ence of learner characteristics on task-oriented tu- torial dialogue. Proceedings of the International Conference on Artificial Intelligence in Educa- tion, 365-372.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Dialogue act modeling in a complex task-oriented domain",
                "authors": [
                    {
                        "first": "K",
                        "middle": [
                            "E"
                        ],
                        "last": "Boyer",
                        "suffix": ""
                    },
                    {
                        "first": "E",
                        "middle": [
                            "Y"
                        ],
                        "last": "Ha",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Phillips",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [
                            "D"
                        ],
                        "last": "Wallis",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Vouk",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [
                            "C"
                        ],
                        "last": "Lester",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL)",
                "volume": "",
                "issue": "",
                "pages": "297--305",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "K.E. Boyer, E.Y. Ha, R. Phillips, M.D. Wallis, M. Vouk, and J.C. Lester. 2010. Dialogue act model- ing in a complex task-oriented domain. Proceed- ings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), 297-305.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Modeling dialogue structure with adjacency pair analysis and hidden Markov models",
                "authors": [
                    {
                        "first": "K",
                        "middle": [
                            "E"
                        ],
                        "last": "Boyer",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Phillips",
                        "suffix": ""
                    },
                    {
                        "first": "E",
                        "middle": [
                            "Y"
                        ],
                        "last": "Ha",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [
                            "D"
                        ],
                        "last": "Wallis",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [
                            "A"
                        ],
                        "last": "Vouk",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [
                            "C"
                        ],
                        "last": "Lester",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics: Short Papers",
                "volume": "",
                "issue": "",
                "pages": "49--52",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "K.E. Boyer, R. Phillips, E.Y. Ha, M.D. Wallis, M.A. Vouk, and J.C. Lester. 2009. Modeling dialogue structure with adjacency pair analysis and hidden Markov models. Proceedings of the Annual Con- ference of the North American Chapter of the As- sociation for Computational Linguistics: Short Papers, 49-52.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Leveraging hidden dialogue state to select tutorial moves",
                "authors": [
                    {
                        "first": "K",
                        "middle": [
                            "E"
                        ],
                        "last": "Boyer",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Phillips",
                        "suffix": ""
                    },
                    {
                        "first": "E",
                        "middle": [
                            "Y"
                        ],
                        "last": "Ha",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [
                            "D"
                        ],
                        "last": "Wallis",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [
                            "A"
                        ],
                        "last": "Vouk",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [
                            "C"
                        ],
                        "last": "Lester",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications",
                "volume": "",
                "issue": "",
                "pages": "66--73",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "K.E. Boyer, R. Phillips, E.Y. Ha, M.D. Wallis, M.A. Vouk, and J.C. Lester. 2010. Leveraging hidden dialogue state to select tutorial moves. Proceed- ings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, 66-73.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Affect Detection: An Interdisciplinary Review of Models, Methods, and Their Applications",
                "authors": [
                    {
                        "first": "R",
                        "middle": [
                            "A"
                        ],
                        "last": "Calvo",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [
                            "D"
                        ],
                        "last": "Mello",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "IEEE Transactions on Affective Computing",
                "volume": "1",
                "issue": "1",
                "pages": "18--37",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "R.A. Calvo and S. D'Mello. 2010. Affect Detection: An Interdisciplinary Review of Models, Methods, and Their Applications. IEEE Transactions on Affec- tive Computing, 1(1): 18-37.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "How was your day? An affective companion ECA prototype",
                "authors": [
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Cavazza",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [
                            "S D L"
                        ],
                        "last": "C\u00e1mara",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Turunen",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Gil",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Hakulinen",
                        "suffix": ""
                    },
                    {
                        "first": "N",
                        "middle": [],
                        "last": "Crook",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL)",
                "volume": "",
                "issue": "",
                "pages": "277--280",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "M. Cavazza, R.S.D.L. C\u00e1mara, M. Turunen, J. Gil, J. Hakulinen, N. Crook, et al. 2010. How was your day? An affective companion ECA prototype. Proceedings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), 277-280.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "The modulation of cooperation and emotion in dialogue: the REC Corpus",
                "authors": [
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Cavicchio",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of the ACL-IJCNLP 2009 Student Research Workshop",
                "volume": "",
                "issue": "",
                "pages": "43--48",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "F. Cavicchio. 2009. The modulation of cooperation and emotion in dialogue: the REC Corpus. Proceed- ings of the ACL-IJCNLP 2009 Student Research Workshop, 43-48.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Automatic Analysis and Recognition of Brow Actions and Head Motion in Spontaneous Facial Behavior",
                "authors": [
                    {
                        "first": "J",
                        "middle": [
                            "F"
                        ],
                        "last": "Cohn",
                        "suffix": ""
                    },
                    {
                        "first": "L",
                        "middle": [
                            "I"
                        ],
                        "last": "Reed",
                        "suffix": ""
                    },
                    {
                        "first": "Z",
                        "middle": [],
                        "last": "Ambadar",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Xiao",
                        "suffix": ""
                    },
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Moriyama",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "IEEE International Conference on Systems, Man and Cybernetics",
                "volume": "",
                "issue": "",
                "pages": "610--616",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "J.F. Cohn, L.I. Reed, Z. Ambadar, J. Xiao, and T. Mori- yama. 2004. Automatic Analysis and Recognition of Brow Actions and Head Motion in Spontaneous Facial Behavior. IEEE International Conference on Systems, Man and Cybernetics, 610-616.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Emotions during learning: The first steps toward an affect sensitive intelligent tutoring system",
                "authors": [
                    {
                        "first": "S",
                        "middle": [
                            "D"
                        ],
                        "last": "Craig",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "D'mello",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Witherspoon",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Sullins",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [
                            "C"
                        ],
                        "last": "Graesser",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "E-learn 2004: World conference on Elearning in Corporate",
                "volume": "",
                "issue": "",
                "pages": "241--250",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "S.D. Craig, S. D'Mello, A. Witherspoon, J. Sullins, and A.C. Graesser. 2004. Emotions during learning: The first steps toward an affect sensitive intelli- gent tutoring system. In J. Nall and R. Robson (Eds.), E-learn 2004: World conference on E- learning in Corporate, Government, Healthcare, & Higher Education, 241-250.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Word to sentence level emotion tagging for Bengali blogs",
                "authors": [
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Das",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Bandyopadhyay",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of the ACL-IJCNLP Conference",
                "volume": "",
                "issue": "",
                "pages": "149--152",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "D. Das and S. Bandyopadhyay. 2009. Word to sentence level emotion tagging for Bengali blogs. Proceed- ings of the ACL-IJCNLP Conference, Short Pa- pers, 149-152.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Mine the easy, classify the hard: a semi-supervised approach to automatic sentiment classification",
                "authors": [
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Dasgupta",
                        "suffix": ""
                    },
                    {
                        "first": "V",
                        "middle": [],
                        "last": "Ng",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of the 46th Annual Meeting of the ACL and the 4th IJCNLP",
                "volume": "",
                "issue": "",
                "pages": "701--709",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "S. Dasgupta and V. Ng. 2009. Mine the easy, classify the hard: a semi-supervised approach to automatic sentiment classification. Proceedings of the 46th Annual Meeting of the ACL and the 4th IJCNLP, 701-709.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Dialogue Act Classification, Higher Order Dialogue Structure, and Instance-Based Learning",
                "authors": [
                    {
                        "first": "B",
                        "middle": [],
                        "last": "Di Eugenio",
                        "suffix": ""
                    },
                    {
                        "first": "Z",
                        "middle": [],
                        "last": "Xie",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Serafin",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Dialogue & Discourse",
                "volume": "1",
                "issue": "2",
                "pages": "1--24",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "B. Di Eugenio, Z. Xie, and R. Serafin. 2010. Dialogue Act Classification, Higher Order Dialogue Struc- ture, and Instance-Based Learning. Dialogue & Discourse, 1(2): 1-24.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "The impact of interpretation problems on tutorial dialogue",
                "authors": [
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Dzikovska",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [
                            "D"
                        ],
                        "last": "Moore",
                        "suffix": ""
                    },
                    {
                        "first": "N",
                        "middle": [],
                        "last": "Steinhauser",
                        "suffix": ""
                    },
                    {
                        "first": "G",
                        "middle": [],
                        "last": "Campbell",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "43--48",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "M. Dzikovska, J.D. Moore, N. Steinhauser, and G. Campbell. 2010. The impact of interpretation problems on tutorial dialogue. Proceedings of the 48th Annual Meeting of the Association for Com- putational Linguistics, Short Papers, 43-48.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Predicting Affective States expressed through an Emote-Aloud Procedure from AutoTutor's Mixed-Initiative Dialogue",
                "authors": [
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Mello",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [
                            "D"
                        ],
                        "last": "Craig",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Sullins",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [
                            "C"
                        ],
                        "last": "Graesser",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "International Journal of Artificial Intelligence in Education",
                "volume": "16",
                "issue": "1",
                "pages": "3--28",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "S. D'Mello, S.D. Craig, J. Sullins, and A.C. Graesser. 2006. Predicting Affective States expressed through an Emote-Aloud Procedure from AutoTu- tor's Mixed-Initiative Dialogue. International Journal of Artificial Intelligence in Education, 16(1): 3-28.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Basic Emotions",
                "authors": [
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Ekman",
                        "suffix": ""
                    }
                ],
                "year": 1999,
                "venue": "Handbook of Cognition and Emotion",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "P. Ekman. 1999. Basic Emotions. In T. Dalgleish and M. J. Power (Eds.), Handbook of Cognition and Emotion. New York: Wiley.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Facial Action Coding System",
                "authors": [
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Ekman",
                        "suffix": ""
                    },
                    {
                        "first": "W",
                        "middle": [
                            "V"
                        ],
                        "last": "Friesen",
                        "suffix": ""
                    }
                ],
                "year": 1978,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "P. Ekman, W.V. Friesen. 1978. Facial Action Coding System. Palo Alto, CA: Consulting Psychologists Press.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Facial Action Coding System: Investigator's Guide",
                "authors": [
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Ekman",
                        "suffix": ""
                    },
                    {
                        "first": "W",
                        "middle": [
                            "V"
                        ],
                        "last": "Friesen",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [
                            "C"
                        ],
                        "last": "Hager",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "P. Ekman, W.V. Friesen, and J.C. Hager. 2002. Facial Action Coding System: Investigator's Guide. Salt Lake City, USA: A Human Face.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "What the Face Reveals: Basic and Applied Studies of Spontaneous Expression Using the Facial Action Coding System (FACS)",
                "authors": [],
                "year": 2005,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "P. Ekman and E.L. Rosenberg (Eds.). 2005. What the Face Reveals: Basic and Applied Studies of Spon- taneous Expression Using the Facial Action Cod- ing System (FACS) (2nd ed.). New York: Oxford University Press.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Exploring affect-context dependencies for adaptive system development. The Conference of the North American Chapter",
                "authors": [
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Forbes-Riley",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Rotaru",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [
                            "J"
                        ],
                        "last": "Litman",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Tetreault",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "41--44",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "K. Forbes-Riley, M. Rotaru, D.J. Litman, and J. Tetreault. 2007. Exploring affect-context depend- encies for adaptive system development. The Con- ference of the North American Chapter of the Association for Computational Linguistics and Human Language Technologies (NAACL HLT), Short Papers, 41-44.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Adapting to student uncertainty improves tutoring dialogues",
                "authors": [
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Forbes-Riley",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Rotaru",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [
                            "J"
                        ],
                        "last": "Litman",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Tetreault",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of the 14th International Conference on Artificial Intelligence in Education (AIED)",
                "volume": "",
                "issue": "",
                "pages": "33--40",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "K. Forbes-Riley, M. Rotaru, D.J. Litman, and J. Tetreault. 2009. Adapting to student uncertainty improves tutoring dialogues. Proceedings of the 14th International Conference on Artificial Intelli- gence in Education (AIED), 33-40.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Question asking and eye tracking during cognitive disequilibrium: comprehending illustrated texts on devices when the devices break down",
                "authors": [
                    {
                        "first": "A",
                        "middle": [
                            "C"
                        ],
                        "last": "Graesser",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Lu",
                        "suffix": ""
                    },
                    {
                        "first": "B",
                        "middle": [],
                        "last": "Olde",
                        "suffix": ""
                    },
                    {
                        "first": "E",
                        "middle": [],
                        "last": "Cooper-Pye",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Whitten",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Memory & Cognition",
                "volume": "33",
                "issue": "7",
                "pages": "1235--1247",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "A.C. Graesser, S. Lu, B. Olde, E. Cooper-Pye, and S. Whitten. 2005. Question asking and eye tracking during cognitive disequilibrium: comprehending illustrated texts on devices when the devices break down. Memory & Cognition, 33(7): 1235-1247.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "More than words: Syntactic packaging and implicit sentiment",
                "authors": [
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Greene",
                        "suffix": ""
                    },
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Resnik",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of the 2009 Annual Conference of the North American Chapter of the ACL and Human Language Technologies (NAACL HLT)",
                "volume": "",
                "issue": "",
                "pages": "503--511",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "S. Greene and P. Resnik. 2009. More than words: Syn- tactic packaging and implicit sentiment. Proceed- ings of the 2009 Annual Conference of the North American Chapter of the ACL and Human Lan- guage Technologies (NAACL HLT), 503-511.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "The WEKA data mining software: An update",
                "authors": [
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Hall",
                        "suffix": ""
                    },
                    {
                        "first": "E",
                        "middle": [],
                        "last": "Frank",
                        "suffix": ""
                    },
                    {
                        "first": "G",
                        "middle": [],
                        "last": "Holmes",
                        "suffix": ""
                    },
                    {
                        "first": "B",
                        "middle": [],
                        "last": "Pfahringer",
                        "suffix": ""
                    },
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Reutemann",
                        "suffix": ""
                    },
                    {
                        "first": "I",
                        "middle": [
                            "H"
                        ],
                        "last": "Witten",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "",
                "volume": "11",
                "issue": "",
                "pages": "10--18",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute- mann, and I.H. Witten. 2009. The WEKA data mining software: An update. SIGKDD Explora- tions, 11(1): 10-18.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Incorporating extra-linguistic information into reference resolution in collaborative task dialogue",
                "authors": [
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Iida",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Kobayashi",
                        "suffix": ""
                    },
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Tokunaga",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "1259--1267",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "R. Iida, S. Kobayashi, and T. Tokunaga. 2010. Incorpo- rating extra-linguistic information into reference resolution in collaborative task dialogue. Proceed- ings of the 48th Annual Meeting of the Associa- tion for Computational Linguistics, 1259-1267.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Nonverbal Communication in Human Interaction",
                "authors": [
                    {
                        "first": "M",
                        "middle": [
                            "L"
                        ],
                        "last": "Knapp",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [
                            "A"
                        ],
                        "last": "Hall",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "M.L. Knapp and J.A. Hall. 2006. Nonverbal Communi- cation in Human Interaction (6th ed.). Belmont, CA: Wadsworth/Thomson Learning.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Toward detecting emotions in spoken dialogs",
                "authors": [
                    {
                        "first": "C",
                        "middle": [
                            "M"
                        ],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [
                            "S"
                        ],
                        "last": "Narayanan",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "IEEE Transactions on Speech and Audio Processing",
                "volume": "13",
                "issue": "2",
                "pages": "293--303",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "C.M. Lee, S.S. Narayanan. 2005. Toward detecting emotions in spoken dialogs. IEEE Transactions on Speech and Audio Processing, 13(2): 293-303.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "F2-New Technique for Recognition of User Emotional States in Spoken Dialogue Systems",
                "authors": [
                    {
                        "first": "R",
                        "middle": [],
                        "last": "L\u00f3pez-C\u00f3zar",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Silovsky",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Griol",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL)",
                "volume": "",
                "issue": "",
                "pages": "281--288",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "R. L\u00f3pez-C\u00f3zar, J. Silovsky, and D. Griol. 2010. F2- New Technique for Recognition of User Emotion- al States in Spoken Dialogue Systems. Proceed- ings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), 281-288.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Facial Features for Affective State Detection in Learning Environments",
                "authors": [
                    {
                        "first": "B",
                        "middle": [
                            "T"
                        ],
                        "last": "Mcdaniel",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "D'mello",
                        "suffix": ""
                    },
                    {
                        "first": "B",
                        "middle": [
                            "G"
                        ],
                        "last": "King",
                        "suffix": ""
                    },
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Chipman",
                        "suffix": ""
                    },
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Tapp",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [
                            "C"
                        ],
                        "last": "Graesser",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Proceedings of the 29th Annual Cognitive Science Society",
                "volume": "",
                "issue": "",
                "pages": "467--472",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "B.T. McDaniel, S. D'Mello, B.G. King, P. Chipman, K. Tapp, and A.C. Graesser. 2007. Facial Features for Affective State Detection in Learning Envi- ronments. Proceedings of the 29th Annual Cogni- tive Science Society, 467-472.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "Hand and mind: What gestures reveal about thought",
                "authors": [
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Mcneill",
                        "suffix": ""
                    }
                ],
                "year": 1992,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "D. McNeill. 1992. Hand and mind: What gestures reveal about thought. Chicago: University of Chicago Press.",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "Nonverbal Communication",
                "authors": [
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Mehrabian",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "A. Mehrabian. 2007. Nonverbal Communication. New Brunswick, NJ: Aldine Transaction.",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "Mood patterns and affective lexicon access in weblogs",
                "authors": [
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Nguyen",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the ACL 2010 Student Research Workshop",
                "volume": "",
                "issue": "",
                "pages": "43--48",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "T. Nguyen. 2010. Mood patterns and affective lexicon access in weblogs. Proceedings of the ACL 2010 Student Research Workshop, 43-48.",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "Machine Analysis of Facial Expressions",
                "authors": [
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Pantic",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [
                            "S"
                        ],
                        "last": "Bartlett",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Face Recognition",
                "volume": "",
                "issue": "",
                "pages": "377--416",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "M. Pantic and M.S. Bartlett. 2007. Machine Analysis of Facial Expressions. In K. Delac and M. Grgic (Eds.), Face Recognition, 377-416. Vienna, Aus- tria: I-Tech Education and Publishing.",
                "links": null
            },
            "BIBREF34": {
                "ref_id": "b34",
                "title": "Core affect and the psychological construction of emotion",
                "authors": [
                    {
                        "first": "J",
                        "middle": [
                            "A"
                        ],
                        "last": "Russell",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Psychological Review",
                "volume": "110",
                "issue": "1",
                "pages": "145--172",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "J.A. Russell. 2003. Core affect and the psychological construction of emotion. Psychological Review, 110(1): 145-172.",
                "links": null
            },
            "BIBREF35": {
                "ref_id": "b35",
                "title": "Facial and vocal expressions of emotion",
                "authors": [
                    {
                        "first": "J",
                        "middle": [
                            "A"
                        ],
                        "last": "Russell",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [
                            "A"
                        ],
                        "last": "Bachorowski",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [
                            "M"
                        ],
                        "last": "Fernandez-Dols",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Annual Review of Psychology",
                "volume": "54",
                "issue": "",
                "pages": "329--349",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "J.A. Russell, J.A. Bachorowski, and J.M. Fernandez- Dols. 2003. Facial and vocal expressions of emo- tion. Annual Review of Psychology, 54, 329-49.",
                "links": null
            },
            "BIBREF36": {
                "ref_id": "b36",
                "title": "Human Facial Expressions as Adaptations: Evolutionary Questions in Facial Expression Research",
                "authors": [
                    {
                        "first": "K",
                        "middle": [
                            "L"
                        ],
                        "last": "Schmidt",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [
                            "F"
                        ],
                        "last": "Cohn",
                        "suffix": ""
                    }
                ],
                "year": 2001,
                "venue": "Am J Phys Anthropol",
                "volume": "33",
                "issue": "",
                "pages": "3--24",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "K.L. Schmidt and J.F. Cohn. 2001. Human Facial Ex- pressions as Adaptations: Evolutionary Questions in Facial Expression Research. Am J Phys An- thropol, 33: 3-24.",
                "links": null
            },
            "BIBREF38": {
                "ref_id": "b38",
                "title": "Combining lexical, syntactic and prosodic cues for improved online dialog act tagging",
                "authors": [
                    {
                        "first": "V",
                        "middle": [
                            "K"
                        ],
                        "last": "Sridar",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Bangalore",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [
                            "S"
                        ],
                        "last": "Narayanan",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Computer Speech & Language",
                "volume": "23",
                "issue": "4",
                "pages": "407--422",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "V.K.R Sridar, S. Bangalore, and S.S. Narayanan. 2009. Combining lexical, syntactic and prosodic cues for improved online dialog act tagging. Computer Speech & Language, 23(4): 407-422. Elsevier Ltd.",
                "links": null
            },
            "BIBREF39": {
                "ref_id": "b39",
                "title": "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech",
                "authors": [
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Stolcke",
                        "suffix": ""
                    },
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Ries",
                        "suffix": ""
                    },
                    {
                        "first": "N",
                        "middle": [],
                        "last": "Coccaro",
                        "suffix": ""
                    },
                    {
                        "first": "E",
                        "middle": [],
                        "last": "Shriberg",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Bates",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Jurafsky",
                        "suffix": ""
                    }
                ],
                "year": 2000,
                "venue": "Computational Linguistics",
                "volume": "26",
                "issue": "3",
                "pages": "339--373",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "A. Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates, D. Jurafsky, et al. 2000. Dialogue Act Modeling for Automatic Tagging and Recognition of Con- versational Speech. Computational Linguistics, 26(3): 339-373.",
                "links": null
            },
            "BIBREF40": {
                "ref_id": "b40",
                "title": "Sentence and expression level annotation of opinions in user-generated discourse",
                "authors": [
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Toprak",
                        "suffix": ""
                    },
                    {
                        "first": "N",
                        "middle": [],
                        "last": "Jakob",
                        "suffix": ""
                    },
                    {
                        "first": "I",
                        "middle": [],
                        "last": "Gurevych",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "575--584",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "C. Toprak, N. Jakob, and I. Gurevych. 2010. Sentence and expression level annotation of opinions in us- er-generated discourse. Proceedings of the 48th Annual Meeting of the Association for Computa- tional Linguistics, 575-584.",
                "links": null
            },
            "BIBREF41": {
                "ref_id": "b41",
                "title": "Recognizing Contextual Polarity: An Exploration of Features for Phrase-Level Sentiment Analysis",
                "authors": [
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Wilson",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Wiebe",
                        "suffix": ""
                    },
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Hoffmann",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "",
                "volume": "35",
                "issue": "",
                "pages": "399--433",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "T. Wilson, J. Wiebe, and P. Hoffmann. 2009. Recogniz- ing Contextual Polarity: An Exploration of Fea- tures for Phrase-Level Sentiment Analysis. Computational Linguistics, 35(3): 399-433.",
                "links": null
            },
            "BIBREF42": {
                "ref_id": "b42",
                "title": "A Survey of Affect Recognition Methods: Audio, Visual, and Spontaneous Expressions",
                "authors": [
                    {
                        "first": "Z",
                        "middle": [],
                        "last": "Zeng",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Pantic",
                        "suffix": ""
                    },
                    {
                        "first": "G",
                        "middle": [
                            "I"
                        ],
                        "last": "Roisman",
                        "suffix": ""
                    },
                    {
                        "first": "T",
                        "middle": [
                            "S"
                        ],
                        "last": "Huang",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "volume": "31",
                "issue": "1",
                "pages": "39--58",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Z. Zeng, M. Pantic, G.I. Roisman, and T.S. Huang. 2009. A Survey of Affect Recognition Methods: Audio, Visual, and Spontaneous Expressions. IEEE Transactions on Pattern Analysis and Ma- chine Intelligence, 31(1): 39-58.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 2. Other facial expressions from the corpus",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Speaker: Speaker for past three utterances \u2022 Lexical features: Word unigrams \u2022 Syntactic features: Top-most syntactic node and its first two children Task-based Features \u2022 Subtask: Hierarchical subtask structure for past three task actions (semantic programming actions taken by student) \u2022 Correctness: Correctness of past three task actions taken by student \u2022 Preceded by task: Indicator for whether the most recent task action immediately preceded the target utterance, or whether it was immediately preceded by the last dialogue move Facial Expression Features \u2022 AU4_1sec: Indicator for the display of the brow lowerer within 1 second prior to this utterance being sent, for the most recent three utterances \u2022 AU4_5sec: Indicator for the display of the brow lowerer within 5 seconds prior to this utterance being sent, for the most recent three utterances \u2022 AU4_10sec: Indicator for the display of the brow lowerer within 10 seconds prior to this utterance being sent, for the most recent three utterances",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table><tr><td>Student Dialogue Act</td><td>Example</td><td>Rel. Freq.</td></tr><tr><td>EXTRA-DOMAIN (EX)</td><td>Little sleep deprived today</td><td>.08</td></tr><tr><td>GROUNDING (G)</td><td>Ok or Thanks</td><td>.21</td></tr><tr><td>NEGATIVE FEEDBACK WITH</td><td>I'm still confused on what this next for loop</td><td>.02</td></tr><tr><td>ELABORATION (NE)</td><td>is doing.</td><td/></tr><tr><td>NEGATIVE FEEDBACK (N)</td><td>I don't see the diff.</td><td>.04</td></tr><tr><td/><td>It makes sense now</td><td/></tr><tr><td>POSITIVE FEEDBACK WITH</td><td>that you explained it, but I never used an</td><td>.04</td></tr><tr><td>ELABORATION (PE)</td><td>else if in any of my</td><td/></tr><tr><td/><td>other programs</td><td/></tr><tr><td>POSITIVE FEEDBACK (P)</td><td colspan=\"2\">Second part complete. .11</td></tr><tr><td>QUESTION (Q)</td><td>Why couldn't I have said if (i&lt;5)</td><td>.11</td></tr><tr><td>STATEMENT (S)</td><td>i is my only index</td><td>.07</td></tr><tr><td/><td>So I need to create a</td><td/></tr><tr><td>REQUEST FOR FEEDBACK (RF)</td><td>new method that sees how many elements</td><td>.16</td></tr><tr><td/><td>are in my array?</td><td/></tr><tr><td>RESPONSE (RSP)</td><td>You mean not the length but the contents</td><td>.14</td></tr><tr><td>UNCERTAIN FEEDBACK WITH ELABORATION (UE)</td><td>I'm trying to remember how to copy arrays</td><td>.008</td></tr><tr><td>UNCERTAIN FEEDBACK (U)</td><td>Not quite yet</td><td>.008</td></tr></table>",
                "type_str": "table",
                "text": "Dialogue act tags and relative frequencies across fourteen dialogues in video corpus",
                "html": null,
                "num": null
            },
            "TABREF1": {
                "content": "<table><tr><td colspan=\"2\">13:38:09 Student: How do I know where to</td></tr><tr><td/><td>end? [RF]</td></tr><tr><td>13:38:26</td><td>Tutor: Well you told me how to get</td></tr><tr><td/><td>how many elements in an</td></tr><tr><td/><td>array by using .length right?</td></tr><tr><td colspan=\"2\">13:38:26 Student: [Task action:</td></tr><tr><td/><td>Subtask 1-a-iv, Buggy]</td></tr><tr><td>13:38:56</td><td>Tutor: Great</td></tr><tr><td colspan=\"2\">13:38:56 Student: [Task action:</td></tr><tr><td/><td>Subtask 1-a-v, Correct]</td></tr><tr><td colspan=\"2\">13:39:35 Student: Well is it \"array.length\"?</td></tr><tr><td/><td>[RF]</td></tr><tr><td/><td>**Facial Expression: AU4</td></tr><tr><td>13:39:46</td><td>Tutor: You just need to use the</td></tr><tr><td/><td>correct array name</td></tr><tr><td colspan=\"2\">13:39:46 Student: [Task action:</td></tr><tr><td/><td>Subtask 1-a-iv, Buggy]</td></tr></table>",
                "type_str": "table",
                "text": "Excerpt from corpus illustrating annotations and interplay between dialogue and task",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Kappa values for inter-annotator agreement on facial action events",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table><tr><td>Presence of AU4 (Brow Lowerer)</td><td>.84</td><td>.87</td><td>.86</td><td>.86</td></tr><tr><td colspan=\"5\">Figure 1. Facial expressions displaying AU4</td></tr><tr><td/><td colspan=\"2\">(Brow Lowerer)</td><td/><td/></tr></table>",
                "type_str": "table",
                "text": "",
                "html": null,
                "num": null
            },
            "TABREF4": {
                "content": "<table><tr><td/><td/><td/><td colspan=\"2\">Classifier</td><td/></tr><tr><td/><td colspan=\"2\">Classifier</td><td colspan=\"2\">without</td><td/></tr><tr><td/><td colspan=\"2\">with AU4</td><td colspan=\"2\">AU4</td><td/></tr><tr><td>Dialogue</td><td>%</td><td/><td>%</td><td/><td>p-</td></tr><tr><td>Act</td><td>acc</td><td>\u03ba</td><td>acc</td><td>\u03ba</td><td>value</td></tr><tr><td>EX</td><td colspan=\"4\">90.7 .62 89.0 .28</td><td>&gt;.05</td></tr><tr><td>G</td><td colspan=\"2\">92.6 .76</td><td>91</td><td>.71</td><td>.018</td></tr><tr><td>P</td><td>93</td><td colspan=\"3\">.49 92.2 .40</td><td>&gt;.05</td></tr><tr><td>Q</td><td colspan=\"4\">94.6 .72 94.2 .72</td><td>&gt;.05</td></tr><tr><td>S</td><td colspan=\"2\">Not chosen in feat. sel.</td><td>93</td><td>.22</td><td>n/a</td></tr><tr><td>RF</td><td colspan=\"4\">90.7 .62 88.3 .53</td><td>.003</td></tr><tr><td>RSP</td><td>93</td><td>.68</td><td>95</td><td>.75</td><td>&gt;.05</td></tr><tr><td>NE</td><td>*</td><td/><td>*</td><td/><td/></tr><tr><td>N</td><td>*</td><td/><td>*</td><td/><td/></tr><tr><td>PE</td><td>*</td><td/><td>*</td><td/><td/></tr><tr><td>U</td><td>*</td><td/><td>*</td><td/><td/></tr><tr><td>UE</td><td>*</td><td/><td>*</td><td/><td/></tr></table>",
                "type_str": "table",
                "text": "Classification accuracy and kappa for specialized DA classifiers. Statistically significant differences (across ten folds, one-tailed t-test) are shown in bold.",
                "html": null,
                "num": null
            },
            "TABREF5": {
                "content": "<table><tr><td/><td># fea-</td><td/></tr><tr><td>Dialogue</td><td>tures</td><td/></tr><tr><td>Act</td><td>selected</td><td>AU4 features selected</td></tr><tr><td>G</td><td>43</td><td>One utterance ago: AU4_1sec, AU4_5sec</td></tr><tr><td/><td/><td>Three utterances ago:</td></tr><tr><td>RF</td><td>37</td><td>AU4_10sec Target utterance:</td></tr><tr><td/><td/><td>AU4_5sec</td></tr><tr><td>EX</td><td>50</td><td>Three utterances ago: AU4_1sec</td></tr><tr><td>P</td><td>36</td><td>Current utterance: AU4_10sec</td></tr><tr><td>Q</td><td>30</td><td>One utterance ago: AU4_5sec</td></tr></table>",
                "type_str": "table",
                "text": "Number of features, and AU4 features selected, for specialized DA classifiers",
                "html": null,
                "num": null
            }
        }
    }
}