{
    "paper_id": "N16-1046",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T11:53:37.332755Z"
    },
    "title": "Agreement on Target-bidirectional Neural Machine Translation",
    "authors": [
        {
            "first": "Lemao",
            "middle": [],
            "last": "Liu",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "National Institute of Information and Communications Technology",
                "location": {
                    "addrLine": "3-5 Hikari-dai, Seika-cho, Soraku-gun",
                    "settlement": "Kyoto",
                    "country": "Japan"
                }
            },
            "email": "lmliu@nict.go.jp"
        },
        {
            "first": "Masao",
            "middle": [],
            "last": "Utiyama",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "National Institute of Information and Communications Technology",
                "location": {
                    "addrLine": "3-5 Hikari-dai, Seika-cho, Soraku-gun",
                    "settlement": "Kyoto",
                    "country": "Japan"
                }
            },
            "email": ""
        },
        {
            "first": "Andrew",
            "middle": [],
            "last": "Finch",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "National Institute of Information and Communications Technology",
                "location": {
                    "addrLine": "3-5 Hikari-dai, Seika-cho, Soraku-gun",
                    "settlement": "Kyoto",
                    "country": "Japan"
                }
            },
            "email": ""
        },
        {
            "first": "Eiichiro",
            "middle": [],
            "last": "Sumita",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "National Institute of Information and Communications Technology",
                "location": {
                    "addrLine": "3-5 Hikari-dai, Seika-cho, Soraku-gun",
                    "settlement": "Kyoto",
                    "country": "Japan"
                }
            },
            "email": ""
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Neural machine translation (NMT) with recurrent neural networks, has proven to be an effective technique for end-to-end machine translation. However, in spite of its promising advances over traditional translation methods, it typically suffers from an issue of unbalanced outputs, that arise from both the nature of recurrent neural networks themselves, and the challenges inherent in machine translation. To overcome this issue, we propose an agreement model for neural machine translation and show its effectiveness on large-scale Japaneseto-English and Chinese-to-English translation tasks. Our results show the model can achieve improvements of up to 1.4 BLEU over the strongest baseline NMT system. With the help of an ensemble technique, this new end-to-end NMT approach finally outperformed phrasebased and hierarchical phrase-based Moses baselines by up to 5.6 BLEU points.",
    "pdf_parse": {
        "paper_id": "N16-1046",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Neural machine translation (NMT) with recurrent neural networks, has proven to be an effective technique for end-to-end machine translation. However, in spite of its promising advances over traditional translation methods, it typically suffers from an issue of unbalanced outputs, that arise from both the nature of recurrent neural networks themselves, and the challenges inherent in machine translation. To overcome this issue, we propose an agreement model for neural machine translation and show its effectiveness on large-scale Japaneseto-English and Chinese-to-English translation tasks. Our results show the model can achieve improvements of up to 1.4 BLEU over the strongest baseline NMT system. With the help of an ensemble technique, this new end-to-end NMT approach finally outperformed phrasebased and hierarchical phrase-based Moses baselines by up to 5.6 BLEU points.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Recurrent neural network (RNN) has achieved great successes on several structured prediction tasks (Graves, 2013; Watanabe and Sumita, 2015; Dyer et al., 2015) , in which RNNs are required to make a sequence of dependent predictions. One of its advantages is that an unbounded history is available to enrich the context for the prediction at the current time-step.",
                "cite_spans": [
                    {
                        "start": 99,
                        "end": 113,
                        "text": "(Graves, 2013;",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 114,
                        "end": 140,
                        "text": "Watanabe and Sumita, 2015;",
                        "ref_id": null
                    },
                    {
                        "start": 141,
                        "end": 159,
                        "text": "Dyer et al., 2015)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Despite its successes, recently, (Liu et al., 2016 ) pointed out that the RNN suffers from a fundamental issue of generating unbalanced outputs: that is to say the suffixes of its outputs are typically worse than the prefixes. This is due to the fact that later predictions directly depend on the accuracy of previous predictions. They empirically demonstrated this issue on two simple sequence-to-sequence learning tasks: machine transliteration and grapheme-to-phoneme conversion.",
                "cite_spans": [
                    {
                        "start": 33,
                        "end": 50,
                        "text": "(Liu et al., 2016",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "On the more general sequence-to-sequence learning task of machine translation (MT), neural machine translation (NMT) based on RNNs has recently become an active research topic (Sutskever et al., 2014; Bahdanau et al., 2014) . Compared to those two simple tasks, MT involves in much larger vocabulary and frequent reordering between input and output sequences. This makes the prediction at each time-step far more challenging. In addition, sequences in MT are much longer, with averaged length of 36.7 being about 5 times longer than that in grapheme-to-phoneme conversion. Therefore, we believe that the history is more likely to contain incorrect predictions and the issue of unbalanced outputs may be more serious. This hypothesis is supported later (see Table 1 in \u00a74.1), by an analysis that shows the quality of the prefixes of translation hypotheses is much higher than that of the suffixes.",
                "cite_spans": [
                    {
                        "start": 176,
                        "end": 200,
                        "text": "(Sutskever et al., 2014;",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 201,
                        "end": 223,
                        "text": "Bahdanau et al., 2014)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 763,
                        "end": 764,
                        "text": "1",
                        "ref_id": "TABREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "To address this issue for NMT, in this paper we extend the agreement model proposed in (Liu et al., 2016) to the task of machine translation. Its key idea is to encourage the agreement between a pair of target-directional (left-to-right and right-to-left) NMT models in order to produce more balanced translations and thus improve the overall translation quality. Our contribution is two-fold:",
                "cite_spans": [
                    {
                        "start": 87,
                        "end": 105,
                        "text": "(Liu et al., 2016)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\u2022 We introduce a simple and general method to address the issue of unbalanced outputs for NMT ( \u00a73). This method is robust without any extra hyperparameters to tune and is easy to implement. In addition, it is general enough to be applied on top of any of the existing RNN translation models, although it was implemented on top of the model in (Bahdanau et al., 2014) in this paper.",
                "cite_spans": [
                    {
                        "start": 344,
                        "end": 367,
                        "text": "(Bahdanau et al., 2014)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\u2022 We provide an empirical evaluation of the technique on large scale Japanese-to-English and Chinese-to-English translation tasks. The results show our model can generate more balanced translation results, and achieves substantial improvements (of up to 1.4 BLEU points) over the strongest NMT baseline ( \u00a74). With the help of an ensemble technique, our new end-to-end NMT gains up to 5.6 BLEU points over phrase-based and hierarchical phrasebased Moses (Koehn et al., 2007) systems.1 ",
                "cite_spans": [
                    {
                        "start": 454,
                        "end": 474,
                        "text": "(Koehn et al., 2007)",
                        "ref_id": "BIBREF2"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "2 Overview of Neural Machine Translation",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Suppose x = x 1 , x 2 , \u2022 \u2022 \u2022 , x m denotes a source sentence, y = y 1 , y 2 , \u2022 \u2022 \u2022 , y n denotes a target sen- tence. In addition, let x <t = x 1 , x 2 , \u2022 \u2022 \u2022 , x t-1 denote a prefix of x.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Neural Machine Translation (NMT) directly maps a source sentence into a target within a probabilistic framework. Formally, it defines a conditional probability over a pair of sequences x and y via a recurrent neural network as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "p(y | x; \u03b8) = n t=1 p(y t | y <t , x; \u03b8) = n t=1 softmax g(h t ) [y t ]",
                        "eq_num": "(1)"
                    }
                ],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "where \u03b8 is the set of model parameters; h t denotes a hidden state (i.e. a vector) of y at timestep t; g is a transformation function from a hidden state to a vector with dimension of the target-side vocabulary size; softmax is the softmax function, and [i] denotes the i th component in a vector. 2 Furthermore,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "h t = f (h t-1 , c(x, y <t )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": ") is defined by a recurrent function over both the previous hidden state h t-1 and the context c(x, y <t ).3 Note that both h t and c(x, y <t ) have dimension d for all t.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this paper, we develop our model on top of the neural machine translation approach of (Bahdanau et al., 2014) , and we refer the reader this paper for a complete description of the model, for example, the definitons of f and c. The proposed method could just as easily been implemented on top of any other RNN models such as that in (Sutskever et al., 2014) .",
                "cite_spans": [
                    {
                        "start": 89,
                        "end": 112,
                        "text": "(Bahdanau et al., 2014)",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 336,
                        "end": 360,
                        "text": "(Sutskever et al., 2014)",
                        "ref_id": "BIBREF5"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this section, we extend the method in (Liu et al., 2016) to address this issue of unbalanced outputs for NMT. The key idea is to: 1) train two kinds of NMT, i.e. one generating targets from left-to-right while the other from right-to-left; 2) encourage the agreement between them by joint search.",
                "cite_spans": [
                    {
                        "start": 41,
                        "end": 59,
                        "text": "(Liu et al., 2016)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Agreement on Target-bidirectional NMT",
                "sec_num": "3"
            },
            {
                "text": "The training objective function for our agreement (or joint) model is formalized as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training",
                "sec_num": "3.1"
            },
            {
                "text": "= x,y log p(y | x; \u03b8 1 ) + log p(y r | x; \u03b8 2 ) (2)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training",
                "sec_num": "3.1"
            },
            {
                "text": "where y r = y n , y n-1 \u2022 \u2022 \u2022 , y 1 is the reverse of sequence y; p(y | x; \u03b8 1 ) denotes the left-to-right model with parameters \u03b8 1 , while p(y r | x; \u03b8 2 ) denotes the right-to-left model with parameters \u03b8 2 , as defined in Eq.(1); and x, y ranges over a given training dataset. Following (Bahdanau et al., 2014) , we employ AdaDelta (Zeiler, 2012) to minimize the loss .",
                "cite_spans": [
                    {
                        "start": 291,
                        "end": 314,
                        "text": "(Bahdanau et al., 2014)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training",
                "sec_num": "3.1"
            },
            {
                "text": "Note that, in parallel to our efforts, Cheng et al. ( 2016) has explored the agreement idea for NMT close to ours. However, unlike their work on the agreement between source and target sides in the spirit of the general idea in (Liang et al., 2006) , we focus on the agreement between left and right directions on the target side oriented to the natural issue of NMT itself. Although our model is orthogonal to theirs, one of our advantage is that our model does not rely on any additional hyperparameters to encourage agreement, given that tuning such hyperparameters for NMT is too costly.",
                "cite_spans": [
                    {
                        "start": 228,
                        "end": 248,
                        "text": "(Liang et al., 2006)",
                        "ref_id": "BIBREF2"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training",
                "sec_num": "3.1"
            },
            {
                "text": "Given a source sentence x and model parameters \u03b8 1 , \u03b8 2 , decoding can be formalized as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Approximate Joint Search",
                "sec_num": "3.2"
            },
            {
                "text": "\u0177 = argmax y p(y | x; \u03b8 1 ) \u00d7 p(y r | x; \u03b8 2 )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Approximate Joint Search",
                "sec_num": "3.2"
            },
            {
                "text": "As pointed out by (Liu et al., 2016) , it is NP-hard to perform an exact search, and so we adapt one of their approximate search methods for the machine translation scenario. The basic idea consists of two steps: 1) run beam search for forward and reverse models independently to obtain two k-best lists; 2) re-score the union of two k-best lists using the joint model to find the best candidate. We refer to the reader to (Liu et al., 2016) for further details.",
                "cite_spans": [
                    {
                        "start": 18,
                        "end": 36,
                        "text": "(Liu et al., 2016)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 423,
                        "end": 441,
                        "text": "(Liu et al., 2016)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Approximate Joint Search",
                "sec_num": "3.2"
            },
            {
                "text": "We conducted experiments on two challenging translation tasks: Japanese-to-English (JP-EN) and Chinese-to-English (CH-EN), using case-insensitive BLEU for evaluation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "4"
            },
            {
                "text": "For the JP-EN task, we use the data from NTCIR-9 (Goto et al., 2011) : the training data consisted of 2.0M sentence pairs, The development and test sets contained 2K sentences with a single referece, respectively. For the CH-EN task, we used the data from the NIST2008 Open Machine Translation Campaign: the training data consisted of 1.8M sentence pairs, the development set was nist02 (878 sentences), and the test sets are were nist05 (1082 sentences), nist06 (1664 sentences) and nist08 (1357 sentences).",
                "cite_spans": [
                    {
                        "start": 49,
                        "end": 68,
                        "text": "(Goto et al., 2011)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "4"
            },
            {
                "text": "Four baselines were used. The first two were the conventional state-of-the-art translation systems, phrase-based and hierarchical phrase-based systems, which are from the latest version of well-known Moses (Koehn et al., 2007) (NMT-J) was also implemented using NMT (Bahdanau et al., 2014) .",
                "cite_spans": [
                    {
                        "start": 206,
                        "end": 226,
                        "text": "(Koehn et al., 2007)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 266,
                        "end": 289,
                        "text": "(Bahdanau et al., 2014)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "4"
            },
            {
                "text": "We followed the standard pipeline to train and run Moses. GIZA++ (Och and Ney, 2000) with grow-diag-final-and was used to build the translation model. We trained 5-gram target language models using the training set for JP-EN and the Gigaword corpus for CH-EN, and used a lexicalized distortion model. All experiments were run with the default settings except for a distortion-limit of 12 in the JP-EN experiment, as suggested by (Goto et al., 2013) . 5To alleviate the negative effects of randomness, the final reported results are averaged over five runs of MERT.",
                "cite_spans": [
                    {
                        "start": 65,
                        "end": 84,
                        "text": "(Och and Ney, 2000)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 429,
                        "end": 448,
                        "text": "(Goto et al., 2013)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "4"
            },
            {
                "text": "To ensure a fair comparison, we employed the same settings for all NMT systems. Specifically, except for the maximum sequence length (seqlen, which was to 80), and the stopping iteration which was selected using development data, we used the default settings set out in (Bahdanau et al., 2014) for all NMT-based systems: the dimension of word embedding was 620, the dimension of hidden units was 1000, the batch size was 80, the source and target side vocabulary sizes were 30000, and the beam size for decoding was 12. Training was conducted on a single Tesla K80 GPU, and it took about 6 days to train a single NMT system on our large-scale data.",
                "cite_spans": [
                    {
                        "start": 270,
                        "end": 293,
                        "text": "(Bahdanau et al., 2014)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "4"
            },
            {
                "text": "In \u00a71, it was claimed that NMT generates unbalanced outputs. To demostrate this, we have to evaluate the partial translations, which is not trivial (Liu and Huang, 2014) . Inspired by (Liu and Huang, 2014) , we employ the idea of partial BLEU rather than potential BLEU, as there is no future string concept during NMT decoding. In addition, since the lower n-gram (for example, 1-gram) is easier to be aligned to the uncovered words in source side, which might negatively affect the absolute statistics of evaluation,6 we employ the partial 4-gram as the metric to evaluate the quality of partial translations (both prefixes and suffixes). In Table 1 , we can see that the prefixes are of higher quality than the suffixes for a single left-to-right model (NMT-l2r). In contrast to this, it can be seen that our joint model (NMT-J) that includes one left-to-right and one right-to-left model, successfully addresses this issue, producing balanced outputs.",
                "cite_spans": [
                    {
                        "start": 148,
                        "end": 169,
                        "text": "(Liu and Huang, 2014)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 184,
                        "end": 205,
                        "text": "(Liu and Huang, 2014)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 650,
                        "end": 651,
                        "text": "1",
                        "ref_id": "TABREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Results and Analysis on the JP-EN Task",
                "sec_num": "4.1"
            },
            {
                "text": "Table 2 shows the main results on the JP-EN task. From this table, we can see that, although a single NMT model (either left-to-right or right-to-left) comfortably outperforms the Moses and Moses-hier baselines, our simple NMT-J (with one l2r and one r2l NMT model) obtain gains of 1.5 BLEU points over a single NMT. In addition, the more powerful joint model NMT-J-5, which is an ensemble of five l2r and five r2l NMT models, gains 0.7 BLEU points over the strongest NMT ensemble NMT-r2l-5, i.e. an ensemble of five r2l NMT models. The ensemble of joint models achieved considerable gains of 5.6 and 4.8 BLEU points over the state-of-the-art Moses and Moses-hier, respectively. To the best of our knowlege, it is the first time that an end-to-end neural machine translation system has achieved such improvements on the very challenging task of JP-EN translation. One might argue that our NMT-J-5 contained ten NMT models in total, while the NMT-l2r-5 or NMT-r2l-5 only used five models, and thus such a comparison is unfair. Therefore, we integrated ten NMT models into the NMT-r2l-10 ensemble. In Table 2 , we can see that NMT-r2l-10 is not necessarily better than NMT-r2l-5, which is consistent with the findings reported in (Zhou et al., 2002) .",
                "cite_spans": [
                    {
                        "start": 1228,
                        "end": 1247,
                        "text": "(Zhou et al., 2002)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "2",
                        "ref_id": "TABREF1"
                    },
                    {
                        "start": 1105,
                        "end": 1106,
                        "text": "2",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Results and Analysis on the JP-EN Task",
                "sec_num": "4.1"
            },
            {
                "text": "Table 3 shows the comparison between our method and the baselines on the CH-EN task. 7 The results were similar in character to the results for JP-EN. The proposed joint model (NMT-J-5) consistently outperformed the strongest neural baseline (NMT-l2r-5), an ensemble of five l2r NMT models, on all the test sets with gains up to 1.4 BLEU points. Furthermore, our model again achieved substantial gains over the Moses and Moses-hier systems, in the range 1.9\u223c5.2 BLEU points, depending on the test set.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "3",
                        "ref_id": "TABREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Results on the CH-EN Task",
                "sec_num": "4.2"
            },
            {
                "text": "Target-bidirectional transduction techniques were pioneered in the field of machine translation (Watanabe and Sumita, 2002; Finch and Sumita, 2009; Zhang et al., 2013) . They used the techniques for traditional SMT models, under the IBM framework (Watanabe and Sumita, 2002) or the feature-driven linear models (Finch and Sumita, 2009; Zhang et al., 2013) . However, the target-bidirectional techniques we have developed for the unified neural network framework, target a pressing need directly motivated by a fundamental issue suffered by recurrent neural networks.",
                "cite_spans": [
                    {
                        "start": 96,
                        "end": 123,
                        "text": "(Watanabe and Sumita, 2002;",
                        "ref_id": null
                    },
                    {
                        "start": 124,
                        "end": 147,
                        "text": "Finch and Sumita, 2009;",
                        "ref_id": null
                    },
                    {
                        "start": 148,
                        "end": 167,
                        "text": "Zhang et al., 2013)",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 247,
                        "end": 274,
                        "text": "(Watanabe and Sumita, 2002)",
                        "ref_id": null
                    },
                    {
                        "start": 311,
                        "end": 335,
                        "text": "(Finch and Sumita, 2009;",
                        "ref_id": null
                    },
                    {
                        "start": 336,
                        "end": 355,
                        "text": "Zhang et al., 2013)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "5"
            },
            {
                "text": "Target-directional neural network models have also been successfully employed in (Devlin et al., 2014) . However, their approach was concerned with feedforward networks, which can not make full use of rich contextual information. As a result, their models could only be used as features (i.e. submodels) to augment traditional translation techniques in contrast to the end-to-end neural network framework for machine translation in our proposal.",
                "cite_spans": [
                    {
                        "start": 81,
                        "end": 102,
                        "text": "(Devlin et al., 2014)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "5"
            },
            {
                "text": "Our approach is related to that in (Bengio et al., 2015) in some sense. Both approaches can alleviate the mismatch between the training and testing stages: the history predictions are always correct in training while may be incorrect in testing. Bengio et al. (2015) introduce noise into history predictions in training to balance the mistmatch, while we try to make the history predictions in testing as accurate as those in training by using of two directional models. Therefore, theirs focuses on this problem from the view of training instead of both modeling and training as ours, but it is possible and promising to apply their approach to optimize our joint model.",
                "cite_spans": [
                    {
                        "start": 35,
                        "end": 56,
                        "text": "(Bengio et al., 2015)",
                        "ref_id": null
                    },
                    {
                        "start": 246,
                        "end": 266,
                        "text": "Bengio et al. (2015)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "5"
            },
            {
                "text": "In this paper, we investigate the issue of unbalanced outputs suffered by recurrent neural networks, and empirically show its existence in the context of machine translation. To address this issue, we propose an easy to implement agreement model that extends the method of (Liu et al., 2016) from simple sequence-to-sequence learning tasks to machine translation.",
                "cite_spans": [
                    {
                        "start": 273,
                        "end": 291,
                        "text": "(Liu et al., 2016)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "6"
            },
            {
                "text": "On two challenging JP-EN and CH-EN translation tasks, our approach was empirically shown to be effective in addressing the issue; by generating balanced outputs, it was able to consistently outperform a respectable NMT baseline on all test sets, delivering gains of up to 1.4 BLEU points. To put these results in the broader context of machine translation research, our approach (even without special handling of unknown words) achieved gains of up to 5.6 BLEU points over strong phrase-based and hierarchical phrase-based Moses baselines, with the help of an ensemble technique.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "6"
            },
            {
                "text": "The absolute gains of our model can be expected to be further increased by applying the well-known techniques in (Jean et al.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": ", 2015;Luong et al., 2015) that address the problems presented by unknown words, but these techniques are beyond the scope of this paper.2 In that sense, yt in Eq.(1) also denotes the index of this word in its vocabulary.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Both hidden states and context vectors are dependent on the model parameter \u03b8, but we remove it from the expressions here for simplicity.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "This configuration achieved the significant improvements over the default setting on JP-EN.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "In training SMT(Liu and Huang, 2014), we update weights towards higher BLEU translations and thus we care more about the relative statistics of BLEU; but in this paper, we care more about the absolute statistics, in order to show how severe the problem of unbalanced outputs is.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "We did not run NMT-l2r-10 and NMT-r2l-10, because it is too time-consuming to train 10 NMT models on both target directions and especially NMT-r2l-10 is not necessarily better than NMT-r2l-5 as shown in Table2.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "We would like to thank the three anonymous reviewers for helpful comments and suggestions. In addition, we would like to thank Rico Sennrich for fruitful discussions.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgments",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Agreement-based joint training for bidirectional attention-based neural machine translation",
                "authors": [
                    {
                        "first": "Dzmitry",
                        "middle": [],
                        "last": "Bahdanau",
                        "suffix": ""
                    },
                    {
                        "first": "Kyunghyun",
                        "middle": [],
                        "last": "Cho",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio ; Samy",
                        "suffix": ""
                    },
                    {
                        "first": "Oriol",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    },
                    {
                        "first": "Navdeep",
                        "middle": [],
                        "last": "Vinyals",
                        "suffix": ""
                    },
                    {
                        "first": "Noam",
                        "middle": [],
                        "last": "Jaitly",
                        "suffix": ""
                    },
                    {
                        "first": ";",
                        "middle": [],
                        "last": "Shazeer",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Cheng",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "1171--1179",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "al.2014] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473. [Bengio et al.2015] Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. 2015. Scheduled sampling for sequence prediction with recurrent neural networks. In Advances in Neural Information Processing Systems, pages 1171-1179. [Cheng et al.2016] Yong Cheng, Shiqi Shen, Zhongjun He, Wei He, Hua Wu, Maosong Sun, and Yang Liu. 2016. Agreement-based joint training for bidirectional attention-based neural machine translation. CoRR, abs/1512.04650. [Devlin et al.2014] Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, and John Makhoul. 2014. Fast and robust neural network joint models for statistical machine translation. In Proceedings of ACL. [Dyer et al.2015] Chris Dyer, Miguel Ballesteros, Wang Ling, Austin Matthews, and Noah A. Smith. 2015. Transition-based dependency parsing with stack long short-term memory. In Proceedings of ACL-IJCNLP. [Finch and Sumita2009] Andrew Finch and Eiichiro Sumita. 2009. Bidirectional phrase-based statistical machine translation. In Proceedings of EMNLP. [Goto et al.2011] Isao Goto, Bin Lu, Ka-Po Chow, Ei- ichiro Sumita, and Benjamin K. Tsou. 2011. Overview of the patent machine translation task at the NTCIR-9 workshop. In Proceedings of NTCIR-9. [Goto et al.2013] Isao Goto, Masao Utiyama, Eiichiro Sumita, Akihiro Tamura, and Sadao Kurohashi. 2013. Distortion model considering rich context for statisti- cal machine translation. In Proceedings of ACL.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "On using very large target vocabulary for neural machine translation",
                "authors": [
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Graves",
                        "suffix": ""
                    },
                    {
                        "first": ";",
                        "middle": [],
                        "last": "Jean",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of ACL-IJCNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alex Graves. 2013. Generating sequences with recurrent neural networks. CoRR. [Jean et al.2015] S\u00e9bastien Jean, Kyunghyun Cho, Roland Memisevic, and Yoshua Bengio. 2015. On using very large target vocabulary for neural machine translation. In Proceedings of ACL-IJCNLP.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Moses: open source toolkit for statistical machine translation",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Koehn",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Proceedings of ACL: Demonstrations",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Koehn et al.2007] P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst. 2007. Moses: open source toolkit for statistical machine translation. In Proceedings of ACL: Demonstrations. [Liang et al.2006] Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In Proceed- ings of HLT-NAACL.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Agreement on target-bidirectional lstms for sequence-to-sequence learning",
                "authors": [
                    {
                        "first": "Huang",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Lemao",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Liang",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": ";",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Thang Luong, Ilya Sutskever, Quoc Le, Oriol Vinyals, and Wojciech Zaremba",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Liu and Huang2014] Lemao Liu and Liang Huang. 2014. Search-aware tuning for machine translation. In Proceedings of EMNLP. [Liu et al.2016] Lemao Liu, Andrew Finch, Masao Utiyama, and Eiichiro Sumita. 2016. Agreement on target-bidirectional lstms for sequence-to-sequence learning. In Proceedings of AAAI. [Luong et al.2015] Thang Luong, Ilya Sutskever, Quoc Le, Oriol Vinyals, and Wojciech Zaremba. 2015. Addressing the rare word problem in neural machine translation. In Proceedings of ACL-IJCNLP.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Improved statistical alignment models",
                "authors": [
                    {
                        "first": "Ney",
                        "middle": [
                            ";"
                        ],
                        "last": "Och",
                        "suffix": ""
                    },
                    {
                        "first": "Josef",
                        "middle": [],
                        "last": "Franz",
                        "suffix": ""
                    },
                    {
                        "first": "Hermann",
                        "middle": [],
                        "last": "Och",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Ney",
                        "suffix": ""
                    }
                ],
                "year": 2000,
                "venue": "Proceedings of ACL",
                "volume": "",
                "issue": "",
                "pages": "440--447",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Och and Ney2000] Franz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In Pro- ceedings of ACL, pages 440-447.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Watanabe and Sumita2002] Taro Watanabe and Eiichiro Sumita. 2002. Bidirectional decoding for statistical machine translation",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of ACL-IJCNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sutskever et al.2014] Ilya Sutskever, Oriol Vinyals, and Quoc V. V Le. 2014. Sequence to sequence learning with neural networks. In Proceedings of NIPS. [Watanabe and Sumita2002] Taro Watanabe and Eiichiro Sumita. 2002. Bidirectional decoding for statistical machine translation. In Proceeding of COLING. [Watanabe and Sumita2015] Taro Watanabe and Eiichiro Sumita. 2015. Transition-based neural constituent parsing. In Proceedings of ACL-IJCNLP.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Beyond left-to-right: Multiple decomposition structures for smt",
                "authors": [
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Matthew",
                        "suffix": ""
                    },
                    {
                        "first": ";",
                        "middle": [],
                        "last": "Zeiler",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "HLT-NAACL",
                "volume": "",
                "issue": "",
                "pages": "12--21",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Matthew D. Zeiler. 2012. ADADELTA: an adaptive learning rate method. CoRR. [Zhang et al.2013] Hui Zhang, Kristina Toutanova, Chris Quirk, and Jianfeng Gao. 2013. Beyond left-to-right: Multiple decomposition structures for smt. In HLT- NAACL, pages 12-21.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Ensembling neural networks: Many could be better than all",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Artif. Intell",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zhou et al.2002] Zhi-Hua Zhou, Jianxin Wu, and Wei Tang. 2002. Ensembling neural networks: Many could be better than all. Artif. Intell.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "and are respectively denoted as Moses and Moses-hier. The other two were neural machine translation systems implemented using the open source NMT toolkit (Bahdanau et al., 2014): 4 left-to-right NMT (NMT-l2r) and right-toleft NMT (NMT-r2l",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table><tr><td colspan=\"3\">Systems Prefix Suffix</td></tr><tr><td>NMT-l2r</td><td>29.4</td><td>25.4</td></tr><tr><td>NMT-r2l</td><td>26.2</td><td>26.7</td></tr><tr><td>NMT-J</td><td>29.5</td><td>28.6</td></tr></table>",
                "type_str": "table",
                "text": "). The proposed joint model 4 See https://github.com/lisa-groundhog/GroundHog/tree/ master/experiments/nmt. Quality of 5-word prefixes and suffices of translations in the JP-EN test set, evaluated using partial BLEU.",
                "html": null,
                "num": null
            },
            "TABREF1": {
                "content": "<table><tr><td>Systems</td><td>dev test</td></tr><tr><td>Moses</td><td>27.9 29.4</td></tr><tr><td colspan=\"2\">Moses-hier 28.6 30.2</td></tr><tr><td>NMT-l2r</td><td>31.5 32.4</td></tr><tr><td>NMT-r2l</td><td>31.5 32.6</td></tr><tr><td>NMT-J</td><td>33.0 34.1</td></tr><tr><td>NMT-l2r-5</td><td>32.6 33.7</td></tr><tr><td>NMT-r2l-5</td><td>33.0 34.3</td></tr><tr><td>NMT-J-5</td><td>33.8 35.0</td></tr><tr><td colspan=\"2\">NMT-l2r-10 32.5 33.6</td></tr><tr><td colspan=\"2\">NMT-r2l-10 33.0 34.2</td></tr></table>",
                "type_str": "table",
                "text": "BLEU comparison of the proposed model NMT-Joint with three baselines on JP-EN task.",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table><tr><td/><td colspan=\"3\">nist05 nist06 nist08</td></tr><tr><td>Moses</td><td>35.4</td><td>33.7</td><td>25.0</td></tr><tr><td>Moses-hier</td><td>35.6</td><td>33.8</td><td>25.3</td></tr><tr><td>NMT-l2r</td><td>34.2</td><td>34.9</td><td>27.7</td></tr><tr><td>NMT-r2l</td><td>34.0</td><td>34.1</td><td>26.9</td></tr><tr><td>NMT-J</td><td>36.8</td><td>36.9</td><td>28.5</td></tr><tr><td>NMT-l2r-5</td><td>37.0</td><td>37.5</td><td>28.2</td></tr><tr><td>NMT-r2l-5</td><td>36.9</td><td>37.1</td><td>27.3</td></tr><tr><td>NMT-J-5</td><td>37.5</td><td>38.9</td><td>28.8</td></tr></table>",
                "type_str": "table",
                "text": "BLEU comparison of the proposed model NMT-Joint with baselines on CH-EN task.",
                "html": null,
                "num": null
            }
        }
    }
}