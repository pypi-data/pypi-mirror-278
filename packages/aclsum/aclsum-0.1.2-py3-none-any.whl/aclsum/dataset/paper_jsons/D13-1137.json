{
    "paper_id": "D13-1137",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:11:25.831240Z"
    },
    "title": "Simple Customization of Recursive Neural Networks for Semantic Relation Classification",
    "authors": [
        {
            "first": "Kazuma",
            "middle": [],
            "last": "Hashimoto",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "\u2020The University of Manchester",
                "location": {
                    "addrLine": "131 Princess Street",
                    "postCode": "M1 7DN",
                    "settlement": "Manchester",
                    "country": "UK"
                }
            },
            "email": ""
        },
        {
            "first": "Makoto",
            "middle": [],
            "last": "Miwa",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "\u2020The University of Manchester",
                "location": {
                    "addrLine": "131 Princess Street",
                    "postCode": "M1 7DN",
                    "settlement": "Manchester",
                    "country": "UK"
                }
            },
            "email": "makoto.miwa@manchester.ac.uk"
        },
        {
            "first": "Yoshimasa",
            "middle": [],
            "last": "Tsuruoka",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "\u2020The University of Manchester",
                "location": {
                    "addrLine": "131 Princess Street",
                    "postCode": "M1 7DN",
                    "settlement": "Manchester",
                    "country": "UK"
                }
            },
            "email": "tsuruoka@logos.t.u-tokyo.ac.jp"
        },
        {
            "first": "Takashi",
            "middle": [],
            "last": "Chikayama",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "\u2020The University of Manchester",
                "location": {
                    "addrLine": "131 Princess Street",
                    "postCode": "M1 7DN",
                    "settlement": "Manchester",
                    "country": "UK"
                }
            },
            "email": "chikayama@logos.t.u-tokyo.ac.jp"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "In this paper, we present a recursive neural network (RNN) model that works on a syntactic tree. Our model differs from previous RNN models in that the model allows for an explicit weighting of important phrases for the target task. We also propose to average parameters in training. Our experimental results on semantic relation classification show that both phrase categories and task-specific weighting significantly improve the prediction accuracy of the model. We also show that averaging the model parameters is effective in stabilizing the learning and improves generalization capacity. The proposed model marks scores competitive with state-of-the-art RNN-based models.",
    "pdf_parse": {
        "paper_id": "D13-1137",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "In this paper, we present a recursive neural network (RNN) model that works on a syntactic tree. Our model differs from previous RNN models in that the model allows for an explicit weighting of important phrases for the target task. We also propose to average parameters in training. Our experimental results on semantic relation classification show that both phrase categories and task-specific weighting significantly improve the prediction accuracy of the model. We also show that averaging the model parameters is effective in stabilizing the learning and improves generalization capacity. The proposed model marks scores competitive with state-of-the-art RNN-based models.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Recursive Neural Network (RNN) models are promising deep learning models which have been applied to a variety of natural language processing (NLP) tasks, such as sentiment classification, compound similarity, relation classification and syntactic parsing (Hermann and Blunsom, 2013; Socher et al., 2012; Socher et al., 2013) . RNN models can represent phrases of arbitrary length in a vector space of a fixed dimension. Most of them use minimal syntactic information (Socher et al., 2012) .",
                "cite_spans": [
                    {
                        "start": 255,
                        "end": 282,
                        "text": "(Hermann and Blunsom, 2013;",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 283,
                        "end": 303,
                        "text": "Socher et al., 2012;",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 304,
                        "end": 324,
                        "text": "Socher et al., 2013)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 467,
                        "end": 488,
                        "text": "(Socher et al., 2012)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Recently, Hermann and Blunsom (2013) proposed a method for leveraging syntactic information, namely CCG combinatory operators, to guide composition of phrases in RNN models. While their models were successfully applied to binary sentiment classification and compound similarity tasks, there are questions yet to be answered, e.g., whether such enhancement is beneficial in other NLP tasks as well, and whether a similar improvement can be achieved by using syntactic information of more commonly available types such as phrase categories and syntactic heads.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this paper, we present a supervised RNN model for a semantic relation classification task. Our model is different from existing RNN models in that important phrases can be explicitly weighted for the task. Syntactic information used in our model includes part-of-speech (POS) tags, phrase categories and syntactic heads. POS tags are used to assign vector representations to word-POS pairs. Phrase categories are used to determine which weight matrices are chosen to combine phrases. Syntactic heads are used to determine which phrase is weighted during combining phrases. To incorporate task-specific information, phrases on the path between entity pairs are further weighted.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The second contribution of our work is the introduction of parameter averaging into RNN models. In our preliminary experiments, we observed that the prediction performance of the model often fluctuates significantly between training iterations. This fluctuation not only leads to unstable performance of the resulting models, but also makes it difficult to fine-tune the hyperparameters of the model. Inspired by Swersky et al. (2010) , we propose to average the model parameters in the course of training. A recent technique for deep learning models of similar vein is dropout (Hinton et al., 2012) , but averaging is simpler to implement.",
                "cite_spans": [
                    {
                        "start": 413,
                        "end": 434,
                        "text": "Swersky et al. (2010)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 578,
                        "end": 599,
                        "text": "(Hinton et al., 2012)",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Our experimental results show that our model per-Figure 1 : A recursive representations of a phrase \"a word vector\" with POS tags of the words (DT, NN and NN respectively). For example, the two word-POS pairs \"word NN\" and \"vector NN\" with a syntactic category N are combined to represent the phrase \"word vector\".",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 56,
                        "end": 57,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "forms better than standard RNN models. By averaging the model parameters, our model achieves performance competitive with the MV-RNN model in Socher et al. (2012) , without using computationally expensive word-dependent matrices.",
                "cite_spans": [
                    {
                        "start": 142,
                        "end": 162,
                        "text": "Socher et al. (2012)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Our model is a supervised RNN that works on a binary syntactic tree. As our first step to leverage information available in the tree, we distinguish words with the same spelling but POS tags in the vector space. Our model also uses different weight matrices dependent on the phrase categories of child nodes (phrases or words) in combining phrases. Our model further weights those nodes that appear to be important.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "An Averaged RNN Model with Syntax",
                "sec_num": "2"
            },
            {
                "text": "Compositional functions of our model follow those of the SU-RNN model in Socher et al. (2013) .",
                "cite_spans": [
                    {
                        "start": 73,
                        "end": 93,
                        "text": "Socher et al. (2013)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "An Averaged RNN Model with Syntax",
                "sec_num": "2"
            },
            {
                "text": "Our model simply assigns vector representations to word-POS pairs. For example, a word \"caused\" can be represented in two ways: \"caused VBD\" and \"caused VBN\". The vectors are represented as column vectors in a matrix W e \u2208 R d\u00d7|V| , where d is the dimension of a vector and V is a set of all word-POS pairs we consider.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word-POS Vector Representations",
                "sec_num": "2.1"
            },
            {
                "text": "In construction of parse trees, we associate each of the tree node with its d-dimensional vector representation computed from vector representations of its subtrees. For leaf nodes, we look up word-POS vec-tor representations in V. Figure 1 shows an example of such recursive representations. A parent vector p \u2208 R d\u00d71 is computed from its direct child vectors c l and c r \u2208 R d\u00d71 :",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 239,
                        "end": 240,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Compositional Functions with Syntax",
                "sec_num": "2.2"
            },
            {
                "text": "p = tanh(\u03b1 l W Tc l ,Tc r l c l +\u03b1 r W Tc l ,Tc r r c r +b Tc l ,Tc r ),",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Compositional Functions with Syntax",
                "sec_num": "2.2"
            },
            {
                "text": "where W Tc l ,Tc r l and W Tc l ,Tc r r \u2208 R d\u00d7d are weight matrices that depend on the phrase categories of c l and c r . Here, c l and c r have phrase categories T c l and T cr respectively (such as N, V, etc.). b Tc l ,Tc r \u2208 R d\u00d71 is a bias vector. To incorporate the importance of phrases into the model, two subtrees of a node may have different weights \u03b1 l \u2208 [0, 1] and \u03b1 r (= 1 -\u03b1 l ), taking phrase importance into account. The value of \u03b1 l is manually specified and automatically applied to all nodes based on prior knowledge about the task. In this way, we can compute vector representations for phrases of arbitrary length. We denote a set of such matrices as W lr and bias vectors as b.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Compositional Functions with Syntax",
                "sec_num": "2.2"
            },
            {
                "text": "As with other RNN models, we add on the top of a node x a softmax classifier. The classifier is used to predict a K-class distribution d(x) \u2208 R K\u00d71 over a specific task to train our model:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Objective Function and Learning",
                "sec_num": "2.3"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "d(x) = softmax(W label x + b label ),",
                        "eq_num": "(1)"
                    }
                ],
                "section": "Objective Function and Learning",
                "sec_num": "2.3"
            },
            {
                "text": "where W label \u2208 R K\u00d7d is a weight matrix and b label \u2208 R K\u00d71 is a bias vector. We denote t(x) \u2208 R K\u00d71 as the target distribution vector at node x. t(x) has a 0-1 encoding: the entry at the correct label of t(x) is 1, and the remaining entries are 0. We then compute the cross entropy error between d(x) and t(x):",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Objective Function and Learning",
                "sec_num": "2.3"
            },
            {
                "text": "E(x) = - K \u2211 k=1 t k (x)logd k (x),",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Objective Function and Learning",
                "sec_num": "2.3"
            },
            {
                "text": "and define an objective function as the sum of E(x) over all training data:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Objective Function and Learning",
                "sec_num": "2.3"
            },
            {
                "text": "J(\u03b8) = \u2211 x E(x) + \u03bb 2 \u2225\u03b8\u2225 2 ,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Objective Function and Learning",
                "sec_num": "2.3"
            },
            {
                "text": "where \u03b8 = (W e , W lr , b, W label , b label ) is the set of our model parameters that should be learned. \u03bb is a vector of regularization parameters.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Objective Function and Learning",
                "sec_num": "2.3"
            },
            {
                "text": "To compute d(x), we can directly leverage any other nodes' feature vectors in the same tree. We denote such additional feature vectors as x \u2032 i \u2208 R d\u00d71 , and extend Eq. (1):",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Objective Function and Learning",
                "sec_num": "2.3"
            },
            {
                "text": "d(x) = softmax(W label x + \u2211 i W add i x \u2032 i + b label ),",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Objective Function and Learning",
                "sec_num": "2.3"
            },
            {
                "text": "where W add i \u2208 R K\u00d7d are weight matrices for additional features. We denote these matrices W add i as W add . We also add W add to \u03b8:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Objective Function and Learning",
                "sec_num": "2.3"
            },
            {
                "text": "\u03b8 = (W e , W lr , b, W label , W add , b label ).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Objective Function and Learning",
                "sec_num": "2.3"
            },
            {
                "text": "The gradient of J(\u03b8)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Objective Function and Learning",
                "sec_num": "2.3"
            },
            {
                "text": "\u2202J(\u03b8) \u2202\u03b8 = \u2211 x \u2202E(x) \u2202\u03b8 + \u03bb\u03b8",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Objective Function and Learning",
                "sec_num": "2.3"
            },
            {
                "text": "is efficiently computed via backpropagation through structure (Goller and K\u00fcchler, 1996) . To minimize J(\u03b8), we use batch L-BFGS1 (Hermann and Blunsom, 2013; Socher et al., 2012) .",
                "cite_spans": [
                    {
                        "start": 62,
                        "end": 88,
                        "text": "(Goller and K\u00fcchler, 1996)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 158,
                        "end": 178,
                        "text": "Socher et al., 2012)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Objective Function and Learning",
                "sec_num": "2.3"
            },
            {
                "text": "We use averaged model parameters",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Averaging",
                "sec_num": "2.4"
            },
            {
                "text": "\u03b8 = 1 T + 1 T \u2211 t=0 \u03b8 t",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Averaging",
                "sec_num": "2.4"
            },
            {
                "text": "at test time, where \u03b8 t is the vector of model parameters after t iterations of the L-BFGS optimization. Our preliminary experimental results suggest that averaging \u03b8 except W e works well.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Averaging",
                "sec_num": "2.4"
            },
            {
                "text": "We used the Enju parser (Miyao and Tsujii, 2008) for syntactic parsing. We used 13 phrase categories given by Enju.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Settings",
                "sec_num": "3"
            },
            {
                "text": "We evaluated our model on a semantic relation classification task: SemEval 2010 Task 8 (Hendrickx et al., 2010) . Following Socher et al. (2012) , we regarded the task as a 19-class classification problem. There are 8,000 samples for training, and 2,717 for test. For the validation set, we randomly sampled 2,182 samples from the training data.",
                "cite_spans": [
                    {
                        "start": 87,
                        "end": 111,
                        "text": "(Hendrickx et al., 2010)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 124,
                        "end": 144,
                        "text": "Socher et al. (2012)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Task: Semantic Relation Classification",
                "sec_num": "3.1"
            },
            {
                "text": "To predict a class label, we first find the minimal phrase that covers the target entities and then use the vector representation of the phrase (Figure 2 ).",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 152,
                        "end": 153,
                        "text": "2",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Task: Semantic Relation Classification",
                "sec_num": "3.1"
            },
            {
                "text": "As explained in Section 2.3, we can directly connect features on any other nodes to the softmax classifier. In this work, we used three such internal features: two vector representations of target entities and one averaged vector representation of words between the entities2 .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Task: Semantic Relation Classification",
                "sec_num": "3.1"
            },
            {
                "text": "We tuned the weight \u03b1 l (or \u03b1 r ) introduced in Section 2.2 for this particular task. There are two factors: syntactic heads and syntactic path between target entities. Our model puts a weight \u03b2 \u2208 [0.5, 1] on head phrases, and 1 -\u03b2 on the others. For relation classification tasks, syntactic paths between target entities are important (Zhang et al., 2006) , so our model also puts another weight \u03b3 \u2208 [0.5, 1] on phrases on the path, and 1 -\u03b3 on the others. When both child nodes are on the path or neither of them on the path, we set \u03b3 = 0.5. The two weight factors are summed up and divided by 2 to be the final weights \u03b1 l and \u03b1 r to combine the phrases. For example, we set \u03b1 l = (1-\u03b2)+\u03b3 2 and \u03b1 r = \u03b2+(1-\u03b3) 2 when the right child node is the head and the left child node is on the path.",
                "cite_spans": [
                    {
                        "start": 336,
                        "end": 356,
                        "text": "(Zhang et al., 2006)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Weights on Phrases",
                "sec_num": "3.2"
            },
            {
                "text": "We initialized W e with 50-dimensional word vectors3 trained with the model of al. ( 2011), and W lr with I 2 + \u03b5, where I \u2208 R d\u00d7d is an identity matrix. Here, \u03b5 is zero-mean gaussian random variable with a variance of 0.01. The initialization of W lr is the same as that of Socher et al. (2013) . The remaining model parameters were initialized with 0.",
                "cite_spans": [
                    {
                        "start": 275,
                        "end": 295,
                        "text": "Socher et al. (2013)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Initialization of Model Parameters and Tuning of Hyperparameters",
                "sec_num": "3.3"
            },
            {
                "text": "We tuned hyperparameters in our model using the validation set for each experimental setting. The hyperparameters include the regularization parameters for W e , W lr , W label and W add , and the weights \u03b2 and \u03b3. For example, the best performance for our model with all the proposed methods was obtained with the values: 10 -6 , 10 -4 , 10 -3 , 10 -3 , 0.7 and 0.9 respectively.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Initialization of Model Parameters and Tuning of Hyperparameters",
                "sec_num": "3.3"
            },
            {
                "text": "Table 1 shows the performance of our model and that of previously reported systems on the test set. The performance of an SVM system with bag-of-words features was reported in Rink and Harabagiu (2010) , and the performance of the RNN and MV-RNN models was reported in Socher et al. (2012) . Our model achieves an F1 score of 79.4% and outperforms the RNN model (74.8% F1) as well as the simple SVM-based system (73.1% F1). More no- tably, the score of our model is competitive with that of the MV-RNN model (79.1% F1), which is computationally much more expensive. Readers are referred to Hermann and Blunsom ( 2013) for the discussion about the computational complexity of the MV-RNN model. We improved the performance of RNN models on the task without much increasing the complexity. This is a significant practical advantage of our model, although its expressive power is not the same as that of the MV-RNN model.",
                "cite_spans": [
                    {
                        "start": 176,
                        "end": 201,
                        "text": "Rink and Harabagiu (2010)",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 269,
                        "end": 289,
                        "text": "Socher et al. (2012)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "1",
                        "ref_id": "TABREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Results and Discussion",
                "sec_num": "4"
            },
            {
                "text": "Our model outperforms the RNN model with one lexical and two semantic external features: POS tags, WordNet hypernyms and named entity tags (NER) of target word pairs (external features). The MV-RNN model with external features shows better performance than our model. An SVM with rich lexical and semantic features (Rink and Harabagiu, 2010 ) also outperforms ours. Note, however, that this is not a fair comparison because those models use rich external resources such as WordNet and named entity tags.",
                "cite_spans": [
                    {
                        "start": 315,
                        "end": 340,
                        "text": "(Rink and Harabagiu, 2010",
                        "ref_id": "BIBREF5"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results and Discussion",
                "sec_num": "4"
            },
            {
                "text": "We conducted additional experiments to quantify the contributions of phrase categories, heads, paths and averaging to our classification score. As shown in Table 2 , our model without phrase categories, heads or paths still outperforms the RNN model with external features. On the other hand, our model without averaging yields a lower score than the RNN model with external features, though it is still bet-ter than the RNN model alone. Without utilizing these four properties, our model obtained only 74.1% F1. These results indicate that syntactic and task-specific information and averaging contribute to the performance improvement. The improvement is achieved by a simple modification of compositional functions in RNN models.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 162,
                        "end": 163,
                        "text": "2",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Contributions of Proposed Methods",
                "sec_num": "4.1"
            },
            {
                "text": "Figure 3 shows the training curves in terms of F1 scores. These curves clearly demonstrate that parameter averaging helps to stabilize the learning and improve generalization capacity.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "3",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Effects of Averaging in Training",
                "sec_num": "4.2"
            },
            {
                "text": "We have presented an averaged RNN model for semantic relation classification. Our experimental results show that syntactic information such as phrase categories and heads improves the performance, and the task-specific weighting is also beneficial. The results also demonstrate that averaging the model parameters not only stabilizes the learning but also improves the generalization capacity of the model. As future work, we plan to combine deep learning models with richer information such as predicateargument structures.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "5"
            },
            {
                "text": "We used libLBFGS provided at http://www. chokkan.org/software/liblbfgs/.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Socher et al. (2012) used richer features including words around entity pairs in their implementation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "The word vectors are provided at http://ronan. collobert.com/senna/. We used the vectors without any modifications such as normalization.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "We thank the anonymous reviewers for their insightful comments.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgments",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Learning Task-Dependent Distributed Representations by Backpropagation Through Structure",
                "authors": [
                    {
                        "first": "Christoph",
                        "middle": [],
                        "last": "Goller",
                        "suffix": ""
                    },
                    {
                        "first": "Andreas",
                        "middle": [],
                        "last": "K\u00fcchler",
                        "suffix": ""
                    }
                ],
                "year": 1996,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Christoph Goller and Andreas K\u00fcchler. 1996. Learning Task-Dependent Distributed Representations by Back- propagation Through Structure. In ICNN.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "SemEval-2010 Task 8: Multi-Way Classication of Semantic Relations Between Pairs of Nominals",
                "authors": [
                    {
                        "first": "Iris",
                        "middle": [],
                        "last": "Hendrickx",
                        "suffix": ""
                    },
                    {
                        "first": "Nam",
                        "middle": [],
                        "last": "Su",
                        "suffix": ""
                    },
                    {
                        "first": "Zornitsa",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "Preslav",
                        "middle": [],
                        "last": "Kozareva",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Nakov",
                        "suffix": ""
                    },
                    {
                        "first": "\u00d3",
                        "middle": [],
                        "last": "Diarmuid",
                        "suffix": ""
                    },
                    {
                        "first": "Sebastian",
                        "middle": [],
                        "last": "S\u00e9aghdha",
                        "suffix": ""
                    },
                    {
                        "first": "Marco",
                        "middle": [],
                        "last": "Pad\u00f3",
                        "suffix": ""
                    },
                    {
                        "first": "Lorenza",
                        "middle": [],
                        "last": "Pennacchiotti",
                        "suffix": ""
                    },
                    {
                        "first": "Stan",
                        "middle": [],
                        "last": "Romano",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Szpakowicz",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "SemEval",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid \u00d3 S\u00e9aghdha, Sebastian Pad\u00f3, Marco Pennacchiotti, Lorenza Romano and Stan Szpakowicz. 2010. SemEval-2010 Task 8: Multi-Way Classication of Semantic Relations Between Pairs of Nominals. In SemEval 2010.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "The Role of Syntax in Vector Space Models of Compositional Semantics",
                "authors": [
                    {
                        "first": "Karl",
                        "middle": [],
                        "last": "Moritz",
                        "suffix": ""
                    },
                    {
                        "first": "Hermann",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Phil",
                        "middle": [],
                        "last": "Blunsom",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "ACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Karl Moritz Hermann and Phil Blunsom. 2013. The Role of Syntax in Vector Space Models of Compositional Se- mantics. In ACL.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Improving neural networks by preventing coadaptation of feature detectors",
                "authors": [
                    {
                        "first": "Geoffrey",
                        "middle": [
                            "E"
                        ],
                        "last": "Hinton",
                        "suffix": ""
                    },
                    {
                        "first": "Nitish",
                        "middle": [],
                        "last": "Srivastava",
                        "suffix": ""
                    },
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Krizhevsky",
                        "suffix": ""
                    },
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    },
                    {
                        "first": "Ruslan",
                        "middle": [
                            "R"
                        ],
                        "last": "Salakhutdinov",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Computational Linguistics",
                "volume": "34",
                "issue": "1",
                "pages": "35--80",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1207.0580"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever and Ruslan R. Salakhutdinov. 2012. Improving neural networks by preventing co- adaptation of feature detectors. In arXiv:1207.0580. Yusuke Miyao and Jun'ichi Tsujii. 2008. Feature Forest Models for Probabilistic HPSG Parsing. In Computa- tional Linguistics, 34(1):35-80, MIT Press.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "UTD: Classifying Semantic Relations by Combining Lexical and Semantic Resources",
                "authors": [
                    {
                        "first": "Bryan",
                        "middle": [],
                        "last": "Rink",
                        "suffix": ""
                    },
                    {
                        "first": "Sanda",
                        "middle": [],
                        "last": "Harabagiu",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Bryan Rink and Sanda Harabagiu. 2010. UTD: Clas- sifying Semantic Relations by Combining Lexical and Semantic Resources. In SemEval 2010.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Semantic Compositionality Through Recursive Matrix-Vector Spaces",
                "authors": [
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    },
                    {
                        "first": "Brody",
                        "middle": [],
                        "last": "Huval",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [
                            "Y"
                        ],
                        "last": "Ng",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "EMNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Richard Socher, Brody Huval, Christopher D. Manning and Andrew Y. Ng. 2012. Semantic Compositionality Through Recursive Matrix-Vector Spaces. In EMNLP.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Parsing with Compositional Vector Grammars",
                "authors": [
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    },
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Bauer",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [
                            "Y"
                        ],
                        "last": "Ng",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "ACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Richard Socher, John Bauer, Christopher D. Manning and Andrew Y. Ng. 2013. Parsing with Compositional Vector Grammars. In ACL.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "A tutorial on stochastic approximation algorithms for training Restricted Boltzmann Machines and Deep Belief Nets",
                "authors": [
                    {
                        "first": "Kevin",
                        "middle": [],
                        "last": "Swersky",
                        "suffix": ""
                    },
                    {
                        "first": "Bo",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Ben",
                        "middle": [],
                        "last": "Marlin",
                        "suffix": ""
                    },
                    {
                        "first": "Nando",
                        "middle": [],
                        "last": "De Freitas",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "ITA workshop",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kevin Swersky, Bo Chen, Ben Marlin and Nando de Fre- itas. 2010. A tutorial on stochastic approximation al- gorithms for training Restricted Boltzmann Machines and Deep Belief Nets. In ITA workshop.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "A Composite Kernel to Extract Relations between Entities with Both Flat and Structured Features",
                "authors": [
                    {
                        "first": "Min",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Jie",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Jian",
                        "middle": [],
                        "last": "Su",
                        "suffix": ""
                    },
                    {
                        "first": "Guodong",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "COL-ING/ACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Min Zhang, Jie Zhang, Jian Su and Guodong Zhou. 2006. A Composite Kernel to Extract Relations between En- tities with Both Flat and Structured Features. In COL- ING/ACL.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 2: Classifying the relation between two entities.",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Figure 3: F1 vs Training iterations.",
                "uris": null,
                "fig_num": "3",
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table><tr><td>Collobert et</td></tr></table>",
                "type_str": "table",
                "text": "Comparison of our model with other methods on SemEval 2010 task 8.",
                "html": null,
                "num": null
            },
            "TABREF1": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Contributions of syntactic and task-specific information and averaging.",
                "html": null,
                "num": null
            }
        }
    }
}