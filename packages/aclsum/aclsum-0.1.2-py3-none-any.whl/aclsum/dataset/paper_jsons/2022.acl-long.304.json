{
    "paper_id": "2022",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:02:13.505867Z"
    },
    "title": "Keywords and Instances: A Hierarchical Contrastive Learning Framework Unifying Hybrid Granularities for Text Generation",
    "authors": [
        {
            "first": "Mingzhe",
            "middle": [],
            "last": "Li",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Peking University",
                "location": {
                    "settlement": "Beijing",
                    "country": "China"
                }
            },
            "email": "li_mingzhe@pku.edu.cn"
        },
        {
            "first": "Xiexiong",
            "middle": [],
            "last": "Lin",
            "suffix": "",
            "affiliation": {},
            "email": "xiexiong.lxx@antfin.com"
        },
        {
            "first": "Xiuying",
            "middle": [],
            "last": "Chen",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Jinxiong",
            "middle": [],
            "last": "Chang",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Qishen",
            "middle": [],
            "last": "Zhang",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Feng",
            "middle": [],
            "last": "Wang",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Taifeng",
            "middle": [],
            "last": "Wang",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Zhongyi",
            "middle": [],
            "last": "Liu",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Wei",
            "middle": [],
            "last": "Chu",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Dongyan",
            "middle": [],
            "last": "Zhao",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Peking University",
                "location": {
                    "settlement": "Beijing",
                    "country": "China"
                }
            },
            "email": ""
        },
        {
            "first": "Rui",
            "middle": [],
            "last": "Yan",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Renmin University of China",
                "location": {}
            },
            "email": ""
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Contrastive learning has achieved impressive success in generation tasks to militate the \"exposure bias\" problem and discriminatively exploit the different quality of references. Existing works mostly focus on contrastive learning on the instance-level without discriminating the contribution of each word, while keywords are the gist of the text and dominant the constrained mapping relationships. Hence, in this work, we propose a hierarchical contrastive learning mechanism, which can unify hybrid granularities semantic meaning in the input text. Concretely, we first propose a keyword graph via contrastive correlations of positive-negative pairs to iteratively polish the keyword representations. Then, we construct intra-contrasts within instance-level and keyword-level, where we assume words are sampled nodes from a sentence distribution. Finally, to bridge the gap between independent contrast levels and tackle the common contrast vanishing problem, we propose an inter-contrast mechanism that measures the discrepancy between contrastive keyword nodes respectively to the instance distribution. Experiments demonstrate that our model outperforms competitive baselines on paraphrasing, dialogue generation, and storytelling tasks.",
    "pdf_parse": {
        "paper_id": "2022",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Contrastive learning has achieved impressive success in generation tasks to militate the \"exposure bias\" problem and discriminatively exploit the different quality of references. Existing works mostly focus on contrastive learning on the instance-level without discriminating the contribution of each word, while keywords are the gist of the text and dominant the constrained mapping relationships. Hence, in this work, we propose a hierarchical contrastive learning mechanism, which can unify hybrid granularities semantic meaning in the input text. Concretely, we first propose a keyword graph via contrastive correlations of positive-negative pairs to iteratively polish the keyword representations. Then, we construct intra-contrasts within instance-level and keyword-level, where we assume words are sampled nodes from a sentence distribution. Finally, to bridge the gap between independent contrast levels and tackle the common contrast vanishing problem, we propose an inter-contrast mechanism that measures the discrepancy between contrastive keyword nodes respectively to the instance distribution. Experiments demonstrate that our model outperforms competitive baselines on paraphrasing, dialogue generation, and storytelling tasks.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Generation tasks such as storytelling, paraphrasing, and dialogue generation aim at learning a certain correlation between text pairs that maps an arbitrary-length input to another arbitrary-length output. Traditional methods are mostly trained with \"teacher forcing\" and lead to an \"exposure bias\" problem (Schmidt, 2019) . Incorporating the generation method with contrastive learning achieved impressive performance on tackling such issues, which takes an extra consideration of synthetic negative samples contrastively (Lee et al., 2021 Existing contrastive mechanisms are mainly focused on the instance level (Lee et al., 2021; Cai et al., 2020) . However, word-level information is also of great importance. Take the case shown in the upper part of Figure 1 for example, the keyword covers the gist of the input text and determines the embedding space of the text. The text representation will be significantly affected if adding a slight perturbation on the keyword, i.e., changing \"cosmology\" to \"astrophysics\". In addition, as shown on the bottom part, under some circumstances, it is too easy for the model to do the classification since the semantic gap between contrastive pairs is huge. Thus, the model fails to distinguish the actual discrepancy, which causes a \"contrast vanishing\" problem at both instance-level and keyword-level.",
                "cite_spans": [
                    {
                        "start": 307,
                        "end": 322,
                        "text": "(Schmidt, 2019)",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 523,
                        "end": 540,
                        "text": "(Lee et al., 2021",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 614,
                        "end": 632,
                        "text": "(Lee et al., 2021;",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 633,
                        "end": 650,
                        "text": "Cai et al., 2020)",
                        "ref_id": "BIBREF1"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 762,
                        "end": 763,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Based on the above motivation, in this paper, we propose a hierarchical contrastive learning method built on top of the classic CVAE structure. We choose CVAE due to its ability in modeling global properties such as syntactic, semantic, and discourse coherence (Li et al., 2015; Yu et al., 2020) . We first learn different granularity representations through two independent contrast, i.e., instancelevel and keyword-level. Specifically, we use the universal and classic TextRank (Mihalcea and Tarau, 2004 ) method to extract keywords from each text, which contain the most important information and need to be highlighted. On the instancelevel, we treat the keyword in the input text as an additional condition for a better prior semantic distribution. Then, we utilize Kullback-Leibler divergence (Kullback and Leibler, 1951) to reduce the distance between prior distribution and positive posterior distribution, and increase the distance with the negative posterior distribution. While on the keyword-level, we propose a keyword graph via contrastive correlations of positive-negative pairs to learn informative and accurate keyword representations. By treating the keyword in the output text as an anchor, the imposter keyword is produced by neighboring nodes of the anchor keyword and forms the keyword-level contrast, where the similarity between the imposter keyword and the anchor keyword is poorer than the positive keyword.",
                "cite_spans": [
                    {
                        "start": 261,
                        "end": 278,
                        "text": "(Li et al., 2015;",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 279,
                        "end": 295,
                        "text": "Yu et al., 2020)",
                        "ref_id": "BIBREF34"
                    },
                    {
                        "start": 480,
                        "end": 505,
                        "text": "(Mihalcea and Tarau, 2004",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 799,
                        "end": 827,
                        "text": "(Kullback and Leibler, 1951)",
                        "ref_id": "BIBREF10"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "To unify individual intra-contrasts and tackle the \"contrast vanishing\" problem in independent contrastive granularities, we leverage an inter-contrast, the Mahalanobis contrast, to investigate the contrastive enhancement based on the Mahalanobis distance (De Maesschalck et al., 2000) , a measure of the distance between a point and a distribution, between the instance distribution and the keyword representation. Concretely, we ensure the distance from the anchor instance distribution to the groundtruth keyword vector is closer than to the imposter keyword vector. The Mahalanobis contrast plays an intermediate role that joins the different granularities contrast via incorporating the distribution of instance with the representation of its crucial part, and makes up a more comprehensive keyworddriven hierarchical contrastive mechanism, so as to ameliorate the generated results.",
                "cite_spans": [
                    {
                        "start": 256,
                        "end": 285,
                        "text": "(De Maesschalck et al., 2000)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "We empirically show that our model outperforms CVAE and other baselines significantly on three generation tasks: paraphrasing, dialogue genera-tion, and storytelling.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Our contributions can be summarized as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\u2022 To our best knowledge, we are the first to propose an inter-level contrastive learning method, which unifies instance-level and keyword-level contrasts in the CVAE framework.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\u2022 We propose three contrastive learning measurements: KL divergence for semantic distribution, cosine distance for points, and Mahalanobis distance for points with distribution.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\u2022 We introduce a global keyword graph to obtain polished keyword representations and construct imposter keywords for contrastive learning.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Contrastive learning is used to learn representations by teaching the model which data points are similar or not. Due to the excellent performance on self-supervised and semi-supervised learning, it has been widely used in natural language processing (NLP). Firstly, Mikolov et al. (2013) proposed to predict neighboring words from context with noise-contrastive estimation. Then, based on word representations, contrastive learning for sentence has been utilized to learn semantic representations. Lee et al. (2021) generated positive and negative examples by adding perturbations to the hidden states. Cai et al. (2020) augmented contrastive dialogue learning with group-wise dual sampling. Moreover, contrastive learning has also been utilized in caption generation (Mao et al., 2016) , summarization (Liu and Liu, 2021) and machine translation (Yang et al., 2019) . Our work differs from previous works in focusing on hierarchical contrastive learning on hybrid granularities.",
                "cite_spans": [
                    {
                        "start": 267,
                        "end": 288,
                        "text": "Mikolov et al. (2013)",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 499,
                        "end": 516,
                        "text": "Lee et al. (2021)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 604,
                        "end": 621,
                        "text": "Cai et al. (2020)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 769,
                        "end": 787,
                        "text": "(Mao et al., 2016)",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 804,
                        "end": 823,
                        "text": "(Liu and Liu, 2021)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 848,
                        "end": 867,
                        "text": "(Yang et al., 2019)",
                        "ref_id": "BIBREF32"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Contrastive Learning",
                "sec_num": "2.1"
            },
            {
                "text": "The Mahalanobis distance is a measure of the distance between a point and a distribution (De Maesschalck et al., 2000) . The distance is zero if the point is on the distribution. Recently, Mahalanobis distance is popularly applied to the NLP tasks (Tran et al., 2019) . Podolskiy et al. (2021) showed that while Transformer is capable of constructing homogeneous representations of in-domain utterances, the Mahalanobis distance captures geometrical disparity from out of domain utterances. Further, Ren et al. (2021) considered that the raw density from deep generative models may fail at out-of-domain detection and proposed to fix this using a likeli-hood ratio between two generative models as a confidence score.",
                "cite_spans": [
                    {
                        "start": 93,
                        "end": 118,
                        "text": "Maesschalck et al., 2000)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 248,
                        "end": 267,
                        "text": "(Tran et al., 2019)",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 270,
                        "end": 293,
                        "text": "Podolskiy et al. (2021)",
                        "ref_id": null
                    },
                    {
                        "start": 500,
                        "end": 517,
                        "text": "Ren et al. (2021)",
                        "ref_id": "BIBREF24"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Mahalanobis Distance",
                "sec_num": "2.2"
            },
            {
                "text": "Variational autoencoder (VAE) was proposed by Kingma and Welling (2013) , and has been widely used in various tasks such as headline generation (Li et al., 2021) , dialogue generation (Serban et al., 2017) and story generation (Yu et al., 2020) . Based on VAE, a more advanced model, Conditional VAE (CVAE), was proposed to generate diverse images conditioned on certain attributes, which was also applied to generate diverse outputs in NLP tasks (Zhao et al., 2017; Qiu et al., 2019) . Existing works concentrate on generating diverse outputs, and we take one step further to utilize prior and posterior latent distribution to compare positive and negative samples, which helps to learn more accurate semantic information.",
                "cite_spans": [
                    {
                        "start": 46,
                        "end": 71,
                        "text": "Kingma and Welling (2013)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 144,
                        "end": 161,
                        "text": "(Li et al., 2021)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 184,
                        "end": 205,
                        "text": "(Serban et al., 2017)",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 227,
                        "end": 244,
                        "text": "(Yu et al., 2020)",
                        "ref_id": "BIBREF34"
                    },
                    {
                        "start": 447,
                        "end": 466,
                        "text": "(Zhao et al., 2017;",
                        "ref_id": "BIBREF36"
                    },
                    {
                        "start": 467,
                        "end": 484,
                        "text": "Qiu et al., 2019)",
                        "ref_id": "BIBREF22"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conditional Variational Auto-Encoder",
                "sec_num": "2.3"
            },
            {
                "text": "3.1 Background VAE: Variational auto-encoder (VAE) is a typical encoder-decoder structural model with certain types of latent variables. Given an input x, VAE models the latent variable z through the prior distribution p \u03b8 (z) , and the observed data x is reconstructed by the generative distribution p \u03b8 (x|z) which is the likelihood function that generates x conditioned on z. Since z is unknown, it should be estimated according to the given data x as p \u03b8 (z|x). While the posterior density p \u03b8 (z|x) = p \u03b8 (x|z)p \u03b8 (z)/p \u03b8 (x) is intractable, VAE introduces a recognition posterior distribution q \u03d5 (z|x) approximates to the true posterior p \u03b8 (z|x). Thus, VAE is trained by optimizing the lower bound on the marginal likelihood of data x as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Method",
                "sec_num": "3"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "logp \u03b8 (x) \u2265 E z\u223cq \u03d5 (z|x) [logp \u03b8 (x|z)] -D KL (q \u03d5 (z|x)||p \u03b8 (z)),",
                        "eq_num": "(1)"
                    }
                ],
                "section": "Method",
                "sec_num": "3"
            },
            {
                "text": "where D KL is the Kullback-Leibler divergence.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Method",
                "sec_num": "3"
            },
            {
                "text": "The conditional variational auto-encoder (CVAE) is the supervised version of VAE with an additional output variable. Giving a dataset {x i , y i } N i=1 consisting of N samples, CVAE is trained to maximize the conditional log-likelihood, and the variational lower bound of the model is written as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "CVAE:",
                "sec_num": null
            },
            {
                "text": "logp \u03b8 (y|x) \u2265 E z\u223cq \u03d5 (z|x,y) [logp(y|x, z)] -D KL (q \u03d5 (z|x, y)||p \u03b8 (z|x)).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "CVAE:",
                "sec_num": null
            },
            {
                "text": "(2)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "CVAE:",
                "sec_num": null
            },
            {
                "text": "Assuming the type of latent variable obeys Gaussian distribution, the first right-hand side term can be approximated by drawing samples {z i } N i=1 from the recognition posterior distribution q \u03d5 (z|x, y), where z \u223c N (\u00b5, \u03c3 2 I), and then objective of the CVAE with Gaussian distribution can be written as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "CVAE:",
                "sec_num": null
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "L cvae (x, y; \u03b8, \u03d5) = - 1 N N i=1 logp \u03b8 (y|x, z i ) +D KL (q \u03d5 (z|x, y)||p \u03b8 (z|x)),",
                        "eq_num": "(3)"
                    }
                ],
                "section": "CVAE:",
                "sec_num": null
            },
            {
                "text": "where",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "CVAE:",
                "sec_num": null
            },
            {
                "text": "z i = g \u03d5 (x, y, \u03f5 i ), \u03f5 i \u223c N (0, I).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "CVAE:",
                "sec_num": null
            },
            {
                "text": "The distribution q \u03d5 (z|x, y) is reparameterized with a differentiable function g \u03d5 , which enables the model trainable via stochastic gradient descent.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "CVAE:",
                "sec_num": null
            },
            {
                "text": "Inspired by Wu et al. (2019) , we add keyword u as an additional condition to the prior distribution to control the generation process, which turns the p \u03b8 (z|x) in Equaton 3 into p \u03b8 (z|x, u).",
                "cite_spans": [
                    {
                        "start": 12,
                        "end": 28,
                        "text": "Wu et al. (2019)",
                        "ref_id": "BIBREF31"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "CVAE:",
                "sec_num": null
            },
            {
                "text": "In this section, we introduce our hierarchical contrastive learning method, which is comprised of three parts: instance-level contrast based on KL divergence (sec.3.2.1), keyword-level contrast based on keyword graph (sec.3.2.2), and inter-contrast: Mahalanobis contrast (sec.3.2.3).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Hierarchical Contrastive Learning",
                "sec_num": "3.2"
            },
            {
                "text": "To tackle the \"exposure bias\" problem and discriminatively exploit the different quality of references, instance-level contrastive learning is introduced to learn discrepancies of targets. Specifically, in addition to the observed input data x and positive output y + , a negative output y -is added to construct a contrastive pair {(x, y + ), (x, y -)}. In this case, the prior distribution p \u03b8 (z|x) is learned from a prior network, which is denoted as f \u03b8 (x). The approximate posteriors q \u03d5 (z|x, y + ) and q \u03d5 (z|x, y -) are learned from a posterior network and represented as f \u03d5 (x, y + ) and f \u03d5 (x, y -), respectively. The objective here is to make the distance between a prior distribution and positive posterior distribution closer than with the negative posterior distribution. Thus, the instance-level contrastive loss function can be written as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Instance-level Contrastive Learning",
                "sec_num": "3.2.1"
            },
            {
                "text": "Lins = -E f \u03d5 [log(1 - e h(f \u03d5 (x,y + ),f \u03b8 (x))/\u03c4 y * \u2208Y e h(f \u03d5 (x,y * ),f \u03b8 (x))/\u03c4 )],",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Instance-level Contrastive Learning",
                "sec_num": "3.2.1"
            },
            {
                "text": "where the y * \u2208 Y can be positive sample y + or negative sample y -, and the \u03c4 is a temperature parameter to control push and pull force. The function h(\u2022) denotes the distance between elements, which is set as Kullback-Leibler divergence (Kullback and Leibler, 1951) in instance-level contrast, D KL (f \u03d5 (x, y * )||f \u03b8 (x)), to measure the difference between two distributions.",
                "cite_spans": [
                    {
                        "start": 239,
                        "end": 267,
                        "text": "(Kullback and Leibler, 1951)",
                        "ref_id": "BIBREF10"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Instance-level Contrastive Learning",
                "sec_num": "3.2.1"
            },
            {
                "text": "Since the instance-level contrast focuses on learning high-level information and fails to discriminate the contribution of each word, we incorporate it with a keyword-level contrast to pay more attention to the specific keyword.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Keyword-level Contrastive Learning",
                "sec_num": "3.2.2"
            },
            {
                "text": "Keyword Graph: Given an input-output text pair (x, y), keywords k x , k y can be extracted from x and y, respectively. For an input text x i with keyword k x,i , input texts that contain the same keyword are gathered into a cluster C i = {x j } n j=1 , k x,j \u2208 x j , where n is the number of texts in C i . Each text x j \u2208 C i has a positive-negative output text pair {(y + j , y - j )} containing a positive output keyword k + y,j and a negative one k - y,j , respectively. Thus, spreading to the entire cluster C i , for the output text y i , there exists positive relations r + i,j between its keyword k y,i and each of the surrounded positive keywords {k + y,j } n j=1 . Likewise, negative relations r - i,j correlates the output keyword k y,i and the surrounded negative ones {k - y,j } n j=1 . Based on these keywords as nodes and their relations as edges (Chen et al., 2021) , the key-word graph G k is constructed. Each node representation h 0 i is initialized as the average BERT embedding (Devlin et al., 2018) of texts in the cluster C i with the same corresponding keyword k x,i . Then, the relation edge r 0 ij that connects node i and node j is learned via a feedforward layer r 0 ij = FFN([h 0 i ; h 0 j ]). Then, the representations of nodes and relation edges are iteratively updated with their connected nodes via the graph attention (GAT) layer and the feed-forward (FFN) layer. In the t-th iteration, we first update each edge representation by paying attention to the connected nodes, denoted as:",
                "cite_spans": [
                    {
                        "start": 862,
                        "end": 881,
                        "text": "(Chen et al., 2021)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 999,
                        "end": 1020,
                        "text": "(Devlin et al., 2018)",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Keyword-level Contrastive Learning",
                "sec_num": "3.2.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "\u03b2 t r * = softmax( (r t ij W p )(h t * W h ) T \u221a d ),",
                        "eq_num": "(4)"
                    }
                ],
                "section": "Keyword-level Contrastive Learning",
                "sec_num": "3.2.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "p t ij = \u03b2 t ri h t i + \u03b2 t rj h t j ,",
                        "eq_num": "(5)"
                    }
                ],
                "section": "Keyword-level Contrastive Learning",
                "sec_num": "3.2.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "r t+1 ij = FFN(r t ij + p t ij ),",
                        "eq_num": "(6)"
                    }
                ],
                "section": "Keyword-level Contrastive Learning",
                "sec_num": "3.2.2"
            },
            {
                "text": "where h t * can be h t i or h t j . Then, based on the obtained edge representation r t+1 ij , we update the node representations considering both the related nodes and relation edges by the graph attention layer, GAT(h t i , h t j , r t ij ), which is designed as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Keyword-level Contrastive Learning",
                "sec_num": "3.2.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "e t ij = (h t i Wq)(h t j W k +r t+1 ij Wr) T \u221a d ,",
                        "eq_num": "(7)"
                    }
                ],
                "section": "Keyword-level Contrastive Learning",
                "sec_num": "3.2.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "\u03b1 t ij = exp(e t ij ) l\u2208N i exp(e t il ) ,",
                        "eq_num": "(8)"
                    }
                ],
                "section": "Keyword-level Contrastive Learning",
                "sec_num": "3.2.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "u t i = j\u2208N i \u03b1 t ij (h t j W v + r t+1 ij ),",
                        "eq_num": "(9)"
                    }
                ],
                "section": "Keyword-level Contrastive Learning",
                "sec_num": "3.2.2"
            },
            {
                "text": "where W q , W k , W r and W v are all learnable parameters, and the \u03b1 t ij is the attention weight between h t i and h t j . Besides, to avoid gradient vanishing after several iterations, a residual connection is added to the output u t i and the updated node representations h t+1 i is obtained. In this way, the new representation of each keyword node consists of the relation dependency information from neighbor nodes N i . We take the node representations from the last iteration as the final keyword representations, denoted as u for brevity.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Keyword-level Contrastive Learning",
                "sec_num": "3.2.2"
            },
            {
                "text": "The keyword-level contrastive learning arises from input keywords against positive output keywords and negative impostor keywords. The input keyword u in is extracted from the input text as an anchor, and the output keyword u out is extracted from ground-truth output text. While the impostor keyword is calculated from the negative neighbours of the output keyword u out , written as u imp = i W i u i , where u i is the representation of keyword node which is obtained by the keyword graph learning procedure described above. In this way, with the help of neighbour nodes in the graph, we can obtain a more indistinguishable and difficult negative sample. The loss of keyword level contrastive learning thus can be written as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Keyword-level Contrast:",
                "sec_num": null
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "L keyword = -E[log e h ( u in ,uout)/\u03c4 u * \u2208U e h(u in ,u * )/\u03c4 ],",
                        "eq_num": "(10)"
                    }
                ],
                "section": "Keyword-level Contrast:",
                "sec_num": null
            },
            {
                "text": "where u * \u2208 U denotes the positive output keyword u out or imposter keyword u imp . In keyword-level contrast, h(\u2022) utilizes cosine similarity to calculate the distance between points.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Keyword-level Contrast:",
                "sec_num": null
            },
            {
                "text": "Note that there exists a space gap between the instance-level contrast and the keyword-level contrast, which disturbs the completeness of this hierarchical contrastive architecture. Besides, the contrastive values vanish when the distance metric is hard to measure the actual discrepancy between positive and negative merely in instance distributions or in keyword representations. To mitigate such problems, we design a Mahalanobis contrastive mechanism to correlate the instance distribution and keyword representation, where the objective is to minimize the margin between the output keyword u out and the posterior semantic distribution q \u03d5 (z|x, y) \u225c f \u03d5 (x, y) and maximize the margin between the imposter keyword u imp and the posterior distribution f \u03d5 (x, y):",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Mahalanobis Contrastive Learning",
                "sec_num": "3.2.3"
            },
            {
                "text": "L ma = -E f \u03d5 [log(1 - e h(f \u03d5 (x,y),uout)/\u03c4",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Mahalanobis Contrastive Learning",
                "sec_num": "3.2.3"
            },
            {
                "text": "u * \u2208U e h(f \u03d5 (x,y),u * )/\u03c4 )], (11) where u * \u2208 U can be the positive output keyword u out or negative imposter keyword u imp . In Mahalanobis contrast, h(\u2022) utilizes Mahalanobis distance (De Maesschalck et al., 2000) to measure the similarity from keyword point to the instance distribution. In the univariate Gaussian case, z \u223c p(z|x, y) = N (\u00b5, \u03c3 2 ), then the h(",
                "cite_spans": [
                    {
                        "start": 190,
                        "end": 219,
                        "text": "(De Maesschalck et al., 2000)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Mahalanobis Contrastive Learning",
                "sec_num": "3.2.3"
            },
            {
                "text": "f \u03d5 (x, y), u * ) \u225c D M A (p \u03b8 (z|x, y)||u * ) = (u * -\u00b5)\u03c3 2 I(u * -\u00b5).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Mahalanobis Contrastive Learning",
                "sec_num": "3.2.3"
            },
            {
                "text": "Finally, we equip the CVAE model with the proposed hierarchical contrastive learning framework to unify hybrid granularities by adding L ins , L keyword and L ma to the reconstructed loss of Equation 3.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Mahalanobis Contrastive Learning",
                "sec_num": "3.2.3"
            },
            {
                "text": "We conduct experiments on three public datasets QQP, Douban, RocStories for paraphrasing, dialogue generation, and storytelling task, respectively. The details of the datasets are as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Tasks and Datasets",
                "sec_num": "4.1"
            },
            {
                "text": "Dialogue (Douban) Douban (Cai et al., 2020) consists of Chinese daily conversations between pairs of speakers, collected from a popular social network website, Douban group1 . The dataset contains 218,039/10,000/10,000 context-response pairs for training/validation/test, with an average of 3.94 turns per context and 38.32 characters per utterance. We concatenate historical dialogues and turn it into a single-turn dialogue training corpus.",
                "cite_spans": [
                    {
                        "start": 25,
                        "end": 43,
                        "text": "(Cai et al., 2020)",
                        "ref_id": "BIBREF1"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Tasks and Datasets",
                "sec_num": "4.1"
            },
            {
                "text": "Paraphrasing (QQP) QQP (Iyer et al., 2017; Wang et al., 2019 ) is a dataset published by the community question-answering website Quora on whether a pair of questions is semantically consistent. To adapt it to the contrastive learning task, we only keep question pairs that have positive and negative rewriting for the same input. Thus, there remain 44,949 samples in the dataset, which are split into training/validation/test sets of 40,441/2,254/2,254 samples.",
                "cite_spans": [
                    {
                        "start": 23,
                        "end": 42,
                        "text": "(Iyer et al., 2017;",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 43,
                        "end": 60,
                        "text": "Wang et al., 2019",
                        "ref_id": "BIBREF30"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Tasks and Datasets",
                "sec_num": "4.1"
            },
            {
                "text": "Storytelling (RocStories) RocStories consists of 98,163 high-quality hand-crafted stories, which capture causal and temporal commonsense relations of daily events (Mostafazadeh et al., 2016) . Each story paragraph contains 5 sentences with an average of 43 words. Following the previous work Yu et al. (2021) , we split the dataset into 8:1:1 for training, validation, and test.",
                "cite_spans": [
                    {
                        "start": 163,
                        "end": 190,
                        "text": "(Mostafazadeh et al., 2016)",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 292,
                        "end": 308,
                        "text": "Yu et al. (2021)",
                        "ref_id": "BIBREF33"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Tasks and Datasets",
                "sec_num": "4.1"
            },
            {
                "text": "For the above three datasets, in order to construct different levels of contrastive learning, we performed the same preprocessing of extracting keywords. We utilize the TextRank model (Mihalcea and Tarau, 2004) to extract keywords from each input and output sample, respectively. Besides, the vocabulary size of both datasets is the same as BERT (Devlin et al., 2018) setting.",
                "cite_spans": [
                    {
                        "start": 184,
                        "end": 210,
                        "text": "(Mihalcea and Tarau, 2004)",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 346,
                        "end": 367,
                        "text": "(Devlin et al., 2018)",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Tasks and Datasets",
                "sec_num": "4.1"
            },
            {
                "text": "Our experiments are implemented in Tensorflow (Abadi et al., 2016) on an NVIDIA Tesla P100 GPU. For our model and all baselines, we follow the same setting as described below. We pad or cut the input to 100, 20, 100 words for dialogue generation, paraphrasing, and storytelling, respectively. The truncation length is decided based on the observation that there is no significant improvement when increasing input length. The minimum decoding step is 5, and the maximum step is 20 for all tasks. Experiments were performed with a batch size of 256, and we use Adam optimizer (Kingma and Ba, 2015) as our optimizing algorithm. During the test stage, the beam-search size is set to 4 for all methods and the checkpoint with the smallest validation loss is chosen. Note that for better performance, our model is built based on BERT, and the decoding process is the same as Transformer (Vaswani et al., 2017) . Finally, due to the limitation of time and memory, small settings are used in the pre-training baselines.",
                "cite_spans": [
                    {
                        "start": 46,
                        "end": 66,
                        "text": "(Abadi et al., 2016)",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 575,
                        "end": 596,
                        "text": "(Kingma and Ba, 2015)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 882,
                        "end": 904,
                        "text": "(Vaswani et al., 2017)",
                        "ref_id": "BIBREF29"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementation Details",
                "sec_num": "4.2"
            },
            {
                "text": "We compare our method against several traditional generation models, pretrained-based generation models, and contrastive learning models.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Compared Baselines",
                "sec_num": "4.3"
            },
            {
                "text": "Traditional generation models: (1) CVAE (Zhao et al., 2017) generates sentences based on latent variables, sampling from potential semantic distribution. (2) Seq2Seq (Sutskever et al., 2014) is a sequence-to-sequence framework combined with attention mechanism and pointer network. (3) Transformer (Vaswani et al., 2017) is an abstractive method based solely on attention mechanisms. Pretrained-based generation models: (4) Seq2Seq-DU (Feng et al., 2021) is concerned with dialogue state tracking in a task-oriented dialogue system. ( 5) DialoGPT (Zhang et al., 2020) proposes a large, tunable neural conversational response generation model trained on more conversation-like exchanges. ( 6) BERT-GEN (Devlin et al., 2018) augments Seq2Seq with BERT as the encoder. ( 7) T5 (Raffel et al., 2020) introduces a unified framework that converts all text-based language problems into a text-to-text format.",
                "cite_spans": [
                    {
                        "start": 40,
                        "end": 59,
                        "text": "(Zhao et al., 2017)",
                        "ref_id": "BIBREF36"
                    },
                    {
                        "start": 166,
                        "end": 190,
                        "text": "(Sutskever et al., 2014)",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 298,
                        "end": 320,
                        "text": "(Vaswani et al., 2017)",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 435,
                        "end": 454,
                        "text": "(Feng et al., 2021)",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 547,
                        "end": 567,
                        "text": "(Zhang et al., 2020)",
                        "ref_id": "BIBREF35"
                    },
                    {
                        "start": 692,
                        "end": 722,
                        "text": "BERT-GEN (Devlin et al., 2018)",
                        "ref_id": null
                    },
                    {
                        "start": 774,
                        "end": 795,
                        "text": "(Raffel et al., 2020)",
                        "ref_id": "BIBREF23"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Compared Baselines",
                "sec_num": "4.3"
            },
            {
                "text": "Contrastive learning methods: (8) Groupwise (Cai et al., 2020) augments contrastive dialogue learning with group-wise dual sampling. (9) T5-CLAPS (Lee et al., 2021) generates negative and positive samples for contrastive learning by adding small and large perturbations, respectively.",
                "cite_spans": [
                    {
                        "start": 44,
                        "end": 62,
                        "text": "(Cai et al., 2020)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 146,
                        "end": 164,
                        "text": "(Lee et al., 2021)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Compared Baselines",
                "sec_num": "4.3"
            },
            {
                "text": "To evaluate the performance of our model against baselines, we adopt the following metrics widely used in existing studies.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation Metrics",
                "sec_num": "4.4"
            },
            {
                "text": "BLEU We utilize BLEU score (Papineni et al., 2002) to measure word overlap between the generated text and the ground-truth. Specifically, following the conventional setting of (Gu et al., 2019) , we adopt BLEU-1\u223c4 scores under the smoothing techniques (smoothing 7).",
                "cite_spans": [
                    {
                        "start": 27,
                        "end": 50,
                        "text": "(Papineni et al., 2002)",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 176,
                        "end": 193,
                        "text": "(Gu et al., 2019)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation Metrics",
                "sec_num": "4.4"
            },
            {
                "text": "Embedding To evaluate our model more comprehensively, we also capture the semantic matching degrees between the bag-of-words (BOW) embeddings of generated text and reference (Gu et al., 2019) . Particularly we adopt three metrics: 1) Extrema, cosine similarity between the largest extreme values among the word embeddings in the two texts; 2) Average, cosine similarity between the averaged word embeddings of generated text and reference; 3) Greedy, greedily matching words in the two texts based on cosine similarities. Secondly, we can find that the performance is significantly improved after adding contrast learning. Finally, our method outperforms T5-CLAPS by 2.7%, 3.6% on QQP, by 20.3%, 24.9% on Douban, and by 3.9%, 6.3% on RocStories in terms of BLEU-1, BLEU-2, respectively, which proves the superiority of our model.",
                "cite_spans": [
                    {
                        "start": 174,
                        "end": 191,
                        "text": "(Gu et al., 2019)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation Metrics",
                "sec_num": "4.4"
            },
            {
                "text": "Human Evaluation We also assessed system performance by eliciting human judgments on 100 randomly selected test instances on QQP dataset. Three annotators are asked to rate paraphrasing questions generated by T5-CLAPS, DialoGPT, Seq2Seq-DU, and our model according to Fluency (Flu), Meaningfulness (Mean), and Differential (Diff). The rating score ranges from 1 to 3, with 3 being the best. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overall Performance Automatic Evaluation The experimental results ars summarized in",
                "sec_num": "4.5.1"
            },
            {
                "text": "To study the hierarchical contrastive learning, we visualize the vectors of keyword, input text, positive and negative output text on randomly sampled cases from QQP dataset, as shown in Figure 3 . For visualization purposes, we reduce the dimension of the latent vector with t-SNE (Maaten and Hinton, 2008) . It can be observed that the input sentence representation is located close to the keyword, which shows that the keyword, as the most important information in the sentence, determines the semantic distribution. Moreover, in contrastive learning, it can be seen that after training, the position of the input sentence is close to the positive samples and far away from the negative samples. This suggests that contrastive learning can correct the semantic distribution. What are the best online sites or apps with games for learning German?",
                "cite_spans": [
                    {
                        "start": 282,
                        "end": 307,
                        "text": "(Maaten and Hinton, 2008)",
                        "ref_id": "BIBREF15"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 194,
                        "end": 195,
                        "text": "3",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Visualization of Different Levels of Contrastive Learning",
                "sec_num": "4.5.3"
            },
            {
                "text": "Which is the best site to learn German ? ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Visualization of Different Levels of Contrastive Learning",
                "sec_num": "4.5.3"
            },
            {
                "text": "We finally investigate the influence of sampling different keywords. As shown in Table 4 , for an input question, we provide keywords extracted by Tex-tRank and randomly-selected keywords as the condition to control the semantic distribution and examine the quality of the generated text. As the most important information unit, different keywords lead to different semantic distributions and will result in different generated texts. The more properly the keywords are selected, the more accurately the sentences will be generated. When utilizing the keywords extracted by TextRank as a condition, the information \"belly fat\" is focused during the generation of paraphrasing questions, and the generated sentences are more accurate. On the contrary, after adding the random-selected keyword \"disposable\", the generated question emphasizes \"one-off exercise\", which brings incorrect information.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 87,
                        "end": 88,
                        "text": "4",
                        "ref_id": "TABREF6"
                    }
                ],
                "eq_spans": [],
                "section": "Analysis of Different Keywords",
                "sec_num": "4.5.4"
            },
            {
                "text": "We also compare our model with several baselines in Table 4 . Most baselines can generate fluent questions in this case. However, they focus on \"lose weight\", and miss the significant information \"belly fat\". Based on the above analysis, we can observe that keywords can emphasize and protect the highlight information in sentences, and affect the semantic distribution of as a condition.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 58,
                        "end": 59,
                        "text": "4",
                        "ref_id": "TABREF6"
                    }
                ],
                "eq_spans": [],
                "section": "Analysis of Different Keywords",
                "sec_num": "4.5.4"
            },
            {
                "text": "In this paper, we propose a hierarchical contrastive learning mechanism, which consists of intra-contrasts within instance-level and keywordlevel and inter-contrast with Mahalanobis contrast. The experimental results yield significant out-performance over baselines when applied in the CVAE framework. In the future, we aim to extend the contrastive learning mechanism to different basic models, and will explore contrastive learning methods based on external knowledge.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "5"
            },
            {
                "text": "https://www.douban.com/",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "We would like to thank the anonymous reviewers for their constructive comments. This work was supported by National Key Research and Development Program of China (No. 2020AAA0106600), National Natural Science Foundation of China (NSFC Grant No. 62122089, No. 61832017 & No. 61876196), and Beijing Outstanding Young Scientist Program No. BJJWZYJH012019100020098. This work was also supported by Alibaba Group through Alibaba Research Intern Program.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgments",
                "sec_num": "6"
            },
            {
                "text": "In this paper, we propose an inter-level contrastive learning method, which unifies instance-level and keyword-level contrasts in the CVAE framework. The positive impact lies in that it can help improve the capability of generation models on paraphrasing, dialogue generation, and storytelling tasks. The negative impact may be that the generation process of the system is not fully controllable, so it is possible to generate inaccurate or unreasonable content in some extreme cases. Hence, extra processing steps might be needed if this method were to be used in scenarios where high accuracy is required.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Ethics Impact",
                "sec_num": "7"
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Tensorflow: A system for large-scale machine learning",
                "authors": [
                    {
                        "first": "Mart\u00edn",
                        "middle": [],
                        "last": "Abadi",
                        "suffix": ""
                    },
                    {
                        "first": "Paul",
                        "middle": [],
                        "last": "Barham",
                        "suffix": ""
                    },
                    {
                        "first": "Jianmin",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Zhifeng",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Andy",
                        "middle": [],
                        "last": "Davis",
                        "suffix": ""
                    },
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Dean",
                        "suffix": ""
                    },
                    {
                        "first": "Matthieu",
                        "middle": [],
                        "last": "Devin",
                        "suffix": ""
                    },
                    {
                        "first": "Sanjay",
                        "middle": [],
                        "last": "Ghemawat",
                        "suffix": ""
                    },
                    {
                        "first": "Geoffrey",
                        "middle": [],
                        "last": "Irving",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Isard",
                        "suffix": ""
                    },
                    {
                        "first": "Manjunath",
                        "middle": [],
                        "last": "Kudlur",
                        "suffix": ""
                    },
                    {
                        "first": "Josh",
                        "middle": [],
                        "last": "Levenberg",
                        "suffix": ""
                    },
                    {
                        "first": "Rajat",
                        "middle": [],
                        "last": "Monga",
                        "suffix": ""
                    },
                    {
                        "first": "Sherry",
                        "middle": [],
                        "last": "Moore",
                        "suffix": ""
                    },
                    {
                        "first": "Derek",
                        "middle": [
                            "Gordon"
                        ],
                        "last": "Murray",
                        "suffix": ""
                    },
                    {
                        "first": "Benoit",
                        "middle": [],
                        "last": "Steiner",
                        "suffix": ""
                    },
                    {
                        "first": "Paul",
                        "middle": [
                            "A"
                        ],
                        "last": "Tucker",
                        "suffix": ""
                    },
                    {
                        "first": "Vijay",
                        "middle": [],
                        "last": "Vasudevan",
                        "suffix": ""
                    },
                    {
                        "first": "Pete",
                        "middle": [],
                        "last": "Warden",
                        "suffix": ""
                    },
                    {
                        "first": "Martin",
                        "middle": [],
                        "last": "Wicke",
                        "suffix": ""
                    },
                    {
                        "first": "Yuan",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaoqiang",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "OSDI",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mart\u00edn Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek Gordon Murray, Benoit Steiner, Paul A. Tucker, Vijay Vasudevan, Pete Warden, Mar- tin Wicke, Yuan Yu, and Xiaoqiang Zhang. 2016. Tensorflow: A system for large-scale machine learn- ing. In OSDI.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Group-wise contrastive learning for neural dialogue generation",
                "authors": [
                    {
                        "first": "Hengyi",
                        "middle": [],
                        "last": "Cai",
                        "suffix": ""
                    },
                    {
                        "first": "Hongshen",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Yonghao",
                        "middle": [],
                        "last": "Song",
                        "suffix": ""
                    },
                    {
                        "first": "Zhuoye",
                        "middle": [],
                        "last": "Ding",
                        "suffix": ""
                    },
                    {
                        "first": "Yongjun",
                        "middle": [],
                        "last": "Bao",
                        "suffix": ""
                    },
                    {
                        "first": "Weipeng",
                        "middle": [],
                        "last": "Yan",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaofang",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, EMNLP 2020, Online Event",
                "volume": "",
                "issue": "",
                "pages": "793--802",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2020.findings-emnlp.70"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Hengyi Cai, Hongshen Chen, Yonghao Song, Zhuoye Ding, Yongjun Bao, Weipeng Yan, and Xiaofang Zhao. 2020. Group-wise contrastive learning for neu- ral dialogue generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Lan- guage Processing: Findings, EMNLP 2020, Online Event, 16-20 November 2020, volume EMNLP 2020 of Findings of ACL, pages 793-802. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Capturing relations between scientific papers: An abstractive model for related work section generation",
                "authors": [
                    {
                        "first": "Xiuying",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Hind",
                        "middle": [],
                        "last": "Alamro",
                        "suffix": ""
                    },
                    {
                        "first": "Mingzhe",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Shen",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Xiangliang",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Dongyan",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Rui",
                        "middle": [],
                        "last": "Yan",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
                "volume": "1",
                "issue": "",
                "pages": "6068--6077",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xiuying Chen, Hind Alamro, Mingzhe Li, Shen Gao, Xi- angliang Zhang, Dongyan Zhao, and Rui Yan. 2021. Capturing relations between scientific papers: An abstractive model for related work section generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Lan- guage Processing (Volume 1: Long Papers), pages 6068-6077.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "The mahalanobis distance. Chemometrics and intelligent laboratory systems",
                "authors": [
                    {
                        "first": "Roy",
                        "middle": [],
                        "last": "De Maesschalck",
                        "suffix": ""
                    },
                    {
                        "first": "Delphine",
                        "middle": [],
                        "last": "Jouan-Rimbaud",
                        "suffix": ""
                    },
                    {
                        "first": "D\u00e9sir\u00e9",
                        "middle": [
                            "L"
                        ],
                        "last": "Massart",
                        "suffix": ""
                    }
                ],
                "year": 2000,
                "venue": "",
                "volume": "50",
                "issue": "",
                "pages": "1--18",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Roy De Maesschalck, Delphine Jouan-Rimbaud, and D\u00e9sir\u00e9 L Massart. 2000. The mahalanobis distance. Chemometrics and intelligent laboratory systems, 50(1):1-18.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
                "authors": [
                    {
                        "first": "Jacob",
                        "middle": [],
                        "last": "Devlin",
                        "suffix": ""
                    },
                    {
                        "first": "Ming-Wei",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "Kenton",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Kristina",
                        "middle": [],
                        "last": "Toutanova",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1810.04805"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understand- ing. arXiv preprint arXiv:1810.04805.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "A sequenceto-sequence approach to dialogue state tracking",
                "authors": [
                    {
                        "first": "Yue",
                        "middle": [],
                        "last": "Feng",
                        "suffix": ""
                    },
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Hang",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yue Feng, Yang Wang, and Hang Li. 2021. A sequence- to-sequence approach to dialogue state tracking. ACL 2021.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "DialogWAE: Multimodal response generation with conditional wasserstein autoencoder",
                "authors": [
                    {
                        "first": "Xiaodong",
                        "middle": [],
                        "last": "Gu",
                        "suffix": ""
                    },
                    {
                        "first": "Kyunghyun",
                        "middle": [],
                        "last": "Cho",
                        "suffix": ""
                    },
                    {
                        "first": "Jung-Woo",
                        "middle": [],
                        "last": "Ha",
                        "suffix": ""
                    },
                    {
                        "first": "Sunghun",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "International Conference on Learning Representations",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xiaodong Gu, Kyunghyun Cho, Jung-Woo Ha, and Sunghun Kim. 2019. DialogWAE: Multimodal re- sponse generation with conditional wasserstein auto- encoder. In International Conference on Learning Representations.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "First quora dataset release: Question pairs",
                "authors": [
                    {
                        "first": "Shankar",
                        "middle": [],
                        "last": "Iyer",
                        "suffix": ""
                    },
                    {
                        "first": "Nikhil",
                        "middle": [],
                        "last": "Dandekar",
                        "suffix": ""
                    },
                    {
                        "first": "Kornel",
                        "middle": [],
                        "last": "Csernai",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Shankar Iyer, Nikhil Dandekar, and Kornel Csernai. 2017. First quora dataset release: Question pairs.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Adam: A method for stochastic optimization",
                "authors": [
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Diederik",
                        "suffix": ""
                    },
                    {
                        "first": "Jimmy",
                        "middle": [],
                        "last": "Kingma",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Ba",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Diederik P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. ICLR.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Autoencoding variational bayes",
                "authors": [
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Diederik",
                        "suffix": ""
                    },
                    {
                        "first": "Max",
                        "middle": [],
                        "last": "Kingma",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Welling",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1312.6114"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Diederik P Kingma and Max Welling. 2013. Auto- encoding variational bayes. arXiv preprint arXiv:1312.6114.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "On information and sufficiency",
                "authors": [
                    {
                        "first": "Solomon",
                        "middle": [],
                        "last": "Kullback",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [
                            "A"
                        ],
                        "last": "Leibler",
                        "suffix": ""
                    }
                ],
                "year": 1951,
                "venue": "The annals of mathematical statistics",
                "volume": "22",
                "issue": "",
                "pages": "79--86",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Solomon Kullback and Richard A Leibler. 1951. On information and sufficiency. The annals of mathe- matical statistics, 22(1):79-86.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Contrastive learning with adversarial perturbations for conditional text generation",
                "authors": [
                    {
                        "first": "Seanie",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Dong",
                        "middle": [],
                        "last": "Bok Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Sung",
                        "middle": [
                            "Ju"
                        ],
                        "last": "Hwang",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "9th International Conference on Learning Representations, ICLR 2021, Virtual Event",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Seanie Lee, Dong Bok Lee, and Sung Ju Hwang. 2021. Contrastive learning with adversarial perturbations for conditional text generation. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenRe- view.net.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "A hierarchical neural autoencoder for paragraphs and documents",
                "authors": [
                    {
                        "first": "Jiwei",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Minh-Thang",
                        "middle": [],
                        "last": "Luong",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Jurafsky",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1506.01057"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jiwei Li, Minh-Thang Luong, and Dan Jurafsky. 2015. A hierarchical neural autoencoder for paragraphs and documents. arXiv preprint arXiv:1506.01057.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "The stylecontent duality of attractiveness: Learning to write eye-catching headlines via disentanglement",
                "authors": [
                    {
                        "first": "Mingzhe",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Xiuying",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Min",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Shen",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Dongyan",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Rui",
                        "middle": [],
                        "last": "Yan",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "volume": "35",
                "issue": "",
                "pages": "13252--13260",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mingzhe Li, Xiuying Chen, Min Yang, Shen Gao, Dongyan Zhao, and Rui Yan. 2021. The style- content duality of attractiveness: Learning to write eye-catching headlines via disentanglement. In Pro- ceedings of the AAAI Conference on Artificial Intelli- gence, volume 35, pages 13252-13260.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Simcls: A simple framework for contrastive learning of abstractive summarization",
                "authors": [
                    {
                        "first": "Yixin",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Pengfei",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2106.01890"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yixin Liu and Pengfei Liu. 2021. Simcls: A simple framework for contrastive learning of abstractive summarization. arXiv preprint arXiv:2106.01890.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Visualizing data using t-sne",
                "authors": [
                    {
                        "first": "Laurens",
                        "middle": [],
                        "last": "Van Der Maaten",
                        "suffix": ""
                    },
                    {
                        "first": "Geoffrey",
                        "middle": [],
                        "last": "Hinton",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Journal of machine learning research",
                "volume": "9",
                "issue": "",
                "pages": "2579--2605",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-sne. Journal of machine learning research, 9(Nov):2579-2605.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Generation and comprehension of unambiguous object descriptions",
                "authors": [
                    {
                        "first": "Junhua",
                        "middle": [],
                        "last": "Mao",
                        "suffix": ""
                    },
                    {
                        "first": "Jonathan",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Alexander",
                        "middle": [],
                        "last": "Toshev",
                        "suffix": ""
                    },
                    {
                        "first": "Oana",
                        "middle": [],
                        "last": "Camburu",
                        "suffix": ""
                    },
                    {
                        "first": "Alan",
                        "middle": [
                            "L"
                        ],
                        "last": "Yuille",
                        "suffix": ""
                    },
                    {
                        "first": "Kevin",
                        "middle": [],
                        "last": "Murphy",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
                "volume": "",
                "issue": "",
                "pages": "11--20",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, and Kevin Murphy. 2016. Generation and comprehension of unambiguous ob- ject descriptions. In Proceedings of the IEEE con- ference on computer vision and pattern recognition, pages 11-20.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Textrank: Bringing order into text",
                "authors": [
                    {
                        "first": "Rada",
                        "middle": [],
                        "last": "Mihalcea",
                        "suffix": ""
                    },
                    {
                        "first": "Paul",
                        "middle": [],
                        "last": "Tarau",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proceedings of the 2004 conference on empirical methods in natural language processing",
                "volume": "",
                "issue": "",
                "pages": "404--411",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Rada Mihalcea and Paul Tarau. 2004. Textrank: Bring- ing order into text. In Proceedings of the 2004 con- ference on empirical methods in natural language processing, pages 404-411.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Distributed representations of words and phrases and their compositionality",
                "authors": [
                    {
                        "first": "Tomas",
                        "middle": [],
                        "last": "Mikolov",
                        "suffix": ""
                    },
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    },
                    {
                        "first": "Kai",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Greg",
                        "middle": [
                            "S"
                        ],
                        "last": "Corrado",
                        "suffix": ""
                    },
                    {
                        "first": "Jeff",
                        "middle": [],
                        "last": "Dean",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Advances in neural information processing systems",
                "volume": "",
                "issue": "",
                "pages": "3111--3119",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor- rado, and Jeff Dean. 2013. Distributed representa- tions of words and phrases and their compositionality. In Advances in neural information processing sys- tems, pages 3111-3119.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "A corpus and cloze evaluation for deeper understanding of commonsense stories",
                "authors": [
                    {
                        "first": "Nasrin",
                        "middle": [],
                        "last": "Mostafazadeh",
                        "suffix": ""
                    },
                    {
                        "first": "Nathanael",
                        "middle": [],
                        "last": "Chambers",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaodong",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Devi",
                        "middle": [],
                        "last": "Parikh",
                        "suffix": ""
                    },
                    {
                        "first": "Dhruv",
                        "middle": [],
                        "last": "Batra",
                        "suffix": ""
                    },
                    {
                        "first": "Lucy",
                        "middle": [],
                        "last": "Vanderwende",
                        "suffix": ""
                    },
                    {
                        "first": "Pushmeet",
                        "middle": [],
                        "last": "Kohli",
                        "suffix": ""
                    },
                    {
                        "first": "James",
                        "middle": [],
                        "last": "Allen",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the 2016 Conference of the North American Chapter",
                "volume": "",
                "issue": "",
                "pages": "839--849",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. 2016. A corpus and cloze evaluation for deeper understanding of commonsense stories. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 839-849.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Bleu: a method for automatic evaluation of machine translation",
                "authors": [
                    {
                        "first": "Kishore",
                        "middle": [],
                        "last": "Papineni",
                        "suffix": ""
                    },
                    {
                        "first": "Salim",
                        "middle": [],
                        "last": "Roukos",
                        "suffix": ""
                    },
                    {
                        "first": "Todd",
                        "middle": [],
                        "last": "Ward",
                        "suffix": ""
                    },
                    {
                        "first": "Wei-Jing",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "ACL",
                "volume": "",
                "issue": "",
                "pages": "311--318",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic evalu- ation of machine translation. In ACL, pages 311-318. ACL.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Ekaterina Artemova, and Irina Piontkovskaya. 2021. Revisiting mahalanobis distance for transformer-based out-of-domain detection",
                "authors": [
                    {
                        "first": "Alexander",
                        "middle": [],
                        "last": "Podolskiy",
                        "suffix": ""
                    },
                    {
                        "first": "Dmitry",
                        "middle": [],
                        "last": "Lipin",
                        "suffix": ""
                    },
                    {
                        "first": "Andrey",
                        "middle": [],
                        "last": "Bout",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "volume": "35",
                "issue": "",
                "pages": "13675--13682",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alexander Podolskiy, Dmitry Lipin, Andrey Bout, Eka- terina Artemova, and Irina Piontkovskaya. 2021. Re- visiting mahalanobis distance for transformer-based out-of-domain detection. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 13675-13682.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Are training samples correlated? learning to generate dialogue responses with multiple references",
                "authors": [
                    {
                        "first": "Lisong",
                        "middle": [],
                        "last": "Qiu",
                        "suffix": ""
                    },
                    {
                        "first": "Juntao",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Bi",
                        "suffix": ""
                    },
                    {
                        "first": "Dongyan",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Rui",
                        "middle": [],
                        "last": "Yan",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "3826--3835",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Lisong Qiu, Juntao Li, Wei Bi, Dongyan Zhao, and Rui Yan. 2019. Are training samples correlated? learning to generate dialogue responses with multiple refer- ences. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3826-3835.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
                "authors": [
                    {
                        "first": "Colin",
                        "middle": [],
                        "last": "Raffel",
                        "suffix": ""
                    },
                    {
                        "first": "Noam",
                        "middle": [],
                        "last": "Shazeer",
                        "suffix": ""
                    },
                    {
                        "first": "Adam",
                        "middle": [],
                        "last": "Roberts",
                        "suffix": ""
                    },
                    {
                        "first": "Katherine",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Sharan",
                        "middle": [],
                        "last": "Narang",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Matena",
                        "suffix": ""
                    },
                    {
                        "first": "Yanqi",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Peter",
                        "middle": [
                            "J"
                        ],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "J. Mach. Learn. Res",
                "volume": "21",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text trans- former. J. Mach. Learn. Res., 21:140:1-140:67.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "A simple fix to mahalanobis distance for improving near-ood detection",
                "authors": [
                    {
                        "first": "Jie",
                        "middle": [],
                        "last": "Ren",
                        "suffix": ""
                    },
                    {
                        "first": "Stanislav",
                        "middle": [],
                        "last": "Fort",
                        "suffix": ""
                    },
                    {
                        "first": "Jeremiah",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Abhijit Guha",
                        "middle": [],
                        "last": "Roy",
                        "suffix": ""
                    },
                    {
                        "first": "Shreyas",
                        "middle": [],
                        "last": "Padhy",
                        "suffix": ""
                    },
                    {
                        "first": "Balaji",
                        "middle": [],
                        "last": "Lakshminarayanan",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2106.09022"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jie Ren, Stanislav Fort, Jeremiah Liu, Abhijit Guha Roy, Shreyas Padhy, and Balaji Lakshminarayanan. 2021. A simple fix to mahalanobis distance for improving near-ood detection. arXiv preprint arXiv:2106.09022.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Generalization in generation: A closer look at exposure bias. EMNLP-IJCNLP",
                "authors": [
                    {
                        "first": "Florian",
                        "middle": [],
                        "last": "Schmidt",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Florian Schmidt. 2019. Generalization in generation: A closer look at exposure bias. EMNLP-IJCNLP 2019, page 157.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "A hierarchical latent variable encoder-decoder model for generating dialogues",
                "authors": [
                    {
                        "first": "Iulian",
                        "middle": [],
                        "last": "Serban",
                        "suffix": ""
                    },
                    {
                        "first": "Alessandro",
                        "middle": [],
                        "last": "Sordoni",
                        "suffix": ""
                    },
                    {
                        "first": "Ryan",
                        "middle": [],
                        "last": "Lowe",
                        "suffix": ""
                    },
                    {
                        "first": "Laurent",
                        "middle": [],
                        "last": "Charlin",
                        "suffix": ""
                    },
                    {
                        "first": "Joelle",
                        "middle": [],
                        "last": "Pineau",
                        "suffix": ""
                    },
                    {
                        "first": "Aaron",
                        "middle": [],
                        "last": "Courville",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "volume": "31",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Iulian Serban, Alessandro Sordoni, Ryan Lowe, Lau- rent Charlin, Joelle Pineau, Aaron Courville, and Yoshua Bengio. 2017. A hierarchical latent variable encoder-decoder model for generating dialogues. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 31.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Sequence to sequence learning with neural networks",
                "authors": [
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    },
                    {
                        "first": "Oriol",
                        "middle": [],
                        "last": "Vinyals",
                        "suffix": ""
                    },
                    {
                        "first": "V",
                        "middle": [],
                        "last": "Quoc",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Le",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "3104--3112",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Sys- tems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Mon- treal, Quebec, Canada, pages 3104-3112.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Adversarial mahalanobis distance-based attentive song recommender for automatic playlist continuation",
                "authors": [
                    {
                        "first": "Thanh",
                        "middle": [],
                        "last": "Tran",
                        "suffix": ""
                    },
                    {
                        "first": "Renee",
                        "middle": [],
                        "last": "Sweeney",
                        "suffix": ""
                    },
                    {
                        "first": "Kyumin",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval",
                "volume": "",
                "issue": "",
                "pages": "245--254",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Thanh Tran, Renee Sweeney, and Kyumin Lee. 2019. Adversarial mahalanobis distance-based attentive song recommender for automatic playlist continu- ation. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 245-254.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Attention is all you need",
                "authors": [
                    {
                        "first": "Ashish",
                        "middle": [],
                        "last": "Vaswani",
                        "suffix": ""
                    },
                    {
                        "first": "Noam",
                        "middle": [],
                        "last": "Shazeer",
                        "suffix": ""
                    },
                    {
                        "first": "Niki",
                        "middle": [],
                        "last": "Parmar",
                        "suffix": ""
                    },
                    {
                        "first": "Jakob",
                        "middle": [],
                        "last": "Uszkoreit",
                        "suffix": ""
                    },
                    {
                        "first": "Llion",
                        "middle": [],
                        "last": "Jones",
                        "suffix": ""
                    },
                    {
                        "first": "Aidan",
                        "middle": [
                            "N"
                        ],
                        "last": "Gomez",
                        "suffix": ""
                    },
                    {
                        "first": "Lukasz",
                        "middle": [],
                        "last": "Kaiser",
                        "suffix": ""
                    },
                    {
                        "first": "Illia",
                        "middle": [],
                        "last": "Polosukhin",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "5998--6008",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Pro- cessing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 5998-6008.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
                "authors": [
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Amanpreet",
                        "middle": [],
                        "last": "Singh",
                        "suffix": ""
                    },
                    {
                        "first": "Julian",
                        "middle": [],
                        "last": "Michael",
                        "suffix": ""
                    },
                    {
                        "first": "Felix",
                        "middle": [],
                        "last": "Hill",
                        "suffix": ""
                    },
                    {
                        "first": "Omer",
                        "middle": [],
                        "last": "Levy",
                        "suffix": ""
                    },
                    {
                        "first": "Samuel",
                        "middle": [
                            "R"
                        ],
                        "last": "Bowman",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019. GLUE: A multi-task benchmark and analysis plat- form for natural language understanding. In the Pro- ceedings of ICLR.",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "Guiding variational response generator to exploit persona",
                "authors": [
                    {
                        "first": "Bowen",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Mengyuan",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Zongsheng",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Yifu",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Derek",
                        "middle": [],
                        "last": "Wong",
                        "suffix": ""
                    },
                    {
                        "first": "Qihang",
                        "middle": [],
                        "last": "Feng",
                        "suffix": ""
                    },
                    {
                        "first": "Junhong",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Baoxun",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1911.02390"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Bowen Wu, Mengyuan Li, Zongsheng Wang, Yifu Chen, Derek Wong, Qihang Feng, Junhong Huang, and Baoxun Wang. 2019. Guiding variational re- sponse generator to exploit persona. arXiv preprint arXiv:1911.02390.",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "Reducing word omission errors in neural machine translation: A contrastive learning approach",
                "authors": [
                    {
                        "first": "Zonghan",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Yong",
                        "middle": [],
                        "last": "Cheng",
                        "suffix": ""
                    },
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Maosong",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zonghan Yang, Yong Cheng, Yang Liu, and Maosong Sun. 2019. Reducing word omission errors in neural machine translation: A contrastive learning approach.",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "Content learning with structure-aware writing: A graph-infused dual conditional variational autoencoder for automatic storytelling",
                "authors": [
                    {
                        "first": "Meng-Hsuan",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Juntao",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Zhangming",
                        "middle": [],
                        "last": "Chan",
                        "suffix": ""
                    },
                    {
                        "first": "Dongyan",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Rui",
                        "middle": [],
                        "last": "Yan",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "volume": "35",
                "issue": "",
                "pages": "6021--6029",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Meng-Hsuan Yu, Juntao Li, Zhangming Chan, Dongyan Zhao, and Rui Yan. 2021. Content learning with structure-aware writing: A graph-infused dual con- ditional variational autoencoder for automatic story- telling. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 6021-6029.",
                "links": null
            },
            "BIBREF34": {
                "ref_id": "b34",
                "title": "Draft and edit: Automatic storytelling through multipass hierarchical conditional variational autoencoder",
                "authors": [
                    {
                        "first": "Meng-Hsuan",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Juntao",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Danyang",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Dongyan",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Rui",
                        "middle": [],
                        "last": "Yan",
                        "suffix": ""
                    },
                    {
                        "first": "Bo",
                        "middle": [],
                        "last": "Tang",
                        "suffix": ""
                    },
                    {
                        "first": "Haisong",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "volume": "34",
                "issue": "",
                "pages": "1741--1748",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Meng-Hsuan Yu, Juntao Li, Danyang Liu, Dongyan Zhao, Rui Yan, Bo Tang, and Haisong Zhang. 2020. Draft and edit: Automatic storytelling through multi- pass hierarchical conditional variational autoencoder. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 1741-1748.",
                "links": null
            },
            "BIBREF35": {
                "ref_id": "b35",
                "title": "Dialogpt: Large-scale generative pre-training for conversational response generation",
                "authors": [
                    {
                        "first": "Yizhe",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Siqi",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Michel",
                        "middle": [],
                        "last": "Galley",
                        "suffix": ""
                    },
                    {
                        "first": "Yen-Chun",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Brockett",
                        "suffix": ""
                    },
                    {
                        "first": "Xiang",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Jianfeng",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Jingjing",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Bill",
                        "middle": [],
                        "last": "Dolan",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "ACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, and Bill Dolan. 2020. Dialogpt: Large-scale generative pre-training for conversational response generation. In ACL, system demonstration.",
                "links": null
            },
            "BIBREF36": {
                "ref_id": "b36",
                "title": "Learning discourse-level diversity for neural dialog models using conditional variational autoencoders",
                "authors": [
                    {
                        "first": "Tiancheng",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Ran",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Maxine",
                        "middle": [],
                        "last": "Eskenazi",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1703.10960"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Tiancheng Zhao, Ran Zhao, and Maxine Eskenazi. 2017. Learning discourse-level diversity for neural dialog models using conditional variational autoencoders. arXiv preprint arXiv:1703.10960.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 3: Visualization of contrastive learning. The square, circle and triangle represents the input text, positive output sample, and negative output sample, respectively. Blue represents the sentence, and yellow represents the keyword.",
                "uris": null,
                "fig_num": "3",
                "type_str": "figure"
            },
            "TABREF3": {
                "content": "<table><tr><td>Models</td><td colspan=\"7\">BLEU-1 BLEU-2 BLEU-3 BLEU-4 Extrema Average Greedy</td></tr><tr><td>ours</td><td>0.6430</td><td>0.3517</td><td>0.1845</td><td>0.1153</td><td>0.6701</td><td>0.8495</td><td>0.8661</td></tr><tr><td>w/o graph</td><td>0.6295</td><td>0.3333</td><td>0.1675</td><td>0.1001</td><td>0.6685</td><td>0.8455</td><td>0.8647</td></tr><tr><td>w/o keyword</td><td>0.5764</td><td>0.2993</td><td>0.1499</td><td>0.0892</td><td>0.6673</td><td>0.8450</td><td>0.8539</td></tr><tr><td>w/o MA</td><td>0.6013</td><td>0.3208</td><td>0.1628</td><td>0.0981</td><td>0.6605</td><td>0.8436</td><td>0.8524</td></tr></table>",
                "type_str": "table",
                "text": "Ablation results on dataset QQP.",
                "html": null,
                "num": null
            },
            "TABREF4": {
                "content": "<table><tr><td/><td>Flu</td><td>Mean</td><td>Diff</td></tr><tr><td>Seq2Seq-DU</td><td>2.03</td><td>2.12</td><td>1.76</td></tr><tr><td>DialoGPT</td><td>2.18</td><td>2.04</td><td>1.97</td></tr><tr><td>T5-CLAPS</td><td>2.24</td><td>2.16</td><td>2.19</td></tr><tr><td>Ours</td><td colspan=\"3\">2.51 \u25b2 2.45 \u25b2 2.43 \u25b2</td></tr><tr><td colspan=\"4\">Table 3: Fluency (Flu), Meaningfulness (Mean), and</td></tr><tr><td colspan=\"4\">Differential (Diff) comparison by human evaluation.</td></tr><tr><td colspan=\"4\">\u25b2 shows the statistical significance tested by a two tailed</td></tr><tr><td>paired t-test.</td><td/><td/><td/></tr><tr><td colspan=\"4\">effects of traditional generation methods such as</td></tr><tr><td colspan=\"4\">Seq2Seq and Transformer, and the lower part</td></tr><tr><td colspan=\"4\">shows the latest pretrained-based methods includ-</td></tr><tr><td colspan=\"4\">ing DialoGPT and T5. Overall, pretrained-based</td></tr><tr><td colspan=\"4\">methods generally outperform traditional methods,</td></tr><tr><td colspan=\"4\">and this also proves the effectiveness of the pre-</td></tr><tr><td colspan=\"4\">trained language model on the generation tasks.</td></tr></table>",
                "type_str": "table",
                "text": "The upper part lists the",
                "html": null,
                "num": null
            },
            "TABREF5": {
                "content": "<table><tr><td>lists the average scores of</td></tr><tr><td>each model, showing that our model outperforms</td></tr><tr><td>other baselines among all metrics, which indicates</td></tr><tr><td>that our model generates paraphrasing sentences</td></tr><tr><td>more readable successfully. The kappa statistics</td></tr><tr><td>are 0.53, 0.61, and 0.56 for fluency, meaningful-</td></tr><tr><td>ness, and differential, respectively, which indicates</td></tr><tr><td>the moderate agreement between annotators.</td></tr></table>",
                "type_str": "table",
                "text": "Input text: What one exercise will help me lose belly fat? Reference paraphrasing text: How do i remove belly fat? Keyword extracted by TextRank: belly fat Generated text 1: What is the best exercise way to lose belly fat? Keyword from random-selected: disposable Generated text 2: Can one-off exercise lose belly fat? Seq2Seq-DU: What are the best ways to lose weight?",
                "html": null,
                "num": null
            },
            "TABREF6": {
                "content": "<table><tr><td>lanobis contrast (w/o MA contrast), and the</td></tr><tr><td>results are shown in Table 2. Concretely, after re-</td></tr><tr><td>moving the keywords (w/o keyword), using only</td></tr><tr><td>instance-level contrastive, the effect of our model is</td></tr><tr><td>greatly reduced by about 10.4%, which illustrates</td></tr><tr><td>the desirability of considering the contributions of</td></tr><tr><td>words in a sentence. On this basis, adding keyword</td></tr><tr><td>contrastive learning with removing the keyword</td></tr><tr><td>graph, the effect of the model has been improved</td></tr><tr><td>but is still lower than our model by 2.1%. This</td></tr><tr><td>shows that keywords are indeed conducive to cap-</td></tr><tr><td>turing important information, and it also illustrates</td></tr><tr><td>the significance of a keyword graph. Finally, the ex-</td></tr><tr><td>periment of removing the Mahalanobis contrastive</td></tr><tr><td>loss indicates that only with granularity indepen-</td></tr><tr><td>dent contrast is not sufficient, and the Mahalanobis</td></tr><tr><td>contrast plays a critical intermediate role.</td></tr></table>",
                "type_str": "table",
                "text": "Case study to verify the influence of sampling different keywords. The texts in red and blue indicate the parts corresponding to the extracted keyword and random-selected keyword, respectively.",
                "html": null,
                "num": null
            }
        }
    }
}