{
    "paper_id": "2022",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:34:58.372526Z"
    },
    "title": "KroneckerBERT: Significant Compression of Pre-trained Language Models Through Kronecker Decomposition and Knowledge Distillation",
    "authors": [
        {
            "first": "Marzieh",
            "middle": [
                "S"
            ],
            "last": "Tahaei",
            "suffix": "",
            "affiliation": {
                "laboratory": "Huawei Noah's Ark Lab",
                "institution": "",
                "location": {}
            },
            "email": "marzieh.tahaei@huawei.com"
        },
        {
            "first": "Ella",
            "middle": [],
            "last": "Charlaix",
            "suffix": "",
            "affiliation": {
                "laboratory": "Huawei Noah's Ark Lab",
                "institution": "",
                "location": {}
            },
            "email": "charlaixe@gmail.com"
        },
        {
            "first": "Vahid",
            "middle": [],
            "last": "Partovi",
            "suffix": "",
            "affiliation": {
                "laboratory": "Huawei Noah's Ark Lab",
                "institution": "",
                "location": {}
            },
            "email": "vahid.partovinia@huawei.com"
        },
        {
            "first": "Ali",
            "middle": [],
            "last": "Ghodsi",
            "suffix": "",
            "affiliation": {
                "laboratory": "Huawei Noah's Ark Lab",
                "institution": "",
                "location": {}
            },
            "email": "ali.ghodsi@uwaterloo.com"
        },
        {
            "first": "Mehdi",
            "middle": [],
            "last": "Rezagholizadeh",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Waterloo",
                "location": {}
            },
            "email": "mehdi.rezagholizadeh@huawei.com"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "The development of over-parameterized pretrained language models has made a significant contribution toward the success of natural language processing. While over-parameterization of these models is the key to their generalization power, it makes them unsuitable for deployment on low-capacity devices. We push the limits of state-of-the-art Transformer-based pre-trained language model compression using Kronecker decomposition. We present our KroneckerBERT, a compressed version of the BERT BASE model obtained by compressing the embedding layer and the linear mappings in the multi-head attention, and the feed-forward network modules in the Transformer layers. Our KroneckerBERT is trained via a very efficient two-stage knowledge distillation scheme using far fewer data samples than state-of-the-art models like MobileBERT and TinyBERT. We evaluate the performance of KroneckerBERT on well-known NLP benchmarks. We show that our KroneckerBERT with compression factors of 7.7\u00d7 and 21\u00d7 outperforms state-of-theart compression methods on the GLUE and SQuAD benchmarks. In particular, using only 13% of the teacher model parameters, it retain more than 99% of the accuracy on the majority of GLUE tasks.",
    "pdf_parse": {
        "paper_id": "2022",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "The development of over-parameterized pretrained language models has made a significant contribution toward the success of natural language processing. While over-parameterization of these models is the key to their generalization power, it makes them unsuitable for deployment on low-capacity devices. We push the limits of state-of-the-art Transformer-based pre-trained language model compression using Kronecker decomposition. We present our KroneckerBERT, a compressed version of the BERT BASE model obtained by compressing the embedding layer and the linear mappings in the multi-head attention, and the feed-forward network modules in the Transformer layers. Our KroneckerBERT is trained via a very efficient two-stage knowledge distillation scheme using far fewer data samples than state-of-the-art models like MobileBERT and TinyBERT. We evaluate the performance of KroneckerBERT on well-known NLP benchmarks. We show that our KroneckerBERT with compression factors of 7.7\u00d7 and 21\u00d7 outperforms state-of-theart compression methods on the GLUE and SQuAD benchmarks. In particular, using only 13% of the teacher model parameters, it retain more than 99% of the accuracy on the majority of GLUE tasks.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "In recent years, the emergence of Pre-trained Language Models (PLMs) has led to a significant breakthrough in Natural Language Processing (NLP). The introduction of Transformers and unsupervised pre-training on enormous unlabeled data are the two main factors that contribute to this success.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Transformer-based models (Devlin et al., 2018; Radford et al., 2019; Yang et al., 2019; Shoeybi et al., 2019) are powerful yet highly overparameterized. The enormous size of these models does not meet the constraints imposed by edge devices on memory, latency, and energy consumption. Therefore there has been a growing interest in developing new methodologies and frameworks for the compression of these large PLMs. Similar to other deep learning models, the main directions for the compression of these models include low-bit quantization (Gong et al., 2014; Prato et al., 2019) , network pruning (Han et al., 2015) , matrix decomposition (Yu et al., 2017; Lioutas et al., 2020) and Knowledge distillation (KD) (Hinton et al., 2015) . These methods are either used in isolation or in combination to improve compression-performance trade-off.",
                "cite_spans": [
                    {
                        "start": 25,
                        "end": 46,
                        "text": "(Devlin et al., 2018;",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 47,
                        "end": 68,
                        "text": "Radford et al., 2019;",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 69,
                        "end": 87,
                        "text": "Yang et al., 2019;",
                        "ref_id": "BIBREF42"
                    },
                    {
                        "start": 88,
                        "end": 109,
                        "text": "Shoeybi et al., 2019)",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 541,
                        "end": 560,
                        "text": "(Gong et al., 2014;",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 561,
                        "end": 580,
                        "text": "Prato et al., 2019)",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 599,
                        "end": 617,
                        "text": "(Han et al., 2015)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 641,
                        "end": 658,
                        "text": "(Yu et al., 2017;",
                        "ref_id": "BIBREF43"
                    },
                    {
                        "start": 659,
                        "end": 680,
                        "text": "Lioutas et al., 2020)",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 713,
                        "end": 734,
                        "text": "(Hinton et al., 2015)",
                        "ref_id": "BIBREF10"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Recent works have been relatively successful in compressing Transformer-based PLMs to a certain degree (Sanh et al., 2019; Sun et al., 2019; Jiao et al., 2019; Sun et al., 2020; Xu et al., 2020; Wang et al., 2020; Kim et al., 2021) ; however, moderate and extreme compression of these models (compression factors >5 and 10 resepctively) is still quite challenging. In particular, several works (Mao et al., 2020; Zhao et al., 2019a Zhao et al., , 2021) ) that have tried to go beyond the compression factor of 10, have done so at the expense of a significant drop in performance.",
                "cite_spans": [
                    {
                        "start": 103,
                        "end": 122,
                        "text": "(Sanh et al., 2019;",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 123,
                        "end": 140,
                        "text": "Sun et al., 2019;",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 141,
                        "end": 159,
                        "text": "Jiao et al., 2019;",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 160,
                        "end": 177,
                        "text": "Sun et al., 2020;",
                        "ref_id": "BIBREF31"
                    },
                    {
                        "start": 178,
                        "end": 194,
                        "text": "Xu et al., 2020;",
                        "ref_id": "BIBREF41"
                    },
                    {
                        "start": 195,
                        "end": 213,
                        "text": "Wang et al., 2020;",
                        "ref_id": "BIBREF37"
                    },
                    {
                        "start": 214,
                        "end": 231,
                        "text": "Kim et al., 2021)",
                        "ref_id": null
                    },
                    {
                        "start": 394,
                        "end": 412,
                        "text": "(Mao et al., 2020;",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 413,
                        "end": 431,
                        "text": "Zhao et al., 2019a",
                        "ref_id": null
                    },
                    {
                        "start": 432,
                        "end": 454,
                        "text": "Zhao et al., , 2021) )",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Following the classical assumption that matrices often follow a low-rank structure, low-rank decomposition methods have been used for compression of weight matrices in deep learning models (Yu et al., 2017; Swaminathan et al., 2020; Winata et al., 2019) and especially Transformer-based models (Noach and Goldberg, 2020; Mao et al., 2020) . However, low-rank decomposition methods only exploit redundancies of the weight matrix in the horizontal and vertical dimensions and thus limit the flexibility of the compressed model. Kronecker decomposition on the other hand exploits redun- dancies in predefined patches and hence allows for more flexibility in their representation. Recent works prove Kronecker product to be more effective in retaining accuracy after compression than SVD (Thakker et al., 2019) .",
                "cite_spans": [
                    {
                        "start": 189,
                        "end": 206,
                        "text": "(Yu et al., 2017;",
                        "ref_id": "BIBREF43"
                    },
                    {
                        "start": 207,
                        "end": 232,
                        "text": "Swaminathan et al., 2020;",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 233,
                        "end": 253,
                        "text": "Winata et al., 2019)",
                        "ref_id": "BIBREF40"
                    },
                    {
                        "start": 294,
                        "end": 320,
                        "text": "(Noach and Goldberg, 2020;",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 321,
                        "end": 338,
                        "text": "Mao et al., 2020)",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 784,
                        "end": 806,
                        "text": "(Thakker et al., 2019)",
                        "ref_id": "BIBREF33"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "This work proposes a novel framework that uses Kronecker decomposition for compression of Transformer-based PLMs and provides a very promising compression-performance trade-off for medium and high compression levels, with 13% and 5% of the original model parameters respectively. We use Kronecker decomposition for the compression of both Transformer layers and the embedding layer. For Transformer layers, the compression is achieved by representing every weight matrix both in the multi-head attention (MHA) and the feed-forward neural network (FFN) as a Kronecker product of two smaller matrices. We also propose a Kronecker decomposition for compression of the embedding layer. Previous works have tried different techniques to reduce the enormous memory consumption of this layer (Khrulkov et al., 2019; Li et al., 2018) . Our Kronecker decomposition method can substantially reduce the amount of required memory while maintaining low computation.",
                "cite_spans": [
                    {
                        "start": 785,
                        "end": 808,
                        "text": "(Khrulkov et al., 2019;",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 809,
                        "end": 825,
                        "text": "Li et al., 2018)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Using Kronecker decomposition for large compression factors leads to a reduction in the model expressiveness. This is due to the nature of the Kronecker product and the fact that elements in this representation are tied together. To address this issue, we propose to distill knowledge from the intermediate layers of the original uncompressed network to the Kronecker network during training.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Training of the state-of-the art BERT compression models (Zhao et al., 2019a,b; Sun et al., 2020 Sun et al., , 2019) ) involve an extensive training which requires vast computational resources. For example in (Sun et al., 2020) , first a specially designed teacher, i.e IB-BERT LARGE is trained from scratch on the en-tire English wikipedia and Book Corpus. The student is then pretrained on the same corpus via KD while undergoing an additional progressive KD phase. Another example is TinyBERT (Jiao et al., 2019) which requires pretraining on the entire English Wikipedia and also uses extensive data augmentation (20\u00d7) for fine-tuning on the downstream tasks. We show that our Kronecker BERT can out perform state-of-the-art with significantly less training requirements. More precisely, our Kronecker-BERT model undergoes a very light pretraining on only 10% of the English Wikipedia for 3 epochs followed by finetuning on the original downstream data.",
                "cite_spans": [
                    {
                        "start": 57,
                        "end": 79,
                        "text": "(Zhao et al., 2019a,b;",
                        "ref_id": null
                    },
                    {
                        "start": 80,
                        "end": 96,
                        "text": "Sun et al., 2020",
                        "ref_id": "BIBREF31"
                    },
                    {
                        "start": 97,
                        "end": 118,
                        "text": "Sun et al., , 2019) )",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 209,
                        "end": 227,
                        "text": "(Sun et al., 2020)",
                        "ref_id": "BIBREF31"
                    },
                    {
                        "start": 496,
                        "end": 515,
                        "text": "(Jiao et al., 2019)",
                        "ref_id": "BIBREF12"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Note that, while our evaluations in this work are limited to BERT, this proposed compression method can be directly used to compress other Transformer-based NLP models. The main contributions of this paper are as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\u2022 Compression of the embedding layer using the Kronecker decomposition with very low computational overhead. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this section, we first go through some of the most related works for BERT compression in the literature and then review the few works that have used Kronecker decomposition for compression of CNNs and RNNs.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "In recent years, many model compression methods have been proposed to reduce the size of PLMs while maintaining their performance on different tasks. KD, which was first introduced by (Bucilu\u01ce et al., 2006) and then later generalized by (Hinton et al., 2015) , is a popular compression method where a small student network is trained to mimic the behavior of a larger teacher network. Recently, using KD for the compression of PLMs has gained a growing interest in the NLP community. BERT-PKD (Sun et al., 2019) , uses KD to transfer knowl-edge from the teacher's intermediate layers to the student in the fine-tuning stage. TinyBERT (Jiao et al., 2019) uses a two-step distillation method applied both at the pre-training and at the finetuning stage. MobileBERT (Sun et al., 2020) To the best of our knowledge, this work is the first attempt to compress Transformer-based language models using Kronecker decomposition. Unlike prior arts, we use a simple Kronecker product of two matrices for the representation of linear layers and uses KD framework to improve the performance.",
                "cite_spans": [
                    {
                        "start": 184,
                        "end": 206,
                        "text": "(Bucilu\u01ce et al., 2006)",
                        "ref_id": null
                    },
                    {
                        "start": 237,
                        "end": 258,
                        "text": "(Hinton et al., 2015)",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 493,
                        "end": 511,
                        "text": "(Sun et al., 2019)",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 634,
                        "end": 653,
                        "text": "(Jiao et al., 2019)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 763,
                        "end": 781,
                        "text": "(Sun et al., 2020)",
                        "ref_id": "BIBREF31"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Pre-trained Language Model Compression",
                "sec_num": "2.1"
            },
            {
                "text": "In this section, we first introduce the background of Kronecker decomposition and then explain our compression method in detail.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Methodology",
                "sec_num": "3"
            },
            {
                "text": "Kronecker product is an operation that is applied on two matrices resulting in a block matrix. Let A be a matrix \u2208 IR m 1 \u00d7n 1 , and let B be a matrix \u2208 IR m 2 \u00d7n 2 , then the Kronecker product of A and B denoted by \u2297 is a block matrix, where each block (i, j) is obtained by multiplying the element A i,j by matrix B. Therefore, the resulting matrix A\u2297B is \u2208 IR m\u00d7n where m = m 1 m 2 and n = n 1 n 2 . Figure 1 illustrates the Kronecker product between two small matrices. See (Graham, 2018) for more detailed information on Kronecker products. Replacing matrix product with Kronecker product replaces the projection of the original linear space by a more constrained linear space in in which the projection angle is defined by the core tensors, see Figure 5 in the appendix.",
                "cite_spans": [
                    {
                        "start": 478,
                        "end": 492,
                        "text": "(Graham, 2018)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 410,
                        "end": 411,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    },
                    {
                        "start": 758,
                        "end": 759,
                        "text": "5",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Kronecker Product",
                "sec_num": "3.1"
            },
            {
                "text": "Given a shape for A and B, i.e. (m 1 , n 1 , m 2 , n 2 ), any matrix W \u2208 IR m\u00d7n , can be approximated as a summation of Kronecker product of matrices A r \u2208 IR m 1 \u00d7n 1 and B r \u2208 IR m 2 \u00d7n 2 :",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Kronecker Decomposition",
                "sec_num": "3.2"
            },
            {
                "text": "W \u2248 I i=1 A i \u2297 B i (1)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Kronecker Decomposition",
                "sec_num": "3.2"
            },
            {
                "text": "we can obtain exact representation of W by setting the number of Kronecker summations I equal to min(m 1 n 1 , m 2 n 2 ). However, in order to achieve compression, a much smaller value of I is often used. In fact prior arts show promising results using a single Kronecker product (Thakker et al., 2019 (Thakker et al., , 2020)) . When decomposing a matrix W \u2208 IR m\u00d7n , as A\u2297B, there are different choices for the shapes of A and B. The dimensions of A i.e m 1 and n 1 can be any factor of m and n respectively, the dimensions of B will subsequently be equal to m 2 = m/m 1 and n 2 = n/n 1 .",
                "cite_spans": [
                    {
                        "start": 280,
                        "end": 301,
                        "text": "(Thakker et al., 2019",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 302,
                        "end": 327,
                        "text": "(Thakker et al., , 2020))",
                        "ref_id": "BIBREF34"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Kronecker Decomposition",
                "sec_num": "3.2"
            },
            {
                "text": "The nearest Kronecker problem is defined as finding matrices A and B that their Kronecker product best approximate a given W (for a given shape of A and B):",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Nearest Kronecker Product",
                "sec_num": "3.2.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "min A,B \u2225W -A \u2297 B\u2225 F .",
                        "eq_num": "(2)"
                    }
                ],
                "section": "The Nearest Kronecker Product",
                "sec_num": "3.2.1"
            },
            {
                "text": "(Van Loan and Pitsianis, 1993) show that this problem can be solved using rank-1 SVD approximation of rearranged W:",
                "cite_spans": [
                    {
                        "start": 14,
                        "end": 30,
                        "text": "Pitsianis, 1993)",
                        "ref_id": "BIBREF35"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Nearest Kronecker Product",
                "sec_num": "3.2.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "min A,B R n 1 ,m 1 (W) -V(A)V(B) \u22a4 F .",
                        "eq_num": "(3)"
                    }
                ],
                "section": "The Nearest Kronecker Product",
                "sec_num": "3.2.1"
            },
            {
                "text": "Here, V is an operation that transforms a matrix to a vector (vectorizes) by stacking its columns and R m 2 ,n 2 is a rearrangement operation that extracts patches of size m 2 \u00d7 n 2 , vectorizes the resulting patches and finally concatenates them together to form a matrix of size m 2 n 2 \u00d7 m 1 n 1 . The rearrangement operation turns the Kronecker product into a matrix of rank one while retaining the Frobenius norm making the minimizations in Eq.3 and Eq.2 equivalant. Hence, the rank-one SVD solution U(:, 1)\u03c3V(:, 1) T can be used to obtain the optimum A and B as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Nearest Kronecker Product",
                "sec_num": "3.2.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "A = V -1 m 1 ,n 1 \u221a \u03c3U(:, 1)",
                        "eq_num": "(4)"
                    }
                ],
                "section": "The Nearest Kronecker Product",
                "sec_num": "3.2.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "B = V -1 m 2 ,n 2 \u221a \u03c3V(:, 1)",
                        "eq_num": "(5)"
                    }
                ],
                "section": "The Nearest Kronecker Product",
                "sec_num": "3.2.1"
            },
            {
                "text": "Here, V -1 m 1 ,n 1 (x) is an operation that transforms a vector x to a matrix of size m 1 \u00d7 n 1 by dividing the vector to columns of size m 1 and concatenating the resulting columns together. Similarly, rank-r SVD decomposition can be used to approximate summation of Kronecker products. We use this method for the initialization of Kronecker layers from the non-compressed model.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Nearest Kronecker Product",
                "sec_num": "3.2.1"
            },
            {
                "text": "By choosing n 1 = 1 and m 2 = 1, A becomes a column vector of size \u2208 IR m\u00d71 and B becomes a row vector of size IR 1\u00d7n , then the Kronecker decomposition becomes equivalent to rank-1 SVD decomposition. Therefore rank-1 SVD is a special case of Kronecker product decomposition and rank-r SVD is a special case of Kronecker product summation decomposition. This indicates that with Kronecker product one can achieve more flexibility than low rank decomposition.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Relation to SVD",
                "sec_num": "3.2.2"
            },
            {
                "text": "When representing W as A \u2297 B, the number of elements is reduced from mn to m 1 n 1 + m 2 n 2 . Moreover, using the Kronecker product to represent linear layers can reduce the required computation. In fact, a linear projection of any vector x can be performed efficiently without explicit reconstruction of A \u2297 B using the following popular property of Kronecker product:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Memory and Computation Reduction",
                "sec_num": "3.2.3"
            },
            {
                "text": "(A \u2297 B)X = V(BV -1 n 2 ,n 1 (X)A \u22a4 ) (6)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Memory and Computation Reduction",
                "sec_num": "3.2.3"
            },
            {
                "text": "where A \u22a4 is A transpose. The consequence of performing multiplication in this way is that it reduces the number of FLOPs from (2m 1 m 2 -1)n 1 n 2 to:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Memory and Computation Reduction",
                "sec_num": "3.2.3"
            },
            {
                "text": "min (2n 2 -1)m 2 n 1 + (2n 1 -1)m 2 m 1 , (2n 1 -1)n 2 m 1 + (2n 2 -1)m 2 m 1 (7)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Memory and Computation Reduction",
                "sec_num": "3.2.3"
            },
            {
                "text": "The embedding layer in large language models is a very large lookup table X \u2208 IR v\u00d7d , where v is the size of the dictionary and d is the embedding dimension. In order to compress X using Kronecker decomposition, the first step is to define the shape of Kronecker factors A E and B E . We define A E to be a matrix of size v \u00d7 d n and B E to be a row vector of size n. There are two reasons for defining B E as a row vector. 1) it allows disentangled embedding of each word since every word has a unique row in A E . 2) the embedding of each word can be obtained efficiently in O(d). More precisely, the embedding for the i'th word in the dictionary can be obtained by the Kronecker product between A E i and B E :",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Kronecker Embedding Layer",
                "sec_num": "3.3"
            },
            {
                "text": "X i = A E i \u2297 B E (8)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Kronecker Embedding Layer",
                "sec_num": "3.3"
            },
            {
                "text": "whereA E is stored as a lookup table. Note that since A E i is of size 1\u00d7 d n and B E is of size 1\u00d7n, the computation complexity of this operation is O(d).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Kronecker Embedding Layer",
                "sec_num": "3.3"
            },
            {
                "text": "Figure 2 shows an illustration of the Kronecker embedding layer. ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Kronecker Embedding Layer",
                "sec_num": "3.3"
            },
            {
                "text": "The Transformer layer is composed of two main components: MHA and FFN. We use Kronecker decomposition to compress both. In the Transformer block, the self-attention mechanism is done by projecting the input into the Key, Query, and Value embeddings and obtaining the attention matrices through the following:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Kronecker Transformer",
                "sec_num": "3.4"
            },
            {
                "text": "O = QK \u22a4 \u221a d k (9) Attention(Q, K, V) = softmax(O)V",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Kronecker Transformer",
                "sec_num": "3.4"
            },
            {
                "text": "where Q, K, and V are obtained by multiplying the input by W Q , W K , W V respectively. In a MHA module, there is a separate W Q l , W K l , and W V l matrix per attention head to allow for a richer representation of the data. In the implementation usually, matrices from all heads are stacked together resulting in 3 matrices W \u2032k , W \u2032Q and W \u2032V . Instead of decomposing the matrices of each head separately, we use Kronecker decomposition after concatenation:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Kronecker Transformer",
                "sec_num": "3.4"
            },
            {
                "text": "W \u2032K = A k \u2297 B K (10) W \u2032Q = A Q \u2297 B Q W \u2032V = A V \u2297 B V",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Kronecker Transformer",
                "sec_num": "3.4"
            },
            {
                "text": "By choosing m 2 to be smaller than the output dimension of each attention head, matrix B in the Kronecker decomposition is shared among all attention heads resulting in more compression. The result of applying Eq.9 is then fed to a linear mapping (W O ) to produce the MHA output. We use Kronecker decomposition for compressing this linear mapping as well the two weight matrices in the subsequent FFN block:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Kronecker Transformer",
                "sec_num": "3.4"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "W O = A O \u2297 B O (11) W 1 = A 1 \u2297 B 1",
                        "eq_num": "(12)"
                    }
                ],
                "section": "Kronecker Transformer",
                "sec_num": "3.4"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "W 2 = A 2 \u2297 B 2",
                        "eq_num": "(13)"
                    }
                ],
                "section": "Kronecker Transformer",
                "sec_num": "3.4"
            },
            {
                "text": "In the following section, we describe how KD is used to improve the training of the KroneckerBERT model.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Knowledge Distillation",
                "sec_num": "3.5"
            },
            {
                "text": "Let S be the student, and T be the teacher, then for a batch of data (X, y), we define f S l (X) andf T l (X) as the output of the l th layer for the student network and the teacher network respectively. The teacher here is the BERT BASE and the student is its corresponding KroneckerBERT that is obtained by replacing the embedding layer and the linear mappings in MHA and FFN modules with Kronecker factors(see Sections 3.3 and 3.4 for details). Note that like other decomposition methods, when we use Kronecker factorization to compress the model, the number of layers and the dimensions of the input and output of each layer remain intact. Therefore, when performing intermediate layer KD, we can directly obtain the difference in the output of a specific layer in the teacher and student networks without the need for projection. In the proposed framework, the intermediate KD from the teacher to student occurs at the embedding layer output, attention matrices and FFN outputs:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Intermediate KD",
                "sec_num": "3.5.1"
            },
            {
                "text": "Model Compression Factor FLOPS W K , W Q , W V , W O W 1 , W 2 T W E Number of",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Intermediate KD",
                "sec_num": "3.5.1"
            },
            {
                "text": "L Embedding (X) = MSE E S , E T L Attention (X) = l MSE O S l , O T l L FFN (X) = l MSE H S l , H T l",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Intermediate KD",
                "sec_num": "3.5.1"
            },
            {
                "text": "where E S and E T are the output of the embedding layer from the student and the teacher respectively. O S l and O T l are the attention matrices (Eq.9), H S l and H T l are the outputs of the FFN, of layer l in the student and the teacher respectively.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Intermediate KD",
                "sec_num": "3.5.1"
            },
            {
                "text": "Our final loss is as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Intermediate KD",
                "sec_num": "3.5.1"
            },
            {
                "text": "L(x, y) = (x,y) L Embedding (x) + (14) L Attention (x) + L FFN (x) + L Logit (x) + L Student (x, y),",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Intermediate KD",
                "sec_num": "3.5.1"
            },
            {
                "text": "where L Student (x) is the supervised loss of the student, e.g. the cross entropy loss when fine-tuning for sequence classification tasks.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Intermediate KD",
                "sec_num": "3.5.1"
            },
            {
                "text": "Inspired by prior works we use KD at the pretraining stage to capture the general domain knowledge from the teacher. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "KD at pre-training",
                "sec_num": "3.5.2"
            },
            {
                "text": "The first step of the proposed framework is to design the Kronecker layers by defining the shape of A and B. Once the shape of one of them is set, the shape of the other one can be obtained accordingly. Therefore we only searched among different choices for m 1 and n 1 which are limited to the factors of the original weight matrix (m and n respectively). We used the same configuration for all the matrices in the MHA. Also For the FFN, we chose the configuration for one layer, and for the other layer, the dimensions are swapped. For the embedding layer, since B E is a row vector, we only need to choose n. The shapes of the Kronecker factors were chosen to obtain the desired compression factor and FLOPS reduction according to Eq.7. To investigate the effect of summation we also selected one configuration with summation of 4 Kronecker products. Similarly, after fixing the number of summation we chose the configuration that provided the desired compression and latency reduction. Table 1 summarises the configuration of Kronecker factorization for the three compression factors used in this work.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 996,
                        "end": 997,
                        "text": "1",
                        "ref_id": "TABREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Model Settings",
                "sec_num": "3.6"
            },
            {
                "text": "For KD at the pre-training stage, the Kronecker-BERT model was initialized using the teacher (pretrained BERT BASE model). This means that for layers that were not compressed like the last layer, the values are copied from the teacher to the student. For initialization of the compressed layers in the pre-training stage, the nearest Kronecker solution explained in section 3.2.1 is used to approximate Kronecker factors (A and B) from the pre-trained BERT BASE model. In the pre-training stage, 10% of the English Wikipedia was used for 3 epochs. Table 2 : Results on the test set of GLUE official benchmark. The results for BERT, BERT 4 -PKD and TinyBERT are taken from (Jiao et al., 2019) . For all other baselines, the results are taken from their associated papers. Note that our KroneckerBERT only performs pre-training KD on 10% of the Wikipedia. Also MobileBERT distils knowledge from a specially designed teacher that is trained from scratch and TinyBERT uses an extensive data augmentation in the fine-tuning stage. The batch size in pre-training was set to 64 and the learning rate was set to e-3. After pre-training, the obtained Kronecker model is used to initialize the Kronecker layers in the student model for task-specific fine-tuning. The Prediction layer is initialized from the fine-tuned BERT BASE teacher.",
                "cite_spans": [
                    {
                        "start": 672,
                        "end": 691,
                        "text": "(Jiao et al., 2019)",
                        "ref_id": "BIBREF12"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 554,
                        "end": 555,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Implementation details",
                "sec_num": "3.7"
            },
            {
                "text": "For fine-tuning on each task, we optimize the hyperparameters based on the performance of the model on the dev set. See appendix for more details on the results and the selected hyperparameters.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model",
                "sec_num": null
            },
            {
                "text": "In this section, we compare our KroneckerBERT with the sate-of-the-art compression methods applied to BERT on GLUE and SQuAD. We also perform an ablation study to investigate the effect of pretraining and KD.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "4"
            },
            {
                "text": "As for baselines we select two main categories of compression methods, those with compression factor <10 and those with compression factor >10.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baselines",
                "sec_num": "4.1"
            },
            {
                "text": "In the first category, we have BERT PKD (Sun et al., 2019) with a low compression factor, and models with similar compression factor as our KroneckerBERT 8 : MobileBERT (Sun et al., 2020) and TinyBERT (Jiao et al., 2019) . We also compare our results to the dynaBERT model (Hou et al., 2020) . For the second category, we compare our results with SharedProject (Zhao et al., 2019a) and LadaBERT (Mao et al., 2020) with compression factors in the rage of 10-20x.",
                "cite_spans": [
                    {
                        "start": 40,
                        "end": 58,
                        "text": "(Sun et al., 2019)",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 169,
                        "end": 187,
                        "text": "(Sun et al., 2020)",
                        "ref_id": "BIBREF31"
                    },
                    {
                        "start": 201,
                        "end": 220,
                        "text": "(Jiao et al., 2019)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 273,
                        "end": 291,
                        "text": "(Hou et al., 2020)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 347,
                        "end": 381,
                        "text": "SharedProject (Zhao et al., 2019a)",
                        "ref_id": null
                    },
                    {
                        "start": 395,
                        "end": 413,
                        "text": "(Mao et al., 2020)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baselines",
                "sec_num": "4.1"
            },
            {
                "text": "We evaluated the proposed framework on the General Language Understanding Evaluation (GLUE) (Wang et al., 2018) benchmark which consists of 9 natural language understanding tasks. We submitted the predictions of our proposed models on the test data sets for different tasks to the official GLUE benchmark (https:// gluebenchmark.com/). Table 2 summarizes the results on GLUE test set for compression factors less than 10. We can see that KroneckerBERT 8 outperforms other baselines in the majority of tasks as well as on average. Moreover, the average performance of KroneckerBERT 8 excluding CoLA is 82.4 which is only 0.5% less than that of the teacher.",
                "cite_spans": [
                    {
                        "start": 92,
                        "end": 111,
                        "text": "(Wang et al., 2018)",
                        "ref_id": "BIBREF36"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 342,
                        "end": 343,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Results on the GLUE Benchmark",
                "sec_num": "4.2"
            },
            {
                "text": "Table 3 shows the results for extreme compression on the GLUE test set. As indicated in the table, the baselines for the higher compression factors only provided results on a limited set of GLUE tasks. We can see that for higher compression factors, KroneckerBERT 21 outperforms the baselines on all available results.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "3",
                        "ref_id": "TABREF6"
                    }
                ],
                "eq_spans": [],
                "section": "Results on the GLUE Benchmark",
                "sec_num": "4.2"
            },
            {
                "text": "In table 4 we compare the performance of Kro-neckerBERT with the dynaBERT model (Hou et al., 2020) . We compare the results on dev set since the results on test set were not provided in their paper. We see that KroneckerBERT can outperform dynaBERT with fewer number of parameters on all GLUE tasks. ",
                "cite_spans": [
                    {
                        "start": 80,
                        "end": 98,
                        "text": "(Hou et al., 2020)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results on the GLUE Benchmark",
                "sec_num": "4.2"
            },
            {
                "text": "In this section, we evaluate the performance of the proposed model on SQuAD datasets. SQuAD1.1 (Rajpurkar et al., 2016 ) is a large-scale reading comprehension which contains questions that have answers in given context. SQuAD2.0 (Kudo and Richardson, 2018 ) also contains unanswerable questions. Table 5 summarises the performance on dev set. For both SQuAD1.1 and SQuAD2.0, KroneckerBERT 8 with fewer number of parameters can significantly outperform both TinyBERT and BERT 4 -PKD baselines. We have also listed the performance of KroneckerBERT 21 . The results of baselines with higher compression factors on SQuAD were not available.",
                "cite_spans": [
                    {
                        "start": 95,
                        "end": 118,
                        "text": "(Rajpurkar et al., 2016",
                        "ref_id": null
                    },
                    {
                        "start": 230,
                        "end": 256,
                        "text": "(Kudo and Richardson, 2018",
                        "ref_id": "BIBREF15"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 303,
                        "end": 304,
                        "text": "5",
                        "ref_id": "TABREF8"
                    }
                ],
                "eq_spans": [],
                "section": "Results on SQuAD",
                "sec_num": "4.3"
            },
            {
                "text": "In this section, we investigate the effect of pretraining and KD in reducing the gap between the original BERT BASE model and the compressed Kro-neckerBERT. the teacher. We perform experiments on 3 tasks from the GLUE benchmark with different sizes of training data, namely MNLI-m, SST-2, and MRPC. For all tasks, the highest performance is obtained when the two-stage KD is used (first row). Note that our light pretraining plays an important row in improving the performance as shown in the first and the second row (with and without pretraining respectively). As the size of the task dataset decreases the effect of pretraining becomes more significant. Also, removing KD from the fine-tuning stage (task-agnostic compression) leads to an accuracy drop on all task. However, the drop is not as pronounced as removing the pretraining stage. It seems that KD in the fine-tuning stage has a larger impact on tasks with larger datasets. We also used t-SNE to visualize the output of the FFN of the middle layer (layer 6) of the finetuned KroneckerBERT 8 with and without KD in comparison with the fine-tuned teacher, on SST-2 dev. Figure 4 shows the results. See how KD helps the features of the middle layer to be more separable with respect to the task compared to the no KD case.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 1137,
                        "end": 1138,
                        "text": "4",
                        "ref_id": "FIGREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Ablation Study",
                "sec_num": "4.4"
            },
            {
                "text": "We introduced a novel method for compressing Transformer-based language models that uses Kronecker decomposition for the compression of the embedding layer and the linear mappings within the Transformer blocks. The proposed framework was used to compress the BERT BASE model. We used a very light two-stage KD method to train the compressed model. We show that the proposed framework can significantly reduce the size and the number of computations while outperforming stateof-the-art. The proposed method can be directly applied for compression of other Transformer-based language models. The combination of the proposed method with other compression techniques such layer truncation, pruning and quantization can be an interesting direction for future work.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "5"
            }
        ],
        "back_matter": [
            {
                "text": "Authors would like to thank Mahdi Zolnouri, Seyed Alireza Ghaffari and Eyy\u00fcb Sari for informative discussions throughout this project. We also would like to thank Aref Jaffari for preparing the out of domain datasets.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgements",
                "sec_num": null
            },
            {
                "text": "In this section, we include more details of our experimental settings presented in Section 4 of the paper.For optimization, we used BERTadam and searched learning rate in range {5e -5, e -4, 5e -4, }. For pretraining the learning rate was set to 1e-13 and the number of epochs was set to 3. The batch size in all experiments were set to 32. For the GLUE benchmark, We searched epochs in range {5-15} for all tasks except CoLA. For CoLA we searched epochs in range {15-30}. This is because similar to other studies (Mosbach et al., 2020; Zhang et al., 2020) we noticed that running CoLA for more epochs is necessary to reduce its sensitivity to random seed. The sequence length at the pre-training stage is set to 512 and at the fine-tuning stage is set to 128 for GLUE benchmark. For SQuAD1.1 and SQuAD2 the sequence length is set 384 and the batch size was set to 64 and the epochs were varied in the range {1-14}. Also, the learning rate was set to e-4 and Table 7 shows the result of the best-performing models on dev set for KroneckerBERT 21 . Tables 8 shows the learning rate for the best-performing models. The training was performed on V100 GPU and the average latency for training of KroneckerBERT 21 for a batch size of 64 was 32ms. All the values are the results of single runs.",
                "cite_spans": [
                    {
                        "start": 514,
                        "end": 536,
                        "text": "(Mosbach et al., 2020;",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 537,
                        "end": 556,
                        "text": "Zhang et al., 2020)",
                        "ref_id": "BIBREF44"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 965,
                        "end": 966,
                        "text": "7",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "A.1 Training Details",
                "sec_num": null
            },
            {
                "text": "It is shown that pre-trained Transformer-based language models are robust to out-of-domain (OOD) samples (Hendrycks et al., 2020) . In this section, we investigate how the proposed compression method affects the OOD robustness of BERT by evaluating the fined-tuned models on MRPC and SST-2 on PAWS (Zhang et al., 2019) and IMDb (Maas et al., 2011) respectively. We compare OOD robustness with the teacher, BERT BASE and Tiny-BERT. TinyBERT fine-tuned checkpoints are obtained from their repository. Table 9 lists the results. KroencekrBERT 8 outperforms TinyBERT on two of the three OOD experiments. We can see the fine-tuned KroneckerBERT 8 models on MRPC is robust to OOD since there is a small increase in performance compared to BERT BASE . On IMDb our KroenckerBERT 8 has a small drop in accuracy (1.5% compared to 9.5% for TinyBert) after compression.",
                "cite_spans": [
                    {
                        "start": 105,
                        "end": 129,
                        "text": "(Hendrycks et al., 2020)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 298,
                        "end": 318,
                        "text": "(Zhang et al., 2019)",
                        "ref_id": "BIBREF45"
                    },
                    {
                        "start": 328,
                        "end": 347,
                        "text": "(Maas et al., 2011)",
                        "ref_id": "BIBREF18"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 505,
                        "end": 506,
                        "text": "9",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "A.2 Out of domain robustness",
                "sec_num": null
            },
            {
                "text": "Table 10 shows the training requirements of different compression methods in terms of their training data. Some models require pretraining a designed teacher from scratch before pretraining the student. KroneckerBERT however only pretrain on 10% of Wikipedia for 3 epochs. For fine-tuning in contrast to TinyBERT our KroneckerBERT model is trained for on the original data. The number of fine-tuning epochs for the majority of the GLUE tasks is less than 15.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 8,
                        "text": "10",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "A.3 Training efficiency",
                "sec_num": null
            },
            {
                "text": "Figure 5 shows a geometrical interpretation of Kronecker product projection versus the original linear projection. It shows how Kronecker product constraints the space of possible projections. The flexibility of this space is a function of the shape of the core matrices A and B. ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "5",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "A.4 Geometrical interpretation of Kronecker product projection",
                "sec_num": null
            },
            {
                "text": "We evaluate the proposed framework on the General Language Understanding Evaluation (GLUE) (Wang et al., 2018) benchmark (https:// gluebenchmark.com/). This benchmark consist of the following tasks in English language: Stanford Sentiment Treebank (SST-2) (Socher et al., 2013) and CoLA (Warstadt et al., 2019) for Sentiment Classification. , on Microsoft Research Paraphrase Corpus (MRPC) (Dolan and Brockett, 2005) Quora Question Pairs (QQP) (Chen et al., 2018) for Paraphrase Similarity Matching, Multi-Genre Natural Language Inference (MNLI) (Williams et al., 2017) , and Recognizing Textual Entailment (RTE) (Bentivogli et al., 2009) for Natural Language inference.We also evaluate the performance of the model on SQuAD datasets (https://rajpurkar. github.io/SQuAD-explore). The datasets are distributed under the CC BY-SA 4.0 license. SQuAD1.1 (Rajpurkar et al., 2016 ) is a large-scale English reading comprehension that contains 87K question that have answers in the training set(10k in the dev set). SQuAD2.0 (Rajpurkar et al., 2018) combines the questions in SQuAD1.1 with over 50,000 unanswerable questions (130k samples in the training and 11k in the dev set).",
                "cite_spans": [
                    {
                        "start": 91,
                        "end": 110,
                        "text": "(Wang et al., 2018)",
                        "ref_id": "BIBREF36"
                    },
                    {
                        "start": 255,
                        "end": 276,
                        "text": "(Socher et al., 2013)",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 286,
                        "end": 309,
                        "text": "(Warstadt et al., 2019)",
                        "ref_id": "BIBREF38"
                    },
                    {
                        "start": 443,
                        "end": 462,
                        "text": "(Chen et al., 2018)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 545,
                        "end": 568,
                        "text": "(Williams et al., 2017)",
                        "ref_id": "BIBREF39"
                    },
                    {
                        "start": 612,
                        "end": 637,
                        "text": "(Bentivogli et al., 2009)",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 849,
                        "end": 872,
                        "text": "(Rajpurkar et al., 2016",
                        "ref_id": null
                    },
                    {
                        "start": 1017,
                        "end": 1041,
                        "text": "(Rajpurkar et al., 2018)",
                        "ref_id": "BIBREF24"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A.5 Datasets",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "The fifth pascal recognizing textual entailment challenge",
                "authors": [
                    {
                        "first": "Luisa",
                        "middle": [],
                        "last": "Bentivogli",
                        "suffix": ""
                    },
                    {
                        "first": "Peter",
                        "middle": [],
                        "last": "Clark",
                        "suffix": ""
                    },
                    {
                        "first": "Ido",
                        "middle": [],
                        "last": "Dagan",
                        "suffix": ""
                    },
                    {
                        "first": "Danilo",
                        "middle": [],
                        "last": "Giampiccolo",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. 2009. The fifth pascal recognizing textual entailment challenge. In TAC.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining",
                "authors": [],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "535--541",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 535-541.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Quora question pairs",
                "authors": [
                    {
                        "first": "Zihan",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Hongbo",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaoji",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Leqi",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zihan Chen, Hongbo Zhang, Xiaoji Zhang, and Leqi Zhao. 2018. Quora question pairs. University of Waterloo.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
                "authors": [
                    {
                        "first": "Jacob",
                        "middle": [],
                        "last": "Devlin",
                        "suffix": ""
                    },
                    {
                        "first": "Ming-Wei",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "Kenton",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Kristina",
                        "middle": [],
                        "last": "Toutanova",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1810.04805"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understand- ing. arXiv preprint arXiv:1810.04805.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Automatically constructing a corpus of sentential paraphrases",
                "authors": [
                    {
                        "first": "B",
                        "middle": [],
                        "last": "William",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Dolan",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Brockett",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of the Third International Workshop on Paraphrasing",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "William B Dolan and Chris Brockett. 2005. Automati- cally constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005).",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Compressing deep convolutional networks using vector quantization",
                "authors": [
                    {
                        "first": "Yunchao",
                        "middle": [],
                        "last": "Gong",
                        "suffix": ""
                    },
                    {
                        "first": "Liu",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Ming",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Lubomir",
                        "middle": [],
                        "last": "Bourdev",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1412.6115"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yunchao Gong, Liu Liu, Ming Yang, and Lubomir Bourdev. 2014. Compressing deep convolutional networks using vector quantization. arXiv preprint arXiv:1412.6115.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Kronecker products and matrix calculus with applications",
                "authors": [
                    {
                        "first": "Alexander",
                        "middle": [],
                        "last": "Graham",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alexander Graham. 2018. Kronecker products and matrix calculus with applications. Courier Dover Publications.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding",
                "authors": [
                    {
                        "first": "Song",
                        "middle": [],
                        "last": "Han",
                        "suffix": ""
                    },
                    {
                        "first": "Huizi",
                        "middle": [],
                        "last": "Mao",
                        "suffix": ""
                    },
                    {
                        "first": "William",
                        "middle": [
                            "J"
                        ],
                        "last": "Dally",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1510.00149"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Song Han, Huizi Mao, and William J Dally. 2015. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman cod- ing. arXiv preprint arXiv:1510.00149.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Pretrained transformers improve out-of-distribution robustness",
                "authors": [
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Hendrycks",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaoyuan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Eric",
                        "middle": [],
                        "last": "Wallace",
                        "suffix": ""
                    },
                    {
                        "first": "Adam",
                        "middle": [],
                        "last": "Dziedzic",
                        "suffix": ""
                    },
                    {
                        "first": "Rishabh",
                        "middle": [],
                        "last": "Krishnan",
                        "suffix": ""
                    },
                    {
                        "first": "Dawn",
                        "middle": [],
                        "last": "Song",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2004.06100"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, and Dawn Song. 2020. Pretrained transformers improve out-of-distribution robustness. arXiv preprint arXiv:2004.06100.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Distilling the knowledge in a neural network",
                "authors": [
                    {
                        "first": "Geoffrey",
                        "middle": [],
                        "last": "Hinton",
                        "suffix": ""
                    },
                    {
                        "first": "Oriol",
                        "middle": [],
                        "last": "Vinyals",
                        "suffix": ""
                    },
                    {
                        "first": "Jeff",
                        "middle": [],
                        "last": "Dean",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1503.02531"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Dynabert: Dynamic bert with adaptive width and depth",
                "authors": [
                    {
                        "first": "Lu",
                        "middle": [],
                        "last": "Hou",
                        "suffix": ""
                    },
                    {
                        "first": "Lifeng",
                        "middle": [],
                        "last": "Shang",
                        "suffix": ""
                    },
                    {
                        "first": "Xin",
                        "middle": [],
                        "last": "Jiang",
                        "suffix": ""
                    },
                    {
                        "first": "Qun",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2004.04037"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Lu Hou, Lifeng Shang, Xin Jiang, and Qun Liu. 2020. Dynabert: Dynamic bert with adaptive width and depth. arXiv preprint arXiv:2004.04037.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Tinybert: Distilling bert for natural language understanding",
                "authors": [
                    {
                        "first": "Xiaoqi",
                        "middle": [],
                        "last": "Jiao",
                        "suffix": ""
                    },
                    {
                        "first": "Yichun",
                        "middle": [],
                        "last": "Yin",
                        "suffix": ""
                    },
                    {
                        "first": "Lifeng",
                        "middle": [],
                        "last": "Shang",
                        "suffix": ""
                    },
                    {
                        "first": "Xin",
                        "middle": [],
                        "last": "Jiang",
                        "suffix": ""
                    },
                    {
                        "first": "Xiao",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Linlin",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Fang",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Qun",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1909.10351"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. 2019. Tinybert: Distilling bert for natural language under- standing. arXiv preprint arXiv:1909.10351.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Tensorized embedding layers for efficient model compression",
                "authors": [
                    {
                        "first": "Valentin",
                        "middle": [],
                        "last": "Khrulkov",
                        "suffix": ""
                    },
                    {
                        "first": "Oleksii",
                        "middle": [],
                        "last": "Hrinchuk",
                        "suffix": ""
                    },
                    {
                        "first": "Leyla",
                        "middle": [],
                        "last": "Mirvakhabova",
                        "suffix": ""
                    },
                    {
                        "first": "Ivan",
                        "middle": [],
                        "last": "Oseledets",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1901.10787"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Valentin Khrulkov, Oleksii Hrinchuk, Leyla Mir- vakhabova, and Ivan Oseledets. 2019. Tensorized embedding layers for efficient model compression. arXiv preprint arXiv:1901.10787.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "2021. I-bert: Integer-only bert quantization",
                "authors": [
                    {
                        "first": "Sehoon",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "Amir",
                        "middle": [],
                        "last": "Gholami",
                        "suffix": ""
                    },
                    {
                        "first": "Zhewei",
                        "middle": [],
                        "last": "Yao",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [
                            "W"
                        ],
                        "last": "Mahoney",
                        "suffix": ""
                    },
                    {
                        "first": "Kurt",
                        "middle": [],
                        "last": "Keutzer",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2101.01321"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. 2021. I-bert: Integer-only bert quantization. arXiv preprint arXiv:2101.01321.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing",
                "authors": [
                    {
                        "first": "Taku",
                        "middle": [],
                        "last": "Kudo",
                        "suffix": ""
                    },
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Richardson",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1808.06226"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Taku Kudo and John Richardson. 2018. Sentencepiece: A simple and language independent subword tok- enizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Slim embedding layers for recurrent neural language models",
                "authors": [
                    {
                        "first": "Zhongliang",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Raymond",
                        "middle": [],
                        "last": "Kulhanek",
                        "suffix": ""
                    },
                    {
                        "first": "Shaojun",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Yunxin",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Shuang",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "volume": "32",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zhongliang Li, Raymond Kulhanek, Shaojun Wang, Yunxin Zhao, and Shuang Wu. 2018. Slim embed- ding layers for recurrent neural language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Improving word embedding factorization for compression using distilled nonlinear neural decomposition",
                "authors": [
                    {
                        "first": "Vasileios",
                        "middle": [],
                        "last": "Lioutas",
                        "suffix": ""
                    },
                    {
                        "first": "Ahmad",
                        "middle": [],
                        "last": "Rashid",
                        "suffix": ""
                    },
                    {
                        "first": "Krtin",
                        "middle": [],
                        "last": "Kumar",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings",
                "volume": "",
                "issue": "",
                "pages": "2774--2784",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Vasileios Lioutas, Ahmad Rashid, Krtin Kumar, Md Ak- mal Haidar, and Mehdi Rezagholizadeh. 2020. Im- proving word embedding factorization for compres- sion using distilled nonlinear neural decomposition. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, pages 2774-2784.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Learning word vectors for sentiment analysis",
                "authors": [
                    {
                        "first": "Andrew",
                        "middle": [
                            "L"
                        ],
                        "last": "Maas",
                        "suffix": ""
                    },
                    {
                        "first": "Raymond",
                        "middle": [
                            "E"
                        ],
                        "last": "Daly",
                        "suffix": ""
                    },
                    {
                        "first": "Peter",
                        "middle": [
                            "T"
                        ],
                        "last": "Pham",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [
                            "Y"
                        ],
                        "last": "Ng",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [],
                        "last": "Potts",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies",
                "volume": "",
                "issue": "",
                "pages": "142--150",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 142-150, Portland, Oregon, USA. Association for Computational Lin- guistics.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Ladabert: Lightweight adaptation of bert through hybrid model compression",
                "authors": [
                    {
                        "first": "Yihuan",
                        "middle": [],
                        "last": "Mao",
                        "suffix": ""
                    },
                    {
                        "first": "Yujing",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Chufan",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Chen",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Yaming",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Quanlu",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Yunhai",
                        "middle": [],
                        "last": "Tong",
                        "suffix": ""
                    },
                    {
                        "first": "Jing",
                        "middle": [],
                        "last": "Bai",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2004.04124"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yihuan Mao, Yujing Wang, Chufan Wu, Chen Zhang, Yang Wang, Yaming Yang, Quanlu Zhang, Yunhai Tong, and Jing Bai. 2020. Ladabert: Lightweight adaptation of bert through hybrid model compression. arXiv preprint arXiv:2004.04124.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "On the stability of fine-tuning bert: Misconceptions, explanations, and strong baselines",
                "authors": [
                    {
                        "first": "Marius",
                        "middle": [],
                        "last": "Mosbach",
                        "suffix": ""
                    },
                    {
                        "first": "Maksym",
                        "middle": [],
                        "last": "Andriushchenko",
                        "suffix": ""
                    },
                    {
                        "first": "Dietrich",
                        "middle": [],
                        "last": "Klakow",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2006.04884"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Marius Mosbach, Maksym Andriushchenko, and Diet- rich Klakow. 2020. On the stability of fine-tuning bert: Misconceptions, explanations, and strong base- lines. arXiv preprint arXiv:2006.04884.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Compressing pre-trained language models by matrix decomposition",
                "authors": [
                    {
                        "first": "Matan",
                        "middle": [],
                        "last": "Ben",
                        "suffix": ""
                    },
                    {
                        "first": "Noach",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Yoav",
                        "middle": [],
                        "last": "Goldberg",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "884--889",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Matan Ben Noach and Yoav Goldberg. 2020. Com- pressing pre-trained language models by ma- trix decomposition. In Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 884-889.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Fully quantized transformer for machine translation",
                "authors": [
                    {
                        "first": "Gabriele",
                        "middle": [],
                        "last": "Prato",
                        "suffix": ""
                    },
                    {
                        "first": "Ella",
                        "middle": [],
                        "last": "Charlaix",
                        "suffix": ""
                    },
                    {
                        "first": "Mehdi",
                        "middle": [],
                        "last": "Rezagholizadeh",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1910.10485"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Gabriele Prato, Ella Charlaix, and Mehdi Reza- gholizadeh. 2019. Fully quantized trans- former for machine translation. arXiv preprint arXiv:1910.10485.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Language models are unsupervised multitask learners",
                "authors": [
                    {
                        "first": "Alec",
                        "middle": [],
                        "last": "Radford",
                        "suffix": ""
                    },
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Rewon",
                        "middle": [],
                        "last": "Child",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Luan",
                        "suffix": ""
                    },
                    {
                        "first": "Dario",
                        "middle": [],
                        "last": "Amodei",
                        "suffix": ""
                    },
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "OpenAI blog",
                "volume": "1",
                "issue": "8",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Know what you don't know: Unanswerable questions for squad",
                "authors": [
                    {
                        "first": "Pranav",
                        "middle": [],
                        "last": "Rajpurkar",
                        "suffix": ""
                    },
                    {
                        "first": "Robin",
                        "middle": [],
                        "last": "Jia",
                        "suffix": ""
                    },
                    {
                        "first": "Percy",
                        "middle": [],
                        "last": "Liang",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don't know: Unanswerable questions for squad.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text",
                "authors": [
                    {
                        "first": "Pranav",
                        "middle": [],
                        "last": "Rajpurkar",
                        "suffix": ""
                    },
                    {
                        "first": "Jian",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Konstantin",
                        "middle": [],
                        "last": "Lopyrev",
                        "suffix": ""
                    },
                    {
                        "first": "Percy",
                        "middle": [],
                        "last": "Liang",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1606.05250"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ Questions for Machine Comprehension of Text. arXiv e-prints, page arXiv:1606.05250.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Squad: 100,000+ questions for machine comprehension of text",
                "authors": [
                    {
                        "first": "Pranav",
                        "middle": [],
                        "last": "Rajpurkar",
                        "suffix": ""
                    },
                    {
                        "first": "Jian",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Konstantin",
                        "middle": [],
                        "last": "Lopyrev",
                        "suffix": ""
                    },
                    {
                        "first": "Percy",
                        "middle": [],
                        "last": "Liang",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
                "authors": [
                    {
                        "first": "Victor",
                        "middle": [],
                        "last": "Sanh",
                        "suffix": ""
                    },
                    {
                        "first": "Lysandre",
                        "middle": [],
                        "last": "Debut",
                        "suffix": ""
                    },
                    {
                        "first": "Julien",
                        "middle": [],
                        "last": "Chaumond",
                        "suffix": ""
                    },
                    {
                        "first": "Thomas",
                        "middle": [],
                        "last": "Wolf",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1910.01108"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Megatron-lm: Training multi-billion parameter language models using model parallelism",
                "authors": [
                    {
                        "first": "Mohammad",
                        "middle": [],
                        "last": "Shoeybi",
                        "suffix": ""
                    },
                    {
                        "first": "Mostofa",
                        "middle": [],
                        "last": "Patwary",
                        "suffix": ""
                    },
                    {
                        "first": "Raul",
                        "middle": [],
                        "last": "Puri",
                        "suffix": ""
                    },
                    {
                        "first": "Patrick",
                        "middle": [],
                        "last": "Legresley",
                        "suffix": ""
                    },
                    {
                        "first": "Jared",
                        "middle": [],
                        "last": "Casper",
                        "suffix": ""
                    },
                    {
                        "first": "Bryan",
                        "middle": [],
                        "last": "Catanzaro",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1909.08053"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catan- zaro. 2019. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
                "authors": [
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    },
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Perelygin",
                        "suffix": ""
                    },
                    {
                        "first": "Jean",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Chuang",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [
                            "Y"
                        ],
                        "last": "Ng",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [],
                        "last": "Potts",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of the 2013 conference on empirical methods in natural language processing",
                "volume": "",
                "issue": "",
                "pages": "1631--1642",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631- 1642.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "Patient knowledge distillation for bert model compression",
                "authors": [
                    {
                        "first": "Siqi",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Yu",
                        "middle": [],
                        "last": "Cheng",
                        "suffix": ""
                    },
                    {
                        "first": "Zhe",
                        "middle": [],
                        "last": "Gan",
                        "suffix": ""
                    },
                    {
                        "first": "Jingjing",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1908.09355"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019. Patient knowledge distillation for bert model com- pression. arXiv preprint arXiv:1908.09355.",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "Mobilebert: a compact task-agnostic bert for resource-limited devices",
                "authors": [
                    {
                        "first": "Zhiqing",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Hongkun",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaodan",
                        "middle": [],
                        "last": "Song",
                        "suffix": ""
                    },
                    {
                        "first": "Renjie",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Yiming",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Denny",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2004.02984"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou. 2020. Mobilebert: a compact task-agnostic bert for resource-limited de- vices. arXiv preprint arXiv:2004.02984.",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "Sparse low rank factorization for deep neural network compression",
                "authors": [
                    {
                        "first": "Deepak",
                        "middle": [],
                        "last": "Sridhar Swaminathan",
                        "suffix": ""
                    },
                    {
                        "first": "Rajkumar",
                        "middle": [],
                        "last": "Garg",
                        "suffix": ""
                    },
                    {
                        "first": "Frederic",
                        "middle": [],
                        "last": "Kannan",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Andres",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Neurocomputing",
                "volume": "398",
                "issue": "",
                "pages": "185--196",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sridhar Swaminathan, Deepak Garg, Rajkumar Kan- nan, and Frederic Andres. 2020. Sparse low rank factorization for deep neural network compression. Neurocomputing, 398:185-196.",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "Compressing rnns for iot devices by 15-38x using kronecker products",
                "authors": [
                    {
                        "first": "Urmish",
                        "middle": [],
                        "last": "Thakker",
                        "suffix": ""
                    },
                    {
                        "first": "Jesse",
                        "middle": [],
                        "last": "Beu",
                        "suffix": ""
                    },
                    {
                        "first": "Dibakar",
                        "middle": [],
                        "last": "Gope",
                        "suffix": ""
                    },
                    {
                        "first": "Chu",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Igor",
                        "middle": [],
                        "last": "Fedorov",
                        "suffix": ""
                    },
                    {
                        "first": "Ganesh",
                        "middle": [],
                        "last": "Dasika",
                        "suffix": ""
                    },
                    {
                        "first": "Matthew",
                        "middle": [],
                        "last": "Mattina",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1906.02876"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Urmish Thakker, Jesse Beu, Dibakar Gope, Chu Zhou, Igor Fedorov, Ganesh Dasika, and Matthew Mat- tina. 2019. Compressing rnns for iot devices by 15-38x using kronecker products. arXiv preprint arXiv:1906.02876.",
                "links": null
            },
            "BIBREF34": {
                "ref_id": "b34",
                "title": "Compressing language models using doped kronecker products",
                "authors": [
                    {
                        "first": "Urmish",
                        "middle": [],
                        "last": "Thakker",
                        "suffix": ""
                    },
                    {
                        "first": "Paul",
                        "middle": [],
                        "last": "Whatamough",
                        "suffix": ""
                    },
                    {
                        "first": "Matthew",
                        "middle": [],
                        "last": "Mattina",
                        "suffix": ""
                    },
                    {
                        "first": "Jesse",
                        "middle": [],
                        "last": "Beu",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2001.08896"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Urmish Thakker, Paul Whatamough, Matthew Mattina, and Jesse Beu. 2020. Compressing language mod- els using doped kronecker products. arXiv preprint arXiv:2001.08896.",
                "links": null
            },
            "BIBREF35": {
                "ref_id": "b35",
                "title": "Approximation with kronecker products",
                "authors": [
                    {
                        "first": "Charles",
                        "middle": [
                            "F"
                        ],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Van",
                        "middle": [],
                        "last": "Loan",
                        "suffix": ""
                    },
                    {
                        "first": "Nikos",
                        "middle": [],
                        "last": "Pitsianis",
                        "suffix": ""
                    }
                ],
                "year": 1993,
                "venue": "Linear algebra for large scale and real-time applications",
                "volume": "",
                "issue": "",
                "pages": "293--314",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Charles F Van Loan and Nikos Pitsianis. 1993. Approx- imation with kronecker products. In Linear algebra for large scale and real-time applications, pages 293- 314. Springer.",
                "links": null
            },
            "BIBREF36": {
                "ref_id": "b36",
                "title": "Glue: A multi-task benchmark and analysis platform for natural language understanding",
                "authors": [
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Amanpreet",
                        "middle": [],
                        "last": "Singh",
                        "suffix": ""
                    },
                    {
                        "first": "Julian",
                        "middle": [],
                        "last": "Michael",
                        "suffix": ""
                    },
                    {
                        "first": "Felix",
                        "middle": [],
                        "last": "Hill",
                        "suffix": ""
                    },
                    {
                        "first": "Omer",
                        "middle": [],
                        "last": "Levy",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Samuel R Bowman",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1804.07461"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 2018. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461.",
                "links": null
            },
            "BIBREF37": {
                "ref_id": "b37",
                "title": "Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers",
                "authors": [
                    {
                        "first": "Wenhui",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Furu",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Li",
                        "middle": [],
                        "last": "Dong",
                        "suffix": ""
                    },
                    {
                        "first": "Hangbo",
                        "middle": [],
                        "last": "Bao",
                        "suffix": ""
                    },
                    {
                        "first": "Nan",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Ming",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2002.10957"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. 2020. Minilm: Deep self-attention distillation for task-agnostic compres- sion of pre-trained transformers. arXiv preprint arXiv:2002.10957.",
                "links": null
            },
            "BIBREF38": {
                "ref_id": "b38",
                "title": "Neural network acceptability judgments",
                "authors": [
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Warstadt",
                        "suffix": ""
                    },
                    {
                        "first": "Amanpreet",
                        "middle": [],
                        "last": "Singh",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Samuel R Bowman",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Transactions of the Association for Computational Linguistics",
                "volume": "7",
                "issue": "",
                "pages": "625--641",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alex Warstadt, Amanpreet Singh, and Samuel R Bow- man. 2019. Neural network acceptability judgments. Transactions of the Association for Computational Linguistics, 7:625-641.",
                "links": null
            },
            "BIBREF39": {
                "ref_id": "b39",
                "title": "A broad-coverage challenge corpus for sentence understanding through inference",
                "authors": [
                    {
                        "first": "Adina",
                        "middle": [],
                        "last": "Williams",
                        "suffix": ""
                    },
                    {
                        "first": "Nikita",
                        "middle": [],
                        "last": "Nangia",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Samuel R Bowman",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1704.05426"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Adina Williams, Nikita Nangia, and Samuel R Bow- man. 2017. A broad-coverage challenge corpus for sentence understanding through inference. arXiv preprint arXiv:1704.05426.",
                "links": null
            },
            "BIBREF40": {
                "ref_id": "b40",
                "title": "On the effectiveness of low-rank matrix factorization for lstm model compression",
                "authors": [
                    {
                        "first": "Genta",
                        "middle": [],
                        "last": "Indra Winata",
                        "suffix": ""
                    },
                    {
                        "first": "Andrea",
                        "middle": [],
                        "last": "Madotto",
                        "suffix": ""
                    },
                    {
                        "first": "Jamin",
                        "middle": [],
                        "last": "Shin",
                        "suffix": ""
                    },
                    {
                        "first": "Elham",
                        "middle": [
                            "J"
                        ],
                        "last": "Barezi",
                        "suffix": ""
                    },
                    {
                        "first": "Pascale",
                        "middle": [],
                        "last": "Fung",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1908.09982"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Genta Indra Winata, Andrea Madotto, Jamin Shin, Elham J Barezi, and Pascale Fung. 2019. On the effectiveness of low-rank matrix factoriza- tion for lstm model compression. arXiv preprint arXiv:1908.09982.",
                "links": null
            },
            "BIBREF41": {
                "ref_id": "b41",
                "title": "Bert-of-theseus: Compressing bert by progressive module replacing",
                "authors": [
                    {
                        "first": "Canwen",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Wangchunshu",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Tao",
                        "middle": [],
                        "last": "Ge",
                        "suffix": ""
                    },
                    {
                        "first": "Furu",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Ming",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei, and Ming Zhou. 2020. Bert-of-theseus: Compressing bert by progressive module replacing.",
                "links": null
            },
            "BIBREF42": {
                "ref_id": "b42",
                "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
                "authors": [
                    {
                        "first": "Zhilin",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Zihang",
                        "middle": [],
                        "last": "Dai",
                        "suffix": ""
                    },
                    {
                        "first": "Yiming",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Jaime",
                        "middle": [],
                        "last": "Carbonell",
                        "suffix": ""
                    },
                    {
                        "first": "Ruslan",
                        "middle": [],
                        "last": "Salakhutdinov",
                        "suffix": ""
                    },
                    {
                        "first": "Quoc V",
                        "middle": [],
                        "last": "Le",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1906.08237"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car- bonell, Ruslan Salakhutdinov, and Quoc V Le. 2019. Xlnet: Generalized autoregressive pretrain- ing for language understanding. arXiv preprint arXiv:1906.08237.",
                "links": null
            },
            "BIBREF43": {
                "ref_id": "b43",
                "title": "On compressing deep models by low rank and sparse decomposition",
                "authors": [
                    {
                        "first": "Xiyu",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Tongliang",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Xinchao",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Dacheng",
                        "middle": [],
                        "last": "Tao",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": "",
                "issue": "",
                "pages": "7370--7379",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xiyu Yu, Tongliang Liu, Xinchao Wang, and Dacheng Tao. 2017. On compressing deep models by low rank and sparse decomposition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7370-7379.",
                "links": null
            },
            "BIBREF44": {
                "ref_id": "b44",
                "title": "Revisiting few-sample bert fine-tuning",
                "authors": [
                    {
                        "first": "Tianyi",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Felix",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Arzoo",
                        "middle": [],
                        "last": "Katiyar",
                        "suffix": ""
                    },
                    {
                        "first": "Kilian",
                        "middle": [
                            "Q"
                        ],
                        "last": "Weinberger",
                        "suffix": ""
                    },
                    {
                        "first": "Yoav",
                        "middle": [],
                        "last": "Artzi",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2006.05987"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Tianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q Wein- berger, and Yoav Artzi. 2020. Revisiting few-sample bert fine-tuning. arXiv preprint arXiv:2006.05987.",
                "links": null
            },
            "BIBREF45": {
                "ref_id": "b45",
                "title": "Paws: Paraphrase adversaries from word scrambling",
                "authors": [
                    {
                        "first": "Yuan",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Baldridge",
                        "suffix": ""
                    },
                    {
                        "first": "Luheng",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1904.01130"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yuan Zhang, Jason Baldridge, and Luheng He. 2019. Paws: Paraphrase adversaries from word scrambling. arXiv preprint arXiv:1904.01130.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 1: An example of Kronecker product of two 2 by 2 matrices.",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Figure 3: Illustration of the proposed framework. Left: A diagram of the teacher BERT model and the student KroneckerBERT. Right: The two-stage KD methodology used to train KroneckerBERT.",
                "uris": null,
                "fig_num": "3",
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "Figure 4: T-SNE visualization of the output of the middle Transformer layer of the fine-tuned models on SST-2 dev. Left: Fine-tuned BERT BASE , middle: KroneckerBERT 8 fine-tuned without KD, right: KroneckerBERT 8 when trained using KD in two stages. The colours indicate the positive and negative classes.",
                "uris": null,
                "fig_num": "4",
                "type_str": "figure"
            },
            "TABREF2": {
                "content": "<table><tr><td>Conventional</td><td>Proposed (Kronecker Embedding)</td></tr><tr><td>Look-up table</td><td>Look-up table</td></tr><tr><td/><td>=</td></tr><tr><td>see</td><td>see</td></tr></table>",
                "type_str": "table",
                "text": "Figure2: Illustration of our proposed method for the compression of the embedding layer. Left: conventional embedding stored in a lookup table. Right: Our proposed compression method where the original embedding matrix is represented as a Kronecker product of a matrix and a row vector. The matrix is stored in a lookup table to minimize computation overhead.",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table><tr><td>sums</td></tr></table>",
                "type_str": "table",
                "text": "Configuration of the Kronecker layers for the three KroneckerBERT models used in this paper. n and m are the input and output dimensions of the weight matrices (W \u2208 IR m\u00d7n ). m 1 , n 1 indicates the shape of the first Kronecker factor (A \u2208 IR m1\u00d7n1 ). For embedding layer we only need to set the size of the row vector B E \u2208 IR 1\u00d7n .",
                "html": null,
                "num": null
            },
            "TABREF6": {
                "content": "<table><tr><td>BERT BASE SharedProject LadaBERT 4 KroneckerBERT 21</td><td>Params MNLI-(m/mm) SST-2 MRPC CoLA QQP QNLI RTE STS-B 108.5M 83.9/83.4 93.4 87.9 52.8 71.1 90.9 67 85.2 5.6M 76.4/75.2 84.7 84.9 -----11M 75.8/76.1 84.0 --67.4 75.1 --5.2M 81.3/80.1 88.4 87.1 28.3 70.5 86.1 64.7 81.3</td></tr></table>",
                "type_str": "table",
                "text": "Results on the test set of the GLUE official benchmark for extreme compression factors. The results of the baselines are taken from their associated papers. LadaBERT and SharedProject refer to(Mao et al., 2020) and  (Zhao et al., 2019a)  respectively.",
                "html": null,
                "num": null
            },
            "TABREF7": {
                "content": "<table><tr><td colspan=\"2\">Model KroneckerBERT5 KroneckerBERT8 14.3M Params MNLI-(m/mm) SST-2 MRPC CoLA QQP QNLI RTE STS-B Avg 20M 82.8/83.5 91.2 87.2 49.5 91.1 89.4 68.9 88.1 81.3 82.8/83.5 91.1 87.5 43.6 90.9 90.7 69.66 88.0 80.9 DynaBERT 27M 83/83.6 91.6 83.1 48.5 91.0 90.0 67.9 88.2 80.8</td></tr><tr><td>Model</td><td>CMP SQuAD1.1 SQuAD2.0 Factor EM F1 EM F1</td></tr><tr><td>BERT BASE BERT 4 -PKD TinyBERT</td><td>1\u00d7 2.1\u00d7 70.1 79.5 60.8 64.6 80.5 88 74.5 77.7 7.5\u00d7 72.7 82.1 68.2 71.8</td></tr><tr><td>KroneckerBERT 8 KroneckerBERT 21</td><td>7.8\u00d7 78.1 86.3 70.4 73.8 21\u00d7 70.7 80.5 66.9 69.3</td></tr></table>",
                "type_str": "table",
                "text": "Results on the dev set of the GLUE. The results for DynaBERT are taken from(Hou et al., 2020)",
                "html": null,
                "num": null
            },
            "TABREF8": {
                "content": "<table><tr><td colspan=\"5\">Pre-training Fine-tuning MNLI-m SST-2 MRPC (393k) (67k) (3.7k) w KD w KD 82.8 91.0 87.5 None w KD 80.7 86.6 70.8</td></tr><tr><td>w KD</td><td>w/o KD</td><td>80.0</td><td>88.8</td><td>86.5</td></tr></table>",
                "type_str": "table",
                "text": "Results of the baselines and KroneckerBERT on question SQuAD dev dataset. The results of the baselines are taken from(Jiao et al., 2019).",
                "html": null,
                "num": null
            },
            "TABREF9": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Ablation study of the effect pretraining and KD in the fine-tuning stage. The results show the performance of the KroneckerBERT 8 on GLUE dev. w and w/o denote with and without, respectively.",
                "html": null,
                "num": null
            },
            "TABREF10": {
                "content": "<table><tr><td>Text</td></tr></table>",
                "type_str": "table",
                "text": "Table 6 summarises the results for KroneckerBERT 8 . Our proposed method uses KD in both the pre-training and the fine-tuning stages. For this ablation study, pre-training is only performed via KD with the pre-trained BERT BASE as",
                "html": null,
                "num": null
            }
        }
    }
}