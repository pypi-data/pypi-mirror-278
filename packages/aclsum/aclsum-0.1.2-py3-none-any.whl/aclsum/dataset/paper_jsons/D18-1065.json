{
    "paper_id": "D18-1065",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:06:40.817114Z"
    },
    "title": "Surprisingly Easy Hard-Attention for Sequence to Sequence Learning",
    "authors": [
        {
            "first": "Shiv",
            "middle": [],
            "last": "Shankar",
            "suffix": "",
            "affiliation": {},
            "email": "shiv_shankar@iitb.ac.in"
        },
        {
            "first": "Siddhant",
            "middle": [],
            "last": "Garg",
            "suffix": "",
            "affiliation": {},
            "email": "sidgarg@cs.wisc.edu"
        },
        {
            "first": "Sunita",
            "middle": [],
            "last": "Sarawagi",
            "suffix": "",
            "affiliation": {},
            "email": "sunita@iitb.ac.in"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "In this paper we show that a simple beam approximation of the joint distribution between attention and output is an easy, accurate, and efficient attention mechanism for sequence to sequence learning. The method combines the advantage of sharp focus in hard attention and the implementation ease of soft attention. On five translation and two morphological inflection tasks we show effortless and consistent gains in BLEU compared to existing attention mechanisms.",
    "pdf_parse": {
        "paper_id": "D18-1065",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "In this paper we show that a simple beam approximation of the joint distribution between attention and output is an easy, accurate, and efficient attention mechanism for sequence to sequence learning. The method combines the advantage of sharp focus in hard attention and the implementation ease of soft attention. On five translation and two morphological inflection tasks we show effortless and consistent gains in BLEU compared to existing attention mechanisms.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "In structured input-output models as used in tasks like translation and image captioning, the attention variable decides which part of the input aligns to the current output. Many attention mechanisms have been proposed (Xu et al., 2015; Bahdanau et al., 2014; Luong et al., 2015; Martins and Astudillo, 2016) but the de facto standard is a soft attention mechanism that first assigns attention weights to input encoder states, then computes an attention weighted 'soft' aligned input state, which finally derives the output distribution. This method is end to end differentiable and easy to implement.",
                "cite_spans": [
                    {
                        "start": 220,
                        "end": 237,
                        "text": "(Xu et al., 2015;",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 238,
                        "end": 260,
                        "text": "Bahdanau et al., 2014;",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 261,
                        "end": 280,
                        "text": "Luong et al., 2015;",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 281,
                        "end": 309,
                        "text": "Martins and Astudillo, 2016)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Another less popular variant is hard attention that aligns each output to exactly one input state but requires intricate training to teach the network to choose that state. When successfully trained, hard attention is often found to be more accurate (Xu et al., 2015; Zaremba and Sutskever, 2015) . In NLP, a recent success has been in a monotonic hard attention setting in morphological inflection tasks (Yu et al., 2016; Aharoni and Goldberg, 2017) . For general seq2seq learning, methods like Sparse-Max (Martins and Astudillo, 2016) and local attention (Luong et al., 2015) were proposed to bridge the gap between soft and hard attention. * Both authors contributed equally to this work",
                "cite_spans": [
                    {
                        "start": 250,
                        "end": 267,
                        "text": "(Xu et al., 2015;",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 268,
                        "end": 296,
                        "text": "Zaremba and Sutskever, 2015)",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 405,
                        "end": 422,
                        "text": "(Yu et al., 2016;",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 423,
                        "end": 450,
                        "text": "Aharoni and Goldberg, 2017)",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 507,
                        "end": 536,
                        "text": "(Martins and Astudillo, 2016)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 557,
                        "end": 577,
                        "text": "(Luong et al., 2015)",
                        "ref_id": "BIBREF10"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this paper we propose a surprisingly simpler alternative based on the original joint distribution between output and attention, of which existing soft and hard attention mechanisms are approximations. The joint model couples input states individually to the output like in hard attention, but it combines the advantage of end-to-end trainability of soft attention. When the number of input states is large, we propose to use a simple approximation of the full joint distribution called Beam-joint. This approximation is also easily trainable and does not suffer from the high variance of Monte-Carlo sampling gradients of hard attention.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "We evaluated our model on five translation tasks and increased BLEU by 0.8 to 1.7 over soft attention, which in turn was better than hard and the recent Sparsemax (Martins and Astudillo, 2016) attention. More importantly, the training process was as easy as soft attention. For further support, we also evaluate on two morphological inflection tasks and got gains over soft and hard attention.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "For sequence to sequence (seq2seq) learning the encoder-decoder model is the standard and we review it here. We then review related work on attention mechanisms on these models.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Background and Related Work",
                "sec_num": "2"
            },
            {
                "text": "Let x 1 , . . . , x m denote the tokens in the input sequence that have been transformed by an encoder network to state vectors x 1 , . . . , x m , which we jointly denote as x 1...m . Let y 1 , . . . , y n denote the output tokens in the target sequence. The Encoder-Decoder (ED) network factorizes Pr(y 1 , . . . , y n |x 1...m ) as n t=1 Pr(y t |x 1...m , s t ) where s t is a decoder state summarizing y 1 , . . . y t-1 . For each t, a hidden attention variable a t is used to denote which part of x 1...m aligns with y t . Let P (a t = j|x 1...m , s t ) denote the probability that encoder state x j is relevant for output y t . Typically this is estimated using a softmax function over attention scores computed from x j and decoder state s t as follows. xr,st) (1)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Attention-based Encoder Decoder Model",
                "sec_num": "2.1"
            },
            {
                "text": "P (a t = j|x 1...m , s t ) = e A \u03b8 (x j ,st) m r=1 e A \u03b8 (",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Attention-based Encoder Decoder Model",
                "sec_num": "2.1"
            },
            {
                "text": "where A \u03b8 (., .) is the attention unit that scores each input state x j as per the decoder state s t . Thereafter, in the popular soft-attention mechanism, the attention weighted sum of the input states is used to model log likelihood for each y t as log Pr(y",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Attention-based Encoder Decoder Model",
                "sec_num": "2.1"
            },
            {
                "text": "t |x 1...m ) = log Pr(y t | a P t (a)x a ) (2)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Attention-based Encoder Decoder Model",
                "sec_num": "2.1"
            },
            {
                "text": "where P t (a t = j) is the short form for P (a t = j|x 1...m , s t ). Also, here and in the rest of the paper we drop s t from P (y t ) and P t (a) for ease of notation. The weighted sum a P t (a)x a is called an input context c t which is fed to the decoder RNN along with y t for computing the next state s t+1 .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Attention-based Encoder Decoder Model",
                "sec_num": "2.1"
            },
            {
                "text": "We next review existing attention types.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2.2"
            },
            {
                "text": "Soft Attention is the attention method described in the previous section and is the current standard for seq2seq learning (Xu Chen, 2018; Koehn, 2017) . It was proposed for translation in (Bahdanau et al., 2014) and refined further in (Luong et al., 2015) . As shown in Eq 2, here each output is derived from an attention averaged input. This diffuses the coupling between the input and output. The advantage of soft attention is end to end differentiability, and fast training and inference.",
                "cite_spans": [
                    {
                        "start": 122,
                        "end": 137,
                        "text": "(Xu Chen, 2018;",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 138,
                        "end": 150,
                        "text": "Koehn, 2017)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 188,
                        "end": 211,
                        "text": "(Bahdanau et al., 2014)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 235,
                        "end": 255,
                        "text": "(Luong et al., 2015)",
                        "ref_id": "BIBREF10"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2.2"
            },
            {
                "text": "Hard Attention was proposed in its current form in (Xu et al., 2015) and attends to exactly one input state for an output1 . During training, log-likelihood is an expectation over sampled attentions:",
                "cite_spans": [
                    {
                        "start": 51,
                        "end": 68,
                        "text": "(Xu et al., 2015)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2.2"
            },
            {
                "text": "log P t (y t |x 1...m ) = M l=1 log P t (y t |x \u00e3l ) (3)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2.2"
            },
            {
                "text": "where \u00e31 , . . . , \u00e3 M are sampled from the multinomial P t (a). Because of the sampling, the gradient has to be computed by Monte Carlo gradient/REINFORCE (Williams, 1992) and is subject to high variance. Many tricks are required to train hard attention and there is little standardization across implementations. Xu et al (2015) use a combination of REINFORCE and soft attention. Zaremba et al(2015) uses curriculum learning that starts as soft-attention and gradually becomes discrete. Ling& Rush (2017) aggregates multiple samples during training, and a single sampled attention while testing. However, once trained well the sharp focus on memory provided by hard-attention has been found to yield superior performance (Xu et al., 2015; Shankar and Sarawagi, 2018) .",
                "cite_spans": [
                    {
                        "start": 156,
                        "end": 172,
                        "text": "(Williams, 1992)",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 315,
                        "end": 330,
                        "text": "Xu et al (2015)",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 382,
                        "end": 401,
                        "text": "Zaremba et al(2015)",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 723,
                        "end": 740,
                        "text": "(Xu et al., 2015;",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 741,
                        "end": 768,
                        "text": "Shankar and Sarawagi, 2018)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2.2"
            },
            {
                "text": "Sparse/Local Attention Many attempts have been made to bridge the gap between soft and hard attention. Luong et al (2015) proposes local attention that averages a window of input. This has been refined later to include syntax (Chen et al., 2017; Sennrich and Haddow, 2016; Chen et al., 2018) .",
                "cite_spans": [
                    {
                        "start": 103,
                        "end": 121,
                        "text": "Luong et al (2015)",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 226,
                        "end": 245,
                        "text": "(Chen et al., 2017;",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 246,
                        "end": 272,
                        "text": "Sennrich and Haddow, 2016;",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 273,
                        "end": 291,
                        "text": "Chen et al., 2018)",
                        "ref_id": "BIBREF5"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2.2"
            },
            {
                "text": "Another idea is to replace the softmax in soft attention with sparsity inducing operators (Martins and Astudillo, 2016; Niculae and Blondel, 2017). However, all sparse/local attention methods continue to compute P (y) from an attention weighted sum of inputs (Eq: 2) unlike hard attention.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2.2"
            },
            {
                "text": "We start from an explicit joint representation of the uncertainty of the attention and output variables.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Joint Attention-Output Models",
                "sec_num": "3"
            },
            {
                "text": "log P t (y t |x 1...m ) = log a P t (a)P t (y t |x a ) (4)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Joint Attention-Output Models",
                "sec_num": "3"
            },
            {
                "text": "The joint model directly couples individual input states to the output, and thus is a type of hard attention. Also, by taking an expectation, instead of a single hard attention, it enjoys differentiability as in soft-attention. We call this the full-joint method.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Joint Attention-Output Models",
                "sec_num": "3"
            },
            {
                "text": "Unfortunately, either when the vocabulary or the number of encoder states (m) is large, full-joint is not practical. Existing hard and soft attentions can be viewed as its approximations that either marginalize early or hard select attention. We show a surprisingly simple alternative approximation that provides hard attention without its training complexity. Our method called Beam-joint deterministically selects the top-k highest attention values and approximates the full joint log probability as log P t (y t |x 1...m ) \u2248 log a\u2208TopK(Pt(a)) P t (a)P t (y t |x a ) (5) Thus, in beam-joint, we first compute the multinomial attention distribution in O(m) time using Eq 1, select the Top-K input positions from the multinomial, next with hard attention on each position compute K output softmax, and finally compute the attention weighted output mixture distribution. The number of output softmax is K times in normal soft-attention but the actual running time overhead is only 20-30% for translation tasks. We used the default pass-through TopK operator (which is not differentiable) and optimize the beamapproximation directly. We also experimented with a version which smoothly shifts from soft-attention to beam-attention, but found that training the beamapproximation directly leads to best results.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Joint Attention-Output Models",
                "sec_num": "3"
            },
            {
                "text": "We show empirically that this very simple scheme is surprisingly effective compared to existing hard and soft attention over several translation tasks. Unlike sampling and variational methods that require careful tuning and exotic tricks during training, this simple scheme trains as easily as softattention, without significant increase in training time because even K = 6 works well enough.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Joint Attention-Output Models",
                "sec_num": "3"
            },
            {
                "text": "Another reason why our 'sum of probabilities' form performs better could be the softmax barrier effect highlighted in (Yang et al., 2018) . The authors argue that the richness of natural language cannot be captured in normal softmax due to the low rank constraint it imposes on input-to-output matrix. They improve performance using a Mixture of Softmax model. Our beam-joint also is a mixture of softmax and possibly achieves higher rank than a single softmax. However their mixture requires learning multiple softmax matrices, whereas ours are due to varying attention and we do not learn any extra parameters than soft attention.",
                "cite_spans": [
                    {
                        "start": 118,
                        "end": 137,
                        "text": "(Yang et al., 2018)",
                        "ref_id": "BIBREF18"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Joint Attention-Output Models",
                "sec_num": "3"
            },
            {
                "text": "We compare attention models on two NLP tasks: machine translation and morphological inflection.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "4"
            },
            {
                "text": "We experiment on five language pairs from three datasets: IWSLT15 English\u2194Vietnamese (Cettolo et al., 2015) which contains 133k train, 1.5k validation(tst2012) and 1.2k test(tst2013) sentence pairs respectively; IWSLT14 German\u2194English (Cettolo et al., 2014) which contains 160k train, 7.2k validation and 6.7k test sentence pairs respectively ; Workshop on Asian Translation 2017 Japanese\u2192English (Nakazawa et al., 2016) which contains 2M train, 1.8k validation and 1.8k test sentence pairs respectively. We use a 2 layer bi-directional encoder and a 2 layer unidirectional decoder with 512 hidden LSTM units and 0.2 dropout rate with vanilla SGD optimizer. We base our implementation2 on the NMT code3 in Tensorflow. We did no special hyper-parameter tuning and used standard-softmax tuned parameters on a batch size of 64.",
                "cite_spans": [
                    {
                        "start": 235,
                        "end": 257,
                        "text": "(Cettolo et al., 2014)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 397,
                        "end": 420,
                        "text": "(Nakazawa et al., 2016)",
                        "ref_id": "BIBREF12"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Machine translation",
                "sec_num": "4.1"
            },
            {
                "text": "Comparing attention models We compare beam-joint (default K = 6) with standard soft and hard attention. To further dissect the reasons behind beam-joint's gains, we compare beam-joint with a sampling based approximation of full-joint called Sample-Joint that replaces the TopK in Eq 5 with K attention weighted samples. We train samplejoint as well as hard-attention with REINFORCE with 6-samples. Also to ascertain that our gains are not explained by sparsity alone, we compare with Sparsemax (Martins and Astudillo, 2016) .",
                "cite_spans": [
                    {
                        "start": 484,
                        "end": 523,
                        "text": "Sparsemax (Martins and Astudillo, 2016)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Machine translation",
                "sec_num": "4.1"
            },
            {
                "text": "In Table 1 we show perplexity and BLEU with three beam sizes (B). Beam-joint significantly outperforms all other variants, including the standard soft attention by 0.8 to 1.7 BLEU points. The perplexity shows even a more impressive drop in all five datasets. Also we observe training times for beam-joint to be only 20-30% higher than softattention, establishing that beam-joint is both practical and more accurate.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 9,
                        "end": 10,
                        "text": "1",
                        "ref_id": "TABREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Machine translation",
                "sec_num": "4.1"
            },
            {
                "text": "Sample-joint is much worse than beam-joint. Apart from the problem of high variance of gradients in the reinforce step, another problem is that sampling repeats states whereas TopK in beamjoint gets distinct states. Hard attention too faces training issues and performs worse than soft attention, explaining why it is not commonly used in NMT. Sample-joint is better than Hard attention, further highlighting the merits of the joint distribution. Sparsemax is competitive but marginally worse than soft attention. This is concordant with the recent experiments of (Niculae and Blondel, 2017).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Machine translation",
                "sec_num": "4.1"
            },
            {
                "text": "Comparison with Full Joint Next we evaluate the impact of our beam-joint approximation against full-joint and soft attention. Full-joint cannot scale to large vocabularies, therefore we only compare on En-Vi with a batch size of 32. Figure 1a shows final BLEU of these methods as well as BLEU against increasing training steps. Beam-joint both converges faster and to a higher score than soft- This shows that an attention-beam of size 6 suffices to approximate full joint almost perfectly.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 240,
                        "end": 242,
                        "text": "1a",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Machine translation",
                "sec_num": "4.1"
            },
            {
                "text": "Next, in Figure 1b , we compare beam-joint (solid lines) and soft attention (dotted lines) for convergence rates on three other datasets. For each dataset beam-joint trains faster with a consistent improvement of more than 1 BLEU.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 16,
                        "end": 18,
                        "text": "1b",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Machine translation",
                "sec_num": "4.1"
            },
            {
                "text": "Effect of K in Beam-joint We show the effect of K used in TopK of beam-joint in Figure 2 on the En-Vi and De-En tasks. On En-Vi BLEU increases from 16.0 to 25.7 to 26.5 as K increases from 1 to 2 to 3; and then saturates quickly. Similar behavior is observed in the other dataset. This shows that small K values like 6 suffice for translation.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 87,
                        "end": 88,
                        "text": "2",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Machine translation",
                "sec_num": "4.1"
            },
            {
                "text": "We further evaluate whether the performance gain of beam-joint is due to the softmax barrier alone in Table 2 . We used our models trained with K=6, and deployed them for test-time greedy decoding with K set to 1. Since the output now has only a single softmax component, this model faces the same bottleneck as soft-attention. One can observe that as expected these results are worse than beam-joint with K=6, however they still exceed soft-attention by a significant margin, demonstrating that the performance gain is not solely due to the effect of ensembling or softmax-barrier.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 108,
                        "end": 109,
                        "text": "2",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Machine translation",
                "sec_num": "4.1"
            },
            {
                "text": "To demonstrate the use of this approach beyond translation, we next consider two morphological ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Morphological Inflection",
                "sec_num": "4.2"
            },
            {
                "text": "In this paper we showed a simple yet effective approximation of the joint attention-output distribution in sequence to sequence learning. Our joint model consistently provides higher accuracy without significant running time overheads in five translation and two morphological inflection tasks. An interesting direction for future work is to extend beam-joint to multi-head attention architectures as in (Vaswani et al., 2017; Xu Chen, 2018 ).",
                "cite_spans": [
                    {
                        "start": 404,
                        "end": 426,
                        "text": "(Vaswani et al., 2017;",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 427,
                        "end": 440,
                        "text": "Xu Chen, 2018",
                        "ref_id": "BIBREF17"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": null
            },
            {
                "text": "Note, attention on a single input encoder state does not imply attention on a single input token because RNNs or selfattention capture the context around the token.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://github.com/sid7954/beam-joint-attention",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://github.com/tensorflow/nmt",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://github.com/roeeaharoni/morphologicalreinflection",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "5 Our numbers are lower than earlier reported because ours use a single model whereas(Aharoni and Goldberg, 2017) and others report from an ensemble of five models.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "Acknowledgements We thank NVIDIA Corporation for supporting this research by the donation of Titan X GPU.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "acknowledgement",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Morphological inflection generation with hard monotonic attention",
                "authors": [
                    {
                        "first": "Roee",
                        "middle": [],
                        "last": "Aharoni",
                        "suffix": ""
                    },
                    {
                        "first": "Yoav",
                        "middle": [],
                        "last": "Goldberg",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
                "volume": "1",
                "issue": "",
                "pages": "2004--2015",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Roee Aharoni and Yoav Goldberg. 2017. Morphologi- cal inflection generation with hard monotonic atten- tion. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 -August 4, Vol- ume 1: Long Papers, pages 2004-2015.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Neural machine translation by jointly learning to align and translate",
                "authors": [
                    {
                        "first": "Dzmitry",
                        "middle": [],
                        "last": "Bahdanau",
                        "suffix": ""
                    },
                    {
                        "first": "Kyunghyun",
                        "middle": [],
                        "last": "Cho",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "The iwslt 2015 evaluation campaign",
                "authors": [
                    {
                        "first": "Mauro",
                        "middle": [],
                        "last": "Cettolo",
                        "suffix": ""
                    },
                    {
                        "first": "Jan",
                        "middle": [],
                        "last": "Niehues",
                        "suffix": ""
                    },
                    {
                        "first": "Sebastian",
                        "middle": [],
                        "last": "St\u00fcker",
                        "suffix": ""
                    },
                    {
                        "first": "Luisa",
                        "middle": [],
                        "last": "Bentivogli",
                        "suffix": ""
                    },
                    {
                        "first": "Roldano",
                        "middle": [],
                        "last": "Cattoni",
                        "suffix": ""
                    },
                    {
                        "first": "Marcello",
                        "middle": [],
                        "last": "Federico",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "IWSLT 2015, International Workshop on Spoken Language Translation",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mauro Cettolo, Jan Niehues, Sebastian St\u00fcker, Luisa Bentivogli, Roldano Cattoni, and Marcello Federico. 2015. The iwslt 2015 evaluation campaign. In IWSLT 2015, International Workshop on Spoken Language Translation.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Report on the 11th iwslt evaluation campaign",
                "authors": [
                    {
                        "first": "Mauro",
                        "middle": [],
                        "last": "Cettolo",
                        "suffix": ""
                    },
                    {
                        "first": "Jan",
                        "middle": [],
                        "last": "Niehues",
                        "suffix": ""
                    },
                    {
                        "first": "Sebastian",
                        "middle": [],
                        "last": "St\u00fcker",
                        "suffix": ""
                    },
                    {
                        "first": "Luisa",
                        "middle": [],
                        "last": "Bentivogli",
                        "suffix": ""
                    },
                    {
                        "first": "Marcello",
                        "middle": [],
                        "last": "Federico",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mauro Cettolo, Jan Niehues, Sebastian St\u00fcker, Luisa Bentivogli, and Marcello Federico. 2014. Report on the 11th iwslt evaluation campaign, iwslt 2014.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Improved neural machine translation with a syntax-aware encoder and decoder",
                "authors": [
                    {
                        "first": "Huadong",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Shujian",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Chiang",
                        "suffix": ""
                    },
                    {
                        "first": "Jiajun",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "ACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Huadong Chen, Shujian Huang, David Chiang, and Ji- ajun Chen. 2017. Improved neural machine transla- tion with a syntax-aware encoder and decoder. In ACL.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Syntax-directed attention for neural machine translation",
                "authors": [
                    {
                        "first": "Kehai",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Rui",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Masao",
                        "middle": [],
                        "last": "Utiyama",
                        "suffix": ""
                    },
                    {
                        "first": "Eiichiro",
                        "middle": [],
                        "last": "Sumita",
                        "suffix": ""
                    },
                    {
                        "first": "Tiejun",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kehai Chen, Rui Wang, Masao Utiyama, Eiichiro Sumita, and Tiejun Zhao. 2018. Syntax-directed attention for neural machine translation. CoRR, abs/1711.04231.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Supervised learning of complete morphological paradigms",
                "authors": [
                    {
                        "first": "Greg",
                        "middle": [],
                        "last": "Durrett",
                        "suffix": ""
                    },
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Denero",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of the North American Chapter",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Greg Durrett and John DeNero. 2013. Supervised learning of complete morphological paradigms. In Proceedings of the North American Chapter of the Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Adam: A method for stochastic optimization",
                "authors": [
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Diederik",
                        "suffix": ""
                    },
                    {
                        "first": "Jimmy",
                        "middle": [],
                        "last": "Kingma",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Ba",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Diederik P. Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. CoRR, abs/1412.6980.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Neural machine translation",
                "authors": [
                    {
                        "first": "Philipp",
                        "middle": [],
                        "last": "Koehn",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Philipp Koehn. 2017. Neural machine translation. CoRR, abs/1709.07809.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Coarse-tofine attention models for document summarization",
                "authors": [
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Ling",
                        "suffix": ""
                    },
                    {
                        "first": "Alexander",
                        "middle": [
                            "M"
                        ],
                        "last": "Rush",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the Workshop on New Frontiers in Summarization",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jeffrey Ling and Alexander M. Rush. 2017. Coarse-to- fine attention models for document summarization. In Proceedings of the Workshop on New Frontiers in Summarization, NFiS at EMNLP 2017, Copenhagen, Denmark.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Effective approaches to attentionbased neural machine translation",
                "authors": [
                    {
                        "first": "Minh-Thang",
                        "middle": [],
                        "last": "Luong",
                        "suffix": ""
                    },
                    {
                        "first": "Hieu",
                        "middle": [],
                        "last": "Pham",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Minh-Thang Luong, Hieu Pham, and Christopher D. Manning. 2015. Effective approaches to attention- based neural machine translation. EMNLP.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "From softmax to sparsemax: A sparse model of attention and multi-label classification",
                "authors": [
                    {
                        "first": "F",
                        "middle": [
                            "T"
                        ],
                        "last": "Andr\u00e9",
                        "suffix": ""
                    },
                    {
                        "first": "Ram\u00f3n",
                        "middle": [],
                        "last": "Martins",
                        "suffix": ""
                    },
                    {
                        "first": "Astudillo",
                        "middle": [],
                        "last": "Fern\u00e1ndez",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "ICML",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Andr\u00e9 F. T. Martins and Ram\u00f3n Fern\u00e1ndez Astudillo. 2016. From softmax to sparsemax: A sparse model of attention and multi-label classification. In ICML.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "A regularized framework for sparse and structured neural attention",
                "authors": [
                    {
                        "first": "Toshiaki",
                        "middle": [],
                        "last": "Nakazawa",
                        "suffix": ""
                    },
                    {
                        "first": "Manabu",
                        "middle": [],
                        "last": "Yaguchi",
                        "suffix": ""
                    },
                    {
                        "first": "Kiyotaka",
                        "middle": [],
                        "last": "Uchimoto",
                        "suffix": ""
                    },
                    {
                        "first": "Masao",
                        "middle": [],
                        "last": "Utiyama",
                        "suffix": ""
                    },
                    {
                        "first": "Eiichiro",
                        "middle": [],
                        "last": "Sumita",
                        "suffix": ""
                    },
                    {
                        "first": "Sadao",
                        "middle": [],
                        "last": "Kurohashi",
                        "suffix": ""
                    },
                    {
                        "first": "Hitoshi",
                        "middle": [],
                        "last": "Isahara",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "LREC. Vlad Niculae and Mathieu Blondel",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Toshiaki Nakazawa, Manabu Yaguchi, Kiyotaka Uchi- moto, Masao Utiyama, Eiichiro Sumita, Sadao Kuro- hashi, and Hitoshi Isahara. 2016. Aspec: Asian sci- entific paper excerpt corpus. In LREC. Vlad Niculae and Mathieu Blondel. 2017. A regular- ized framework for sparse and structured neural at- tention. In NIPS.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Linguistic input features improve neural machine translation",
                "authors": [
                    {
                        "first": "Rico",
                        "middle": [],
                        "last": "Sennrich",
                        "suffix": ""
                    },
                    {
                        "first": "Barry",
                        "middle": [],
                        "last": "Haddow",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "AAAI",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Rico Sennrich and Barry Haddow. 2016. Linguistic in- put features improve neural machine translation. In WMT. Shiv Shankar and Sunita Sarawagi. 2018. Labeled memory networks for online model adaptation. In AAAI.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Attention is all you need",
                "authors": [
                    {
                        "first": "Ashish",
                        "middle": [],
                        "last": "Vaswani",
                        "suffix": ""
                    },
                    {
                        "first": "Noam",
                        "middle": [],
                        "last": "Shazeer",
                        "suffix": ""
                    },
                    {
                        "first": "Niki",
                        "middle": [],
                        "last": "Parmar",
                        "suffix": ""
                    },
                    {
                        "first": "Jakob",
                        "middle": [],
                        "last": "Uszkoreit",
                        "suffix": ""
                    },
                    {
                        "first": "Llion",
                        "middle": [],
                        "last": "Jones",
                        "suffix": ""
                    },
                    {
                        "first": "Aidan",
                        "middle": [
                            "N"
                        ],
                        "last": "Gomez",
                        "suffix": ""
                    },
                    {
                        "first": "\u0141 Ukasz",
                        "middle": [],
                        "last": "Kaiser",
                        "suffix": ""
                    },
                    {
                        "first": "Illia",
                        "middle": [],
                        "last": "Polosukhin",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "NIPS",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In NIPS.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning",
                "authors": [
                    {
                        "first": "Ronald",
                        "middle": [
                            "J"
                        ],
                        "last": "Williams",
                        "suffix": ""
                    }
                ],
                "year": 1992,
                "venue": "Mach. Learn",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ronald J. Williams. 1992. Simple statistical gradient- following algorithms for connectionist reinforce- ment learning. Mach. Learn.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Show, attend and tell: Neural image caption generation with visual attention",
                "authors": [
                    {
                        "first": "Kelvin",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Jimmy",
                        "middle": [],
                        "last": "Ba",
                        "suffix": ""
                    },
                    {
                        "first": "Ryan",
                        "middle": [],
                        "last": "Kiros",
                        "suffix": ""
                    },
                    {
                        "first": "Kyunghyun",
                        "middle": [],
                        "last": "Cho",
                        "suffix": ""
                    },
                    {
                        "first": "Aaron",
                        "middle": [],
                        "last": "Courville",
                        "suffix": ""
                    },
                    {
                        "first": "Ruslan",
                        "middle": [],
                        "last": "Salakhudinov",
                        "suffix": ""
                    },
                    {
                        "first": "Rich",
                        "middle": [],
                        "last": "Zemel",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the 32nd International Conference on Machine Learning",
                "volume": "37",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. 2015. Show, attend and tell: Neural image caption generation with visual atten- tion. In Proceedings of the 32nd International Con- ference on Machine Learning, volume 37 of Pro- ceedings of Machine Learning Research.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "The best of both worlds: Combining recent advances in neural machine translation",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Mia",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "ACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mia et al Xu Chen. 2018. The best of both worlds: Combining recent advances in neural machine trans- lation. In ACL.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Breaking the softmax bottleneck: A high-rank RNN language model",
                "authors": [
                    {
                        "first": "Zhilin",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Zihang",
                        "middle": [],
                        "last": "Dai",
                        "suffix": ""
                    },
                    {
                        "first": "Ruslan",
                        "middle": [],
                        "last": "Salakhutdinov",
                        "suffix": ""
                    },
                    {
                        "first": "William",
                        "middle": [
                            "W"
                        ],
                        "last": "Cohen",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "International Conference on Learning Representations",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W. Cohen. 2018. Breaking the softmax bot- tleneck: A high-rank RNN language model. In Inter- national Conference on Learning Representations.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Online segment to segment neural transduction",
                "authors": [
                    {
                        "first": "Lei",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Jan",
                        "middle": [],
                        "last": "Buys",
                        "suffix": ""
                    },
                    {
                        "first": "Phil",
                        "middle": [],
                        "last": "Blunsom",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "EMNLP",
                "volume": "",
                "issue": "",
                "pages": "1307--1316",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Lei Yu, Jan Buys, and Phil Blunsom. 2016. Online segment to segment neural transduction. In EMNLP, pages 1307-1316.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Reinforcement learning neural turing machines",
                "authors": [
                    {
                        "first": "Wojciech",
                        "middle": [],
                        "last": "Zaremba",
                        "suffix": ""
                    },
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Wojciech Zaremba and Ilya Sutskever. 2015. Rein- forcement learning neural turing machines. CoRR, abs/1505.00521.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 1: Test BLEU in various settings (Beam=1). Best viewed in color",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table><tr><td>Dataset</td><td>Attention</td><td>PPL</td><td colspan=\"2\">BLEU B=1 B=4 B=10</td></tr><tr><td/><td>Soft</td><td colspan=\"2\">9.61 27.7 28.6</td><td>28.5</td></tr><tr><td/><td>Hard</td><td colspan=\"2\">9.50 25.3 25.6</td><td>25.5</td></tr><tr><td>IWSLT14 DE-EN</td><td>Sparse</td><td colspan=\"2\">9.85 27.2 28.4</td><td>28.0</td></tr><tr><td/><td>Sample-Joint</td><td colspan=\"2\">9.96 26.3 27.8</td><td>27.8</td></tr><tr><td/><td>Beam-Joint</td><td colspan=\"2\">8.47 29.0 29.7</td><td>29.6</td></tr><tr><td/><td>Soft</td><td colspan=\"2\">10.68 23.1 24.2</td><td>24.2</td></tr><tr><td/><td>Hard</td><td colspan=\"2\">10.15 21.4 21.8</td><td>21.7</td></tr><tr><td>IWSLT14 EN-DE</td><td>Sparse</td><td colspan=\"2\">10.89 22.5 23.4</td><td>23.3</td></tr><tr><td/><td colspan=\"3\">Sample-Joint 10.05 22.8 23.8</td><td>23.6</td></tr><tr><td/><td>Beam-Joint</td><td colspan=\"2\">8.72 24.7 25.4</td><td>25.3</td></tr><tr><td/><td>Soft</td><td colspan=\"2\">10.27 26.0 26.6</td><td>26.4</td></tr><tr><td/><td>Hard</td><td colspan=\"2\">10.53 24.1 24.3</td><td>24.0</td></tr><tr><td>IWSLT15 EN-VI</td><td>Sparse</td><td colspan=\"2\">10.13 25.9 26.6</td><td>26.1</td></tr><tr><td/><td colspan=\"3\">Sample-Joint 11.00 25.8 26.3</td><td>25.9</td></tr><tr><td/><td>Beam-Joint</td><td colspan=\"2\">9.67 27.0 27.4</td><td>27.3</td></tr><tr><td/><td>Soft</td><td colspan=\"2\">8.30 23.6 24.7</td><td>24.6</td></tr><tr><td/><td>Hard</td><td colspan=\"2\">8.28 21.1 21.9</td><td>21.5</td></tr><tr><td>IWSLT14 VI-EN</td><td>Sparse</td><td colspan=\"2\">8.48 22.8 24.2</td><td>23.9</td></tr><tr><td/><td>Sample-Joint</td><td colspan=\"2\">8.28 22.7 24.0</td><td>23.9</td></tr><tr><td/><td>Beam-Joint</td><td colspan=\"2\">7.57 24.5 25.8</td><td>25.7</td></tr><tr><td/><td>Soft</td><td colspan=\"2\">12.46 17.6 18.9</td><td>18.5</td></tr><tr><td/><td>Hard</td><td colspan=\"2\">12.78 13.2 13.1</td><td>12.7</td></tr><tr><td>WAT17 JA-EN</td><td>Sparse</td><td colspan=\"2\">14.18 16.7 17.5</td><td>16.8</td></tr><tr><td/><td colspan=\"3\">Sample-Joint 13.21 16.2 18.1</td><td>17.9</td></tr><tr><td/><td>Beam-Joint</td><td colspan=\"2\">10.00 19.6 20.6</td><td>20.2</td></tr><tr><td colspan=\"2\">B=1 B=4 B=10 26.0 26.7 26.6 Beam-Joint 27.0 27.1 26.9 Soft Full-Joint 26.8 27.1 26.9</td><td/><td/></tr></table>",
                "type_str": "table",
                "text": "Perplexity and test BLEU with three inference beam widths (B) on five translation tasks. Beam-joint consistently and substantially outperforms soft-attention.",
                "html": null,
                "num": null
            },
            "TABREF1": {
                "content": "<table><tr><td colspan=\"2\">Dataset Soft</td><td>Beam-Joint (K=1)</td><td>Beam-Joint (K=6)</td></tr><tr><td>En-De</td><td>23.1</td><td>24.5</td><td>24.7</td></tr><tr><td>De-En</td><td>27.7</td><td>28.4</td><td>29.0</td></tr><tr><td>En-Vi</td><td>26.0</td><td>26.5</td><td>27.0</td></tr></table>",
                "type_str": "table",
                "text": "Comparing soft attention with Beam-Joint using different values of K during inference. During training K = 6 for both Beam-Joint models. inflection tasks. We use(Durrett and DeNero, 2013)'s dataset containing 8 inflection forms for German Nouns (de-N) and 27 forms for German Verbs (de-V). The number of training words is 2364 and 1627 respectively while the validation and test words are 200 each. We train a one layer encoder and decoder with 128 hidden LSTM units each with a dropout rate of 0.2 using Adam(Kingma and Ba, 2014) and measure 0/1 accuracy for soft, hard and full-joint attention models. Due to limited input length and vocabulary, we were able to run directly the full-joint model. We also ran the 100 units wide two layer LSTM with hard-monotonic attention provided by(Aharoni and Goldberg, 2017) labeled Hard-Mono 4 . The table below shows that even for this task full-joint scores over existing attention models 5 . The generic full-joint attention provides slight gains even over the task specific hard-monotonic attention.",
                "html": null,
                "num": null
            }
        }
    }
}