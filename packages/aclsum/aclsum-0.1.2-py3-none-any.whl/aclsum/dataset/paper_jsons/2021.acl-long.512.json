{
    "paper_id": "2021",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:20:36.804063Z"
    },
    "title": "Determinantal Beam Search",
    "authors": [
        {
            "first": "Clara",
            "middle": [],
            "last": "Meister",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "ETH Z\u00fcrich University of Cambridge",
                "location": {}
            },
            "email": "meistecl@inf.ethz.ch"
        },
        {
            "first": "Martina",
            "middle": [],
            "last": "Forster",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "ETH Z\u00fcrich University of Cambridge",
                "location": {}
            },
            "email": ""
        },
        {
            "first": "Ryan",
            "middle": [],
            "last": "Cotterell",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "ETH Z\u00fcrich University of Cambridge",
                "location": {}
            },
            "email": "ryan.cotterell@inf.ethz.ch"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Beam search is a go-to strategy for decoding neural sequence models. The algorithm can naturally be viewed as a subset optimization problem, albeit one where the corresponding set function does not reflect interactions between candidates. Empirically, this leads to sets often exhibiting high overlap, e.g., strings may differ by only a single word. Yet in use-cases that call for multiple solutions, a diverse or representative set is often desired. To address this issue, we propose a reformulation of beam search, which we call determinantal beam search. Determinantal beam search has a natural relationship to determinantal point processes (DPPs), models over sets that inherently encode intra-set interactions. By posing iterations in beam search as a series of subdeterminant maximization problems, we can turn the algorithm into a diverse subset selection process. In a case study, we use the string subsequence kernel to explicitly encourage n-gram coverage in text generated from a sequence model. We observe that our algorithm offers competitive performance against other diverse set generation strategies in the context of language generation, while providing a more general approach to optimizing for diversity.",
    "pdf_parse": {
        "paper_id": "2021",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Beam search is a go-to strategy for decoding neural sequence models. The algorithm can naturally be viewed as a subset optimization problem, albeit one where the corresponding set function does not reflect interactions between candidates. Empirically, this leads to sets often exhibiting high overlap, e.g., strings may differ by only a single word. Yet in use-cases that call for multiple solutions, a diverse or representative set is often desired. To address this issue, we propose a reformulation of beam search, which we call determinantal beam search. Determinantal beam search has a natural relationship to determinantal point processes (DPPs), models over sets that inherently encode intra-set interactions. By posing iterations in beam search as a series of subdeterminant maximization problems, we can turn the algorithm into a diverse subset selection process. In a case study, we use the string subsequence kernel to explicitly encourage n-gram coverage in text generated from a sequence model. We observe that our algorithm offers competitive performance against other diverse set generation strategies in the context of language generation, while providing a more general approach to optimizing for diversity.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "The decoding of neural sequence models is a fundamental component of many tasks in NLP. Yet, many proposed decoding methods aim to produce only a single solution; further, decoding strategies that provide a set, such as beam search, admit high overlap between solutions. Such approaches fail to reflect that for many NLP tasks,1 there can be multiple correct solutions-or that we may desire a diverse set of solutions. As it stands, standard beam search chooses items based purely on individual scores, with no means for encoding interaction between candidates; this is the limitation which we attempt to address in this work.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "We derive determinantal beam search, a novel generalization of beam search that casts subset selection as the subdeterminant optimization problem. Specifically, we formulate each iteration of beam search as a subdeterminant maximization problem parameterized by a positive semi-definite matrix that encodes interactions between the possible candidates; standard beam search is recovered by a specific diagonal matrix. This framing creates a natural paradigm for taking the relationships between candidates during the decoding process, and can thus assign higher scores to diversified sets; we show how this approach relates to k-determinantal point processes (DPPs). Given the wealth of research on efficient kernel computation (Rousu and Shawe-Taylor, 2005; Farhan et al., 2017) and DPP inference strategies (Li et al., 2016; Han et al., 2017; Chen et al., 2018) , we find the impact on runtime to be quite reasonable in comparison to standard decoding techniques.",
                "cite_spans": [
                    {
                        "start": 728,
                        "end": 758,
                        "text": "(Rousu and Shawe-Taylor, 2005;",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 759,
                        "end": 779,
                        "text": "Farhan et al., 2017)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 809,
                        "end": 826,
                        "text": "(Li et al., 2016;",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 827,
                        "end": 844,
                        "text": "Han et al., 2017;",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 845,
                        "end": 863,
                        "text": "Chen et al., 2018)",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In a case study on neural machine translation (NMT), we demonstrate how to make use of the string subsequence kernel (Lodhi et al., 2002) to encode the notion of n-gram diversity in the language generation process, allowing us to derive an elegant diverse beam search. Under this scheme, we observe that determinantal beam search generates more diverse sets than standard beam search with minimal trade-off in terms of BLEU. We see improved performance over stochastic beam search (SBS; Kool et al., 2019) , which is reported to encourage diversity, and a slight improvement over Vijayakumar et al. (2018) 's diverse beam search (DBS) while providing a more general approach to optimizing for intra-set diversity.",
                "cite_spans": [
                    {
                        "start": 117,
                        "end": 137,
                        "text": "(Lodhi et al., 2002)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 487,
                        "end": 505,
                        "text": "Kool et al., 2019)",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 580,
                        "end": 605,
                        "text": "Vijayakumar et al. (2018)",
                        "ref_id": "BIBREF32"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Neural sequence models are probability distributions p(y | x) over sequences y in an output space Y conditioned on an input x. 2 Here we define Y as the set of all valid sequences derived from a vocabulary V that are bookended by distinguished BOS and EOS tokens, indicating the beginning and end of the sequence, respectively. Typically, the sequence length is upper-bounded by some value n max \u2208 Z + , which may depend on x. In this work, we consider locally normalized models, i.e. where p is a probability distribution over V def = V \u222a {EOS} conditioned on previously generated tokens y <t . The probability of the full sequence y = y 1 , y 2 , . . . is then calculated via the chain rule of probability:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Neural Sequence Models",
                "sec_num": "2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "p(y | x) = |y| t=1 p(y t | y <t , x)",
                        "eq_num": "(1)"
                    }
                ],
                "section": "Neural Sequence Models",
                "sec_num": "2"
            },
            {
                "text": "where y <1 = y 0 def = BOS. Our model p is typically parameterized by a neural network with weights \u03b8. As we do not focus on the underlying model itself in this work, we omit the dependence of p on the parameters \u03b8.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Neural Sequence Models",
                "sec_num": "2"
            },
            {
                "text": "We define the decoding problem as the search for the highest-scoring y among all sequences in Y according to the model p(y | x), which is also called maximum-a-posteriori (MAP) inference:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Neural Sequence Models",
                "sec_num": "2"
            },
            {
                "text": "y = argmax y\u2208Y log p(y | x) (2)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Neural Sequence Models",
                "sec_num": "2"
            },
            {
                "text": "where the log transform of p is used by convention. We further define the set decoding problem as the search for a set Y of a specified cardinality k among all valid subsets {Y \u2286 Y | |Y | = k} that has the highest score where, by overloading, we define",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Neural Sequence Models",
                "sec_num": "2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "p(Y | x) def = y\u2208Y p(y | x)",
                        "eq_num": "(3)"
                    }
                ],
                "section": "Neural Sequence Models",
                "sec_num": "2"
            },
            {
                "text": "Similarly to Eq. ( 2), the set-decoding problem is then defined as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Neural Sequence Models",
                "sec_num": "2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "Y = argmax Y \u2286Y, |Y |=k log p(Y | x)",
                        "eq_num": "(4)"
                    }
                ],
                "section": "Neural Sequence Models",
                "sec_num": "2"
            },
            {
                "text": "However, as has been noted in the literature, there are a number of issues with both Eq. ( 2) and (4). First, as Y may be an exponentially large (in V) space and p is typically non-Markovian, we cannot efficiently search over Y, much less over Y k . Second, specifically for language generation tasks, these might not be useful objectives.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Neural Sequence Models",
                "sec_num": "2"
            },
            {
                "text": "2 x may be, e.g., a source sentence or an image.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Neural Sequence Models",
                "sec_num": "2"
            },
            {
                "text": "Degenerate Objective. It is important to note that the highest-probability solutions under neural sequence models are not always high-quality; specifically for tasks involving language generation, e.g., machine translation, prior work has shown the tendency for MAP decoding to lead to generic or degenerate solutions (Stahlberg and Byrne, 2019; Meister et al., 2020; Eikema and Aziz, 2020) while superior solutions assigned only slightly lower probability are often overlooked (Holtzman et al., 2020) . Consequently, heuristic search methods or alternative objectives are frequently employed for decoding language generators.",
                "cite_spans": [
                    {
                        "start": 318,
                        "end": 345,
                        "text": "(Stahlberg and Byrne, 2019;",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 346,
                        "end": 367,
                        "text": "Meister et al., 2020;",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 368,
                        "end": 390,
                        "text": "Eikema and Aziz, 2020)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 478,
                        "end": 501,
                        "text": "(Holtzman et al., 2020)",
                        "ref_id": "BIBREF12"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Neural Sequence Models",
                "sec_num": "2"
            },
            {
                "text": "A common heuristic to approximate the decoding problem in Eq. ( 2) is to sequentially choose the token y t at each time step t that maximizes p(y t | y <t , x) until the EOS token is generated or the maximum sequence length n max is reached. This procedure is known as greedy search. Beam search is an oft-employed generalization of greedy search that returns k candidates and explores more of the search space. 3 In this work, we focus on a framing of beam search as iterative subset selection, which allows for a remarkably concise formulation of the algorithm. Given an initial set Y 0 containing only the BOS token, we choose subsequent Y t for t \u2208 {1, . . . , n max } according to the following recursion:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Beam Search",
                "sec_num": "2.1"
            },
            {
                "text": "Standard Beam Search Y 0 \u2190 {BOS} (5) Y t \u2190 argmax Y t \u2286Bt, |Y t |=k log p(Y t | Y t-1 , x)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Beam Search",
                "sec_num": "2.1"
            },
            {
                "text": "where we are constrained to only extending candidates present in the beam set, which we define as ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Beam Search",
                "sec_num": "2.1"
            },
            {
                "text": "B t def = {y <t \u2022 y | y <t \u2208 Y t-",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Beam Search",
                "sec_num": "2.1"
            },
            {
                "text": "We now introduce an alternative, equivalent notation for Eq. ( 5) using matrices and determinants that will shed light on the straightforward generalization of beam search that we present as the primary contribution of this paper. We define a timestep-dependent4 diagonal matrix D \u2208 R |Bt|\u00d7|Bt| where we take the diagonal entry",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Determinantal Reformulation",
                "sec_num": "2.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "D ii def = p(y (i) \u2264t | x)",
                        "eq_num": "(7)"
                    }
                ],
                "section": "A Determinantal Reformulation",
                "sec_num": "2.2"
            },
            {
                "text": "Here y",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Determinantal Reformulation",
                "sec_num": "2.2"
            },
            {
                "text": "(i)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Determinantal Reformulation",
                "sec_num": "2.2"
            },
            {
                "text": "\u2264t is the i th candidate in B t according to a unique mapping of every element y \u2264t \u2208 B t to an integer between 1 and |B t |. Furthermore, we use the notation D Yt where Y t \u2286 B t , to indicate the submatrix that only contains those rows and columns corresponding to the elements of Y t . We may now rewrite Eq. ( 5) as",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Determinantal Reformulation",
                "sec_num": "2.2"
            },
            {
                "text": "Determinantal Standard Beam Search Y 0 \u2190 {BOS} (8) Y t \u2190 argmax Y t \u2286Bt, |Y t |=k log det(D Y t )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Determinantal Reformulation",
                "sec_num": "2.2"
            },
            {
                "text": "where equivalence follows from the definition of the determinant for diagonal matrices. Formally, Eq. ( 8) is known as the subdeterminant maximization problem5 (Klee et al., 1995; Ebrahimi et al., 2017) , which-as the name suggestsrefers to the problem of finding the determinant maximizing subset of a matrix. While the notation introduced in Eq. ( 8) may seem contrived, it allows us to perform the subsequent generalization.",
                "cite_spans": [
                    {
                        "start": 160,
                        "end": 179,
                        "text": "(Klee et al., 1995;",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 180,
                        "end": 202,
                        "text": "Ebrahimi et al., 2017)",
                        "ref_id": "BIBREF5"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Determinantal Reformulation",
                "sec_num": "2.2"
            },
            {
                "text": "We are now in a position to ask the fundamental question of this work: What happens if we replace the diagonal matrix D with a non-diagonal matrix? This substitution allows us to account for interactions between the elements in the beam. Formally, we consider a timestep-dependent positive semi-definite (PSD) matrix D + w \u2022 K where the off-diagonal matrix K indicates the strength of the interactions between candidates. The nonnegative weight w \u2265 0 controls the importance of these interactions during the decoding process. In this case, the beam search recursion becomes:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Determinantal Beam Search",
                "sec_num": "3"
            },
            {
                "text": "Full Determinantal Beam Search Y 0 \u2190 {BOS} (9) Y t \u2190 argmax Y t \u2286Bt, |Y t |=k log det(D Y t + w \u2022 K Y t )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Determinantal Beam Search",
                "sec_num": "3"
            },
            {
                "text": "Clearly, we recover beam search when w = 0; however, we can now select subsets based additionally on candidate interactions. That is, Eq. ( 9) now has an interpretation as a diversity objective function (Indyk et al., 2014) when K is chosen wisely. Due to the presence of the log, Eq. ( 9) is only well defined when the matrix",
                "cite_spans": [
                    {
                        "start": 203,
                        "end": 223,
                        "text": "(Indyk et al., 2014)",
                        "ref_id": "BIBREF13"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Determinantal Beam Search",
                "sec_num": "3"
            },
            {
                "text": "D Y + w \u2022 K Y is PSD. 6 3.1 Constructing K",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Determinantal Beam Search",
                "sec_num": "3"
            },
            {
                "text": "One simple way to construct K is as a Gram matrix, where each i, j element of K is computed via a kernel function K : S \u00d7 S \u2192 R that maps two items in a space S to a real number. Specifically, we define K ij = K(s i , s j ) where s i , s j \u2208 S are the i th and j th elements of S, respectively. In slight abuse of notation, we overload the kernel function K to take a set S such that K = K(S) is the kernel matrix resulting from pairwise computation over elements of S.7 Following from Mercer's theorem, the matrix K = K(S) is necessarily PSD and, thus the matrix",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Determinantal Beam Search",
                "sec_num": "3"
            },
            {
                "text": "D Y + w \u2022 K Y is PSD for any Y \u2286 S. 8",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Determinantal Beam Search",
                "sec_num": "3"
            },
            {
                "text": "The efficient computation of kernel functions is a well-studied problem-largely due to the prevalence of kernels in various machine learning techniques. For example, dynamic programming techniques are often employed in computation of K(S) (Rousu and Shawe-Taylor, 2005) or approximate low-rank kernel matrices can be used in place of K(S) (Si et al., 2017) .",
                "cite_spans": [
                    {
                        "start": 239,
                        "end": 269,
                        "text": "(Rousu and Shawe-Taylor, 2005)",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 339,
                        "end": 356,
                        "text": "(Si et al., 2017)",
                        "ref_id": "BIBREF29"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Determinantal Beam Search",
                "sec_num": "3"
            },
            {
                "text": "One interpretation of Eq. ( 9) is as a determinantal point process (DPP). Specifically, it is a k-DPP (Kulesza and Taskar, 2011) in the L-ensemble parameterization where we have L = D+w\u2022K. This interpretation as a k-DPP gives us a very clear understanding of why Eq. ( 8) yields a diverse beam search. The diagonal entries encode quality, which tells how \"good\" each candidate on the beam is, while the off-diagonal entries encode how similar two elements are and, thus, how much they should be repulsed. For an overview of DPPs we refer the reader to Kulesza and Taskar (2012) .",
                "cite_spans": [
                    {
                        "start": 102,
                        "end": 128,
                        "text": "(Kulesza and Taskar, 2011)",
                        "ref_id": null
                    },
                    {
                        "start": 552,
                        "end": 577,
                        "text": "Kulesza and Taskar (2012)",
                        "ref_id": "BIBREF18"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Relation to a DPPs",
                "sec_num": "3.2"
            },
            {
                "text": "Unfortunately, computing the argmax9 in Eq. ( 9) is an NP-hard problem (Ko et al., 1995) . However, as the subdeterminant maximization problem has many applications, there has been much research on efficient algorithms for approximating logdeterminants in the context of, e.g., determinantal point processes (Gillenwater et al., 2012; Han et al., 2017) . 10 One such algorithm uses a first-order approximation of the log-determinant function (Han et al., 2017) . The work of Chen et al. ( 2018) uses a greedy, iterative approach; by updating the Cholesky factorization of the matrix kernel incrementally, the algorithm reduces inference time to O(k 2 |S|) to return k candidates from set S. Pseudocode for the latter approach can be found in Chen et al. (2018) ; pseudocode for the algorithm in log-space-since probabilistic models are often worked with in log-space for numerical stability-can be found in App. A.",
                "cite_spans": [
                    {
                        "start": 71,
                        "end": 88,
                        "text": "(Ko et al., 1995)",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 308,
                        "end": 334,
                        "text": "(Gillenwater et al., 2012;",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 335,
                        "end": 352,
                        "text": "Han et al., 2017)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 442,
                        "end": 460,
                        "text": "(Han et al., 2017)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 742,
                        "end": 760,
                        "text": "Chen et al. (2018)",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Computing Log-Determinants",
                "sec_num": "3.3"
            },
            {
                "text": "We consider the runtime of selecting k candidates at any given time step in the recursion of Eq. ( 9). At each time step, we must first construct the matrix K. This computation is highly dependent on the set interactions being modeled; as such, let O(c(k)) be a runtime bound for K's computation when our search uses a beam size of k. Once we have constructed our matrix D + w \u2022 K, we must next select k items. The set of hypotheses at any time step is at most k| V|. While as discussed in \u00a73.3, finding the size-k subset that exactly maximizes Eq. ( 9) has exponential runtime, we assume approximate methods are employed. Using the method given by Chen et al. ( 2018), approximate MAP inference takes k 3 | V| time to return k items from a set of size k| V|. Thus, the runtime at each iteration of determinantal beam search under these conditions would be O(c(k) + k 3 | V|). Note that standard beam search runs in O(k| V| log(k| V|)) time at each iteration. As k is generally small (\u2264 20) and the impact of c(k) can be made reasonable ( \u00a73.1), the practical increase in runtime is typically only moderate.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Runtime Analysis",
                "sec_num": "3.4"
            },
            {
                "text": "We now consider the task of language generation, where our vocabulary V is a set of words and Y is the set of all valid strings derived from V. When the space of our kernel function S = B t , one simple way of modeling interactions is through a string subsequence kernel (Lodhi et al., 2002) .",
                "cite_spans": [
                    {
                        "start": 271,
                        "end": 291,
                        "text": "(Lodhi et al., 2002)",
                        "ref_id": "BIBREF21"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Case Study: Diverse Beam Search",
                "sec_num": "4"
            },
            {
                "text": "The string subsequence kernel, proposed by Lodhi et al. (2002) , is a function over two strings s and t computed as:",
                "cite_spans": [
                    {
                        "start": 43,
                        "end": 62,
                        "text": "Lodhi et al. (2002)",
                        "ref_id": "BIBREF21"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Computing the String Kernel",
                "sec_num": "4.1"
            },
            {
                "text": "K(s, t) = u\u2208V n i:u=s[i] \u03bb l(i) j:u=t[j]",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Computing the String Kernel",
                "sec_num": "4.1"
            },
            {
                "text": "\u03bb l(j) (10) where V n is the set of all finite strings of length n over the alphabet V; i (or j) denotes a vector of indices i = (i 1 , . . . , i |u| ) where",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Computing the String Kernel",
                "sec_num": "4.1"
            },
            {
                "text": "1 < i 1 < i |u| \u2264 |s|; l(i) def = i |u| -i 1 +1",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Computing the String Kernel",
                "sec_num": "4.1"
            },
            {
                "text": "is the length of the substring u in s; \u03bb \u2208 (0, 1] is a decay factor which serves as a penalty for gaps within a compared subsequence.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Computing the String Kernel",
                "sec_num": "4.1"
            },
            {
                "text": "Direct computation of Eq. ( 10) is exponential in |V|, but efficient dynamic programs can be utilized: In this work, we employ the trie-based methods of Rousu and Shawe-Taylor (2005) to compute Eq. ( 10). Under this scheme, the computation of the kernel between two strings s and t is O(n \u2022 M \u2022 log(max(|s|, |t|)), where n is the chosen subsequence length (a hyperparameter) and M is the number of words that strings s and t have in common. Note that |s|, and thus M , are bounded by the time step t. Further, we can reuse many of the computations between subsequent decoding rounds due to the iterative nature of both beam search and the subsequence kernel computations. Additionally, since the magnitude of Eq. ( 10) is influenced by the lengths of s and t, we normalize the kernel as follows:",
                "cite_spans": [
                    {
                        "start": 153,
                        "end": 182,
                        "text": "Rousu and Shawe-Taylor (2005)",
                        "ref_id": "BIBREF26"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Computing the String Kernel",
                "sec_num": "4.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "K norm (s, t) = K(s, t) K(s, s) \u2022 K(t, t)",
                        "eq_num": "(11)"
                    }
                ],
                "section": "Computing the String Kernel",
                "sec_num": "4.1"
            },
            {
                "text": "The string subsequence kernel gives us a straightforward method for decoding diverse sets of strings from language generators. We construct the matrix",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Integration into DetBS",
                "sec_num": "4.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "D + w \u2022 K(B t )",
                        "eq_num": "(12)"
                    }
                ],
                "section": "Integration into DetBS",
                "sec_num": "4.2"
            },
            {
                "text": "using the dynamic program mentioned above to compute K(B t ). Intuitively, we can expect the argmax-i.e., the size k set corresponding to the objective-maximizing submatrix-of D + w \u2022 K(B t ) to have higher subsequence diversity as w is increased. This is perhaps most easily seen when viewing our problem as a k-DPP: if strings y (i) and y (j) have high overlap, this will be reflected in the matrix K(B t ) at position i, j. Higher values of K(B t ) i,j = K(y (i) , y (j) ) lead to lower probability of both y (i) and y (j) being in the set drawn according to the k-DPP parameterized by D + w \u2022 K(B t ), which follows from the properties of DPPs outlined in \u00a72.2. In short, higher values of K(y (i) , y (j) ) decrease the value of log det(D Y + w \u2022 K(B t ) Y ) for sets Y containing both y (i) and y (j) , which makes Y less likely to be chosen in the recursion of Eq. ( 9).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Integration into DetBS",
                "sec_num": "4.2"
            },
            {
                "text": "In our experiments, we explore the use of determinantal beam search as a diverse decoding strategy for language generation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "5"
            },
            {
                "text": "Various diverse decoding strategies exist in the NLP literature. We first discuss those strategies that we employ as baselines in our experiments.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baselines",
                "sec_num": "5.1"
            },
            {
                "text": "Standard Beam Search. Beam search is one of the most widely used decoding algorithms in NLP, where many problems require efficient strategies for decoding solutions from structured predictors. Specifically, for language generation tasks, beam search has repeatedly proved its effectiveness at decoding state-of-the-art solutions (Wu et al., 2016; Serban et al., 2017; Edunov et al., 2018; Yang et al., 2019) . We refer back to \u00a72.1 for the algorithm.",
                "cite_spans": [
                    {
                        "start": 329,
                        "end": 346,
                        "text": "(Wu et al., 2016;",
                        "ref_id": "BIBREF34"
                    },
                    {
                        "start": 347,
                        "end": 367,
                        "text": "Serban et al., 2017;",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 368,
                        "end": 388,
                        "text": "Edunov et al., 2018;",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 389,
                        "end": 407,
                        "text": "Yang et al., 2019)",
                        "ref_id": "BIBREF35"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baselines",
                "sec_num": "5.1"
            },
            {
                "text": "Stochastic Beam Search. Kool et al. (2019) propose stochastic beam search (SBS), a decoding technique that samples without replacement from sequence models according to their distribution over the entire space Y. For random sampling methods such as SBS, it is customary to use a sampling temperature T > 0 at generation time to control for the peakiness of the sampling distribution. This results in the generalized softmax: The algorithm further divides the beam into G groups B 1 t , . . . , B G t , where G is a hyperparameter of the algorithm, and optimizes for diversity between the different groups by subtracting a similarity term \u2206(y \u2264t , B g t ) from the decoding objective.11 Specifically, \u2206(y \u2264t , B g t ) represents the degree of similarity between a hypothesis y \u2264t and a group of hypotheses B g t . They find G = k, i.e., each group contains a single hypothesis, and the Hamming distance similarity metric lead to the best results; we use these settings in our experiments. Note that under this scheme, the solution set may have duplicates if the diversity penalty is not large enough.",
                "cite_spans": [
                    {
                        "start": 24,
                        "end": 42,
                        "text": "Kool et al. (2019)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baselines",
                "sec_num": "5.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "p T (y | y <t , x)",
                        "eq_num": "(13)"
                    }
                ],
                "section": "Baselines",
                "sec_num": "5.1"
            },
            {
                "text": "Notably, under the above experimental settings, the runtimes of diverse beam search and our algorithm are the same, up to computation of the hamming loss and string kernel, respectively. However, while string kernel computations in our algorithm can be done in parallel, the diversity penalty in diverse beam search must be computed sequentially for each hypothesis, as it is based on the previously chosen groups.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baselines",
                "sec_num": "5.1"
            },
            {
                "text": "We run experiments on neural machine translation (NMT) models trained on the WMT'14 (Bojar et al., 2014) En-Fr and the WMT'19 (Barrault (Ott et al., 2019) . We evaluate on the newstest set from the respective datasets, each containing 3003 sentences. Further details can be found in App. B.",
                "cite_spans": [
                    {
                        "start": 84,
                        "end": 104,
                        "text": "(Bojar et al., 2014)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 136,
                        "end": 154,
                        "text": "(Ott et al., 2019)",
                        "ref_id": "BIBREF25"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Setup",
                "sec_num": "5.2"
            },
            {
                "text": "For determinantal beam search (DetBS), we perform a hyperparameter search (precise details likewise in App. B) over \u03bb and n, the decay factor and subsequence length, respectively. Search is performed for fixed w = 0.1 and k = 10 on validation sets for both languages; we omit a search over the entire space of w, k, \u03bb, n so as to not create an unfair advantage for DetBS in comparison with the other decoding strategies, for which no hyperparameters are tuned. We use subsequence length n = 2 and \u03bb \u2208 {0.1, 0.3} for De-En and En-Fr, respectively.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Setup",
                "sec_num": "5.2"
            },
            {
                "text": "We decode sets of size k \u2208 {5, 10, 20} with each strategy, comparing sentence-level ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Setup",
                "sec_num": "5.2"
            },
            {
                "text": "Fig. 1 shows the sentence-level BLEU score and averaged n-gram diversity on the newstest set for different decoding strategies; Tab. 2 shows explicit coverage of 1, 2, 3, 4-grams and averaged across 1, 2, 3, 4-grams for different decoding strategies when BLEU is controlled for. The 3 lines Source Sentence Zum Abschluss wurde eine Tombola verlost. Die Wahrheit zu sagen ist aber kein Verbrechen.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 5,
                        "end": 6,
                        "text": "1",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "5.3"
            },
            {
                "text": "Beam Search (T = 0.6) \u2022 A raffle was held to close the event.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "5.3"
            },
            {
                "text": "\u2022 But telling the truth is not a crime. \u2022 A raffle was held to conclude the event.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "5.3"
            },
            {
                "text": "\u2022 Telling the truth is not a crime. \u2022 A raffle was held at the end.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "5.3"
            },
            {
                "text": "\u2022 But telling the truth isn't a crime. \u2022 At the end a raffle was held.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "5.3"
            },
            {
                "text": "\u2022 But telling the truth is no crime. \u2022 A raffle was held to close the draw.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "5.3"
            },
            {
                "text": "\u2022 However, telling the truth is not a crime. Diverse Beam Search (w = 0.4) \u2022 To conclude, a raffle was held.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "5.3"
            },
            {
                "text": "\u2022 But telling the truth is not a crime. \u2022 A raffle was held to close the event.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "5.3"
            },
            {
                "text": "\u2022 But telling the truth is not a crime. \u2022 A raffle was held to close the event.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "5.3"
            },
            {
                "text": "\u2022 But telling the truth is not a crime. \u2022 A raffle was held to close the event.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "5.3"
            },
            {
                "text": "\u2022 But telling the truth is not a crime. \u2022 At the end of the event, a raffle was held.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "5.3"
            },
            {
                "text": "\u2022 Telling the truth, however, is not a crime. Determinantal Beam Search (w = 0.12) \u2022 Finally, a raffle was held.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "5.3"
            },
            {
                "text": "\u2022 But telling the truth is not a crime. \u2022 A raffle was held at the end.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "5.3"
            },
            {
                "text": "\u2022 But telling the truth isn't a crime. \u2022 At the end a raffle was held.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "5.3"
            },
            {
                "text": "\u2022 Telling the truth is not a crime. \u2022 To conclude, a raffle was held.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "5.3"
            },
            {
                "text": "\u2022 However, telling the truth is not a crime. \u2022 A raffle was held to close the event.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "5.3"
            },
            {
                "text": "\u2022 But to tell the truth is not a crime.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "5.3"
            },
            {
                "text": "Table 1 : Generated translations for given source sentence from WMT'19 De-En dataset using different decoding strategies. All use a beam size of five. 13 We see large overlap in Beam Search results while DBS actually returns several of the same results. In comparision, DetBS turns qualitatively diverse results even for simple sentences.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "5.3"
            },
            {
                "text": "per decoding strategy in Fig. 1 represent the minimum, median, and maximum sentence-level BLEU score out of the k translation options, averaged across the corpus. We consider median BLEU to be the best metric of set text-quality, as a good diverse decoding algorithm should not completely sacrifice BLEU for the sake of diversity. The plots are analogous to those in Kool et al. (2019) .",
                "cite_spans": [
                    {
                        "start": 367,
                        "end": 385,
                        "text": "Kool et al. (2019)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 30,
                        "end": 31,
                        "text": "1",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "5.3"
            },
            {
                "text": "On both datasets and across different set sizes, results indicate that DetBS generates diverse sets of strings while maintaining high median and maximum BLEU scores. We see similar or higher n-gram diversity in comparison to DBS for the same median BLEU and a notably better n-gram diversity vs. BLEU trade-off than standard beam search and SBS. Further, the highest quality translation (shown by max BLEU) does not appear to be sacrificed when the diversity parameter is increased for DetBS. In contrast, there is a notable drop-off for generation strategies in which diversity is controlled for using temperature. We show samples of generated text in Tab. 1.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "5.3"
            },
            {
                "text": "Our work is built upon much of the subset optimization literature in machine learning. We base our algorithm off the subdeterminant maximization problem (Agarwal et al., 2004) , which has been used to find core sets-a concept originating in computational geometry concerning the existence of a small, representative set of core items-in data summarization problems (Mirzasoleiman et al., 2013) , nearest neighbor search (Abbar et al., 2013) and streaming algorithms (Indyk et al., 2014) inter alia. Informally, we can connect our problem to the notion of decoding a core set from sequence models. To the best of our knowledge, our work is the first to use this concept when decoding sequence models. Wang and Chan (2019) incorporate DPPs into a reinforcement learning objective to optimize for diverse text when training image captioning models. We optimize for diversity during decoding, rather than training, which makes our methods applicable with out-of-the-box models and allows us to avoid highly hyperparametersensitive techniques, like minimum-risk training or reinforcement learning-based algorithms, while achieving the same goal. While the application of our methods at training times is an interesting research direction, we foresee technical challenges corresponding to such approaches that may outweigh their benefits.",
                "cite_spans": [
                    {
                        "start": 153,
                        "end": 175,
                        "text": "(Agarwal et al., 2004)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 365,
                        "end": 393,
                        "text": "(Mirzasoleiman et al., 2013)",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 420,
                        "end": 440,
                        "text": "(Abbar et al., 2013)",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 466,
                        "end": 486,
                        "text": "(Indyk et al., 2014)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 700,
                        "end": 720,
                        "text": "Wang and Chan (2019)",
                        "ref_id": "BIBREF33"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "6"
            },
            {
                "text": "As a decoding method, our work is closest Table 2 : We report the coverage (as defined in Eq. ( 14)) of 1, 2, 3, 4-grams and averaged across 1, 2, 3, 4-grams as well as median BLEU for k = 20 on the newstest dataset. For each decoding strategy, we report metrics on the generated set that has highest (average) d n , where we set the constraint that median BLEU for the set is still within 1 point of the highest median BLEU (across decoding strategies and diversity parameters). 15 to that of Vijayakumar et al. (2018) , who propose a variation of beam search (described in \u00a75.3). However, their algorithm lacks theoretical motivation and is not guaranteed to provide a nonoverlapping set; the same solution may appear multiple times in the decoded set if the diversity penalty is not large enough, as shown in Tab. 2. Additionally, groups at each time step t must be processed in order since the score of all hypotheses considered for group g + 1 depend on hypotheses in groups 1, . . . , g, which creates a large bottleneck under the recommended settings of G = k. Random sampling strategies for decoding neural sequence models have received much attention in recent years. While techniques such as stochastic beam search and the UniqueRandomizer (Shi et al., 2020) are convenient for creating statistical estimators and have uses in reinforcement learning techniques due to their clear probabilistic interpretation, there are no diversity guarantees for the set of generated sequences.",
                "cite_spans": [
                    {
                        "start": 494,
                        "end": 519,
                        "text": "Vijayakumar et al. (2018)",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 1250,
                        "end": 1268,
                        "text": "(Shi et al., 2020)",
                        "ref_id": "BIBREF28"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 48,
                        "end": 49,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "6"
            },
            {
                "text": "Tam (2020) likewise adapts beam search, proposing a k-means clustering version that clusters solutions by averaged word embeddings. As there lacks an interpretation of distance between averaged word embeddings though, it is unclear if the method can explicitly optimize for any tangible notion of coverage or diversity.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "6"
            },
            {
                "text": "We propose determinantal beam search (DetBS): a new way of framing beam search that allows us to optimize set generation for diversity and coverage rather than simply individual scores. Formally, we redefine beam search as an iterative subdeterminant maximization problems where we select the approximately maximizing set according to the PSD matrix parameterizing our score function. This gives us the ability to encode the notion of intra-set diversity into the beam search optimization problem. We discuss and experiment with efficient methods for inference and kernel computation that make DetBS an efficient decoding strategy in practice. We use DetBS in the context of language generation, where we explicitly encourage n-gram coverage through the string subsequence kernel. In our NMT experiments, we find DetBS generates much more diverse sets of strings than standard beam search and stochastic beam search with a small tradeoff in median BLEU. We observe competitive performance compared with diverse beam search.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "7"
            },
            {
                "text": "Algorithm 1 Fast Greedy MAP Inference with log-space parameterization (Chen et al., 2018) . We transform computations according to (Li and Eisner, 2009) . ",
                "cite_spans": [
                    {
                        "start": 70,
                        "end": 89,
                        "text": "(Chen et al., 2018)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 131,
                        "end": 152,
                        "text": "(Li and Eisner, 2009)",
                        "ref_id": "BIBREF20"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Log-Space Computations",
                "sec_num": null
            },
            {
                "text": "Input: L: log of PSD matrix k: desired set size 1: function GREEDY_MAP_INFERENCE( ) 2: ci = [ ], di = Lii, si = [ ] 3: j =",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Log-Space Computations",
                "sec_num": null
            },
            {
                "text": "Hyperparameters. As we use the string subsequence kernel of section \u00a74 in DetBS, there are a number of hyperparameters that can be adjusted beyond the diversity weight w: the decay factor \u03bb indicates the degree to which interior gaps are penalized and subsequence length n indicates the length of the considered substrings u. For each language, we perform a search over these two hyperparameters for set size k = 10 and diversity coefficient w = 0.1 on validation sets. We use a grid search over n = [2, 3, 4, 5, 6, 7, 8 ] and \u03bb = [0.1, 0.3, 0.5, 0.7, 1.0]. We choose the configuration that yields the highest (average n-gram diversity)*BLEU, using this configuration in all subsequent experiments. While there may be better performing hyperparameters under different k and w, we omit searching over the entire space to create a fairer comparison with the other decoding strategies.",
                "cite_spans": [
                    {
                        "start": 500,
                        "end": 503,
                        "text": "[2,",
                        "ref_id": null
                    },
                    {
                        "start": 504,
                        "end": 506,
                        "text": "3,",
                        "ref_id": null
                    },
                    {
                        "start": 507,
                        "end": 509,
                        "text": "4,",
                        "ref_id": null
                    },
                    {
                        "start": 510,
                        "end": 512,
                        "text": "5,",
                        "ref_id": null
                    },
                    {
                        "start": 513,
                        "end": 515,
                        "text": "6,",
                        "ref_id": null
                    },
                    {
                        "start": 516,
                        "end": 518,
                        "text": "7,",
                        "ref_id": null
                    },
                    {
                        "start": 519,
                        "end": 520,
                        "text": "8",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B Experimental Setup",
                "sec_num": null
            },
            {
                "text": "n decay (\u03bb)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dataset",
                "sec_num": null
            },
            {
                "text": "WMT'14 En-Fr 2 0.3 WMT'19 De-En 2 0.1 Likewise, data preprocessing steps, model hyperparameters and baseline performances can be found in Ng et al. (2019) . We similarly use the pretrained model checkpoints made available by fairseq. ",
                "cite_spans": [
                    {
                        "start": 138,
                        "end": 154,
                        "text": "Ng et al. (2019)",
                        "ref_id": "BIBREF24"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dataset",
                "sec_num": null
            },
            {
                "text": "As concrete examples, in machine translation there almost always exist multiple ways to translate a sentence; in story generation, we often seek creative language or multiple options to choose from.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "A number of NLP tasks only take the highest-scoring element of the returned set Y while other tasks utilize the entire set of solutions.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "We have omitted the time-step dependence of D for notational brevity as it is always clear from context.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "5 Albeit with a cardinality constraint.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "To see this, recall that the determinant is the product of the eigenvalues. To ensure that the determinant is strictly positive, we can simply enforce that all the eigenvalues are positive, which is necessarily the case for PSD matrices. Note that in the case where any of the eigenvalues of a submatrix are zero, we take log det(\u2022) = -",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "\u221e.7 In machine learning literature, the term \"kernel\" is often used to refer to both the function K and the kernel",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "matrix K.8 To see this, note that the matrix D is necessarily PSD. Since PSD matrices are closed under addition and multiplication by a positive scalar, then necessarily D + w \u2022 K is PSD. Lastly, any submatrix of a PSD matrix is also PSD, which makes DY + w \u2022 KY a PSD matrix.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "We may also sample from the k-DPP modeled by Eq. (9) rather than taking the approximate mode; this would only require changing the inference algorithm and can be done in a similarly efficient manner(Li et al., 2016). We focus on deterministic methods in this work as we aim to find the objective maximizing",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "set.10 As beam search is already a heuristic approach, such an approximation does not have any theoretical implications for the results of our algorithm.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "The diversity term has coefficient w to determine the strength of the penalty. When this weight is 0 or sufficiently small, all groups will return the same solution(s).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://github.com/pytorch/fairseq/ tree/master/examples/translation",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "For each decoding strategy, we choose the diversity parameter corresponding to the most diverse set that had median BLEU 28.5 \u00b1 0.05.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Recall w = 0 recovers standard beam search with a temperature of T = 1.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "In the case that not all strategies had such a set, we instead bounded BLEU by the lowest of the median BLEU across decoding strategies.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "available at http://statmt.org/wmt14/ translation-task.html",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "available at http://www.statmt.org/wmt19/ translation-task.html",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "We would like to thank the anonymous reviewers for their helpful feedback and recommendations.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgements",
                "sec_num": null
            },
            {
                "text": "While language generation can be used for malicious purposes, e.g., to propagate misinformation or offensive text, we do not foresee any specific ethical concerns with the techniques in this work.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Ethical Considerations",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Diverse near neighbor problem",
                "authors": [
                    {
                        "first": "Sofiane",
                        "middle": [],
                        "last": "Abbar",
                        "suffix": ""
                    },
                    {
                        "first": "Sihem",
                        "middle": [],
                        "last": "Amer-Yahia",
                        "suffix": ""
                    },
                    {
                        "first": "Piotr",
                        "middle": [],
                        "last": "Indyk",
                        "suffix": ""
                    },
                    {
                        "first": "Sepideh",
                        "middle": [],
                        "last": "Mahabadi",
                        "suffix": ""
                    },
                    {
                        "first": "Kasturi",
                        "middle": [
                            "R"
                        ],
                        "last": "Varadarajan",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of the Twenty-Ninth Annual Symposium on Computational Geometry. Association for Computing Machinery",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.1145/2462356.2462401"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Sofiane Abbar, Sihem Amer-Yahia, Piotr Indyk, Sepi- deh Mahabadi, and Kasturi R. Varadarajan. 2013. Diverse near neighbor problem. In Proceedings of the Twenty-Ninth Annual Symposium on Computa- tional Geometry. Association for Computing Ma- chinery.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Approximating extent measures of points",
                "authors": [
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Pankaj",
                        "suffix": ""
                    },
                    {
                        "first": "Sariel",
                        "middle": [],
                        "last": "Agarwal",
                        "suffix": ""
                    },
                    {
                        "first": "Kasturi",
                        "middle": [
                            "R"
                        ],
                        "last": "Har-Peled",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Varadarajan",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Journal of the Association for Computing Machinery",
                "volume": "51",
                "issue": "4",
                "pages": "606--635",
                "other_ids": {
                    "DOI": [
                        "10.1145/1008731.1008736"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Pankaj K. Agarwal, Sariel Har-Peled, and Kasturi R. Varadarajan. 2004. Approximating extent measures of points. Journal of the Association for Computing Machinery, 51(4):606-635.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Findings of the 2019 conference on machine translation",
                "authors": [
                    {
                        "first": "Lo\u00efc",
                        "middle": [],
                        "last": "Barrault",
                        "suffix": ""
                    },
                    {
                        "first": "Ond\u0159ej",
                        "middle": [],
                        "last": "Bojar",
                        "suffix": ""
                    },
                    {
                        "first": "Marta",
                        "middle": [
                            "R"
                        ],
                        "last": "Costa-Juss\u00e0",
                        "suffix": ""
                    },
                    {
                        "first": "Christian",
                        "middle": [],
                        "last": "Federmann",
                        "suffix": ""
                    },
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Fishel",
                        "suffix": ""
                    },
                    {
                        "first": "Yvette",
                        "middle": [],
                        "last": "Graham",
                        "suffix": ""
                    },
                    {
                        "first": "Barry",
                        "middle": [],
                        "last": "Haddow",
                        "suffix": ""
                    },
                    {
                        "first": "Matthias",
                        "middle": [],
                        "last": "Huck",
                        "suffix": ""
                    },
                    {
                        "first": "Philipp",
                        "middle": [],
                        "last": "Koehn",
                        "suffix": ""
                    },
                    {
                        "first": "Shervin",
                        "middle": [],
                        "last": "Malmasi",
                        "suffix": ""
                    },
                    {
                        "first": "Christof",
                        "middle": [],
                        "last": "Monz",
                        "suffix": ""
                    },
                    {
                        "first": "Mathias",
                        "middle": [],
                        "last": "M\u00fcller",
                        "suffix": ""
                    },
                    {
                        "first": "Santanu",
                        "middle": [],
                        "last": "Pal",
                        "suffix": ""
                    },
                    {
                        "first": "Matt",
                        "middle": [],
                        "last": "Post",
                        "suffix": ""
                    },
                    {
                        "first": "Marcos",
                        "middle": [],
                        "last": "Zampieri",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the Fourth Conference on Machine Translation",
                "volume": "",
                "issue": "",
                "pages": "1--61",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/W19-5301"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Lo\u00efc Barrault, Ond\u0159ej Bojar, Marta R. Costa-juss\u00e0, Christian Federmann, Mark Fishel, Yvette Gra- ham, Barry Haddow, Matthias Huck, Philipp Koehn, Shervin Malmasi, Christof Monz, Mathias M\u00fcller, Santanu Pal, Matt Post, and Marcos Zampieri. 2019. Findings of the 2019 conference on machine trans- lation. In Proceedings of the Fourth Conference on Machine Translation, pages 1-61. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Findings of the 2014 workshop on statistical machine translation",
                "authors": [
                    {
                        "first": "Ond\u0159ej",
                        "middle": [],
                        "last": "Bojar",
                        "suffix": ""
                    },
                    {
                        "first": "Christian",
                        "middle": [],
                        "last": "Buck",
                        "suffix": ""
                    },
                    {
                        "first": "Christian",
                        "middle": [],
                        "last": "Federmann",
                        "suffix": ""
                    },
                    {
                        "first": "Barry",
                        "middle": [],
                        "last": "Haddow",
                        "suffix": ""
                    },
                    {
                        "first": "Philipp",
                        "middle": [],
                        "last": "Koehn",
                        "suffix": ""
                    },
                    {
                        "first": "Johannes",
                        "middle": [],
                        "last": "Leveling",
                        "suffix": ""
                    },
                    {
                        "first": "Christof",
                        "middle": [],
                        "last": "Monz",
                        "suffix": ""
                    },
                    {
                        "first": "Pavel",
                        "middle": [],
                        "last": "Pecina",
                        "suffix": ""
                    },
                    {
                        "first": "Matt",
                        "middle": [],
                        "last": "Post",
                        "suffix": ""
                    },
                    {
                        "first": "Herve",
                        "middle": [],
                        "last": "Saint-Amand",
                        "suffix": ""
                    },
                    {
                        "first": "Radu",
                        "middle": [],
                        "last": "Soricut",
                        "suffix": ""
                    },
                    {
                        "first": "Lucia",
                        "middle": [],
                        "last": "Specia",
                        "suffix": ""
                    },
                    {
                        "first": "Ale\u0161",
                        "middle": [],
                        "last": "Tamchyna",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of the Ninth Workshop on Statistical Machine Translation",
                "volume": "",
                "issue": "",
                "pages": "12--58",
                "other_ids": {
                    "DOI": [
                        "10.3115/v1/W14-3302"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ond\u0159ej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, and Ale\u0161 Tamchyna. 2014. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the Ninth Workshop on Statistical Machine Trans- lation, pages 12-58. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Fast greedy map inference for determinantal point process to improve recommendation diversity",
                "authors": [
                    {
                        "first": "Laming",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Guoxin",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Eric",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "5622--5633",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Laming Chen, Guoxin Zhang, and Eric Zhou. 2018. Fast greedy map inference for determinantal point process to improve recommendation diversity. In Advances in Neural Information Processing Sys- tems, pages 5622-5633.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Subdeterminant maximization via nonconvex relaxations and anti-concentration",
                "authors": [
                    {
                        "first": "J",
                        "middle": [
                            "B"
                        ],
                        "last": "Ebrahimi",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Straszak",
                        "suffix": ""
                    },
                    {
                        "first": "N",
                        "middle": [
                            "K"
                        ],
                        "last": "Vishnoi",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS)",
                "volume": "",
                "issue": "",
                "pages": "1020--1031",
                "other_ids": {
                    "DOI": [
                        "10.1109/FOCS.2017.98"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "J. B. Ebrahimi, D. Straszak, and N. K. Vishnoi. 2017. Subdeterminant maximization via nonconvex relax- ations and anti-concentration. In 2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS), pages 1020-1031. IEEE Computer Society.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Understanding back-translation at scale",
                "authors": [
                    {
                        "first": "Sergey",
                        "middle": [],
                        "last": "Edunov",
                        "suffix": ""
                    },
                    {
                        "first": "Myle",
                        "middle": [],
                        "last": "Ott",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Auli",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Grangier",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "489--500",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/D18-1045"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Sergey Edunov, Myle Ott, Michael Auli, and David Grangier. 2018. Understanding back-translation at scale. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Process- ing, pages 489-500. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Is MAP decoding all you need? The inadequacy of the mode in neural machine translation",
                "authors": [
                    {
                        "first": "Bryan",
                        "middle": [],
                        "last": "Eikema",
                        "suffix": ""
                    },
                    {
                        "first": "Wilker",
                        "middle": [],
                        "last": "Aziz",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 28th International Conference on Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "4506--4520",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2020.coling-main.398"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Bryan Eikema and Wilker Aziz. 2020. Is MAP de- coding all you need? The inadequacy of the mode in neural machine translation. In Proceedings of the 28th International Conference on Computational Linguistics, pages 4506-4520. International Com- mittee on Computational Linguistics.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Efficient approximation algorithms for strings kernel based sequence classification",
                "authors": [
                    {
                        "first": "Muhammad",
                        "middle": [],
                        "last": "Farhan",
                        "suffix": ""
                    },
                    {
                        "first": "Juvaria",
                        "middle": [],
                        "last": "Tariq",
                        "suffix": ""
                    },
                    {
                        "first": "Arif",
                        "middle": [],
                        "last": "Zaman",
                        "suffix": ""
                    },
                    {
                        "first": "Mudassir",
                        "middle": [],
                        "last": "Shabbir",
                        "suffix": ""
                    },
                    {
                        "first": "Imdad Ullah",
                        "middle": [],
                        "last": "Khan",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "30",
                "issue": "",
                "pages": "6935--6945",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Muhammad Farhan, Juvaria Tariq, Arif Zaman, Mu- dassir Shabbir, and Imdad Ullah Khan. 2017. Ef- ficient approximation algorithms for strings kernel based sequence classification. In Advances in Neu- ral Information Processing Systems, volume 30, pages 6935-6945. Curran Associates, Inc.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Convolutional sequence to sequence learning",
                "authors": [
                    {
                        "first": "Jonas",
                        "middle": [],
                        "last": "Gehring",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Auli",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Grangier",
                        "suffix": ""
                    },
                    {
                        "first": "Denis",
                        "middle": [],
                        "last": "Yarats",
                        "suffix": ""
                    },
                    {
                        "first": "Yann",
                        "middle": [
                            "N"
                        ],
                        "last": "Dauphin",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the 34th International Conference on Machine Learning",
                "volume": "70",
                "issue": "",
                "pages": "1243--1252",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jonas Gehring, Michael Auli, David Grangier, De- nis Yarats, and Yann N. Dauphin. 2017. Convolu- tional sequence to sequence learning. In Proceed- ings of the 34th International Conference on Ma- chine Learning, volume 70, pages 1243-1252.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Near-optimal MAP inference for determinantal point processes",
                "authors": [
                    {
                        "first": "Jennifer",
                        "middle": [],
                        "last": "Gillenwater",
                        "suffix": ""
                    },
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Kulesza",
                        "suffix": ""
                    },
                    {
                        "first": "Ben",
                        "middle": [],
                        "last": "Taskar",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "25",
                "issue": "",
                "pages": "2735--2743",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jennifer Gillenwater, Alex Kulesza, and Ben Taskar. 2012. Near-optimal MAP inference for determinan- tal point processes. In Advances in Neural Informa- tion Processing Systems, volume 25, pages 2735- 2743. Curran Associates, Inc.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Faster greedy MAP inference for determinantal point processes",
                "authors": [
                    {
                        "first": "Insu",
                        "middle": [],
                        "last": "Han",
                        "suffix": ""
                    },
                    {
                        "first": "Prabhanjan",
                        "middle": [],
                        "last": "Kambadur",
                        "suffix": ""
                    },
                    {
                        "first": "Kyoungsoo",
                        "middle": [],
                        "last": "Park",
                        "suffix": ""
                    },
                    {
                        "first": "Jinwoo",
                        "middle": [],
                        "last": "Shin",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of Machine Learning Research",
                "volume": "70",
                "issue": "",
                "pages": "1384--1393",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Insu Han, Prabhanjan Kambadur, Kyoungsoo Park, and Jinwoo Shin. 2017. Faster greedy MAP inference for determinantal point processes. volume 70 of Proceedings of Machine Learning Research, pages 1384-1393.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "The curious case of neural text degeneration",
                "authors": [
                    {
                        "first": "Ari",
                        "middle": [],
                        "last": "Holtzman",
                        "suffix": ""
                    },
                    {
                        "first": "Jan",
                        "middle": [],
                        "last": "Buys",
                        "suffix": ""
                    },
                    {
                        "first": "Li",
                        "middle": [],
                        "last": "Du",
                        "suffix": ""
                    },
                    {
                        "first": "Maxwell",
                        "middle": [],
                        "last": "Forbes",
                        "suffix": ""
                    },
                    {
                        "first": "Yejin",
                        "middle": [],
                        "last": "Choi",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the International Conference on Learning Representations",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The curious case of neural text degeneration. In Proceedings of the International Conference on Learning Representations.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Composable core-sets for diversity and coverage maximization",
                "authors": [
                    {
                        "first": "Piotr",
                        "middle": [],
                        "last": "Indyk",
                        "suffix": ""
                    },
                    {
                        "first": "Sepideh",
                        "middle": [],
                        "last": "Mahabadi",
                        "suffix": ""
                    },
                    {
                        "first": "Mohammad",
                        "middle": [],
                        "last": "Mahdian",
                        "suffix": ""
                    },
                    {
                        "first": "Vahab",
                        "middle": [
                            "S"
                        ],
                        "last": "Mirrokni",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of the Thirty-Third Association for Computing Machinery SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems",
                "volume": "",
                "issue": "",
                "pages": "100--108",
                "other_ids": {
                    "DOI": [
                        "10.1145/2594538.2594560"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Piotr Indyk, Sepideh Mahabadi, Mohammad Mahdian, and Vahab S. Mirrokni. 2014. Composable core-sets for diversity and coverage maximization. In Pro- ceedings of the Thirty-Third Association for Com- puting Machinery SIGMOD-SIGACT-SIGART Sym- posium on Principles of Database Systems, page 100-108. Association for Computing Machinery.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Largest j-simplices in n-polytopes",
                "authors": [
                    {
                        "first": "V",
                        "middle": [],
                        "last": "Klee",
                        "suffix": ""
                    },
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Gritzmann",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Larman",
                        "suffix": ""
                    }
                ],
                "year": 1995,
                "venue": "Discrete and Computational Geometry",
                "volume": "13",
                "issue": "3-4",
                "pages": "477--516",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "V. Klee, P. Gritzmann, and D. Larman. 1995. Largest j-simplices in n-polytopes. Discrete and Computa- tional Geometry, 13(3-4):477-516.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "An exact algorithm for maximum entropy sampling",
                "authors": [
                    {
                        "first": "Chun-Wa",
                        "middle": [],
                        "last": "Ko",
                        "suffix": ""
                    },
                    {
                        "first": "Jon",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Maurice",
                        "middle": [],
                        "last": "Queyranne",
                        "suffix": ""
                    }
                ],
                "year": 1995,
                "venue": "Operations Research",
                "volume": "43",
                "issue": "4",
                "pages": "684--691",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chun-Wa Ko, Jon Lee, and Maurice Queyranne. 1995. An exact algorithm for maximum entropy sampling. Operations Research, 43(4):684-691.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Stochastic beams and where to find them: The Gumbel-top-k trick for sampling sequences without replacement",
                "authors": [
                    {
                        "first": "Wouter",
                        "middle": [],
                        "last": "Kool",
                        "suffix": ""
                    },
                    {
                        "first": "Herke",
                        "middle": [],
                        "last": "Van Hoof",
                        "suffix": ""
                    },
                    {
                        "first": "Max",
                        "middle": [],
                        "last": "Welling",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the International Conference on Machine Learning",
                "volume": "",
                "issue": "",
                "pages": "3499--3508",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Wouter Kool, Herke Van Hoof, and Max Welling. 2019. Stochastic beams and where to find them: The Gumbel-top-k trick for sampling sequences without replacement. In Proceedings of the Inter- national Conference on Machine Learning, pages 3499-3508.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "2011. k-DPPs: fixedsize determinantal point processes",
                "authors": [
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Kulesza",
                        "suffix": ""
                    },
                    {
                        "first": "Ben",
                        "middle": [],
                        "last": "Taskar",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "Proceedings of the 28th International Conference on Machine Learning",
                "volume": "",
                "issue": "",
                "pages": "1193--1200",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alex Kulesza and Ben Taskar. 2011. k-DPPs: fixed- size determinantal point processes. In Proceedings of the 28th International Conference on Machine Learning, pages 1193-1200.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Determinantal Point Processes for Machine Learning",
                "authors": [
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Kulesza",
                        "suffix": ""
                    },
                    {
                        "first": "Ben",
                        "middle": [],
                        "last": "Taskar",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.5555/2481023"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Alex Kulesza and Ben Taskar. 2012. Determinantal Point Processes for Machine Learning. Now Pub- lishers Inc.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Efficient sampling for k-determinantal point processes",
                "authors": [
                    {
                        "first": "Chengtao",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Stefanie",
                        "middle": [],
                        "last": "Jegelka",
                        "suffix": ""
                    },
                    {
                        "first": "Suvrit",
                        "middle": [],
                        "last": "Sra",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of Machine Learning Research",
                "volume": "51",
                "issue": "",
                "pages": "1328--1337",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chengtao Li, Stefanie Jegelka, and Suvrit Sra. 2016. Efficient sampling for k-determinantal point pro- cesses. volume 51 of Proceedings of Machine Learning Research, pages 1328-1337.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "First-and secondorder expectation semirings with applications to minimum-risk training on translation forests",
                "authors": [
                    {
                        "first": "Zhifei",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Eisner",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "40--51",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zhifei Li and Jason Eisner. 2009. First-and second- order expectation semirings with applications to minimum-risk training on translation forests. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 40-51. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Text classification using string kernels",
                "authors": [
                    {
                        "first": "Huma",
                        "middle": [],
                        "last": "Lodhi",
                        "suffix": ""
                    },
                    {
                        "first": "Craig",
                        "middle": [],
                        "last": "Saunders",
                        "suffix": ""
                    },
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Shawe-Taylor",
                        "suffix": ""
                    },
                    {
                        "first": "Nello",
                        "middle": [],
                        "last": "Cristianini",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Watkins",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Journal of Machine Learning Research",
                "volume": "2",
                "issue": "",
                "pages": "419--444",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello Cristianini, and Chris Watkins. 2002. Text classification using string kernels. Journal of Ma- chine Learning Research, 2:419-444.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "If beam search is the answer, what was the question?",
                "authors": [
                    {
                        "first": "Clara",
                        "middle": [],
                        "last": "Meister",
                        "suffix": ""
                    },
                    {
                        "first": "Ryan",
                        "middle": [],
                        "last": "Cotterell",
                        "suffix": ""
                    },
                    {
                        "first": "Tim",
                        "middle": [],
                        "last": "Vieira",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "volume": "",
                "issue": "",
                "pages": "2173--2185",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Clara Meister, Ryan Cotterell, and Tim Vieira. 2020. If beam search is the answer, what was the ques- tion? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2173-2185. Association for Com- putational Linguistics.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Distributed submodular maximization: Identifying representative elements in massive data",
                "authors": [
                    {
                        "first": "Baharan",
                        "middle": [],
                        "last": "Mirzasoleiman",
                        "suffix": ""
                    },
                    {
                        "first": "Amin",
                        "middle": [],
                        "last": "Karbasi",
                        "suffix": ""
                    },
                    {
                        "first": "Rik",
                        "middle": [],
                        "last": "Sarkar",
                        "suffix": ""
                    },
                    {
                        "first": "Andreas",
                        "middle": [],
                        "last": "Krause",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "26",
                "issue": "",
                "pages": "2049--2057",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Baharan Mirzasoleiman, Amin Karbasi, Rik Sarkar, and Andreas Krause. 2013. Distributed submodular maximization: Identifying representative elements in massive data. In Advances in Neural Information Processing Systems, volume 26, pages 2049-2057.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Facebook FAIR's WMT19 news translation task submission",
                "authors": [
                    {
                        "first": "Nathan",
                        "middle": [],
                        "last": "Ng",
                        "suffix": ""
                    },
                    {
                        "first": "Kyra",
                        "middle": [],
                        "last": "Yee",
                        "suffix": ""
                    },
                    {
                        "first": "Alexei",
                        "middle": [],
                        "last": "Baevski",
                        "suffix": ""
                    },
                    {
                        "first": "Myle",
                        "middle": [],
                        "last": "Ott",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Auli",
                        "suffix": ""
                    },
                    {
                        "first": "Sergey",
                        "middle": [],
                        "last": "Edunov",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the Fourth Conference on Machine Translation",
                "volume": "2",
                "issue": "",
                "pages": "314--319",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/W19-5333"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Nathan Ng, Kyra Yee, Alexei Baevski, Myle Ott, Michael Auli, and Sergey Edunov. 2019. Facebook FAIR's WMT19 news translation task submission. In Proceedings of the Fourth Conference on Ma- chine Translation (Volume 2: Shared Task Papers, Day 1), pages 314-319. Association for Computa- tional Linguistics.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "fairseq: A fast, extensible toolkit for sequence modeling",
                "authors": [
                    {
                        "first": "Myle",
                        "middle": [],
                        "last": "Ott",
                        "suffix": ""
                    },
                    {
                        "first": "Sergey",
                        "middle": [],
                        "last": "Edunov",
                        "suffix": ""
                    },
                    {
                        "first": "Alexei",
                        "middle": [],
                        "last": "Baevski",
                        "suffix": ""
                    },
                    {
                        "first": "Angela",
                        "middle": [],
                        "last": "Fan",
                        "suffix": ""
                    },
                    {
                        "first": "Sam",
                        "middle": [],
                        "last": "Gross",
                        "suffix": ""
                    },
                    {
                        "first": "Nathan",
                        "middle": [],
                        "last": "Ng",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Grangier",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Auli",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)",
                "volume": "",
                "issue": "",
                "pages": "48--53",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/N19-4009"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. 2019. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the 2019 Conference of the North American Chap- ter of the Association for Computational Linguis- tics (Demonstrations), pages 48-53. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Efficient computation of gapped substring kernels on large alphabets",
                "authors": [
                    {
                        "first": "Juho",
                        "middle": [],
                        "last": "Rousu",
                        "suffix": ""
                    },
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Shawe-Taylor",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Journal of Machine Learning Research",
                "volume": "6",
                "issue": "",
                "pages": "1323--1344",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Juho Rousu and John Shawe-Taylor. 2005. Efficient computation of gapped substring kernels on large alphabets. Journal of Machine Learning Research, 6:1323-1344.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Multiresolution recurrent neural networks: An application to dialogue response generation",
                "authors": [
                    {
                        "first": "Iulian",
                        "middle": [],
                        "last": "Vlad Serban",
                        "suffix": ""
                    },
                    {
                        "first": "Tim",
                        "middle": [],
                        "last": "Klinger",
                        "suffix": ""
                    },
                    {
                        "first": "Gerald",
                        "middle": [],
                        "last": "Tesauro",
                        "suffix": ""
                    },
                    {
                        "first": "Kartik",
                        "middle": [],
                        "last": "Talamadupula",
                        "suffix": ""
                    },
                    {
                        "first": "Bowen",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    },
                    {
                        "first": "Aaron",
                        "middle": [],
                        "last": "Courville",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence",
                "volume": "",
                "issue": "",
                "pages": "3288--3294",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Iulian Vlad Serban, Tim Klinger, Gerald Tesauro, Kar- tik Talamadupula, Bowen Zhou, Yoshua Bengio, and Aaron Courville. 2017. Multiresolution recur- rent neural networks: An application to dialogue response generation. In Proceedings of the Thirty- First AAAI Conference on Artificial Intelligence, page 3288-3294. AAAI Press.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Incremental sampling without replacement for sequence models",
                "authors": [
                    {
                        "first": "Kensen",
                        "middle": [],
                        "last": "Shi",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Bieber",
                        "suffix": ""
                    },
                    {
                        "first": "Charles",
                        "middle": [],
                        "last": "Sutton",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 37th International Conference on Machine Learning",
                "volume": "119",
                "issue": "",
                "pages": "8785--8795",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kensen Shi, David Bieber, and Charles Sutton. 2020. Incremental sampling without replacement for se- quence models. In Proceedings of the 37th Inter- national Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 8785-8795. PMLR.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Memory efficient kernel approximation",
                "authors": [
                    {
                        "first": "Si",
                        "middle": [],
                        "last": "Si",
                        "suffix": ""
                    },
                    {
                        "first": "Cho-Jui",
                        "middle": [],
                        "last": "Hsieh",
                        "suffix": ""
                    },
                    {
                        "first": "Inderjit",
                        "middle": [
                            "S"
                        ],
                        "last": "Dhillon",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Journal of Machine Learning Research",
                "volume": "18",
                "issue": "20",
                "pages": "1--32",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Si Si, Cho-Jui Hsieh, and Inderjit S. Dhillon. 2017. Memory efficient kernel approximation. Journal of Machine Learning Research, 18(20):1-32.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "On NMT search errors and model errors: Cat got your tongue?",
                "authors": [
                    {
                        "first": "Felix",
                        "middle": [],
                        "last": "Stahlberg",
                        "suffix": ""
                    },
                    {
                        "first": "Bill",
                        "middle": [],
                        "last": "Byrne",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "volume": "",
                "issue": "",
                "pages": "3356--3362",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/D19-1331"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Felix Stahlberg and Bill Byrne. 2019. On NMT search errors and model errors: Cat got your tongue? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan- guage Processing (EMNLP-IJCNLP), pages 3356- 3362. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "Cluster-based beam search for pointer-generator chatbot grounded by knowledge",
                "authors": [
                    {
                        "first": "Yik-Cheung",
                        "middle": [],
                        "last": "Tam",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Computer Speech and Language",
                "volume": "64",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.1016/j.csl.2020.101094"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yik-Cheung Tam. 2020. Cluster-based beam search for pointer-generator chatbot grounded by knowledge. Computer Speech and Language, 64:101094.",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "Diverse beam search for improved description of complex scenes",
                "authors": [
                    {
                        "first": "Ashwin",
                        "middle": [],
                        "last": "Vijayakumar",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Cogswell",
                        "suffix": ""
                    },
                    {
                        "first": "Ramprasaath",
                        "middle": [],
                        "last": "Selvaraju",
                        "suffix": ""
                    },
                    {
                        "first": "Qing",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Stefan",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Crandall",
                        "suffix": ""
                    },
                    {
                        "first": "Dhruv",
                        "middle": [],
                        "last": "Batra",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "AAAI Conference on Artificial Intelligence",
                "volume": "",
                "issue": "",
                "pages": "7371--7379",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ashwin Vijayakumar, Michael Cogswell, Ramprasaath Selvaraju, Qing Sun, Stefan Lee, David Crandall, and Dhruv Batra. 2018. Diverse beam search for improved description of complex scenes. In AAAI Conference on Artificial Intelligence, pages 7371- 7379.",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "Towards diverse and accurate image captions via reinforcing determinantal point process",
                "authors": [
                    {
                        "first": "Qingzhong",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Antoni",
                        "middle": [
                            "B"
                        ],
                        "last": "Chan",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Qingzhong Wang and Antoni B. Chan. 2019. To- wards diverse and accurate image captions via reinforcing determinantal point process. CoRR, abs/1908.04919.",
                "links": null
            },
            "BIBREF34": {
                "ref_id": "b34",
                "title": "Google's neural machine translation system: Bridging the gap between human and machine translation",
                "authors": [
                    {
                        "first": "Yonghui",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Mike",
                        "middle": [],
                        "last": "Schuster",
                        "suffix": ""
                    },
                    {
                        "first": "Zhifeng",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Quoc",
                        "middle": [
                            "V"
                        ],
                        "last": "Le",
                        "suffix": ""
                    },
                    {
                        "first": "Mohammad",
                        "middle": [],
                        "last": "Norouzi",
                        "suffix": ""
                    },
                    {
                        "first": "Wolfgang",
                        "middle": [],
                        "last": "Macherey",
                        "suffix": ""
                    },
                    {
                        "first": "Maxim",
                        "middle": [],
                        "last": "Krikun",
                        "suffix": ""
                    },
                    {
                        "first": "Yuan",
                        "middle": [],
                        "last": "Cao",
                        "suffix": ""
                    },
                    {
                        "first": "Qin",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Klaus",
                        "middle": [],
                        "last": "Macherey",
                        "suffix": ""
                    },
                    {
                        "first": "Jeff",
                        "middle": [],
                        "last": "Klingner",
                        "suffix": ""
                    },
                    {
                        "first": "Apurva",
                        "middle": [],
                        "last": "Shah",
                        "suffix": ""
                    },
                    {
                        "first": "Melvin",
                        "middle": [],
                        "last": "Johnson",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaobing",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Lukasz",
                        "middle": [],
                        "last": "Kaiser",
                        "suffix": ""
                    },
                    {
                        "first": "Stephan",
                        "middle": [],
                        "last": "Gouws",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshikiyo",
                        "middle": [],
                        "last": "Kato",
                        "suffix": ""
                    },
                    {
                        "first": "Taku",
                        "middle": [],
                        "last": "Kudo",
                        "suffix": ""
                    },
                    {
                        "first": "Hideto",
                        "middle": [],
                        "last": "Kazawa",
                        "suffix": ""
                    },
                    {
                        "first": "Keith",
                        "middle": [],
                        "last": "Stevens",
                        "suffix": ""
                    },
                    {
                        "first": "George",
                        "middle": [],
                        "last": "Kurian",
                        "suffix": ""
                    },
                    {
                        "first": "Nishant",
                        "middle": [],
                        "last": "Patil",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Cliff",
                        "middle": [],
                        "last": "Young",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Smith",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Riesa",
                        "suffix": ""
                    },
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Rudnick",
                        "suffix": ""
                    },
                    {
                        "first": "Oriol",
                        "middle": [],
                        "last": "Vinyals",
                        "suffix": ""
                    },
                    {
                        "first": "Gregory",
                        "middle": [
                            "S"
                        ],
                        "last": "Corrado",
                        "suffix": ""
                    },
                    {
                        "first": "Macduff",
                        "middle": [],
                        "last": "Hughes",
                        "suffix": ""
                    },
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Dean",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Gregory S. Corrado, Macduff Hughes, and Jeffrey Dean. 2016. Google's neural machine translation system: Bridg- ing the gap between human and machine translation.",
                "links": null
            },
            "BIBREF35": {
                "ref_id": "b35",
                "title": "XLNet: Generalized autoregressive pretraining for language understanding",
                "authors": [
                    {
                        "first": "Zhilin",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Zihang",
                        "middle": [],
                        "last": "Dai",
                        "suffix": ""
                    },
                    {
                        "first": "Yiming",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Jaime",
                        "middle": [],
                        "last": "Carbonell",
                        "suffix": ""
                    },
                    {
                        "first": "Rusland",
                        "middle": [],
                        "last": "Salakhutdinov",
                        "suffix": ""
                    },
                    {
                        "first": "Quoc V",
                        "middle": [],
                        "last": "Le",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "32",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car- bonell, Rusland Salakhutdinov, and Quoc V Le. 2019. XLNet: Generalized autoregressive pretrain- ing for language understanding. In Advances in Neural Information Processing Systems, volume 32.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "exp log p(y | y <t , x)/T y \u2208V exp log p(y | y <t , x)/T where larger T may lead to more diverse sets simply due to additional smoothing. Diverse Beam Search. Vijayakumar et al. (2018) propose a modification to the standard beam search algorithm-which they term diverse beam search (DBS)-to alleviate lack of diversity.",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Figure 1: Averaged n-gram diversity vs. minimum, median, and maximum BLEU score for beam sizes k = 5, 10, 20 on WMT'14 En-Fr and WMT'19 De-En newstest using various decoding strategies. The free parameter for each strategy is either the softmax temperature or the weight of the diversity parameter (see \u00a75.2).",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "BLEU and n-gram coverage d n averaged across n \u2208 {1, 2, 3, 4} in the decoded sets, where we define d n as d n = #of unique n-grams in k strings #of n-grams in k strings (14) While d n has a more natural interpretation as coverage of different n-grams, the above quantity is often referred to as n-gram diversity in the literature and so we transition to this term for consistency. Following the experimental setup of Kool et al. (2019), we vary sampling temperature T \u2208 {0.1, 0.2, . . . , 0.8} in the case of beam search and stochastic beam search and diversity weight w \u2208 {0.1, 0.2, . . . , 0.8} in the case of diverse beam search. For DetBS, we observe that larger sets require a smaller diversity penalty to achieve good n-gram diversity: in Fig. 1 we show results for DetBS with the string subsequence kernel for w \u2208 {0.01, 0.02, \u2022 \u2022 \u2022 , 0.1, 0.2, 0.3, 0.4} for k = 5, w \u2208 {0.01, 0.02, \u2022 \u2022 \u2022 , 0.15] for k = 10, and w \u2208 {0.01, 0.02, \u2022 \u2022 \u2022 , 0.05} for k = 20. 14 To observe how BLEU is affected by larger diversity coefficients under DetBS, we explore a finer grain of weights for DetBS in App. C.",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF3": {
                "num": null,
                "text": "Figure 2: n-gram diversity vs. minimum, median and maximum BLEU score for beam sizes k = 5, 10, 20 on WMT'19 De-En newstest using a larger range of the diversity weight w.",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "FIGREF4": {
                "num": null,
                "text": "Figure 3: n-gram diversity vs. minimum, median and maximum BLEU score for beam sizes k = 5, 10, 20 on WMT'14 En-Fr newstest using a larger range of the diversity weight w.",
                "uris": null,
                "fig_num": "3",
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table><tr><td>1 and y \u2208 V} (6)</td></tr><tr><td>where \u2022 is used to indicate string concatenations.</td></tr><tr><td>Note that candidates in Y t-1 already ending</td></tr><tr><td>in EOS are simply added directly to B t , i.e.,</td></tr><tr><td>EOS \u2022</td></tr></table>",
                "type_str": "table",
                "text": "EOS = EOS. Under this definition, we have the cardinality constraint |B t | \u2264 | V| \u2022 k.",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Best performing configurations found in search over string subsequence kernel parameters.Interestingly, larger values of n did not improve performance, and were more computationally expensive; small values of n and decay \u03bb appear to offer the best BLEU vs. n-gram diversity trade-off.Dataset and Model StatisticsWe use a convolutional sequence-to-sequence model trained according toGehring et al. (2017) on the WMT'14 En-Fr dataset.16 Data preprocessing steps, model hyperparameters and baseline performances can be found in their work. We use the pre-trained model checkpoints made available by fairseq at",
                "html": null,
                "num": null
            }
        }
    }
}