{
    "paper_id": "E09-1056",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:06:39.385196Z"
    },
    "title": "Improvements in Analogical Learning: Application to Translating multi-Terms of the Medical Domain",
    "authors": [
        {
            "first": "Philippe",
            "middle": [],
            "last": "Langlais",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "DIRO Univ. of Montreal",
                "location": {
                    "country": "Canada"
                }
            },
            "email": ""
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Handling terminology is an important matter in a translation workflow. However, current Machine Translation (MT) systems do not yet propose anything proactive upon tools which assist in managing terminological databases. In this work, we investigate several enhancements to analogical learning and test our implementation on translating medical terms. We show that the analogical engine works equally well when translating from and into a morphologically rich language, or when dealing with language pairs written in different scripts. Combining it with a phrasebased statistical engine leads to significant improvements.",
    "pdf_parse": {
        "paper_id": "E09-1056",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Handling terminology is an important matter in a translation workflow. However, current Machine Translation (MT) systems do not yet propose anything proactive upon tools which assist in managing terminological databases. In this work, we investigate several enhancements to analogical learning and test our implementation on translating medical terms. We show that the analogical engine works equally well when translating from and into a morphologically rich language, or when dealing with language pairs written in different scripts. Combining it with a phrasebased statistical engine leads to significant improvements.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "If machine translation is to meet commercial needs, it must offer a sensible approach to translating terms. Currently, MT systems offer at best database management tools which allow a human (typically a translator, a terminologist or even the vendor of the system) to specify bilingual terminological entries. More advanced tools are meant to identify inconsistencies in terminological translations and might prove useful in controlledlanguage situations (Itagaki et al., 2007) .",
                "cite_spans": [
                    {
                        "start": 455,
                        "end": 477,
                        "text": "(Itagaki et al., 2007)",
                        "ref_id": "BIBREF5"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "One approach to translate terms consists in using a domain-specific parallel corpus with standard alignment techniques (Brown et al., 1993) to mine new translations. Massive amounts of parallel data are certainly available in several pairs of languages for domains such as parliament debates or the like. However, having at our disposal a domain-specific (e.g. computer science) bitext with an adequate coverage is another issue. One might argue that domain-specific comparable (or perhaps unrelated) corpora are easier to acquire, in which case context-vector techniques (Rapp, 1995; Fung and McKeown, 1997 ) can be used to identify the translation of terms. We certainly agree with that point of view to a certain extent, but as discussed by Morin et al. (2007) , for many specific domains and pairs of languages, such resources simply do not exist. Furthermore, the task of translation identification is more difficult and error-prone.",
                "cite_spans": [
                    {
                        "start": 119,
                        "end": 139,
                        "text": "(Brown et al., 1993)",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 572,
                        "end": 584,
                        "text": "(Rapp, 1995;",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 585,
                        "end": 607,
                        "text": "Fung and McKeown, 1997",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 744,
                        "end": 763,
                        "text": "Morin et al. (2007)",
                        "ref_id": "BIBREF13"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Analogical learning has recently regained some interest in the NLP community. Lepage and Denoual (2005) proposed a machine translation system entirely based on the concept of formal analogy, that is, analogy on forms. Stroppa and Yvon (2005) applied analogical learning to several morphological tasks also involving analogies on words. Langlais and Patry (2007) applied it to the task of translating unknown words in several European languages, an idea investigated as well by Denoual (2007) for a Japanese to English translation task.",
                "cite_spans": [
                    {
                        "start": 78,
                        "end": 103,
                        "text": "Lepage and Denoual (2005)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 218,
                        "end": 241,
                        "text": "Stroppa and Yvon (2005)",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 336,
                        "end": 361,
                        "text": "Langlais and Patry (2007)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 477,
                        "end": 491,
                        "text": "Denoual (2007)",
                        "ref_id": "BIBREF2"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this study, we improve the state-of-the-art of analogical learning by (i) proposing a simple yet effective implementation of an analogical solver; (ii) proposing an efficient solution to the search issue embedded in analogical learning, (iii) investigating whether a classifier can be trained to recognize bad candidates produced by analogical learning. We evaluate our analogical engine on the task of translating terms of the medical domain; a domain well-known for its tendency to create new words, many of which being complex lexical constructions. Our experiments involve five language pairs, including languages with very different morphological systems.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In the remainder of this paper, we first present in Section 2 the principle of analogical learning. Practical issues in analogical learning are discussed in Section 3 along with our solutions. In Section 4, we report on experiments we conducted with our analogical device. We conclude this study and discuss future work in Section 5.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "A proportional analogy, or analogy for short, is a relation between four items noted [x : y = z : t] which reads as \"x is to y as z is to t\". Among proportional analogies, we distinguish formal analogies, that is, those we can identify at a graphemic level, such as [adrenergic beta-agonists, adrenergic beta-antagonists, adrenergic alpha-agonists, adrenergic alpha-antagonists].",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Definitions",
                "sec_num": "2.1"
            },
            {
                "text": "Formal analogies can be defined in terms of factorizations1 . Let x be a string over an alphabet \u03a3,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Definitions",
                "sec_num": "2.1"
            },
            {
                "text": "a factorization of x, noted f x , is a se- quence of n factors f x = (f 1 x , . . . , f n x ), such that x = f 1 x f 2 x . . . f n",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Definitions",
                "sec_num": "2.1"
            },
            {
                "text": "x , where denotes the concatenation operator. After (Stroppa and Yvon, 2005) we thus define a formal analogy as:",
                "cite_spans": [
                    {
                        "start": 52,
                        "end": 76,
                        "text": "(Stroppa and Yvon, 2005)",
                        "ref_id": "BIBREF15"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Definitions",
                "sec_num": "2.1"
            },
            {
                "text": "Definition 1 \u2200(x, y, z, t) \u2208 \u03a3 4 , [x : y = z : t] iff there exist factorizations (f x , f y , f z , f t ) \u2208 (\u03a3 d ) 4 of (x, y, z, t) such that, \u2200i \u2208 [1, d], (f i y , f i z ) \u2208 (f i x , f i t ), (f i t , f i x ) .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Definitions",
                "sec_num": "2.1"
            },
            {
                "text": "The smallest d for which this definition holds is called the degree of the analogy.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Definitions",
                "sec_num": "2.1"
            },
            {
                "text": "Intuitively, this definition states that (x, y, z, t) are made up of a common set of alternating substrings. It is routine to check that it captures the exemplar analogy introduced above, based on the following set of factorizations:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Definitions",
                "sec_num": "2.1"
            },
            {
                "text": "f x \u2261 (adrenergic bet, a-agonists) f y \u2261 (adrenergic bet, a-antagonists) f z \u2261 (adrenergic alph, a-agonists) f t \u2261 (adrenergic alph, a-antagonists)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Definitions",
                "sec_num": "2.1"
            },
            {
                "text": "As no smaller factorization can be found, the degree of this analogy is 2. In the sequel, we call an analogical equation an analogy where one item (usually the fourth) is missing and we note it [x : y = z : ? ].",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Definitions",
                "sec_num": "2.1"
            },
            {
                "text": "Let L = {(i, o) | i \u2208 I, o \u2208 O} be a learning set of observations, where I (O) is the set of possible forms of the input (output) linguistic system under study. We denote I(u) (O(u)) the projection of u into the input (output) space; that is, if u = (i, o), then I(u) \u2261 i and O(u) \u2261 o. For an incomplete observation u = (i, ?), the inference procedure is: In the sequel, we distinguish the generator which implements the first two steps, from the selector which implements step 3.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Analogical Inference",
                "sec_num": "2.2"
            },
            {
                "text": "1. building E I (u) = {(x, y, z) \u2208 L 3 | [I(x) : I(y) = I(z) : I(u) ]},",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Analogical Inference",
                "sec_num": "2.2"
            },
            {
                "text": "To give an example, assume L contains the following entries: (beeta-agonistit, adrenergic beta-agonists), (beetasalpaajat, adrenergic beta-antagonists) and (alfa-agonistit, adrenergic alpha-agonists). We might translate the Finnish term alfasalpaajat into the English term adrenergic alpha-antagonists by 1) identifying the input triplet: (beeta-agonistit, beetasalpaajat, alfa-agonistit) ; 2) projecting it into the equation [adrenergic beta-agonists : adrenergic betaantagonists = adrenergic alpha-agonists : ? ]; and solving it: adrenergic alpha-antagonists is one of its solutions.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Analogical Inference",
                "sec_num": "2.2"
            },
            {
                "text": "During inference, analogies are recognized independently in the input and the output space, and nothing pre-establishes which subpart of one input form corresponds to which subpart of the output one. This \"knowledge\" is passively captured thanks to the inductive bias of the learning strategy (an analogy in the input space corresponds to one in the output space). Also worth mentioning, this procedure does not rely on any pre-defined notion of word. This might come at an advantage for languages that are hard to segment (Lepage and Lardilleux, 2007) .",
                "cite_spans": [
                    {
                        "start": 523,
                        "end": 552,
                        "text": "(Lepage and Lardilleux, 2007)",
                        "ref_id": "BIBREF10"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Analogical Inference",
                "sec_num": "2.2"
            },
            {
                "text": "Each step of analogical learning, that is, searching for input triplets, solving output equations and selecting good candidates involves some practical issues. Since searching for input triplets might involve the need for solving (input) equations, we discuss the solver first.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Practical issues",
                "sec_num": "3"
            },
            {
                "text": "3.1 The solver Lepage (1998) proposed an algorithm for solving an analogical equation [x : y = z : ? ]. An alignment between x and y and between x and z is first computed (by edit-distance) as illustrated in Figure 1 . Then, the three strings are synchronized using x as a backbone of the synchronization. The algorithm can be seen as a deterministic finite-state machine where a state is defined by the two edit-operations being visited in the two tables. This is schematized by the two cursors in the figure. Two actions are allowed: copy one symbol from y or z into the solution and move one or both cursors.",
                "cite_spans": [
                    {
                        "start": 15,
                        "end": 28,
                        "text": "Lepage (1998)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 215,
                        "end": 216,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Practical issues",
                "sec_num": "3"
            },
            {
                "text": "x: r e a d e r",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Practical issues",
                "sec_num": "3"
            },
            {
                "text": "x: r e a d e r y: r e a d a b l e z:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Practical issues",
                "sec_num": "3"
            },
            {
                "text": "d o e r",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Practical issues",
                "sec_num": "3"
            },
            {
                "text": "Figure 1 : Illustration of the synchronization done by the solver described in (Lepage, 1998) .",
                "cite_spans": [
                    {
                        "start": 79,
                        "end": 93,
                        "text": "(Lepage, 1998)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Practical issues",
                "sec_num": "3"
            },
            {
                "text": "There are two things to realize with this algorithm. First, since several (minimal-cost) alignments can be found between two strings, several synchronizations are typically carried out while solving an equation, leading to (possibly many) different solutions. Indeed, in adverse situations, an exponential number of synchronizations will have to be computed. Second, the algorithm fails to deliver an expected form in a rather frequent situation where two identical symbols align fortuitously in two strings. This is for instance the case in our running example where the symbol d in doer aligns to the one in reader, which puzzles the synchronization. Indeed, dabloe is the only form proposed to [reader : readable = doer : ? ], while the expected one is doable. The algorithm would have no problem, however, to produce the form writable out of the equation [reader : readable = writer : ? ]. Yvon et al. (2004) proposed an analogical solver which is not exposed to the latter problem. It consists in building a finite state transducer which generates the solutions to [x : y = z : ? ] while recognizing the form x.",
                "cite_spans": [
                    {
                        "start": 894,
                        "end": 912,
                        "text": "Yvon et al. (2004)",
                        "ref_id": "BIBREF18"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Practical issues",
                "sec_num": "3"
            },
            {
                "text": "Theorem 1 t is a solution to [x : y = z : ?] iff t belongs to {y \u2022 z}\\x.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Practical issues",
                "sec_num": "3"
            },
            {
                "text": "shuffle and complement are two rational operations. The shuffle of two strings w and v, noted w \u2022 v, is the regular language containing the strings obtained by selecting (without replacement) alternatively in w and v, sequences of characters in a left-to-right manner. For instance, spondyondontilalgiatis and ondspondonylaltitisgia are two strings belonging to spondylalgia \u2022 ondontitis). The complementary set of w with respect to v, noted w\\v, is the set of strings formed by removing from w, in a left-to-right manner, the symbols in v. For instance, spondylitis and spydoniltis are belonging to spondyondontilalgiatis \\ ondontalgia. Our implementation of the two rational operations are sketched in Algorithm 1.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Practical issues",
                "sec_num": "3"
            },
            {
                "text": "Because the shuffle of two strings may contain an exponential number of elements with respect to the length of those strings, building such an automaton may face combinatorial problems. Our solution simply consists in randomly sampling strings in the shuffle set. Our solver, depicted in Algorithm 2, is thus controlled by a sampling size s, the impact of which is illustrated in Table 1. By increasing s, the solver generates more (mostly spurious) solutions, but also increases the relative frequency with which the expected output is generated. In practice, provided a large enough sampling size,2 the expected form very often appears among the most frequent ones.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Practical issues",
                "sec_num": "3"
            },
            {
                "text": "s nb (solution,frequency) 10 11 (doable,7) (dabloe,3) (adbloe,3) 10 2 22 (doable,28) (dabloe,21) (abldoe,21) 10 3 29 (doable,333) (dabloe,196) (abldoe,164) ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Practical issues",
                "sec_num": "3"
            },
            {
                "text": "A brute-force approach to identifying the input triplets that define an analogy with the incomplete observation u = (t, ?) consists in enumerating triplets in the input space and checking for an ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Searching for input triplets",
                "sec_num": "3.2"
            },
            {
                "text": "Input: m \u2208 y \u2022 z, x Output: the set m \\ x if (m = ) then if (x = ) then s \u2190 s \u222a r else complementary(m[2:],x,r.m[1],s) if m[1] = x[1] then complementary(m[2:],",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Searching for input triplets",
                "sec_num": "3.2"
            },
            {
                "text": "x[2:],r,s) Algorithm 1: Simulation of the two rational operations required by the solver. x[a:b] denotes the sequence of symbols x starting from index a to index b inclusive. x[a:] denotes the suffix of x starting at index a.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Searching for input triplets",
                "sec_num": "3.2"
            },
            {
                "text": "analogical relation with t. This amounts to check o(|I| 3 ) analogies, which is manageable for toy problems only. Instead, Langlais and Patry (2007) proposed to solve analogical equations [y : x = t : ? ] for some pairs x, y belonging to the neighborhood 3 of I(u), denoted N (t). Those solutions that belong to the input space are the z-forms retained;",
                "cite_spans": [
                    {
                        "start": 123,
                        "end": 148,
                        "text": "Langlais and Patry (2007)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Searching for input triplets",
                "sec_num": "3.2"
            },
            {
                "text": "E I (u) = { x, y, z : x \u2208 N (t) , y \u2208 N (x), z \u2208 [y : x = t : ? ] \u2229 I }",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Searching for input triplets",
                "sec_num": "3.2"
            },
            {
                "text": "This strategy (hereafter named LP) directly follows from a symmetrical property of an analogy ([x : y = z : t] \u21d4 [y : x = t : z]), and reduces the search procedure to the resolution of a number of analogical equations which is quadratic with the number of pairs x, y sampled.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Searching for input triplets",
                "sec_num": "3.2"
            },
            {
                "text": "We found this strategy to be of little use for input spaces larger than a few tens of thousands forms. To solve this problem, we exploit a property on symbol counts that an analogical relation must fulfill (Lepage, 1998) : where A is the alphabet on which the forms are built, and |x| c stands for the number of occurrences of symbol c in x.",
                "cite_spans": [
                    {
                        "start": 206,
                        "end": 220,
                        "text": "(Lepage, 1998)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Searching for input triplets",
                "sec_num": "3.2"
            },
            {
                "text": "[x : y = z : t] \u21d2 |x| c + |t| c = |y| c + |z| c \u2200c \u2208 A",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Searching for input triplets",
                "sec_num": "3.2"
            },
            {
                "text": "Our search strategy (named TC) begins by selecting an x-form in the input space. This enforces a set of necessary constraints on the counts of characters that any two forms y and z must satisfy for [x : y = z : t] to be true. By considering all forms x in turn,4 we collect a set of candidate triplets for t. A verification of those that define with t an analogy must then be carried out. Formally, we built:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Searching for input triplets",
                "sec_num": "3.2"
            },
            {
                "text": "E I (u) = { x, y, z : x \u2208 I, y, z \u2208 C( x, t ), [x : y = z : t] }",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Searching for input triplets",
                "sec_num": "3.2"
            },
            {
                "text": "where C( x, t ) denotes the set of pairs y, z which satisfy the count property. This strategy will only work if (i) the number of quadruplets to check is much smaller than the number of triplets we can form in the input space (which happens to be the case in practice), and if (ii) we can efficiently identify the pairs y, z that satisfy a set of constraints on character counts. To this end, we proposed in (Langlais and Yvon, 2008) to organize the input space into a data structure which supports efficient runtime retrieval.",
                "cite_spans": [
                    {
                        "start": 408,
                        "end": 433,
                        "text": "(Langlais and Yvon, 2008)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Searching for input triplets",
                "sec_num": "3.2"
            },
            {
                "text": "Step 3 of analogical learning consists in selecting one or several solutions from the set of candidate forms produced by the generator. We trained in a supervised manner a binary classifier to distinguish good translation candidates (as defined by a reference) from spurious ones. We applied to this end the voted-perceptron algorithm described by Freund and Schapire (1999) . Online votedperceptrons have been reported to work well in a number of NLP tasks (Collins, 2002; Liang et al., 2006) . Training such a classifier is mainly a matter of feature engineering. An example e is a pair of source-target analogical relations (r, r) identified by the generator, and which elects t as a translation for the term t:",
                "cite_spans": [
                    {
                        "start": 348,
                        "end": 374,
                        "text": "Freund and Schapire (1999)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 458,
                        "end": 473,
                        "text": "(Collins, 2002;",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 474,
                        "end": 493,
                        "text": "Liang et al., 2006)",
                        "ref_id": "BIBREF12"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The selector",
                "sec_num": "3.3"
            },
            {
                "text": "e \u2261 (r, r) \u2261 ([x : y = z : t], [x : \u0177 = \u1e91 : t])",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The selector",
                "sec_num": "3.3"
            },
            {
                "text": "where x, \u0177, and \u1e91 are respectively the projections of the source terms x, y and z. We investigated many features including (i) the degree of r and r, (ii) the frequency with which a form is generated, 5 (iii) length ratios between t and t, (iv) likelihoods scores (min, max, avg.) computed by a characterbased n-gram model trained on a large general corpus (without overlap to DEV or TRAIN), etc.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The selector",
                "sec_num": "3.3"
            },
            {
                "text": "We compared the two aforementioned searching strategies on a task of identifying triplets in an input space of French words for 1 000 randomly selected test words. We considered input spaces of various sizes. The results are reported in Table 2. TC clearly outperforms LP by systematically identifying more triplets in much less time. For the largest input space of 84 000 forms, TC could identify an average of 746 triplets for 946 test words in 1.2 seconds, while the best compromise we could settle with LP allows the identification of 56 triplets on average for 889 words in 6.3 seconds on average. Note that in this experiment, LP was calibrated for each input space so that the best compromise between recall (%s) and speed could be found. Reducing the size of the neighborhood in LP improves computation time, but significantly affects recall. In the following, we only consider the TC search strategy.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Calibrating the engine",
                "sec_num": "4.1"
            },
            {
                "text": "Datasets The data we used in this study comes from the Medical Subject Headings (MeSH) thesaurus. This thesaurus is used by the US National Library of Medicine to index the biomedical sci- We considered five language pairs with three relatively close European languages (English-French, English-Spanish and English-Swedish), a more distant one (English-Finnish) and one pair involving different scripts (English-Russian). 7The material was split in three randomly selected parts, so that the development and test material contain exactly 1 000 terms each. The characteristics of this material are reported in Table 3 . For the Finnish-English and Swedish-English language pairs, the ratio of uni-terms in the Foreign language (u f %) is twice the ratio of uni-terms in the English counterpart. This is simply due to the agglutinative nature of these two languages. For instance, according to MeSH, the English multi-term speech articulation tests corresponds to the Finnish uni-term \u00e4\u00e4nt\u00e4miskokeet and to the Swedish one artikulationstester. The ratio of outof-vocabulary forms (space-separated words unseen in TRAIN) in the TEST material is rather high: between 36% and 68% for all Foreignto-English translation directions, but Finnish-to-English, where surprisingly, only 6% of the word forms are unknown.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 615,
                        "end": 616,
                        "text": "3",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Experimental Protocol",
                "sec_num": "4.2"
            },
            {
                "text": "Evaluation metrics For each experimental condition, we compute the following measures: Coverage the fraction of input words for which the system can generate translations. If N t words receive translations among N , coverage is N t /N . SW 17 090 67.9 32.5 1 000 67.4 67.9 68.4",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Protocol",
                "sec_num": "4.2"
            },
            {
                "text": "Table 3 : Main characteristics of our datasets. nb indicates the number of pairs of terms in a bitext, u f % (u e %) stands for the percentage of uniterms in the Foreign (English) part. oov% indicates the percentage of out-of-vocabulary forms (space-separated forms of TEST unseen in TRAIN).",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "3",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Experimental Protocol",
                "sec_num": "4.2"
            },
            {
                "text": "Precision among the N t words for which the system proposes an answer, precision is the proportion of those for which a correct translation is output. Depending on the number of output translations k that one is willing to examine, a correct translation will be output for N k input words. Precision at rank k is thus defined as",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Protocol",
                "sec_num": "4.2"
            },
            {
                "text": "P k = N k /N t .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Protocol",
                "sec_num": "4.2"
            },
            {
                "text": "Recall is the proportion of the N input words for which a correct translation is output. Recall at rank k is defined as",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Protocol",
                "sec_num": "4.2"
            },
            {
                "text": "R k = N k /N .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Protocol",
                "sec_num": "4.2"
            },
            {
                "text": "In all our experiments, candidate translations are sorted in decreasing order of frequency with which they were generated.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Protocol",
                "sec_num": "4.2"
            },
            {
                "text": "The performances of the generator on the 10 translation sessions are reported in Table 4 . The coverage of the generator varies between 38.5% (French-to-English) and 47.1% (Englishto-Finnish), which is rather low. In most cases, the silence of the generator is due to a failure to identify analogies in the input space (step 1). The last column of Table 4 reports the maximum recall we can obtain if we consider all the candidates output by the generator. The relative accuracy of the generator, expressed by the ratio of R \u221e to cov, ranges from 64.3% (English-French) to 79.1% (Spanishto-English), for an average value of 73.8% over all translation directions. This roughly means that one fourth of the test terms with at least one solution do not contain the reference.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 87,
                        "end": 88,
                        "text": "4",
                        "ref_id": "TABREF5"
                    },
                    {
                        "start": 354,
                        "end": 355,
                        "text": "4",
                        "ref_id": "TABREF5"
                    }
                ],
                "eq_spans": [],
                "section": "The generator",
                "sec_num": "4.3"
            },
            {
                "text": "Overall, we conclude that analogical learning offers comparable performances for all translation directions, although some fluctuations are observed. We do not observe that the approach is affected by language pairs which do not share the same script (Russian/English). The best (worse) case (as far as R \u221e is concerned) corresponds to translating into Spanish (French). Admittedly, the largest recall and R \u221e values reported in Table 4 are disappointing. Clearly, for analogical learning to work efficiently, enough linguistic phenomena must be attested in the TRAIN material. To illustrate this, we collected for the Spanish-English language pair a set of medical terms from the Medical Drug Regulatory Activities thesaurus (MedDRA) which contains roughly three times more terms than the Spanish-English material used in this study. This extra material allows to raise the coverage to 73.4% (Spanish to English) and 79.7% (English to Spanish), an absolute improvement of more than 30%.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 435,
                        "end": 436,
                        "text": "4",
                        "ref_id": "TABREF5"
                    }
                ],
                "eq_spans": [],
                "section": "The generator",
                "sec_num": "4.3"
            },
            {
                "text": "Cov P 1 R 1 P 100 R 100 R \u221e \u2192 FI",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The generator",
                "sec_num": "4.3"
            },
            {
                "text": "We trained our classifiers on the several millions of examples generated while translating the development material. Since we considered numerous feature representations in this study, this implies saving many huge datafiles on disk. In order to save some space, we decided to remove forms that were generated less than 3 times. 8 Each classifier was trained using 20 epochs.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The selector",
                "sec_num": "4.4"
            },
            {
                "text": "It is important to note that we face a very unbalanced task. For instance, for the English to Finnish task, the generator produces no less than 2.7 millions of examples, among which only 4 150 are positive ones. Clearly, classifying all the examples as negative will achieve a very high classification accuracy, but will be of no practical use. Therefore, we measure the ability of a classifier to iden- tify the few positive forms among the set of candidates. We measure precision as the percentage of forms selected by the classifier that are sanctioned by the reference lexicon, and recall as the percentage of forms selected by the classifier over the total number of sanctioned forms that the classifier could possibly select. (Recall that the generator often fails to produce oracle forms.)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The selector",
                "sec_num": "4.4"
            },
            {
                "text": "The performance measured on the TEST material of the best classifier we monitored on DEV are reported in Table 5 for the Foreign-to-English translation directions (we made consistent observations on the reverse directions). For comparison purposes, we implemented a baseline classifier (lines argmax-f1) which selects the mostfrequent candidate form. This is the selector used as a default in several studies on analogical learning (Lepage and Denoual, 2005; Stroppa and Yvon, 2005) . The baseline identifies between 56.7% to 65.6% of the sanctioned forms, at precision rates ranging from 41.3% to 49.2%. We observe for all translation directions that the best classifier we trained systematically outperforms this baseline, both in terms of precision and recall.",
                "cite_spans": [
                    {
                        "start": 432,
                        "end": 458,
                        "text": "(Lepage and Denoual, 2005;",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 459,
                        "end": 482,
                        "text": "Stroppa and Yvon, 2005)",
                        "ref_id": "BIBREF15"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 111,
                        "end": 112,
                        "text": "5",
                        "ref_id": "TABREF6"
                    }
                ],
                "eq_spans": [],
                "section": "The selector",
                "sec_num": "4.4"
            },
            {
                "text": "Table 6 shows the overall performance of the analogical translation device in terms of precision, recall and coverage rates as defined in Section 4.2. Overall, our best configuration (the one embedding the s-best classifier) translates between 19.3% and 22.5% of the test material, with a precision ranging from 50.4% to 63.2%. This is better than the variant which always proposes the most frequent generated form (argmax-f1). Allowing more answers increases both precision and recall. If we allow up to 10 candidates per source term, the analogical translator translates one fourth of the terms (26.1%) with a precision of 70.9%, averaged over all translation directions. The oracle variant, which looks at the reference for selecting the good candidates produced by the generator, gives an upper bound of the performance that could be obtained with our approach: less than a third of the source terms can be translated correctly. Recall however that increasing the TRAIN material leads to drastic improvements in coverage.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "6",
                        "ref_id": "TABREF7"
                    }
                ],
                "eq_spans": [],
                "section": "The overall system",
                "sec_num": "4.4.1"
            },
            {
                "text": "To put these figures in perspective, we measured the performance of a phrase-based statistical MT (PB-SMT) engine trained to handle the same translation task. We trained a phrase table on TRAIN, using the standard approach.9 However, because of the small training size, and the rather huge OOV rate of the translation tasks we address, we did not train translation models on word-tokens, but at the character level. Therefore a phrase is indeed a sequence of characters. This idea has been successively investigated in a Catalan-to-Spanish translation task by Vilar et al. (2007) . We tuned the 8 coefficients of the so-called log-linear combination maximized at decoding time on the first 200 pairs of terms of the DEV corpora. On the DEV set, BLEU scores10 range from 67.2 (English-to-Finnish) to 77.0 (Russian-to-English).",
                "cite_spans": [
                    {
                        "start": 560,
                        "end": 579,
                        "text": "Vilar et al. (2007)",
                        "ref_id": "BIBREF17"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Comparison with a PB-SMT engine",
                "sec_num": "4.5"
            },
            {
                "text": "Table 7 reports the precision and recall of both translation engines. Note that because the SMT engine always propose a translation, its precision equals its recall. First, we observe that the precision of the SMT engine is not high (between 17% and 31%), which demonstrates the difficulty of the task. The analogical device does better for all translation directions (see Table 6 ), but at a much lower recall, remaining silent more than half of the time. This suggests that combining both systems could be advantageous. To verify this, we ran a straightforward combination: whenever the analogical device produces a translation, we pick it; otherwise, the statistical output is considered. The gains of the resulting system over the SMT alone are reported in column \u2206B. Averaged over We noticed a tendency of the statistical engine to produce literal translations; a default the analogical device does not show. For instance, the Spanish term instituciones de atenci\u00f3n ambulatoria is translated word for word by Pharaoh into institutions, atention ambulatory while analogical learning produces ambulatory care facilities. We also noticed that analogical learning sometimes produces wrong translations based on morphological regularities that are applied blindly. This is, for instance, the case in a Russian/English example where mouthal manifestations is produced, instead of oral manifestations.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "7",
                        "ref_id": "TABREF8"
                    },
                    {
                        "start": 379,
                        "end": 380,
                        "text": "6",
                        "ref_id": "TABREF7"
                    }
                ],
                "eq_spans": [],
                "section": "Comparison with a PB-SMT engine",
                "sec_num": "4.5"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "FI\u2192EN FR\u2192EN RU\u2192EN SP\u2192EN SW\u2192EN k P k R k P k R k P k R k P k R k P k R k argmax-f",
                        "eq_num": "1"
                    }
                ],
                "section": "Comparison with a PB-SMT engine",
                "sec_num": "4.5"
            },
            {
                "text": "In this study, we proposed solutions to practical issues involved in analogical learning. A simple yet effective implementation of a solver is described. A search strategy is proposed which outperforms the one described in (Langlais and Patry, 2007) . Also, we showed that a classifier trained to select good candidate translations outperforms the most-frequently-generated heuristic used in several works on analogical learning.",
                "cite_spans": [
                    {
                        "start": 223,
                        "end": 249,
                        "text": "(Langlais and Patry, 2007)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discussion and future work",
                "sec_num": "5"
            },
            {
                "text": "Our analogical device was used to translate medical terms in different language pairs. The approach rates comparably across the 10 translation directions we considered. In particular, we do not see a drop in performance when translating into a morphology rich language (such as Finnish), or when translating into languages with different scripts. Averaged over all translation directions, the best variant could translate in first position 21% of the terms with a precision of 57%, while at best, one could translate 30% of the terms with a perfect precision. We show that the analogical translations are of better quality than those produced by a phrase-based engine trained at the character level, albeit with much lower recall. A straightforward combination of both approaches led an improvement of 5.3 BLEU points over the SMT alone. Better SMT performance could be obtained with a system based on morphemes, see for instance (Toutanova et al., 2008) . However, since lists of morphemes specific to the medical domain do not exist for all the languages pairs we considered here, unsupervised methods for acquiring morphemes would be necessary, which is left as a future work. In any case, this comparison is meaningful, since both the SMT and the analogical device work at the character level. This work opens up several avenues. First, we will test our approach on terminologies from different domains, varying the size of the training material. Second, analyzing the segmentation induced by analogical learning would be interesting. Third, we need to address the problem of combining the translations produced by analogy into a front-end statistical translation engine. Last, there is no reason to constrain ourselves to translating terminology only. We targeted this task in the first place, because terminology typically plugs translation systems, but we think that analogical learning could be useful for translating infrequent entities.",
                "cite_spans": [
                    {
                        "start": 930,
                        "end": 954,
                        "text": "(Toutanova et al., 2008)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discussion and future work",
                "sec_num": "5"
            },
            {
                "text": "Factorizations of strings correspond to segmentations. We keep the former term, to emphasize the genericity of the definition, which remains valid for other algebraic structures, for which factorization and segmentation are no longer synomymous.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "We used s = 2 000 in this study.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Anagram forms do not have to be considered separately.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "The MeSH thesaurus and its translations are included in the UMLS Metathesaurus.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Russian MeSH is normally written in Cyrillic, but some terms are simply English terms written in uppercase Latin script (e.g., ACHROMOBACTER for English Achromobacter). We removed those terms.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Averaged over all translation directions, this incurs an absolute reduction of the coverage of 3.4%.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "We used the scripts distributed by Philipp Koehn to train the phrase-table, and Pharaoh(Koehn, 2004) for producing the translations.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "We computed BLEU scores at the character level.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "The mathematics of statistical machine translation: Parameter estimation",
                "authors": [
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Brown",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [
                            "A"
                        ],
                        "last": "Della Pietra",
                        "suffix": ""
                    },
                    {
                        "first": "V",
                        "middle": [
                            "J"
                        ],
                        "last": "Della Pietra",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [
                            "L"
                        ],
                        "last": "Mercer",
                        "suffix": ""
                    }
                ],
                "year": 1993,
                "venue": "Computational Linguistics",
                "volume": "19",
                "issue": "2",
                "pages": "263--311",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Compu- tational Linguistics, 19(2):263-311.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms",
                "authors": [
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Collins",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "EMNLP",
                "volume": "",
                "issue": "",
                "pages": "1--8",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "M. Collins. 2002. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms. In EMNLP, pages 1-8, Mor- ristown, NJ, USA.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Analogical translation of unknown words in a statistical machine translation framework",
                "authors": [
                    {
                        "first": "E",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "MT Summit, XI",
                "volume": "",
                "issue": "",
                "pages": "10--14",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "E. Denoual. 2007. Analogical translation of unknown words in a statistical machine translation framework. In MT Summit, XI, pages 10-14, Copenhagen.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Large margin classification using the perceptron algorithm",
                "authors": [
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Freund",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [
                            "E"
                        ],
                        "last": "Schapire",
                        "suffix": ""
                    }
                ],
                "year": 1999,
                "venue": "Mach. Learn",
                "volume": "37",
                "issue": "3",
                "pages": "277--296",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Y. Freund and R. E. Schapire. 1999. Large margin classification using the perceptron algorithm. Mach. Learn., 37(3):277-296.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Finding terminology translations from non-parallel corpora",
                "authors": [
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Fung",
                        "suffix": ""
                    },
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Mckeown",
                        "suffix": ""
                    }
                ],
                "year": 1997,
                "venue": "5th Annual Workshop on Very Large Corpora",
                "volume": "",
                "issue": "",
                "pages": "192--202",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "P. Fung and K. McKeown. 1997. Finding terminology translations from non-parallel corpora. In 5th An- nual Workshop on Very Large Corpora, pages 192- 202, Hong Kong.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Automatic validation of terminology translation consistency with statistical method",
                "authors": [
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Itagaki",
                        "suffix": ""
                    },
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Aikawa",
                        "suffix": ""
                    },
                    {
                        "first": "X",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "MT Summit XI",
                "volume": "",
                "issue": "",
                "pages": "269--274",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "M. Itagaki, T. Aikawa, and X. He. 2007. Auto- matic validation of terminology translation consis- tency with statistical method. In MT Summit XI, pages 269-274, Copenhagen, Denmark.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Pharaoh: A beam search decoder for phrase-based statistical machine translation models",
                "authors": [
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Koehn",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "AMTA",
                "volume": "",
                "issue": "",
                "pages": "115--124",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "P. Koehn. 2004. Pharaoh: A beam search decoder for phrase-based statistical machine translation models. In AMTA, pages 115-124, Washington, DC, USA.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Translating unknown words by analogical learning",
                "authors": [
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Langlais",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Patry",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "EMNLP-CoNLL",
                "volume": "",
                "issue": "",
                "pages": "877--886",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "P. Langlais and A. Patry. 2007. Translating unknown words by analogical learning. In EMNLP-CoNLL, pages 877-886, Prague, Czech Republic.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Scaling up analogical learning",
                "authors": [
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Langlais",
                        "suffix": ""
                    },
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Yvon",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "22nd International Conference on Computational Linguistics (COLING 2008)",
                "volume": "",
                "issue": "",
                "pages": "51--54",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "P. Langlais and F. Yvon. 2008. Scaling up analogi- cal learning. In 22nd International Conference on Computational Linguistics (COLING 2008), pages 51-54, Manchester, United Kingdom.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "ALEPH: an EBMT system based on the preservation of proportionnal analogies between sentences across languages",
                "authors": [
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Lepage",
                        "suffix": ""
                    },
                    {
                        "first": "E",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "International Workshop on Statistical Language Translation (IWSLT)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Y. Lepage and E. Denoual. 2005. ALEPH: an EBMT system based on the preservation of proportion- nal analogies between sentences across languages. In International Workshop on Statistical Language Translation (IWSLT), Pittsburgh, PA, October.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "The GREYC Machine Translation System for the IWSLT 2007 Evaluation Campaign",
                "authors": [
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Lepage",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Lardilleux",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "IWLST",
                "volume": "",
                "issue": "",
                "pages": "49--53",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Y. Lepage and A. Lardilleux. 2007. The GREYC Ma- chine Translation System for the IWSLT 2007 Eval- uation Campaign. In IWLST, pages 49-53, Trento, Italy.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Solving analogies on words: an algorithm",
                "authors": [
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Lepage",
                        "suffix": ""
                    }
                ],
                "year": 1998,
                "venue": "COLING-ACL",
                "volume": "",
                "issue": "",
                "pages": "728--734",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Y. Lepage. 1998. Solving analogies on words: an algo- rithm. In COLING-ACL, pages 728-734, Montreal, Canada.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "An end-to-end discriminative approach to machine translation",
                "authors": [
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Liang",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Bouchard-C\u00f4t\u00e9",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Klein",
                        "suffix": ""
                    },
                    {
                        "first": "B",
                        "middle": [],
                        "last": "Taskar",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "21st COLING and 44th ACL",
                "volume": "",
                "issue": "",
                "pages": "761--768",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "P. Liang, A. Bouchard-C\u00f4t\u00e9, D. Klein, and B. Taskar. 2006. An end-to-end discriminative approach to ma- chine translation. In 21st COLING and 44th ACL, pages 761-768, Sydney, Australia.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Bilingual terminology mining -using brain, not brawn comparable corpora",
                "authors": [
                    {
                        "first": "E",
                        "middle": [],
                        "last": "Morin",
                        "suffix": ""
                    },
                    {
                        "first": "B",
                        "middle": [],
                        "last": "Daille",
                        "suffix": ""
                    },
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Takeuchi",
                        "suffix": ""
                    },
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Kageura",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "45th ACL",
                "volume": "",
                "issue": "",
                "pages": "664--671",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "E. Morin, B. Daille, K. Takeuchi, and K. Kageura. 2007. Bilingual terminology mining -using brain, not brawn comparable corpora. In 45th ACL, pages 664-671, Prague, Czech Republic.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Identifying word translation in nonparallel texts",
                "authors": [
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Rapp",
                        "suffix": ""
                    }
                ],
                "year": 1995,
                "venue": "33rd ACL",
                "volume": "",
                "issue": "",
                "pages": "320--322",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "R. Rapp. 1995. Identifying word translation in non- parallel texts. In 33rd ACL, pages 320-322, Cam- bridge,Massachusetts, USA.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "An analogical learner for morphological analysis",
                "authors": [
                    {
                        "first": "N",
                        "middle": [],
                        "last": "Stroppa",
                        "suffix": ""
                    },
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Yvon",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "9th CoNLL",
                "volume": "",
                "issue": "",
                "pages": "120--127",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "N. Stroppa and F. Yvon. 2005. An analogical learner for morphological analysis. In 9th CoNLL, pages 120-127, Ann Arbor, MI.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Applying morphology generation models to machine translation",
                "authors": [
                    {
                        "first": "H",
                        "middle": [],
                        "last": "Toutanova",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Suzuki",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Ruopp",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "ACL-8 HLT",
                "volume": "",
                "issue": "",
                "pages": "514--522",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "K Toutanova, H. Suzuki, and A. Ruopp. 2008. Ap- plying morphology generation models to machine translation. In ACL-8 HLT, pages 514-522, Colom- bus, Ohio, USA.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Can we translate letters?",
                "authors": [
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Vilar",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Peter",
                        "suffix": ""
                    },
                    {
                        "first": "H",
                        "middle": [],
                        "last": "Ney",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Proceedings of the Second Workshop on Statistical Machine Translation",
                "volume": "",
                "issue": "",
                "pages": "33--39",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "D. Vilar, J. Peter, and H. Ney. 2007. Can we trans- late letters? In Proceedings of the Second Work- shop on Statistical Machine Translation, pages 33- 39, Prague, Czech Republic, June.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Solving analogical equations on words",
                "authors": [
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Yvon",
                        "suffix": ""
                    },
                    {
                        "first": "N",
                        "middle": [],
                        "last": "Stroppa",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Delhay",
                        "suffix": ""
                    },
                    {
                        "first": "L",
                        "middle": [],
                        "last": "Miclet",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "\u00c9cole Nationale Sup\u00e9rieure des T\u00e9l\u00e9communications",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "F. Yvon, N. Stroppa, A. Delhay, and L. Miclet. 2004. Solving analogical equations on words. Techni- cal Report D005, \u00c9cole Nationale Sup\u00e9rieure des T\u00e9l\u00e9communications, Paris, France, July.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "The authors proposed to sample x and y among the closest forms in terms of edit-distance to I(u).",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "function solver( x, y, z , s) Input: x, y, z , a triplet, s the sampling size Output: a set of solutions to [x : y = z : ? ] sol \u2190 \u03c6 for i \u2190 1 to s do a, b \u2190 odd(rand(0, 1))? z, y : y, z m \u2190 shuffle(a,b ) c \u2190 complementary(m,x, ,{}) sol \u2190 sol \u222a c return sol Algorithm 2: A Stroppa&Yvon flavored solver. rand(a, b) returns a random integer between a and b (included). The ternary operator ?: is to be understood as in the C language.",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "TABREF1": {
                "content": "<table/>",
                "type_str": "table",
                "text": "The 3-most frequent solutions generated by our solver, for different sampling sizes s, for the equation [reader : readable = doer : ? ]. nb indicates the number of (different) solutions generated. According to our definition, there are 32 distinct solutions to this equation. Note that our solver has no problem producing doable.",
                "html": null,
                "num": null
            },
            "TABREF5": {
                "content": "<table><tr><td>RU 46.2 40.5 18.7 69.9 32.3 34.8</td></tr><tr><td>SP 47.0 41.5 19.5 69.1 32.5 35.9</td></tr><tr><td>SW 42.8 36.0 15.4 66.8 28.6 31.9</td></tr><tr><td>\u2190 FI 44.8 36.6 16.4 66.7 29.9 33.2</td></tr><tr><td>FR 38.5 47.0 18.1 69.9 26.9 29.4</td></tr><tr><td>RU 42.1 49.4 20.8 70.3 29.6 32.3</td></tr><tr><td>SP 42.6 47.7 20.3 75.1 32.0 33.7</td></tr><tr><td>SW 44.6 40.8 18.2 69.5 31.0 32.9</td></tr></table>",
                "type_str": "table",
                "text": "47.1 31.6 14.9 57.7 27.2 31.9 FR 41.2 35.4 14.6 60.4 24.9 26.5 Main characteristics of the generator, as a function of the translation directions (TEST).",
                "html": null,
                "num": null
            },
            "TABREF6": {
                "content": "<table><tr><td/><td/><td>FI\u2192EN</td><td colspan=\"2\">FR\u2192EN</td><td colspan=\"2\">RU\u2192EN</td><td colspan=\"2\">SP\u2192EN</td><td colspan=\"2\">SW\u2192EN</td></tr><tr><td/><td>p</td><td>r</td><td>p</td><td>r</td><td>p</td><td>r</td><td>p</td><td>r</td><td>p</td><td>r</td></tr><tr><td colspan=\"11\">argmax-f1 41.3 56.7 46.7 63.9 48.1 65.6 49.2 63.4 43.2 61.0</td></tr><tr><td>s-best</td><td colspan=\"10\">53.6 61.3 57.5 68.4 61.9 66.7 64.3 70.0 53.1 64.4</td></tr></table>",
                "type_str": "table",
                "text": "Precision (p)  and recall (r) of some classifiers on the TEST material.",
                "html": null,
                "num": null
            },
            "TABREF7": {
                "content": "<table><tr><td/><td/><td/><td>41.3 17.3 46.7 16.8 47.8 18.6 48.7 19.2 43.4 18.1</td></tr><tr><td/><td/><td colspan=\"2\">10 61.6 25.8 62.8 22.6 61.7 24.0 69.3 27.3 62.1 25.9</td></tr><tr><td colspan=\"2\">s-best</td><td colspan=\"2\">1 53.5 20.8 56.9 19.3 58.5 20.3 63.2 22.5 50.4 21</td></tr><tr><td/><td/><td colspan=\"2\">10 69.4 27.0 69.0 23.4 71.8 24.9 78.4 27.9 65.7 27.4</td></tr><tr><td>oracle</td><td/><td colspan=\"2\">1 100 30.5 100 26.3 100 28.5 100 30.6 100 29.5</td></tr><tr><td colspan=\"4\">all translation directions, BLEU scores increase on</td></tr><tr><td colspan=\"4\">TEST from 66.2 to 71.5, that is, an absolute im-</td></tr><tr><td colspan=\"3\">provement of 5.3 points.</td></tr><tr><td/><td/><td>\u2192 EN</td><td>\u2190 EN</td></tr><tr><td/><td colspan=\"3\">P smt \u2206B P smt \u2206B</td></tr><tr><td>FI</td><td colspan=\"2\">20.2 +7.4</td><td>21.6 +6.4</td></tr><tr><td>FR</td><td colspan=\"2\">19.9 +5.3</td><td>17.0 +6.0</td></tr><tr><td>RU</td><td colspan=\"2\">24.1 +3.1</td><td>28.0 +6.4</td></tr><tr><td>SP</td><td colspan=\"2\">22.1 +4.9</td><td>26.4 +5.5</td></tr><tr><td>SW</td><td colspan=\"2\">25.9 +4.2</td><td>31.6 +3.2</td></tr></table>",
                "type_str": "table",
                "text": "Precision and recall at rank 1 and 10 for the Foreign-to-English translation tasks (TEST).",
                "html": null,
                "num": null
            },
            "TABREF8": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Translation performances on TEST. P smt stands for the precision and recall of the SMT engine. \u2206B indicates the absolute gain in BLEU score of the combined system.",
                "html": null,
                "num": null
            }
        }
    }
}