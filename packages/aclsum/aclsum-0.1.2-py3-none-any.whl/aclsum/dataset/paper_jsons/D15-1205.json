{
    "paper_id": "D15-1205",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:01:00.093221Z"
    },
    "title": "Improved Relation Extraction with Feature-Rich Compositional Embedding Models",
    "authors": [
        {
            "first": "Matthew",
            "middle": [
                "R"
            ],
            "last": "Gormley",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Johns Hopkins University",
                "location": {
                    "postCode": "21218",
                    "settlement": "Baltimore",
                    "region": "MD"
                }
            },
            "email": ""
        },
        {
            "first": "Mo",
            "middle": [],
            "last": "Yu",
            "suffix": "",
            "affiliation": {
                "laboratory": "Machine Intelligence and Translation Lab",
                "institution": "Harbin Institute of Technology",
                "location": {
                    "settlement": "Harbin",
                    "country": "China"
                }
            },
            "email": ""
        },
        {
            "first": "Mark",
            "middle": [],
            "last": "Dredze",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Johns Hopkins University",
                "location": {
                    "postCode": "21218",
                    "settlement": "Baltimore",
                    "region": "MD"
                }
            },
            "email": "mdredze@cs.jhu.edu"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Compositional embedding models build a representation (or embedding) for a linguistic structure based on its component word embeddings.\nWe propose a Feature-rich Compositional Embedding Model (FCM) for relation extraction that is expressive, generalizes to new domains, and is easy-to-implement. The key idea is to combine both (unlexicalized) handcrafted features with learned word embeddings. The model is able to directly tackle the difficulties met by traditional compositional embeddings models, such as handling arbitrary types of sentence annotations and utilizing global information for composition. We test the proposed model on two relation extraction tasks, and demonstrate that our model outperforms both previous compositional models and traditional feature rich models on the ACE 2005 relation extraction task, and the SemEval 2010 relation classification task. The combination of our model and a loglinear classifier with hand-crafted features gives state-of-the-art results. We made our implementation available for general use 1 .",
    "pdf_parse": {
        "paper_id": "D15-1205",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Compositional embedding models build a representation (or embedding) for a linguistic structure based on its component word embeddings.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            },
            {
                "text": "We propose a Feature-rich Compositional Embedding Model (FCM) for relation extraction that is expressive, generalizes to new domains, and is easy-to-implement. The key idea is to combine both (unlexicalized) handcrafted features with learned word embeddings. The model is able to directly tackle the difficulties met by traditional compositional embeddings models, such as handling arbitrary types of sentence annotations and utilizing global information for composition. We test the proposed model on two relation extraction tasks, and demonstrate that our model outperforms both previous compositional models and traditional feature rich models on the ACE 2005 relation extraction task, and the SemEval 2010 relation classification task. The combination of our model and a loglinear classifier with hand-crafted features gives state-of-the-art results. We made our implementation available for general use 1 .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Two common NLP feature types are lexical properties of words and unlexicalized linguistic/structural interactions between words. Prior work on relation extraction has extensively studied how to design such features by combining discrete lexical properties (e.g. the identity of a word, \u21e4 \u21e4 Gormley and Yu contributed equally. 1 https://github.com/mgormley/pacaya its lemma, its morphological features) with aspects of a word's linguistic context (e.g. whether it lies between two entities or on a dependency path between them). While these help learning, they make generalization to unseen words difficult. An alternative approach to capturing lexical information relies on continuous word embeddings2 as representative of words but generalizable to new words. Embedding features have improved many tasks, including NER, chunking, dependency parsing, semantic role labeling, and relation extraction (Miller et al., 2004; Turian et al., 2010; Koo et al., 2008; Roth and Woodsend, 2014; Sun et al., 2011; Plank and Moschitti, 2013; Nguyen and Grishman, 2014) . Embeddings can capture lexical information, but alone they are insufficient: in state-of-the-art systems, they are used alongside features of the broader linguistic context.",
                "cite_spans": [
                    {
                        "start": 899,
                        "end": 920,
                        "text": "(Miller et al., 2004;",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 921,
                        "end": 941,
                        "text": "Turian et al., 2010;",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 942,
                        "end": 959,
                        "text": "Koo et al., 2008;",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 960,
                        "end": 984,
                        "text": "Roth and Woodsend, 2014;",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 985,
                        "end": 1002,
                        "text": "Sun et al., 2011;",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 1003,
                        "end": 1029,
                        "text": "Plank and Moschitti, 2013;",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 1030,
                        "end": 1056,
                        "text": "Nguyen and Grishman, 2014)",
                        "ref_id": "BIBREF20"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this paper, we introduce a compositional model that combines unlexicalized linguistic context and word embeddings for relation extraction, a task in which contextual feature construction plays a major role in generalizing to unseen data. Our model allows for the composition of embeddings with arbitrary linguistic structure, as expressed by hand crafted features. In the following sections, we begin with a precise construction of compositional embeddings using word embeddings in conjunction with unlexicalized features. Various feature sets used in prior work (Turian et al., 2010; Nguyen and Grishman, 2014; Hermann et al., 2014; Roth and Woodsend, 2014) A feature that depends on the embedding for this context word could generalize to other lexical indicators of the same relation (e.g. \"operating\") that don't appear with ART during training. But lexical information alone is insufficient; relation extraction requires the identification of lexical roles: where a word appears structurally in the sentence. In (2), the word \"of\" between \"suburbs\" and \"Baghdad\" suggests that the first entity is part of the second, yet the earlier occurrence after \"direction\" is of no significance to the relation. Even finer information can be expressed by a word's role on the dependency path between entities.",
                "cite_spans": [
                    {
                        "start": 566,
                        "end": 587,
                        "text": "(Turian et al., 2010;",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 588,
                        "end": 614,
                        "text": "Nguyen and Grishman, 2014;",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 615,
                        "end": 636,
                        "text": "Hermann et al., 2014;",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 637,
                        "end": 661,
                        "text": "Roth and Woodsend, 2014)",
                        "ref_id": "BIBREF26"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In (3) we can distinguish the word \"died\" from other irrelevant words that don't appear between the entities.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "tured as special cases of this construction. Adding these compositional embeddings directly to a standard log-linear model yields a special case of our full model. We then treat the word embeddings as parameters giving rise to our powerful, efficient, and easy-to-implement log-bilinear model. The model capitalizes on arbitrary types of linguistic annotations by better utilizing features associated with substructures of those annotations, including global information. We choose features to promote different properties and to distinguish different functions of the input words. The full model involves three stages. First, it decomposes the annotated sentence into substructures (i.e. a word and associated annotations). Second, it extracts features for each substructure (word), and combines them with the word's embedding to form a substructure embedding. Third, we sum over substructure embeddings to form a composed annotated sentence embedding, which is used by a final softmax layer to predict the output label (relation).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The result is a state-of-the-art relation extractor for unseen domains from ACE 2005 (Walker et al., 2006) and the relation classification dataset from SemEval-2010 Task 8 (Hendrickx et al., 2010) .",
                "cite_spans": [
                    {
                        "start": 85,
                        "end": 106,
                        "text": "(Walker et al., 2006)",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 172,
                        "end": 196,
                        "text": "(Hendrickx et al., 2010)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Contributions This paper makes several contributions, including:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "1. We introduce the Yu and Dredze (2015) and Yu et al. (2015) . Additionally, we have extended FCM to incorporate a low-rank embedding of the features (Yu et al., 2015) , which focuses on fine-grained relation extraction for ACE and ERE. This paper obtains better results than the low-rank extension on ACE coarse-grained relation extraction.",
                "cite_spans": [
                    {
                        "start": 20,
                        "end": 40,
                        "text": "Yu and Dredze (2015)",
                        "ref_id": null
                    },
                    {
                        "start": 45,
                        "end": 61,
                        "text": "Yu et al. (2015)",
                        "ref_id": null
                    },
                    {
                        "start": 151,
                        "end": 168,
                        "text": "(Yu et al., 2015)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In relation extraction we are given a sentence as input with the goal of identifying, for all pairs of entity mentions, what relation exists between them, if any. For each pair of entity mentions in a sentence S, we construct an instance (y, x), where x = (M 1 , M 2 , S, A). S = {w 1 , w 2 , ..., w n } is a sentence of length n that expresses a relation of type y between two entity mentions M 1 and M 2 , where M 1 and M 2 are sequences of words in S. A is the associated annotations of sentence S, such as part-of-speech tags, a dependency parse, and named entities. We consider directed relations: for a relation type Rel, y=Rel(M 1 , M 2 ) and y 0 =Rel(M 2 , M 1 ) are different relations. Table 1 shows ACE 2005 relations, and has a strong label bias towards negative examples. We also consider the task of relation classification (Se-mEval), where the number of negative examples is artificially reduced.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 702,
                        "end": 703,
                        "text": "1",
                        "ref_id": "TABREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Relation Extraction",
                "sec_num": "2"
            },
            {
                "text": "Embedding Models Word embeddings and compositional embedding models have been successfully applied to a range of NLP tasks, however the applications of these embedding models to relation extraction are still limited. Prior work on relation classification (e.g. SemEval 2010 Task 8) has focused on short sentences with at most one relation per sentence (Socher et al., 2012; Zeng et al., 2014) . For relation extraction, where negative examples abound, prior work has assumed that only the named entity boundaries and not their types were available (Plank and Moschitti, 2013; Nguyen et al., 2015) . Other work has as-sumed that the order of two entities in a relation are given while the relation type itself is unknown (Nguyen and Grishman, 2014; Nguyen and Grishman, 2015) . The standard relation extraction task, as adopted by ACE 2005 (Walker et al., 2006) , uses long sentences containing multiple named entities with known types4 and unknown relation directions. We are the first to apply neural language model embeddings to this task.",
                "cite_spans": [
                    {
                        "start": 352,
                        "end": 373,
                        "text": "(Socher et al., 2012;",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 374,
                        "end": 392,
                        "text": "Zeng et al., 2014)",
                        "ref_id": "BIBREF36"
                    },
                    {
                        "start": 548,
                        "end": 575,
                        "text": "(Plank and Moschitti, 2013;",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 576,
                        "end": 596,
                        "text": "Nguyen et al., 2015)",
                        "ref_id": null
                    },
                    {
                        "start": 720,
                        "end": 747,
                        "text": "(Nguyen and Grishman, 2014;",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 748,
                        "end": 774,
                        "text": "Nguyen and Grishman, 2015)",
                        "ref_id": null
                    },
                    {
                        "start": 839,
                        "end": 860,
                        "text": "(Walker et al., 2006)",
                        "ref_id": "BIBREF33"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Relation Extraction",
                "sec_num": "2"
            },
            {
                "text": "Motivation and Examples Whether a word is indicative of a relation depends on multiple properties, which may relate to its context within the sentence. For example, whether the word is inbetween the entities, on the dependency path between them, or to their left or right may provide additional complementary information. Illustrative examples are given in Table 1 and provide the motivation for our model. In the next section, we will show how we develop informative representations capturing both the semantic information in word embeddings and the contextual information expressing a word's role relative to the entity mentions. We are the first to incorporate all of this information at once. The closest work is that of Nguyen and Grishman (2014) , who use a loglinear model for relation extraction with embeddings as features for only the entity heads. Such embedding features are insensitive to the broader contextual information and, as we show, are not sufficient to elicit the word's role in a relation.",
                "cite_spans": [
                    {
                        "start": 725,
                        "end": 751,
                        "text": "Nguyen and Grishman (2014)",
                        "ref_id": "BIBREF20"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 363,
                        "end": 364,
                        "text": "1",
                        "ref_id": "TABREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Relation Extraction",
                "sec_num": "2"
            },
            {
                "text": "We propose a general framework to construct an embedding of a sentence with annotations on its component words. While we focus on the relation extraction task, the framework applies to any task that benefits from both embeddings and typical hand-engineered lexical features.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Feature-rich Compositional Embedding Model for Relations",
                "sec_num": "3"
            },
            {
                "text": "We begin by describing a precise method for constructing substructure embeddings and annotated sentence embeddings from existing (usually unlexicalized) features and embeddings. Note that these embeddings can be included directly in a log-linear model as features-doing so results in a special case of our full model presented in the next subsection. An annotated sentence is first decomposed into substructures. The type of substructures can vary by task; for relation extraction we consider one substructure per word5 . For each substructure in the sentence we have a hand-crafted feature vector f w i and a dense embedding vector e w i . We represent each substructure as the outer product \u2326 between these two vectors to produce a matrix, herein called a substructure embedding: h w i = f w i \u2326 e w i . The features f w i are based on the local context in S and annotations in A, which can include global information about the annotated sentence. These features allow the model to promote different properties and to distinguish different functions of the words. Feature engineering can be task specific, as relevant annotations can change with regards to each task. In this work we utilize unlexicalized binary features common in relation extraction. Figure 1 depicts the construction of a sentence's substructure embeddings.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 1262,
                        "end": 1263,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Combining Features with Embeddings",
                "sec_num": "3.1"
            },
            {
                "text": "We further sum over the substructure embeddings to form an annotated sentence embedding:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Combining Features with Embeddings",
                "sec_num": "3.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "e x = n X i=1 f w i \u2326 e w i",
                        "eq_num": "(1)"
                    }
                ],
                "section": "Combining Features with Embeddings",
                "sec_num": "3.1"
            },
            {
                "text": "When both the hand-crafted features and word embeddings are treated as inputs, as has previously been the case in relation extraction, this annotated sentence embedding can be used directly as the features of a log-linear model. In fact, we find that the feature sets used in prior work for many other NLP tasks are special cases of this simple construction (Turian et al., 2010; Nguyen and Grishman, 2014; Hermann et al., 2014; Roth and Woodsend, 2014) . This highlights an important connection: when the word embeddings are constant, our constructions of substructure and annotated sentence embeddings are just specific forms of polynomial (specifically quadratic) feature combination-hence their commonality in the literature. Our experimental results suggest that such a construction is more powerful than directly including embeddings into the model.",
                "cite_spans": [
                    {
                        "start": 358,
                        "end": 379,
                        "text": "(Turian et al., 2010;",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 380,
                        "end": 406,
                        "text": "Nguyen and Grishman, 2014;",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 407,
                        "end": 428,
                        "text": "Hermann et al., 2014;",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 429,
                        "end": 453,
                        "text": "Roth and Woodsend, 2014)",
                        "ref_id": "BIBREF26"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Combining Features with Embeddings",
                "sec_num": "3.1"
            },
            {
                "text": "Our full log-bilinear model first forms the substructure and annotated sentence embeddings from ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Log-Bilinear Model",
                "sec_num": "3.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "M i n X i=1 f i \u2326 e w i (",
                        "eq_num": "17"
                    }
                ],
                "section": "The Log-Bilinear Model",
                "sec_num": "3.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": ") (18) + (",
                        "eq_num": "19"
                    }
                ],
                "section": "The Log-Bilinear Model",
                "sec_num": "3.2"
            },
            {
                "text": ")",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Log-Bilinear Model",
                "sec_num": "3.2"
            },
            {
                "text": ") T h f .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Log-Bilinear Model",
                "sec_num": "3.2"
            },
            {
                "text": "tensor T can be written as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Log-Bilinear Model",
                "sec_num": "3.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "f i .",
                        "eq_num": "(4)"
                    }
                ],
                "section": "The Log-Bilinear Model",
                "sec_num": "3.2"
            },
            {
                "text": "e tensor, making the model ross-entropy objective:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Log-Bilinear Model",
                "sec_num": "3.2"
            },
            {
                "text": "[9] to optimize above g; and for each ing P (y|S; T, W ).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Log-Bilinear Model",
                "sec_num": "3.2"
            },
            {
                "text": "Then ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Log-Bilinear Model",
                "sec_num": "3.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "M i n X i=1 f i \u2326 e w i (",
                        "eq_num": "17"
                    }
                ],
                "section": "The Log-Bilinear Model",
                "sec_num": "3.2"
            },
            {
                "text": ") (18) + (19)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Log-Bilinear Model",
                "sec_num": "3.2"
            },
            {
                "text": "core of label y computed with our model. Since we decompose the strucactor f i 2 S will contribute to the score based on the model parameters. corresponds to a slice of the tensor T y , which is a matrix (y, \u2022, \u2022). Then bute a score",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Log-Bilinear Model",
                "sec_num": "3.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "s(y, f i ) = T y g f h f ,",
                        "eq_num": "(3)"
                    }
                ],
                "section": "The Log-Bilinear Model",
                "sec_num": "3.2"
            },
            {
                "text": "ensor product, while in the case of Eq.( 3), it has the equivalent form:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Log-Bilinear Model",
                "sec_num": "3.2"
            },
            {
                "text": "g f h f = T y (g f \u2326 h f ) = ( (y, \u2022, \u2022) \u2022 g f ) T h f .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Log-Bilinear Model",
                "sec_num": "3.2"
            },
            {
                "text": "re of label y given an instance S and parameter tensor T can be written as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Log-Bilinear Model",
                "sec_num": "3.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "(y, S; T ) = n X i=1 s(y, f i ; T ) = n X i=1 T y g f i h f i .",
                        "eq_num": "(4)"
                    }
                ],
                "section": "The Log-Bilinear Model",
                "sec_num": "3.2"
            },
            {
                "text": "forms linear transformations on each view of the tensor, making the model lement.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Log-Bilinear Model",
                "sec_num": "3.2"
            },
            {
                "text": "train the parameters we optimize the following cross-entropy objective:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Log-Bilinear Model",
                "sec_num": "3.2"
            },
            {
                "text": "`(D; T, W ) = X (y,S)2D log P (y|S; T, W )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Log-Bilinear Model",
                "sec_num": "3.2"
            },
            {
                "text": "of all training data. We used AdaGrad [9] to optimize above re we are performing stochastic training; and for each ins function `= `(y, S; T, W ) = log P (y|S; T, W ).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Log-Bilinear Model",
                "sec_num": "3.2"
            },
            {
                "text": "Then 2 4 ce.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Log-Bilinear Model",
                "sec_num": "3.2"
            },
            {
                "text": "epresent each factor as the outer product between the feature rmed embedding g f \u2326h f . The we use a tensor T = L\u2326E \u2326F put matrix to the labels. Here L is the set of labels, E refers to 200) and F is the set of features.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Log-Bilinear Model",
                "sec_num": "3.2"
            },
            {
                "text": "obability of a label y given the structure S, we have",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Log-Bilinear Model",
                "sec_num": "3.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "; T ) = exp{s(y, S; T )} P y 2L exp{s(y , S; T )} ,",
                        "eq_num": "(2)"
                    }
                ],
                "section": "The Log-Bilinear Model",
                "sec_num": "3.2"
            },
            {
                "text": "l y computed with our model. Since we decompose the strucwill contribute to the score based on the model parameters. s to a slice of the tensor T y , which is a matrix (y, \u2022, \u2022). Then",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Log-Bilinear Model",
                "sec_num": "3.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "s(y, f i ) = T y g f h f ,",
                        "eq_num": "(3)"
                    }
                ],
                "section": "The Log-Bilinear Model",
                "sec_num": "3.2"
            },
            {
                "text": "t, while in the case of Eq.( 3), it has the equivalent form:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Log-Bilinear Model",
                "sec_num": "3.2"
            },
            {
                "text": "T y (g f \u2326 h f ) = ( (y, \u2022, \u2022) \u2022 g f ) T h f .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Log-Bilinear Model",
                "sec_num": "3.2"
            },
            {
                "text": "given an instance S and parameter tensor T can be written as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Log-Bilinear Model",
                "sec_num": "3.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "n X i=1 s(y, f i ; T ) = n X i=1 T y g f i h f i .",
                        "eq_num": "(4)"
                    }
                ],
                "section": "The Log-Bilinear Model",
                "sec_num": "3.2"
            },
            {
                "text": "transformations on each view of the tensor, making the model meters we optimize the following cross-entropy objective: the previous subsection. The model uses its parameters to score the annotated sentence embedding and uses a softmax to produce an output label. We call the entire model the Feature-rich Compositional Embedding Model (FCM).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Log-Bilinear Model",
                "sec_num": "3.2"
            },
            {
                "text": "W ) = X",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Log-Bilinear Model",
                "sec_num": "3.2"
            },
            {
                "text": "Our task is to determine the label y (relation) given the instance x = (M 1 , M 2 , S, A). We formulate this as a probability.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Log-Bilinear Model",
                "sec_num": "3.2"
            },
            {
                "text": "P (y|x; T, e) = exp ( P n i=1 T y (f w i \u2326 e w i )) Z(x)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Log-Bilinear Model",
                "sec_num": "3.2"
            },
            {
                "text": "(2) where is the 'matrix dot product' or Frobenious inner product of the two matrices. The normalizing constant which sums over all possible output labels y 0 2 L is given by Z(x) = P y 0 2L exp",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Log-Bilinear Model",
                "sec_num": "3.2"
            },
            {
                "text": "P n i=1 T y 0 (f w i \u2326 e w i )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Log-Bilinear Model",
                "sec_num": "3.2"
            },
            {
                "text": ". The parameters of the model are the word embeddings e for each word type and a list of weight matrix T = [T y ] y2L which is used to score each label y. The model is log-bilinear 6 (i.e. log-quadratic) since we recover a log-linear model by fixing either e or T . We study both the full log-bilinear and the log-linear model obtained by fixing the word embeddings.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Log-Bilinear Model",
                "sec_num": "3.2"
            },
            {
                "text": "Substructure Embeddings Similar words (i.e. those with similar embeddings) with similar functions in the sentence (i.e. those with similar features) will have similar matrix representations. To understand our selection of the outer product, consider the example in Fig. 1 . The word \"driving\" can indicate the ART relation if it appears on the 6 Other popular log-bilinear models are the log-bilinear language models (Mnih and Hinton, 2007; Mikolov et al., 2013) .",
                "cite_spans": [
                    {
                        "start": 417,
                        "end": 440,
                        "text": "(Mnih and Hinton, 2007;",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 441,
                        "end": 462,
                        "text": "Mikolov et al., 2013)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 270,
                        "end": 271,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Discussion of the Model",
                "sec_num": "3.3"
            },
            {
                "text": "dependency path between M 1 and M 2 . Suppose the third feature in f w i indicates this on-path feature. Our model can now learn parameters which give the third row a high weight for the ART label. Other words with embeddings similar to \"driving\" that appear on the dependency path between the mentions will similarly receive high weight for the ART label. On the other hand, if the embedding is similar but is not on the dependency path, it will have 0 weight. Thus, our model generalizes its model parameters across words with similar embeddings only when they share similar functions in the sentence.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discussion of the Model",
                "sec_num": "3.3"
            },
            {
                "text": "Smoothed Lexical Features Another intuition about the selection of outer product is that it is actually a smoothed version of traditional lexical features used in classical NLP systems. Consider a lexical feature f = u ^w, which is a conjunction (logic-and) between non-lexical property u and lexical part (word) w. If we represent w as a one-hot vector, then the outer product exactly recovers the original feature f . Then if we replace the one-hot representation with its word embedding, we get the current form of our FCM. Therefore, our model can be viewed as a smoothed version of lexical features, which keeps the expressive strength, and uses embeddings to generalize to low frequency features. Time Complexity Inference in FCM is much faster than both CNNs (Collobert et al., 2011) and RNNs (Socher et al., 2013b; Bordes et al., 2012) . FCM requires O(snd) products on average with sparse features, where s is the average number of per-word non-zero feature values, n is the length of the sentence, and d is the dimension of word embedding. In contrast, CNNs and RNNs usually have complexity O(C \u2022 nd 2 ), where C is a model dependent constant.",
                "cite_spans": [
                    {
                        "start": 766,
                        "end": 790,
                        "text": "(Collobert et al., 2011)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 800,
                        "end": 822,
                        "text": "(Socher et al., 2013b;",
                        "ref_id": null
                    },
                    {
                        "start": 823,
                        "end": 843,
                        "text": "Bordes et al., 2012)",
                        "ref_id": "BIBREF1"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discussion of the Model",
                "sec_num": "3.3"
            },
            {
                "text": "We present a hybrid model which combines the FCM with an existing log-linear model. We do so by defining a new model:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Hybrid Model",
                "sec_num": "4"
            },
            {
                "text": "p FCM+loglin (y|x) = 1 Z p FCM (y|x)p loglin (y|x) (3)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Hybrid Model",
                "sec_num": "4"
            },
            {
                "text": "The log-linear model has the usual form:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Hybrid Model",
                "sec_num": "4"
            },
            {
                "text": "p loglin (y|x) / exp(\u2713 \u2022 f (x, y))",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Hybrid Model",
                "sec_num": "4"
            },
            {
                "text": ", where \u2713 are the model parameters and f (x, y) is a vector of features. The integration treats each model as a providing a score which we multiply together. The constant Z ensures a normalized distribution.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Hybrid Model",
                "sec_num": "4"
            },
            {
                "text": "FCM training optimizes a cross-entropy objective:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training",
                "sec_num": "5"
            },
            {
                "text": "`(D; T, e) = X (x,y)2D log P (y|x; T, e)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training",
                "sec_num": "5"
            },
            {
                "text": "where D is the set of all training data and e is the set of word embeddings. To optimize the objective, for each instance (y, x) we perform stochastic training on the loss function `= `(y, x; T, e) = log P (y|x; T, e). The gradients of the model parameters are obtained by backpropagation (i.e. repeated application of the chain rule). We define the vector s = [ P i T y (f w i \u2326 e w i )] 1\uf8ffy\uf8ffL , which yields @@ s = h I[y 0 = y] P (y 0 |x; T, e) 1\uf8ffy\uf8ffL i T ,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training",
                "sec_num": "5"
            },
            {
                "text": "where the indicator function I[x] equals 1 if x is true and 0 otherwise. We have the following gradients: @@ T = @@ s \u2326 P n i=1 f w i \u2326 e w i , which is equivalent to:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training",
                "sec_num": "5"
            },
            {
                "text": "@@ T y 0 = I[y = y 0 ] P (y 0 |x; T, e) \u2022 n X i=1 f w i \u2326 e w i .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training",
                "sec_num": "5"
            },
            {
                "text": "When we treat the word embeddings as parameters (i.e. the log-bilinear model), we also fine-tune the word embeddings with the FCM model:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training",
                "sec_num": "5"
            },
            {
                "text": "@@ e w = n X i=1 X y @@ s y T y ! \u2022 f i ! \u2022 I[w i = w].",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training",
                "sec_num": "5"
            },
            {
                "text": "As is common in deep learning, we initialize these embeddings from an neural language model and then fine-tune them for our supervised task. The training process for the hybrid model ( \u00a7 4) is also easily done by backpropagation since each sub-model has separate parameters.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training",
                "sec_num": "5"
            },
            {
                "text": "Set Template HeadEmb {I[i = h1], I[i = h2]} (wi is head of M1/M2) \u21e5{ , t h 1 , t h 2 , t h 1 t h 2 } Context I[i = h1 \u00b1 1] (left/right token of w h 1 ) I[i = h2 \u00b1 1] (left/right token of w h 2 ) In-between I[i > h1]&I[i < h2] (in between ) \u21e5{ , t h 1 , t h 2 , t h 1 t h 2 } On-path I[wi 2 P ] (on path) \u21e5{ , t h 1 , t h 2 , t h 1 t h 2 }",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training",
                "sec_num": "5"
            },
            {
                "text": "Table 2 : Feature sets used in FCM.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Training",
                "sec_num": "5"
            },
            {
                "text": "Features Our FCM features (Table 2 ) use a feature vector f w i over the word w i , the two target entities M 1 , M 2 , and their dependency path.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 33,
                        "end": 34,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Experimental Settings",
                "sec_num": "6"
            },
            {
                "text": "Here h 1 , h 2 are the indices of the two head words of M 1 , M 2 , \u21e5 refers to the Cartesian product between two sets, t h 1 and t h 2 are entity types (named entity tags for ACE 2005 or WordNet supertags for SemEval 2010) of the head words of two entities, and stands for the empty feature. refers to the conjunction of two elements. The In-between features indicate whether a word w i is in between two target entities, and the On-path features indicate whether the word is on the dependency path, on which there is a set of words P , between the two entities. We also use the target entity type as a feature. Combining this with the basic features results in more powerful compound features, which can help us better distinguish the functions of word embeddings for predicting certain relations. For example, if we have a person and a vehicle, we know it will be more likely that they have an ART relation. For the ART relation, we introduce a corresponding weight vector, which is closer to lexical embeddings similar to the embedding of \"drive\".",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Settings",
                "sec_num": "6"
            },
            {
                "text": "All linguistic annotations needed for features (POS, chunks7 , parses) are from Stanford CoreNLP (Manning et al., 2014) . Since SemEval does not have gold entity types we obtained Word-Net and named entity tags using Ciaramita and Altun (2006) . For all experiments we use 200d word embeddings trained on the NYT portion of the Gigaword 5.0 corpus (Parker et al., 2011) , with word2vec (Mikolov et al., 2013) . We use the CBOW model with negative sampling (15 negative words). We set a window size c=5, and remove types occurring less than 5 times.",
                "cite_spans": [
                    {
                        "start": 97,
                        "end": 119,
                        "text": "(Manning et al., 2014)",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 217,
                        "end": 243,
                        "text": "Ciaramita and Altun (2006)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 348,
                        "end": 369,
                        "text": "(Parker et al., 2011)",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 386,
                        "end": 408,
                        "text": "(Mikolov et al., 2013)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Settings",
                "sec_num": "6"
            },
            {
                "text": "We consider several methods. (1) FCM in isolation without fine-tuning. (2) FCM in isolation with fine-tuning (i.e. trained as a log-bilinear model). ( 3) A log-linear model with a rich binary feature set from Sun et al. (2011) (Baseline)this consists of all the baseline features of Zhou et al. (2005) plus several additional carefully-chosen features that have been highly tuned for ACE-style relation extraction over years of research. We exclude the Country gazetteer and WordNet features from Zhou et al. (2005) . The two remaining methods are hybrid models that integrate FCM as a submodel within the log-linear model ( \u00a7 4). We consider two combinations. (4) The feature set of Nguyen and Grishman (2014) obtained by using the embeddings of heads of two entity mentions (+HeadOnly). ( 5) Our full FCM model (+FCM). All models use L2 regularization tuned on dev data.",
                "cite_spans": [
                    {
                        "start": 209,
                        "end": 226,
                        "text": "Sun et al. (2011)",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 283,
                        "end": 301,
                        "text": "Zhou et al. (2005)",
                        "ref_id": "BIBREF37"
                    },
                    {
                        "start": 497,
                        "end": 515,
                        "text": "Zhou et al. (2005)",
                        "ref_id": "BIBREF37"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Models",
                "sec_num": null
            },
            {
                "text": "We evaluate our relation extraction system on the English portion of the ACE 2005 corpus (Walker et al., 2006) . 8 There are 6 domains: Newswire (nw), Broadcast Conversation (bc), Broadcast News (bn), Telephone Speech (cts), Usenet Newsgroups (un), and Weblogs (wl). Following prior work we focus on the domain adaptation setting, where we train on one set (the union of the news domains (bn+nw), tune hyperparameters on a dev domain (half of bc) and evaluate on the remainder (cts, wl, and the remainder of bc) (Plank and Moschitti, 2013; Nguyen and Grishman, 2014) . We assume that gold entity spans and types are available for train and test. We use all pairs of entity mentions to yield 43,518 total relations in the training set. We report precision, recall, and F1 for relation extraction. While it is not our focus, for completeness we include results with unknown entity types following Plank and Moschitti (2013) (Appendix 1).",
                "cite_spans": [
                    {
                        "start": 89,
                        "end": 110,
                        "text": "(Walker et al., 2006)",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 512,
                        "end": 539,
                        "text": "(Plank and Moschitti, 2013;",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 540,
                        "end": 566,
                        "text": "Nguyen and Grishman, 2014)",
                        "ref_id": "BIBREF20"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Datasets and Evaluation ACE 2005",
                "sec_num": "6.1"
            },
            {
                "text": "SemEval 2010 Task 8 We evaluate on the Se-mEval 2010 Task 8 dataset9 (Hendrickx et al., 2010) to compare with other compositional models and highlight the advantages of FCM. This task is to determine the relation type (or no relation) between two entities in a sentence. We adopt the setting of Socher et al. (2012) . We use 10-fold cross validation on the training data to select hyperparameters and do regularization by early stopping. The learning rates for FCM with/without fine-tuning are 5e-3 and 5e-2 respectively. We report macro-F1 and compare to previously published results.",
                "cite_spans": [
                    {
                        "start": 69,
                        "end": 93,
                        "text": "(Hendrickx et al., 2010)",
                        "ref_id": null
                    },
                    {
                        "start": 295,
                        "end": 315,
                        "text": "Socher et al. (2012)",
                        "ref_id": "BIBREF27"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Datasets and Evaluation ACE 2005",
                "sec_num": "6.1"
            },
            {
                "text": "ACE 2005 Despite FCM's (1) simple feature set, it is competitive with the log-linear baseline (3) on out-of-domain test sets (Table 3 ). In the typical gold entity spans and types setting, both Plank and Moschitti (2013) and Nguyen and Grishman (2014) found that they were unable to obtain improvements by adding embeddings to baseline feature sets. By contrast, we find that on all domains the combination baseline + FCM (5) obtains the highest F1 and significantly outperforms the other baselines, yielding the best reported results for this task. We found that fine-tuning of embeddings (2) did not yield improvements on our out-of-domain development set, in contrast to our results below for SemEval. We suspect this is because fine-tuning allows the model to overfit the training domain, which then hurts performance on the unseen ACE test domains. Accordingly, Table 3 shows only the log-linear model.",
                "cite_spans": [
                    {
                        "start": 194,
                        "end": 220,
                        "text": "Plank and Moschitti (2013)",
                        "ref_id": "BIBREF24"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 132,
                        "end": 133,
                        "text": "3",
                        "ref_id": "TABREF3"
                    },
                    {
                        "start": 873,
                        "end": 874,
                        "text": "3",
                        "ref_id": "TABREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "7"
            },
            {
                "text": "Finally, we highlight an important contrast between FCM (1) and the log-linear model (3): the latter uses over 50 feature templates based on a POS tagger, dependency parser, chunker, and constituency parser. FCM uses only a dependency parse but still obtains better results (Avg. F1).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "7"
            },
            {
                "text": "SemEval 2010 Task 8 Table 4 shows FCM compared to the best reported results from the SemEval-2010 Task 8 shared task and several other compositional models.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 26,
                        "end": 27,
                        "text": "4",
                        "ref_id": "TABREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "7"
            },
            {
                "text": "For the FCM we considered two feature sets. We found that using NE tags instead of WordNet tags helps with fine-tuning but hurts without. This may be because the set of WordNet tags is larger making the model more expressive, but also introduces more parameters. When the embeddings are fixed, they can help to better distinguish different functions of embeddings. But when fine-tuning, it becomes easier to over-fit. Alleviating over-fitting is a subject for future work ( \u00a7 9).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "7"
            },
            {
                "text": "With either WordNet or NER features, FCM achieves better performance than the RNN and MVRNN. With NER features and fine-tuning, it outperforms a CNN (Zeng et al., 2014) the combination of an embedding model and a traditional log-linear model (RNN/MVRNN + linear) (Socher et al., 2012) . As with ACE, FCM uses less linguistic resources than many close competitors (Rink and Harabagiu, 2010) . We also compared to concurrent work on enhancing the compositional models with taskspecific information for relation classification, including Hashimoto et al. (2015) (RelEmb), which trained task-specific word embeddings, and dos Santos et al. ( 2015) (CR-CNN), which proposed a task-specific ranking-based loss function. Our Hybrid methods (FCM + linear) get comparable results to theirs. Note that their base compositional model results without any task-specific enhancements, i.e. RelEmb with word2vec embeddings and CR-CNN with log-loss, are still lower than the best FCM result. We believe that FCM can be also improved with these task-specific enhancements, e.g. replacing the word embeddings to the taskspecific ones from (Hashimoto et al., 2015) increases the result to 83.7% (see \u00a77.2 for details). We leave the application of ranking-based loss to future work.",
                "cite_spans": [
                    {
                        "start": 149,
                        "end": 168,
                        "text": "(Zeng et al., 2014)",
                        "ref_id": "BIBREF36"
                    },
                    {
                        "start": 263,
                        "end": 284,
                        "text": "(Socher et al., 2012)",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 363,
                        "end": 389,
                        "text": "(Rink and Harabagiu, 2010)",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 535,
                        "end": 558,
                        "text": "Hashimoto et al. (2015)",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 1121,
                        "end": 1145,
                        "text": "(Hashimoto et al., 2015)",
                        "ref_id": "BIBREF5"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "7"
            },
            {
                "text": "Finally, a concurrent work (Liu et al., 2015) proposes DepNN, which builds representations for the dependency path (and its attached subtrees) between two entities by applying recursive and convolutional neural networks successively. Compared to their model, our FCM achieves comparable results. Of note, our FCM and the RelEmb are also the most efficient models among all above compositional models since they have linear time complexity with respect to the dimension of embeddings.",
                "cite_spans": [
                    {
                        "start": 27,
                        "end": 45,
                        "text": "(Liu et al., 2015)",
                        "ref_id": "BIBREF13"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "7"
            },
            {
                "text": "We next investigate the effects of different types of features on FCM using ablation tests on ACE 2005 (Table 5 .) We focus on FCM alone with the feature templates of Table 2 . Additionally, we show results of using only the head embedding features from Nguyen and Grishman (2014) (HeadOnly). Not surprisingly, the HeadOnly model performs poorly (F1 score = 14.30%), showing the importance of our rich binary feature set. Among all the features templates, removing HeadEmb results in the largest degradation. The second most im- ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 110,
                        "end": 111,
                        "text": "5",
                        "ref_id": "TABREF5"
                    },
                    {
                        "start": 173,
                        "end": 174,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Effects of the embedding sub-models",
                "sec_num": "7.1"
            },
            {
                "text": "Good word embeddings are critical for both FCM and other compositional models. In this section, we show the results of FCM with embeddings used to initialize other recent state-of-the-art models. Those embeddings include the 300-d baseline embeddings trained on English Wikipedia (w2venwiki-d300) and the 100-d task-specific embeddings (task-specific-d100)10 from the RelEmb paper (Hashimoto et al., 2015) , the 400-d embeddings from the CR-CNN paper (dos Santos et al., 2015) . Moreover, we list the best result (DepNN) in Liu et al. (2015) , which uses the same embeddings as ours. Table 6 shows the effects of word embeddings on FCM and provides relative comparisons between FCM and the other state-of-the-art models. We use the same hyperparameters and number of iterations in Table 4 .",
                "cite_spans": [
                    {
                        "start": 381,
                        "end": 405,
                        "text": "(Hashimoto et al., 2015)",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 451,
                        "end": 476,
                        "text": "(dos Santos et al., 2015)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 524,
                        "end": 541,
                        "text": "Liu et al. (2015)",
                        "ref_id": "BIBREF13"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 590,
                        "end": 591,
                        "text": "6",
                        "ref_id": "TABREF6"
                    },
                    {
                        "start": 787,
                        "end": 788,
                        "text": "4",
                        "ref_id": "TABREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Effects of the word embeddings",
                "sec_num": "7.2"
            },
            {
                "text": "The results show that using different embeddings to initialize FCM can improve F1 beyond our previous results. We also find that increasing the dimension of the word embeddings does not necessarily lead to better results due to the problem of over-fitting (e.g.w2v-enwiki-d400 vs. w2v-enwiki-d300). With the same initial embeddings, FCM usually gets better results without any changes to the hyperparameters than the competing model, further confirming the advantage of FCM at the model-level as discussed under Table 4. The only exception is the DepNN model, which gets better result than FCM on the same embeddings. The task-specific embeddings from (Hashimoto et al., 2015) leads to the best performance (an improvement of 0.7%). This observa- ",
                "cite_spans": [
                    {
                        "start": 652,
                        "end": 676,
                        "text": "(Hashimoto et al., 2015)",
                        "ref_id": "BIBREF5"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Effects of the word embeddings",
                "sec_num": "7.2"
            },
            {
                "text": "Compositional Models for Sentences In order to build a representation (embedding) for a sentence based on its component word embeddings and structural information, recent work on compositional models (stemming from the deep learning community) has designed model structures that mimic the structure of the input. For example, these models could take into account the order of the words (as in Convolutional Neural Networks (CNNs)) (Collobert et al., 2011) or build off of an input tree (as in Recursive Neural Networks (RNNs) or the Semantic Matching Energy Function) (Socher et al., 2013b; Bordes et al., 2012) .",
                "cite_spans": [
                    {
                        "start": 431,
                        "end": 455,
                        "text": "(Collobert et al., 2011)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 568,
                        "end": 590,
                        "text": "(Socher et al., 2013b;",
                        "ref_id": null
                    },
                    {
                        "start": 591,
                        "end": 611,
                        "text": "Bordes et al., 2012)",
                        "ref_id": "BIBREF1"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "8"
            },
            {
                "text": "While these models work well on sentence-level representations, the nature of their designs also limits them to fixed types of substructures from the annotated sentence, such as chains for CNNs and trees for RNNs. Such models cannot capture arbitrary combinations of linguistic annotations available for a given task, such as word order, dependency tree, and named entities used for relation extraction. Moreover, these approaches ignore the differences in functions between words appearing in different roles. This does not suit more general substructure labeling tasks in NLP, e.g. these models cannot be directly applied to relation extraction since they will output the same result for any pair of entities in a same sentence.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "8"
            },
            {
                "text": "Compositional Models with Annotation Features To tackle the problem of traditional compositional models, Socher et al. (2012) made the RNN model specific to relation extraction tasks by working on the minimal sub-tree which spans the two target entities. However, these specializations to relation extraction does not generalize easily to other tasks in NLP. There are two ways to achieve such specialization in a more general fashion:",
                "cite_spans": [
                    {
                        "start": 105,
                        "end": 125,
                        "text": "Socher et al. (2012)",
                        "ref_id": "BIBREF27"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "8"
            },
            {
                "text": "1. Enhancing Compositional Models with Features. A recent trend enhances compositional models with annotation features. Such an approach has been shown to significantly improve over pure compositional models. For example, Hermann et al. ( 2014) and Nguyen and Grishman (2014) gave different weights to words with different syntactic context types or to entity head words with different argument IDs. Zeng et al. (2014) use concatenations of embeddings as features in a CNN model, according to their positions relative to the target entity mentions. Belinkov et al. (2014) enrich embeddings with linguistic features before feeding them forward to a RNN model. Socher et al. (2013a) and Hermann and Blunsom (2013) enhanced RNN models by refining the transformation matrices with phrase types and CCG super tags.",
                "cite_spans": [
                    {
                        "start": 400,
                        "end": 418,
                        "text": "Zeng et al. (2014)",
                        "ref_id": "BIBREF36"
                    },
                    {
                        "start": 549,
                        "end": 571,
                        "text": "Belinkov et al. (2014)",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 659,
                        "end": 680,
                        "text": "Socher et al. (2013a)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "8"
            },
            {
                "text": "2. Engineering of Embedding Features. A different approach to combining traditional linguistic features and embeddings is hand-engineering features with word embeddings and adding them to log-linear models. Such approaches have achieved state-of-the-art results in many tasks including NER, chunking, dependency parsing, semantic role labeling, and relation extraction (Miller et al., 2004; Turian et al., 2010; Koo et al., 2008; Roth and Woodsend, 2014; Sun et al., 2011; Plank and Moschitti, 2013) . Roth and Woodsend (2014) considered features similar to ours for semantic role labeling.",
                "cite_spans": [
                    {
                        "start": 369,
                        "end": 390,
                        "text": "(Miller et al., 2004;",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 391,
                        "end": 411,
                        "text": "Turian et al., 2010;",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 412,
                        "end": 429,
                        "text": "Koo et al., 2008;",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 430,
                        "end": 454,
                        "text": "Roth and Woodsend, 2014;",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 455,
                        "end": 472,
                        "text": "Sun et al., 2011;",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 473,
                        "end": 499,
                        "text": "Plank and Moschitti, 2013)",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 502,
                        "end": 526,
                        "text": "Roth and Woodsend (2014)",
                        "ref_id": "BIBREF26"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "8"
            },
            {
                "text": "However, in prior work both of above approaches are only able to utilize limited information, usually one property for each word. Yet there may be different useful properties of a word which can contribute to the performances of the task. By contrast, our FCM can easily utilize these features without changing the model structures.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "8"
            },
            {
                "text": "In order to better utilize the dependency annotations, recently work built their models according to the dependency paths (Ma et al., 2015; Liu et al., 2015) , which share similar motivations to the usage of On-path features in our work.",
                "cite_spans": [
                    {
                        "start": 122,
                        "end": 139,
                        "text": "(Ma et al., 2015;",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 140,
                        "end": 157,
                        "text": "Liu et al., 2015)",
                        "ref_id": "BIBREF13"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "8"
            },
            {
                "text": "Task-Specific Enhancements for Relation Classification An orthogonal direction of improving compositional models for relation classification is to enhance the models with task-specific information. For example, Hashimoto et al. (2015) trained task-specific word embeddings, and dos Santos et al. (2015) proposed a ranking-based loss function for relation classification.",
                "cite_spans": [
                    {
                        "start": 211,
                        "end": 234,
                        "text": "Hashimoto et al. (2015)",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 282,
                        "end": 302,
                        "text": "Santos et al. (2015)",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "8"
            },
            {
                "text": "We have presented FCM, a new compositional model for deriving sentence-level and substructure embeddings from word embeddings. Compared to existing compositional models, FCM can easily handle arbitrary types of input and handle global information for composition, while remaining easy to implement. We have demonstrated that FCM alone attains near state-of-the-art performances on several relation extraction tasks, and in combination with traditional feature based loglinear models it obtains state-of-the-art results.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "9"
            },
            {
                "text": "Our next steps in improving FCM focus on enhancements based on task-specific embeddings or loss functions as in Hashimoto et al. (2015; dos Santos et al. (2015) . Moreover, as the model provides a general idea for representing both sentences and sub-structures in language, it has the potential to contribute useful components to various tasks, such as dependency parsing, SRL and paraphrasing. Also as kindly pointed out by one anonymous reviewer, our FCM can be applied to the TAC-KBP (Ji et al., 2010) tasks, by replacing the training objective to a multi-instance multilabel one (e.g. Surdeanu et al. (2012) ). We plan to explore the above applications of FCM in the future.",
                "cite_spans": [
                    {
                        "start": 112,
                        "end": 135,
                        "text": "Hashimoto et al. (2015;",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 136,
                        "end": 160,
                        "text": "dos Santos et al. (2015)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 487,
                        "end": 504,
                        "text": "(Ji et al., 2010)",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 589,
                        "end": 611,
                        "text": "Surdeanu et al. (2012)",
                        "ref_id": "BIBREF31"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "9"
            },
            {
                "text": "Such embeddings have a long history in NLP, including term-document frequency matrices and their lowdimensional counterparts obtained by linear algebra tools (LSA, PCA, CCA, NNMF), Brown clusters, random projections and vector space models. Recently, neural networks / deep learning have provided several popular methods for obtaining such embeddings.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Since the focus of this paper is relation extraction, we adopt the evaluation setting of prior work which uses gold named entities to better facilitate comparison.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "We use words as substructures for relation extraction, but use the general terminology to maintain model generality.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Obtained from the constituency parse using the CONLL 2000 chunking converter (Perl script).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Many relation extraction systems evaluate on the ACE 2004 corpus(Mitchell et al., 2005). Unfortunately, the most common convention is to use 5-fold cross validation, treating the entirety of the dataset as both train and evaluation data. Rather than continuing to overfit this data by perpetuating the cross-validation convention, we instead focus on ACE 2005.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "http://docs.google.com/View?docid=dfvxd49s_36c28v9pmw",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "In the task-specific setting, FCM will represent entity words and context words with separate sets of embeddings.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "We thank the anonymous reviewers for their comments, and Nicholas Andrews, Francis Ferraro, and Benjamin Van Durme for their input. We thank Kazuma Hashimoto, C\u00edcero Nogueira dos Santos, Bing Xiang and Bowen Zhou for sharing their word embeddings and many helpful discussions. Mo Yu is supported by the China Scholarship Council and by NSFC 61173073.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgments",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Exploring compositional architectures and word vector representations for prepositional phrase attachment",
                "authors": [
                    {
                        "first": "Yonatan",
                        "middle": [],
                        "last": "Belinkov",
                        "suffix": ""
                    },
                    {
                        "first": "Tao",
                        "middle": [],
                        "last": "Lei",
                        "suffix": ""
                    },
                    {
                        "first": "Regina",
                        "middle": [],
                        "last": "Barzilay",
                        "suffix": ""
                    },
                    {
                        "first": "Amir",
                        "middle": [],
                        "last": "Globerson",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Transactions of the Association for Computational Linguistics",
                "volume": "2",
                "issue": "",
                "pages": "561--572",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yonatan Belinkov, Tao Lei, Regina Barzilay, and Amir Globerson. 2014. Exploring compositional archi- tectures and word vector representations for prepo- sitional phrase attachment. Transactions of the As- sociation for Computational Linguistics, 2:561-572.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "A semantic matching energy function for learning with multi-relational data",
                "authors": [
                    {
                        "first": "Antoine",
                        "middle": [],
                        "last": "Bordes",
                        "suffix": ""
                    },
                    {
                        "first": "Xavier",
                        "middle": [],
                        "last": "Glorot",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Weston",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Machine Learning",
                "volume": "",
                "issue": "",
                "pages": "1--27",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Antoine Bordes, Xavier Glorot, Jason Weston, and Yoshua Bengio. 2012. A semantic matching en- ergy function for learning with multi-relational data. Machine Learning, pages 1-27.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger",
                "authors": [
                    {
                        "first": "Massimiliano",
                        "middle": [],
                        "last": "Ciaramita",
                        "suffix": ""
                    },
                    {
                        "first": "Yasemin",
                        "middle": [],
                        "last": "Altun",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "EMNLP2006",
                "volume": "",
                "issue": "",
                "pages": "594--602",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Massimiliano Ciaramita and Yasemin Altun. 2006. Broad-coverage sense disambiguation and informa- tion extraction with a supersense sequence tagger. In EMNLP2006, pages 594-602, July.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Natural language processing (almost) from scratch",
                "authors": [
                    {
                        "first": "Ronan",
                        "middle": [],
                        "last": "Collobert",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Weston",
                        "suffix": ""
                    },
                    {
                        "first": "L\u00e9on",
                        "middle": [],
                        "last": "Bottou",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Karlen",
                        "suffix": ""
                    },
                    {
                        "first": "Koray",
                        "middle": [],
                        "last": "Kavukcuoglu",
                        "suffix": ""
                    },
                    {
                        "first": "Pavel",
                        "middle": [],
                        "last": "Kuksa",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "JMLR",
                "volume": "12",
                "issue": "",
                "pages": "2493--2537",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ronan Collobert, Jason Weston, L\u00e9on Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. JMLR, 12:2493-2537.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Classifying relations by ranking with convolutional neural networks",
                "authors": [
                    {
                        "first": "Bing",
                        "middle": [],
                        "last": "Cicero Dos Santos",
                        "suffix": ""
                    },
                    {
                        "first": "Bowen",
                        "middle": [],
                        "last": "Xiang",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "626--634",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Cicero dos Santos, Bing Xiang, and Bowen Zhou. 2015. Classifying relations by ranking with con- volutional neural networks. In Proceedings of the 53rd Annual Meeting of the Association for Compu- tational Linguistics and the 7th International Joint Conference on Natural Language Processing (Vol- ume 1: Long Papers), pages 626-634, Beijing, China, July. Association for Computational Linguis- tics.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Task-oriented learning of word embeddings for semantic relation classification",
                "authors": [
                    {
                        "first": "Kazuma",
                        "middle": [],
                        "last": "Hashimoto",
                        "suffix": ""
                    },
                    {
                        "first": "Pontus",
                        "middle": [],
                        "last": "Stenetorp",
                        "suffix": ""
                    },
                    {
                        "first": "Makoto",
                        "middle": [],
                        "last": "Miwa",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshimasa",
                        "middle": [],
                        "last": "Tsuruoka",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1503.00095"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Kazuma Hashimoto, Pontus Stenetorp, Makoto Miwa, and Yoshimasa Tsuruoka. 2015. Task-oriented learning of word embeddings for semantic relation classification. arXiv preprint arXiv:1503.00095.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Multi-way classification of semantic relations between pairs of nominals",
                "authors": [],
                "year": null,
                "venue": "Proceedings of SemEval-2 Workshop",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Multi-way classification of semantic relations between pairs of nominals. In Proceedings of SemEval-2 Workshop.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "The role of syntax in vector space models of compositional semantics",
                "authors": [
                    {
                        "first": "Karl",
                        "middle": [],
                        "last": "Moritz",
                        "suffix": ""
                    },
                    {
                        "first": "Hermann",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Phil",
                        "middle": [],
                        "last": "Blunsom",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "894--904",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Karl Moritz Hermann and Phil Blunsom. 2013. The role of syntax in vector space models of composi- tional semantics. In Association for Computational Linguistics, pages 894-904.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Semantic frame identification with distributed word representations",
                "authors": [
                    {
                        "first": "Karl",
                        "middle": [],
                        "last": "Moritz Hermann",
                        "suffix": ""
                    },
                    {
                        "first": "Dipanjan",
                        "middle": [],
                        "last": "Das",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Weston",
                        "suffix": ""
                    },
                    {
                        "first": "Kuzman",
                        "middle": [],
                        "last": "Ganchev",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics",
                "volume": "1",
                "issue": "",
                "pages": "1448--1458",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Karl Moritz Hermann, Dipanjan Das, Jason Weston, and Kuzman Ganchev. 2014. Semantic frame iden- tification with distributed word representations. In Proceedings of the 52nd Annual Meeting of the As- sociation for Computational Linguistics (Volume 1: Long Papers), pages 1448-1458, Baltimore, Mary- land, June. Association for Computational Linguis- tics.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Overview of the tac 2010 knowledge base population track",
                "authors": [
                    {
                        "first": "Ji",
                        "middle": [],
                        "last": "Heng",
                        "suffix": ""
                    },
                    {
                        "first": "Ralph",
                        "middle": [],
                        "last": "Grishman",
                        "suffix": ""
                    },
                    {
                        "first": "Hoa",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Trang",
                        "middle": [],
                        "last": "Dang",
                        "suffix": ""
                    },
                    {
                        "first": "Kira",
                        "middle": [],
                        "last": "Griffitt",
                        "suffix": ""
                    },
                    {
                        "first": "Joe",
                        "middle": [],
                        "last": "Ellis",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Third Text Analysis Conference",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Heng Ji, Ralph Grishman, Hoa Trang Dang, Kira Grif- fitt, and Joe Ellis. 2010. Overview of the tac 2010 knowledge base population track. In Third Text Analysis Conference (TAC 2010).",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Simple semi-supervised dependency parsing",
                "authors": [
                    {
                        "first": "Terry",
                        "middle": [],
                        "last": "Koo",
                        "suffix": ""
                    },
                    {
                        "first": "Xavier",
                        "middle": [],
                        "last": "Carreras",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Collins",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Proceedings of ACL-08: HLT",
                "volume": "",
                "issue": "",
                "pages": "595--603",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing. In Proceedings of ACL-08: HLT, pages 595-603, Columbus, Ohio, June. Association for Computa- tional Linguistics.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Incremental joint extraction of entity mentions and relations",
                "authors": [
                    {
                        "first": "Qi",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Heng",
                        "middle": [],
                        "last": "Ji",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics",
                "volume": "1",
                "issue": "",
                "pages": "402--412",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Qi Li and Heng Ji. 2014. Incremental joint extrac- tion of entity mentions and relations. In Proceed- ings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa- pers), pages 402-412, Baltimore, Maryland, June. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "A dependency-based neural network for relation classification",
                "authors": [
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Furu",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Sujian",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Heng",
                        "middle": [],
                        "last": "Ji",
                        "suffix": ""
                    },
                    {
                        "first": "Ming",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Wang",
                        "middle": [],
                        "last": "Houfeng",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing",
                "volume": "2",
                "issue": "",
                "pages": "285--290",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yang Liu, Furu Wei, Sujian Li, Heng Ji, Ming Zhou, and Houfeng WANG. 2015. A dependency-based neural network for relation classification. In Pro- ceedings of the 53rd Annual Meeting of the Associ- ation for Computational Linguistics and the 7th In- ternational Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 285- 290, Beijing, China, July. Association for Computa- tional Linguistics.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Dependency-based convolutional neural networks for sentence embedding",
                "authors": [
                    {
                        "first": "Mingbo",
                        "middle": [],
                        "last": "Ma",
                        "suffix": ""
                    },
                    {
                        "first": "Liang",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Bowen",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Bing",
                        "middle": [],
                        "last": "Xiang",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing",
                "volume": "2",
                "issue": "",
                "pages": "174--179",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mingbo Ma, Liang Huang, Bowen Zhou, and Bing Xi- ang. 2015. Dependency-based convolutional neural networks for sentence embedding. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 174-179, Beijing, China, July. Association for Computational Linguis- tics.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "The Stanford CoreNLP natural language processing toolkit",
                "authors": [
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Christopher",
                        "suffix": ""
                    },
                    {
                        "first": "Mihai",
                        "middle": [],
                        "last": "Manning",
                        "suffix": ""
                    },
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Surdeanu",
                        "suffix": ""
                    },
                    {
                        "first": "Jenny",
                        "middle": [],
                        "last": "Bauer",
                        "suffix": ""
                    },
                    {
                        "first": "Steven",
                        "middle": [
                            "J"
                        ],
                        "last": "Finkel",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Bethard",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Mc-Closky",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations",
                "volume": "",
                "issue": "",
                "pages": "55--60",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David Mc- Closky. 2014. The Stanford CoreNLP natural lan- guage processing toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computa- tional Linguistics: System Demonstrations, pages 55-60.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Distributed representations of words and phrases and their compositionality",
                "authors": [
                    {
                        "first": "Tomas",
                        "middle": [],
                        "last": "Mikolov",
                        "suffix": ""
                    },
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    },
                    {
                        "first": "Kai",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Greg",
                        "middle": [],
                        "last": "Corrado",
                        "suffix": ""
                    },
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Dean",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1310.4546"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor- rado, and Jeffrey Dean. 2013. Distributed represen- tations of words and phrases and their composition- ality. arXiv preprint arXiv:1310.4546.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Name tagging with word clusters and discriminative training",
                "authors": [
                    {
                        "first": "Scott",
                        "middle": [],
                        "last": "Miller",
                        "suffix": ""
                    },
                    {
                        "first": "Jethran",
                        "middle": [],
                        "last": "Guinness",
                        "suffix": ""
                    },
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Zamanian",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "HLT-NAACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Scott Miller, Jethran Guinness, and Alex Zamanian. 2004. Name tagging with word clusters and dis- criminative training. In Susan Dumais, Daniel Marcu, and Salim Roukos, editors, HLT-NAACL 2004: Main Proceedings. Association for Compu- tational Linguistics.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Ace 2004 multilingual training corpus. Linguistic Data Consortium",
                "authors": [
                    {
                        "first": "Alexis",
                        "middle": [],
                        "last": "Mitchell",
                        "suffix": ""
                    },
                    {
                        "first": "Stephanie",
                        "middle": [],
                        "last": "Strassel",
                        "suffix": ""
                    },
                    {
                        "first": "Shudong",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Ramez",
                        "middle": [],
                        "last": "Zakhary",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alexis Mitchell, Stephanie Strassel, Shudong Huang, and Ramez Zakhary. 2005. Ace 2004 multilin- gual training corpus. Linguistic Data Consortium, Philadelphia.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Three new graphical models for statistical language modelling",
                "authors": [
                    {
                        "first": "Andriy",
                        "middle": [],
                        "last": "Mnih",
                        "suffix": ""
                    },
                    {
                        "first": "Geoffrey",
                        "middle": [],
                        "last": "Hinton",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Proceedings of the 24th international conference on Machine learning",
                "volume": "",
                "issue": "",
                "pages": "641--648",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Andriy Mnih and Geoffrey Hinton. 2007. Three new graphical models for statistical language modelling. In Proceedings of the 24th international conference on Machine learning, pages 641-648. ACM.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Employing word representations and regularization for domain adaptation of relation extraction",
                "authors": [
                    {
                        "first": "Huu",
                        "middle": [],
                        "last": "Thien",
                        "suffix": ""
                    },
                    {
                        "first": "Ralph",
                        "middle": [],
                        "last": "Nguyen",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Grishman",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics",
                "volume": "2",
                "issue": "",
                "pages": "68--74",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Thien Huu Nguyen and Ralph Grishman. 2014. Em- ploying word representations and regularization for domain adaptation of relation extraction. In Pro- ceedings of the 52nd Annual Meeting of the Associa- tion for Computational Linguistics (Volume 2: Short Papers), pages 68-74, Baltimore, Maryland, June. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Relation extraction: Perspective from convolutional neural networks",
                "authors": [
                    {
                        "first": "Huu",
                        "middle": [],
                        "last": "Thien",
                        "suffix": ""
                    },
                    {
                        "first": "Ralph",
                        "middle": [],
                        "last": "Nguyen",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Grishman",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of NAACL Workshop on Vector Space Modeling for NLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Thien Huu Nguyen and Ralph Grishman. 2015. Rela- tion extraction: Perspective from convolutional neu- ral networks. In Proceedings of NAACL Workshop on Vector Space Modeling for NLP.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Semantic representations for domain adaptation: A case study on the tree kernelbased method for relation extraction",
                "authors": [
                    {
                        "first": "Thien",
                        "middle": [],
                        "last": "Huu Nguyen",
                        "suffix": ""
                    },
                    {
                        "first": "Barbara",
                        "middle": [],
                        "last": "Plank",
                        "suffix": ""
                    },
                    {
                        "first": "Ralph",
                        "middle": [],
                        "last": "Grishman",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing",
                "volume": "1",
                "issue": "",
                "pages": "635--644",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Thien Huu Nguyen, Barbara Plank, and Ralph Gr- ishman. 2015. Semantic representations for do- main adaptation: A case study on the tree kernel- based method for relation extraction. In Proceed- ings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th Interna- tional Joint Conference on Natural Language Pro- cessing (Volume 1: Long Papers), pages 635-644, Beijing, China, July. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "English gigaword fifth edition, june. Linguistic Data Consortium",
                "authors": [
                    {
                        "first": "Robert",
                        "middle": [],
                        "last": "Parker",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Graff",
                        "suffix": ""
                    },
                    {
                        "first": "Junbo",
                        "middle": [],
                        "last": "Kong",
                        "suffix": ""
                    },
                    {
                        "first": "Ke",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Kazuaki",
                        "middle": [],
                        "last": "Maeda",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Robert Parker, David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. 2011. English gigaword fifth edition, june. Linguistic Data Consortium, LDC2011T07.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Embedding semantic similarity in tree kernels for domain adaptation of relation extraction",
                "authors": [
                    {
                        "first": "Barbara",
                        "middle": [],
                        "last": "Plank",
                        "suffix": ""
                    },
                    {
                        "first": "Alessandro",
                        "middle": [],
                        "last": "Moschitti",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics",
                "volume": "1",
                "issue": "",
                "pages": "1498--1507",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Barbara Plank and Alessandro Moschitti. 2013. Em- bedding semantic similarity in tree kernels for do- main adaptation of relation extraction. In Proceed- ings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa- pers), pages 1498-1507, Sofia, Bulgaria, August. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Utd: Classifying semantic relations by combining lexical and semantic resources",
                "authors": [
                    {
                        "first": "Bryan",
                        "middle": [],
                        "last": "Rink",
                        "suffix": ""
                    },
                    {
                        "first": "Sanda",
                        "middle": [],
                        "last": "Harabagiu",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the 5th International Workshop on Semantic Evaluation",
                "volume": "",
                "issue": "",
                "pages": "256--259",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Bryan Rink and Sanda Harabagiu. 2010. Utd: Clas- sifying semantic relations by combining lexical and semantic resources. In Proceedings of the 5th Inter- national Workshop on Semantic Evaluation, pages 256-259, Uppsala, Sweden, July. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Composition of word representations improves semantic role labelling",
                "authors": [
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Roth",
                        "suffix": ""
                    },
                    {
                        "first": "Kristian",
                        "middle": [],
                        "last": "Woodsend",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "EMNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Michael Roth and Kristian Woodsend. 2014. Com- position of word representations improves semantic role labelling. In EMNLP.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Semantic compositionality through recursive matrix-vector spaces",
                "authors": [
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    },
                    {
                        "first": "Brody",
                        "middle": [],
                        "last": "Huval",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [
                            "Y"
                        ],
                        "last": "Ng",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning",
                "volume": "",
                "issue": "",
                "pages": "1201--1211",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Richard Socher, Brody Huval, Christopher D. Man- ning, and Andrew Y. Ng. 2012. Semantic composi- tionality through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference on Empir- ical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1201-1211, Jeju Island, Korea, July. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Parsing with compositional vector grammars",
                "authors": [
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    },
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Bauer",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [
                            "Y"
                        ],
                        "last": "Ng",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of the ACL conference",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Richard Socher, John Bauer, Christopher D Manning, and Andrew Y Ng. 2013a. Parsing with compo- sitional vector grammars. In In Proceedings of the ACL conference. Citeseer.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
                "authors": [
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    },
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Perelygin",
                        "suffix": ""
                    },
                    {
                        "first": "Jean",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Chuang",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Ng",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [],
                        "last": "Potts",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "1631--1642",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013b. Recursive deep models for semantic compositionality over a sentiment tree- bank. In Empirical Methods in Natural Language Processing, pages 1631-1642.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "Semi-supervised relation extraction with large-scale word clustering",
                "authors": [
                    {
                        "first": "Ang",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Ralph",
                        "middle": [],
                        "last": "Grishman",
                        "suffix": ""
                    },
                    {
                        "first": "Satoshi",
                        "middle": [],
                        "last": "Sekine",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies",
                "volume": "",
                "issue": "",
                "pages": "521--529",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ang Sun, Ralph Grishman, and Satoshi Sekine. 2011. Semi-supervised relation extraction with large-scale word clustering. In Proceedings of the 49th An- nual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 521-529, Portland, Oregon, USA, June. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "Multi-instance multi-label learning for relation extraction",
                "authors": [
                    {
                        "first": "Mihai",
                        "middle": [],
                        "last": "Surdeanu",
                        "suffix": ""
                    },
                    {
                        "first": "Julie",
                        "middle": [],
                        "last": "Tibshirani",
                        "suffix": ""
                    },
                    {
                        "first": "Ramesh",
                        "middle": [],
                        "last": "Nallapati",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning",
                "volume": "",
                "issue": "",
                "pages": "455--465",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati, and Christopher D Manning. 2012. Multi-instance multi-label learning for relation extraction. In Pro- ceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Com- putational Natural Language Learning, pages 455- 465. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "Word representations: a simple and general method for semi-supervised learning",
                "authors": [
                    {
                        "first": "Joseph",
                        "middle": [],
                        "last": "Turian",
                        "suffix": ""
                    },
                    {
                        "first": "Lev",
                        "middle": [],
                        "last": "Ratinov",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "384--394",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Association for Computational Linguistics, pages 384-394.",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "ACE 2005 multilingual training corpus. Linguistic Data Consortium",
                "authors": [
                    {
                        "first": "Christopher",
                        "middle": [],
                        "last": "Walker",
                        "suffix": ""
                    },
                    {
                        "first": "Stephanie",
                        "middle": [],
                        "last": "Strassel",
                        "suffix": ""
                    },
                    {
                        "first": "Julie",
                        "middle": [],
                        "last": "Medero",
                        "suffix": ""
                    },
                    {
                        "first": "Kazuaki",
                        "middle": [],
                        "last": "Maeda",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Christopher Walker, Stephanie Strassel, Julie Medero, and Kazuaki Maeda. 2006. ACE 2005 multilin- gual training corpus. Linguistic Data Consortium, Philadelphia.",
                "links": null
            },
            "BIBREF34": {
                "ref_id": "b34",
                "title": "Learning composition models for phrase embeddings",
                "authors": [
                    {
                        "first": "Mo",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Dredze",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Transactions of the Association for Computational Linguistics",
                "volume": "3",
                "issue": "",
                "pages": "227--242",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mo Yu and Mark Dredze. 2015. Learning composition models for phrase embeddings. Transactions of the Association for Computational Linguistics, 3:227- 242.",
                "links": null
            },
            "BIBREF35": {
                "ref_id": "b35",
                "title": "Combining word embeddings and feature embeddings for fine-grained relation extraction",
                "authors": [
                    {
                        "first": "Mo",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Matthew",
                        "middle": [
                            "R"
                        ],
                        "last": "Gormley",
                        "suffix": ""
                    },
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Dredze",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of NAACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mo Yu, Matthew R. Gormley, and Mark Dredze. 2015. Combining word embeddings and feature embed- dings for fine-grained relation extraction. In Pro- ceedings of NAACL.",
                "links": null
            },
            "BIBREF36": {
                "ref_id": "b36",
                "title": "Relation classification via convolutional deep neural network",
                "authors": [
                    {
                        "first": "Daojian",
                        "middle": [],
                        "last": "Zeng",
                        "suffix": ""
                    },
                    {
                        "first": "Kang",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Siwei",
                        "middle": [],
                        "last": "Lai",
                        "suffix": ""
                    },
                    {
                        "first": "Guangyou",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Jun",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers",
                "volume": "",
                "issue": "",
                "pages": "2335--2344",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou, and Jun Zhao. 2014. Relation classification via con- volutional deep neural network. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 2335-2344, Dublin, Ireland, August. Dublin City University and Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF37": {
                "ref_id": "b37",
                "title": "Exploring various knowledge in relation extraction",
                "authors": [
                    {
                        "first": "Guodong",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Jian",
                        "middle": [],
                        "last": "Su",
                        "suffix": ""
                    },
                    {
                        "first": "Jie",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Min",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "427--434",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "GuoDong Zhou, Jian Su, Jie Zhang, and Min Zhang. 2005. Exploring various knowledge in relation ex- traction. In Association for Computational Linguis- tics, pages 427-434.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "of an input sentence. (b) roduct between the feature se a tensor T = L\u2326E \u2326F he set of labels, E refers to ture S, we have (2) e we decompose the strucon the model parameters. is a matrix (y, \u2022, \u2022). Then (3) the equivalent form:",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "entation of the FCT model. (a) Representation of an input sentence. (b) rameter space. s, we can represent each factor as the outer product between the feature er of transformed embedding g f \u2326h f . The we use a tensor T = L\u2326E \u2326F sform this input matrix to the labels. Here L is the set of labels, E refers to layer (|E| = 200) and F is the set of features. nditional probability of a label y given the structure S, we have P (y|S; T ) = exp{s(y, S; T )} P y 2L exp{s(y , S; T )} ,",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "Figure1: Example construction of substructure embeddings. Each substructure is a word wi in S, augmented by the target entity information and related information from annotation A (e.g. a dependency tree). We show the factorization of the annotated sentence into substructures (left), the concatenation of the substructure embeddings for the sentence (middle), and a single substructure embedding from that concatenation (right). The annotated sentence embedding (not shown) would be the sum of the substructure embeddings, as opposed to their concatenation.",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table><tr><td>Class</td><td>M1</td><td>M2</td><td>Sentence Snippet</td></tr><tr><td>(1) ART(M1,M2)</td><td>a man</td><td>a taxicab</td><td>A man driving what appeared to be a taxicab</td></tr><tr><td colspan=\"2\">(2) PART-WHOLE(M1,M2) the southern suburbs</td><td>Baghdad</td><td>direction of the southern suburbs of Baghdad</td></tr><tr><td>(3) PHYSICAL(M2,M1)</td><td>the united states</td><td>284 people</td><td>in the united states , 284 people died</td></tr></table>",
                "type_str": "table",
                "text": "are cap-Examples from ACE 2005. In (1) the word \"driving\" is a strong indicator of the relation ART 3 between M1 and M2.",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table><tr><td/><td/><td>bc</td><td/><td/><td>cts</td><td/><td/><td>wl</td><td/><td>Avg.</td></tr><tr><td>Model</td><td>P</td><td>R</td><td>F1</td><td>P</td><td>R</td><td>F1</td><td>P</td><td>R</td><td>F1</td><td>F1</td></tr><tr><td>(1)</td><td/><td/><td/><td/><td/><td/><td/><td/><td/><td/></tr></table>",
                "type_str": "table",
                "text": "and also FCM only (ST) 66.56 57.86 61.90 65.62 44.35 52.93 57.80 44.62 50.36 55.06 (3) Baseline (ST) 74.89 48.54 58.90 74.32 40.26 52.23 63.41 43.20 51.39 54.17 (4) + HeadOnly (ST) 70.87 50.76 59.16 71.16 43.21 53.77 57.71 42.92 49.23 54.05 (5) + FCM (ST) 74.39 55.35 63.48 74.53 45.01 56.12 65.63 47.59 55.17 58.26",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table><tr><td>Classifier</td><td>Features</td><td>F1</td></tr><tr><td>SVM (Rink and Harabagiu, 2010)</td><td>POS, prefixes, morphological, WordNet, dependency parse,</td><td/></tr><tr><td>(Best in SemEval2010)</td><td>Levin classed, ProBank, FrameNet, NomLex-Plus,</td><td>82.2</td></tr><tr><td/><td>Google n-gram, paraphrases, TextRunner</td><td/></tr><tr><td>RNN</td><td>word embedding, syntactic parse</td><td>74.8</td></tr><tr><td>RNN + linear</td><td>word embedding, syntactic parse, POS, NER, WordNet</td><td>77.6</td></tr><tr><td>MVRNN</td><td>word embedding, syntactic parse</td><td>79.1</td></tr><tr><td>MVRNN + linear</td><td>word embedding, syntactic parse, POS, NER, WordNet</td><td>82.4</td></tr><tr><td>CNN (Zeng et al., 2014)</td><td>word embedding, WordNet</td><td>82.7</td></tr><tr><td>CR-CNN (log-loss)</td><td>word embedding</td><td>82.7</td></tr><tr><td>CR-CNN (ranking-loss)</td><td>word embedding</td><td>84.1</td></tr><tr><td>RelEmb (word2vec embedding)</td><td>word embedding</td><td>81.8</td></tr><tr><td>RelEmb (task-spec embedding)</td><td>word embedding</td><td>82.8</td></tr><tr><td colspan=\"2\">RelEmb (task-spec embedding) + linear word embedding, dependency paths, WordNet, NE</td><td>83.5</td></tr><tr><td>DepNN</td><td>word embedding, dependency paths</td><td>82.8</td></tr><tr><td>DepNN + linear</td><td>word embedding, dependency paths, WordNet, NER</td><td>83.6</td></tr><tr><td>(1) FCM (log-linear)</td><td>word embedding, dependency parse, WordNet word embedding, dependency parse, NER</td><td>82.0 81.4</td></tr><tr><td>(2) FCM (log-bilinear)</td><td>word embedding, dependency parse, WordNet word embedding, dependency parse, NER</td><td>82.5 83.0</td></tr><tr><td>(5) FCM (log-linear) + linear (Hybrid)</td><td>word embedding, dependency parse, WordNet word embedding, dependency parse, NER</td><td>83.1 83.4</td></tr></table>",
                "type_str": "table",
                "text": "Comparison of models on ACE 2005 out-of-domain test sets. Baseline + HeadOnly is our reimplementation of the features of Nguyen and Grishman (2014).",
                "html": null,
                "num": null
            },
            "TABREF4": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Comparison of FCM with previously published results for SemEval 2010 Task 8.",
                "html": null,
                "num": null
            },
            "TABREF5": {
                "content": "<table><tr><td>Feature Set</td><td>Prec</td><td>Rec</td><td>F1</td></tr><tr><td>HeadOnly</td><td>31.67</td><td>9.24</td><td>14.30</td></tr><tr><td>FCM</td><td colspan=\"3\">69.17 56.73 62.33</td></tr><tr><td>-HeadEmb</td><td colspan=\"3\">66.06 47.00 54.92</td></tr><tr><td>-Context</td><td colspan=\"3\">70.89 55.27 62.11</td></tr><tr><td>-In-between</td><td colspan=\"3\">66.39 51.86 58.23</td></tr><tr><td>-On-path</td><td colspan=\"3\">69.23 53.97 60.66</td></tr><tr><td>FCM-EntityTypes</td><td colspan=\"3\">71.33 34.68 46.67</td></tr></table>",
                "type_str": "table",
                "text": "Ablation test of FCM on development set. portant feature template is In-between, while Context features have little impact. Removing all entity type features (t h i ) does significantly worse than the full model, showing the value of our entity type features.",
                "html": null,
                "num": null
            },
            "TABREF6": {
                "content": "<table><tr><td>Embeddings</td><td>Model</td><td>F1</td></tr><tr><td>w2v-enwiki-d300</td><td colspan=\"2\">RelEmb (2) FCM (log-bilinear) 83.4 81.8</td></tr><tr><td/><td>RelEmb</td><td>82.8</td></tr><tr><td>task-specific-d100</td><td>RelEmb+linear</td><td>83.5</td></tr><tr><td/><td colspan=\"2\">(2) FCM (log-bilinear) 83.7</td></tr><tr><td>w2v-enwiki-d400</td><td colspan=\"2\">CR-CNN (2) FCM (log-bilinear) 83.0 82.7</td></tr><tr><td>w2v-nyt-d200</td><td colspan=\"2\">DepNN (2) FCM (log-bilinear) 83.0 83.6</td></tr></table>",
                "type_str": "table",
                "text": "Evaluation of FCMs with different word embeddings on SemEval 2010 Task 8.tion suggests that the other compositional models may also benefit from the work ofHashimoto et al. (2015).",
                "html": null,
                "num": null
            }
        }
    }
}