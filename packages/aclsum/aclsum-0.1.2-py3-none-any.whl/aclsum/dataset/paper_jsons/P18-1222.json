{
    "paper_id": "P18-1222",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:22:49.801180Z"
    },
    "title": "hyperdoc2vec: Distributed Representations of Hypertext Documents",
    "authors": [
        {
            "first": "Jialong",
            "middle": [],
            "last": "Han",
            "suffix": "",
            "affiliation": {
                "laboratory": "Tencent AI Lab School of Information",
                "institution": "Renmin University of China",
                "location": {}
            },
            "email": "jialonghan@gmail.com"
        },
        {
            "first": "Yan",
            "middle": [],
            "last": "Song",
            "suffix": "",
            "affiliation": {
                "laboratory": "Tencent AI Lab School of Information",
                "institution": "Renmin University of China",
                "location": {}
            },
            "email": ""
        },
        {
            "first": "Wayne",
            "middle": [
                "Xin"
            ],
            "last": "Zhao",
            "suffix": "",
            "affiliation": {
                "laboratory": "Tencent AI Lab School of Information",
                "institution": "Renmin University of China",
                "location": {}
            },
            "email": ""
        },
        {
            "first": "Shuming",
            "middle": [],
            "last": "Shi",
            "suffix": "",
            "affiliation": {
                "laboratory": "Tencent AI Lab School of Information",
                "institution": "Renmin University of China",
                "location": {}
            },
            "email": "shumingshi@tencent.com"
        },
        {
            "first": "Haisong",
            "middle": [],
            "last": "Zhang",
            "suffix": "",
            "affiliation": {
                "laboratory": "Tencent AI Lab School of Information",
                "institution": "Renmin University of China",
                "location": {}
            },
            "email": "hansonzhang@tencent.com"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Hypertext documents, such as web pages and academic papers, are of great importance in delivering information in our daily life. Although being effective on plain documents, conventional text embedding methods suffer from information loss if directly adapted to hyper-documents. In this paper, we propose a general embedding approach for hyper-documents, namely, hyperdoc2vec, along with four criteria characterizing necessary information that hyper-document embedding models should preserve. Systematic comparisons are conducted between hyperdoc2vec and several competitors on two tasks, i.e., paper classification and citation recommendation, in the academic paper domain. Analyses and experiments both validate the superiority of hyperdoc2vec to other models w.r.t. the four criteria.",
    "pdf_parse": {
        "paper_id": "P18-1222",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Hypertext documents, such as web pages and academic papers, are of great importance in delivering information in our daily life. Although being effective on plain documents, conventional text embedding methods suffer from information loss if directly adapted to hyper-documents. In this paper, we propose a general embedding approach for hyper-documents, namely, hyperdoc2vec, along with four criteria characterizing necessary information that hyper-document embedding models should preserve. Systematic comparisons are conducted between hyperdoc2vec and several competitors on two tasks, i.e., paper classification and citation recommendation, in the academic paper domain. Analyses and experiments both validate the superiority of hyperdoc2vec to other models w.r.t. the four criteria.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "The ubiquitous World Wide Web has boosted research interests on hypertext documents, e.g., personal webpages (Lu and Getoor, 2003) , Wikipedia pages (Gabrilovich and Markovitch, 2007) , as well as academic papers (Sugiyama and Kan, 2010) . Unlike independent plain documents, a hypertext document (hyper-doc for short) links to another hyper-doc by a hyperlink or citation mark in its textual content. Given this essential distinction, hyperlinks or citations are worth specific modeling in many tasks such as link-based classification (Lu and Getoor, 2003) , web retrieval (Page et al., 1999) , entity linking (Cucerzan, 2007) , and citation recommendation (He et al., 2010) .",
                "cite_spans": [
                    {
                        "start": 109,
                        "end": 130,
                        "text": "(Lu and Getoor, 2003)",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 149,
                        "end": 183,
                        "text": "(Gabrilovich and Markovitch, 2007)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 213,
                        "end": 237,
                        "text": "(Sugiyama and Kan, 2010)",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 536,
                        "end": 557,
                        "text": "(Lu and Getoor, 2003)",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 574,
                        "end": 593,
                        "text": "(Page et al., 1999)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 611,
                        "end": 627,
                        "text": "(Cucerzan, 2007)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 658,
                        "end": 675,
                        "text": "(He et al., 2010)",
                        "ref_id": "BIBREF10"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "To model hypertext documents, various efforts (Cohn and Hofmann, 2000; Kataria et al., 2010; Perozzi et al., 2014; Zwicklbauer et al., 2016; Wang et al., 2016) have been made to depict networks of hyper-docs as well as their content. Among potential techniques, distributed representation (Mikolov et al., 2013; Le and Mikolov, 2014) tends to be promising since its validity and effectiveness are proven for plain documents on many natural language processing (NLP) tasks.",
                "cite_spans": [
                    {
                        "start": 46,
                        "end": 70,
                        "text": "(Cohn and Hofmann, 2000;",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 71,
                        "end": 92,
                        "text": "Kataria et al., 2010;",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 93,
                        "end": 114,
                        "text": "Perozzi et al., 2014;",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 115,
                        "end": 140,
                        "text": "Zwicklbauer et al., 2016;",
                        "ref_id": "BIBREF37"
                    },
                    {
                        "start": 141,
                        "end": 159,
                        "text": "Wang et al., 2016)",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 289,
                        "end": 311,
                        "text": "(Mikolov et al., 2013;",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 312,
                        "end": 333,
                        "text": "Le and Mikolov, 2014)",
                        "ref_id": "BIBREF17"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Conventional attempts on utilizing embedding techniques in hyper-doc-related tasks generally fall into two types. The first type (Berger et al., 2017; Zwicklbauer et al., 2016) simply downcasts hyper-docs to plain documents and feeds them into word2vec (Mikolov et al., 2013 ) (w2v for short) or doc2vec (Le and Mikolov, 2014) (d2v for short). These approaches involve downgrading hyperlinks and inevitably omit certain information in hyper-docs. However, no previous work investigates the information loss, and how it affects the performance of such downcasting-based adaptations. The second type designs sophisticated embedding models to fulfill certain tasks, e.g., citation recommendation (Huang et al., 2015b) , paper classification (Wang et al., 2016) , and entity linking (Yamada et al., 2016) , etc. These models are limited to specific tasks, and it is yet unknown whether embeddings learned for those particular tasks can generalize to others. Based on the above facts, we are interested in two questions:",
                "cite_spans": [
                    {
                        "start": 129,
                        "end": 150,
                        "text": "(Berger et al., 2017;",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 151,
                        "end": 176,
                        "text": "Zwicklbauer et al., 2016)",
                        "ref_id": "BIBREF37"
                    },
                    {
                        "start": 253,
                        "end": 274,
                        "text": "(Mikolov et al., 2013",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 693,
                        "end": 714,
                        "text": "(Huang et al., 2015b)",
                        "ref_id": null
                    },
                    {
                        "start": 738,
                        "end": 757,
                        "text": "(Wang et al., 2016)",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 779,
                        "end": 800,
                        "text": "(Yamada et al., 2016)",
                        "ref_id": "BIBREF34"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\u2022 What information should hyper-doc embedding models preserve, and what nice property should they possess?",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\u2022 Is there a general approach to learning taskindependent embeddings of hyper-docs?",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "To answer the two questions, we formalize the hyper-doc embedding task, and propose four criteria, i.e., content awareness, context awareness, newcomer friendliness, and context intent aware-ness, to assess different models. Then we discuss simple downcasting-based adaptations of existing approaches w.r.t. the above criteria, and demonstrate that none of them satisfy all four. To this end, we propose hyperdoc2vec (h-d2v for short), a general embedding approach for hyperdocs. Different from most existing approaches, h-d2v learns two vectors for each hyper-doc to characterize its roles of citing others and being cited. Owning to this, h-d2v is able to directly model hyperlinks or citations without downgrading them. To evaluate the learned embeddings, we employ two tasks in the academic paper domain1 , i.e., paper classification and citation recommendation. Experimental results demonstrate the superiority of h-d2v. Comparative studies and controlled experiments also confirm that h-d2v benefits from satisfying the above four criteria.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "We summarize our contributions as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\u2022 We propose four criteria to assess different hyper-document embedding models.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\u2022 We propose hyperdoc2vec, a general embedding approach for hyper-documents.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\u2022 We systematically conduct comparisons with competing approaches, validating the superiority of h-d2v in terms of the four criteria.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Network representation learning is a related topic to ours since a collection of hyper-docs resemble a network. To embed nodes in a network, Perozzi et al. (2014) propose DeepWalk, where nodes and random walks are treated as pseudo words and texts, and fed to w2v for node vectors. Tang et al. (2015b) explicitly embed second-order proximity via the number of common neighbors of nodes. Grover and Leskovec (2016) extend Deep-Walk with second-order Markovian walks. To improve classification tasks, Tu et al. (2016) explore a semi-supervised setting that accesses partial labels. Compared with these models, h-d2v learns from both documents' connections and contents while they mainly focus on network structures.",
                "cite_spans": [
                    {
                        "start": 141,
                        "end": 162,
                        "text": "Perozzi et al. (2014)",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 282,
                        "end": 301,
                        "text": "Tang et al. (2015b)",
                        "ref_id": null
                    },
                    {
                        "start": 387,
                        "end": 413,
                        "text": "Grover and Leskovec (2016)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 499,
                        "end": 515,
                        "text": "Tu et al. (2016)",
                        "ref_id": "BIBREF32"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "Document embedding for classification is another focused area to apply document embeddings.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "Le and Mikolov (2014) employ learned d2v vectors to build different text classifiers. Tang et al. (2015a) apply the method in (Tang et al., 2015b) on word co-occurrence graphs for word embeddings, and average them for document vectors. For hyper-docs, Ganguly and Pudi (2017) and Wang et al. (2016) target paper classification in unsupervised and semi-supervised settings, respectively. However, unlike h-d2v, they do not explicitly model citation contexts. Yang et al. (2015) 's approach also addresses embedding hyper-docs, but involves matrix factorization and does not scale.",
                "cite_spans": [
                    {
                        "start": 7,
                        "end": 21,
                        "text": "Mikolov (2014)",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 86,
                        "end": 105,
                        "text": "Tang et al. (2015a)",
                        "ref_id": null
                    },
                    {
                        "start": 126,
                        "end": 146,
                        "text": "(Tang et al., 2015b)",
                        "ref_id": null
                    },
                    {
                        "start": 252,
                        "end": 275,
                        "text": "Ganguly and Pudi (2017)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 280,
                        "end": 298,
                        "text": "Wang et al. (2016)",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 458,
                        "end": 476,
                        "text": "Yang et al. (2015)",
                        "ref_id": "BIBREF35"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "Citation recommendation is a direct downstream task to evaluate embeddings learned for a certain kind of hyper-docs, i.e., academic papers. In this paper we concentrate on context-aware citation recommendation (He et al., 2010) . Some previous studies adopt neural models for this task. Huang et al. (2015b) propose Neural Probabilistic Model (NPM) to tackle this problem with embeddings. Their model outperforms non-embedding ones (Kataria et al., 2010; Tang and Zhang, 2009; Huang et al., 2012) . Ebesu and Fang (2017) also exploit neural networks for citation recommendation, but require author information as additional input. Compared with h-d2v, these models are limited in a task-specific setting.",
                "cite_spans": [
                    {
                        "start": 210,
                        "end": 227,
                        "text": "(He et al., 2010)",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 287,
                        "end": 307,
                        "text": "Huang et al. (2015b)",
                        "ref_id": null
                    },
                    {
                        "start": 432,
                        "end": 454,
                        "text": "(Kataria et al., 2010;",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 455,
                        "end": 476,
                        "text": "Tang and Zhang, 2009;",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 477,
                        "end": 496,
                        "text": "Huang et al., 2012)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 499,
                        "end": 520,
                        "text": "Ebesu and Fang (2017)",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "Embedding-based entity linking is another topic that exploits embeddings to model certain hyperdocs, i.e., Wikipedia (Huang et al., 2015a; Yamada et al., 2016; Sun et al., 2015; Fang et al., 2016; He et al., 2013; Zwicklbauer et al., 2016) , for entity linking (Shen et al., 2015) . It resembles citation recommendation in the sense that linked entities highly depend on the contexts. Meanwhile, it requires extra steps like candidate generation, and can benefit from sophisticated techniques such as collective linking (Cucerzan, 2007) .",
                "cite_spans": [
                    {
                        "start": 117,
                        "end": 138,
                        "text": "(Huang et al., 2015a;",
                        "ref_id": null
                    },
                    {
                        "start": 139,
                        "end": 159,
                        "text": "Yamada et al., 2016;",
                        "ref_id": "BIBREF34"
                    },
                    {
                        "start": 160,
                        "end": 177,
                        "text": "Sun et al., 2015;",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 178,
                        "end": 196,
                        "text": "Fang et al., 2016;",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 197,
                        "end": 213,
                        "text": "He et al., 2013;",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 214,
                        "end": 239,
                        "text": "Zwicklbauer et al., 2016)",
                        "ref_id": "BIBREF37"
                    },
                    {
                        "start": 261,
                        "end": 280,
                        "text": "(Shen et al., 2015)",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 520,
                        "end": 536,
                        "text": "(Cucerzan, 2007)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "We introduce notations and definitions, then formally define the embedding problem. We also propose four criteria for hyper-doc embedding models w.r.t their appropriateness and informativeness.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Preliminaries",
                "sec_num": "3"
            },
            {
                "text": "Let w \u2208 W be a word from a vocabulary W , and d \u2208 D be a document id (e.g., web page URLs and paper DOIs) from an id collection D. After filtering out non-textual content, a hyper-document H is reorganized as a sequence of words and doc ids, (Koehn et al., 2007) (Zhao and Gildea, 2010) (Papineni et al., 2002) Original Source doc",
                "cite_spans": [
                    {
                        "start": 242,
                        "end": 262,
                        "text": "(Koehn et al., 2007)",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 287,
                        "end": 310,
                        "text": "(Papineni et al., 2002)",
                        "ref_id": "BIBREF22"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Notations and Definitions",
                "sec_num": "3.1"
            },
            {
                "text": "Target doc \u2026 We also evaluate our model by computing the machine translation BLEU score (Papineni et al., 2002) using the Moses system (Koehn et al., 2007) \u2026",
                "cite_spans": [
                    {
                        "start": 88,
                        "end": 111,
                        "text": "(Papineni et al., 2002)",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 135,
                        "end": 155,
                        "text": "(Koehn et al., 2007)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Context words",
                "sec_num": null
            },
            {
                "text": "\u2026 \u2026",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Context words",
                "sec_num": null
            },
            {
                "text": "(a) Hyper-documents.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Context words",
                "sec_num": null
            },
            {
                "text": "Citation as word BLEU evaluate (Papineni et al., 2012) \"Word\" Vectors \u2026 w2v \u2026We also evaluate our model by computing the machine translation BLEU score (Papineni et al., 2002) using the Moses system (Koehn et al., 2007) et al., 2002) d2v (Koehn et al., 2007) (Zhao and Gildea, 2010) (Papineni et al., 2002) \u2026We also evaluate our model by computing the machine translation BLEU score using the ",
                "cite_spans": [
                    {
                        "start": 31,
                        "end": 54,
                        "text": "(Papineni et al., 2012)",
                        "ref_id": null
                    },
                    {
                        "start": 152,
                        "end": 175,
                        "text": "(Papineni et al., 2002)",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 199,
                        "end": 219,
                        "text": "(Koehn et al., 2007)",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 220,
                        "end": 233,
                        "text": "et al., 2002)",
                        "ref_id": null
                    },
                    {
                        "start": 238,
                        "end": 258,
                        "text": "(Koehn et al., 2007)",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 283,
                        "end": 306,
                        "text": "(Papineni et al., 2002)",
                        "ref_id": "BIBREF22"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Context words",
                "sec_num": null
            },
            {
                "text": "Given a corpus of hyper-docs {H d } d\u2208D with D and W , we want to learn document and word embedding matrices D \u2208 R k\u00d7|D| and W \u2208 R k\u00d7|W | simultaneously. The i-th column d i of D is a kdimensional embedding vector for the i-th hyperdoc with id d i . Similarly, w j , the j-th column of W, is the vector for word w j . Once embeddings for hyper-docs and words are learned, they can facilitate applications like hyper-doc classification and citation recommendation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Problem Statement",
                "sec_num": "3.2"
            },
            {
                "text": "A reasonable model should learn how contents and hyperlinks in hyper-docs impact both D and W. We propose the following criteria for models:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Criteria for Embedding Models",
                "sec_num": "3.3"
            },
            {
                "text": "\u2022 Content aware. Content words of a hyperdoc play the main role in describing it, so the document representation should depend on its own content. For example, the words in Zhao and Gildea (2010) should affect and contribute to its embedding.",
                "cite_spans": [
                    {
                        "start": 173,
                        "end": 195,
                        "text": "Zhao and Gildea (2010)",
                        "ref_id": "BIBREF36"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Criteria for Embedding Models",
                "sec_num": "3.3"
            },
            {
                "text": "\u2022 Context aware. Hyperlink contexts usually provide a summary for the target document. Therefore, the target document's vector should be impacted by words that others use to summarize it, e.g., paper Papineni et al. (2002) and the word \"BLEU\" in Figure 1 (a).",
                "cite_spans": [
                    {
                        "start": 200,
                        "end": 222,
                        "text": "Papineni et al. (2002)",
                        "ref_id": "BIBREF22"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 253,
                        "end": 254,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Criteria for Embedding Models",
                "sec_num": "3.3"
            },
            {
                "text": "\u2022 Newcomer friendly. In a hyper-document network, it is inevitable that some documents are not referred to by any hyperlink in other hyper-docs. If such \"newcomers\" do not get embedded properly, downstream tasks involving them are infeasible or deteriorated.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Criteria for Embedding Models",
                "sec_num": "3.3"
            },
            {
                "text": "\u2022 Context intent aware. Words around a hyperlink, e.g., \"evaluate . . . by\" in Figure 1(a) , normally indicate why the source hyper-doc makes the reference, e.g., for general reference or to follow/oppose the target hyperdoc's opinion or practice. Vectors of those context words should be influenced by both documents to characterize such semantics or intents between the two documents.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 86,
                        "end": 90,
                        "text": "1(a)",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Criteria for Embedding Models",
                "sec_num": "3.3"
            },
            {
                "text": "We note that the first three criteria are for hyperdocs, while the last one is desired for word vectors.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Criteria for Embedding Models",
                "sec_num": "3.3"
            },
            {
                "text": "In this section, we first give the background of two prevailing techniques, word2vec and doc2vec.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Representing Hypertext Documents",
                "sec_num": "4"
            },
            {
                "text": "Then we present two conversion approaches for hyper-documents so that w2v and d2v can be applied. Finally, we address their weaknesses w.r.t. the aforementioned four criteria, and propose our hyperdoc2vec model. In the remainder of this paper, when the context is clear, we mix the use of terms hyper-doc/hyperlink with paper/citation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Representing Hypertext Documents",
                "sec_num": "4"
            },
            {
                "text": "w2v (Mikolov et al., 2013) Model Output",
                "cite_spans": [
                    {
                        "start": 4,
                        "end": 26,
                        "text": "(Mikolov et al., 2013)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "word2vec and doc2vec",
                "sec_num": "4.1"
            },
            {
                "text": "D I D O W I W O w2v d2v (pv-dm) \u00d7 d2v (pv-dbow) \u00d7 \u00d7 h-d2v Table 2: Output of models.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "word2vec and doc2vec",
                "sec_num": "4.1"
            },
            {
                "text": "is regarded as a special context vector to average. Analogously, pv-dbow uses IN document vector to predict its words' OUT vectors, following the same structure of skip-gram. Therefore in pv-dbow, words' IN vectors are omitted.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "word2vec and doc2vec",
                "sec_num": "4.1"
            },
            {
                "text": "To represent hyper-docs, a straightforward strategy is to convert them into plain documents in a certain way and apply w2v and d2v. Two conversions following this strategy are illustrated below.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Adaptation of Existing Approaches",
                "sec_num": "4.2"
            },
            {
                "text": "Citation as word. This approach is adopted by Berger et al. (2017) . 2 As Figure 1 (b) shows, document ids D are treated as a collection of special words. Each citation is regarded as an occurrence of the target document's special word. After applying standard word embedding methods, e.g., w2v, we obtain embeddings for both ordinary words and special \"words\", i.e., documents. In doing so, this approach allows target documents interacting with context words, thus produces context-aware embeddings for them.",
                "cite_spans": [
                    {
                        "start": 46,
                        "end": 66,
                        "text": "Berger et al. (2017)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 81,
                        "end": 82,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Adaptation of Existing Approaches",
                "sec_num": "4.2"
            },
            {
                "text": "Context as content. It is often observed in academic papers when citing others' work, an author briefly summarizes the cited paper in its citation context. Inspired by this, we propose a contextas-content approach as in Figure 1 (c). To start, we remove all citations. Then all citation contexts of a target document d t are copied into d t as additional contents to make up for the lost information. Finally, d2v is applied to the augmented documents to generate document embeddings. With this approach, the generated document embeddings are both context-and content-aware.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 227,
                        "end": 228,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Adaptation of Existing Approaches",
                "sec_num": "4.2"
            },
            {
                "text": "Besides citation-as-word with w2v and contextas-content with d2v (denoted by d2v-cac for short), there is also an alternative using d2v on documents with citations removed (d2v-nc for 2 It is designed for document visualization purposes. short). We made a comparison of these approaches in Table 1 in terms of the four criteria stated in Section 3.3. It is observed that none of them satisfy all criteria, where the reasons are as follows.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 296,
                        "end": 297,
                        "text": "1",
                        "ref_id": "TABREF2"
                    }
                ],
                "eq_spans": [],
                "section": "hyperdoc2vec",
                "sec_num": "4.3"
            },
            {
                "text": "First, w2v is not content aware. Following our examples in the academic paper domain, consider the paper (hyper-doc) Zhao and Gildea (2010) in Figure 1 Zhao and Gildea (2010) , thus not contributing to its embedding. In addition, for papers being just published and having not obtained citations yet, they will not appear as special \"words\" in any text. This makes w2v newcomerunfriendly, i.e., unable to produce embeddings for them. Second, being trained on a corpus without citations, d2v-nc is obviously not context aware. Finally, in both w2v and d2v-cac, context words interact with the target documents without treating the source documents as backgrounds, which forces IN vectors of words with context intents, e.g., \"evaluate\" and \"by\" in Figure 1 (a), to simply remember the target documents, rather than capture the semantics of the citations.",
                "cite_spans": [
                    {
                        "start": 117,
                        "end": 139,
                        "text": "Zhao and Gildea (2010)",
                        "ref_id": "BIBREF36"
                    },
                    {
                        "start": 152,
                        "end": 174,
                        "text": "Zhao and Gildea (2010)",
                        "ref_id": "BIBREF36"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 150,
                        "end": 151,
                        "text": "1",
                        "ref_id": null
                    },
                    {
                        "start": 754,
                        "end": 755,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "hyperdoc2vec",
                "sec_num": "4.3"
            },
            {
                "text": "The above limitations are caused by the conversions of hyper-docs where certain information in citations is lost. For a citation d s , C, d t , citationas-word only keeps the co-occurrence information between C and d t . Context-as-content, on the other hand, mixes C with the original content of d t . Both approaches implicitly downgrade citations d s , C, d t to C, d t for adaptation purposes.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "hyperdoc2vec",
                "sec_num": "4.3"
            },
            {
                "text": "To learn hyper-doc embeddings without such limitations, we propose hyperdoc2vec. In this model, two vectors of a hyper-doc d, i.e., IN and OUT vectors , are adopted to represent the document of its two roles. The IN vector d I characterizes d being a source document. The OUT vector d O encodes its role as a target document. We note that learning those two types of vectors is advantageous. It enables us to model citations and con- ",
                "cite_spans": [
                    {
                        "start": 132,
                        "end": 150,
                        "text": "IN and OUT vectors",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "hyperdoc2vec",
                "sec_num": "4.3"
            },
            {
                "text": "P (d t |d s , C) = exp(x d O t ) d\u2208D exp(x d O ) (3)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "hyperdoc2vec",
                "sec_num": "4.3"
            },
            {
                "text": "To model contents' impact on document vectors, we simply consider an additional objective function that is identical to pv-dm, i.e., enumerate words and contexts, and use the same input architecture as Figure 2 to predict the OUT vector of the current word. Such convenience owes to the fact that using two vectors makes the model parameters compatible with those of pv-dm. Note that combining the citation and content objectives leads to a joint learning framework. To facilitate easier and faster training, we adopt an alternative pre-training/fine-tuning or retrofitting framework (Faruqui et al., 2015) . We initialize with a predefined number of pv-dm iterations, and then optimize Eq. 1 based on the initialization. ",
                "cite_spans": [
                    {
                        "start": 584,
                        "end": 606,
                        "text": "(Faruqui et al., 2015)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 209,
                        "end": 210,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "hyperdoc2vec",
                "sec_num": "4.3"
            },
            {
                "text": "log \u03c3(x d O t ) + n i=1 E d i \u223cP N (d) log \u03c3(-x d O i )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "hyperdoc2vec",
                "sec_num": "4.3"
            },
            {
                "text": "(4) and use it to replace every log P (d t |d s , C). Following Huang et al. (2015b) , we adopt a uniform distribution on D as the distribution P N (d).",
                "cite_spans": [
                    {
                        "start": 64,
                        "end": 84,
                        "text": "Huang et al. (2015b)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "hyperdoc2vec",
                "sec_num": "4.3"
            },
            {
                "text": "Unlike the other models in Table 1 , h-d2v satisfies all four criteria. We refer to the example in Figure 2 to make the points clear. First, when optimizing Eq. 1 with the instance in Figure 2 , the update to d O of Papineni et al. (2002) depends on w I of context words such as \"BLEU\". Second, we pre-train d I with contents, which makes the document embeddings content aware. Third, newcomers can depend on their contents for d I , and update their OUT vectors when they are sampled3 in Eq. 4. Finally, the optimization of Eq. 1 enables mutual enhancement between vectors of hyper-docs and context intent words, e.g., \"evaluate by\". Under the background of a machine translation paper Zhao and Gildea (2010) , the above two words help point the citation to the BLEU paper (Papineni et al., 2002) , thus updating its OUT vector. The intent \"adopting tools/algorithms\" of \"evaluate by\" is also better captured by iterating over many document pairs with them in between.",
                "cite_spans": [
                    {
                        "start": 216,
                        "end": 238,
                        "text": "Papineni et al. (2002)",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 687,
                        "end": 709,
                        "text": "Zhao and Gildea (2010)",
                        "ref_id": "BIBREF36"
                    },
                    {
                        "start": 774,
                        "end": 797,
                        "text": "(Papineni et al., 2002)",
                        "ref_id": "BIBREF22"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 33,
                        "end": 34,
                        "text": "1",
                        "ref_id": "TABREF2"
                    },
                    {
                        "start": 106,
                        "end": 107,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    },
                    {
                        "start": 191,
                        "end": 192,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "hyperdoc2vec",
                "sec_num": "4.3"
            },
            {
                "text": "In this section, we first introduce datasets and basic settings used to learn embeddings. We then discuss additional settings and present experimental results of the two tasks, i.e., document classification and citation recommendation, respectively. Table 5 : F 1 on DBLP when newcomers are discarded.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 256,
                        "end": 257,
                        "text": "5",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "5"
            },
            {
                "text": "We use three datasets from the academic paper domain, i.e., NIPS 4 , ACL anthology 5 and DBLP 6 , as shown in Table 3 . They all contain full text of papers, and are of small, medium, and large size, respectively. We apply ParsCit 2010), we take 50 words before and after a citation as the citation context. Gensim ( \u0158eh\u016f\u0159ek and Sojka, 2010 ) is used to implement all w2v and d2v baselines as well as h-d2v. We use cbow for w2v and pv-dbow for d2v, unless otherwise noted. For all three baselines, we set the (half) context window length to 50. For w2v, d2v, and the pv-dm-based initialization of h-d2v, we run 5 epochs following Gensim's default setting. For h-d2v, its iteration is set to 100 epochs with 1000 negative samples. The dimension size k of all approaches is 100. All other parameters in Gensim are kept as default.",
                "cite_spans": [
                    {
                        "start": 315,
                        "end": 340,
                        "text": "( \u0158eh\u016f\u0159ek and Sojka, 2010",
                        "ref_id": "BIBREF24"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 116,
                        "end": 117,
                        "text": "3",
                        "ref_id": "TABREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Datasets and Experimental Settings",
                "sec_num": "5.1"
            },
            {
                "text": "In this task, we classify the research fields of papers given their vectors learned on DBLP. To obtain labels, we use Cora 8 , a small dataset of Computer Science papers and their field categories. We keep the first levels of the original categories, 4 https://cs.nyu.edu/ roweis/data.html 5 http://clair.eecs.umich.edu/aan/index.php (2013 release) 6 http://zhou142.myweb.cs.uwindsor.ca/academicpaper.html This page has been unavailable recently. They provide a larger CiteSeer dataset and a collection of DBLP paper ids. To better interpret results from the Computer Science perspective, we intersect them and obtain the DBLP dataset.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Document Classification",
                "sec_num": "5.2"
            },
            {
                "text": "7 https://github.com/knmnyn/ParsCit 8 http://people.cs.umass.edu/\u02dcmccallum/data.html e.g., \"Artificial Intelligence\" of \"Artificial Intelligence -Natural Language Processing\", leading to 10 unique classes. We then intersect the dataset with DBLP, and obtain 5,975 labeled papers. For w2v and h-d2v outputing both IN and OUT document vectors, we use IN vectors or concatenations of both vectors as features. For newcomer papers without w2v vectors, we use zero vectors instead. To enrich the features with network structure information, we also try concatenating them with the output of DeepWalk (Perozzi et al., 2014) , a representative network embedding model. The model is trained on the citation network of DBLP with an existing implementation9 and default parameters. An SVM classifier with RBF kernel is used. We perform 5-fold cross validation, and report Macro-and Micro-F 1 scores.",
                "cite_spans": [
                    {
                        "start": 595,
                        "end": 617,
                        "text": "(Perozzi et al., 2014)",
                        "ref_id": "BIBREF23"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Document Classification",
                "sec_num": "5.2"
            },
            {
                "text": "In Table 4 , we demonstrate the classification results. We have the following observations. First, adding DeepWalk information almost always leads to better classification performance, except for Macro-F 1 of the d2v-cac approach.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 9,
                        "end": 10,
                        "text": "4",
                        "ref_id": "TABREF5"
                    }
                ],
                "eq_spans": [],
                "section": "Classification Performance",
                "sec_num": "5.2.1"
            },
            {
                "text": "Second, owning to different context awareness, d2v-cac consistently outperforms d2v-nc in terms of all metrics and settings.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Classification Performance",
                "sec_num": "5.2.1"
            },
            {
                "text": "Third, w2v has the worst performance. The reason may be that w2v is neither content aware nor newcomer friendly. We will elaborate more on the impacts of the two properties in Section 5.2.2.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Classification Performance",
                "sec_num": "5.2.1"
            },
            {
                "text": "Finally, no matter whether DeepWalk vectors are used, h-d2v achieves the best F 1 scores. However, when OUT vectors are involved, h-d2v with DeepWalk has slightly worse performance. A possible explanation is that, when h-d2v IN and DeepWalk vectors have enough information to train the SVM classifiers, adding another 100 features (OUT vectors) only increase the parameter .49 30.98 16.76 16.77 20.12 17.22 8.82 8.87 10.65 h-d2v (pv-dm retrofitting, I4O) 15.73 6.68 6.68 8.80 31.93 17.33 17.34 20.76 21.32 10.83 10.88 13.14 Table 6 : Top-10 citation recommendation results (dimension size k = 100).",
                "cite_spans": [
                    {
                        "start": 373,
                        "end": 523,
                        "text": ".49 30.98 16.76 16.77 20.12 17.22 8.82 8.87 10.65 h-d2v (pv-dm retrofitting, I4O) 15.73 6.68 6.68 8.80 31.93 17.33 17.34 20.76 21.32 10.83 10.88 13.14",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 530,
                        "end": 531,
                        "text": "6",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Classification Performance",
                "sec_num": "5.2.1"
            },
            {
                "text": "space of the classifiers and the training variance. For w2v with or without DeepWalk, it is also the case. This may be because information in w2v's IN and OUT vectors is fairly redundant.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Classification Performance",
                "sec_num": "5.2.1"
            },
            {
                "text": "Because content awareness and newcomer friendliness are highly correlated in Table 1 , to isolate and study their impacts, we decouple them as follows. In the 5,975 labeled papers, we keep 2,052 with at least one citation, and redo experiments in Table 4 . By carrying out such controlled experiments, we expect to remove the impact of newcomers, and compare all approaches only with respect to different content awareness. In Table 5 , we provide the new scores obtained. By comparing Tables 4 and 5 , we observe that w2v benefits from removing newcomers with zero vectors, while all newcomer friendly approaches get lower scores because of fewer training examples. Even though the change, w2v still cannot outperform the other approaches, which reflects the positive impact of content awareness on the classification task. It is also interesting that Deep-Walk becomes very competitive. This implies that structure-based methods favor networks with better connectivity. Finally, we note that Table 5 is based on controlled experiments with intentionally skewed data. The results are not intended for comparison among approaches in practical scenarios.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 83,
                        "end": 84,
                        "text": "1",
                        "ref_id": "TABREF2"
                    },
                    {
                        "start": 253,
                        "end": 254,
                        "text": "4",
                        "ref_id": "TABREF5"
                    },
                    {
                        "start": 433,
                        "end": 434,
                        "text": "5",
                        "ref_id": null
                    },
                    {
                        "start": 493,
                        "end": 494,
                        "text": "4",
                        "ref_id": "TABREF5"
                    },
                    {
                        "start": 499,
                        "end": 500,
                        "text": "5",
                        "ref_id": null
                    },
                    {
                        "start": 1000,
                        "end": 1001,
                        "text": "5",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Impacts of Content Awareness and Newcomer Friendliness",
                "sec_num": "5.2.2"
            },
            {
                "text": "When writing papers, it is desirable to recommend proper citations for a given context. This could be achieved by comparing the vectors of the context and previous papers. We use all three datasets for this task. Embeddings are trained on papers before 1998, 2012, and 2009, respectively. The remaining papers in each dataset are used for testing.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Citation Recommendation",
                "sec_num": "5.3"
            },
            {
                "text": "We compare h-d2v with all approaches in Sec-tion 4.2, as well as NPM10 (Huang et al., 2015b) mentioned in Section 2, the first embedding-based approach for the citation recommendation task. Note that the inference stage involves interactions between word and document vectors and is nontrivial. We describe our choices as below.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Citation Recommendation",
                "sec_num": "5.3"
            },
            {
                "text": "First, for w2v vectors, Nalisnick et al. ( 2016) suggest that the IN-IN similarity favors word pairs with similar functions (e.g., \"red\" and \"blue\"), while the IN-OUT similarity characterizes word co-occurrence or compatibility (e.g., \"red\" and \"bull\"). For citation recommendation that relies on the compatibility between context words and cited papers, we hypothesize that the IN-for-OUT (or I4O for short) approach will achieve better results. Therefore, for w2v-based approaches, we average IN vectors of context words, then score and and rank OUT document vectors by dot product.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Citation Recommendation",
                "sec_num": "5.3"
            },
            {
                "text": "Second, for d2v-based approaches, we use the learned model to infer a document vector d for the context words, and use d to rank IN document vectors by cosine similarity. Among multiple attempts, we find this choice to be optimal.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Citation Recommendation",
                "sec_num": "5.3"
            },
            {
                "text": "Third, for h-d2v, we adopt the same scoring and ranking configurations as for w2v.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Citation Recommendation",
                "sec_num": "5.3"
            },
            {
                "text": "Finally, for NPM, we adopt the same ranking strategy as in Huang et al. (2015b) . Following them, we focus on top-10 results and report the Recall, MAP, MRR, and nDCG scores.",
                "cite_spans": [
                    {
                        "start": 59,
                        "end": 79,
                        "text": "Huang et al. (2015b)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Citation Recommendation",
                "sec_num": "5.3"
            },
            {
                "text": "In Table 6 , we report the citation recommendation results. Our observations are as follows.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 9,
                        "end": 10,
                        "text": "6",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Recommendation Performance",
                "sec_num": "5.3.1"
            },
            {
                "text": "First, among all datasets, all methods perform relatively well on the medium-sized ACL dataset. This is because the smallest NIPS dataset provides too few citation contexts to train a good model. Moreover, DBLP requires a larger dimension size k to store more information in the embedding vectors. We increase k and report the Rec@10 scores in Figure 3 . We see that all approaches have better performance when k increases to 200, though d2v-based ones start to drop beyond this point. Second, the I4I variant of w2v has the worst performance among all approaches. This observation validates our hypothesis in Section 5.3.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 351,
                        "end": 352,
                        "text": "3",
                        "ref_id": "FIGREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Recommendation Performance",
                "sec_num": "5.3.1"
            },
            {
                "text": "Third, the d2v-cac approach outperforms its variant d2v-nc in terms of all datasets and metrics. This indicates that context awareness matters in the citation recommendation task.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Recommendation Performance",
                "sec_num": "5.3.1"
            },
            {
                "text": "Fourth, the performance of NPM is sandwiched between those of w2v's two variants. We have tried our best to reproduce it. Our explanation is that NPM is citation-as-word-based, and only depends on citation contexts for training. Therefore, it is only context aware but neither content aware nor newcomer friendly, and behaves like w2v.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Recommendation Performance",
                "sec_num": "5.3.1"
            },
            {
                "text": "Finally, when retrofitting pv-dm, h-d2v generally has the best performance. When we substitute pv-dm with random initialization, the performance is deteriorated by varying degrees on different datasets. This implies that content awareness is also important, if not so important than context awareness, on the citation recommendation task. Zhao and Gildea (2010) . kernels and default parameters. Following Teufel et al. (2006) , we use 10-fold cross validation. Figure 4 depicts the F 1 scores. Scores of Teufel et al. (2006) 's approach are from the original paper. We omit d2v-nc because it is very inferior to d2v-cac. We have the following observations. First, Teufel et al. (2006) 's feature-engineeringbased approach has the best performance. Note that we cannot obtain their original cross validation split, so the comparison may not be fair and is only for consideration in terms of numbers.",
                "cite_spans": [
                    {
                        "start": 339,
                        "end": 361,
                        "text": "Zhao and Gildea (2010)",
                        "ref_id": "BIBREF36"
                    },
                    {
                        "start": 406,
                        "end": 426,
                        "text": "Teufel et al. (2006)",
                        "ref_id": "BIBREF31"
                    },
                    {
                        "start": 505,
                        "end": 525,
                        "text": "Teufel et al. (2006)",
                        "ref_id": "BIBREF31"
                    },
                    {
                        "start": 665,
                        "end": 685,
                        "text": "Teufel et al. (2006)",
                        "ref_id": "BIBREF31"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 469,
                        "end": 470,
                        "text": "4",
                        "ref_id": "FIGREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Recommendation Performance",
                "sec_num": "5.3.1"
            },
            {
                "text": "W e a k C o C o G M C o C o R 0 C o C o - C o C o X Y P B",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Impact of Newcomer Friendliness",
                "sec_num": "5.3.2"
            },
            {
                "text": "Second, among all embedding-based methods, h-d2v has the best citation function classification results, which is close to Teufel et al. (2006) 's.",
                "cite_spans": [
                    {
                        "start": 122,
                        "end": 142,
                        "text": "Teufel et al. (2006)",
                        "ref_id": "BIBREF31"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Impact of Newcomer Friendliness",
                "sec_num": "5.3.2"
            },
            {
                "text": "Finally, the d2v-cac vectors are only good at Neutral, the largest class. On the other classes and global F 1 , they are outperformed by w2v vectors.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Impact of Newcomer Friendliness",
                "sec_num": "5.3.2"
            },
            {
                "text": "To study how citation function affects citation recommendation, we combine the 2,824 labeled citation contexts and another 1,075 labeled contexts the authors published later to train an SVM, and apply it to the DBLP testing set to get citation functions. We evaluate citation recommendation performance of w2v (I4O), d2v-cac, and h-d2v on a per-citation-function basis. In Figure 5 , we break down Rec@10 scores on citation functions. On the six largest classes (marked by solid dots), h-d2v outperforms all competitors. To better investigate the impact of context intent awareness, Table 9 shows recommended papers of the running example of this paper. Here, Zhao and Gildea (2010) cited the BLEU metric (Papineni et al., 2002) and Moses tools (Koehn et al., 2007) of machine translation. However, the additional words \"machine translation\" lead both w2v and d2v-cac to recommend many machine translation papers. Only our h-d2v manages to recognize the citation function \"using tools/algorithms (PBas)\", and concentrates on the citation intent to return the right papers in top-5 results.",
                "cite_spans": [
                    {
                        "start": 660,
                        "end": 682,
                        "text": "Zhao and Gildea (2010)",
                        "ref_id": "BIBREF36"
                    },
                    {
                        "start": 705,
                        "end": 728,
                        "text": "(Papineni et al., 2002)",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 745,
                        "end": 765,
                        "text": "(Koehn et al., 2007)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 380,
                        "end": 381,
                        "text": "5",
                        "ref_id": "FIGREF4"
                    },
                    {
                        "start": 589,
                        "end": 590,
                        "text": "9",
                        "ref_id": "TABREF8"
                    }
                ],
                "eq_spans": [],
                "section": "Impact of Newcomer Friendliness",
                "sec_num": "5.3.2"
            },
            {
                "text": "We focus on the hyper-doc embedding problem. We propose that hyper-doc embedding algorithms should be content aware, context aware, newcomer friendly, and context intent aware. To meet all four criteria, we propose a general approach, hyperdoc2vec, which assigns two vectors to each hyper-doc and models citations in a straightforward manner. In doing so, the learned embeddings satisfy all criteria, which no existing model is able to. For evaluation, paper classification and citation recommendation are conducted on three academic paper datasets. Results confirm the effectiveness of our approach. Further analyses also demonstrate that possessing the four properties helps h-d2v outperform other models.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "6"
            },
            {
                "text": "Although limited in tasks and domains, we expect that our embedding approach can be potentially generalized to, or serve as basis to more sophisticated methods for, similar tasks in the entity domain, e.g., Wikipedia page classification and entity linking. We leave them for future work.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Given a relatively large n.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://github.com/phanein/deepwalk",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Note that the authors used n = 1000 for negative sampling, and did not report the number of training epoches. After many trials, we find that setting the number of both the negative samples and epoches at 100 to be relatively effective and affordable w.r.t. training time.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "The number is 2,829 in the original paper. The inconsistency may be due to different regular expressions we used.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "cite2vec: Citation-driven document exploration via word embeddings",
                "authors": [
                    {
                        "first": "Matthew",
                        "middle": [],
                        "last": "Berger",
                        "suffix": ""
                    },
                    {
                        "first": "Katherine",
                        "middle": [],
                        "last": "Mcdonough",
                        "suffix": ""
                    },
                    {
                        "first": "Lee",
                        "middle": [
                            "M"
                        ],
                        "last": "Seversky",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "IEEE Trans. Vis. Comput. Graph",
                "volume": "23",
                "issue": "1",
                "pages": "691--700",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Matthew Berger, Katherine McDonough, and Lee M. Seversky. 2017. cite2vec: Citation-driven document exploration via word embeddings. IEEE Trans. Vis. Comput. Graph. 23(1):691-700.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "The missing link -A probabilistic model of document content and hypertext connectivity",
                "authors": [
                    {
                        "first": "David",
                        "middle": [
                            "A"
                        ],
                        "last": "Cohn",
                        "suffix": ""
                    },
                    {
                        "first": "Thomas",
                        "middle": [],
                        "last": "Hofmann",
                        "suffix": ""
                    }
                ],
                "year": 2000,
                "venue": "Advances in Neural Information Processing Systems 13, Papers from Neural Information Processing Systems (NIPS) 2000",
                "volume": "",
                "issue": "",
                "pages": "430--436",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "David A. Cohn and Thomas Hofmann. 2000. The missing link -A probabilistic model of document content and hypertext connectivity. In Advances in Neural Information Processing Systems 13, Papers from Neural Information Processing Systems (NIPS) 2000. pages 430-436.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Parscit: an open-source CRF reference string parsing package",
                "authors": [
                    {
                        "first": "G",
                        "middle": [],
                        "last": "Isaac",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [
                            "Lee"
                        ],
                        "last": "Councill",
                        "suffix": ""
                    },
                    {
                        "first": "Min-Yen",
                        "middle": [],
                        "last": "Giles",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Kan",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Proceedings of the International Conference on Language Resources and Evaluation, LREC",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Isaac G. Councill, C. Lee Giles, and Min-Yen Kan. 2008. Parscit: an open-source CRF reference string parsing package. In Proceedings of the Interna- tional Conference on Language Resources and Eval- uation, LREC 2008.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Large-scale named entity disambiguation based on wikipedia data",
                "authors": [
                    {
                        "first": "Silviu",
                        "middle": [],
                        "last": "Cucerzan",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "EMNLP-CoNLL 2007, Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning",
                "volume": "",
                "issue": "",
                "pages": "708--716",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Silviu Cucerzan. 2007. Large-scale named entity dis- ambiguation based on wikipedia data. In EMNLP- CoNLL 2007, Proceedings of the 2007 Joint Con- ference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. pages 708-716.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Neural citation network for context-aware citation recommendation",
                "authors": [
                    {
                        "first": "Travis",
                        "middle": [],
                        "last": "Ebesu",
                        "suffix": ""
                    },
                    {
                        "first": "Yi",
                        "middle": [],
                        "last": "Fang",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval",
                "volume": "",
                "issue": "",
                "pages": "1093--1096",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Travis Ebesu and Yi Fang. 2017. Neural citation net- work for context-aware citation recommendation. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Infor- mation Retrieval. pages 1093-1096.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Entity disambiguation by knowledge and text jointly embedding",
                "authors": [
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Fang",
                        "suffix": ""
                    },
                    {
                        "first": "Jianwen",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Dilin",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Zheng",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Ming",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning",
                "volume": "",
                "issue": "",
                "pages": "260--269",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Wei Fang, Jianwen Zhang, Dilin Wang, Zheng Chen, and Ming Li. 2016. Entity disambiguation by knowledge and text jointly embedding. In Proceed- ings of the 20th SIGNLL Conference on Computa- tional Natural Language Learning, CoNLL 2016. pages 260-269.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Retrofitting word vectors to semantic lexicons",
                "authors": [
                    {
                        "first": "Manaal",
                        "middle": [],
                        "last": "Faruqui",
                        "suffix": ""
                    },
                    {
                        "first": "Jesse",
                        "middle": [],
                        "last": "Dodge",
                        "suffix": ""
                    },
                    {
                        "first": "Sujay",
                        "middle": [],
                        "last": "Kumar Jauhar",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Dyer",
                        "suffix": ""
                    },
                    {
                        "first": "Eduard",
                        "middle": [
                            "H"
                        ],
                        "last": "Hovy",
                        "suffix": ""
                    },
                    {
                        "first": "Noah",
                        "middle": [
                            "A"
                        ],
                        "last": "Smith",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "NAACL HLT 2015, The 2015 Conference of the North American Chapter",
                "volume": "",
                "issue": "",
                "pages": "1606--1615",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Manaal Faruqui, Jesse Dodge, Sujay Kumar Jauhar, Chris Dyer, Eduard H. Hovy, and Noah A. Smith. 2015. Retrofitting word vectors to semantic lexi- cons. In NAACL HLT 2015, The 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies. pages 1606-1615.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Computing semantic relatedness using wikipediabased explicit semantic analysis",
                "authors": [
                    {
                        "first": "Evgeniy",
                        "middle": [],
                        "last": "Gabrilovich",
                        "suffix": ""
                    },
                    {
                        "first": "Shaul",
                        "middle": [],
                        "last": "Markovitch",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "IJCAI 2007, Proceedings of the 20th International Joint Conference on Artificial Intelligence",
                "volume": "",
                "issue": "",
                "pages": "1606--1611",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Evgeniy Gabrilovich and Shaul Markovitch. 2007. Computing semantic relatedness using wikipedia- based explicit semantic analysis. In IJCAI 2007, Proceedings of the 20th International Joint Confer- ence on Artificial Intelligence. pages 1606-1611.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Paper2vec: Combining graph and text information for scientific paper representation",
                "authors": [
                    {
                        "first": "Soumyajit",
                        "middle": [],
                        "last": "Ganguly",
                        "suffix": ""
                    },
                    {
                        "first": "Vikram",
                        "middle": [],
                        "last": "Pudi",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Advances in Information Retrieval -39th European Conference on IR Research",
                "volume": "",
                "issue": "",
                "pages": "383--395",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Soumyajit Ganguly and Vikram Pudi. 2017. Pa- per2vec: Combining graph and text information for scientific paper representation. In Advances in In- formation Retrieval -39th European Conference on IR Research, ECIR 2017. pages 383-395.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "node2vec: Scalable feature learning for networks",
                "authors": [
                    {
                        "first": "Aditya",
                        "middle": [],
                        "last": "Grover",
                        "suffix": ""
                    },
                    {
                        "first": "Jure",
                        "middle": [],
                        "last": "Leskovec",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
                "volume": "",
                "issue": "",
                "pages": "855--864",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for networks. In Proceed- ings of the 22nd ACM SIGKDD International Con- ference on Knowledge Discovery and Data Mining. pages 855-864.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Context-aware citation recommendation",
                "authors": [
                    {
                        "first": "Qi",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Jian",
                        "middle": [],
                        "last": "Pei",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Kifer",
                        "suffix": ""
                    },
                    {
                        "first": "Prasenjit",
                        "middle": [],
                        "last": "Mitra",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [
                            "Lee"
                        ],
                        "last": "Giles",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the 19th International Conference on World Wide Web, WWW 2010",
                "volume": "",
                "issue": "",
                "pages": "421--430",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Qi He, Jian Pei, Daniel Kifer, Prasenjit Mitra, and C. Lee Giles. 2010. Context-aware citation recom- mendation. In Proceedings of the 19th International Conference on World Wide Web, WWW 2010. pages 421-430.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Learning entity representation for entity disambiguation",
                "authors": [
                    {
                        "first": "Zhengyan",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Shujie",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Mu",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Ming",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Longkai",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Houfeng",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, ACL 2013",
                "volume": "2",
                "issue": "",
                "pages": "30--34",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zhengyan He, Shujie Liu, Mu Li, Ming Zhou, Longkai Zhang, and Houfeng Wang. 2013. Learning entity representation for entity disambiguation. In Pro- ceedings of the 51st Annual Meeting of the Associa- tion for Computational Linguistics, ACL 2013, Vol- ume 2: Short Papers. pages 30-34.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Leveraging deep neural networks and knowledge graphs for entity disambiguation",
                "authors": [
                    {
                        "first": "Hongzhao",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Larry",
                        "middle": [
                            "P"
                        ],
                        "last": "Heck",
                        "suffix": ""
                    },
                    {
                        "first": "Heng",
                        "middle": [],
                        "last": "Ji",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Hongzhao Huang, Larry P. Heck, and Heng Ji. 2015a. Leveraging deep neural networks and knowledge graphs for entity disambiguation. CoRR abs/1504.07678.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Recommending citations: translating papers into references",
                "authors": [
                    {
                        "first": "Wenyi",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Saurabh",
                        "middle": [],
                        "last": "Kataria",
                        "suffix": ""
                    },
                    {
                        "first": "Cornelia",
                        "middle": [],
                        "last": "Caragea",
                        "suffix": ""
                    },
                    {
                        "first": "Prasenjit",
                        "middle": [],
                        "last": "Mitra",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [
                            "Lee"
                        ],
                        "last": "Giles",
                        "suffix": ""
                    },
                    {
                        "first": "Lior",
                        "middle": [],
                        "last": "Rokach",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "21st ACM International Conference on Information and Knowledge Management, CIKM'12",
                "volume": "",
                "issue": "",
                "pages": "1910--1914",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Wenyi Huang, Saurabh Kataria, Cornelia Caragea, Prasenjit Mitra, C. Lee Giles, and Lior Rokach. 2012. Recommending citations: translating papers into references. In 21st ACM International Confer- ence on Information and Knowledge Management, CIKM'12. pages 1910-1914.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Prasenjit Mitra, and C. Lee Giles. 2015b. A neural probabilistic model for context based citation recommendation",
                "authors": [
                    {
                        "first": "Wenyi",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Zhaohui",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Liang",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence",
                "volume": "",
                "issue": "",
                "pages": "2404--2410",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Wenyi Huang, Zhaohui Wu, Liang Chen, Prasenjit Mi- tra, and C. Lee Giles. 2015b. A neural probabilistic model for context based citation recommendation. In Proceedings of the Twenty-Ninth AAAI Confer- ence on Artificial Intelligence. pages 2404-2410.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Utilizing context in generative bayesian models for linked corpus",
                "authors": [
                    {
                        "first": "Saurabh",
                        "middle": [],
                        "last": "Kataria",
                        "suffix": ""
                    },
                    {
                        "first": "Prasenjit",
                        "middle": [],
                        "last": "Mitra",
                        "suffix": ""
                    },
                    {
                        "first": "Sumit",
                        "middle": [],
                        "last": "Bhatia",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2010",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Saurabh Kataria, Prasenjit Mitra, and Sumit Bhatia. 2010. Utilizing context in generative bayesian mod- els for linked corpus. In Proceedings of the Twenty- Fourth AAAI Conference on Artificial Intelligence, AAAI 2010.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Moses: Open source toolkit for statistical machine translation",
                "authors": [
                    {
                        "first": "Philipp",
                        "middle": [],
                        "last": "Koehn",
                        "suffix": ""
                    },
                    {
                        "first": "Hieu",
                        "middle": [],
                        "last": "Hoang",
                        "suffix": ""
                    },
                    {
                        "first": "Alexandra",
                        "middle": [],
                        "last": "Birch",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Callison-Burch",
                        "suffix": ""
                    },
                    {
                        "first": "Marcello",
                        "middle": [],
                        "last": "Federico",
                        "suffix": ""
                    },
                    {
                        "first": "Nicola",
                        "middle": [],
                        "last": "Bertoldi",
                        "suffix": ""
                    },
                    {
                        "first": "Brooke",
                        "middle": [],
                        "last": "Cowan",
                        "suffix": ""
                    },
                    {
                        "first": "Wade",
                        "middle": [],
                        "last": "Shen",
                        "suffix": ""
                    },
                    {
                        "first": "Christine",
                        "middle": [],
                        "last": "Moran",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Zens",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Dyer",
                        "suffix": ""
                    },
                    {
                        "first": "Ondrej",
                        "middle": [],
                        "last": "Bojar",
                        "suffix": ""
                    },
                    {
                        "first": "Alexandra",
                        "middle": [],
                        "last": "Constantin",
                        "suffix": ""
                    },
                    {
                        "first": "Evan",
                        "middle": [],
                        "last": "Herbst",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "ACL 2007, Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In ACL 2007, Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Distributed representations of sentences and documents",
                "authors": [
                    {
                        "first": "V",
                        "middle": [],
                        "last": "Quoc",
                        "suffix": ""
                    },
                    {
                        "first": "Tomas",
                        "middle": [],
                        "last": "Le",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Mikolov",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of the 31th International Conference on Machine Learning, ICML 2014",
                "volume": "",
                "issue": "",
                "pages": "1188--1196",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Quoc V. Le and Tomas Mikolov. 2014. Distributed rep- resentations of sentences and documents. In Pro- ceedings of the 31th International Conference on Machine Learning, ICML 2014. pages 1188-1196.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Link-based classification",
                "authors": [
                    {
                        "first": "Qing",
                        "middle": [],
                        "last": "Lu",
                        "suffix": ""
                    },
                    {
                        "first": "Lise",
                        "middle": [],
                        "last": "Getoor",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Machine Learning, Proceedings of the Twentieth International Conference (ICML 2003)",
                "volume": "",
                "issue": "",
                "pages": "496--503",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Qing Lu and Lise Getoor. 2003. Link-based classifi- cation. In Machine Learning, Proceedings of the Twentieth International Conference (ICML 2003). pages 496-503.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Distributed representations of words and phrases and their compositionality",
                "authors": [
                    {
                        "first": "Tomas",
                        "middle": [],
                        "last": "Mikolov",
                        "suffix": ""
                    },
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    },
                    {
                        "first": "Kai",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Gregory",
                        "middle": [
                            "S"
                        ],
                        "last": "Corrado",
                        "suffix": ""
                    },
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Dean",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "3111--3119",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. 2013. Distributed rep- resentations of words and phrases and their com- positionality. In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013.. pages 3111-3119.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Improving document ranking with dual word embeddings",
                "authors": [
                    {
                        "first": "Eric",
                        "middle": [
                            "T"
                        ],
                        "last": "Nalisnick",
                        "suffix": ""
                    },
                    {
                        "first": "Bhaskar",
                        "middle": [],
                        "last": "Mitra",
                        "suffix": ""
                    },
                    {
                        "first": "Nick",
                        "middle": [],
                        "last": "Craswell",
                        "suffix": ""
                    },
                    {
                        "first": "Rich",
                        "middle": [],
                        "last": "Caruana",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the 25th International Conference on World Wide Web, WWW 2016, Companion Volume",
                "volume": "",
                "issue": "",
                "pages": "83--84",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Eric T. Nalisnick, Bhaskar Mitra, Nick Craswell, and Rich Caruana. 2016. Improving document ranking with dual word embeddings. In Proceedings of the 25th International Conference on World Wide Web, WWW 2016, Companion Volume. pages 83-84.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "The pagerank citation ranking: Bringing order to the web",
                "authors": [
                    {
                        "first": "Lawrence",
                        "middle": [],
                        "last": "Page",
                        "suffix": ""
                    },
                    {
                        "first": "Sergey",
                        "middle": [],
                        "last": "Brin",
                        "suffix": ""
                    },
                    {
                        "first": "Rajeev",
                        "middle": [],
                        "last": "Motwani",
                        "suffix": ""
                    },
                    {
                        "first": "Terry",
                        "middle": [],
                        "last": "Winograd",
                        "suffix": ""
                    }
                ],
                "year": 1999,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1999. The pagerank citation rank- ing: Bringing order to the web. .",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Bleu: a method for automatic evaluation of machine translation",
                "authors": [
                    {
                        "first": "Kishore",
                        "middle": [],
                        "last": "Papineni",
                        "suffix": ""
                    },
                    {
                        "first": "Salim",
                        "middle": [],
                        "last": "Roukos",
                        "suffix": ""
                    },
                    {
                        "first": "Todd",
                        "middle": [],
                        "last": "Ward",
                        "suffix": ""
                    },
                    {
                        "first": "Wei-Jing",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "311--318",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic eval- uation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Compu- tational Linguistics. pages 311-318.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Deepwalk: online learning of social representations",
                "authors": [
                    {
                        "first": "Bryan",
                        "middle": [],
                        "last": "Perozzi",
                        "suffix": ""
                    },
                    {
                        "first": "Rami",
                        "middle": [],
                        "last": "Al-Rfou",
                        "suffix": ""
                    },
                    {
                        "first": "Steven",
                        "middle": [],
                        "last": "Skiena",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "The 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '14",
                "volume": "",
                "issue": "",
                "pages": "701--710",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. Deepwalk: online learning of social repre- sentations. In The 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Min- ing, KDD '14. pages 701-710.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Software Framework for Topic Modelling with Large Corpora",
                "authors": [
                    {
                        "first": "Radim",
                        "middle": [],
                        "last": "\u0158eh\u016f\u0159ek",
                        "suffix": ""
                    },
                    {
                        "first": "Petr",
                        "middle": [],
                        "last": "Sojka",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks",
                "volume": "",
                "issue": "",
                "pages": "45--50",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Radim \u0158eh\u016f\u0159ek and Petr Sojka. 2010. Software Frame- work for Topic Modelling with Large Corpora. In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks. ELRA, Valletta, Malta, pages 45-50. http://is.muni.cz/ publication/884893/en.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Entity linking with a knowledge base: Issues, techniques, and solutions",
                "authors": [
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Shen",
                        "suffix": ""
                    },
                    {
                        "first": "Jianyong",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Jiawei",
                        "middle": [],
                        "last": "Han",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "IEEE Trans. Knowl. Data Eng",
                "volume": "27",
                "issue": "2",
                "pages": "443--460",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Wei Shen, Jianyong Wang, and Jiawei Han. 2015. En- tity linking with a knowledge base: Issues, tech- niques, and solutions. IEEE Trans. Knowl. Data Eng. 27(2):443-460.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Scholarly paper recommendation via user's recent research interests",
                "authors": [
                    {
                        "first": "Kazunari",
                        "middle": [],
                        "last": "Sugiyama",
                        "suffix": ""
                    },
                    {
                        "first": "Min-Yen",
                        "middle": [],
                        "last": "Kan",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the 2010 Joint International Conference on Digital Libraries, JCDL 2010",
                "volume": "",
                "issue": "",
                "pages": "29--38",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kazunari Sugiyama and Min-Yen Kan. 2010. Schol- arly paper recommendation via user's recent re- search interests. In Proceedings of the 2010 Joint In- ternational Conference on Digital Libraries, JCDL 2010. pages 29-38.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Modeling mention, context and entity with neural networks for entity disambiguation",
                "authors": [
                    {
                        "first": "Yaming",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Lei",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Duyu",
                        "middle": [],
                        "last": "Tang",
                        "suffix": ""
                    },
                    {
                        "first": "Nan",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Zhenzhou",
                        "middle": [],
                        "last": "Ji",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaolong",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015",
                "volume": "",
                "issue": "",
                "pages": "1333--1339",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yaming Sun, Lei Lin, Duyu Tang, Nan Yang, Zhen- zhou Ji, and Xiaolong Wang. 2015. Modeling men- tion, context and entity with neural networks for en- tity disambiguation. In Proceedings of the Twenty- Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015. pages 1333-1339.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "PTE: predictive text embedding through large-scale heterogeneous text networks",
                "authors": [
                    {
                        "first": "Jian",
                        "middle": [],
                        "last": "Tang",
                        "suffix": ""
                    },
                    {
                        "first": "Meng",
                        "middle": [],
                        "last": "Qu",
                        "suffix": ""
                    },
                    {
                        "first": "Qiaozhu",
                        "middle": [],
                        "last": "Mei",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
                "volume": "",
                "issue": "",
                "pages": "1165--1174",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jian Tang, Meng Qu, and Qiaozhu Mei. 2015a. PTE: predictive text embedding through large-scale het- erogeneous text networks. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. pages 1165-1174.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "LINE: large-scale information network embedding",
                "authors": [
                    {
                        "first": "Jian",
                        "middle": [],
                        "last": "Tang",
                        "suffix": ""
                    },
                    {
                        "first": "Meng",
                        "middle": [],
                        "last": "Qu",
                        "suffix": ""
                    },
                    {
                        "first": "Mingzhe",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Ming",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Jun",
                        "middle": [],
                        "last": "Yan",
                        "suffix": ""
                    },
                    {
                        "first": "Qiaozhu",
                        "middle": [],
                        "last": "Mei",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the 24th International Conference on World Wide Web, WWW 2015",
                "volume": "",
                "issue": "",
                "pages": "1067--1077",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. 2015b. LINE: large-scale information network embedding. In Proceedings of the 24th International Conference on World Wide Web, WWW 2015. pages 1067-1077.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "A discriminative approach to topic-based citation recommendation",
                "authors": [
                    {
                        "first": "Jie",
                        "middle": [],
                        "last": "Tang",
                        "suffix": ""
                    },
                    {
                        "first": "Jing",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Advances in Knowledge Discovery and Data Mining, 13th Pacific-Asia Conference",
                "volume": "",
                "issue": "",
                "pages": "572--579",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jie Tang and Jing Zhang. 2009. A discriminative ap- proach to topic-based citation recommendation. In Advances in Knowledge Discovery and Data Min- ing, 13th Pacific-Asia Conference, PAKDD 2009. pages 572-579.",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "Automatic classification of citation function",
                "authors": [
                    {
                        "first": "Simone",
                        "middle": [],
                        "last": "Teufel",
                        "suffix": ""
                    },
                    {
                        "first": "Advaith",
                        "middle": [],
                        "last": "Siddharthan",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Tidhar",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "EMNLP 2007, Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "103--110",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Simone Teufel, Advaith Siddharthan, and Dan Tidhar. 2006. Automatic classification of citation function. In EMNLP 2007, Proceedings of the 2006 Confer- ence on Empirical Methods in Natural Language Processing. pages 103-110.",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "Max-margin deepwalk: Discriminative learning of network representation",
                "authors": [
                    {
                        "first": "Cunchao",
                        "middle": [],
                        "last": "Tu",
                        "suffix": ""
                    },
                    {
                        "first": "Weicheng",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Zhiyuan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Maosong",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI 2016",
                "volume": "",
                "issue": "",
                "pages": "3889--3895",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Cunchao Tu, Weicheng Zhang, Zhiyuan Liu, and Maosong Sun. 2016. Max-margin deepwalk: Dis- criminative learning of network representation. In Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI 2016. pages 3889-3895.",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "Linked document embedding for classification",
                "authors": [
                    {
                        "first": "Suhang",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Jiliang",
                        "middle": [],
                        "last": "Tang",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Charu",
                        "suffix": ""
                    },
                    {
                        "first": "Huan",
                        "middle": [],
                        "last": "Aggarwal",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the 25th ACM International Conference on Information and Knowledge Management, CIKM 2016",
                "volume": "",
                "issue": "",
                "pages": "115--124",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Suhang Wang, Jiliang Tang, Charu C. Aggarwal, and Huan Liu. 2016. Linked document embedding for classification. In Proceedings of the 25th ACM In- ternational Conference on Information and Knowl- edge Management, CIKM 2016. pages 115-124.",
                "links": null
            },
            "BIBREF34": {
                "ref_id": "b34",
                "title": "Joint learning of the embedding of words and entities for named entity disambiguation",
                "authors": [
                    {
                        "first": "Ikuya",
                        "middle": [],
                        "last": "Yamada",
                        "suffix": ""
                    },
                    {
                        "first": "Hiroyuki",
                        "middle": [],
                        "last": "Shindo",
                        "suffix": ""
                    },
                    {
                        "first": "Hideaki",
                        "middle": [],
                        "last": "Takeda",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshiyasu",
                        "middle": [],
                        "last": "Takefuji",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning",
                "volume": "",
                "issue": "",
                "pages": "250--259",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ikuya Yamada, Hiroyuki Shindo, Hideaki Takeda, and Yoshiyasu Takefuji. 2016. Joint learning of the em- bedding of words and entities for named entity dis- ambiguation. In Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning, CoNLL 2016. pages 250-259.",
                "links": null
            },
            "BIBREF35": {
                "ref_id": "b35",
                "title": "Network representation learning with rich text information",
                "authors": [
                    {
                        "first": "Cheng",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Zhiyuan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Deli",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Maosong",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Edward",
                        "middle": [
                            "Y"
                        ],
                        "last": "Chang",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015",
                "volume": "",
                "issue": "",
                "pages": "2111--2117",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Cheng Yang, Zhiyuan Liu, Deli Zhao, Maosong Sun, and Edward Y. Chang. 2015. Network representa- tion learning with rich text information. In Proceed- ings of the Twenty-Fourth International Joint Con- ference on Artificial Intelligence, IJCAI 2015. pages 2111-2117.",
                "links": null
            },
            "BIBREF36": {
                "ref_id": "b36",
                "title": "A fast fertility hidden markov model for word alignment using MCMC",
                "authors": [
                    {
                        "first": "Shaojun",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Gildea",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP 2010",
                "volume": "",
                "issue": "",
                "pages": "596--605",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Shaojun Zhao and Daniel Gildea. 2010. A fast fertil- ity hidden markov model for word alignment using MCMC. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Process- ing, EMNLP 2010. pages 596-605.",
                "links": null
            },
            "BIBREF37": {
                "ref_id": "b37",
                "title": "Robust and collective entity disambiguation through semantic embeddings",
                "authors": [
                    {
                        "first": "Stefan",
                        "middle": [],
                        "last": "Zwicklbauer",
                        "suffix": ""
                    },
                    {
                        "first": "Christin",
                        "middle": [],
                        "last": "Seifert",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Granitzer",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval",
                "volume": "",
                "issue": "",
                "pages": "425--434",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Stefan Zwicklbauer, Christin Seifert, and Michael Granitzer. 2016. Robust and collective entity dis- ambiguation through semantic embeddings. In Pro- ceedings of the 39th International ACM SIGIR con- ference on Research and Development in Informa- tion Retrieval, SIGIR 2016. pages 425-434.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "(a), from w2v's perspective in Figure 1(b), \". . . computing the machine translation BLEU . . . \" and other text no longer have association with",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Figure 2: The hyperdoc2vec model.",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "Figure 3: Varying k on DBLP. The scores of w2v keeps increasing to 26.63 at k = 1000, and then begins to drop. Although at the cost of a larger model and longer training/inference time, it still cannot outperform h-d2v of 30.37 at k = 400.",
                "uris": null,
                "fig_num": "3",
                "type_str": "figure"
            },
            "FIGREF3": {
                "num": null,
                "text": "Figure 4: F 1 of citation function classification.",
                "uris": null,
                "fig_num": "4",
                "type_str": "figure"
            },
            "FIGREF4": {
                "num": null,
                "text": "Figure 5: Rec@10 w.r.t. citation functions.",
                "uris": null,
                "fig_num": "5",
                "type_str": "figure"
            },
            "TABREF1": {
                "content": "<table><tr><td>Moses system \u2026</td><td/></tr><tr><td>\u2026 machine</td><td/></tr><tr><td>translation BLEU score \u2026</td><td>\u2026 Moses system \u2026</td></tr><tr><td colspan=\"2\">(c) Context as content.</td></tr><tr><td colspan=\"2\">Figure 1: An example of Zhao and Gildea (2010) citing Papineni et al. (2002) and existing approaches.</td></tr><tr><td>i.e., W \u222aD. For example, web pages could be sim-</td><td/></tr><tr><td>plified as streams of words and URLs, and papers</td><td/></tr><tr><td>are actually sequences of words and cited DOIs.</td><td/></tr><tr><td>If a document id d t with some surrounding</td><td/></tr><tr><td>words C</td><td/></tr></table>",
                "type_str": "table",
                "text": "appear in the hyper-doc of d s , i.e., H ds , we stipulate that a hyper-link d s , C, d t is formed. Herein d s , d t \u2208 D are ids of the source and target documents, respectively; C \u2286 W are context words. Figure 1(a) exemplifies a hyperlink.",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table><tr><td/><td colspan=\"2\">Impacts Task?</td><td colspan=\"3\">Addressed by Approach?</td></tr><tr><td>Desired Property</td><td>Classification</td><td>Citation Rec-ommendation</td><td colspan=\"3\">w2v d2v-nc d2v-cac h-d2v</td></tr><tr><td>Context aware</td><td/><td/><td/><td>\u00d7</td></tr><tr><td>Content aware</td><td/><td/><td>\u00d7</td><td/></tr><tr><td>Newcomer friendly</td><td/><td/><td>\u00d7</td><td/></tr><tr><td>Context intent aware</td><td>\u00d7</td><td/><td>\u00d7</td><td>\u00d7</td><td>\u00d7</td></tr></table>",
                "type_str": "table",
                "text": "Analysis of tasks and approaches w.r.t. desired properties.",
                "html": null,
                "num": null
            },
            "TABREF4": {
                "content": "<table/>",
                "type_str": "table",
                "text": "The statistics of three datasets.Finally, similar to w2v(Mikolov et al., 2013) and d2v (Le andMikolov, 2014), to make training efficient, we adopt negative sampling:",
                "html": null,
                "num": null
            },
            "TABREF5": {
                "content": "<table><tr><td>Model</td><td>Original Macro Micro Macro Micro w/ DeepWalk</td><td>Model</td><td colspan=\"2\">Content Aware/ Newcomer Friendly Macro Micro Macro Micro Original w/ DeepWalk</td></tr><tr><td>DeepWalk</td><td>61.67 69.89 61.67 69.89</td><td>DeepWalk</td><td>-</td><td>66.57 76.56 66.57 76.56</td></tr><tr><td>w2v (I) w2v (I+O) d2v-nc d2v-cac</td><td>10.83 41.84 31.06 50.93 9.36 41.26 25.92 49.56 70.62 77.86 70.64 78.06 71.83 78.09 71.57 78.59</td><td>w2v (I) w2v (I+O) d2v-nc d2v-cac</td><td>\u00d7 / \u00d7 \u00d7 / \u00d7 / /</td><td>19.77 47.32 59.80 72.90 15.97 45.66 50.77 70.08 61.54 73.73 69.37 78.22 65.23 75.93 70.43 78.75</td></tr><tr><td>h-d2v (I)</td><td>68.81 76.33 73.96 79.93</td><td>h-d2v (I)</td><td>/</td><td>58.59 69.79 66.99 75.63</td></tr><tr><td colspan=\"2\">h-d2v (I+O) 72.89 78.99 73.24 79.55</td><td>h-d2v (I+O)</td><td>/</td><td>66.64 75.19 68.96 76.61</td></tr></table>",
                "type_str": "table",
                "text": "F 1 scores on DBLP.",
                "html": null,
                "num": null
            },
            "TABREF8": {
                "content": "<table><tr><td>Model</td><td/><td>Newcomer Friendly</td><td colspan=\"2\">Rec MAP MRR nDCG</td></tr><tr><td colspan=\"2\">w2v (I4O)</td><td>\u00d7</td><td>3.64 3.23 3.41</td><td>2.73</td></tr><tr><td>NPM</td><td/><td>\u00d7</td><td>1.37 1.13 1.15</td><td>0.92</td></tr><tr><td>d2v-nc</td><td/><td/><td>6.48 3.52 3.54</td><td>3.96</td></tr><tr><td>d2v-cac</td><td/><td/><td>8.16 5.13 5.24</td><td>5.21</td></tr><tr><td>h-d2v</td><td/><td/><td>6.41 4.95 5.21</td><td>4.49</td></tr><tr><td colspan=\"5\">Table 7: DBLP results evaluated on 63,342 cita-</td></tr><tr><td colspan=\"4\">tion contexts with newcomer ground-truth.</td></tr><tr><td colspan=\"3\">Category Description</td><td/></tr><tr><td>Weak</td><td colspan=\"3\">Weakness of cited approach</td></tr><tr><td colspan=\"5\">CoCoGM Contrast/Comparison in Goals/Methods (neutral)</td></tr><tr><td>CoCo-</td><td colspan=\"3\">Work stated to be superior to cited work</td></tr><tr><td colspan=\"4\">CoCoR0 Contrast/Comparison in Results (neutral)</td></tr><tr><td colspan=\"4\">CoCoXY Contrast between 2 cited methods</td></tr><tr><td>PBas</td><td colspan=\"4\">Author uses cited work as basis or starting point</td></tr><tr><td>PUse</td><td colspan=\"4\">Author uses tools/algorithms/data/definitions</td></tr><tr><td>PModi</td><td colspan=\"4\">Author adapts or modifies tools/algorithms/data</td></tr><tr><td>PMot</td><td colspan=\"4\">This citation is positive about approach used or</td></tr><tr><td/><td colspan=\"4\">problem addressed (used to motivate work in cur-</td></tr><tr><td/><td colspan=\"2\">rent paper)</td><td/></tr><tr><td>PSim</td><td colspan=\"3\">Author's work and cited work are similar</td></tr><tr><td>PSup</td><td colspan=\"4\">Author's work and cited work are compati-</td></tr><tr><td/><td colspan=\"3\">ble/provide support for each other</td></tr><tr><td>Neut</td><td colspan=\"4\">Neutral description of cited work, or not enough</td></tr><tr><td/><td colspan=\"4\">textual evidence for above categories, or unlisted</td></tr><tr><td/><td colspan=\"2\">citation function</td><td/></tr><tr><td colspan=\"5\">Table 8: Annotation scheme of citation functions</td></tr><tr><td colspan=\"3\">in Teufel et al. (2006).</td><td/></tr><tr><td colspan=\"5\">necessarily get zero scores. The table shows that</td></tr><tr><td colspan=\"5\">newcomer friendly approaches are superior to un-</td></tr><tr><td colspan=\"5\">friendly ones. Note that, like Table 5, this table is</td></tr><tr><td colspan=\"5\">also based on controlled experiments and not in-</td></tr><tr><td colspan=\"4\">tended for comparing approaches.</td></tr><tr><td colspan=\"5\">5.3.3 Impact of Context Intent Awareness</td></tr><tr><td colspan=\"5\">In this section, we analyze the impact of context</td></tr><tr><td colspan=\"5\">intent awareness. We use Teufel et al. (2006)'s</td></tr><tr><td colspan=\"5\">2,824 citation contexts 11 with annotated citation</td></tr><tr><td colspan=\"5\">functions, e.g., emphasizing weakness (Weak) or</td></tr><tr><td colspan=\"5\">using tools/algorithms (PBas) of the cited papers.</td></tr><tr><td colspan=\"5\">Table 8 from Teufel et al. (2006) describes the full</td></tr><tr><td colspan=\"5\">annotating scheme. Teufel et al. (2006) also use</td></tr><tr><td colspan=\"5\">manual features to evaluate citation function clas-</td></tr><tr><td colspan=\"5\">sification. To test all models on capturing con-</td></tr><tr><td colspan=\"5\">text intents, we average all context words' IN vec-</td></tr><tr><td colspan=\"5\">tors (trained on DBLP) as features. Noticing that</td></tr><tr><td colspan=\"5\">pv-dbow does not output IN word vectors, and</td></tr><tr><td colspan=\"5\">OUT vectors do not provide reasonable results, we</td></tr><tr><td colspan=\"5\">use pv-dm here instead. We use SVM with RBF</td></tr></table>",
                "type_str": "table",
                "text": "Table 7 analyzes the impact of newcomer friendliness. Opposite from what is done in Section 5.2.2, we only evaluate on testing examples where at least a ground-truth paper is a newcomer. Please note that newcomer unfriendly approaches do not Papers recommended by different approaches for a citation context in",
                "html": null,
                "num": null
            }
        }
    }
}