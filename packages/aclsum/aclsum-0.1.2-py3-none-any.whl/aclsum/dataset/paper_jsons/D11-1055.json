{
    "paper_id": "D11-1055",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T11:56:34.856315Z"
    },
    "title": "Predicting a Scientific Community's Response to an Article",
    "authors": [
        {
            "first": "Dani",
            "middle": [],
            "last": "Yogatama",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Carnegie Mellon University Pittsburgh",
                "location": {
                    "postCode": "15213",
                    "region": "PA",
                    "country": "USA"
                }
            },
            "email": "dyogatama@cs.cmu.edu"
        },
        {
            "first": "Michael",
            "middle": [],
            "last": "Heilman",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Carnegie Mellon University Pittsburgh",
                "location": {
                    "postCode": "15213",
                    "region": "PA",
                    "country": "USA"
                }
            },
            "email": "mheilman@cs.cmu.edu"
        },
        {
            "first": "Brendan",
            "middle": [],
            "last": "O'connor",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Carnegie Mellon University Pittsburgh",
                "location": {
                    "postCode": "15213",
                    "region": "PA",
                    "country": "USA"
                }
            },
            "email": ""
        },
        {
            "first": "Chris",
            "middle": [],
            "last": "Dyer",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Carnegie Mellon University Pittsburgh",
                "location": {
                    "postCode": "15213",
                    "region": "PA",
                    "country": "USA"
                }
            },
            "email": "cdyer@cs.cmu.edu"
        },
        {
            "first": "Bryan",
            "middle": [
                "R"
            ],
            "last": "Routledge",
            "suffix": "",
            "affiliation": {},
            "email": "routledge@cmu.edu"
        },
        {
            "first": "Noah",
            "middle": [
                "A"
            ],
            "last": "Smith",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Carnegie Mellon University Pittsburgh",
                "location": {
                    "postCode": "15213",
                    "region": "PA",
                    "country": "USA"
                }
            },
            "email": "nasmith@cs.cmu.edu"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "We consider the problem of predicting measurable responses to scientific articles based primarily on their text content.\nSpecifically, we consider papers in two fields (economics and computational linguistics) and make predictions about downloads and within-community citations. Our approach is based on generalized linear models, allowing interpretability; a novel extension that captures first-order temporal effects is also presented. We demonstrate that text features significantly improve accuracy of predictions over metadata features like authors, topical categories, and publication venues.",
    "pdf_parse": {
        "paper_id": "D11-1055",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "We consider the problem of predicting measurable responses to scientific articles based primarily on their text content.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            },
            {
                "text": "Specifically, we consider papers in two fields (economics and computational linguistics) and make predictions about downloads and within-community citations. Our approach is based on generalized linear models, allowing interpretability; a novel extension that captures first-order temporal effects is also presented. We demonstrate that text features significantly improve accuracy of predictions over metadata features like authors, topical categories, and publication venues.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Written communication is an essential component of the complex social phenomenon of science. As such, natural language processing is well-positioned to provide tools for understanding the scientific process, by analyzing the textual artifacts (papers, proceedings, etc.) that it produces. This paper is about modeling collections of scientific documents to understand how their textual content relates to how a scientific community responds to them. While past work has often focused on citation structure (Borner et al., 2003; Qazvinian and Radev, 2008) , our emphasis is on the text content, following Ramage et al. (2010) and Gerrish and Blei (2010) .",
                "cite_spans": [
                    {
                        "start": 506,
                        "end": 527,
                        "text": "(Borner et al., 2003;",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 528,
                        "end": 554,
                        "text": "Qazvinian and Radev, 2008)",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 604,
                        "end": 624,
                        "text": "Ramage et al. (2010)",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 629,
                        "end": 652,
                        "text": "Gerrish and Blei (2010)",
                        "ref_id": "BIBREF10"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Instead of task-independent exploratory data analysis (e.g., topic modeling) or multi-document sum-marization, we consider supervised models of the collective response of a scientific community to a published article. There are many measures of impact of a scientific paper; ours come from direct measurements of the number of downloads (from an established website where prominent economists post papers before formal publication) and citations (within a fixed scientific community). We adopt a discriminative approach based on generalized linear models that can make use of any text or metadata features, and show that simple lexical features offer substantial power in modeling out-ofsample response and in forecasting response for future articles. Realistic forecasting evaluations require methodological care beyond the usual best practices of train/test separation, and we elucidate these issues.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In addition, we introduce a new regularization technique that leverages the intuition that the relationship between observable features and response should evolve smoothly over time. This regularizer allows the learner to rely more strongly on more recent evidence, while taking into account a long history of training data. Our time series-inspired regularizer is computationally efficient in learning and is a significant advance over earlier text-driven forecasting models that ignore the time variable altogether (Kogan et al., 2009; Joshi et al., 2010) .",
                "cite_spans": [
                    {
                        "start": 517,
                        "end": 537,
                        "text": "(Kogan et al., 2009;",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 538,
                        "end": 557,
                        "text": "Joshi et al., 2010)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "We evaluate our approaches in two novel experimental settings: predicting downloads of economics articles and predicting citation of papers at ACL conferences. Our approaches substantially outper-594 form text-ignorant baselines on ground-truth predictions. Our time series models permit flexibility in features and offer a novel and perhaps more interpretable view of the data than summary statistics.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "We make use of two collections of scientific literature, one from the economics domain, and the other from computational linguistics and natural language processing. Statistics are summarized in Table 1 .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 201,
                        "end": 202,
                        "text": "1",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Data",
                "sec_num": "2"
            },
            {
                "text": "Our first dataset consists of research papers in economics from the National Bureau of Economic Research (NBER) from 1999 to 2009 (http:// www.nber.org). Approximately 1,000 research economists are affiliated with the NBER. New NBER working papers are posted to the website weekly. The papers are not yet peer-reviewed, but given the prominence of many economists affiliated with the NBER, many of the papers are widely read.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "NBER",
                "sec_num": "2.1"
            },
            {
                "text": "Text from the abstracts of the papers and related metadata are publicly available. Full text is available to subscribers (universities typically have access).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "NBER",
                "sec_num": "2.1"
            },
            {
                "text": "The NBER provided us with download statistics for these papers. For each paper, we computed the total number of downloads in the first year after each paper's posting. 1 The download counts are log-normally distributed, as shown in Figure 1 , and so our regression models ( \u00a73) minimize squared errors in the log space. Our download logs begin in 1 For the vast majority of papers, most of the downloads occur soon after the paper's posting. We explored different measures with different download windows (two years, for example) with broadly similar results. We leave a more detailed analysis of the time series patterns of downloads to future work. 1999. We use the 8,814 papers from 1999-2009 period (there are 16,334 papers in the full dataset dating back to 1985). We only use text from the abstracts, since we were able to obtain full texts for just a portion of the papers, and since the OCR of the full texts we do have is very noisy.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 239,
                        "end": 240,
                        "text": "1",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "NBER",
                "sec_num": "2.1"
            },
            {
                "text": "Our second dataset consists of research papers from the Association for Computational Linguistics (ACL) from 1980 to 2006 (Radev et al., 2009a; Radev et al., 2009b) . We have the full texts for papers (OCR output) as well as structured citation data. There are 15,689 papers in the whole dataset. For the citation prediction task, we include conference papers from ACL, EACL, HLT, and NAACL. 2 We remove journal papers, since they are characteristically different from conference papers, as well as workshop papers. We do include short papers, interactive demo session papers, and student research papers that are included in the companion volumes for these conferences (such papers are cited less than full papers, but many are still cited). The resulting dataset contains 4,026 papers. The number of papers in each year varies because not all conferences are annual. We look at citations in the three-year window following publication, excluding self-citations and only considering citations from papers within these conferences. Figure 1 shows a histogram; note that many papers (54%) are not cited at all, and the distribution of citations per paper is neither normal nor log-normal. We organize the papers into two classes: those with zero citations and those with non-zero citations in the three-year window.",
                "cite_spans": [
                    {
                        "start": 122,
                        "end": 143,
                        "text": "(Radev et al., 2009a;",
                        "ref_id": null
                    },
                    {
                        "start": 144,
                        "end": 164,
                        "text": "Radev et al., 2009b)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 1039,
                        "end": 1040,
                        "text": "1",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "ACL",
                "sec_num": "2.2"
            },
            {
                "text": "Our forecasting approach is based on generalized linear models for regression and classification. The models are trained with an 2 -penalty, often called a \"ridge\" model (Hoerl and Kennard, 1970) . 3 For the NBER data, where (log) number of downloads is nearly a continuous measure, we use linear regression. For the ACL data, where response is the binary cited-or-not variable we use logistic regression, often referred to as a \"maximum entropy\" model (Berger et al., 1996) or a log-linear model. We briefly review the class of models. Then, we describe a time series model appropriate for time series data.",
                "cite_spans": [
                    {
                        "start": 170,
                        "end": 195,
                        "text": "(Hoerl and Kennard, 1970)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 453,
                        "end": 474,
                        "text": "(Berger et al., 1996)",
                        "ref_id": "BIBREF1"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model",
                "sec_num": "3"
            },
            {
                "text": "Consider a model that predicts a response y given a vector input x = x 1 , . . . , x d \u2208 R d . Our models are linear functions of x and parameterized by the vector \u03b2. Given a corpus of M document features, X, and responses Y , we estimate:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generalized Linear Models",
                "sec_num": "3.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "\u03b2 = argmin \u03b2 R(\u03b2) + L(\u03b2, X, Y )",
                        "eq_num": "(1)"
                    }
                ],
                "section": "Generalized Linear Models",
                "sec_num": "3.1"
            },
            {
                "text": "where L is a model-dependent loss function and R is a regularization penalty to encourage models with small weight vectors. We describe models and loss functions first and then turn to regularization.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generalized Linear Models",
                "sec_num": "3.1"
            },
            {
                "text": "For the NBER data, the (log) number of downloads is continuous, and so we use least-squares linear regression model. The loss function is the sum of the squared errors for the M documents in our training data: L(\u03b2, X, Y ) = M i=1 (y i\u0177i ) 2 , where the prediction rule for new documents is: \u0177 = d j=0 \u03b2 j x j . Probabilistically, this equates to an assumption that \u03b2 x is the mean of a normal (i.e., Gaussian) distribution from which random variable y is drawn.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generalized Linear Models",
                "sec_num": "3.1"
            },
            {
                "text": "For the ACL data, we predict y from a discrete set C (specifically, the binary set of zero citations or more than zero citations), and we use logistic regression. This model assumes that for the ith training input x i , the output y i is drawn according to:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generalized Linear Models",
                "sec_num": "3.1"
            },
            {
                "text": "p(y i | x i ) = exp \u03b2 c x i c \u2208C exp \u03b2 c x i",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generalized Linear Models",
                "sec_num": "3.1"
            },
            {
                "text": "3 Preliminary experiments found no consistent benefit from 1 (\"lasso\") models, though we note that 1-regularization leads to sparse, compact models that may be more interpretable.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generalized Linear Models",
                "sec_num": "3.1"
            },
            {
                "text": "where there is a feature vector \u03b2 c for each class c \u2208 C. Under this interpretation, parameter estimation is maximum a posteriori inference for \u03b2, and R(\u03b2) is a log-prior for the weights. The loss function is the negative log likelihood for the M documents: L(\u03b2, X, Y ) = -M i=1 log p(y i | x i ). The prediction rule for a new document is: \u0177 = argmax c\u2208C d j=0 \u03b2 c,j x j . Generalized linear models and penalized regression are well-studied with an extensive literature (Mccullagh and Nelder, 1989; Hastie et al., 2009) . We leave other types of models, such as Poisson (Cameron and Trivedi, 1998) or ordinal (McCullagh, 1980) regression models, to future work.",
                "cite_spans": [
                    {
                        "start": 471,
                        "end": 499,
                        "text": "(Mccullagh and Nelder, 1989;",
                        "ref_id": null
                    },
                    {
                        "start": 500,
                        "end": 520,
                        "text": "Hastie et al., 2009)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 571,
                        "end": 598,
                        "text": "(Cameron and Trivedi, 1998)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 610,
                        "end": 627,
                        "text": "(McCullagh, 1980)",
                        "ref_id": "BIBREF20"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generalized Linear Models",
                "sec_num": "3.1"
            },
            {
                "text": "With large numbers of features, regularization is crucial to avoid overfitting. In ridge regression (Hoerl and Kennard, 1970) , a standard method to which we compare the time series regularization discussed in \u00a73.3, the penalty R(\u03b2) is proportional to the 2norm of \u03b2:",
                "cite_spans": [
                    {
                        "start": 100,
                        "end": 125,
                        "text": "(Hoerl and Kennard, 1970)",
                        "ref_id": "BIBREF14"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Ridge Regression",
                "sec_num": "3.2"
            },
            {
                "text": "R(\u03b2) = \u03bb \u03b2 2 = \u03bb j \u03b2 2 j",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Ridge Regression",
                "sec_num": "3.2"
            },
            {
                "text": "where \u03bb is a regularization hyperparameter that is tuned on development data or by cross-validation. 4This penalty pushes many \u03b2 j close (but not completely) to zero. In practice, we multiply the penalty by the number of examples M to facilitate tuning of \u03bb.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Ridge Regression",
                "sec_num": "3.2"
            },
            {
                "text": "The ridge linear regression model can be interpreted probabilistically as each coefficient \u03b2 j is drawn i.i.d. from a normal distribution with mean 0 and variance 2\u03bb -1 .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Ridge Regression",
                "sec_num": "3.2"
            },
            {
                "text": "A simple way to capture temporal variation is to conjoin traditional features with a time variable. Here, we divide the dataset into T time steps (years). In the new representation, the feature space expands from R d to R T \u00d7d . For a document published at year t, the elements of x are non-zero only for those features that correspond to year-t; that is x t ,j = 0 for all t = t.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Time Series Regularization",
                "sec_num": "3.3"
            },
            {
                "text": "Estimating this model with the new features using the 2 -penalty would be effectively estimating separate models for each year under the assumption that each \u03b2 t,j is independent; even for features that differed only temporally (e.g., \u03b2 t,j and \u03b2 t+1,j ).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Time Series Regularization",
                "sec_num": "3.3"
            },
            {
                "text": "In this work, we apply time series regularization to GLMs, enabling models that have coefficients that change over time but prefer gradual changes across time steps. Boyd and Vandenberghe (2004, \u00a76. 3) describe a general version of this sort of regularizer. To our knowledge, such regularizers have not previously been applied to temporal modeling of text.",
                "cite_spans": [
                    {
                        "start": 166,
                        "end": 198,
                        "text": "Boyd and Vandenberghe (2004, \u00a76.",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Time Series Regularization",
                "sec_num": "3.3"
            },
            {
                "text": "The time series regularization penalty becomes:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Time Series Regularization",
                "sec_num": "3.3"
            },
            {
                "text": "R(\u03b2) = \u03bb T t=1 d j=1 \u03b2 2 t,j +\u03bb\u03b1 T t=2 d j=0 (\u03b2 t,j -\u03b2 t-1,j ) 2",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Time Series Regularization",
                "sec_num": "3.3"
            },
            {
                "text": "It includes a standard 2 -penalty on the coefficients, and a penalty for differences between coefficients for adjacent time steps to induce smooth changes. 5 Similar to the previous model, in practice, we multiply the regularization constant \u03bb by M T to facilitate tuning of \u03bb for datasets with different numbers of examples M and numbers of time steps T . The new parameter, \u03b1, controls the smoothness of the estimated coefficients. Setting \u03b1 to zero imposes no penalty for time-variation in the coefficients and results in independent ridge regressions at each time step. Also, when the number of examples is constant across time steps, setting a large \u03b1 parameter (\u03b1 \u2192 \u221e) results in a single ridge regression over all years since it imposes \u03b2 t,j = \u03b2 t+1,j for all t \u2208 T .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Time Series Regularization",
                "sec_num": "3.3"
            },
            {
                "text": "The partial derivative is:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Time Series Regularization",
                "sec_num": "3.3"
            },
            {
                "text": "\u2202R/\u2202\u03b2 t,j = 2\u03bb\u03b2 t,j + 1{t > 1}2\u03bb\u03b1(\u03b2 t,j -\u03b2 t-1,j ) + 1{t < T }2\u03bb\u03b1(\u03b2 t,j -\u03b2 t+1,j )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Time Series Regularization",
                "sec_num": "3.3"
            },
            {
                "text": "This time series regularization can be applied more generally, not just to linear and logistic regression.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Time Series Regularization",
                "sec_num": "3.3"
            },
            {
                "text": "With either ridge regularization or this time series regularization scheme, Eq. 1 is an unconstrained convex optimization problem for the linear models 5 Our implementation of the time series regularizer does not penalize the magnitude of the weight for the bias feature (as in ridge regression). It does, however, penalize the difference in the bias weight between time steps (as with other features). we describe here. There exist a number of optimization procedures for it; we use the L-BFGS quasi-Newton algorithm (Liu and Nocedal, 1989) .",
                "cite_spans": [
                    {
                        "start": 518,
                        "end": 541,
                        "text": "(Liu and Nocedal, 1989)",
                        "ref_id": "BIBREF18"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Time Series Regularization",
                "sec_num": "3.3"
            },
            {
                "text": "\u03b2 1 \u03b2 2 \u03b2 3 \u03b2 T Y 1 Y 2 Y 3 Y T ... X 1 X 2 X 3 X T \u03b1,\u03bb",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Time Series Regularization",
                "sec_num": "3.3"
            },
            {
                "text": "We can interpret the time series regularization probabilistically as follows. Let the coefficient for the jth feature over time be \u03b2 j = \u03b2 1,j , \u03b2 2,j , ..., \u03b2 T,j . \u03b2 j are draws from a multivariate normal distribution with a tridiagonal precision matrix",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Probabilistic Interpretation",
                "sec_num": null
            },
            {
                "text": "\u03a3 -1 = \u039b \u2208 R T \u00d7T : \u039b = \u03bb \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 1 + \u03b1 -\u03b1 0 0 . . . -\u03b1 1 + 2\u03b1 -\u03b1 0 . . . 0 -\u03b1 1 + 2\u03b1 -\u03b1 . . . 0 0 -\u03b1 1 + 2\u03b1 . . . . . . . . . . . . . . . . . . \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Probabilistic Interpretation",
                "sec_num": null
            },
            {
                "text": "The form of R(\u03b2) follows from noting:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Probabilistic Interpretation",
                "sec_num": null
            },
            {
                "text": "-2 log p(\u03b2 j ; \u03b1, \u03bb) = \u03b2 j \u039b\u03b2 j + constant",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Probabilistic Interpretation",
                "sec_num": null
            },
            {
                "text": "The squared difference between adjacent time steps comes from the off-diagonal entries in the precision matrix. 6 Figure 2 shows a graphical representation of the time series regularization in our model. Its Markov chain structure corresponds to the offdiagonals.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 121,
                        "end": 122,
                        "text": "2",
                        "ref_id": "FIGREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Probabilistic Interpretation",
                "sec_num": null
            },
            {
                "text": "There is a rich literature on time series analysis (Box et al., 2008; Hamilton, 1994) . The prior distribution over the sequence \u03b2 1,j , . . . , \u03b2 T,j that our regularizer posits is closely linked to a first-order autoregressive process, AR(1). ",
                "cite_spans": [
                    {
                        "start": 51,
                        "end": 69,
                        "text": "(Box et al., 2008;",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 70,
                        "end": 85,
                        "text": "Hamilton, 1994)",
                        "ref_id": "BIBREF12"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Probabilistic Interpretation",
                "sec_num": null
            },
            {
                "text": "\u2022 Authors' last names as binary features.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "ACL metadata features",
                "sec_num": null
            },
            {
                "text": "\u2022 Conference venues. We use first letter of the ACL anthology paper ID, which correlates with its conference venue (e.g., P for the ACL main conference, H for the HLT conference, etc.). 8",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "ACL metadata features",
                "sec_num": null
            },
            {
                "text": "\u2022 Binary indicator features for the presence of each unigram, bigram, and trigram. For the NBER data, we have separate features for titles and abstracts. For the ACL data, we have separate features for titles and full texts. We pruned text features by document frequency (details in \u00a75). \u2022 Log transformed word counts. We include features for the numbers of words in the title and the abstract (NBER) or the full text (ACL).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Text features",
                "sec_num": null
            },
            {
                "text": "7 Almost all NBER papers are tagged with one or more programs (we assign untagged papers a \"null\" tag). The complete list of NBER programs can be found at http://www.nber. org/programs 8 Papers in the ACL dataset have a tag which shows which workshop, conference, or journal they appeared in. However, sometimes a conference is jointly held with another conference, such that meta information in the dataset is different even though the conference is the same. For this reason, we simply use the first letter of the paper ID.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Text features",
                "sec_num": null
            },
            {
                "text": "For each of the datasets in \u00a72, we test our models for two tasks: forecasting about future papers (i.e., making predictions about papers that appeared after a training dataset) and modeling held-out papers from the past (i.e., making predictions within the same time period as the training dataset, on held-out examples).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "5"
            },
            {
                "text": "For the NBER dataset, the task is to predict the number of downloads a paper will receive in its first year after publication. For the ACL dataset, the task is to predict whether a paper will be cited at all (by another ACL paper in our dataset) within the first three years after its publication. To our knowledge, clean, reliable citation counts are not available for the NBER dataset; nor are download statistics available for the ACL dataset. Table 2 summarizes the variables of interest, model types, and evaluation metrics for the tasks.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 453,
                        "end": 454,
                        "text": "2",
                        "ref_id": "TABREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "5"
            },
            {
                "text": "The lag between a paper's publication and when its outcome (download or citation count) can be observed poses a unique methodological challenge. Consider predicting the number of downloads over g future time steps. If t is the time of forecasting, we can observe the texts of all articles published before t. However, any article published in the interval [tg, t] is too recent for the outcome measurement of y to be taken. We refer to the interval [tg, t] as the \"forecast gap\". Since recent articles are sometimes the most relevant predictions at t, we do not want to ignore them. Consider a paper at time step t , t-g < t < t. To extrapolate its number of downloads, we consider the observed number in [t , t], and then estimate the ratio r of downloads that occur in the first t-t time steps, against the first g time steps, using the fully observed portion of the training data. We then scale the observed downloads during [t , t] by r -1 to extrapolate. The same method is used to extrapolate citation counts.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Extrapolation",
                "sec_num": "5.1"
            },
            {
                "text": "In preliminary experiments, we observed that extrapolating responses for papers in the forecast gap led to better performance in general. For example, for the ridge regressions trained on all past years with the full feature set, the error dropped from 262 to 259 when using extrapolation compared to with-598 out extrapolation. Also, the extrapolated download counts were quite close to the true values (which we have but do not use because of the forecast gap): for example, the mean absolute error of the extrapolated responses was 99 when extrapolated based on the median of the fully observed portion of the training data (measured monthly).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Extrapolation",
                "sec_num": "5.1"
            },
            {
                "text": "In our first set of experiments, we predict the number of downloads of an NBER paper within one year of its publication.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Forecasting NBER Downloads",
                "sec_num": "5.2"
            },
            {
                "text": "We compare four approaches for predicting downloads. The first is a baseline that simply uses the median of the log of the training and development data as the prediction. The second and third use GLMs with ridge regression-style regularization ( \u00a73.2), trained on all past years (\"all years\") and on the single most recent past year (\"one year\"), respectively. The last model (\"time series\") is a GLM with time series regularization ( \u00a73.3).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Forecasting NBER Downloads",
                "sec_num": "5.2"
            },
            {
                "text": "We divided papers by year. Figure 3 illustrates the experimental setup. We held out a random 20% of papers for each year from 1999-2007 as a test set for the task of modeling the past. To define the feature set and tune hyperparameters, we used the remaining 80% of papers from 1999-2005 as our training data and the remaining papers in 2006 as our development data. After pruning,9 we have 37,251 total features, of which 2,549 are metadata features. When tuning hyperparameters, we simulated the existence of a forecast gap by using extrapolated responses for papers in the last year of the training data instead of their true responses. We considered \u03bb \u2208 5 {2,1,...,-5,-6} , and \u03b1 \u2208 5 {3,2,...,-1,-2} and selected those that led to the best performance on the development set.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 34,
                        "end": 35,
                        "text": "3",
                        "ref_id": "FIGREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Forecasting NBER Downloads",
                "sec_num": "5.2"
            },
            {
                "text": "We then used the selected feature set and hyperparameters to test the forecasting and modeling capabilities of each model. For forecasting, we predicted numbers of downloads of papers in 2008 and 2009. We used the baseline median, ridge regression, and time series regularization models trained on papers in 1999-2007 and 1999-2008, 2008, respectively) as a forecast gap, since we would not have observed complete responses of papers in these years when forecasting. For the \"one year\" models, we trained ridge regressions only on the most recent past year, using papers in 2007 and 2008, respectively, as training data. 10 To test the additive benefit of text features, we trained models with just metadata features (NBER programs and authors, denoted \"Meta\") and with both metadata 10 Papers from the most recent past year in a training set have incomplete responses, so the models were trained on extrapolated responses for that year. For the NBER development set from 2005, a ridge regression on just 2004 papers (for which extrapolation is needed) outperformed a regression on just 2003 (for which extrapolation is not needed), 278 to 367 mean absolute error. For the ACL development set from 2001, a regression on just 2000 (for which extrapolation is needed) led to slightly lower performance (59% versus 61%) than a regression on just 1998 (for which extrapolation is not needed), probably due to the relatively small number of conferences and papers in 2000. For consistency with the other models and with the NBER experiments, we evaluated regressions on the most recent (extrapolated) year in our ACL experiments. and text features (denoted \"Full\").",
                "cite_spans": [
                    {
                        "start": 308,
                        "end": 332,
                        "text": "1999-2007 and 1999-2008,",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Forecasting NBER Downloads",
                "sec_num": "5.2"
            },
            {
                "text": "To evaluate the modeling capabilities, we trained the ridge regression and time series regularization models on papers from 1999-2008 and predicted the numbers of downloads of held-out papers in 1999-2007. For comparison, we also trained ridge regression models on each individual year (\"one year\") and predicted the numbers of downloads of the heldout papers in the corresponding year.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Features",
                "sec_num": null
            },
            {
                "text": "Table 3 shows mean absolute errors for each method on both forecasting test splits, and mean absolute errors averaged across papers over nine modeling test splits. For interpretability, we report predictions in terms of download counts, though the models were trained with log counts ( \u00a72.1). The results show that even a simple n-gram representation of text contains a valuable, learnable signal that is predictive of future downloads. While the time series model did not significantly outperform ridge regression at predicting future downloads, it did result in significantly better performance for modeling papers in the past.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "3",
                        "ref_id": "TABREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Features",
                "sec_num": null
            },
            {
                "text": "We now turn to the problem of predicting citation levels. Recall that here we aim to predict whether an ACL paper will be cited within our dataset within three years. Our experimental setup (Figure 3 ) is similar to the setup for the NBER dataset, except that we use logistic regression to model the discrete cited-or-not response variable. We also make the simplifying assumption that all citations occur at the end of each year. Therefore, the forecast gap is only Table 4 : Classification accuracy (%) for predicting whether ACL papers will be cited within three years. \" * \" indicates statistical significance between time series models using metadata features and the full feature set (binomial sign test, p < 0.01). With the full feature set, differences between the time series and ridge (all years) models are not statistically significant at the 0.01 level, but for the modeling task p is estimated at 0.026, and for the 2006 forecasting task, p is estimated at 0.050.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 198,
                        "end": 199,
                        "text": "3",
                        "ref_id": "FIGREF3"
                    },
                    {
                        "start": 473,
                        "end": 474,
                        "text": "4",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Forecasting ACL Citations",
                "sec_num": "5.3"
            },
            {
                "text": "two years (we have observed complete citations in the test year). After feature pruning, there were 30,760 total features, of which 1,694 are metadata features. We considered \u03bb \u2208 5 {2,1,\u2022\u2022\u2022 ,-8,-9} (\"Full\") and \u03bb \u2208 5 {2,1,\u2022\u2022\u2022 ,-11,-12} (\"Meta\"); and \u03b1 \u2208 5 {6,5,\u2022\u2022\u2022 ,0,-1} (both \"Full\" and \"Meta\"), selecting the best values using the development data.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Forecasting ACL Citations",
                "sec_num": "5.3"
            },
            {
                "text": "Again, we compare four methods: a baseline of always predicting the most frequent class in the training data, \"all years\" and \"one year\" logistic regression models, and a logistic regression with the time series regularizer.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Forecasting ACL Citations",
                "sec_num": "5.3"
            },
            {
                "text": "For the forecasting task, we used papers in 2004, 2005, and 2006 as test sets. As the training sets for the \"all years\" and time series models, we used papers from 1980 up to the last year before each test set, with the last two years extrapolated. As the training sets for the \"one year\" models, we used papers from the year immediately before the test set, with extrapolated responses.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Forecasting ACL Citations",
                "sec_num": "5.3"
            },
            {
                "text": "To evaluate modeling capabilities, we predicted citation levels of held-out papers in 1980-2003. We used the \"all years\" and time series models trained on 1980-2005. We trained \"one year\" models separately for each year and predicted downloads for the held-out papers in that year.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Forecasting ACL Citations",
                "sec_num": "5.3"
            },
            {
                "text": "Table 4 shows classification accuracy for each model on the test data for both the forecasting and modeling tasks. It is again clear that adding text sig-600 nificantly improved the performance of the model. Also, the time series regression model shows a small, though not statistically significant, gain for modeling whether past papers will be cited-as well as similarly small gains on two of the three forecasting test years.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "4",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Forecasting ACL Citations",
                "sec_num": "5.3"
            },
            {
                "text": "We can also use the models for ranking to help decide which papers are expected to have the greatest impact. With rankings, we can use the same metric both for download and citation predictions. For the NBER data, we ranked test-set papers based on the predicted numbers of downloads and computed the correlation to the actual numbers of downloads. For the ACL data, we ranked papers based on the probability of being cited (within the next three years) and computed the correlation to the actual numbers of citations. 11 To measure ranking models' ranking quality, we used Kendall's \u03c4 , a nonparametric statistic that measures the similarity of two different orderings over the same set of items. Here, the items are scientific papers and the two metrics are the gold standard numbers of downloads (or citations) and model predictions for the numbers of downloads, or citation probabilities. If q is the chance that a randomly drawn pair of items will be ranked in the same way by the two metrics, then \u03c4 = 2(q -0.5).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Ranking",
                "sec_num": "5.4"
            },
            {
                "text": "Table 5 shows Kendall's \u03c4 for each model for the forecasting tasks (i.e., prediction of future citations or downloads) in both datasets. As in the previous experiments, we see small benefits for the time series regression model on most held-out data splitsand larger benefits for including text features along with metadata features.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "5",
                        "ref_id": "TABREF6"
                    }
                ],
                "eq_spans": [],
                "section": "Ranking",
                "sec_num": "5.4"
            },
            {
                "text": "An advantage of the time series regularized regression model is its interpretability. Inspecting feature coefficients in the model allows us to identify trends and changes of interests over time within a scientific community.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Analysis",
                "sec_num": "6"
            },
            {
                "text": "11 Here, we use models of responses to individual papers for ranking (i.e., in a pointwise ranking scheme). Time series regularization could also be applied to ranking models that model pairwise preferences to optimize metrics like Kendall's \u03c4 directly, as discussed by Joachims (2002) First, we illustrate the difference between the time series and the other models in Figure 4 , for NBER models' weights for unemployment rate and inflation rate appearing in a paper's abstract. The yearto-year weights of \"one year\" models fluctuate substantially, and the \"all years\" model is necessarily constant, but the time series regularizer gives a smooth trajectory.",
                "cite_spans": [
                    {
                        "start": 270,
                        "end": 285,
                        "text": "Joachims (2002)",
                        "ref_id": "BIBREF15"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 377,
                        "end": 378,
                        "text": "4",
                        "ref_id": "FIGREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Analysis",
                "sec_num": "6"
            },
            {
                "text": "Previous work has examined the flow of ideas as trends in word and phrase frequencies, as in the Google Books Ngram Viewer (Michel et al., 2011) .12 Topic models have been used extensively to explore trends in low-dimensional spaces (Blei and Lafferty, 2006; Wang et al., 2008; Wang and McCallum, 2006; Ahmed and Xing, 2010) . By contrast, our approach allows us to examine trends in the impact of text related to specific observation variables: the coefficient trendline for a feature illustrates its association with measurements of scholarly impact (citation and download frequency).",
                "cite_spans": [
                    {
                        "start": 123,
                        "end": 144,
                        "text": "(Michel et al., 2011)",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 233,
                        "end": 258,
                        "text": "(Blei and Lafferty, 2006;",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 259,
                        "end": 277,
                        "text": "Wang et al., 2008;",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 278,
                        "end": 302,
                        "text": "Wang and McCallum, 2006;",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 303,
                        "end": 324,
                        "text": "Ahmed and Xing, 2010)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Trends",
                "sec_num": "6.1"
            },
            {
                "text": "Text frequencies can be quite different from the discriminative weights our model assigns to features. Figure 5 illustrates the \u03b2 t,j trends in the ACL time series model for some selected terms that oc- cur frequently in conference session titles. On the right are term frequencies (with smoothing, since year-to-year frequencies are bumpy). Most terms decline over time. On the left, by contrast, are the weights learned by our time series model. They tell a very different story: for example, parsing has shown a definite increase in interest, while interest in grammars (e.g., formalisms) has declined somewhat. These trends have face validity, giving credence to our analysis; they also broadly agree with Hall et al. (2008) .",
                "cite_spans": [
                    {
                        "start": 710,
                        "end": 728,
                        "text": "Hall et al. (2008)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 110,
                        "end": 111,
                        "text": "5",
                        "ref_id": "FIGREF5"
                    }
                ],
                "eq_spans": [],
                "section": "Trends",
                "sec_num": "6.1"
            },
            {
                "text": "The regression method also allows analysis of author influence, since we fit a coefficient for each of the authors in the ACL dataset. Figure 6 (a) addresses the following question: do prolific authors get cited more often, even after accounting for the content of their papers? 13 The effect is present but relatively small according to our model: the total number of papers co-authored by an author has a weak correlation to the author's citation prediction coefficient (\u03c4 = 0.16).",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 142,
                        "end": 143,
                        "text": "6",
                        "ref_id": "FIGREF7"
                    }
                ],
                "eq_spans": [],
                "section": "Authors",
                "sec_num": "6.2"
            },
            {
                "text": "Next, does the model provide more information than the simple citation probability of an author? Figure 6 (b) compares coefficients to an author's papers' probability of being cited. Since we did not prune author features, there are many authors with 13 More precisely: if a prolific author and a non-prolific author write a paper, does the prolific author's paper have a higher probability of being cited than the non-prolific author's, all other things being equal? q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0 10 20 30 40 -0.002 0.001 0.004 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Citation proportion \u03b2 (b) Citation prop. vs. coef. only a few papers, resulting in unsmoothed probabilities of 0, 0.5, 1, etc. (these correspond to the vertical \"bands\" in the plot). By contrast, the 2 -penalty of the model naturally assigned coefficients close to zero for such authors if it is justified.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 104,
                        "end": 105,
                        "text": "6",
                        "ref_id": "FIGREF7"
                    }
                ],
                "eq_spans": [],
                "section": "Authors",
                "sec_num": "6.2"
            },
            {
                "text": "In general, the simple probability agrees with the coefficient, but there are differences. The semantics of the regression imply we are measuring the relative citation probability of an author, controlling for text and venue effects. If an author has a high citation prediction coefficient but a low citation probability, that implies the author has better-cited work than would be expected according to the n-grams in his or her papers. We have omitted names of authors from the figure for clarity and confidentiality, but high outlier authors tend to be well-known researchers in the ACL community. Obviously, since the prediction model is not perfect, it is not possible to completely verify this hypothesis, but we feel this analysis is reasonably suggestive.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Authors",
                "sec_num": "6.2"
            },
            {
                "text": "Previous work on modeling scientific literature mostly focused on citation graphs (Borner et al., 2003; Qazvinian and Radev, 2008) . Some researchers, e.g., Erosheva et al. (2004) , have used text content. Most of these are based on topic models: Gerrish and Blei (2010) measure scholarly impact, Hall et al. (2008) study the \"history of ideas\", and Ramage et al. (2010) rank universities based on scholarly output using topic models. Download rates and citation prediction were two of the main tasks in the KDD Cup 2003 (McGovern et al., 2003; Brank and Leskovec, 2003) . Bethard and Jurafsky (2010) considered the problem slightly differently and proposed an information retrieval approach to citation prediction. Our approach is novel in that we formulate the problem as a forecasting task and we seek to predict future impact of articles.",
                "cite_spans": [
                    {
                        "start": 82,
                        "end": 103,
                        "text": "(Borner et al., 2003;",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 104,
                        "end": 130,
                        "text": "Qazvinian and Radev, 2008)",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 157,
                        "end": 179,
                        "text": "Erosheva et al. (2004)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 297,
                        "end": 315,
                        "text": "Hall et al. (2008)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 350,
                        "end": 370,
                        "text": "Ramage et al. (2010)",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 508,
                        "end": 520,
                        "text": "KDD Cup 2003",
                        "ref_id": null
                    },
                    {
                        "start": 521,
                        "end": 544,
                        "text": "(McGovern et al., 2003;",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 545,
                        "end": 570,
                        "text": "Brank and Leskovec, 2003)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 573,
                        "end": 600,
                        "text": "Bethard and Jurafsky (2010)",
                        "ref_id": "BIBREF2"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "7"
            },
            {
                "text": "Linear regression with text features has been used to predict financial risk (Kogan et al., 2009) and movie revenues (Joshi et al., 2010) . While the forecasts in those papers are similar to ours, those authors did not consider a forecast gap or allowing the parameters of the model to vary over time.",
                "cite_spans": [
                    {
                        "start": 77,
                        "end": 97,
                        "text": "(Kogan et al., 2009)",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 117,
                        "end": 137,
                        "text": "(Joshi et al., 2010)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "7"
            },
            {
                "text": "Our time series regularization is closely related to the fused lasso (Tibshirani et al., 2005) . It penalizes a loss function by the 1 -norm of the coefficients and their differences. The 1 -penalty for differences between coefficients encourages sparsity in the differences. We use the 2 -norm to induce smooth changes across time steps.",
                "cite_spans": [
                    {
                        "start": 69,
                        "end": 94,
                        "text": "(Tibshirani et al., 2005)",
                        "ref_id": "BIBREF27"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "7"
            },
            {
                "text": "We presented a statistical approach to predicting a scientific community's response to an article, based on its textual content. To improve the interpretability of the linear model, we developed a novel time series regularizer that encourages gradual changes across time steps. Our experiments showed that text features significantly improve accuracy of predictions over baseline models, and we found that the feature weights learned with the time series regularizer reflect important trends in the literature.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusions",
                "sec_num": "8"
            },
            {
                "text": "EMNLP is a relatively recent conference, and, in this collection, complete data for its papers postdate the end of the last training period, so we chose to exclude it from our dataset.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "The linear regression has a bias \u03b20 that is always active. The logistic regression also has an unpenalized bias \u03b2c,0 for each class c. This weight is not regularized.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Consistent with the previous section, we assume that parameters for different features, \u03b2 j and \u03b2 k , are independent.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "For NBER, text features appearing in less than 0.1% or more than 99.9% of the training documents were removed. For ACL, the thresholds were 2% and 98%.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "http://ngrams.googlelabs.com",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "We thank the National Bureau of Economic Research for providing the NBER dataset for this research, Fallaw Sowell for helpful discussions, and three anonymous reviewers for comments on an earlier draft of this paper. This research was supported by the Intelligence Advanced Research Projects Activity under grant number N10PC20222 and TeraGrid resources provided by the Pittsburgh Supercomputing Center under grant number TG-DBS110003.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgements",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Timeline: A dynamic hierarchical Dirichlet process model for recovering birth/death and evolution of topics in text stream",
                "authors": [
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Ahmed",
                        "suffix": ""
                    },
                    {
                        "first": "E",
                        "middle": [
                            "P"
                        ],
                        "last": "Xing",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proc. of UAI",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "A. Ahmed and E. P. Xing. 2010. Timeline: A dy- namic hierarchical Dirichlet process model for recov- ering birth/death and evolution of topics in text stream. In Proc. of UAI.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "A maximum entropy approach to natural language processing",
                "authors": [
                    {
                        "first": "A",
                        "middle": [
                            "L"
                        ],
                        "last": "Berger",
                        "suffix": ""
                    },
                    {
                        "first": "V",
                        "middle": [
                            "J"
                        ],
                        "last": "Della Pietra",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [
                            "A"
                        ],
                        "last": "Della",
                        "suffix": ""
                    },
                    {
                        "first": "Pietra",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 1996,
                "venue": "Computational Linguistics",
                "volume": "22",
                "issue": "1",
                "pages": "39--71",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "A. L. Berger, V. J. Della Pietra, and S. A. Della Pietra. 1996. A maximum entropy approach to nat- ural language processing. Computational Linguistics, 22(1):39-71.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Who should I cite? Learning literature search models from citation behavior",
                "authors": [
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Bethard",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Jurafsky",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proc. of CIKM",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "S. Bethard and D. Jurafsky. 2010. Who should I cite? Learning literature search models from citation behav- ior. In Proc. of CIKM.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Dynamic topic models",
                "authors": [
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Blei",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Lafferty",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Proc. of ICML",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "D. Blei and J. Lafferty. 2006. Dynamic topic models. In Proc. of ICML.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Visualizing knowledge domains",
                "authors": [
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Borner",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Boyack",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Annual Review of Information Science and Technology",
                "volume": "37",
                "issue": "",
                "pages": "179--255",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "K. Borner, C. Chen, and K. Boyack. 2003. Visualiz- ing knowledge domains. In B. Cronin, editor, Annual Review of Information Science and Technology, vol- ume 37, pages 179-255. Information Today, Inc.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Time Series Analysis: Forecasting and Control",
                "authors": [
                    {
                        "first": "G",
                        "middle": [],
                        "last": "Box",
                        "suffix": ""
                    },
                    {
                        "first": "G",
                        "middle": [
                            "M"
                        ],
                        "last": "Jenkins",
                        "suffix": ""
                    },
                    {
                        "first": "G",
                        "middle": [],
                        "last": "Reinsel",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "G. Box, G. M. Jenkins, and G. Reinsel. 2008. Time Se- ries Analysis: Forecasting and Control. Wiley Series in Probability and Statistics.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Convex Optimization",
                "authors": [
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Boyd",
                        "suffix": ""
                    },
                    {
                        "first": "L",
                        "middle": [],
                        "last": "Vandenberghe",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "S. Boyd and L. Vandenberghe. 2004. Convex Optimiza- tion. Cambridge University Press.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "The download estimation task on KDD Cup",
                "authors": [
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Brank",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Leskovec",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "SIGKDD Explorations",
                "volume": "5",
                "issue": "2",
                "pages": "160--162",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "J. Brank and J. Leskovec. 2003. The download estima- tion task on KDD Cup 2003. SIGKDD Explorations, 5(2):160-162.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Regression Analysis of Count Data",
                "authors": [
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Cameron",
                        "suffix": ""
                    },
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Trivedi",
                        "suffix": ""
                    }
                ],
                "year": 1998,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "A. Cameron and P. Trivedi. 1998. Regression Analysis of Count Data. Cambridge University Press.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Mixed membership models of scientific publications",
                "authors": [
                    {
                        "first": "E",
                        "middle": [],
                        "last": "Erosheva",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Fienberg",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Lafferty",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proc. of PNAS",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "E. Erosheva, S. Fienberg, and J. Lafferty. 2004. Mixed membership models of scientific publications. In Proc. of PNAS.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "A language-based approach to measuring scholarly impact",
                "authors": [
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Gerrish",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [
                            "M"
                        ],
                        "last": "Blei",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proc. of ICML",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "S. Gerrish and D. M. Blei. 2010. A language-based approach to measuring scholarly impact. In Proc. of ICML.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Studying the history of ideas using topic models",
                "authors": [
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Hall",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Jurafsky",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Proc. of EMNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "D. Hall, D. Jurafsky, and C. D. Manning. 2008. Studying the history of ideas using topic models. In Proc. of EMNLP.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Time Series Analysis",
                "authors": [
                    {
                        "first": "J",
                        "middle": [
                            "D"
                        ],
                        "last": "Hamilton",
                        "suffix": ""
                    }
                ],
                "year": 1994,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "J. D. Hamilton. 1994. Time Series Analysis. Princeton University Press.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction",
                "authors": [
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Hastie",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Tibshirani",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Friedman",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "T. Hastie, R. Tibshirani, and J. Friedman. 2009. The Ele- ments of Statistical Learning: Data Mining, Inference, and Prediction. Springer.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Ridge regression: Biased estimation for nonorthogonal problems",
                "authors": [
                    {
                        "first": "A",
                        "middle": [
                            "E"
                        ],
                        "last": "Hoerl",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [
                            "W"
                        ],
                        "last": "Kennard",
                        "suffix": ""
                    }
                ],
                "year": 1970,
                "venue": "",
                "volume": "12",
                "issue": "",
                "pages": "55--67",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "A. E. Hoerl and R. W. Kennard. 1970. Ridge regression: Biased estimation for nonorthogonal problems. Tech- nometrics, 12(1):55-67.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Optimizing search engines using clickthrough data",
                "authors": [
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Joachims",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Proc. of KDD",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "T. Joachims. 2002. Optimizing search engines using clickthrough data. In Proc. of KDD.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Movie reviews and revenues: An experiment in text regression",
                "authors": [
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Joshi",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Das",
                        "suffix": ""
                    },
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Gimpel",
                        "suffix": ""
                    },
                    {
                        "first": "N",
                        "middle": [
                            "A"
                        ],
                        "last": "Smith",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proc. of HLT-NAACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "M. Joshi, D. Das, K. Gimpel, and N. A. Smith. 2010. Movie reviews and revenues: An experiment in text regression. In Proc. of HLT-NAACL.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Predicting risk from financial reports with regression",
                "authors": [
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Kogan",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Levin",
                        "suffix": ""
                    },
                    {
                        "first": "B",
                        "middle": [
                            "R"
                        ],
                        "last": "Routledge",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [
                            "S"
                        ],
                        "last": "Sagi",
                        "suffix": ""
                    },
                    {
                        "first": "N",
                        "middle": [
                            "A"
                        ],
                        "last": "Smith",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proc. of HLT-NAACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "S. Kogan, D. Levin, B. R. Routledge, J. S. Sagi, and N. A. Smith. 2009. Predicting risk from financial reports with regression. In Proc. of HLT-NAACL.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "On the limited memory BFGS method for large scale optimization",
                "authors": [
                    {
                        "first": "D",
                        "middle": [
                            "C"
                        ],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 1989,
                "venue": "Mathematical Programming B",
                "volume": "45",
                "issue": "3",
                "pages": "503--528",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "D. C. Liu and J. Nocedal. 1989. On the limited memory BFGS method for large scale optimization. Mathemat- ical Programming B, 45(3):503-528.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Regression models for ordinal data",
                "authors": [
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Mccullagh",
                        "suffix": ""
                    }
                ],
                "year": 1980,
                "venue": "Journal of the Royal Statistical Society B",
                "volume": "42",
                "issue": "2",
                "pages": "109--142",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "P. McCullagh. 1980. Regression models for ordinal data. Journal of the Royal Statistical Society B, 42(2):109- 142.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Exploiting relational structure to understand publication patterns in high-energy physics",
                "authors": [
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Mcgovern",
                        "suffix": ""
                    },
                    {
                        "first": "L",
                        "middle": [],
                        "last": "Friedland",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Hay",
                        "suffix": ""
                    },
                    {
                        "first": "B",
                        "middle": [],
                        "last": "Gallagher",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Fast",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Neville",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Jensen",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "SIGKDD Explorations",
                "volume": "5",
                "issue": "",
                "pages": "165--172",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "A. McGovern, L. Friedland, M. Hay, B. Gallagher, A. Fast, J. Neville, and D. Jensen. 2003. Exploit- ing relational structure to understand publication pat- terns in high-energy physics. SIGKDD Explorations, 5(2):165-172.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Quantitative analysis of culture using millions of digitized books",
                "authors": [
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Michel",
                        "suffix": ""
                    },
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Shen",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Aiden",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Veres",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Gray",
                        "suffix": ""
                    },
                    {
                        "first": ";",
                        "middle": [
                            "J"
                        ],
                        "last": "Pickett",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Hoiberg",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Clancy",
                        "suffix": ""
                    },
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Norvig",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Orwant",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Pinker",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Nowak",
                        "suffix": ""
                    },
                    {
                        "first": "E",
                        "middle": [],
                        "last": "Aiden",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Science",
                "volume": "331",
                "issue": "6014",
                "pages": "176--182",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "J. Michel, Y. Shen, A. Aiden, A. Veres, M. Gray, The Google Books Team, J. Pickett, D. Hoiberg, D. Clancy, P. Norvig, J. Orwant, S. Pinker, M. Nowak, and E. Aiden. 2011. Quantitative analysis of culture using millions of digitized books. Science, 331(6014):176- 182.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Scientific paper summarization using citation summary networks",
                "authors": [
                    {
                        "first": "V",
                        "middle": [],
                        "last": "Qazvinian",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [
                            "R"
                        ],
                        "last": "Radev",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Proc. of COLING",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "V. Qazvinian and D. R. Radev. 2008. Scientific paper summarization using citation summary networks. In Proc. of COLING.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "A bibliometric and network analysis of the field of computational linguistics",
                "authors": [
                    {
                        "first": "D",
                        "middle": [
                            "R"
                        ],
                        "last": "Radev",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [
                            "T"
                        ],
                        "last": "Joseph",
                        "suffix": ""
                    },
                    {
                        "first": "B",
                        "middle": [],
                        "last": "Gibson",
                        "suffix": ""
                    },
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Muthukrishnan",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Journal of the American Society for Information Science and Technology",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "D. R. Radev, M. T. Joseph, B. Gibson, and P. Muthukrish- nan. 2009a. A bibliometric and network analysis of the field of computational linguistics. Journal of the American Society for Information Science and Tech- nology.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "The ACL anthology network corpus",
                "authors": [
                    {
                        "first": "D",
                        "middle": [
                            "R"
                        ],
                        "last": "Radev",
                        "suffix": ""
                    },
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Muthukrishnan",
                        "suffix": ""
                    },
                    {
                        "first": "V",
                        "middle": [],
                        "last": "Qazvinian",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proc. of ACL Workshop on Natural Language Processing and Information Retrieval for Digital Libraries",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "D. R. Radev, P. Muthukrishnan, and V. Qazvinian. 2009b. The ACL anthology network corpus. In Proc. of ACL Workshop on Natural Language Processing and Infor- mation Retrieval for Digital Libraries.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Which universities lead and lag? Toward university rankings based on scholarly output",
                "authors": [
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Ramage",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [
                            "A"
                        ],
                        "last": "Mcfarland",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proc. of NIPS Workshop on Computational Social Science and the Wisdom of the Crowds",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "D. Ramage, C. D. Manning, and D. A. McFarland. 2010. Which universities lead and lag? Toward university rankings based on scholarly output. In Proc. of NIPS Workshop on Computational Social Science and the Wisdom of the Crowds.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Sparsity and smoothness via the fused lasso",
                "authors": [
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Tibshirani",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Saunders",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Rosset",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    },
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Knight",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Journal of the Royal Statistical Society B",
                "volume": "67",
                "issue": "1",
                "pages": "91--108",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "R. Tibshirani, M. Saunders, S. Rosset, J. Zhu, and K. Knight. 2005. Sparsity and smoothness via the fused lasso. Journal of the Royal Statistical Society B, 67(1):91-108.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Topics over time: A non-Markov continuous-time model of topical trends",
                "authors": [
                    {
                        "first": "X",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Mccallum",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Proc. of KDD",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "X. Wang and A. McCallum. 2006. Topics over time: A non-Markov continuous-time model of topical trends. In Proc. of KDD.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Continuous time dynamic topic models",
                "authors": [
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Blei",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Heckerman",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Proc. of UAI",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "C. Wang, D. Blei, and D. Heckerman. 2008. Continuous time dynamic topic models. In Proc. of UAI.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF1": {
                "num": null,
                "text": "Figure 1: Left: the distribution of log download counts for papers in the NBER dataset one year after posting. Right: the distribution of within-dataset citations of ACL papers within three years of publication (outliers excluded for readability).",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "Figure 2: Time series regression as a graphical model; the variables X t and Y t are the sets of feature vectors and response variables from documents dated t.",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "FIGREF3": {
                "num": null,
                "text": "Figure 3: An illustration of how the datasets were segmented for the experiments. Portions of data for which we report results are shaded. Time spans are not to scale.",
                "uris": null,
                "fig_num": "3",
                "type_str": "figure"
            },
            "FIGREF4": {
                "num": null,
                "text": "Figure 4: Coefficients for two NBER bigram features.",
                "uris": null,
                "fig_num": "4",
                "type_str": "figure"
            },
            "FIGREF5": {
                "num": null,
                "text": "Figure5: Feature trends: model coefficients vs. term frequencies over time in the ACL corpus. Term freq. is the fraction of tokens (or bigrams for m.t.) that year, that are the term, averaged over a centered five-year window.",
                "uris": null,
                "fig_num": "5",
                "type_str": "figure"
            },
            "FIGREF7": {
                "num": null,
                "text": "Figure 6: Analysis of author citation coefficients. Every point is one ACL author, and the vertical axis shows the citation coefficient, compared to (a) the number of documents co-authored by the author; and (b) the proportion of an author's papers that are cited within three years. The vertical bar is the macro-averaged citation proportion across authors, 41%.",
                "uris": null,
                "fig_num": "6",
                "type_str": "figure"
            },
            "TABREF1": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Descriptive statistics about the datasets.",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table><tr><td>Response GLM type Metric 1 Metric 2</td><td>NBER log(#downloads+1) normal / squared-loss mean absolute error Kendall's \u03c4</td><td>ACL 1{#citations &gt; 0} logistic / log-loss accuracy Kendall's \u03c4</td></tr><tr><td colspan=\"2\">4 Features</td><td/></tr><tr><td colspan=\"2\">NBER metadata features</td><td/></tr><tr><td>\u2022</td><td/><td/></tr></table>",
                "type_str": "table",
                "text": "Summary of the setup for the NBER download and ACL citation prediction experiments. Authors' last names. We treat each name as a binary feature. If a paper has multiple authors, all authors are used and they have equal weights regardless of their ordering. \u2022 NBER program(s).",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table><tr><td/><td/><td/><td/><td colspan=\"2\">NBER Experiments</td></tr><tr><td/><td>'99</td><td>...</td><td/><td>'04</td><td>'05</td><td>'06</td></tr><tr><td>80% 20%</td><td colspan=\"5\">training modeling test (unused) gap dev.</td><td>(tuning, feature pruning)</td></tr><tr><td/><td>'99</td><td>...</td><td/><td/><td>'06</td><td>'07</td><td>'08</td></tr><tr><td>80%</td><td colspan=\"3\">training</td><td/><td>gap test</td></tr><tr><td>20%</td><td colspan=\"5\">modeling test (unused)</td></tr><tr><td/><td>'99</td><td>...</td><td/><td/><td>'07</td><td>'08</td><td>'09</td></tr><tr><td>80%</td><td colspan=\"3\">training</td><td/><td>gap test</td></tr><tr><td>20%</td><td colspan=\"4\">modeling test</td></tr><tr><td/><td/><td/><td/><td colspan=\"2\">ACL Experiments</td></tr><tr><td/><td>'80</td><td>...</td><td>'98</td><td>'99 '00</td><td>'01</td></tr><tr><td>80%</td><td colspan=\"3\">training</td><td>gap</td><td>dev.</td><td>(tuning, feature pruning)</td></tr><tr><td>20%</td><td colspan=\"5\">modeling test (unused)</td></tr><tr><td/><td>'80</td><td>...</td><td/><td/><td>'01 '02 '03</td><td>'04</td></tr><tr><td>80%</td><td colspan=\"3\">training</td><td/><td>gap</td><td>test</td></tr><tr><td>20%</td><td colspan=\"5\">modeling test (unused)</td></tr><tr><td>80%</td><td colspan=\"3\">training '80 ...</td><td/><td>gap '02 '03 '04</td><td>test '05</td></tr><tr><td>20%</td><td colspan=\"5\">modeling test (unused)</td></tr><tr><td>80%</td><td colspan=\"3\">training '80 ...</td><td/><td>'03</td><td>gap '05 '04</td><td>test '06</td></tr><tr><td>20%</td><td colspan=\"4\">modeling test</td></tr></table>",
                "type_str": "table",
                "text": "respectively. We treated the last year of the training data(2007 and",
                "html": null,
                "num": null
            },
            "TABREF4": {
                "content": "<table><tr><td/><td>Model</td><td>Modeling 1999-07</td><td colspan=\"2\">Forecasting 2008 2009</td></tr><tr><td>-</td><td>median</td><td>333</td><td>371</td><td>397</td></tr><tr><td>Meta</td><td>one year</td><td>279</td><td>354</td><td>375</td></tr><tr><td>Meta</td><td>all years</td><td>303</td><td>334</td><td>378</td></tr><tr><td>Meta</td><td>time series</td><td>279</td><td>353</td><td>375</td></tr><tr><td>Full</td><td>one year</td><td>271</td><td>346</td><td>351</td></tr><tr><td>Full Full</td><td>all years time series</td><td>265  *  \u2020 245</td><td>\u2020 300</td><td>339</td></tr></table>",
                "type_str": "table",
                "text": "Mean absolute errors for the NBER download predictions. \" * \" indicates statistical significance between time series models using metadata features and the full feature set. \" \u2020\" indicates statistical significance between the time series and ridge regression models using the full feature set (Wilcoxon signed-rank test, p < 0.01).",
                "html": null,
                "num": null
            },
            "TABREF6": {
                "content": "<table><tr><td>Feat.</td><td>Model</td><td>NBER '08 '09 '04 '05 '06 ACL</td></tr><tr><td>Meta</td><td>one year</td><td>.29 .22 .17 .08 .16</td></tr><tr><td>Meta</td><td>all years</td><td>.31 .22 .15 .12 .21</td></tr><tr><td colspan=\"3\">Meta time series .29 .22 .14 .10 .17</td></tr><tr><td>Full Full Full</td><td colspan=\"2\">one year all years time series .43 .38 .47 .44 .43 .35 .31 .44 .39 .33 .43 .37 .42 .43 .40</td></tr><tr><td colspan=\"2\">unemployment_rate</td><td>inflation_rate</td></tr><tr><td>0.004</td><td/><td>time series</td></tr><tr><td/><td/><td>all years</td></tr><tr><td>-0.004</td><td/><td>one year</td></tr><tr><td colspan=\"2\">2000 2005 2010</td><td>2000 2005 2010</td></tr></table>",
                "type_str": "table",
                "text": ". Kendall's \u03c4 rank correlation for future prediction models on both datasets.",
                "html": null,
                "num": null
            }
        }
    }
}