{
    "paper_id": "2021",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:12:47.080171Z"
    },
    "title": "ENPAR:Enhancing Entity and Entity Pair Representations for Joint Entity Relation Extraction",
    "authors": [
        {
            "first": "Yijun",
            "middle": [],
            "last": "Wang",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Shanghai Jiao Tong University",
                "location": {}
            },
            "email": "yijunwang.cs@gmail.com"
        },
        {
            "first": "Changzhi",
            "middle": [],
            "last": "Sun",
            "suffix": "",
            "affiliation": {},
            "email": "sunchangzhi@bytedance.com"
        },
        {
            "first": "Yuanbin",
            "middle": [],
            "last": "Wu",
            "suffix": "",
            "affiliation": {
                "laboratory": "Lab",
                "institution": "East China Normal University",
                "location": {
                    "addrLine": "4 ByteDance",
                    "region": "AI"
                }
            },
            "email": "ybwu@cs.ecnu.edu.cn"
        },
        {
            "first": "Hao",
            "middle": [],
            "last": "Zhou",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Lei",
            "middle": [],
            "last": "Li",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Junchi",
            "middle": [],
            "last": "Yan",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Shanghai Jiao Tong University",
                "location": {}
            },
            "email": "yanjunchi@sjtu.edu.cn"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Current state-of-the-art systems for joint entity relation extraction (Luan et al., 2019; Wadden et al., 2019) usually adopt the multi-task learning framework. However, annotations for these additional tasks such as coreference resolution and event extraction are always equally hard (or even harder) to obtain. In this work, we propose a pre-training method ENPAR to improve the joint extraction performance. EN-PAR requires only the additional entity annotations that are much easier to collect. Unlike most existing works that only consider incorporating entity information into the sentence encoder, we further utilize the entity pair information. Specifically, we devise four novel objectives, i.e., masked entity typing, masked entity prediction, adversarial context discrimination, and permutation prediction, to pretrain an entity encoder and an entity pair encoder. Comprehensive experiments show that the proposed pre-training method achieves significant improvement over BERT on ACE05, SciERC, and NYT, and outperforms current state-of-the-art on ACE05.",
    "pdf_parse": {
        "paper_id": "2021",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Current state-of-the-art systems for joint entity relation extraction (Luan et al., 2019; Wadden et al., 2019) usually adopt the multi-task learning framework. However, annotations for these additional tasks such as coreference resolution and event extraction are always equally hard (or even harder) to obtain. In this work, we propose a pre-training method ENPAR to improve the joint extraction performance. EN-PAR requires only the additional entity annotations that are much easier to collect. Unlike most existing works that only consider incorporating entity information into the sentence encoder, we further utilize the entity pair information. Specifically, we devise four novel objectives, i.e., masked entity typing, masked entity prediction, adversarial context discrimination, and permutation prediction, to pretrain an entity encoder and an entity pair encoder. Comprehensive experiments show that the proposed pre-training method achieves significant improvement over BERT on ACE05, SciERC, and NYT, and outperforms current state-of-the-art on ACE05.",
                "cite_spans": [
                    {
                        "start": 70,
                        "end": 89,
                        "text": "(Luan et al., 2019;",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 90,
                        "end": 110,
                        "text": "Wadden et al., 2019)",
                        "ref_id": "BIBREF20"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Joint extraction of entities and relations is a fundamental task in information extraction, it aims to extract entities and relations with a unified model. Current approaches (Luan et al., 2019; Wadden et al., 2019) usually adopt the multi-task learning framework that optimizes many objectives simultaneously, including entity recognition, relation extraction, coreference resolution, and event extraction. However, as large-scale manually labeled data required by these methods is unavailable in many domains, their applicability is severely restricted. Therefore, we expect to catch or even surpass the multi-task based joint models with less annotation cost. Compared with the annotations of coreference resolution and event extraction, entity annotations can be easily obtained through automatic NER annotation tools (e.g., spaCy1 ). In this paper, we focus on improving the model's performance with just extra entity annotations.",
                "cite_spans": [
                    {
                        "start": 175,
                        "end": 194,
                        "text": "(Luan et al., 2019;",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 195,
                        "end": 215,
                        "text": "Wadden et al., 2019)",
                        "ref_id": "BIBREF20"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Although pre-trained models, like BERT, have shown impressive performance in many downstream tasks, they have mainly two limitations when applied in the joint entity relation extraction task. One is that currently pre-trained objectives are insufficient for this task. Specifically, these commonly used universal pre-trained model (e.g., BERT) do not consider the entity-related knowledge that is crucial for better extracting entities and relations. The other is that these models only provide pre-trained representations for tokens and sentences, but not entities and entity pairs. To obtain the representations for entities and entity pairs, additional parameters that are not pre-trained are introduced in the fine-tuning stage, which may futher impair the joint extraction performance.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "To address the first limitation, recent several works try to incorporate entity-related information into pre-training objectives. Zhang et al. (2019) fuses heterogeneous information from both texts and knowledge graphs and proposes a denoising entity auto-encoder objective based on BERT. Sun et al. (2019c) presents two knowledge masking strategies in the pre-training stage (entity-level masking and phrase-level masking). Both of them utilize extra entity annotations (i.e., entities in knowledge graphs and automatic entity annotations, respectively). In this paper, we follow this line of works and build a large-scale entity annotated corpus using the spaCy NER tool.",
                "cite_spans": [
                    {
                        "start": 130,
                        "end": 149,
                        "text": "Zhang et al. (2019)",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 289,
                        "end": 307,
                        "text": "Sun et al. (2019c)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "For the second limitation, we propose ENPAR, a pre-training method customized for entity relation extraction. ENPAR consists of an underlying sentence encoder, an entity encoder, and an entity pair encoder. Compared with BERT (Figure 1 (a)), the proposed entity encoder and entity pair encoder directly provide representations of entities and entity pairs. To train the three encoders, we devise four novel pre-training tasks: masked entity typing, masked entity prediction, adversarial context discrimination and permutation prediction (Figure 1(b) ). In the first two tasks, we randomly mask some entity words and then predict the masked tokens and the entity type. These two tasks are natural extensions of the masked language model. To learn a better entity pair encoder, we draw inspirations from the denoising auto-encoder (Zhang et al., 2019) and propose the last two tasks. Specifically, when the entity pair or its context in a sentence are perturbed, we hope that the entity pair encoder is capable of tracking such changes. We employ the parameter-sharing method for these four tasks and train these objectives jointly.",
                "cite_spans": [
                    {
                        "start": 829,
                        "end": 849,
                        "text": "(Zhang et al., 2019)",
                        "ref_id": "BIBREF24"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 234,
                        "end": 235,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    },
                    {
                        "start": 545,
                        "end": 549,
                        "text": "1(b)",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "To sum up, our main contributions are as follows2 :",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\u2022 We introduce an entity encoder and an entity pair encoder to incorporate not only the entity information but also the entity pair information, which were ignored in current universal pre-trained models.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\u2022 We propose four novel pre-training tasks that help to learn the proposed encoders. These tasks only require additional entity annotations (with commonly used entity types), which can be automatically generated by public annotation tools, such as spaCy NER.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\u2022 We conduct comprehensive experiments and demonstrate that the proposed method achieves significant improvement on ACE05 and NYT dataset and is comparable with the state-of-the-art on the SciERC dataset.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Given an input sentence s = x 1 , . . . , x |s| and a set of entities E (automatically annotated) in s, ENPAR is to encode each entity e \u2208 E and each entity pair (e 1 , e 2 ) into a contextual representation vector. As shown in Figure 2 , ENPAR is composed of a shared Transformer (Vaswani et al., 2017) , an entity-level CNN followed by an MLP (multi-layer perceptron), a context-level CNN followed by an MLP, and the last MLP. In the pre-training stage, we optimize ENPAR with four objectives, namely, masked entity typing, masked entity prediction, adversarial context discrimination and permutation prediction. These pre-training objectives can integrate rich entityrelated information into the proposed network. After pre-training, we can easily fine-tune the pretrained network for entity relation extraction task.",
                "cite_spans": [
                    {
                        "start": 281,
                        "end": 303,
                        "text": "(Vaswani et al., 2017)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 235,
                        "end": 236,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Approach",
                "sec_num": "2"
            },
            {
                "text": "In this section, we will introduce the overall EN-PAR architecture in three parts: the sentence encoder, the entity encoder, and the entity pair encoder.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Pre-training Network Architecture",
                "sec_num": "2.1"
            },
            {
                "text": "Sentence Encoder As previous pre-training models (UNILM, BERT, and XLM), we also apply the multi-layer Transformer (Vaswani et al., 2017) as the basic sentence encoder for obtaining the contextual representations h i for each token in the sentence s. The output of multi-layer Transformer is computed via:",
                "cite_spans": [
                    {
                        "start": 115,
                        "end": 137,
                        "text": "(Vaswani et al., 2017)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Pre-training Network Architecture",
                "sec_num": "2.1"
            },
            {
                "text": "{h 1 , . . . , h |s| } = Transformer({x 1 , . . . , x |s| })",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Pre-training Network Architecture",
                "sec_num": "2.1"
            },
            {
                "text": "The word representation x i of x i follow that of BERT (Devlin et al., 2018) , which is a sum of the corresponding token, segment and position embeddings.",
                "cite_spans": [
                    {
                        "start": 55,
                        "end": 76,
                        "text": "(Devlin et al., 2018)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Pre-training Network Architecture",
                "sec_num": "2.1"
            },
            {
                "text": "Entity Encoder For each entity e \u2208 E in the sentence s, the corresponding contextual entity representation h e can be obtained by employing a CNN (a single convolution layer with a maxpooling layer) followed by an MLP on vectors {h i |x i \u2208 e}, as shown in Figure 2(a) . Entity Pair Encoder For each entity pair (e 1 , e 2 ) in the sentence s, to obtain the corresponding contextual entity pair representation h e 1 ,e 2 , we extract two types of features. The first is the features regarding words in e 1 , e 2 , and the second is the features regarding contexts of the entity pair (e 1 , e 2 ). For features on words in e 1 , e 2 , we use the output of entity encoder, namely, h e 1 and h e 2 . For context features of the entity pair (e 1 , e 2 ), we extract three feature vectors by looking at left context words, middle context words and right context words of the entity pair (e 1 , e 2 ). Similar to entity encoder, we compute three feature vectors by employing another CNN followed by an MLP. Finally, we concatenate the five feature vectors into a single vector. To get the resulting entity pair representation h e 1 ,e 2 , the single vector was fed into another MLP, as shown in Figure 2 (b).",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 264,
                        "end": 268,
                        "text": "2(a)",
                        "ref_id": "FIGREF1"
                    },
                    {
                        "start": 1196,
                        "end": 1197,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Pre-training Network Architecture",
                "sec_num": "2.1"
            },
            {
                "text": "We design four pre-training objectives to guide ENPAR to absorb more entity-related knowledge, which is particularly important for entity relation extraction task. The four objectives can be divided into two groups: the first two objectives are to enhance the representations of single entities, and the latter two objectives are to enhance the representations of entity pairs. These objectives are trained jointly (simply sum the objective functions). Our pre-training objectives are based on a dataset with entity annotation, which can be obtained through the public annotation tool. For instance, PER(\"Obama\"), ORG(\"Labour Party\" ) were annotated by spaCy NER, and PER, ORG are entity types (there are 18 entity types).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Pre-training Objectives",
                "sec_num": "2.2"
            },
            {
                "text": "In this task, we simply mask some entity words at random, and then predict the corresponding entity type3 . For instance, given a masked word sequence \"x 1 , [M], [M], x 4 \", to predict the masked entity type (e.g., PER), we first use the entity encoder to extract the contextual masked entity representation, and then predict the entity type. The objective is to minimize the cross-entropy loss computed using the predicted entity type and the original entity type, as shown in Figure 3(a) .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 486,
                        "end": 490,
                        "text": "3(a)",
                        "ref_id": "FIGREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Masked Entity Typing (MET)",
                "sec_num": null
            },
            {
                "text": "This task is similar to the masked LM in BERT and is identical to the entity-level masking in (Sun et al., 2019c) . Specifically, we randomly choose some entity words in the sentence, and replace them with special word [M]. Then we feed their corre-",
                "cite_spans": [
                    {
                        "start": 94,
                        "end": 113,
                        "text": "(Sun et al., 2019c)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Masked Entity Prediction (MEP)",
                "sec_num": null
            },
            {
                "text": "x 2 x3 x 1 e 1 e 2",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Masked Entity Prediction (MEP)",
                "sec_num": null
            },
            {
                "text": "x 4 x5 x6 x7",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Masked Entity Prediction (MEP)",
                "sec_num": null
            },
            {
                "text": "Entity Pair Encoder",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Masked Entity Prediction (MEP)",
                "sec_num": null
            },
            {
                "text": "x 1 e 1 e 2",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Masked Entity Prediction (MEP)",
                "sec_num": null
            },
            {
                "text": "x 4 x5 x6 x7",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Masked Entity Prediction (MEP)",
                "sec_num": null
            },
            {
                "text": "Entity Pair Encoder",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Masked Entity Prediction (MEP)",
                "sec_num": null
            },
            {
                "text": "x 2 x3 \u21e5 1 MLP(h + e1,e2 ) + MLP(h e1,e2 ) \u21e4 + E + E (a) ACD x 2 x 3 x 1 x 4 x 5 x 6 x 7",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Masked Entity Prediction (MEP)",
                "sec_num": null
            },
            {
                "text": "Entity Pair Encoder sponding output vectors computed by the sentence encoder into a softmax classifier to predict the masked entity word. The model is learned to recover the masked entity words, as shown in Figure 3 (b). In short, both mask entity typing and mask entity prediction encourage the model to learn the information of single entities.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 214,
                        "end": 215,
                        "text": "3",
                        "ref_id": "FIGREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Masked Entity Prediction (MEP)",
                "sec_num": null
            },
            {
                "text": "Given an input sentence s with an entity pair (e 1 , e 2 ), we regard it as a positive sample E + = (s, e 1 , e 2 ). According to E + , we can generate a negative sample E -= (s , e 1 , e 2 ) that has a main property: the context words and the order of context words w.r.t. the entity pair (e 1 , e 2 ) in E -are minimally different from those in the original sample E + . If the entity pair encoder can characterize the context words well, it should be able to recognize small context differences of the entity pair between E + and E -. We refer to this objective as adversarial context discrimination. The hinge loss function was imposed into the positive sample and corresponding negative samples to achieve the goal. Specifically, as shown in Figure 4 (a), we can obtain a score using the entity pair encoder with an MLP for each sample, then the hinge loss function is computed via:",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 754,
                        "end": 755,
                        "text": "4",
                        "ref_id": "FIGREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Adversarial Context Discrimination (ACD)",
                "sec_num": null
            },
            {
                "text": "1 -MLP(h + e 1 ,e 2 ) + MLP(h - e 1 ,e 2 ) + .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Adversarial Context Discrimination (ACD)",
                "sec_num": null
            },
            {
                "text": "where [u] + = max(u, 0) is the hinge loss and MLP outputs a scalar value. h + e 1 ,e 2 and h - e 1 ,e 2 are the output of entity pair encoder for positive sample E + and negative sample E -.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Adversarial Context Discrimination (ACD)",
                "sec_num": null
            },
            {
                "text": "Here we introduce our strategies of generating a negative sample E -= (s , e 1 , e 2 ) according to a positive sample E + = (s, e 1 , e 2 ). In fact, there are many negative samples. For the sake of simplicity and clarity, we only adopt following simple rules to generate five negative samples.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Adversarial Context Discrimination (ACD)",
                "sec_num": null
            },
            {
                "text": "\u2022 Swap entity e 1 and entity e 2 in the original sample E + ;",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Adversarial Context Discrimination (ACD)",
                "sec_num": null
            },
            {
                "text": "\u2022 Shift the entity e 1 few positions (n s ) to the left or right.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Adversarial Context Discrimination (ACD)",
                "sec_num": null
            },
            {
                "text": "\u2022 Shift the entity e 2 few positions (n s ) to the left or right.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Adversarial Context Discrimination (ACD)",
                "sec_num": null
            },
            {
                "text": "Permutation Prediction (PP) Given an input sentence s with an entity pair (e 1 , e 2 ), the sentence s was split into five parts, namely, left context, e 1 , middle context, e 2 and right context. If we shuffle the five parts, does the entity pair encoder have the ability to recognize it? Inspired by this question, we propose an enhanced objective, named permutation prediction, to help to learn a better entity pair encoder. Formally, let P be the set of all possible permutation of the fives parts, as shown in Figure 5 . Obviously, the number of all possible permutations is 5! (|P| = 120). For each permutation p \u2208 P, we first assign it a unique permutation class N p (1 \u2264 N p \u2264 120), and then use the entity pair encoder to extract the contextual entity pair representation for predicting the permutation class, as shown in Figure 4(b) . The objective is to optimize the cross-entropy loss computed using the predicted permutation class and the gold permutation class. It is costly to consider all permutations. So we sample n p permutations in practice (we always include the correct permutation).",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 522,
                        "end": 523,
                        "text": "5",
                        "ref_id": null
                    },
                    {
                        "start": 838,
                        "end": 842,
                        "text": "4(b)",
                        "ref_id": "FIGREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Adversarial Context Discrimination (ACD)",
                "sec_num": null
            },
            {
                "text": "In the pre-training stage, we directly optimize the sum of the above four objective functions. Same as UNILM (Dong et al., 2019) , we use gelu as activation function. And the sentence encoder is initialized with BERT BASE weights. We use the English Wikipedia4 as pre-training corpus, which has been processed in similarly as (Devlin et al., 2018) . The spaCy NER5 was used to annotate entities. After preprocessing the corpus, there are nearly 820M words and 95M entities in the annotated input. We discard the sentences having less than 3 entities for effectiveness, and only pre-train our model for one epoch. The vocabulary size is 28996, The maximum length of the input sequence is 256. For each entity, we replace the entity words with [M] with probability 15%, randomly replace other entity with probability 5% and keep the original entity words for the rest. Adam (Kingma and Ba, 2014) with \u03b2 1 = 0.9, \u03b2 2 = 0.999 is used for optimization. The batch size is 512. The learning rate is 5e-5, with linear warmup rate over the first 10% steps and linear decay. The dropout rate is 0.1. The weight decay is 0.01. It takes about 20 hours for 10, 000 steps using 1 Nvidia Telsa V100 16GB GPU.",
                "cite_spans": [
                    {
                        "start": 109,
                        "end": 128,
                        "text": "(Dong et al., 2019)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 326,
                        "end": 347,
                        "text": "(Devlin et al., 2018)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Pre-training Setup",
                "sec_num": "2.3"
            },
            {
                "text": "In the fine-tuning stage, we adopt the same definition of entity relation extraction task as (Sun et al., 2019a) . The joint entity relation extraction task can be decomposed into three objectives: entity span detection, entity typing, and relation typing. Firstly, we treat the entity span detection as a sequence labeling task. We regard the first sub-word's output of the sentence encoder as the token-level representation. Then we take the token-level representation as the input to the softmax classifier and compute the cross-entropy loss with respect to gold entity span labels. Secondly, for each detected entity span, the entity classifier uses the corresponding output of the entity encoder to predict entity type. For each detected entity span pair, the relation classifier uses the corresponding output of the entity pair encoder to predict relation type. Both the entity classifier and the relation classifier are randomly initialized softmax layer. Also, we adopt the cross-entropy loss for these two tasks. Besides, all three objectives are optimized simultaneously.",
                "cite_spans": [
                    {
                        "start": 93,
                        "end": 112,
                        "text": "(Sun et al., 2019a)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Fine-tuning for Entity Relation Extraction",
                "sec_num": "2.4"
            },
            {
                "text": "We only tune the hyperparameters on the ACE2005 development set based on the joint performance of entity and relation, then apply the same hyperparameters on the SciERC and NYT datasets. Scheduled sampling strategy (Miwa and Bansal, 2016) and discriminative fine-tuning strategy (Howard and Ruder, 2018) are emploied in fine-tuning. We kepp the same dropout rate as pre-training (i.e., 0.1). The learning rate is 2.5e-5 with weight decay 0.01. We apply a linear warmup scheduler over the first 20% steps and then linear decay. We train our model with a maximum of 200 epochs with early stop strategy in a single Nvidia GeForce GTX 1080 Ti GPU.",
                "cite_spans": [
                    {
                        "start": 279,
                        "end": 303,
                        "text": "(Howard and Ruder, 2018)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Fine-tuning for Entity Relation Extraction",
                "sec_num": "2.4"
            },
            {
                "text": "We conduct experiments on three benchmark entity relation extraction datasets: ACE05, SciERC, and NYT. For space limitation, we will mainly discuss the results on ACE05 and report basic results on the remaining two datasets.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "3"
            },
            {
                "text": "ACE05 The ACE05 dataset6 that is a standard corpus for entity relation extraction task annotates entity and relation labels for a collection of documents. ACE05 contains 7 entity types and 6 relation types. We use the same data split and preprocessing of ACE05 dataset (351 training, 80 validating and 80 testing) as (Miwa and Bansal, 2016) and (Sun et al., 2018) .",
                "cite_spans": [
                    {
                        "start": 345,
                        "end": 363,
                        "text": "(Sun et al., 2018)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "3"
            },
            {
                "text": "SciERC The SciERC dataset7 annotates entity, coreference and relation labels for 500 scientific abstracts from 12 AI conference/workshop proceedings. We only use the annotations of entities and relations. SciERC contains 6 scientific term (entity) types and 7 relation types. We use the same data split and preprocessing of SciERC dataset (350 training, 50 validating and 100 testing) as (Luan et al., 2019) .",
                "cite_spans": [
                    {
                        "start": 388,
                        "end": 407,
                        "text": "(Luan et al., 2019)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "3"
            },
            {
                "text": "NYT The NYT dataset8 is a large-scale corpus which automatically annotates a collection of New York Times news articles. NYT contains 3 types of entities and 12 types of relations. The training set is automatically annotated by distant supervision. While the validation and testing data are manually labeled by (Jia et al., 2019) . We choose the latest version of NYT released by (Jia et al., 2019) .",
                "cite_spans": [
                    {
                        "start": 311,
                        "end": 329,
                        "text": "(Jia et al., 2019)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 380,
                        "end": 398,
                        "text": "(Jia et al., 2019)",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "3"
            },
            {
                "text": "Evaluation. As previous works (Miwa and Bansal, 2016; Sun et al., 2019a) performances using F1 score. Specifically, an output entity is correct if its type label and head region match with a gold entity, then an output relation is correct if both its type and its two argument entities are all correct (i.e. exactly match). While some previous works (Luan et al., 2019; Wadden et al., 2019; Sanh et al., 2019) do not consider entity type in relation evaluation. Thus, we also report this result for comparison.",
                "cite_spans": [
                    {
                        "start": 30,
                        "end": 53,
                        "text": "(Miwa and Bansal, 2016;",
                        "ref_id": null
                    },
                    {
                        "start": 54,
                        "end": 72,
                        "text": "Sun et al., 2019a)",
                        "ref_id": null
                    },
                    {
                        "start": 350,
                        "end": 369,
                        "text": "(Luan et al., 2019;",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 370,
                        "end": 390,
                        "text": "Wadden et al., 2019;",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 391,
                        "end": 409,
                        "text": "Sanh et al., 2019)",
                        "ref_id": "BIBREF13"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "3"
            },
            {
                "text": "First, we compare the proposed pre-training method with previous works in Table 1 . In general, the relation performance of ENPAR significantly exceeds all existing models in two relation evaluation criteria. Specifically, in exactly matching mode, our method achieves 4.4 points improvement compared with the LSTM-based GCN joint model (Sun et al., 2019a) and increases by 3.3 points compared with the BERT-based QA model (Li et al., 2019) . Even compared to the multi-task learning models based on BERT (Wadden et al., 2019) , our method still achieves 2.7 points improvement on relation performance. Although the entity performance of our method inferior to the multi-task learning models (Luan et al., 2019; Wadden et al., 2019) , we believe that those additional supervision signals such as coreference and event information in the fine-tuning step may cause the gap. Besides, they even consider all spans and cross-sentence context, which are empirically beneficial to entity performance. However, even with the slightly inferior entity encoder and lack of additional multi-task training data, our pre-trained entity pair encoder still achieves significantly superior relation performance, which fully demonstrates the powerfulness of the proposed pre-training method.",
                "cite_spans": [
                    {
                        "start": 337,
                        "end": 356,
                        "text": "(Sun et al., 2019a)",
                        "ref_id": null
                    },
                    {
                        "start": 423,
                        "end": 440,
                        "text": "(Li et al., 2019)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 505,
                        "end": 526,
                        "text": "(Wadden et al., 2019)",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 692,
                        "end": 711,
                        "text": "(Luan et al., 2019;",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 712,
                        "end": 732,
                        "text": "Wadden et al., 2019)",
                        "ref_id": "BIBREF20"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 80,
                        "end": 81,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Results on ACE05",
                "sec_num": "3.1"
            },
            {
                "text": "Next, we evaluate the proposed pre-training method with different settings. We have following four detailed observations regarding the results in Table 2 .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 152,
                        "end": 153,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Results on ACE05",
                "sec_num": "3.1"
            },
            {
                "text": "\u2022 ENPAR (line 2) achieves superior relation performance (1.0 point and 1.3 points improvement) and comparable entity performance compared with BERT (line 1). This result demonstrates that the proposed pre-training objectives inject more entityrelated information into the pre-trained model and enhance the relation extraction performance.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results on ACE05",
                "sec_num": "3.1"
            },
            {
                "text": "\u2022 Overall, the entity performances of all models fluctuate quite slightly (0.5 points). Interestingly, \"-MEP\" (line 4) achieves the best entity performance though its relation performance is not the best. The stable entity performance reflects that the token-level information is likely enough for entity recognition. Besides, the final training objectives may bias entity objectives or entity pair objectives, which leads to more improvement in relation performance than entity performance. Thus, we pay more attention relation perfromance in this paper. \u2022 When any pre-training objective is removed, the relation performance will decrease with varying degrees. Particularly, \"-PP\" (line 6) drops the most (both 1.9 points decline in two relation evaluation criteria). This phenomenon just indicates the importance of the \"PP\" objective for the entity pair encoder and relation extraction.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results on ACE05",
                "sec_num": "3.1"
            },
            {
                "text": "\u2022 We also try only to use the pre-trained weights of the Transformer (sentence encoder), while the other parameters are randomly initialized in the fine-tuning step (line 7). The relation performance of the \"-CNN\" slightly declines compared with ENPAR, but it still outperforms BERT. This result reflects that the sentence encoder has absorbed the entity-related knowledge and can achieve more encouraging performance with the entity encoder and the entity pair encoder.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results on ACE05",
                "sec_num": "3.1"
            },
            {
                "text": "Thirdly, we present the influences of pretraining data size (Table 3 ). In experiments, we found the loss of the pre-trained model tends to be stable after iterating 5k steps (about 25% of all data), as shown in Figure 6 . Table 3 demonstrates that the performance of the pre-trained model was quite competitive using 25% of pre-training data 9 , and more pre-training data does not further improve performance. There is a similar conclusion on text classification (Sun et al., 2019b) , which performs within-task further pre-training. This observation shows that the proposed pre-training method does not require expensive training costs.",
                "cite_spans": [
                    {
                        "start": 465,
                        "end": 484,
                        "text": "(Sun et al., 2019b)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 67,
                        "end": 68,
                        "text": "3",
                        "ref_id": "TABREF1"
                    },
                    {
                        "start": 219,
                        "end": 220,
                        "text": "6",
                        "ref_id": "FIGREF5"
                    },
                    {
                        "start": 229,
                        "end": 230,
                        "text": "3",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Results on ACE05",
                "sec_num": "3.1"
            },
            {
                "text": "Finally, we examine the relation performance with respect to different distances between entity pairs (Figure 7(a) ) and different the number of re-9 In all experiments, we choose the number of pre-training data, np and ns according to the performance on the development set during the fine-tuning step. 4 ) 10 . Comparing with BERT, ENPAR is good at handling long-distance relation dependencies and interactions between multiple relations in a sentence. For S1, BERT does not detect the long distance relation PHYS between \"[barbara starr]\" and \"[pentagon]\" while ENPAR can handle it. For S2, ENPAR identifies a relation PHYS between \"[charles]\" and \"[london]\", but BERT fails even the relation PHYS between \"[vladimir putin]\" and \"[london]\" was detected. It shows BERT does not fully exploit the multiple relations in a sentence. We attribute these results to the powerful representations learned by the proposed pre-training objectives.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 110,
                        "end": 114,
                        "text": "7(a)",
                        "ref_id": null
                    },
                    {
                        "start": 304,
                        "end": 305,
                        "text": "4",
                        "ref_id": "TABREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Results on ACE05",
                "sec_num": "3.1"
            },
            {
                "text": "The upper part of Table 5 shows the results of SciERC. Compared with BERT, ENPAR achieves 0.5 points improvement on entity performance and 2.2 points (exactly match) improvement on relation performance. This result reflects the effectiveness of the proposed pre-training objectives for entity relation extraction. Compared with the previous state-of-the-art model (Wadden et al., 2019) , which is a multi-task learning model based on BERT, ENPAR achieves superior entity performance and comparable relation performance without additional multi-task training data. It worth noting that, the entities in the SciERC dataset are different from our pre-training data. Therefore, the entity encoder and the entity pair encoder can encode contexts of entities rather than only encoding information respect to specific entities. This property is desired as it means that we could utilize any entity annotator. NYT For entity relation extraction task, there are no previous works on this dataset. We list the BERT and ENPAR results in the same way as the previous two datasets. For the bottom part of the Table 5 , we observe that ENPAR significantly outperforms BERT on entity performance and relation performance. This again verifies the effectiveness of our proposed pre-training method.",
                "cite_spans": [
                    {
                        "start": 364,
                        "end": 385,
                        "text": "(Wadden et al., 2019)",
                        "ref_id": "BIBREF20"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 24,
                        "end": 25,
                        "text": "5",
                        "ref_id": "TABREF3"
                    },
                    {
                        "start": 1102,
                        "end": 1103,
                        "text": "5",
                        "ref_id": "TABREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Results on SciERC and NYT SciERC",
                "sec_num": "3.2"
            },
            {
                "text": "Joint entity relation extraction is an important task that has been extensively studied. One simple method to achieve joint learning is through parameters sharing, which usually share some input embeddings or sentence encoders (Miwa and Bansal, 2016; Katiyar and Cardie, 2017) . To further explore the interactions between the outputs of the entity model and the relation model, many joint decoding algorithms were introduced into this joint task (Yang and Cardie, 2013; Li and Ji, 2014; Katiyar and Cardie, 2016; Zheng et al., 2017; Ren et al., 2017; Wang et al., 2018; Sun et al., 2018; Fu et al., 2019) . Besides, (Li et al., 2019) tackle this task under the framework of multi-turn QA. And (Sun et al., 2019a) conduct joint type inference via GCN on a bipartite graph composed of entities and relations. Recently, transfer learning (Sun and Wu, 2019) , multi-task learning (Sanh et al., 2019; Wadden et al., 2019; Luan et al., 2019) were also applied in this task. In this work, we investigate the pre-trained model for entity relation extraction.",
                "cite_spans": [
                    {
                        "start": 227,
                        "end": 250,
                        "text": "(Miwa and Bansal, 2016;",
                        "ref_id": null
                    },
                    {
                        "start": 251,
                        "end": 276,
                        "text": "Katiyar and Cardie, 2017)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 447,
                        "end": 470,
                        "text": "(Yang and Cardie, 2013;",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 471,
                        "end": 487,
                        "text": "Li and Ji, 2014;",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 488,
                        "end": 513,
                        "text": "Katiyar and Cardie, 2016;",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 514,
                        "end": 533,
                        "text": "Zheng et al., 2017;",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 534,
                        "end": 551,
                        "text": "Ren et al., 2017;",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 552,
                        "end": 570,
                        "text": "Wang et al., 2018;",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 571,
                        "end": 588,
                        "text": "Sun et al., 2018;",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 589,
                        "end": 605,
                        "text": "Fu et al., 2019)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 617,
                        "end": 634,
                        "text": "(Li et al., 2019)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 694,
                        "end": 713,
                        "text": "(Sun et al., 2019a)",
                        "ref_id": null
                    },
                    {
                        "start": 836,
                        "end": 854,
                        "text": "(Sun and Wu, 2019)",
                        "ref_id": null
                    },
                    {
                        "start": 877,
                        "end": 896,
                        "text": "(Sanh et al., 2019;",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 897,
                        "end": 917,
                        "text": "Wadden et al., 2019;",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 918,
                        "end": 936,
                        "text": "Luan et al., 2019)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "4"
            },
            {
                "text": "For simplicity, we restrict the joint model of parameters sharing, which can be easily extended to jont decoding methods. Pre-trained models (Yang et al., 2019; Dong et al., 2019; Joshi et al., 2020) have made many amazing breakthroughs on various NLP downstream tasks. Pre-training paradigm first pretrain networks with some pre-training objectives on large-scale unlabeled text corpora, and then fine-tune the pre-trained networks on downstream tasks. These pre-training objectives determine the knowledge absorbed by the pre-trained models. For example, BERT (Devlin et al., 2018) adopts masked language model and next sentence prediction to learn deep contextual representations and the relation between two sentences respectively. XLNet (Yang et al., 2019) uses permutation language model to learn bidirectional representations with autoregressive model. UNILM (Dong et al., 2019) fuses three types of language model: unidirectional, bidirectional and seq2seq prediction. However, these models and objectives all ignore the entity-related information, which is crucial for entity relation extraction task. Recently, there are several public works that explore how to properly integrate entity inforamtion into pre-trained models. Specifically, (Zhang et al., 2019) achieve enhanced language representation by injecting informative entities in KGs into pre-training models. And (Sun et al., 2019c) propose two higherlevel masking strategies: entity-level masking and phrase-level masking. In this work, we not only integrate entity information, but also extend to entity pair information and learn more powerful representations for entities and entity pairs.",
                "cite_spans": [
                    {
                        "start": 141,
                        "end": 160,
                        "text": "(Yang et al., 2019;",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 161,
                        "end": 179,
                        "text": "Dong et al., 2019;",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 180,
                        "end": 199,
                        "text": "Joshi et al., 2020)",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 562,
                        "end": 583,
                        "text": "(Devlin et al., 2018)",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 742,
                        "end": 761,
                        "text": "(Yang et al., 2019)",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 866,
                        "end": 885,
                        "text": "(Dong et al., 2019)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 1249,
                        "end": 1269,
                        "text": "(Zhang et al., 2019)",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 1382,
                        "end": 1401,
                        "text": "(Sun et al., 2019c)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "4"
            },
            {
                "text": "We propose ENPAR, a pre-training method customized for entity relation extraction only with additional entity annotations. Instead of only pretraining sentence encoder in universal pre-trained models, we also pre-train an entity encoder and an entity pair encoder. Then the proposed four objec-tives can incorporate entity and entity-pair knowledge into the pre-trained encoders to enhance the encoders' representations. Experiments on three datasets demonstrate that ENPAR achieves comparable or even superior performances compared with multi-task based joint models.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "5"
            },
            {
                "text": "Table 6 and Table 7 show the performances of \"BERT\" and our \"ENPAR\" on each entity type and relation type, respectively. For entity performance, \"ENPAR\" is almost the same as \"BERT\". However, for relation performance, we can find \"ENPAR\" is superior to \"BERT\" on all relation types except \"PER-SOC\" relation. The major reason is \"ENPAR\" significantly improves recall while keeping or sacrificing little precision.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "6",
                        "ref_id": "TABREF4"
                    },
                    {
                        "start": 18,
                        "end": 19,
                        "text": "7",
                        "ref_id": "TABREF5"
                    }
                ],
                "eq_spans": [],
                "section": "Appendices A More Evaluations",
                "sec_num": null
            },
            {
                "text": "Figure 8 and Figure 9 demonstrate the relation performances (exactly match) of \"BERT\" and \"ENPAR\" in different situations. Specifically, Figure 8 shows the results with respect to the number of entities for each sentence, while Figure 9 shows the results with respect to the sentence length for each sentence. In Figure 8 , our \"ENPAR\" significantly outperforms \"BERT\" when the number of entities in a sentence is more than 2. Similarly, when the sentence length is more than 10, our \"ENPAR\" is also superior to \"BERT\" as shown in Figure 9 . Both results show the powerful ability of \"ENPAR\" to identify relations in complicate sentences and long sentences. Moreover, it also proves that the proposed pre-training objectives indeed prompt the model to learn entity related information, which contributes to improving the relation extraction performance. ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "8",
                        "ref_id": null
                    },
                    {
                        "start": 20,
                        "end": 21,
                        "text": "9",
                        "ref_id": null
                    },
                    {
                        "start": 144,
                        "end": 145,
                        "text": "8",
                        "ref_id": null
                    },
                    {
                        "start": 235,
                        "end": 236,
                        "text": "9",
                        "ref_id": null
                    },
                    {
                        "start": 320,
                        "end": 321,
                        "text": "8",
                        "ref_id": null
                    },
                    {
                        "start": 538,
                        "end": 539,
                        "text": "9",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Appendices A More Evaluations",
                "sec_num": null
            },
            {
                "text": "https://spacy.io/",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Source code and pre-trained models are available at https://github.com/Receiling/ENPAR.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "It is worth noting that the entity types annotated by spaCy NER may be different from the entity types of downstream datasets.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Wikipedia version: enwiki-20190301.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "The spaCy model is \"en core web md\" in version: 2.1.8, which trained on OntoNotes dataset.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://github.com/tticoin/LSTM-ER",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "http://nlp.cs.washington.edu/sciIE/",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://github.com/PaddlePaddle/models/tree/develop/ PaddleNLP/Research/ACL2019-ARNOR/",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "The authors wish to thank the reviewers for their helpful comments and suggestions. This research is (partially) supported by NSFC (62076097, 61972250, U19B2035), National Key Research and Development Program of China (2018AAA0100704), STCSM (18ZR1411500), the Foundation of State Key Laboratory of Cognitive Intelligence, iFLYTEK(COGOS-20190003). The corresponding authors are Junchi Yan, Changzhi Sun, and Yuanbin Wu.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgement",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
                "authors": [
                    {
                        "first": "Jacob",
                        "middle": [],
                        "last": "Devlin",
                        "suffix": ""
                    },
                    {
                        "first": "Ming-Wei",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "Kenton",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Kristina",
                        "middle": [],
                        "last": "Toutanova",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1810.04805"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understand- ing. arXiv preprint arXiv:1810.04805.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Unified language model pre-training for natural language understanding and generation",
                "authors": [
                    {
                        "first": "Li",
                        "middle": [],
                        "last": "Dong",
                        "suffix": ""
                    },
                    {
                        "first": "Nan",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Wenhui",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Furu",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaodong",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Yu",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Jianfeng",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Ming",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Hsiao-Wuen",
                        "middle": [],
                        "last": "Hon",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1905.03197"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. 2019. Unified language model pre-training for natural language understanding and generation. arXiv preprint arXiv:1905.03197.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Graphrel: Modeling text as relational graphs for joint entity and relation extraction",
                "authors": [
                    {
                        "first": "Tsu-Jui",
                        "middle": [],
                        "last": "Fu",
                        "suffix": ""
                    },
                    {
                        "first": "Peng-Hsuan",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Wei-Yun",
                        "middle": [],
                        "last": "Ma",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proc. of ACL",
                "volume": "",
                "issue": "",
                "pages": "1409--1418",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tsu-Jui Fu, Peng-Hsuan Li, and Wei-Yun Ma. 2019. Graphrel: Modeling text as relational graphs for joint entity and relation extraction. In Proc. of ACL, pages 1409-1418.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Universal language model fine-tuning for text classification",
                "authors": [
                    {
                        "first": "Jeremy",
                        "middle": [],
                        "last": "Howard",
                        "suffix": ""
                    },
                    {
                        "first": "Sebastian",
                        "middle": [],
                        "last": "Ruder",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1801.06146"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jeremy Howard and Sebastian Ruder. 2018. Universal language model fine-tuning for text classification. arXiv preprint arXiv:1801.06146.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Arnor: Attention regularization based noise reduction for distant supervision relation classification",
                "authors": [
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Jia",
                        "suffix": ""
                    },
                    {
                        "first": "Dai",
                        "middle": [],
                        "last": "Dai",
                        "suffix": ""
                    },
                    {
                        "first": "Xinyan",
                        "middle": [],
                        "last": "Xiao",
                        "suffix": ""
                    },
                    {
                        "first": "Hua",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proc. of ACL",
                "volume": "",
                "issue": "",
                "pages": "1399--1408",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Wei Jia, Dai Dai, Xinyan Xiao, and Hua Wu. 2019. Arnor: Attention regularization based noise reduc- tion for distant supervision relation classification. In Proc. of ACL, pages 1399-1408.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Spanbert: Improving pre-training by representing and predicting spans",
                "authors": [
                    {
                        "first": "Mandar",
                        "middle": [],
                        "last": "Joshi",
                        "suffix": ""
                    },
                    {
                        "first": "Danqi",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Yinhan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Luke",
                        "middle": [],
                        "last": "Daniel S Weld",
                        "suffix": ""
                    },
                    {
                        "first": "Omer",
                        "middle": [],
                        "last": "Zettlemoyer",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Levy",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Transactions of the Association for Computational Linguistics",
                "volume": "8",
                "issue": "",
                "pages": "64--77",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy. 2020. Spanbert: Improving pre-training by representing and predict- ing spans. Transactions of the Association for Com- putational Linguistics, 8:64-77.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Investigating lstms for joint extraction of opinion entities and relations",
                "authors": [
                    {
                        "first": "Arzoo",
                        "middle": [],
                        "last": "Katiyar",
                        "suffix": ""
                    },
                    {
                        "first": "Claire",
                        "middle": [],
                        "last": "Cardie",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proc. of ACL",
                "volume": "",
                "issue": "",
                "pages": "919--929",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Arzoo Katiyar and Claire Cardie. 2016. Investigating lstms for joint extraction of opinion entities and re- lations. In Proc. of ACL, pages 919-929.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Going out on a limb: Joint extraction of entity mentions and relations without dependency trees",
                "authors": [
                    {
                        "first": "Arzoo",
                        "middle": [],
                        "last": "Katiyar",
                        "suffix": ""
                    },
                    {
                        "first": "Claire",
                        "middle": [],
                        "last": "Cardie",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proc. of ACL",
                "volume": "",
                "issue": "",
                "pages": "917--928",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Arzoo Katiyar and Claire Cardie. 2017. Going out on a limb: Joint extraction of entity mentions and re- lations without dependency trees. In Proc. of ACL, pages 917-928.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Adam: A method for stochastic optimization",
                "authors": [
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Diederik",
                        "suffix": ""
                    },
                    {
                        "first": "Jimmy",
                        "middle": [],
                        "last": "Kingma",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Ba",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1412.6980"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Incremental joint extraction of entity mentions and relations",
                "authors": [
                    {
                        "first": "Qi",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Heng",
                        "middle": [],
                        "last": "Ji",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proc. of ACL",
                "volume": "",
                "issue": "",
                "pages": "402--412",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Qi Li and Heng Ji. 2014. Incremental joint extraction of entity mentions and relations. In Proc. of ACL, pages 402-412.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Entity-relation extraction as multi-turn question answering",
                "authors": [
                    {
                        "first": "Xiaoya",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Fan",
                        "middle": [],
                        "last": "Yin",
                        "suffix": ""
                    },
                    {
                        "first": "Zijun",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Xiayu",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Arianna",
                        "middle": [],
                        "last": "Yuan",
                        "suffix": ""
                    },
                    {
                        "first": "Duo",
                        "middle": [],
                        "last": "Chai",
                        "suffix": ""
                    },
                    {
                        "first": "Mingxin",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Jiwei",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1905.05529"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Xiaoya Li, Fan Yin, Zijun Sun, Xiayu Li, Arianna Yuan, Duo Chai, Mingxin Zhou, and Jiwei Li. 2019. Entity-relation extraction as multi-turn question an- swering. arXiv preprint arXiv:1905.05529.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "A general framework for information extraction using dynamic span graphs",
                "authors": [
                    {
                        "first": "Yi",
                        "middle": [],
                        "last": "Luan",
                        "suffix": ""
                    },
                    {
                        "first": "Dave",
                        "middle": [],
                        "last": "Wadden",
                        "suffix": ""
                    },
                    {
                        "first": "Luheng",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Amy",
                        "middle": [],
                        "last": "Shah",
                        "suffix": ""
                    },
                    {
                        "first": "Mari",
                        "middle": [],
                        "last": "Ostendorf",
                        "suffix": ""
                    },
                    {
                        "first": "Hannaneh",
                        "middle": [],
                        "last": "Hajishirzi",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Makoto Miwa and Mohit Bansal. 2016. End-to-end relation extraction using lstms on sequences and tree structures",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1904.03296"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yi Luan, Dave Wadden, Luheng He, Amy Shah, Mari Ostendorf, and Hannaneh Hajishirzi. 2019. A general framework for information extraction using dynamic span graphs. arXiv preprint arXiv:1904.03296. Makoto Miwa and Mohit Bansal. 2016. End-to-end re- lation extraction using lstms on sequences and tree structures. arXiv preprint arXiv:1601.00770.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Cotype: Joint extraction of typed entities and relations with knowledge bases",
                "authors": [
                    {
                        "first": "Xiang",
                        "middle": [],
                        "last": "Ren",
                        "suffix": ""
                    },
                    {
                        "first": "Zeqiu",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Wenqi",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Meng",
                        "middle": [],
                        "last": "Qu",
                        "suffix": ""
                    },
                    {
                        "first": "Clare",
                        "middle": [
                            "R"
                        ],
                        "last": "Voss",
                        "suffix": ""
                    },
                    {
                        "first": "Heng",
                        "middle": [],
                        "last": "Ji",
                        "suffix": ""
                    },
                    {
                        "first": "Jiawei",
                        "middle": [],
                        "last": "Tarek F Abdelzaher",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Han",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proc. of WWW",
                "volume": "",
                "issue": "",
                "pages": "1015--1024",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xiang Ren, Zeqiu Wu, Wenqi He, Meng Qu, Clare R Voss, Heng Ji, Tarek F Abdelzaher, and Jiawei Han. 2017. Cotype: Joint extraction of typed entities and relations with knowledge bases. In Proc. of WWW, pages 1015-1024.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "A hierarchical multi-task approach for learning embeddings from semantic tasks",
                "authors": [
                    {
                        "first": "Victor",
                        "middle": [],
                        "last": "Sanh",
                        "suffix": ""
                    },
                    {
                        "first": "Thomas",
                        "middle": [],
                        "last": "Wolf",
                        "suffix": ""
                    },
                    {
                        "first": "Sebastian",
                        "middle": [],
                        "last": "Ruder",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proc. of AAAI",
                "volume": "",
                "issue": "",
                "pages": "6949--6956",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Victor Sanh, Thomas Wolf, and Sebastian Ruder. 2019. A hierarchical multi-task approach for learning em- beddings from semantic tasks. In Proc. of AAAI, pages 6949-6956.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Joint type inference on entities and relations via graph convolutional networks",
                "authors": [
                    {
                        "first": "Changzhi",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Yeyun",
                        "middle": [],
                        "last": "Gong",
                        "suffix": ""
                    },
                    {
                        "first": "Yuanbin",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Ming",
                        "middle": [],
                        "last": "Gong",
                        "suffix": ""
                    },
                    {
                        "first": "Daxin",
                        "middle": [],
                        "last": "Jiang",
                        "suffix": ""
                    },
                    {
                        "first": "Man",
                        "middle": [],
                        "last": "Lan",
                        "suffix": ""
                    },
                    {
                        "first": "Shiliang",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Nan",
                        "middle": [],
                        "last": "Duan",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proc. of ACL",
                "volume": "",
                "issue": "",
                "pages": "1361--1370",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Changzhi Sun, Yeyun Gong, Yuanbin Wu, Ming Gong, Daxin Jiang, Man Lan, Shiliang Sun, and Nan Duan. 2019a. Joint type inference on entities and relations via graph convolutional networks. In Proc. of ACL, pages 1361-1370.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Distantly supervised entity relation extraction with adapted manual annotations",
                "authors": [
                    {
                        "first": "Changzhi",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Yuanbin",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proc. of AAAI",
                "volume": "33",
                "issue": "",
                "pages": "7039--7046",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Changzhi Sun and Yuanbin Wu. 2019. Distantly super- vised entity relation extraction with adapted manual annotations. In Proc. of AAAI, volume 33, pages 7039-7046.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Extracting entities and relations with joint minimum risk training",
                "authors": [
                    {
                        "first": "Changzhi",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Yuanbin",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Man",
                        "middle": [],
                        "last": "Lan",
                        "suffix": ""
                    },
                    {
                        "first": "Shiliang",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Wenting",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Kuang-Chih",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Kewen",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proc. of EMNLP",
                "volume": "",
                "issue": "",
                "pages": "2256--2265",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Changzhi Sun, Yuanbin Wu, Man Lan, Shiliang Sun, Wenting Wang, Kuang-Chih Lee, and Kewen Wu. 2018. Extracting entities and relations with joint minimum risk training. In Proc. of EMNLP, pages 2256-2265.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "How to fine-tune bert for text classification? arXiv preprint",
                "authors": [
                    {
                        "first": "Chi",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Xipeng",
                        "middle": [],
                        "last": "Qiu",
                        "suffix": ""
                    },
                    {
                        "first": "Yige",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Xuanjing",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1905.05583"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Chi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang. 2019b. How to fine-tune bert for text classification? arXiv preprint arXiv:1905.05583.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Enhanced representation through knowledge integration",
                "authors": [
                    {
                        "first": "Yu",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Shuohuan",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Yukun",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Shikun",
                        "middle": [],
                        "last": "Feng",
                        "suffix": ""
                    },
                    {
                        "first": "Xuyi",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Han",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Xin",
                        "middle": [],
                        "last": "Tian",
                        "suffix": ""
                    },
                    {
                        "first": "Danxiang",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    },
                    {
                        "first": "Hua",
                        "middle": [],
                        "last": "Hao Tian",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1904.09223"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, and Hua Wu. 2019c. Ernie: Enhanced rep- resentation through knowledge integration. arXiv preprint arXiv:1904.09223.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Attention is all you need",
                "authors": [
                    {
                        "first": "Ashish",
                        "middle": [],
                        "last": "Vaswani",
                        "suffix": ""
                    },
                    {
                        "first": "Noam",
                        "middle": [],
                        "last": "Shazeer",
                        "suffix": ""
                    },
                    {
                        "first": "Niki",
                        "middle": [],
                        "last": "Parmar",
                        "suffix": ""
                    },
                    {
                        "first": "Jakob",
                        "middle": [],
                        "last": "Uszkoreit",
                        "suffix": ""
                    },
                    {
                        "first": "Llion",
                        "middle": [],
                        "last": "Jones",
                        "suffix": ""
                    },
                    {
                        "first": "Aidan",
                        "middle": [
                            "N"
                        ],
                        "last": "Gomez",
                        "suffix": ""
                    },
                    {
                        "first": "\u0141ukasz",
                        "middle": [],
                        "last": "Kaiser",
                        "suffix": ""
                    },
                    {
                        "first": "Illia",
                        "middle": [],
                        "last": "Polosukhin",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Advances in neural information processing systems",
                "volume": "",
                "issue": "",
                "pages": "5998--6008",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information pro- cessing systems, pages 5998-6008.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Entity, relation, and event extraction with contextualized span representations",
                "authors": [
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Wadden",
                        "suffix": ""
                    },
                    {
                        "first": "Ulme",
                        "middle": [],
                        "last": "Wennberg",
                        "suffix": ""
                    },
                    {
                        "first": "Yi",
                        "middle": [],
                        "last": "Luan",
                        "suffix": ""
                    },
                    {
                        "first": "Hannaneh",
                        "middle": [],
                        "last": "Hajishirzi",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1909.03546"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "David Wadden, Ulme Wennberg, Yi Luan, and Han- naneh Hajishirzi. 2019. Entity, relation, and event extraction with contextualized span representations. arXiv preprint arXiv:1909.03546.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Joint extraction of entities and relations based on a novel graph scheme",
                "authors": [
                    {
                        "first": "Shaolei",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Yue",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Wanxiang",
                        "middle": [],
                        "last": "Che",
                        "suffix": ""
                    },
                    {
                        "first": "Ting",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "IJCAI",
                "volume": "",
                "issue": "",
                "pages": "4461--4467",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Shaolei Wang, Yue Zhang, Wanxiang Che, and Ting Liu. 2018. Joint extraction of entities and relations based on a novel graph scheme. In IJCAI, pages 4461-4467.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Joint inference for fine-grained opinion extraction",
                "authors": [
                    {
                        "first": "Bishan",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Claire",
                        "middle": [],
                        "last": "Cardie",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proc. of ACL",
                "volume": "",
                "issue": "",
                "pages": "1640--1649",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Bishan Yang and Claire Cardie. 2013. Joint inference for fine-grained opinion extraction. In Proc. of ACL, pages 1640-1649.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
                "authors": [
                    {
                        "first": "Zhilin",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Zihang",
                        "middle": [],
                        "last": "Dai",
                        "suffix": ""
                    },
                    {
                        "first": "Yiming",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Jaime",
                        "middle": [],
                        "last": "Carbonell",
                        "suffix": ""
                    },
                    {
                        "first": "Russ",
                        "middle": [
                            "R"
                        ],
                        "last": "Salakhutdinov",
                        "suffix": ""
                    },
                    {
                        "first": "Quoc V",
                        "middle": [],
                        "last": "Le",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Advances in neural information processing systems",
                "volume": "",
                "issue": "",
                "pages": "5753--5763",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car- bonell, Russ R Salakhutdinov, and Quoc V Le. 2019. Xlnet: Generalized autoregressive pretraining for language understanding. In Advances in neural in- formation processing systems, pages 5753-5763.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Ernie: Enhanced language representation with informative entities",
                "authors": [
                    {
                        "first": "Zhengyan",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Xu",
                        "middle": [],
                        "last": "Han",
                        "suffix": ""
                    },
                    {
                        "first": "Zhiyuan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Xin",
                        "middle": [],
                        "last": "Jiang",
                        "suffix": ""
                    },
                    {
                        "first": "Maosong",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Qun",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1905.07129"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. 2019. Ernie: En- hanced language representation with informative en- tities. arXiv preprint arXiv:1905.07129.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Joint extraction of entities and relations based on a novel tagging scheme",
                "authors": [
                    {
                        "first": "Suncong",
                        "middle": [],
                        "last": "Zheng",
                        "suffix": ""
                    },
                    {
                        "first": "Feng",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Hongyun",
                        "middle": [],
                        "last": "Bao",
                        "suffix": ""
                    },
                    {
                        "first": "Yuexing",
                        "middle": [],
                        "last": "Hao",
                        "suffix": ""
                    },
                    {
                        "first": "Peng",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Bo",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1706.05075"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Suncong Zheng, Feng Wang, Hongyun Bao, Yuexing Hao, Peng Zhou, and Bo Xu. 2017. Joint extrac- tion of entities and relations based on a novel tagging scheme. arXiv preprint arXiv:1706.05075.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 1: The network architectures and objectives of BERT and ENPAR. Our work introduce an entity encoder and an entity pair encoder. MLM = masked language model, NSP = next sentence prediction, MET = masked entity typing, MEP = masked entity prediction, ACD = adversarial context discrimination, PP = permutation prediction.",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Figure 2: Entity encoder and entity pair encoder based on a shared sentence encoder. Both share the entity-level CNN with MLP, and entity pair encoder contains a context-level CNN with MLP.",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "Figure 3: Two objectives to learn enhanced representation of single entities.",
                "uris": null,
                "fig_num": "3",
                "type_str": "figure"
            },
            "FIGREF3": {
                "num": null,
                "text": "Figure 4: Two objectives to learn enhanced representation of entitie pairs.",
                "uris": null,
                "fig_num": "4",
                "type_str": "figure"
            },
            "FIGREF4": {
                "num": null,
                "text": "Figure 5: Permutation samples.",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF5": {
                "num": null,
                "text": "Figure 6: Pre-training loss with respect to the number of pre-training iterations.",
                "uris": null,
                "fig_num": "6",
                "type_str": "figure"
            },
            "FIGREF6": {
                "num": null,
                "text": "Figure 7: (a) F1 score with respect to the distance between the entity pairs. (b) F1 score with respect to the number of relation for each sentence.",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF7": {
                "num": null,
                "text": "More detailed evaluations are in the Appendix A S1 . . . talk about what [barbara starr] PER:\u2665\u2663\u2660 PHYS-1:\u2665\u2660 was reporting from the [pentagon] FAC:\u2665\u2663\u2660 PHYS-2:\u2665\u2660 . S2 . . . [vladimir putin] PER:\u2665\u2663\u2660 PHYS-1:\u2665\u2663\u2660 was greeted by prince [charles] PER:\u2665\u2663\u2660 PHYS-3:\u2665\u2660 as he arrived in [london] GPE:\u2665\u2663\u2660 PHYS-2:\u2665\u2663\u2660|PHYS-4:\u2665\u2660 today .",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF8": {
                "num": null,
                "text": "Figure 8: The relaiton F1 score (exactly match) with respect to the number of entities on ACE05 test data.",
                "uris": null,
                "fig_num": "89",
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table><tr><td>Model</td><td colspan=\"2\">Entity Relation</td><td colspan=\"2\">Relation (exactly)</td></tr><tr><td>Sun, 2019a</td><td>84.2</td><td>-</td><td colspan=\"2\">59.1</td></tr><tr><td>Li, 2019</td><td>84.8</td><td>-</td><td colspan=\"2\">60.2</td></tr><tr><td>Luan, 2019 , \u2022</td><td>88.4</td><td>63.2</td><td>-</td></tr><tr><td>Wadden, 2019 , \u2022</td><td>88.6</td><td>63.4</td><td>-</td></tr><tr><td>ENPAR</td><td>86.9</td><td>66.1</td><td colspan=\"2\">63.5</td></tr><tr><td colspan=\"4\">Table 1: Results on the ACE05 test data.</td><td>means</td></tr><tr><td colspan=\"5\">that the model uses BERT. means that the model</td></tr><tr><td colspan=\"5\">uses ELMo as token embeddings. \u2022 stands for train-ing the model with multi-task learning. ENPAR is the</td></tr><tr><td colspan=\"4\">proposed model fine-tuned on ACE05 dataset.</td></tr></table>",
                "type_str": "table",
                "text": ", we evaluate the",
                "html": null,
                "num": null
            },
            "TABREF1": {
                "content": "<table><tr><td>Model</td><td colspan=\"2\">Entity Relation</td><td>Relation (exactly)</td></tr><tr><td>BERT</td><td>87.2</td><td>65.1</td><td>62.2</td></tr><tr><td>ENPAR</td><td>86.9</td><td>66.1</td><td>63.5</td></tr><tr><td>-MET</td><td>87.1</td><td>65.3</td><td>62.5</td></tr><tr><td>-MEP</td><td>87.4</td><td>65.6</td><td>62.7</td></tr><tr><td>-ACD</td><td>87.2</td><td>65.7</td><td>62.9</td></tr><tr><td>-PP</td><td>87.1</td><td>64.2</td><td>61.6</td></tr><tr><td>-CNN</td><td>87.1</td><td>66.0</td><td>62.9</td></tr><tr><td colspan=\"4\">Table 2: Results on the ACE05 test data in differ-</td></tr><tr><td colspan=\"4\">ent settings. BERT is our model without pre-training,</td></tr><tr><td colspan=\"4\">which is initialized by BERT BASE and fine-tuned on</td></tr><tr><td colspan=\"4\">ACE05 dataset. \"-*\" is ENPAR without * task, where</td></tr><tr><td colspan=\"4\">*  \u2208 {MET, MEP, ACD, PP} ; \"-CNN\" means that only loading parameters of the sentence encoder from</td></tr><tr><td colspan=\"4\">ENPAR, and the other parameters (i.e., the entity en-</td></tr><tr><td colspan=\"4\">coder and the entity pair encoder) are randomly initial-</td></tr><tr><td>ized.</td><td/><td/><td/></tr><tr><td>Percentage of Data</td><td colspan=\"2\">Entity Relation</td><td>Relation (exactly)</td></tr><tr><td>100%</td><td>86.9</td><td>65.2</td><td>61.7</td></tr><tr><td>75%</td><td>86.8</td><td>65.3</td><td>62.5</td></tr><tr><td>50%</td><td>87.1</td><td>64.6</td><td>61.5</td></tr><tr><td>25%</td><td>86.9</td><td>66.1</td><td>63.5</td></tr><tr><td>15%</td><td>87.3</td><td>65.9</td><td>63.6</td></tr><tr><td>5%</td><td>87.4</td><td>65.9</td><td>63.4</td></tr></table>",
                "type_str": "table",
                "text": "Results on the ACE05 test data by varying the size of pre-training data.",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table><tr><td>Model</td><td colspan=\"2\">Entity Relation</td><td>Relation (exactly)</td></tr><tr><td>Luan, 2019 , \u2022</td><td>65.2</td><td>41.6</td><td>-</td></tr><tr><td>Wadden, 2019 , \u2022</td><td>67.5</td><td>48.4</td><td>-</td></tr><tr><td>BERT</td><td>67.4</td><td>46.0</td><td>34.3</td></tr><tr><td>ENPAR</td><td>67.9</td><td>48.0</td><td>36.5</td></tr><tr><td>BERT</td><td>92.7</td><td>50.8</td><td>49.3</td></tr><tr><td>ENPAR</td><td>94.8</td><td>54.4</td><td>52.6</td></tr></table>",
                "type_str": "table",
                "text": "Examples from the ACE05 dataset with label annotations from BERT and ENPAR for comparison. The \u2665 is the gold standard, and the \u2663, \u2660 are the output of the BERT ,ENPAR respectively.",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Results on the SciERC test data (upper part) and the NYT test data (bottom part).",
                "html": null,
                "num": null
            },
            "TABREF4": {
                "content": "<table><tr><td>Entity Type</td><td>Model</td><td>P</td><td>R</td><td>F</td></tr><tr><td>WEA</td><td>BERT</td><td colspan=\"3\">75.9 78.0 76.9</td></tr><tr><td>(109)</td><td colspan=\"4\">ENPAR 79.0 72.5 75.6</td></tr><tr><td>FAC</td><td>BERT</td><td colspan=\"3\">77.9 75.2 76.5</td></tr><tr><td>(286)</td><td colspan=\"4\">ENPAR 78.1 76.2 77.2</td></tr><tr><td>VEH</td><td>BERT</td><td colspan=\"3\">80.9 80.2 80.5</td></tr><tr><td>(116)</td><td colspan=\"4\">ENPAR 80.9 80.2 80.5</td></tr><tr><td>LOC</td><td>BERT</td><td colspan=\"3\">72.4 77.2 74.7</td></tr><tr><td>(136)</td><td colspan=\"4\">ENPAR 70.6 79.4 74.7</td></tr><tr><td>PER</td><td>BERT</td><td colspan=\"3\">91.7 92.3 92.0</td></tr><tr><td>(2928)</td><td colspan=\"4\">ENPAR 91.0 92.1 91.5</td></tr><tr><td>GPE</td><td>BERT</td><td colspan=\"3\">86.8 89.4 88.1</td></tr><tr><td>(1013)</td><td colspan=\"4\">ENPAR 86.8 89.9 88.3</td></tr><tr><td>ORG</td><td>BERT</td><td colspan=\"3\">77.6 77.0 77.3</td></tr><tr><td>(817)</td><td colspan=\"4\">ENPAR 77.3 75.8 76.5</td></tr></table>",
                "type_str": "table",
                "text": "The entity performance of \"BERT\" and \"ENPAR\" for different entity types on ACE05 test data. The numbers in the first column are counts of entities.",
                "html": null,
                "num": null
            },
            "TABREF5": {
                "content": "<table/>",
                "type_str": "table",
                "text": "The relation performance (exactly match) of \"BERT\" and \"ENPAR\" for different relation types on ACE05 test data. The numbers in the first column are counts of relations.",
                "html": null,
                "num": null
            }
        }
    }
}