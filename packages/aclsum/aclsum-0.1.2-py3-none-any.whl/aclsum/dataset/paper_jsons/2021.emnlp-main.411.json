{
    "paper_id": "2021",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:35:32.849682Z"
    },
    "title": "OSCAR: Orthogonal Subspace Correction and Rectification of Biases in Word Embeddings Sunipa Dev UCLA",
    "authors": [
        {
            "first": "Tao",
            "middle": [],
            "last": "Li",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Utah",
                "location": {}
            },
            "email": ""
        },
        {
            "first": "Jeff",
            "middle": [
                "M"
            ],
            "last": "Phillips",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Utah",
                "location": {}
            },
            "email": "jeffmp@cs.utah.edu"
        },
        {
            "first": "Vivek",
            "middle": [],
            "last": "Srikumar",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Utah",
                "location": {}
            },
            "email": ""
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Language representations are known to carry certain associations (e.g., gendered connotations) which may lead to invalid and harmful predictions in downstream tasks. While existing methods are effective at mitigating such unwanted associations by linear projection, we argue that they are too aggressive: not only do they remove such associations, they also erase information that should be retained. To address this issue, we propose OS-CAR (Orthogonal Subspace Correction and Rectification), a balanced approach of mitigation that focuses on disentangling associations between concepts that are deemed problematic, instead of removing concepts wholesale. We develop new measurements for evaluating information retention relevant to the debiasing goal. Our experiments on genderoccupation associations show that OSCAR is a well-balanced approach that ensures that semantic information is retained in the embeddings and unwanted associations are also effectively mitigated.\nKeeping in mind that removing bias and retaining information have to be done in synergy, we present how to obtain aggregated measurements for these two components. We will first describe the de-",
    "pdf_parse": {
        "paper_id": "2021",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Language representations are known to carry certain associations (e.g., gendered connotations) which may lead to invalid and harmful predictions in downstream tasks. While existing methods are effective at mitigating such unwanted associations by linear projection, we argue that they are too aggressive: not only do they remove such associations, they also erase information that should be retained. To address this issue, we propose OS-CAR (Orthogonal Subspace Correction and Rectification), a balanced approach of mitigation that focuses on disentangling associations between concepts that are deemed problematic, instead of removing concepts wholesale. We develop new measurements for evaluating information retention relevant to the debiasing goal. Our experiments on genderoccupation associations show that OSCAR is a well-balanced approach that ensures that semantic information is retained in the embeddings and unwanted associations are also effectively mitigated.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            },
            {
                "text": "Keeping in mind that removing bias and retaining information have to be done in synergy, we present how to obtain aggregated measurements for these two components. We will first describe the de-",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Word embeddings are used extensively across natural language processing (NLP) and succinctly capture not only the syntactic and semantic structure of language, but also word meaning in context. As such, word embeddings are essential building blocks for today's state-of-the-art in NLP. But they are also known to capture a significant amount of stereotypical associations (e.g., Bolukbasi et al., 2016; Zhao et al., 2017; Dev and Phillips, 2019; Sun et al., 2019) related to gender, race, nationality, or religion, which can manifest in unwanted and/or potentially harmful ways in downstream tasks (Webster et al., 2018; Zhao et al., 2019; Dev et al., 2020) . Such potentially problematic associations, when embedded in word representations, can lead to incorrect and unfair decisions about large groups of people. While the term \"bias\" has many meanings, in this paper we use it to refer to these unwanted stereotypical associations.",
                "cite_spans": [
                    {
                        "start": 379,
                        "end": 402,
                        "text": "Bolukbasi et al., 2016;",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 403,
                        "end": 421,
                        "text": "Zhao et al., 2017;",
                        "ref_id": "BIBREF31"
                    },
                    {
                        "start": 422,
                        "end": 445,
                        "text": "Dev and Phillips, 2019;",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 446,
                        "end": 463,
                        "text": "Sun et al., 2019)",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 598,
                        "end": 620,
                        "text": "(Webster et al., 2018;",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 621,
                        "end": 639,
                        "text": "Zhao et al., 2019;",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 640,
                        "end": 657,
                        "text": "Dev et al., 2020)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Existing methods to mitigate these effects either require expensive retraining of vectors (Lemoine et al., 2018) which can be inefficient, or projecting out information contained along an entire subspace representing a protected concept (such as gender or race) in the embedding space (e.g., Bolukbasi et al., 2016; Dev and Phillips, 2019; Ravfogel et al., 2020) . Projective approaches are difficult to control as they are either insufficient: removing a subspace can still leave residual bias (Gonen and Goldberg, 2019; Lauscher et al., 2020) , or too aggressive: in the case of gender, also unnecessarily altering the association between the word pregnant and words like female and mother. In tasks such as coreference resolution, removing such associations could hinder reference resolution.",
                "cite_spans": [
                    {
                        "start": 90,
                        "end": 112,
                        "text": "(Lemoine et al., 2018)",
                        "ref_id": null
                    },
                    {
                        "start": 292,
                        "end": 315,
                        "text": "Bolukbasi et al., 2016;",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 316,
                        "end": 339,
                        "text": "Dev and Phillips, 2019;",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 340,
                        "end": 362,
                        "text": "Ravfogel et al., 2020)",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 495,
                        "end": 521,
                        "text": "(Gonen and Goldberg, 2019;",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 522,
                        "end": 544,
                        "text": "Lauscher et al., 2020)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "To quantify how much valid information is retained, we look at the output space of models. Following Dev et al. (2020) , we use Natural Language Inference (NLI) as an effective quantitative probe. Here, we construct the hypothesis by making minimal edits to a premise, and observe model prediction conditioned on such changes. For example, Premise: A matriarch sold a watch. Hypothesis: A woman sold a watch.",
                "cite_spans": [
                    {
                        "start": 101,
                        "end": 118,
                        "text": "Dev et al. (2020)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Here, the objective is to determine if the hypothesis is entailed by the premise, contradicted by it, or neither (neutral to it). A GloVe-based NLI (Parikh et al., 2016) model, without any explicit form of bias mitigation, predicts label entail with a high probability of 97%; the notion of a matriarch being a woman is correctly identified by the model. However, after projective debiasing, the model classifies the pair as neutral with a probability 62% while the probability of the label entail drops to much lower at 16%. That is, aggressive mitigation of gender representations erases valid gender associations.",
                "cite_spans": [
                    {
                        "start": 148,
                        "end": 169,
                        "text": "(Parikh et al., 2016)",
                        "ref_id": "BIBREF22"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Ideally, we should correct problematic associations without erasing valid ones. To this end, we propose OSCAR (Othogonal Subspace Correction and Rectification) which orthogonalizes and rectifies identified subspaces of concepts that are incorrectly associated in an embedding space. Embeddings outside the subspace are stretched in a graded manner or untouched. Our contributions are:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "1. We argue that mitigating unwanted stereotypical associations should go beyond information removal (e.g., projecting out features), and should also preserve pertinent associations.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "2. We present OSCAR1 , a completely different method from the existing projective approaches; it uses orthogonalization of subspaces desired not to have interdependence, and so minimal change is made to embeddings to prevent loss of desired associations.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "3. We develop a combination of tests based on NLI to evaluate both bias mitigation and information retention.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "4. Our experiments show that OSCAR is a wellbalanced approach that mitigates biases as good as projective approaches while retaining more valid associations.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Our contributions, focusing specifically on representations rather than classifiers, are important given the preponderance of distributed representations of text across NLP. Predictions from systems that use such representations, if unchecked, could lead to real-world decisions (involving, e.g. hiring) that \"systematically and unfairly discriminate against certain individuals or groups of individuals in favor of others\" (Friedman and Nissenbaum, 1996) . In an effort to prevent such transition of representational harms into allocational harms (cf. Crawford, 2017; Abbasi et al., 2019; Blodgett et al., 2020) , we look at mitigating stereotyping biases at the source, i.e. the embedding space.",
                "cite_spans": [
                    {
                        "start": 424,
                        "end": 455,
                        "text": "(Friedman and Nissenbaum, 1996)",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 548,
                        "end": 568,
                        "text": "(cf. Crawford, 2017;",
                        "ref_id": null
                    },
                    {
                        "start": 569,
                        "end": 589,
                        "text": "Abbasi et al., 2019;",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 590,
                        "end": 612,
                        "text": "Blodgett et al., 2020)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "2 Bias, Gender and NLP Bias and Related Work. Social biases in tasks using machine learning have the potential to cause harms (Barocas and Selbst, 2016) and are widely studied. Such biases in language technologies have been detected (Bolukbasi et al., 2016; Gonen and Goldberg, 2019) , measured (Caliskan et al., 2017; Webster et al., 2018; Lauscher et al., 2020) and mitigated (Zhao et al., 2018; Ravfogel et al., 2020) . Most work focuses on gender bias, and in particular, the stereotypical associations of occupations to males and females (Rudinger et al., 2018; De-Arteaga et al., 2019; Gaut et al., 2019; Dev et al., 2020) . In this work too, bias refers to stereotypical associations in language representations.",
                "cite_spans": [
                    {
                        "start": 126,
                        "end": 152,
                        "text": "(Barocas and Selbst, 2016)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 233,
                        "end": 257,
                        "text": "(Bolukbasi et al., 2016;",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 258,
                        "end": 283,
                        "text": "Gonen and Goldberg, 2019)",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 295,
                        "end": 318,
                        "text": "(Caliskan et al., 2017;",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 319,
                        "end": 340,
                        "text": "Webster et al., 2018;",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 341,
                        "end": 363,
                        "text": "Lauscher et al., 2020)",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 378,
                        "end": 397,
                        "text": "(Zhao et al., 2018;",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 398,
                        "end": 420,
                        "text": "Ravfogel et al., 2020)",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 543,
                        "end": 566,
                        "text": "(Rudinger et al., 2018;",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 567,
                        "end": 591,
                        "text": "De-Arteaga et al., 2019;",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 592,
                        "end": 610,
                        "text": "Gaut et al., 2019;",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 611,
                        "end": 628,
                        "text": "Dev et al., 2020)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Treatment of Gender. The concept of gender is a complex one (cf. Larson, 2017; Dev et al., 2021) .",
                "cite_spans": [
                    {
                        "start": 65,
                        "end": 78,
                        "text": "Larson, 2017;",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 79,
                        "end": 96,
                        "text": "Dev et al., 2021)",
                        "ref_id": "BIBREF12"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In our experiments, we identify a subspace in the representation associated with gender, typically derived by terms specifically associated with notions of male and female. This aligns with prominent recent NLP research (Caliskan et al., 2017; Bolukbasi et al., 2016; Dev and Phillips, 2019; Ravfogel et al., 2020) that try to measure and mitigate gender bias (either implicitly or explicitly), directly focusing on male versus female associations.",
                "cite_spans": [
                    {
                        "start": 220,
                        "end": 243,
                        "text": "(Caliskan et al., 2017;",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 244,
                        "end": 267,
                        "text": "Bolukbasi et al., 2016;",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 268,
                        "end": 291,
                        "text": "Dev and Phillips, 2019;",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 292,
                        "end": 314,
                        "text": "Ravfogel et al., 2020)",
                        "ref_id": "BIBREF24"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The underlying goal of this work is to disassociate two concepts. We mainly use concepts male-vsfemale and occupations to demonstrate this. There is explicit and measurable bias encoded in word vector representations (Bolukbasi et al., 2016) by the male-vs-female relation to occupations, and standard evaluation metrics (Caliskan et al., 2017; Dev et al., 2020) are designed to measure this. As such many of our experiments are geared towards these male-vs-female issues.",
                "cite_spans": [
                    {
                        "start": 217,
                        "end": 241,
                        "text": "(Bolukbasi et al., 2016)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 321,
                        "end": 344,
                        "text": "(Caliskan et al., 2017;",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 345,
                        "end": 362,
                        "text": "Dev et al., 2020)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The notion of gender is not restricted to male and female. But because \"conceptualizations of gender as binary are reinforced linguistically in English through pronoun conventions\" (Blodgett, 2021), gender notions beyond male and female are either missing or poorly represented. As a result, their associations with other concepts are also not robust. Quantifying this effect, and evaluating its attenuation, is beyond the scope of this paper; however the techniques we study for disassociating two concepts are amenable to such tasks in general. We remark that a potential harm towards nonmale/female people arises through omission (Cao and Daum\u00e9 III, 2020) , and as a result, we explicitly encourage further research on this topic. sign of our evaluation dataset in \u00a73.1, then present evaluation metrics. Then, we will discuss how to measure biases in \u00a73.2 and information retention in \u00a73.3. To be comprehensive, we will use metrics that are both intrinsic to embedding geometry and extrinsic ones that focus on downstream NLI model outputs.",
                "cite_spans": [
                    {
                        "start": 642,
                        "end": 658,
                        "text": "Daum\u00e9 III, 2020)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Inspired by Dev et al. (2020) , we use NLI as a probe to assess the impact of embeddings in model outputs. Consider, for instance:",
                "cite_spans": [
                    {
                        "start": 12,
                        "end": 29,
                        "text": "Dev et al. (2020)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Designing Evaluation Data",
                "sec_num": "3.1"
            },
            {
                "text": "Premise: The doctor bought a bagel. Hypothesis 1: The woman bought a bagel. Hypothesis 2: The man bought a bagel.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Designing Evaluation Data",
                "sec_num": "3.1"
            },
            {
                "text": "Both hypotheses are neutral with respect to the premise. However, GloVe, using the decomposible attention model (Parikh et al., 2016) , deems that the premise entails hypothesis 1 with a probability 84% and contradicts hypothesis 2 with a probability 91%. Models that use contextualized representations (e.g., ELMo, BERT, and RoBERTa) are no better and perpetrate similarly biased associations. It has also been demonstrated that this effect can be mitigated by methods which project word vectors along the relevant subspace (Dev et al., 2020) ; these incorrect inferences are reduced, implying a reduction in the association encoded.",
                "cite_spans": [
                    {
                        "start": 112,
                        "end": 133,
                        "text": "(Parikh et al., 2016)",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 525,
                        "end": 543,
                        "text": "(Dev et al., 2020)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Designing Evaluation Data",
                "sec_num": "3.1"
            },
            {
                "text": "But what happens if definitionally gendered information is relayed by a sentence pair? The existing tasks and datasets for embedding quality do not directly evaluate that. For instance:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Designing Evaluation Data",
                "sec_num": "3.1"
            },
            {
                "text": "Premise: The gentleman drove a car. Hypothesis 1: The woman drove a car. Hypothesis 2: The man drove a car.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Designing Evaluation Data",
                "sec_num": "3.1"
            },
            {
                "text": "The premise should contradict the first hypothesis and entail the second. We thus expand the use of NLI task as a probe not just to measure the amount of the incorrect gender association expressed, but also the amount of correct gender information (or other relevant attributes) expressed. This permits a balanced view in evaluating valid correlations retained and information lost in an explicit manner.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Designing Evaluation Data",
                "sec_num": "3.1"
            },
            {
                "text": "For stereotyping biases, we will use the existing WEAT (Caliskan et al., 2017) as the intrinsic metric. For extrinsic measurement, we will use the existing NLI data in (Dev et al., 2020) which has \u223c 1.9 million sentence pairs generated via template instantiation. We will refer to this data as the neutrality dataset since examples there are intentionally constructed to have label neutral.",
                "cite_spans": [
                    {
                        "start": 55,
                        "end": 78,
                        "text": "(Caliskan et al., 2017)",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 168,
                        "end": 186,
                        "text": "(Dev et al., 2020)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Measuring Stereotypical Associations",
                "sec_num": "3.2"
            },
            {
                "text": "To measure information retention in word embeddings, we define two new metrics in this work: the intrinsic WEAT * measure, a modified version of WEAT, and the extrinsic SIRT measure that captures the downstream impact of word embeddings and its bias-mitigated versions.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Measures for Information Preservation",
                "sec_num": "3.3"
            },
            {
                "text": "New intrinsic measure (WEAT * ). We modify WEAT (Caliskan et al., 2017) to measure meaningful male-vs-female associations instead of stereotypical ones. Specifically, we use the following two sets of target words (gendered): X: {man, male, boy, brother, him, his, son} Y : {woman, female, girl, sister, her, hers, daugh-ter} and another two sets of definitionally male-vsfemale words, A for male (e.g., gentleman) and B for female (e.g. matriarch). Our WEAT * test has the same formulation as the original WEAT: where, \u00b5 denotes the average. The score is then normalized by stddev w\u2208X\u222aY s(w, A, B).",
                "cite_spans": [
                    {
                        "start": 48,
                        "end": 71,
                        "text": "(Caliskan et al., 2017)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Measures for Information Preservation",
                "sec_num": "3.3"
            },
            {
                "text": "s(X, Y, A, B) = x\u2208X s(x, A, B)-",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Measures for Information Preservation",
                "sec_num": "3.3"
            },
            {
                "text": "Different from WEAT, in WEAT * , since the sets A and B are definitionally male-vs-female, the higher the s(\u2022) value, the more meaningful malevs-female association is captured. We will present more details in \u00a76 and the Supplement D.7.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Measures for Information Preservation",
                "sec_num": "3.3"
            },
            {
                "text": "New extrinsic measure (SIRT). Existing benchmark datasets for NLI do not explicitly evaluate for gendered information retention. As a result, maintaining a high F1 score on these test sets does not necessarily suggest low information loss.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Measures for Information Preservation",
                "sec_num": "3.3"
            },
            {
                "text": "For a more focused evaluation, we extend the use of the NLI-based probe (Dev et al., 2020) to evaluate correctly labeled male-vs-female information to define the Sentence Inference Retention Test (SIRT). Unlike the original neutrality dataset, here sentences are constructed in a way that the ground truth labels are either entailment or contradiction.",
                "cite_spans": [
                    {
                        "start": 72,
                        "end": 90,
                        "text": "(Dev et al., 2020)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Measures for Information Preservation",
                "sec_num": "3.3"
            },
            {
                "text": "In our entailment dataset, a sentence pair would have words conveying the same gender as the subject words in both the premise and the hypothesis:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Measures for Information Preservation",
                "sec_num": "3.3"
            },
            {
                "text": "Premise: The lady bought a bagel.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Measures for Information Preservation",
                "sec_num": "3.3"
            },
            {
                "text": "The woman bought a bagel.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Hypothesis:",
                "sec_num": null
            },
            {
                "text": "We should note here that not all identically gendered words can be used interchangeably in the premise and hypothesis. For instance, the same premise with subject mother should entail the hypothesis with woman, but not the opposite. Thus such directional subject pairs are excluded in our data. Our data is generated via template instantiation where we use 12 subjects (6 male and 6 female) in the premise, 4 subjects (man, male, woman and female) in the hypothesis, 27 verbs, and 184 objects. The verbs are arranged into categories (e.g., commerce verbs like bought or sold; interaction verbs like spoke etc) to appropriately match objects for coherence (i.e. avoiding absurd instantiations like The man ate a car).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Hypothesis:",
                "sec_num": null
            },
            {
                "text": "For our contradiction dataset, we use similar example constructions where the premise clearly contradicts the hypothesis, e.g., Premise: The lady bought a bagel. Hypothesis: The man bought a bagel.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Hypothesis:",
                "sec_num": null
            },
            {
                "text": "Unlike the case for entailment, here all words of the other out of male/female set can be used interchangebly in the premise and hypothesis sentences, as all combinations should be contradicted. Our template consists of 16 gendered words (8 male and 8 female) in the premise and the hypothesis, and the same verbs and objects as we used in the entailment data.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Hypothesis:",
                "sec_num": null
            },
            {
                "text": "For each dataset, we have \u223c 47 thousand NLI examples. We will obtain aggregated statistics over model predictions on these large datasets. Specifically, to measure performances on the entailment dataset, we define Net Entail as average of predicted probability mass: ( i P e (D i ))/|D|, where P e (\u2022) denotes the predicted probability on label entailment and |D| is the data size. Furthermore, we count the Fraction Entail score as the accuracy of model predictions. Similarly, we define Net Contradict and Fraction Contradict for our contradiction dataset. The higher the values these metrics, the more valid male-vs-female information is retained. Finally, for the neutrality data ( \u00a73.2), we similarly define Net Neutral and Fraction Neutral, and the higher scores, the more stereotypical association is mitigated in model predictions.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Hypothesis:",
                "sec_num": null
            },
            {
                "text": "We describe a new geometric operation, an alternative to the linear projection-based ones in the literature. This operator applies a graded rotation on the embedding space; it rectifies two identified directions (e.g., male-vs-female and occupations) which should ideally be independent of each other, so they become orthogonal, and remaining parts of their span are rotated at a varying rate so the operation is sub-differentiable. Given d-dimensional embeddings, OSCAR requires us to first identify two subspaces, denoted V 1 and V 2 , for which we desire the embedding space to not have interdependence. We will use subspaces representing male-vs-female (V 1 ) and occupation (V 2 ) as running examples, following previous work (Bolukbasi et al., 2016; Caliskan et al., 2017; Dev and Phillips, 2019) . Specifically we identify 1-dimensional vectors v 1 \u2208 V 1 and v 2 \u2208 V 2 best capturing these subspaces from word lists; those used for male-vs-female and occupation are listed in the Supplement. We ensure v 1 = v 2 , and normalize so v 1 = v 2 = 1.",
                "cite_spans": [
                    {
                        "start": 731,
                        "end": 755,
                        "text": "(Bolukbasi et al., 2016;",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 756,
                        "end": 778,
                        "text": "Caliskan et al., 2017;",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 779,
                        "end": 802,
                        "text": "Dev and Phillips, 2019)",
                        "ref_id": "BIBREF13"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Orthogonal Subspace Correction and Rectification (OSCAR)",
                "sec_num": "4"
            },
            {
                "text": "We restrict the representation adjustment to the subspace S = (v 1 , v 2 ) that is defined by, and spans the male-vs-female and occupation directions. In particular, we can identify an orthogonal basis for S using vectors v 1 and",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Orthogonal Subspace Correction and Rectification (OSCAR)",
                "sec_num": "4"
            },
            {
                "text": "v 2 = v 2 -v 1 v 1 ,v 2 v 2 -v 1 v 1 ,v 2 (",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Orthogonal Subspace Correction and Rectification (OSCAR)",
                "sec_num": "4"
            },
            {
                "text": "so the v 1 and v 2 vectors are orthogonal). We then restrict any word vector x \u2208 R d (e.g., job) to S as two coordinates \u03c0 S (x) = ( v 1 , x , v 2 , x ). We will adjust only these coordinates, and leave all d-2 other orthogonal components fixed.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Orthogonal Subspace Correction and Rectification (OSCAR)",
                "sec_num": "4"
            },
            {
                "text": "We now restrict our attention to within the subspace S. In detail, we do this by defining a d \u00d7 d rotation matrix U . The first two rows are v 1 and v 2 . The next d-2 rows are any set u 3 , . . . , u d which complete the orthogonal basis. We then rotate all data vectors x by U (as U x). Then we manipulate the first 2 coordinates (x 1 , x 2 ) to f (x 1 , x 2 ), described below, and then reattach the last d-2 coordinates, and rotate the space back by U T .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Orthogonal Subspace Correction and Rectification (OSCAR)",
                "sec_num": "4"
            },
            {
                "text": "Next we devise the function f which is applied to each word vector x \u2208 S (we can assume now that x is two-dimensional). See illustration in Figure 1 . Given male-vs-female v 1 and occupation v 2 subspaces let \u03b8 = arccos( v 1 , v 2 ) be the angle which captures their correlation, and \u03b8 = \u03c0 2 -\u03b8 . Now define a 2 \u00d7 2 rotation matrix which would rotate v 2 to v 2 (orthogonal to v 1 ). R = cos \u03b8 -sin \u03b8 sin \u03b8 cos \u03b8 .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 147,
                        "end": 148,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Orthogonal Subspace Correction and Rectification (OSCAR)",
                "sec_num": "4"
            },
            {
                "text": "However, the function f should be the identity male-vs-female o c c u p a t io n occupation -corrected the occupation x male-vs-female subspace e.g., \"job\" ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Orthogonal Subspace Correction and Rectification (OSCAR)",
                "sec_num": "4"
            },
            {
                "text": "v 1 < l a t e x i t s h a 1 _ b a s e 6 4 = \" 6 Q M K i s 1 k J Z h s h B J V M H + W O y Q J V k E = \" > A A A E K 3 i c d V P L b t Q w F H U b C m V 4 t W U H G 4 s p E p t G y Z S 0 Q 6 V K I 6 F K L B A U l T 6 k y a h y n D t T a x z H i p 3 C y M o n s G E B H 8 C a r 2 E F Y s O C / 8 D O d N T O D L W U 6 O a e c + 4 9 N 7 Y T y Z n S Q f B z Y d G 7 s X T z 1 v L t x p 2 7 9 + 4 / W F l d O 1 J 5 W V A 4 p D n P i 5 O E K O B M w K F m m s O J L I B k C Y f j Z P j S 4 c f n U C i W i / d 6 J K G X k Y F g f U a J t q m D 8 9 P w d K U Z + F u t I G h H O P C D z a 0 o e m G D M A o 3 n 2 / j 0 A / q 1 e w 8 + r P z + d v S w f 7 p q r c W p z k t M x C a c q J U N w y k 7 h l S a E Y 5 V I 2 4 V C A J H Z I B d G 0 o S A a q Z 2 q v F X 5 q M y n u 5 4 V 9 h M Z 1 t h E r s M 7 F Q J + Z W M N H / Y G l + m w 3 8 i M m q i n Q j X k t K E n B R G p d V S b I s j l M D Z m s T M u P a u z S o y G Z U q M s s d 4 y o s / U L O a S / 8 O 6 p e 6 3 e 4 Y J W W o Q t M J T Q i g V L Z j U c 6 3 q c o 3 Y z k H z L C M i N T F I V Z n 4 n B Q 2 Y D w X M 7 A d y W 2 w s n P F T E N m y + G N l t T 1 U C B 3 N 8 J x r H M 5 g W y F A q 7 W o F U 3 7 N l O Q p U F O A s m 3 i s P a o e m G V b V n C C Z F 7 h 3 k k z o V 8 n D F K r x z i l q 3 M c M n p S c W 4 L I x / u D 1 + s M 6 H X s / t o U k 6 d 1 Y x v 1 c a z 6 f Z I x P s K u 5 z T R n f W x R S s x 8 W t S D M D R L k U z A s r 0 R e X 6 z J m E l 2 D Z m o k R 7 j b D 3 m y D t / t 7 b 6 5 6 n u g K S K 3 M N X W M n Q p b Y a N h b 9 H k q u D r g 6 O W H 2 7 6 r X d h s 9 N G 4 7 W M H q M n 6 B k K 0 T b q o F d o H x 0 i i g b o E / q C v n r f v R / e L + / 3 m L q 4 c K F 5 i K a W 9 / c f e I l x Y w = = < / l a t e x i t > v 2 < l a t e x i t s h a 1 _ b a s e 6 4 = \" L O 9 B R u k W i U u q m 7 r a 8 9 P C 6 i x Q Y X 0 = \" > A A A E K 3 i c d V P L b t Q w F H U b C m V 4 t W U H G 4 s p E p t G y Z S 0 Q 6 V K I 6 F K L B A U l T 6 k y a h y n D t T a 2 L H s p 3 C y M o n s G E B H 8 C a r 2 E F Y s O C / 8 D J d N T O D L W U 6 O a e c + 4 9 N 7 Y T m T F t g u D n w q J 3 Y + n m r e X b j T t 3 7 9 1 / s L K 6 d q T z Q l E 4 p H m W q 5 O E a M i Y g E P D T A Y n U g H h S Q b H y f B l h R + f g 9 I s F + / N S E K P k 4 F g f U a J c a m D 8 9 P W 6 U o z 8 L d a Q d C O c O A H m 1 t R 9 M I F Y R R u P t / G o R / U q 9 l 5 9 G f n 8 7 e l g / 3 T V W 8 t T n N a c B C G Z k T r b h h I 0 7 N E G U Y z K B t x o U E S O i Q D 6 L p Q E A 6 6 Z 2 u v J X 7 q M i n u 5 8 o 9 w u A 6 2 4 g 1 O O d i Y M 5 s b O C j + c B S c 7 Y b + R E T 5 R R Y j X k t K I l i I n W u S h t w P o f p I Z O l b f l R j V 1 6 t I R r P e K J 8 8 a J O d O z W J X 8 H 9 Y t T L / d s 0 z I w o C g J Z 4 S Q q G p Y t L M t a r L N W I 3 B 8 0 5 J y K 1 M U h d 2 v i c K B e w L B c z s B u p 2 m D t 5 o q Z A e 7 K 4 Y 2 W N P V Q I H c 3 w n F s c j m B X A U F V 2 v Q s h v 2 X C e h C w W V B R v v F Q e 1 Q 9 s M y 3 J O k M w L q n e S T O h X y c M U y v H O a W q r j x k 8 K b L M E U Q + 3 h + 8 X m f A r O P q r 0 0 x s 7 R u 7 K I + j n W / T z j L R r j q O U 2 s z v r Y o p P Y + D V R A 6 h o l 6 I Z A W X m o n J 9 5 m y S F e D Y h o k R 7 j b D 3 m y D t / t 7 b 6 5 6 n u g U p E 5 W N a 0 Y O y V 2 w k b D 3 a L J V c H X B 0 c t P 9 z 0 W + / C Z q e N x m s Z P U Z P 0 D M U o m 3 U Q a / Q P j p E F A 3 Q J / Q F f f W + e z + 8 X 9 7 v M X V x 4 U L z E E 0 t 7 + 8 / f E 5 x Z A = = < / l a t e x i t > v < l a t e x i t s h a 1 _ b a s e 6 4 = \" r T F 5 F d h X 0 I 8 D F j f P S 0 5 j X f Y 7 B R 8 = \" > A A A E L H i c d V N b b 9 M w F P Y W L q N c d u G R F 2 s d G k J a l Q T K J q R J k 9 A Q D w i K Y B f R V J P j n L Z W H S e y n U F l 5 S / w C i / 8 B X 4 N L w j x y u / A T l p t b Z m l R C f n + 7 5 z v h P b c c 6 Z 0 r 7 / a 2 n Z u 3 b 9 x s 2 V W 4 3 b d + 7 e W 1 1 b 3 z h W W S E p H N G M Z / I 0 J g o 4 E 3 C k m e Z w m k s g a c z h J B 6 9 c P j J O U j F M v F B j 3 P o p W Q g W J 9 R o l 3 q / C z c P l t r + i 2 / W t h v t f 3 w 2 W 6 I 6 8 z T A A c T q H m w u Z 0 9 / v j y e + d s 3 d u I k o w W K Q h N O V G q G / i 5 7 h k i N a M c y k Z U K M g J H Z E B d G 0 o S A q q Z y q z J X 5 o M w n u Z 9 I + Q u M q 2 4 g U W O t i o I c m 0 v B Z f 2 K J H u 6 3 W 2 0 m y h n Q z X k l m B P J R G J d l c Z P 0 w V M j V h e m r D V r r A L j 4 a k S o 3 T 2 H p L i R 6 q e c w l / 4 d 1 C 9 3 f 6 x k m 8 k K D o C W e E U K h q G S 5 X m h V l W t E d g 6 a p S k R i Y k g V 6 W J z o m 0 A e O Z m I P t S G 6 H l Z 0 r Y h p S W w 7 v h L m u h o J 8 f y e o Y 5 3 l U 8 h W k H C 5 B i 2 7 Q c 9 2 E q q Q 4 C y Y 6 L B 4 X z k 0 z a A s F w T x o s C 9 4 3 h K v 0 w e J V D W O 6 e o c R 9 z e F x w b g k i q / c H b 1 U Z 0 F v Y / b U Z J k + q x j b q 4 0 j 1 + y R l f I x d z 1 m i O + y 1 R S s x 0 W s i B + B o F 6 I 5 A W V 6 U r k 6 c y b m B V i 2 Z m K M u 8 2 g N 9 / g b e f w z W X P U 5 2 E x M p c U 8 d 4 X m I r b D T s L Z p e F X x 1 c B y 2 g i e t 8 F 3 Q P N h D 9 V p B D 9 A m e o Q C t I s O 0 C v U Q U e I o i H 6 g r 6 i b 9 4 P 7 6 f 3 2 / t T U 5 e X J p r 7 a G Z 5 f / 8 B S 4 d w x w = = < / l a t e x i t > x < l a t e x i t s h a 1 _ b a s e 6 4 = \" v O h I G k Y E k 9 + C g H e t J e Z h + i / C d w A = \" > A A A E K X i c d V P L b h M x F H U 7 P E J 4 t W X J Z k S K 1 E 2 j T K q I C q l S J F S J B Y J U k L Z S J q o 8 n p v E i u 2 x b E 9 p Z M 0 X s G A D X 8 F f 8 A f s g C 3 f A G v s S a L m A Z Z m d O e e c + 4 9 d 2 w n k l F t G o 0 f G 5 v B j Z u 3 b l f u V O / e u / / g 4 d b 2 z q n O c k W g S z K W q f M E a 2 B U Q N d Q w + B c K s A 8 Y X C W j F 9 4 / O w S l K a Z e G c m E v o c D w U d U I K N S 5 1 c X W z V G v V G u c L 1 I J o F t X b l 9 9 e P 8 d 6 f z s V 2 s B O n G c k 5 C E M Y 1 r o X N a T p W 6 w M J Q y K a p x r k J i M 8 R B 6 L h S Y g + 7 b 0 m k R P n W Z N B x k y j 3 C h G W 2 G m t w v s X Q j G x s 4 M q 8 p 6 k Z H b X q L S q K J d A P + V 9 Q Y k V F 6 l w V t s H 5 G q b H V B a 2 W W + V 2 L V H i 7 n W E 5 4 4 b x y b k V 7 F f P J f W C 8 3 g 8 O + p U L m B g Q p w i U h 5 J o o K s 1 a q 7 J c N X Z z k I x z L F I b g 9 S F j S + x c g F l m V i B 3 U h + e 7 W b K 6 Y G u C s X 7 j e l K Y c C e b Q f T W O T y T n k K i h Y r E G K X t R 3 n Y T O F X g L N j 7 O 3 5 Y O b S 0 q i j V B s i 7 w 7 y S Z 0 x f J 4 x S K 6 c 5 p Y v 3 H C p 7 k j D m C y K b 7 E + 6 W G T C 7 o f 9 r S 0 y W l o 1 d N A h j P R h g T t k k 9 D 2 X i f 6 k T y 0 6 i Y 1 f Y T U E T 7 s W r Q g I N b P K 5 Z m z C c v B s Q 0 V k 7 B X i / q r D d 5 0 j l 8 v e p 7 r F K R O 5 p t 6 x v M i d M J q 1 d 2 i a P X O r A e n z X p 0 U G + e R L X 2 I Z q u C n q M n q A 9 F K F n q I 1 e o g 7 q I o I A f U C f 0 O f g S / A t + B 7 8 n F I 3 N 2 a a R 2 h p B b / + A r v t c Y E = < / l a t e x i t > f < l a t e x i t s h a 1 _ b a s e 6 4 = \" y I V 5 4 k v s G p 5 d E o V e T X n e p 3 C 7 o Q 8 = \" > A A A E K X i c d V P L b t Q w F H U b H m V 4 9 M G S B R b T S m w 6 i g e V T p E q V U K V W C B o B X 1 I k 1 H l O D d T a x L H s p 3 C y M q O H V v 4 C v 6 D P T v o F o k v 4 A O w M x 2 1 M 0 M t J b q 5 5 5 x 7 z 4 3 t W G Z c m z D 8 N T c f 3 L h 5 6 / b C n c b d e / c f L C 4 t r x z q o l Q M D l i R F e o 4 p h o y L u D A c J P B s V R A 8 z i D o 3 j w 0 u N H Z 6 A 0 L 8 R 7 M 5 T Q y 2 l f 8 J Q z a l x q P z 1 Z a o a t M A w J I d g H Z P N 5 6 I K t r U 6 b d D D x k F v N n c d r n / 5 + / 3 O + d 7 I c r E R J w c o c h G E Z 1 b p L Q m l 6 l i r D W Q Z V I y o 1 S M o G t A 9 d F w q a g + 7 Z 2 m m F 1 1 w m w W m h 3 C M M r r O N S I P z L f r m 1 E Y G P p o P P D G n 2 x u t D S 6 q C d A P e S 0 o q e I i c a 4 q G + b 5 D K Y H X F a 2 3 d q o s U u P l u Z a D / P Y e c u p O d X T m E / + D + u W J u 3 0 L B e y N C B Y h S e E U G q m u D Q z r e p y j c j N w Y o 8 p y K x E U h d 2 e i M K h f w r B B T s B v J b 6 9 2 c 0 X c Q O 7 K 4 f W 2 N P V Q I L f X y S g 2 h R x D r o K C q z V Y 1 S U 9 1 0 n o U o G 3 Y K P d 8 l 3 t 0 D Z J V c 0 I 4 l m B f 8 f x m H 6 V P E i g G u 2 c Z t Z / T O F x m W W O I I r R / u D V O g N m F f u / N s H M k r q x i 1 I c 6 T S l O c + G 2 P e c J P q T P r L o J D Z 6 T V U f P O 1 S N C V g 3 F x U r s + c j b M S H N t w M c T d J u l N N 3 i 7 t / v m q u e x T k H i Z L 6 p Z 7 y o s B M 2 G u 4 W j a 8 K v j 4 4 b L f I s 1 Z 7 n z R 3 O m i 0 F t A j 9 A Q 9 R Q R t o h 3 0 C u 2 h A 8 Q Q o M / o C / o a f A t + B D + D 8 x F 1 f u 5 C 8 x B N r O D 3 P / K c c h 8 = < / l a t e x i t > S < l a t e x i t s h a 1 _ b a s e 6 4 = \" L v l D U s / 4 y U h C f y 4 W F I 7 3 G i i E f Q U = \" > A A A E K X i c d V P L b h M x F H U 7 P E J 4 t W X J Z k S K 1 E 2 j T K q I C q l S J F S J B Y J U J W 2 l T F R 5 P D e J F d t j 2 Z 5 C Z M 0 X s G A D X 8 F f 8 A f s g C 3 f A G v s S a L m A Z Z m d O e e c + 4 9 d 2 w n k l F t G o 0 f G 5 v B j Z u 3 b l f u V O / e u / / g 4 d b 2 z p n O c k W g S z K W q Y s E a 2 B U Q N d Q w + B C K s A 8 Y X C e j F 9 4 / P w K l K a Z e G s m E v o c D w U d U I K N S 5 2 c X m 7 V G v V G u c L 1 I J o F t X b l 9 9 e P 8 d 6 f z u V 2 s B O n G c k 5 C E M Y 1 r o X N a T p W 6 w M J Q y K a p x r k J i M 8 R B 6 L h S Y g + 7 b 0 m k R P n W Z N B x k y j 3 C h G W 2 G m t w v s X Q j G x s 4 L 1 5 R 1 M z O m r V W 1 Q U S 6 A f 8 r + g x I q K 1 L k q b I P z N U y P q S x s s 9 4 q s W u P F n O t J z x x 3 j g 2 I 7 2 K + e S / s F 5 u B o d 9 S 4 X M D Q h S h E t C y D V R V J q 1 V m W 5 a u z m I B n n W K Q 2 B q k L G 1 9 h 5 Q L K M r E C u 5 H 8 9 m o 3 V 0 w N c F c u 3 G 9 K U w 4 F 8 m g / m s Y m k 3 P I V V C w W I M U v a j v O g m d K / A W b H y c n 5 Y O b S 0 q i j V B s i 7 w 7 y S Z 0 x f J 4 x S K 6 c 5 p Y v 3 H C p 7 k j D m C y K b 7 E + 6 W G T C 7 o f 9 r S 0 y W l o 1 d N A h j P R h g T t k k 9 D 2 X i f 6 k T y 0 6 i Y 1 f Y T U E T 7 s W r Q g I N b P K 5 Z m z C c v B s Q 0 V k 7 B X i / q r D d 5 0 j l 8 v e p 7 r F K R O 5 p t 6 x v M i d M J q 1 d 2 i a P X O r A d n z X p 0 U G + e R L X 2 I Z q u C n q M n q A 9 F K F n q I 1 e o g 7 q I o I A f U C f 0 O f g S / A t + B 7 8 n F I 3 N 2 a a R 2 h p B b / + A j B 0 c V w = < / l a t e x i t > f < l a t e x i t s h a 1 _ b a s e 6 4 = \" y I V 5 4 k v s G p 5 d E o V e T X n e p 3 C 7 o Q 8 = \" > A A A E K X i c d V P L b t Q w F H U b H m V 4 9 M G S B R b T S m w 6 i g e V T p E q V U K V W C B o B X 1 I k 1 H l O D d T a x L H s p 3 C y M q O H V v 4 C v 6 D P T v o F o k v 4 A O w M x 2 1 M 0 M t J b q 5 5 5 x 7 z 4 3 t W G Z c m z D 8 N T c f 3 L h 5 6 / b C n c b d e / c f L C 4 t r x z q o l Q M D l i R F e o 4 p h o y L u D A c J P B s V R A 8 z i D o 3 j w 0 u N H Z 6 A 0 L 8 R 7 M 5 T Q y 2 l f 8 J Q z a l x q P z 1 Z a o a t M A w J I d g H Z P N 5 6 I K t r U 6 b d D D x k F v N n c d r n / 5 + / 3 O + d 7 I c r E R J w c o c h G E Z 1 b p L Q m l 6 l i r D W Q Z V I y o 1 S M o G t A 9 d F w q a g + 7 Z 2 m m F 1 1 w m w W m h 3 C M M r r O N S I P z L f r m 1 E Y G P p o P P D G n 2 x u t D S 6 q C d A P e S 0 o q e I i c a 4 q G + b 5 D K Y H X F a 2 3 d q o s U u P l u Z a D / P Y e c u p O d X T m E / + D + u W J u 3 0 L B e y N C B Y h S e E U G q m u D Q z r e p y j c j N w Y o 8 p y K x E U h d 2 e i M K h f w r B B T s B v J b 6 9 2 c 0 X c Q O 7 K 4 f W 2 N P V Q I L f X y S g 2 h R x D r o K C q z V Y 1 S U 9 1 0 n o U o G 3 Y K P d 8 l 3 t 0 D Z J V c 0 I 4 l m B f 8 f x m H 6 V P E i g G u 2 c Z t Z / T O F x m W W O I I r R / u D V O g N m F f u / N s H M k r q x i 1 I c 6 T S l O c + G 2 P e c J P q T P r L o J D Z 6 T V U f P O 1 S N C V g 3 F x U r s + c j b M S H N t w M c T d J u l N N 3 i 7 t / v m q u e x T k H i Z L 6 p Z 7 y o s B M 2 G u 4 W j a 8 K v j 4 4 b L f I s 1 Z 7 n z R 3 O m i 0 F t A j 9 A Q 9 R Q R t o h 3 0 C u 2 h A 8 Q Q o M / o C / o a f A t + B D + D 8 x F 1 f u 5 C 8 x B N r O D 3 P / K c c h 8 = < / l a t e x i t > f < l a t e x i t s h a 1 _ b a s e 6 4 = \" y I V 5 4 k v s G p 5 d E o V e T X n e p 3 C 7 o Q 8 = \" > A A A E K X i c d V P L b t Q w F H U b H m V 4 9 M G S B R b T S m w 6 i g e V T p E q V U K V W C B o B X 1 I k 1 H l O D d T a x L H s p 3 C y M q O H V v 4 C v 6 D P T v o F o k v 4 A O w M x 2 1 M 0 M t J b q 5 5 5 x 7 z 4 3 t W G Z c m z D 8 N T c f 3 L h 5 6 / b C n c b d e / c f L C 4 t r x z q o l Q M D l i R F e o 4 p h o y L u D A c J P B s V R A 8 z i D o 3 j w 0 u N H Z 6 A 0 L 8 R 7 M 5 T Q y 2 l f 8 J Q z a l x q P z 1 Z a o a t M A w J I d g H Z P N 5 6 I K t r U 6 b d D D x k F v N n c d r n / 5 + / 3 O + d 7 I c r E R J w c o c h G E Z 1 b p L Q m l 6 l i r D W Q Z V I y o 1 S M o G t A 9 d F w q a g + 7 Z 2 m m F 1 1 w m w W m h 3 C M M r r O N S I P z L f r m 1 E Y G P p o P P D G n 2 x u t D S 6 q C d A P e S 0 o q e I i c a 4 q G + b 5 D K Y H X F a 2 3 d q o s U u P l u Z a D / P Y e c u p O d X T m E / + D + u W J u 3 0 L B e y N C B Y h S e E U G q m u D Q z r e p y j c j N w Y o 8 p y K x E U h d 2 e i M K h f w r B B T s B v J b 6 9 2 c 0 X c Q O 7 K 4 f W 2 N P V Q I L f X y S g 2 h R x D r o K C q z V Y 1 S U 9 1 0 n o U o G 3 Y K P d 8 l 3 t 0 D Z J V c 0 I 4 l m B f 8 f x m H 6 V P E i g G u 2 c Z t Z / T O F x m W W O I I r R / u D V O g N m F f u / N s H M k r q x i 1 I c 6 T S l O c + G 2 P e c J P q T P r L o J D Z 6 T V U f P O 1 S N C V g 3 F x U r s + c j b M S H N t w M c T d J u l N N 3 i 7 t / v m q u e x T k H i Z L 6 p Z 7 y o s B M 2 G u 4 W j a 8 K v j 4 4 b L f I s 1 Z 7 n z R 3 O m i 0 F t A j 9 A Q 9 R Q R t o h 3 0 C u 2 h A 8 Q Q o M / o C / o a f A t + B D + D 8 x F 1 f u 5 C 8 x B N r O D 3 P / K c c h 8 = < / l a t e x i t > f < l a t e x i t s h a 1 _ b a s e 6 4 = \" y I V 5 4 k v s G p 5 d E o V e T X n e p 3 C 7 o Q 8 = \" > A A A E K X i c d V P L b t Q w F H U b H m V 4 9 M G S B R b T S m w 6 i g e V T p E q V U K V W C B o B X 1 I k 1 H l O D d T a x L H s p 3 C y M q O H V v 4 C v 6 D P T v o F o k v 4 A O w M x 2 1 M 0 M t J b q 5 5 5 x 7 z 4 3 t W G Z c m z D 8 N T c f 3 L h 5 6 / b C n c b d e / c f L C 4 t r x z q o l Q M D l i R F e o 4 p h o y L u D A c J P B s V R A 8 z i D o 3 j w 0 u N H Z 6 A 0 L 8 R 7 M 5 T Q y 2 l f 8 J Q z a l x q P z 1 Z a o a t M A w J I d g H Z P N 5 6 I K t r U 6 b d D D x k F v N n c d r n / 5 + / 3 O + d 7 I c r E R J w c o c h G E Z 1 b p L Q m l 6 l i r D W Q Z V I y o 1 S M o G t A 9 d F w q a g + 7 Z 2 m m F 1 1 w m w W m h 3 C M M r",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Orthogonal Subspace Correction and Rectification (OSCAR)",
                "sec_num": "4"
            },
            {
                "text": "K x E U h d 2 e i M K h f w r B B T s B v J b 6 9 2 c 0 X c Q O 7 K 4 f W 2 N P V Q I L f X y S g 2 h R x D r o K C q z V Y 1 S U 9 1 0 n o U o G 3 Y K P d 8 l 3 t 0 D Z J V c 0 I 4 l m B f 8 f x m H 6 V P E i g G u 2 c Z t Z / T O F x m W W O I I r R / u D V O g N m F f u / N s H M k r q x i 1 I c 6 T S l O c + G 2 P e c J P q T P r L o J D Z 6 T V U f P O 1 S N C V g 3 F x U r s + c j b M S H N t w M c T d J u l N N 3 i 7 t / v m q u e x T k H i Z L 6 p Z 7 y o s B M 2 G u 4 W j a 8 K v j 4 4 b L f I s 1 Z 7 n z R 3 O m i 0 F t A j 9 A Q 9 R Q R t o h 3 0 C u 2 h A 8 Q Q o M / o C / o a f A t + B D + D 8 x F 1 f u 5 C 8 x B N r O D 3 P / K c c h 8 = < / l a t e x i t > \u2713 < l a t e x i t s h a 1 _ b a s e 6 4 = \" r y l g y H l n 5 z W l d 2 F Z 4 L 8 4 p K G D k 5 w = \" > A A A E L n i c d V P L b h M x F H U 7 P E J 4 t W X J Z k S K 1 E 2 j T F B E h V Q p E q r E A k E Q 9 C H F U e X x 3 C Q m t s e y P Y X I m m + A L X w F 3 8 B H I L F A b P k B W G N P W j U P s D S j O / e c c + + 5 Y z t V n B n b a n 1 f W 4 + u X L 1 2 v X a j f v P W 7 T t 3 N z a 3 j k x e a A q H N O e 5 P k m J A c 4 k H F p m O Z w o D U S k H I 7 T y d O A H 5 + B N i y X b + x U w U C Q k W R D R o n 1 q S N s x 2 D J 6 U a j 1 W x V K 1 4 N k v O g 0 a 3 9 / v o B 7 / z p n W 5 G W z j L a S F A W s q J M f 2 k p e z A E W 0 Z 5 V D W c W F A E T o h I + j 7 U B I B Z u A q u 2 X 8 0 G e y e J h r / 0 g b V 9 k 6 N u D N y 5 E d O 2 z h v X 3 H M j v e 7 z Q 7 T J Y L Y J j 0 v 6 A i m s n M u y p d S 4 g V z E y Y K l 2 7 2 a m w S 4 + O C G O m I v X e B L F j s 4 y F 5 L + w f m G H e w P H p C o s S F r G C 0 I o D N V M 2 Z V W V b k 6 9 n P Q X A g i M 4 d B m d L h M 6 J 9 w H g u l 2 A / U t h j 4 + f C z I L w 5 e L d t r L V U K D 2 d 5 N Z b H N 1 A f k K G u Z r 0 L K f D H w n a Q o N w Y L D B 8 X r y q F r J G W 5 I k h X B e G d p h f 0 e f I k g 3 K 2 c 4 a 6 8 L G E p w X n n i D z 2 f 7 E 2 1 U G 7 H Y c / t o C k 2 d V Y x 8 N Y 2 y G Q y I Y",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Orthogonal Subspace Correction and Rectification (OSCAR)",
                "sec_num": "4"
            },
            {
                "text": "n 8 a h 5 y I x H P e Z R S 9 x + D n R I w i 0 S 9 G S g D J 7 X r k 6 c y 7 l B X i 2 Z X I a 9 x v J Y L n B y 9 7 B i 3 n P F z o N m Z e F p o H x p I y 9 s F 7 3 t y h Z v j O r w V G 7 m T x q t l 8 l j e 4 e m q 0 a u o 8 e o B 2 U o M e o i 5 6 h H j p E F L 1 F H 9 E n 9 D n 6 E n 2 L f k Q / Z 9 T 1 t X P N P b S w o l 9 / A f m m c 6 0 = < / l a t e x i t > \u2713 0 < l a t e x i t s h a 1 _ b a s e 6 4 = \" d 5 J y",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Orthogonal Subspace Correction and Rectification (OSCAR)",
                "sec_num": "4"
            },
            {
                "text": "K t W 4 Z b k x k D K h j S r X r s 3 6 S c w = \" > A A A E L 3 i c d V P L b h M x F H U 7 P E p 4 t W X J Z k S K 6 K Z R J i i i Q q p U C V V i g S A I 0 l S K o 8 r j u U m s 2 B 7 L 9 h Q i a 7 4 B s Y W v 4 B f 4 C c Q G s e U D Y I 0 9 a d U 8 w N K M 7 t x z z r 3 n j u 1 U c W Z s s / l 9 b T 2 6 c v X a 9 Y 0 b t Z u 3 b t + 5 u 7 m 1 f W z y Q l P o 0 p z n + i Q l B j i T 0 L X M c j h R G o h I O f T S y b O A 9 8 5 A G 5 b L t 3 a q Y C D I S L I h o 8 T 6 V A / b M V j y 6 H S z 3 m w 0 q x W v B s l 5 U D / c + P 3 1 A 9 7 9 0 z n d i r Z x l t N C g L S U E 2 P 6 S V P Z g S P a M s q h r O H C g C J 0 Q k b Q 9 6 E k A s z A V X 7 L + K H P Z P E w 1 / 6 R N q 6 y N W z A u 5 c j O 3 b Y w n v 7 j m V 2 f N B u t J k s F 8 A w 6 n 9 B R T S T m X d V u q Y Q K 5 i Z M F W 6 V q N d Y Z c e H R H G T E X q v Q l i x 2 Y Z C 8 l / Y f 3 C D v c H j k l V W J C 0 j B e E U B i q m b I r r a p y N e z n o L k Q R G Y O g z K l w 2 d E + 4 D x X C 7 B f q S w y c b P h Z k F 4 c v F e y 1 l q 6 F A H e w l s 9 j m 6 g L y F T T M 1 6 B l P x n 4 T t I U G o I F h 4 + K N 5 V D V 0 / K c k W Q r g r C O 0 0 v 6 P P k S Q b l b O c M d e F j C U 8 L z j 1 B 5 r P 9 i X e q D N i d O P y 1 B S b P q s Y + G s b Y D I d E M D 6 N Q 8 9 F Y j j v M 4 t e 4 v A L o k c Q a J e i J Q F l 9 r x y d e Z c y g v w b M v k N O 7 X k 8 F y g 1 e d o 5 f z n i 9 0 G j I v C 0 0 D 4 2 k Z e 2 G t 5 m 9 R s n x n V o P j V i N 5 3 G i 9 T u q H + 2 i 2 N t B 9 9 A D t o g Q 9 Q Y f o O e q g L",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Orthogonal Subspace Correction and Rectification (OSCAR)",
                "sec_num": "4"
            },
            {
                "text": "q J o g j 6 i T + h z 9 C X 6 F v 2 I f s 6 o 6 2 v n m n t o Y U W / / g K 2 W X P e < / l a t e x i t > 1 < l a t e x i t s h a 1 _ b a s e 6 4 = \" q I",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Orthogonal Subspace Correction and Rectification (OSCAR)",
                "sec_num": "4"
            },
            {
                "text": "A h L T f L 4 6 2 Y e W I D W Z F z 4 2 o o T 1 I = \" > A A A E L 3 i c d V P L b h M x F H U 7 P E p 4 9 M G S B R Z p J T a N M k E R F V K l S q g S C w R F 0 K Z S J o o 8 n p v E i u 2 x b E 8 h s m b H D 7 C F r + A n + A U E C 8 Q K i T 0 f g D 1 p 1 T z A 0 o z u 3 H P O v e e O 7 V R x Z m y z + W 1 l N b p y 9 d r 1 t R u 1 m 7 d u 3 1 n f 2 N w 6 M X m h K R z T n O f 6 N C U G O J N w b J n l c K o 0 E J F y 6 K T j p w H v n I E 2 L J d v 7 E R B T 5 C h Z A N G i f W p T q J G r B / j / k a 9 2 W h W C y 8 H 8 X l Q P 7 i / 8 / 7 P l 1 / f j / q b 0 V a S 5 b Q Q I C 3 l x J h u 3 F S 2 5 4 i 2 j H I o a 0 l h Q B E 6 J k P o + l A S A a b n K r 8 l 3 v G Z D A 9 y 7 R 9 p c Z W t J Q a 8 e z m 0 I 5 d Y e G f f s s y O 9 t u N N p P l H B h G / S + o i G Y y 8 6 5 K 1 x R i C T N j p k r X a r Q r 7 N K j I 8 K Y i U i 9 N 0 H s y C x i I f k v r F v Y w V 7 P M a k K C 5 K W e E 4 I h a G a K b v U q i p X S / w c N B e C y M w l o E z p k j O i f c B 4 L h d g P 1 L Y Z O P n S p g F 4 c v h 3 Z a y 1 V C g 9 n f j a W x z d Q H 5 C h p m a 9 C y G / d 8 J 2 k K D c G C S w 6 L 1 5 V D V 4 / L c k m Q L g v C O 0 0 v 6 L P k c Q b l d O c",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Orthogonal Subspace Correction and Rectification (OSCAR)",
                "sec_num": "4"
            },
            {
                "text": "M d e F j A U 8 L z j 1 B 5 t P 9 w d t V B u w 2 D n 9 t j s m z q r G P B j g x g w E R j E 9 w 6 D l P D O d 9 a t F L X P K c 6 C E E 2 q V o Q U C Z P a 9 c n T m X 8 g I 8 2 z I 5 w d 1 6 3 F t s 8 P L o 8 M W s 5 w u d h s z L Q t P A e F J i L 6 z V / C 2 K F + / M c n D S a s S P G q 1 X c f 1 g D 0 3 X G r q H H q C H K E a P 0 Q F 6 h o 7 Q M a J o j D 6 g j + h T 9 D n 6 G v 2 I f k 6 p q y v n m r t o b k W / / w J L S X Q M < / l a t e x i t > d 2 < l a t e x i t s h a 1 _ b a s e 6 4 = \" K n T s R a X 9 6 k 4 L F 7 z 0 N U z t L 8 p B map for v 1 so f (v 1 ) = v 1 , but apply R to v 2 so f (v 2 ) = Rv 2 ; this maps v 2 to v 2 . And for any other word vector x, it should provide a smooth partial application of this rotation so f is continuous.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Orthogonal Subspace Correction and Rectification (OSCAR)",
                "sec_num": "4"
            },
            {
                "text": "P R M = \" > A A A E K 3 i c d V P L b h M x F H U 7 F E p 4 9 M E O N i N S J D a N M k E R V a V K k V A l F g i C Q t p K m a j y e G 4 S K 7 b H s j 2 F y J p P Y M M C P o A 1 X 8 M K x I Y F / 4 E 9 S d Q 8 w N K M 7 t x z z r 3 n j u 1 E M q p N v f 5 j Y z O 4 s X X z 1 v b t y p 2 7 9 + 7 v 7 O 7 t n + k s V w S 6 J G O Z u k i w B k Y F d A 0 1 D C 6 k A s w T B u f J + I X H z 6 9 A a Z q J d 2 Y i o c / x U N A B J d i 4 V C e 9 b F z u V u u 1 e r n C 9 S C a B d X W w 9 / H n 7 5 u d d q X e 8 F + n G Y k 5 y A M Y V j r X l S X p m + x M p Q w K C p x r k F i M s Z D 6 L l Q Y A 6 6 b 0 u v R f j E Z d J w k C n 3 C B O W 2 U q s w T k X Q z O y s Y E P 5 j 1 N z e i k W W t S U S y B f s z / g h I r K l L n q r B 1 z t c w P a a y s I 1 a s 8 S u P V r M t Z 7 w x H n j 2 I z 0 K u a T / 8 J 6 u R k c 9 S 0 V M j c g S B E u C S H X R F F p 1 l q V 5 S q x m 4 N k n G O R 2 h i k L m x 8 h Z U L K M v E C u x G 8 h u s 3 V w x N c B d u f C w I U 0 5 F M i T w 2 g a m 0 z O I V d B w W I N U v S i v u s k d K 7 A W 7 D x a d 4 p H d p q V B R r g m R d 4 N 9 J M q c v k s c p F N O d 0 8 T 6 j x U 8 y R l z B J F N 9 y c 8 K D N g D k L / 1 5 a Y L C 0 b u 2 g Q x n o w w J y y S e h 7 L h P 9 W Z 9 a d B I b v 8 J q C J 5 2 L V o R E G p m l c s z Z x O W g 2 M b K i Z h r x r 1 V x u 8 a Z + + X v Q 8 1 y l I n c w 3 9 Y z j I n T C S s X d o m j 1 z q w H Z 4 1 a 9 K z W e B t V W 0 d o u r b R I / Q Y P U U R",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Orthogonal Subspace Correction and Rectification (OSCAR)",
                "sec_num": "4"
            },
            {
                "text": "In particular, for each data point x \u2208 S, we will determine an angle \u03b8 x and apply a rotation matrix f (x) = R x x defined as",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Orthogonal Subspace Correction and Rectification (OSCAR)",
                "sec_num": "4"
            },
            {
                "text": "R x = cos \u03b8 x -sin \u03b8 x sin \u03b8 x cos \u03b8 x .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Orthogonal Subspace Correction and Rectification (OSCAR)",
                "sec_num": "4"
            },
            {
                "text": "Towards defining \u03b8 x (how much to rotate a word vector x on this plane), we calculate two other measurements \u03c6 = arccos v 1 , x x and",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Orthogonal Subspace Correction and Rectification (OSCAR)",
                "sec_num": "4"
            },
            {
                "text": "d 2 = v 2 , x",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Orthogonal Subspace Correction and Rectification (OSCAR)",
                "sec_num": "4"
            },
            {
                "text": "x , which determines which quadrant x is in, in Figure 1 . Now we have a case analysis:",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 55,
                        "end": 56,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Orthogonal Subspace Correction and Rectification (OSCAR)",
                "sec_num": "4"
            },
            {
                "text": "\u03b8 x = \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3 \u03b8 \u03c6 1 \u03b8 if d > 0 and \u03c6 1 < \u03b8 \u03b8 \u03c0-\u03c6 1 \u03c0-\u03b8 if d > 0 and \u03c6 1 > \u03b8 \u03b8 \u03c0-\u03c6 1 \u03b8 if d < 0 and \u03c6 1 \u2265 \u03c0 -\u03b8 \u03b8 \u03c6 1 \u03c0-\u03b8 if d < 0 and \u03c6 1 < \u03c0 -\u03b8 .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Orthogonal Subspace Correction and Rectification (OSCAR)",
                "sec_num": "4"
            },
            {
                "text": "This effectively orthogonalizes the components of all points exactly along the male-vs-female v 1 and occupation v 2 directions. So, points lying in subspaces especially near v 2 get moved the most, those near v 1 are moved little, and the rest of the embedding space faces a graded rotation. The information contained outside the occupation\u00d7malevs-female subspace S remains the same, thus preserving most of the inherent structure and content of the original embedding representation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Orthogonal Subspace Correction and Rectification (OSCAR)",
                "sec_num": "4"
            },
            {
                "text": "We have an almost everywhere differentiable operation applied onto all vectors in the space, enabling us to extend this method to contextualized embeddings. It can now be part of the model specification and integrated with the gradient-based finetuning step. Further, it is a post processing step applied onto an embedding space, thus its computational cost is relatively low, and it is easily malleable for a given task for which specific subspaces may be desired as independent.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Orthogonal Subspace Correction and Rectification (OSCAR)",
                "sec_num": "4"
            },
            {
                "text": "Debiasing methods. Gender association with other concepts and its reduction has been observed on GloVe embeddings (Bolukbasi et al., 2016; Dev and Phillips, 2019; Lauscher et al., 2020; Ravfogel et al., 2020; Rathore et al., 2021, inter alia) . However, they are mostly projection-based, and begin by identifying a subspace represented as a vector v. For fair comparison across all these methods, we determine v with embeddings of he and she. Some methods require an auxillary set of definitionally male/female words G are treated separately (Supplement D.3 and D.4). Linear Projection (LP): We consider this as the simplest operation. For every word vector w, we project it onto an identified direction v to remove the respective component:",
                "cite_spans": [
                    {
                        "start": 114,
                        "end": 138,
                        "text": "(Bolukbasi et al., 2016;",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 139,
                        "end": 162,
                        "text": "Dev and Phillips, 2019;",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 163,
                        "end": 185,
                        "text": "Lauscher et al., 2020;",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 186,
                        "end": 208,
                        "text": "Ravfogel et al., 2020;",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 209,
                        "end": 242,
                        "text": "Rathore et al., 2021, inter alia)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Methodology",
                "sec_num": "5"
            },
            {
                "text": "w = w -w w, v .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Methodology",
                "sec_num": "5"
            },
            {
                "text": "Afterwards, the w vector (of d-dim) lies in a (d-1)dim subspace, but still retains d coordinates; the subspace v is removed. Lauscher et al. (2020) showed such simple projection reduces the most bias and has the least residual bias. Hard Debiasing (HD): The original debiasing method, by Bolukbasi et al. (2016) , begins with the same projection operation as above, but uses G to only apply it to a subset of all words. First, it trains a linear SVM to separate G from random words. All words labeled as part of G, called G , are also assumed definitionally male-vs-female, and not adjusted by projection. The exception is another subset G \u2282 G (see Supplement D.3) which come in pairs (e.g., man-woman). These words are projected along v, but then are put through another operation called equalize. This ensures that after projection, each pair is the same distance apart as they were before projection, but entirely not within the subspace defined by v. As we will observe (similar to Gonen and Goldberg (2019) ), equalization and the set G retains certain male-vs-female information in the embedding (compared to projection), but has trouble generalizing when used on words that may carry stereotypical male-vs-female connotations outside of G (such as proper names). Iterative Nullspace Projection (INLP): This method (Ravfogel et al., 2020) begins with LP using v on all words except the set G. It then automatically identifies a second set B of most biased words: these are the most extreme words along the direction v (or -v). After the operation, it identifies the residual bias by building a linear classifier on B. The normal of this classifier is then chosen as the next direction v 1 on which to apply the next LP operation, removing another subspace. It iterates 35 times, finding v 2 and so on, until no significant residual association can be identified. OSCAR: Finally, we also apply OSCAR, using he-she as v 2 , and the subspace defined by an occupation list (see Supplement D.2) as v 1 . This subspace is determined by the first principal component of the word vectors in the list. Our code for reproducing experiments will be released upon publication. It is important to note that OSCAR, unlike HD or INLP, does not use any pre-determined lists of words that determine which words to not debias. This is especially advantageous as it is expensive and not possible to demarcate entire vocabularies of languages into two groups-one that has meaningful associations and are not to be debiased, and another list of other words to debias.",
                "cite_spans": [
                    {
                        "start": 125,
                        "end": 147,
                        "text": "Lauscher et al. (2020)",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 288,
                        "end": 311,
                        "text": "Bolukbasi et al. (2016)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 986,
                        "end": 1011,
                        "text": "Gonen and Goldberg (2019)",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 1321,
                        "end": 1344,
                        "text": "(Ravfogel et al., 2020)",
                        "ref_id": "BIBREF24"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Methodology",
                "sec_num": "5"
            },
            {
                "text": "Debiasing for contextualized embeddings. The operations above are described for a noncontextualized embedding; we use one of the largest such embeddings GloVe (on 840 B token Common Crawl). They can be applied to contextualized embedding as well; we use RoBERTa (the base version released by Wolf et al. (2019) in (v4.2.1)), the widely adopted state-of-the-art architecture. As advocated by Dev et al. (2020) , we only apply the operation (e.g., LP) on the context independent word embeddings that constitute part of the first layer of RoBERTa. Technically, these are subword-embeddings, but all words in G (e.g. he, she) are full subwords in RoBERTa, and so there is no ambiguity. Ravfogel et al. (2020) extend INLP to contextualized embeddings, but only in a task specific way, whereas we focus on debiasing and evaluating information retention in general. So, for consistency, we only apply INLP on the first (context-independent) layer of RoBERTa, and refer to it as INLP .",
                "cite_spans": [
                    {
                        "start": 292,
                        "end": 310,
                        "text": "Wolf et al. (2019)",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 391,
                        "end": 408,
                        "text": "Dev et al. (2020)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 682,
                        "end": 704,
                        "text": "Ravfogel et al. (2020)",
                        "ref_id": "BIBREF24"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Methodology",
                "sec_num": "5"
            },
            {
                "text": "When fine-tuning RoBERTa, we initialize subword embeddings by one of the debiasing methods, and allow gradient to update them. This offers a realistic application scenario where all transformer weights are usually subject to update. It also allows a fair comparison between LP/OSCAR and HD/INLP since the later are not differentiable. 2Extrinsic measurement through NLI. We train our NLI models using the SNLI (Bowman et al., 2015) dataset. While MultiNLI (Williams et al., 2018) contains more complex and diverse sentences than SNLI, making it a good contender for training entailment models, we observed that MultiNLI also carries significantly stronger implicit male-vs-female association. When training on MultiNLI, our model tends to return a significantly higher proportion of stereotyped outcomes (Table S2 in the Supplement). Over 90% of sentence pairs that should have neutral associations are classified incorrectly using both GloVe and RoBERTa. In contrast, using SNLI yields less than 70% unwanted associations and incorrect classifications on the same dataset. Since we focus on the unwanted representational association in word embeddings and not in the datasets used for NLI, using MultiNLI adds a confounding factor. Moreover, MultiNLI has more complex sentences than SNLI, which means that there is a larger probability of having noise via irrelevant information. This would in turn weaken the implication of an incorrect inference of stereotypes expressed.",
                "cite_spans": [
                    {
                        "start": 410,
                        "end": 431,
                        "text": "(Bowman et al., 2015)",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 456,
                        "end": 479,
                        "text": "(Williams et al., 2018)",
                        "ref_id": "BIBREF28"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 811,
                        "end": 813,
                        "text": "S2",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Experimental Methodology",
                "sec_num": "5"
            },
            {
                "text": "Keeping train/test separate. Two of the debiasing methods (HD and INLP) use a male-vs-female word list G as an essential step, and the subset G \u2282 G of words which are equalized by HD overlap with some words in WEAT (in Supplement D.6), WEAT * (in Supplement D.7), and the extrinsic NLI tests (in Supplement D.5). We allowed this overlap between the training phase for HD and INLP and the testing phase, as the word lists are recommended as ideal for their respective operations by their authors. As a result, HD and INLP may have an advantage in the evaluations. 3Since we only use the words he and she to determine the male-vs-female subspace (which are not used in WEAT or NLI tasks), we can use all other male/female words in the list in the Supplement to generate templates for NLI tasks. We also avoid using he and she in WEAT * , except for one controlled example (WEAT * (1) in Table S3 ). For occupations, however, since we use a subset of occupation words to construct the occupation subspace for the OS-CAR operation (Supplement D.2), we have disjoint lists of occupations: one set for identifying the subspace and one for testing with WEAT (Supplement D.6) and NLI templates (Supplement D.5).",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 891,
                        "end": 893,
                        "text": "S3",
                        "ref_id": "TABREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Experimental Methodology",
                "sec_num": "5"
            },
            {
                "text": "Intrinsic measurement of bias. Table 1 shows the results on WEAT (Caliskan et al., 2017) between groups of attribute words (i.e. he and she) and target words (i.e. 3 sets of stereotypes: occupations, work vs. home, and math vs. art). It also shows results for the Embedding Coherence Test (ECT, Dev and Phillips, 2019) , showing the association of vectors from X (male/female words) with a list of attribute neutral words Y (occupation) using the Spearman Coefficient. The score ranges in [-1, 1] with 1 being ideal. We see that, across different metrics, OSCAR substantially reduces biases and performs on par with the other best performing methods. Its best performance is on WEAT (1) on occupations, but rectifying occupations still generalizes to other tasks. (3) math versus art related words. ECT also measures bias and a higher score implies less bias.",
                "cite_spans": [
                    {
                        "start": 65,
                        "end": 88,
                        "text": "(Caliskan et al., 2017)",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 295,
                        "end": 318,
                        "text": "Dev and Phillips, 2019)",
                        "ref_id": "BIBREF13"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 37,
                        "end": 38,
                        "text": "1",
                        "ref_id": "TABREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Experimental Results",
                "sec_num": "6"
            },
            {
                "text": "Extrinsic measurement of bias. Intrinsic metric of male-vs-female information preserved. Table 3 demonstrates how much useful male-vs-female information is retained according to our WEAT * metric after different mitigation methods. To verify our WEAT * evaluation is reliable, we first used random word sets, and after 1000 trials the average WEAT or WEAT * score was 0.001 with standard deviation of 0.33. The first column uses sets A = {he} and B = {she} to represent male-vs-female, the same words used to define the mitigation direction. This is the only experiment where the test/train split is not maintained for OSCAR (and other methods). The second column uses two sets of 59 definitionally male and female words as A and B, and the third uses 700 statistically male and female names for each set A and B (both in Supplement D.7). The baseline row (row 1, unmodified GloVe) shows these methods provide similar WEAT * scores.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 95,
                        "end": 96,
                        "text": "3",
                        "ref_id": "TABREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Experimental Results",
                "sec_num": "6"
            },
            {
                "text": "The LP method retains the least information for tests based on he-she or other male-vs-female words. These scores are followed closely behind by the scores for INLP in these tasks. The single projection of LP retains more info when compared against statistically male and female names, whereas INLP's multiple projections appear to remove more of this information, since only some names appear on its pre-specified lists of words to shield from debiasing whereas many male and female words do. HD also performs well for he-she and definitionally male and female words; we suspect the equalize steps allows the information to be retained for these types of words. However, since only some names are pre-specified to be equalized or kept as is, HD loses much more information when compared against statistically male and female names. We use these names in this test as a proxy of many other statistically male and female words which might encode such information but due to the sheer size of the vocabulary, would be impossible to pre-specify exhaustively. Finally, OSCAR performs at or near the best in all evaluations. HD (with its equalize step and pre-specified word lists), performs better for he-she and other male and female words, but this does not generalize to words not specifically set to be adjusted as observed in the names evaluation. In fact, since names do not come in explicit pairs (like uncleaunt), this equalize step is not even possible.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Results",
                "sec_num": "6"
            },
            {
                "text": "Extrinsic metric of gendered information preserved. For the new SIRT task we observe the performance using GloVe and RoBERTa in Table 4 . GloVe, without any debiasing, which is our baseline, correctly preserves male-vs-female information as seen by the first row. The fraction of correct entails (male premise\u2192male hypothesis, female premise\u2192female hypothesis) and contradicts (male premise\u2192female hypothesis, female premise\u2192male hypothesis) are both high. We see a fall in these scores in all projection based methods (LP, HD and INLP), with a uniform projections step (LP) doing the best among the three. OSCAR does better than all three methods in all four scores measuring valid entailments and contradictions. RoBERTa, with its different layers learning con- ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 134,
                        "end": 135,
                        "text": "4",
                        "ref_id": "TABREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Experimental Results",
                "sec_num": "6"
            },
            {
                "text": "The propagation of undesirable and stereotypical associations learned from data into decisions made by language models maintains a vicious cycle of biases. Combating biases before deploying representations is thus extremely vital. But this poses its own challenges. Word embeddings capture a lot of information implicitly in relatively few dimensions. These implicit associations are what makes them state-of-the-art at tackling different language modeling tasks. Breaking down these associations for bias rectification, thus, has to be done carefully so as to not destroy the structure of the embeddings. OSCAR's rectification of associations helps integrate both these aspects, allowing it to be more suitable at making word embeddings more usable. Moreover, being computationally lightweight and sub-differentiable, it is simple to apply adaptively without extensive retraining. OSCAR dissociates concepts which may be otherwise lost or overlapped in distributed representations by re-orthogonalizing them. We envision that this method can be used for many types of unwanted associations (age-ability, religion-virtue perception, etc). Since it only decouples specific associations, informative components of these features will remain unchanged. All the experiments in this paper are based on English embeddings, models and tests; extending our analysis to other languages is important future work. Further, we be-lieve OSCAR can extend in a straightforward way beyond NLP to other distributed, vectorized representations (e.g., of images, graphs, or spatial data) which can also exhibit stereotypical associations.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discussion & Conclusion",
                "sec_num": "7"
            },
            {
                "text": "Debiasing word embeddings is a very important and nuanced requirement for making embeddings suitable to be used in different tasks. It is not possible to have exactly one operation that works on all different embeddings and identifies and subsequently reduces all different biases it has significantly. What is more important to note is that this ability is not beneficial in every task uniformly, ie, not all tasks require bias mitigation to the same degree. Further, the debiasing need not apply to all word groups as generally. Disassociating only biased associations in a continuous manner is what needs to be achieved. Hence, having the ability to debias embeddings specifically for a given scenario or task with respect to specific biases is extremely advantageous.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Broader Impact",
                "sec_num": "8"
            },
            {
                "text": "Our method of orthogonal correction is easy to adapt to different types of biased associations, such as the good-bad notions attached to different races (Caliskan et al., 2017; Crenshaw, 1991) or religions (Dev et al., 2020) etc. Creating metrics is harder with not as many words to create templates or tests out of, making comprehensive evaluation of bias reduction or information retention harder in these types of biases. We leave that for future exploration.",
                "cite_spans": [
                    {
                        "start": 153,
                        "end": 176,
                        "text": "(Caliskan et al., 2017;",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 177,
                        "end": 192,
                        "text": "Crenshaw, 1991)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 206,
                        "end": 224,
                        "text": "(Dev et al., 2020)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Discussion",
                "sec_num": null
            },
            {
                "text": "We have used four distinct debiasing methods in this paper. They differ in some key aspects as listed in Table S1 .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 111,
                        "end": 113,
                        "text": "S1",
                        "ref_id": "TABREF0"
                    }
                ],
                "eq_spans": [],
                "section": "B Comparison of Debiasing Methods",
                "sec_num": null
            },
            {
                "text": "While linear projection, hard debiasing and INLP all determine just the gender subspace for debiasing, INLP does so itertively to maximize bias retention. However, the list of words required for the same does not change between iterations. So all of them require a single word list for this segment. OSCAR determines two subspaces -malevs-female and occupation -in the case of this paper and thus it requires two word lists for this purpose. These word lists are small and contain roughly 10 words.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B Comparison of Debiasing Methods",
                "sec_num": null
            },
            {
                "text": "However, hard debiasing and INLP also require additional word lists for the task of debiasing. These are much longer lists of words. Hard debiaising requires 4 lists: one is a hard coded list of words which helps determine three word lists of which one it debiaises, one it equalizes about the gender axis and the rest it leaves as is. INLP also begins with one hard coded set of words to generate a second set of words it does not debias. This effectively means that both of these methods figure in using a set of hard coded words, a set of words that it protects from debiasing. While effective in many cases, it also is cumbersome in that it is not possible to detect or predict each and every word that might have a meaningful gendered component that should not be debiased. Linear projection and OSCAR do not require any supplementary word lists for the task of debiasing.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B Comparison of Debiasing Methods",
                "sec_num": null
            },
            {
                "text": "Further, since INLP and hard debaisng require these large lists of words for effective debiasing, often in tasks for measuring bias, there is little separation between test and train sets, which could render results for them that are better than in fairer settings.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B Comparison of Debiasing Methods",
                "sec_num": null
            },
            {
                "text": "We compare here the amount of gender bias contained by the templates in SNLI and MultiNLI. While MultiNLI has more complex sentences, it also contains more bias as seen in Table S2 using the standard metrics for neutrality defined by an earlier paper (Dev et al., 2020) . Since our work attempts to understand and mitigate the bias in language representation and not the bias in data used in various tasks, we restrict our experiments to SNLI which expresses significantly less bias that MultiNLI.",
                "cite_spans": [
                    {
                        "start": 251,
                        "end": 269,
                        "text": "(Dev et al., 2020)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 178,
                        "end": 180,
                        "text": "S2",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "C.1 SNLI versus MultiNLI",
                "sec_num": null
            },
            {
                "text": "Non-contextual word embeddings like GloVe or word2vec are characterized by their ability to capture semantic information reflected by valid similarities between word pairs and the ability to complete analogies. These properties reflect the quality of the word embedding obtained and should not be diminished post debiasing. We ascertain that in Table S3 . We use a standard word similarity test (Finkelstein et al., 2002) and an analogy test (Mikolov et al., 2013) which measure these properties in word embeddings across our baseline GloVe model and all the debiased GloVe models. All of them perform similarly to the baseline GloVe model, thus indicating that the structure of the embedding has been preserved. While this helps in preliminary",
                "cite_spans": [
                    {
                        "start": 395,
                        "end": 421,
                        "text": "(Finkelstein et al., 2002)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 442,
                        "end": 464,
                        "text": "(Mikolov et al., 2013)",
                        "ref_id": "BIBREF21"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 351,
                        "end": 353,
                        "text": "S3",
                        "ref_id": "TABREF3"
                    }
                ],
                "eq_spans": [],
                "section": "C.2 Standard Tests for Word Embedding Quality",
                "sec_num": null
            },
            {
                "text": "https://github.com/sunipa/OSCaR-Orth ogonal-Subspace-Correction-and-Rectifica tion/tree/transformer",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "We also experimented on a more constrained setting where the first layer of embeddings is frozen during finetuning. However, we found high variance in the metrics in preliminary experiments.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "3 Importantly, OSCAR does not have any overlap between the words used for training and evaluation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "Acknowledgements: We thank the support from NSF grant #2030859 to the Computing Research Association for the CIFellows Project, from NSF #1801446 (SATC), IIS-1927554, NSFCCF-1350888, CNS-1514520, CNS-1564287, IIS-1816149, and Visa Research. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "acknowledgement",
                "sec_num": null
            },
            {
                "text": "examination of the retention of information in the embeddings, these tests do not contain a large number of sensitive gender comparisons or word pairs. It is thus, not sufficient to claim using this that bias has been removed or that valid gender associations have been retained, leading to the requirement of methods described in the paper and methods in other work (Dev and Phillips, 2019; Dev et al., 2020; Bolukbasi et al., 2016; Caliskan et al., 2017) for the same. ",
                "cite_spans": [
                    {
                        "start": 367,
                        "end": 391,
                        "text": "(Dev and Phillips, 2019;",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 392,
                        "end": 409,
                        "text": "Dev et al., 2020;",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 410,
                        "end": 433,
                        "text": "Bolukbasi et al., 2016;",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 434,
                        "end": 456,
                        "text": "Caliskan et al., 2017)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "annex",
                "sec_num": null
            },
            {
                "text": "All these methods primarily use the words 'he' and 'she' to determine the male-vs-female subspace, though both hard debiasing and INLP use other pre-determined sets of male-vs-female words to guide the process of both debiasing and retention of some male-vs-female information.Statistically male/female names too have been seen to be good at helping capture the male-vsfemale subspace (Dev and Phillips, 2019) . We compare in Table S4 the correctly male-vs-female information retention when debiasing is done using projection or correction. We represent simple projection, hard debiasing and INLP by simple projection since it is the core debiasing step used by all three. Both rows have been debiaised using the male-vs-female subspace determined using most common statistically male and female names in Wikipedia (listed in Supplement) and correction uses in addition, the same occupation subspace as used in Table 3 . Each value is again a WEAT* calculation where the two sets of words (X and Y) being compared against are kept constant as the same in table 3. The first column of this table, thus represents the association with the subspace determined by 'he -she' and correction results in a higher association, thus implying that more correctly male-vs-female information retained. We see a similar pattern in columns 2 and 3 which represent other male-vs-female words and male/female names sans the names used to determine the subspace used for debiasing. That correction fares significantly better even among other statistically male and female names reflects the higher degree of precision of information removal and retention. Table S4 : Correctly male-vs-female information contained by embeddings; Larger scores better as they imply more correctly male-vs-female information expressed",
                "cite_spans": [
                    {
                        "start": 385,
                        "end": 409,
                        "text": "(Dev and Phillips, 2019)",
                        "ref_id": "BIBREF13"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 432,
                        "end": 434,
                        "text": "S4",
                        "ref_id": null
                    },
                    {
                        "start": 917,
                        "end": 918,
                        "text": "3",
                        "ref_id": null
                    },
                    {
                        "start": 1644,
                        "end": 1646,
                        "text": "S4",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "C.3 Male/Female Names and Debiasing",
                "sec_num": null
            },
            {
                "text": "In Table S5 , we show the TPR-Gap-RMS metric as used in (Ravfogel et al., 2020) which is an aggregated measurement of male-vs-female bias score over professions. Lower scores imply lesser malevs-female bias in professions. We refer readers to (Ravfogel et al., 2020) for detailed definition. We follow the same experiment steps, except that we apply different debiasing algorithms to the input word embeddings (instead of the CLS token). This allows us to compare debiasing methods with static use of contextualized embeddings (i.e. without fine-tuning). We see that RoBERTa and RoBERTa HD perform on par while both linear projection and iterative projection methods and our method OSCAR perform close to each other. Male-vs-female Words. These are the dictionarydefined male/female words used by the HD operation to determine what words are correctly male-vsfemale and which should be neutral in the embedding space. Here is the filtered version of the list G used in our experiments as per our description in \u00a75 about the test/train word list split. actress, actresses, aunt, aunts, bachelor, ballerina, barbershop, baritone, beard, beards, beau, bloke, blokes, boy, boyfriend, boyfriends, boyhood, boys, brethren, bride, brides, brother, brotherhood, brothers, bull, bulls, businessman, businessmen, businesswoman, chairman, chairwoman, chap, colt, colts, congressman, congresswoman, convent, councilman, councilmen, councilwoman, countryman, countrymen, czar, dad, daddy, dads, daughter, daughters, deer, diva, dowry, dude, dudes, estrogen, fathered, fatherhood, fathers, fella, fellas, females, feminism, fiance, fiancee, fillies, filly, fraternal, fraternities, fraternity, gal, gals, gelding, gentlemen, girlfriend, girlfriends, girls, goddess, godfather, granddaughter, granddaughters, grandma,grandmothers, grandpa, grandson, grandsons, handyman, heiress, hen, hens, her, heroine, hers, herself, him, himself, his, horsemen, hostess, housewife, housewives, hubby, husband, husbands, kings, lad, ladies, lads, lesbian, lesbians, lion, lions, ma, macho, maid, maiden, maids, males, mama, mare, maternal, maternity, men\", menopause, mistress, mom, mommy, moms, monastery, monk, monks, motherhood, mothers, nephew, nephews, niece, nieces, nun, nuns, obstetrics, pa, paternity, penis, prince, princes, princess, prostate, queens, salesman, salesmen, schoolboy, schoolgirl, semen, she, sir, sister, sisters, son, sons, sorority, sperm, spokesman, spokesmen, spokeswoman, stallion, statesman, stepdaughter, stepfather, stepmother, stepson, strongman, stud, studs, suitor, suitors, testosterone, uncle, uncles, uterus, vagina, viagra, waitress, widow, widower, widows, wife, witch, witches, wives, womb, women Equalized Words These male-vs-female words G are paired (one male, one female) and are equalized by the operation. Here is the filtered version of the list used in our experiments as per our description in \u00a75 about the test/train word list split. Each pair in this list is \"equalized\". monastery-convent spokesman-spokeswoman priest-nun Dad-Mom Men-Women councilman-councilwoman grandpa-grandma grandsons-granddaughters testosterone-estrogen uncle-aunt wives-husbands Father-Mother Grandpa-Grandma He-She boys-girls brother-sister brothers-sisters businessman-businesswoman chairman-chairwoman colt-filly congressman-congresswoman dad-mom dads-moms dudes-gals fatherhood-motherhood fathers-mothers fella-granny fraternity-sorority gelding-mare gentlemen-ladies grandson-granddaughter himself-herself his-her king-queen kings-queens males-females men-women nephew-niece prince-princess schoolboy-schoolgirl son-daughter sons-daughters More details about their word lists and code is available at: //github.com/tolga-b/debiaswe",
                "cite_spans": [
                    {
                        "start": 56,
                        "end": 79,
                        "text": "(Ravfogel et al., 2020)",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 243,
                        "end": 266,
                        "text": "(Ravfogel et al., 2020)",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 1045,
                        "end": 2710,
                        "text": "split. actress, actresses, aunt, aunts, bachelor, ballerina, barbershop, baritone, beard, beards, beau, bloke, blokes, boy, boyfriend, boyfriends, boyhood, boys, brethren, bride, brides, brother, brotherhood, brothers, bull, bulls, businessman, businessmen, businesswoman, chairman, chairwoman, chap, colt, colts, congressman, congresswoman, convent, councilman, councilmen, councilwoman, countryman, countrymen, czar, dad, daddy, dads, daughter, daughters, deer, diva, dowry, dude, dudes, estrogen, fathered, fatherhood, fathers, fella, fellas, females, feminism, fiance, fiancee, fillies, filly, fraternal, fraternities, fraternity, gal, gals, gelding, gentlemen, girlfriend, girlfriends, girls, goddess, godfather, granddaughter, granddaughters, grandma,grandmothers, grandpa, grandson, grandsons, handyman, heiress, hen, hens, her, heroine, hers, herself, him, himself, his, horsemen, hostess, housewife, housewives, hubby, husband, husbands, kings, lad, ladies, lads, lesbian, lesbians, lion, lions, ma, macho, maid, maiden, maids, males, mama, mare, maternal, maternity, men\", menopause, mistress, mom, mommy, moms, monastery, monk, monks, motherhood, mothers, nephew, nephews, niece, nieces, nun, nuns, obstetrics, pa, paternity, penis, prince, princes, princess, prostate, queens, salesman, salesmen, schoolboy, schoolgirl, semen, she, sir, sister, sisters, son, sons, sorority, sperm, spokesman, spokesmen, spokeswoman, stallion, statesman, stepdaughter, stepfather, stepmother, stepson, strongman, stud, studs, suitor, suitors, testosterone, uncle, uncles, uterus, vagina, viagra, waitress, widow, widower, widows, wife, witch, witches, wives, womb, women",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 9,
                        "end": 11,
                        "text": "S5",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "C.4 TPR-Gap-RMS",
                "sec_num": null
            },
            {
                "text": "Gendered Word List (G) for INLP consists of 1425 words found under h t t p s : / / g i t h u b . c o m / S h a u l 1 3 2 1 / n u l l s p a c e _ p r o j e c t i o n / b l o b / m a s t e r / d a t a / l i s t s / as the list gender_specific_full.json. This list has been filtered of words used in generating templates (Supplement D.5 and for WEAT (Supplement D.6.More details about their word lists and code is available at: https://github.com/Shaul1321/n ullspace_projection.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "D.4 Word Lists for INLP",
                "sec_num": null
            },
            {
                "text": "We keep most word lists the same for template generation as used in the paper : For occupations, we change the list to remove those words that we use for subspace determination of occupations for OSCAR. This creates a test/train split for our experiments. We also modify the malevs-female word lists to create the word lists used in premise/hypothesis for the entail and contradict templates for SIRT.Male actuary, administrator, advisor, aide, ambassador, architect, artist, astronaut, astronomer, athlete, attendant, attorney, author, babysitter, baker, biologist, broker, builder, butcher, butler, captain, cardi-ologist, caregiver, carpenter, cashier, caterer, chauffeur, chef, chemist, clerk, coach, contractor, cook, cop, cryptographer, dentist, detective, dictator, director, driver, ecologist, economist, editor, educator, electrician, engineer, entrepreneur, executive, farmer, financier, firefighter, gardener, general, geneticist, geologist, golfer, governor, grocer, guard, hairdresser, housekeeper, hunter, inspector, instructor, intern, interpreter, inventor, investigator, janitor, jester, journalist, judge, laborer, landlord, lawyer, lecturer, librarian, lifeguard, linguist, lobbyist, magician, manager, manufacturer, marine, marketer, mason, mathematician, mayor, mechanic, messenger, miner, model, musician, novelist, official, operator, optician, painter, paralegal, pathologist, pediatrician, pharmacist, philosopher, photographer, physician, physicist, pianist, pilot, plumber, poet, politician, postmaster, president, principal, producer, professor, programmer, psychiatrist, psychologist, publisher, radiologist, receptionist, reporter, representative, researcher, retailer, sailor, salesperson, scholar, senator, sheriff, singer, soldier, spy, statistician, stockbroker, supervisor, surgeon, surveyor, tailor, teacher, technician, trader, translator, tutor, undertaker, valet, veterinarian, violinist, waiter, warden, warrior, watchmaker, writer, zookeeper, zoologist Objects apple, apron, armchair, auto, bagel, banana, bed, bench, beret, blender, blouse, bookshelf, breakfast, brownie, buffalo, burger, bus, cabinet, cake, calculator, calf, camera, cap, cape, car, cart, cat, chair, chicken, clock, coat, computer, costume, cot, couch, cow, cupboard, dinner, dog, donkey, donut, dress, dresser, duck, goat, headphones, heater, helmet, hen, horse, jacket, jeep, lamb, lamp, lantern, laptop, lunch, mango, meal, muffin, mule, oven, ox, pancake, peach, phone, pig, pizza, potato, printer, pudding, rabbit, radio, recliner, refrigerator, ring, roll, rug, salad, sandwich, shirt, Male: actor, bachelor, bridegroom, brother, count, czar, dad, daddy, duke, emperor, father, fiance, gentleman, giant, god, governor, grandfather, grandson, headmaster, heir, hero, host, hunter, husband, king, lad, landlord, lord, male, manager, manservant, masseur, master, mayor, milkman, millionaire, monitor, monk, mr, murderer, nephew, papa, poet, policeman, postman, postmaster, priest, prince, shepherd, sir, stepfather, stepson, steward, sultan, uncle, waiter, washerman, widower, wizard, Female: actress, spinster, bride, sister, countess, czarina, mum, mummy, duchess, empress, mother, fiancee, lady, giantess, goddess, matron, grandmother, granddaughter, headmistress, heiress, heroine, hostess, huntress, wife, queen, lass, landlady, lady, female, manageress, maidservant, masseuse, mistress, mayoress, milkmaid, millionairess, monitress, nun, mrs, murderess, niece, mama, poetess, policewoman, postwoman, postmistress, priestess, princess, shepherdess, madam, stepmother, stepdaughter, stewardess, sultana, aunt, waitress, washerwoman, widow, witchStatistically male-vs-female Names for WEAT* There are generated using data curated by social security in USA (https://www.ssa.gov/oact/babynames/). We take the top 1000 names in each male/female gender (as labeled at birth) which are also among the top 100K most frequent words in Wikipedia (to ensure robustly embedded words). Here we present them in lower case. Male: liam, noah, william, james, logan, benjamin, mason, elijah, oliver, jacob, lucas, michael, alexander, ethan, daniel, matthew, aiden, henry, joseph, jackson, samuel, sebastian, david, carter, wyatt, jayden, john, owen, dylan, luke, gabriel, anthony, isaac, grayson, jack, julian, levi, christopher, joshua, andrew, lincoln, mateo, ryan, nathan, aaron, isaiah, thomas, charles, caleb, josiah, christian, hunter, eli, jonathan, connor, landon, adrian, asher, cameron, leo, theodore, jeremiah, hudson, robert, easton, nolan, nicholas, ezra, colton, angel, jordan, dominic, austin, ian, adam, elias, jose, ezekiel, carson, evan, maverick, bryson, jace, cooper, xavier, parker, roman, jason, santiago, chase, sawyer, gavin, leonardo, jameson, kevin, bentley, zachary, everett, axel, tyler, micah, vincent, weston, miles, wesley, nathaniel, harrison, brandon, cole, declan, luis, braxton, damian, silas, tristan, ryder, bennett, george, emmett, justin, kai, max, diego, luca, carlos, maxwell, kingston, ivan, maddox, juan, ashton, rowan, giovanni, eric, jesus, calvin, abel, king, camden, amir, blake, alex, brody, malachi, emmanuel, jonah, beau, jude, antonio, alan, elliott, elliot, waylon, xander, timothy, victor, bryce, finn, brantley, edward, abraham, patrick, grant, hayden, richard, miguel, joel, gael, tucker, rhett, avery, steven, graham, jasper, jesse, matteo, dean, preston, august, oscar, jeremy, alejandro, marcus, dawson, lorenzo, messiah, zion, maximus, river, zane, mark, brooks, nicolas, paxton, judah, emiliano, bryan, kyle, myles, peter, charlie, kyrie, thiago, brian, kenneth, andres, lukas, aidan, jax, caden, milo, paul, beckett, brady, colin, omar, bradley, javier, knox, jaden, barrett, israel, matias, jorge, zander, derek, holden, griffin, arthur, leon, felix, remington, jake, killian, clayton, sean, riley, archer, legend, erick, enzo, corbin, francisco, dallas, emilio, gunner, simon, andre, walter, damien, chance, phoenix, colt, tanner, stephen, tobias, manuel, amari, emerson, louis, cody, finley, martin, rafael, nash, beckham, cash, reid, theo, ace, eduardo, spencer, raymond, maximiliano, anderson, ronan, lane, cristian, titus, travis, jett, ricardo, bodhi, gideon, fernando, mario, conor, keegan, ali, cesar, ellis, walker, cohen, arlo, hector, dante, garrett, donovan, seth, jeffrey, tyson, jase, desmond, gage, atlas, major, devin, edwin, angelo, orion, conner, julius, marco, jensen, peyton, zayn, collin, dakota, prince, johnny, cruz, hendrix, atticus, troy, kane, edgar, sergio, kash, mar-shall, johnathan, romeo, shane, warren, joaquin, wade, leonel, trevor, dominick, muhammad, erik, odin, quinn, dalton, nehemiah, frank, grady, gregory, andy, solomon, malik, rory, clark, reed, harvey, jay, jared, noel, shawn, fabian, ibrahim, adonis, ismael, pedro, leland, malcolm, alexis, porter, sullivan, raiden, allen, ari, russell, princeton, winston, kendrick, roberto, lennox, hayes, finnegan, nasir, kade, nico, emanuel, landen, moises, ruben, hugo, abram, adan, khalil, augustus, marcos, philip, phillip, cyrus, esteban, albert, bruce, lawson, jamison, sterling, damon, gunnar, luka, franklin, ezequiel, pablo, derrick, zachariah, cade, jonas, dexter, remy, hank, tate, trenton, kian, drew, mohamed, dax, rocco, bowen, mathias, ronald, francis, matthias, milan, maximilian, royce, skyler, corey, drake, gerardo, jayson, sage, benson, moses, rhys, otto, oakley, armando, jaime, nixon, saul, scott, ariel, enrique, donald, chandler, asa, eden, davis, keith, frederick, lawrence, leonidas, aden, julio, darius, johan, deacon, cason, danny, nikolai, taylor, alec, royal, armani, kieran, luciano, omari, rodrigo, arjun, ahmed, brendan, cullen, raul, raphael, ronin, brock, pierce, alonzo, casey, dillon, uriel, dustin, gianni, roland, kobe, dorian, emmitt, ryland, apollo, roy, duke, quentin, sam, lewis, tony, uriah, dennis, moshe, braden, quinton, cannon, mathew, niko, edison, jerry, gustavo, marvin, mauricio, ahmad, mohammad, justice, trey, mohammed, sincere, yusuf, arturo, callen, keaton, wilder, memphis, conrad, soren, colby, bryant, lucian, alfredo, cassius, marcelo, nikolas, brennan, darren, jimmy, lionel, reece, ty, chris, forrest, tatum, jalen, santino, case, leonard, alvin, issac, bo, quincy, mack, samson, rex, alberto, callum, curtis, hezekiah, briggs, zeke, neil, titan, julien, kellen, devon, roger, axton, carl, douglas, larry, crosby, fletcher, makai, nelson, hamza, lance, alden, gary, wilson, alessandro, ares, bruno, jakob, stetson, zain, cairo, nathanael, byron, harry, harley, mitchell, maurice, orlando, kingsley, trent, ramon, boston, lucca, noe, jagger, randy, thaddeus, lennon, kannon, kohen, valentino, salvador, langston, rohan, kristopher, yosef, lee, callan, tripp, deandre, joe, morgan, reese, ricky, bronson, terry, eddie, jefferson, lachlan, layne, clay, madden, tomas, kareem, stanley, amos, kase, kristian, clyde, ernesto, tommy, ford, crew, hassan, axl, boone, leandro, samir, magnus, abdullah, yousef, branson, layton, franco, ben, grey, kelvin, chaim, demetrius, blaine, ridge, colson, melvin, anakin, aryan, jon, canaan, dash, zechariah, alonso, otis, zaire, marcel, brett, stefan, aldo, jeffery, baylor, talon, dominik, flynn, carmelo, dane, jamal, kole, enoch, kye, vicente, fisher, ray, fox, jamie, rey, zaid, allan, emery, gannon, rodney, sonny, terrance, augustine, cory, felipe, aron, jacoby, harlan Female: emma, olivia, ava, isabella, sophia, mia, charlotte, amelia, evelyn, abigail, harper, emily, elizabeth, avery, sofia, ella, madison, scarlett, victoria, aria, grace, chloe, camila, penelope, riley, layla, lillian, nora, zoey, mila, aubrey, hannah, lily, addison, eleanor, natalie, luna, savannah, brooklyn, leah, zoe, stella, hazel, ellie, paisley, audrey, skylar, violet, claire, bella, aurora, lucy, anna, samantha, caroline, genesis, aaliyah, kennedy, kinsley, allison, maya, sarah, adeline, alexa, ariana, elena, gabriella, naomi, alice, sadie, hailey, eva, emilia, autumn, quinn, piper, ruby, serenity, willow, everly, cora, lydia, arianna, eliana, peyton, melanie, gianna, isabelle, julia, valentina, nova, clara, vivian, reagan, mackenzie, madeline, delilah, isla, katherine, sophie, josephine, ivy, liliana, jade, maria, taylor, hadley, kylie, emery, natalia, annabelle, faith, alexandra, ximena, ashley, brianna, bailey, mary, athena, andrea, leilani, jasmine, lyla, margaret, alyssa, arya, norah, kayla, eden, eliza, rose, ariel, melody, alexis, isabel, sydney, juliana, lauren, iris, emerson, london, morgan, lilly, charlie, aliyah, valeria, arabella, sara, finley, trinity, jocelyn, kimberly, esther, molly, valerie, cecilia, anastasia, daisy, reese, laila, mya, amy, amaya, elise, harmony, paige, fiona, alaina, nicole, genevieve, lucia, alina, mckenzie, callie, payton, eloise, brooke, mariah, julianna, rachel, daniela, gracie, catherine, angelina, presley, josie, harley, vanessa, parker, juliette, amara, marley, lila, ana, rowan, alana, michelle, malia, rebecca, summer, sloane, leila, sienna, adriana, sawyer, kendall, juliet, destiny, diana, hayden, ayla, dakota, angela, noelle, rosalie, joanna, lola, georgia, selena, june, tessa, maggie, jessica, remi, delaney, camille, vivienne, hope, mckenna, gemma, olive, alexandria, blakely, catalina, gabrielle, lucille, ruth, evangeline, blake, thea, amina, giselle, melissa, river, kate, adelaide, vera, leia, gabriela, zara, jane, journey, miriam, stephanie, cali, ember, logan, annie, mariana, kali, haven, elsie, paris, lena, freya, lyric, camilla, sage, jennifer, talia, alessandra, juniper, fatima, amira, arielle, phoebe, ada, nina, samara, cassidy, aspen, allie, keira, kaia, amanda, heaven, joy, lia, laura, lexi, haley, miranda, kaitlyn, daniella, felicity, jacqueline, evie, angel, danielle, ainsley, dy-lan, kiara, millie, jordan, maddison, alicia, maeve, margot, phoenix, heidi, alondra, lana, madeleine, kenzie, miracle, shelby, elle, adrianna, bianca, kira, veronica, gwendolyn, esmeralda, chelsea, alison, skyler, magnolia, daphne, jenna, kyla, harlow, annalise, dahlia, scarlet, luciana, kelsey, nadia, amber, gia, carmen, jimena, erin, christina, katie, ryan, viviana, alexia, anaya, serena, ophelia, regina, helen, remington, cadence, royalty, amari, kathryn, skye, jada, saylor, kendra, cheyenne, fernanda, sabrina, francesca, eve, mckinley, frances, sarai, carolina, tatum, lennon, raven, leslie, winter, abby, mabel, sierra, april, willa, carly, jolene, rosemary, selah, renata, lorelei, briana, celeste, wren, leighton, annabella, mira, oakley, malaysia, edith, maryam, hattie, bristol, demi, maia, sylvia, allyson, lilith, holly, meredith, nia, liana, megan, justice, bethany, alejandra, janelle, elisa, adelina, myra, blair, charley, virginia, kara, helena, sasha, julie, michaela, carter, matilda, henley, maisie, hallie, priscilla, marilyn, cecelia, danna, colette, elliott, cameron, celine, hanna, imani, angelica, kalani, alanna, lorelai, macy, karina, aisha, johanna, mallory, leona, mariam, karen, karla, beatrice, gloria, milani, savanna, rory, giuliana, lauryn, liberty, charli, jillian, anne, dallas, azalea, tiffany, shiloh, jazmine, esme, elaine, lilian, kyra, kora, octavia, irene, kelly, lacey, laurel, anika, dorothy, sutton, julieta, kimber, remy, cassandra, rebekah, collins, elliot, emmy, sloan, hayley, amalia, jemma, jamie, melina, leyla, wynter, alessia, monica, anya, antonella, ivory, greta, maren, alena, emory, cynthia, alia, angie, alma, crystal, aileen, siena, zelda, marie, pearl, reyna, mae, zahra, jessie, tiana, armani, lennox, lillie, jolie, laney, mara, joelle, rosa, bridget, liv, aurelia, clarissa, elyse, marissa, monroe, kori, elsa, rosie, amelie, eileen, poppy, royal, chaya, frida, bonnie, amora, stevie, tatiana, malaya, mina, reign, annika, linda, kenna, faye, reina, brittany, marina, astrid, briar, teresa, hadassah, guadalupe, rayna, chanel, lyra, noa, laylah, livia, ellen, meadow, ellis, milan, hunter, princess, nathalie, clementine, nola, simone, lina, marianna, martha, louisa, emmeline, kenley, belen, erika, lara, amani, ansley, salma, dulce, nala, natasha, mercy, penny, ariadne, deborah, elisabeth, zaria, hana, raina, lexie, thalia, annabel, christine, estella, adele, aya, estelle, landry, tori, perla, miah, angelique, romina, ari, jaycee, kai, louise, mavis, belle, lea, rivka, calliope, sky, jewel, paola, giovanna, isabela, azariah, dream, claudia, corinne, erica, milena, alyson, joyce, tinsley, whitney, carolyn, frankie, andi, judith, paula, amia, hadlee, rayne, cara, celia, opal, clare, gwen, veda, alisha, davina, rhea, noor, danica, kathleen, lindsey, maxine, paulina, nancy, raquel, zainab, chana, lisa, heavenly, patricia, india, paloma, ramona, sandra, abril, vienna, rosalyn, hadleigh, barbara, jana, brenda, casey, selene, adrienne, aliya, miley, bexley, joslyn, zion, breanna, melania, estrella, ingrid, jayden, kaya, dana, legacy, marjorie, courtney, holland ",
                "cite_spans": [
                    {
                        "start": 406,
                        "end": 2602,
                        "text": "actuary, administrator, advisor, aide, ambassador, architect, artist, astronaut, astronomer, athlete, attendant, attorney, author, babysitter, baker, biologist, broker, builder, butcher, butler, captain, cardi-ologist, caregiver, carpenter, cashier, caterer, chauffeur, chef, chemist, clerk, coach, contractor, cook, cop, cryptographer, dentist, detective, dictator, director, driver, ecologist, economist, editor, educator, electrician, engineer, entrepreneur, executive, farmer, financier, firefighter, gardener, general, geneticist, geologist, golfer, governor, grocer, guard, hairdresser, housekeeper, hunter, inspector, instructor, intern, interpreter, inventor, investigator, janitor, jester, journalist, judge, laborer, landlord, lawyer, lecturer, librarian, lifeguard, linguist, lobbyist, magician, manager, manufacturer, marine, marketer, mason, mathematician, mayor, mechanic, messenger, miner, model, musician, novelist, official, operator, optician, painter, paralegal, pathologist, pediatrician, pharmacist, philosopher, photographer, physician, physicist, pianist, pilot, plumber, poet, politician, postmaster, president, principal, producer, professor, programmer, psychiatrist, psychologist, publisher, radiologist, receptionist, reporter, representative, researcher, retailer, sailor, salesperson, scholar, senator, sheriff, singer, soldier, spy, statistician, stockbroker, supervisor, surgeon, surveyor, tailor, teacher, technician, trader, translator, tutor, undertaker, valet, veterinarian, violinist, waiter, warden, warrior, watchmaker, writer, zookeeper, zoologist Objects apple, apron, armchair, auto, bagel, banana, bed, bench, beret, blender, blouse, bookshelf, breakfast, brownie, buffalo, burger, bus, cabinet, cake, calculator, calf, camera, cap, cape, car, cart, cat, chair, chicken, clock, coat, computer, costume, cot, couch, cow, cupboard, dinner, dog, donkey, donut, dress, dresser, duck, goat, headphones, heater, helmet, hen, horse, jacket, jeep, lamb, lamp, lantern, laptop, lunch, mango, meal, muffin, mule, oven, ox, pancake, peach, phone, pig, pizza, potato, printer, pudding, rabbit, radio, recliner, refrigerator, ring, roll, rug, salad, sandwich, shirt,",
                        "ref_id": null
                    },
                    {
                        "start": 4028,
                        "end": 14941,
                        "text": "Male: liam, noah, william, james, logan, benjamin, mason, elijah, oliver, jacob, lucas, michael, alexander, ethan, daniel, matthew, aiden, henry, joseph, jackson, samuel, sebastian, david, carter, wyatt, jayden, john, owen, dylan, luke, gabriel, anthony, isaac, grayson, jack, julian, levi, christopher, joshua, andrew, lincoln, mateo, ryan, nathan, aaron, isaiah, thomas, charles, caleb, josiah, christian, hunter, eli, jonathan, connor, landon, adrian, asher, cameron, leo, theodore, jeremiah, hudson, robert, easton, nolan, nicholas, ezra, colton, angel, jordan, dominic, austin, ian, adam, elias, jose, ezekiel, carson, evan, maverick, bryson, jace, cooper, xavier, parker, roman, jason, santiago, chase, sawyer, gavin, leonardo, jameson, kevin, bentley, zachary, everett, axel, tyler, micah, vincent, weston, miles, wesley, nathaniel, harrison, brandon, cole, declan, luis, braxton, damian, silas, tristan, ryder, bennett, george, emmett, justin, kai, max, diego, luca, carlos, maxwell, kingston, ivan, maddox, juan, ashton, rowan, giovanni, eric, jesus, calvin, abel, king, camden, amir, blake, alex, brody, malachi, emmanuel, jonah, beau, jude, antonio, alan, elliott, elliot, waylon, xander, timothy, victor, bryce, finn, brantley, edward, abraham, patrick, grant, hayden, richard, miguel, joel, gael, tucker, rhett, avery, steven, graham, jasper, jesse, matteo, dean, preston, august, oscar, jeremy, alejandro, marcus, dawson, lorenzo, messiah, zion, maximus, river, zane, mark, brooks, nicolas, paxton, judah, emiliano, bryan, kyle, myles, peter, charlie, kyrie, thiago, brian, kenneth, andres, lukas, aidan, jax, caden, milo, paul, beckett, brady, colin, omar, bradley, javier, knox, jaden, barrett, israel, matias, jorge, zander, derek, holden, griffin, arthur, leon, felix, remington, jake, killian, clayton, sean, riley, archer, legend, erick, enzo, corbin, francisco, dallas, emilio, gunner, simon, andre, walter, damien, chance, phoenix, colt, tanner, stephen, tobias, manuel, amari, emerson, louis, cody, finley, martin, rafael, nash, beckham, cash, reid, theo, ace, eduardo, spencer, raymond, maximiliano, anderson, ronan, lane, cristian, titus, travis, jett, ricardo, bodhi, gideon, fernando, mario, conor, keegan, ali, cesar, ellis, walker, cohen, arlo, hector, dante, garrett, donovan, seth, jeffrey, tyson, jase, desmond, gage, atlas, major, devin, edwin, angelo, orion, conner, julius, marco, jensen, peyton, zayn, collin, dakota, prince, johnny, cruz, hendrix, atticus, troy, kane, edgar, sergio, kash, mar-shall, johnathan, romeo, shane, warren, joaquin, wade, leonel, trevor, dominick, muhammad, erik, odin, quinn, dalton, nehemiah, frank, grady, gregory, andy, solomon, malik, rory, clark, reed, harvey, jay, jared, noel, shawn, fabian, ibrahim, adonis, ismael, pedro, leland, malcolm, alexis, porter, sullivan, raiden, allen, ari, russell, princeton, winston, kendrick, roberto, lennox, hayes, finnegan, nasir, kade, nico, emanuel, landen, moises, ruben, hugo, abram, adan, khalil, augustus, marcos, philip, phillip, cyrus, esteban, albert, bruce, lawson, jamison, sterling, damon, gunnar, luka, franklin, ezequiel, pablo, derrick, zachariah, cade, jonas, dexter, remy, hank, tate, trenton, kian, drew, mohamed, dax, rocco, bowen, mathias, ronald, francis, matthias, milan, maximilian, royce, skyler, corey, drake, gerardo, jayson, sage, benson, moses, rhys, otto, oakley, armando, jaime, nixon, saul, scott, ariel, enrique, donald, chandler, asa, eden, davis, keith, frederick, lawrence, leonidas, aden, julio, darius, johan, deacon, cason, danny, nikolai, taylor, alec, royal, armani, kieran, luciano, omari, rodrigo, arjun, ahmed, brendan, cullen, raul, raphael, ronin, brock, pierce, alonzo, casey, dillon, uriel, dustin, gianni, roland, kobe, dorian, emmitt, ryland, apollo, roy, duke, quentin, sam, lewis, tony, uriah, dennis, moshe, braden, quinton, cannon, mathew, niko, edison, jerry, gustavo, marvin, mauricio, ahmad, mohammad, justice, trey, mohammed, sincere, yusuf, arturo, callen, keaton, wilder, memphis, conrad, soren, colby, bryant, lucian, alfredo, cassius, marcelo, nikolas, brennan, darren, jimmy, lionel, reece, ty, chris, forrest, tatum, jalen, santino, case, leonard, alvin, issac, bo, quincy, mack, samson, rex, alberto, callum, curtis, hezekiah, briggs, zeke, neil, titan, julien, kellen, devon, roger, axton, carl, douglas, larry, crosby, fletcher, makai, nelson, hamza, lance, alden, gary, wilson, alessandro, ares, bruno, jakob, stetson, zain, cairo, nathanael, byron, harry, harley, mitchell, maurice, orlando, kingsley, trent, ramon, boston, lucca, noe, jagger, randy, thaddeus, lennon, kannon, kohen, valentino, salvador, langston, rohan, kristopher, yosef, lee, callan, tripp, deandre, joe, morgan, reese, ricky, bronson, terry, eddie, jefferson, lachlan, layne, clay, madden, tomas, kareem, stanley, amos, kase, kristian, clyde, ernesto, tommy, ford, crew, hassan, axl, boone, leandro, samir, magnus, abdullah, yousef, branson, layton, franco, ben, grey, kelvin, chaim, demetrius, blaine, ridge, colson, melvin, anakin, aryan, jon, canaan, dash, zechariah, alonso, otis, zaire, marcel, brett, stefan, aldo, jeffery, baylor, talon, dominik, flynn, carmelo, dane, jamal, kole, enoch, kye, vicente, fisher, ray, fox, jamie, rey, zaid, allan, emery, gannon, rodney, sonny, terrance, augustine, cory, felipe, aron, jacoby, harlan Female: emma, olivia, ava, isabella, sophia, mia, charlotte, amelia, evelyn, abigail, harper, emily, elizabeth, avery, sofia, ella, madison, scarlett, victoria, aria, grace, chloe, camila, penelope, riley, layla, lillian, nora, zoey, mila, aubrey, hannah, lily, addison, eleanor, natalie, luna, savannah, brooklyn, leah, zoe, stella, hazel, ellie, paisley, audrey, skylar, violet, claire, bella, aurora, lucy, anna, samantha, caroline, genesis, aaliyah, kennedy, kinsley, allison, maya, sarah, adeline, alexa, ariana, elena, gabriella, naomi, alice, sadie, hailey, eva, emilia, autumn, quinn, piper, ruby, serenity, willow, everly, cora, lydia, arianna, eliana, peyton, melanie, gianna, isabelle, julia, valentina, nova, clara, vivian, reagan, mackenzie, madeline, delilah, isla, katherine, sophie, josephine, ivy, liliana, jade, maria, taylor, hadley, kylie, emery, natalia, annabelle, faith, alexandra, ximena, ashley, brianna, bailey, mary, athena, andrea, leilani, jasmine, lyla, margaret, alyssa, arya, norah, kayla, eden, eliza, rose, ariel, melody, alexis, isabel, sydney, juliana, lauren, iris, emerson, london, morgan, lilly, charlie, aliyah, valeria, arabella, sara, finley, trinity, jocelyn, kimberly, esther, molly, valerie, cecilia, anastasia, daisy, reese, laila, mya, amy, amaya, elise, harmony, paige, fiona, alaina, nicole, genevieve, lucia, alina, mckenzie, callie, payton, eloise, brooke, mariah, julianna, rachel, daniela, gracie, catherine, angelina, presley, josie, harley, vanessa, parker, juliette, amara, marley, lila, ana, rowan, alana, michelle, malia, rebecca, summer, sloane, leila, sienna, adriana, sawyer, kendall, juliet, destiny, diana, hayden, ayla, dakota, angela, noelle, rosalie, joanna, lola, georgia, selena, june, tessa, maggie, jessica, remi, delaney, camille, vivienne, hope, mckenna, gemma, olive, alexandria, blakely, catalina, gabrielle, lucille, ruth, evangeline, blake, thea, amina, giselle, melissa, river, kate, adelaide, vera, leia, gabriela, zara, jane, journey, miriam, stephanie, cali, ember, logan, annie, mariana, kali, haven, elsie, paris, lena, freya, lyric, camilla, sage, jennifer, talia, alessandra, juniper, fatima, amira, arielle, phoebe, ada, nina, samara, cassidy, aspen, allie, keira, kaia, amanda, heaven, joy, lia, laura, lexi, haley, miranda, kaitlyn, daniella, felicity, jacqueline, evie, angel, danielle, ainsley, dy-lan, kiara, millie, jordan, maddison, alicia, maeve, margot, phoenix, heidi, alondra, lana, madeleine, kenzie, miracle, shelby, elle, adrianna, bianca, kira, veronica, gwendolyn, esmeralda, chelsea, alison, skyler, magnolia, daphne, jenna, kyla, harlow, annalise, dahlia, scarlet, luciana, kelsey, nadia, amber, gia, carmen, jimena, erin, christina, katie, ryan, viviana, alexia, anaya, serena, ophelia, regina, helen, remington, cadence, royalty, amari, kathryn, skye, jada, saylor, kendra, cheyenne, fernanda, sabrina, francesca, eve, mckinley, frances, sarai, carolina, tatum, lennon, raven, leslie, winter, abby, mabel, sierra, april, willa, carly, jolene, rosemary, selah, renata, lorelei, briana, celeste, wren, leighton, annabella, mira, oakley, malaysia, edith, maryam, hattie, bristol, demi, maia, sylvia, allyson, lilith, holly, meredith, nia, liana, megan, justice, bethany, alejandra, janelle, elisa, adelina, myra, blair, charley, virginia, kara, helena, sasha, julie, michaela, carter, matilda, henley, maisie, hallie, priscilla, marilyn, cecelia, danna, colette, elliott, cameron, celine, hanna, imani, angelica, kalani, alanna, lorelai, macy, karina, aisha, johanna, mallory, leona, mariam, karen, karla, beatrice, gloria, milani, savanna, rory, giuliana, lauryn, liberty, charli, jillian, anne, dallas, azalea, tiffany, shiloh, jazmine, esme, elaine, lilian, kyra, kora, octavia, irene, kelly, lacey, laurel, anika, dorothy, sutton, julieta, kimber, remy, cassandra, rebekah, collins, elliot, emmy, sloan, hayley, amalia, jemma, jamie, melina, leyla, wynter, alessia, monica, anya, antonella, ivory, greta, maren, alena, emory, cynthia, alia, angie, alma, crystal, aileen, siena, zelda, marie, pearl, reyna, mae, zahra, jessie, tiana, armani, lennox, lillie, jolie, laney, mara, joelle, rosa, bridget, liv, aurelia, clarissa, elyse, marissa, monroe, kori, elsa, rosie, amelie, eileen, poppy, royal, chaya, frida, bonnie, amora, stevie, tatiana, malaya, mina, reign, annika, linda, kenna, faye, reina, brittany, marina, astrid, briar, teresa, hadassah, guadalupe, rayna, chanel, lyra, noa, laylah, livia, ellen, meadow, ellis, milan, hunter, princess, nathalie, clementine, nola, simone, lina, marianna, martha, louisa, emmeline, kenley, belen, erika, lara, amani, ansley, salma, dulce, nala, natasha, mercy, penny, ariadne, deborah, elisabeth, zaria, hana, raina, lexie, thalia, annabel, christine, estella, adele, aya, estelle, landry, tori, perla, miah, angelique, romina, ari, jaycee, kai, louise, mavis, belle, lea, rivka, calliope, sky, jewel, paola, giovanna, isabela, azariah, dream, claudia, corinne, erica, milena, alyson, joyce, tinsley, whitney, carolyn, frankie, andi, judith, paula, amia, hadlee, rayne, cara, celia, opal, clare, gwen, veda, alisha, davina, rhea, noor, danica, kathleen, lindsey, maxine, paulina, nancy, raquel, zainab, chana, lisa, heavenly, patricia, india, paloma, ramona, sandra, abril, vienna, rosalyn, hadleigh, barbara, jana, brenda, casey, selene, adrienne, aliya, miley, bexley, joslyn, zion, breanna, melania, estrella, ingrid, jayden, kaya, dana, legacy, marjorie, courtney, holland",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "D.5 Words Lists for Template Generation",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Fairness in Representation: Quantifying Stereotyping as a Representational Harm",
                "authors": [
                    {
                        "first": "Mohsen",
                        "middle": [],
                        "last": "Abbasi",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Sorelle",
                        "suffix": ""
                    },
                    {
                        "first": "Carlos",
                        "middle": [],
                        "last": "Friedler",
                        "suffix": ""
                    },
                    {
                        "first": "Suresh",
                        "middle": [],
                        "last": "Scheidegger",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Venkatasubramanian",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 2019 SIAM International Conference on Data Mining",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "abs/10.1137/1.9781611975673.90"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Mohsen Abbasi, Sorelle A Friedler, Carlos Scheideg- ger, and Suresh Venkatasubramanian. 2019. Fair- ness in Representation: Quantifying Stereotyping as a Representational Harm. In Proceedings of the 2019 SIAM International Conference on Data Min- ing.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Big data's disparate impact",
                "authors": [
                    {
                        "first": "Solon",
                        "middle": [],
                        "last": "Barocas",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [
                            "D"
                        ],
                        "last": "Selbst",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Calif. L. Rev",
                "volume": "104",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Solon Barocas and Andrew D Selbst. 2016. Big data's disparate impact. Calif. L. Rev., 104:671.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Sociolinguistically Driven Approaches for Just Natural Language Processing",
                "authors": [
                    {
                        "first": "Lin",
                        "middle": [],
                        "last": "Su",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Blodgett",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Su Lin Blodgett. 2021. Sociolinguistically Driven Approaches for Just Natural Language Processing. Ph.D. thesis, University of Massachusetts Amherst.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Language (technology) is power: A critical survey of \"bias\" in NLP",
                "authors": [
                    {
                        "first": "Lin",
                        "middle": [],
                        "last": "Su",
                        "suffix": ""
                    },
                    {
                        "first": "Solon",
                        "middle": [],
                        "last": "Blodgett",
                        "suffix": ""
                    },
                    {
                        "first": "Hal",
                        "middle": [],
                        "last": "Barocas",
                        "suffix": ""
                    },
                    {
                        "first": "Iii",
                        "middle": [],
                        "last": "Daum\u00e9",
                        "suffix": ""
                    },
                    {
                        "first": "Hanna",
                        "middle": [],
                        "last": "Wallach",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2020.acl-main.485"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Su Lin Blodgett, Solon Barocas, Hal Daum\u00e9 III, and Hanna Wallach. 2020. Language (technology) is power: A critical survey of \"bias\" in NLP. In Pro- ceedings of the 58th Annual Meeting of the Associa- tion for Computational Linguistics.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Man is to computer programmer as woman is to homemaker? debiasing word embeddings",
                "authors": [
                    {
                        "first": "K W",
                        "middle": [],
                        "last": "Bolukbasi",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "V",
                        "middle": [],
                        "last": "Zou",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Saligrama",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Kalai",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "ACM Transactions of Information Systems",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "T Bolukbasi, K W Chang, J Zou, V Saligrama, and A Kalai. 2016. Man is to computer programmer as woman is to homemaker? debiasing word embed- dings. In ACM Transactions of Information Systems.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "A large annotated corpus for learning natural language inference",
                "authors": [
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Samuel",
                        "suffix": ""
                    },
                    {
                        "first": "Gabor",
                        "middle": [],
                        "last": "Bowman",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [],
                        "last": "Angeli",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Potts",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "EMNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large anno- tated corpus for learning natural language inference. In EMNLP.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Semantics derived automatically from language corpora contain human-like biases",
                "authors": [
                    {
                        "first": "Aylin",
                        "middle": [],
                        "last": "Caliskan",
                        "suffix": ""
                    },
                    {
                        "first": "Joanna",
                        "middle": [
                            "J"
                        ],
                        "last": "Bryson",
                        "suffix": ""
                    },
                    {
                        "first": "Arvind",
                        "middle": [],
                        "last": "Narayanan",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Science",
                "volume": "356",
                "issue": "6334",
                "pages": "183--186",
                "other_ids": {
                    "DOI": [
                        "10.1126/science.aal4230"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Aylin Caliskan, Joanna J. Bryson, and Arvind Narayanan. 2017. Semantics derived automatically from language corpora contain human-like biases. Science, 356(6334):183-186.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Toward gender-inclusive coreference resolution",
                "authors": [
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Trista",
                        "middle": [],
                        "last": "Cao",
                        "suffix": ""
                    },
                    {
                        "first": "Hal",
                        "middle": [],
                        "last": "Daum\u00e9",
                        "suffix": ""
                    },
                    {
                        "first": "Iii",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2020.acl-main.418"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yang Trista Cao and Hal Daum\u00e9 III. 2020. Toward gender-inclusive coreference resolution. Proceed- ings of the 58th Annual Meeting of the Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "The Trouble with Bias",
                "authors": [],
                "year": 2017,
                "venue": "Conference on Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kate Crawford. 2017. The Trouble with Bias. In Con- ference on Neural Information Processing Systems, invited speaker.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Mapping the margins: Intersectionality, identity politics, and violence against women of color",
                "authors": [
                    {
                        "first": "Kimberle",
                        "middle": [],
                        "last": "Crenshaw",
                        "suffix": ""
                    }
                ],
                "year": 1991,
                "venue": "Stanford Law Review",
                "volume": "43",
                "issue": "6",
                "pages": "1241--1299",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kimberle Crenshaw. 1991. Mapping the margins: In- tersectionality, identity politics, and violence against women of color. Stanford Law Review, 43(6):1241- 1299.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Bias in bios: A case study of semantic representation bias in a high-stakes setting",
                "authors": [
                    {
                        "first": "Maria",
                        "middle": [],
                        "last": "De-Arteaga",
                        "suffix": ""
                    },
                    {
                        "first": "Alexey",
                        "middle": [],
                        "last": "Romanov",
                        "suffix": ""
                    },
                    {
                        "first": "Hanna",
                        "middle": [],
                        "last": "Wallach",
                        "suffix": ""
                    },
                    {
                        "first": "Jennifer",
                        "middle": [],
                        "last": "Chayes",
                        "suffix": ""
                    },
                    {
                        "first": "Christian",
                        "middle": [],
                        "last": "Borgs",
                        "suffix": ""
                    },
                    {
                        "first": "Alexandra",
                        "middle": [],
                        "last": "Chouldechova",
                        "suffix": ""
                    },
                    {
                        "first": "Sahin",
                        "middle": [],
                        "last": "Geyik",
                        "suffix": ""
                    },
                    {
                        "first": "Krishnaram",
                        "middle": [],
                        "last": "Kenthapadi",
                        "suffix": ""
                    },
                    {
                        "first": "Adam",
                        "middle": [],
                        "last": "Tauman",
                        "suffix": ""
                    },
                    {
                        "first": "Kalai",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the Conference on Fairness, Accountability, and Transparency",
                "volume": "",
                "issue": "",
                "pages": "120--128",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Maria De-Arteaga, Alexey Romanov, Hanna Wal- lach, Jennifer Chayes, Christian Borgs, Alexandra Chouldechova, Sahin Geyik, Krishnaram Kentha- padi, and Adam Tauman Kalai. 2019. Bias in bios: A case study of semantic representation bias in a high-stakes setting. In Proceedings of the Confer- ence on Fairness, Accountability, and Transparency, pages 120-128.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "On Measuring and Mitigating Biased Inferences of Word Embeddings",
                "authors": [
                    {
                        "first": "Sunipa",
                        "middle": [],
                        "last": "Dev",
                        "suffix": ""
                    },
                    {
                        "first": "Tao",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Jeff",
                        "middle": [
                            "M"
                        ],
                        "last": "Phillips",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Vivek",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sunipa Dev, Tao Li, Jeff M. Phillips, and Vivek Sriku- mar. 2020. On Measuring and Mitigating Biased In- ferences of Word Embeddings. AAAI.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Harms of gender exclusivity and challenges in non-binary representation in language technologies",
                "authors": [
                    {
                        "first": "Sunipa",
                        "middle": [],
                        "last": "Dev",
                        "suffix": ""
                    },
                    {
                        "first": "Masoud",
                        "middle": [],
                        "last": "Monajatipoor",
                        "suffix": ""
                    },
                    {
                        "first": "Anaelia",
                        "middle": [],
                        "last": "Ovalle",
                        "suffix": ""
                    },
                    {
                        "first": "Arjun",
                        "middle": [],
                        "last": "Subramonian",
                        "suffix": ""
                    },
                    {
                        "first": "Jeff",
                        "middle": [
                            "M"
                        ],
                        "last": "Phillips",
                        "suffix": ""
                    },
                    {
                        "first": "Kai-Wei",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sunipa Dev, Masoud Monajatipoor, Anaelia Ovalle, Arjun Subramonian, Jeff M Phillips, and Kai-Wei Chang. 2021. Harms of gender exclusivity and chal- lenges in non-binary representation in language tech- nologies.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Attenuating bias in word vectors",
                "authors": [
                    {
                        "first": "Sunipa",
                        "middle": [],
                        "last": "Dev",
                        "suffix": ""
                    },
                    {
                        "first": "Jeff",
                        "middle": [
                            "M"
                        ],
                        "last": "Phillips",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "AISTATS, Proceedings of Machine Learning Research",
                "volume": "",
                "issue": "",
                "pages": "879--887",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sunipa Dev and Jeff M. Phillips. 2019. Attenuat- ing bias in word vectors. In AISTATS, Proceed- ings of Machine Learning Research, pages 879-887. PMLR.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Placing search in context : The concept revisited",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Finkelstein",
                        "suffix": ""
                    },
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Gabrilovich",
                        "suffix": ""
                    },
                    {
                        "first": "E",
                        "middle": [],
                        "last": "Matias",
                        "suffix": ""
                    },
                    {
                        "first": "Z",
                        "middle": [],
                        "last": "Rivlin",
                        "suffix": ""
                    },
                    {
                        "first": "G",
                        "middle": [],
                        "last": "Solan",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Wolfman",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "In ACM Transactions of Information Systems",
                "volume": "20",
                "issue": "",
                "pages": "116--131",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "L Finkelstein, E Gabrilovich, Y Matias, E Rivlin, Z Solan, G Wolfman, and etal. 2002. Placing search in context : The concept revisited. In ACM Trans- actions of Information Systems, volume 20, pages 116-131.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Bias in computer systems",
                "authors": [
                    {
                        "first": "Batya",
                        "middle": [],
                        "last": "Friedman",
                        "suffix": ""
                    },
                    {
                        "first": "Helen",
                        "middle": [],
                        "last": "Nissenbaum",
                        "suffix": ""
                    }
                ],
                "year": 1996,
                "venue": "ACM Transactions on Information Systems (TOIS)",
                "volume": "14",
                "issue": "3",
                "pages": "330--347",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Batya Friedman and Helen Nissenbaum. 1996. Bias in computer systems. ACM Transactions on Informa- tion Systems (TOIS), 14(3):330-347.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Towards understanding gender bias in relation extraction",
                "authors": [
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Gaut",
                        "suffix": ""
                    },
                    {
                        "first": "Tony",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Shirlyn",
                        "middle": [],
                        "last": "Tang",
                        "suffix": ""
                    },
                    {
                        "first": "Yuxin",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Jing",
                        "middle": [],
                        "last": "Qian",
                        "suffix": ""
                    },
                    {
                        "first": "Mai",
                        "middle": [],
                        "last": "Elsherief",
                        "suffix": ""
                    },
                    {
                        "first": "Jieyu",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Diba",
                        "middle": [],
                        "last": "Mirza",
                        "suffix": ""
                    },
                    {
                        "first": "Elizabeth",
                        "middle": [
                            "M"
                        ],
                        "last": "Belding",
                        "suffix": ""
                    },
                    {
                        "first": "Kai-Wei",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "William",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Wang",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Andrew Gaut, Tony Sun, Shirlyn Tang, Yuxin Huang, Jing Qian, Mai ElSherief, Jieyu Zhao, Diba Mirza, Elizabeth M. Belding, Kai-Wei Chang, and William Yang Wang. 2019. Towards understand- ing gender bias in relation extraction. CoRR, abs/1911.03642.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them",
                "authors": [
                    {
                        "first": "Hila",
                        "middle": [],
                        "last": "Gonen",
                        "suffix": ""
                    },
                    {
                        "first": "Yoav",
                        "middle": [],
                        "last": "Goldberg",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of NAACL-HLT 2019",
                "volume": "",
                "issue": "",
                "pages": "609--614",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/N19-1061"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Hila Gonen and Yoav Goldberg. 2019. Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them. In Proceedings of NAACL-HLT 2019, pages 609- 614. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Gender as a Variable in Natural-Language Processing: Ethical Considerations",
                "authors": [
                    {
                        "first": "Brian",
                        "middle": [],
                        "last": "Larson",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the First ACL Workshop on Ethics in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/W17-1601"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Brian Larson. 2017. Gender as a Variable in Natural- Language Processing: Ethical Considerations. In Proceedings of the First ACL Workshop on Ethics in Natural Language Processing.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "A general framework for implicit and explicit debiasing of distributional word vector spaces",
                "authors": [
                    {
                        "first": "Anne",
                        "middle": [],
                        "last": "Lauscher",
                        "suffix": ""
                    },
                    {
                        "first": "Goran",
                        "middle": [],
                        "last": "Glavas",
                        "suffix": ""
                    },
                    {
                        "first": "Simone",
                        "middle": [],
                        "last": "Paolo Ponzetto",
                        "suffix": ""
                    },
                    {
                        "first": "Ivan",
                        "middle": [],
                        "last": "Vulic",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "AAAI",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Anne Lauscher, Goran Glavas, Simone Paolo Ponzetto, and Ivan Vulic. 2020. A general framework for im- plicit and explicit debiasing of distributional word vector spaces. In AAAI.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "2018. Mitigating Unwanted Biases with Adversarial Learning",
                "authors": [
                    {
                        "first": "Blake",
                        "middle": [],
                        "last": "Lemoine",
                        "suffix": ""
                    },
                    {
                        "first": "Brian",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Mitchell",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Blake Lemoine, Brian Zhang, and M. Mitchell, editors. 2018. Mitigating Unwanted Biases with Adversarial Learning.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Efficient estimation of word representations in vector space",
                "authors": [
                    {
                        "first": "Tomas",
                        "middle": [],
                        "last": "Mikolov",
                        "suffix": ""
                    },
                    {
                        "first": "Kai",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Greg",
                        "middle": [],
                        "last": "Corrado",
                        "suffix": ""
                    },
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Dean",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1301.3781"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Tomas Mikolov, Kai Chen, Greg Corrado, and Jef- frey Dean. 2013. Efficient estimation of word representations in vector space. Technical report, arXiv:1301.3781.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "A decomposable attention model for natural language inference",
                "authors": [
                    {
                        "first": "Ankur",
                        "middle": [],
                        "last": "Parikh",
                        "suffix": ""
                    },
                    {
                        "first": "Oscar",
                        "middle": [],
                        "last": "T\u00e4ckstr\u00f6m",
                        "suffix": ""
                    },
                    {
                        "first": "Dipanjan",
                        "middle": [],
                        "last": "Das",
                        "suffix": ""
                    },
                    {
                        "first": "Jakob",
                        "middle": [],
                        "last": "Uszkoreit",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "EMNLP",
                "volume": "",
                "issue": "",
                "pages": "2249--2255",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ankur Parikh, Oscar T\u00e4ckstr\u00f6m, Dipanjan Das, and Jakob Uszkoreit. 2016. A decomposable attention model for natural language inference. In EMNLP, pages 2249-2255.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Verb: Visualizing and interpreting bias mitigation techniques for word representations",
                "authors": [
                    {
                        "first": "Archit",
                        "middle": [],
                        "last": "Rathore",
                        "suffix": ""
                    },
                    {
                        "first": "Sunipa",
                        "middle": [],
                        "last": "Dev",
                        "suffix": ""
                    },
                    {
                        "first": "Jeff",
                        "middle": [
                            "M"
                        ],
                        "last": "Phillips",
                        "suffix": ""
                    },
                    {
                        "first": "Vivek",
                        "middle": [],
                        "last": "Srikumar",
                        "suffix": ""
                    },
                    {
                        "first": "Yan",
                        "middle": [],
                        "last": "Zheng",
                        "suffix": ""
                    },
                    {
                        "first": "Chin-Chia",
                        "middle": [],
                        "last": "Michael Yeh",
                        "suffix": ""
                    },
                    {
                        "first": "Junpeng",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Bei",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Archit Rathore, Sunipa Dev, Jeff M. Phillips, Vivek Srikumar, Yan Zheng, Chin-Chia Michael Yeh, Jun- peng Wang, Wei Zhang, and Bei Wang. 2021. Verb: Visualizing and interpreting bias mitigation tech- niques for word representations.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Null it out: Guarding protected attributes by iterative nullspace projection",
                "authors": [
                    {
                        "first": "Shauli",
                        "middle": [],
                        "last": "Ravfogel",
                        "suffix": ""
                    },
                    {
                        "first": "Yanai",
                        "middle": [],
                        "last": "Elazar",
                        "suffix": ""
                    },
                    {
                        "first": "Hila",
                        "middle": [],
                        "last": "Gonen",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Twiton",
                        "suffix": ""
                    },
                    {
                        "first": "Yoav",
                        "middle": [],
                        "last": "Goldberg",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Shauli Ravfogel, Yanai Elazar, Hila Gonen, Michael Twiton, and Yoav Goldberg. 2020. Null it out: Guarding protected attributes by iterative nullspace projection.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Gender bias in coreference resolution",
                "authors": [
                    {
                        "first": "Rachel",
                        "middle": [],
                        "last": "Rudinger",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Naradowsky",
                        "suffix": ""
                    },
                    {
                        "first": "Brian",
                        "middle": [],
                        "last": "Leonard",
                        "suffix": ""
                    },
                    {
                        "first": "Benjamin",
                        "middle": [],
                        "last": "Van Durme",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "NAACL",
                "volume": "",
                "issue": "",
                "pages": "8--14",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Rachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van Durme. 2018. Gender bias in coreference resolution. In NAACL, pages 8-14.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Mitigating gender bias in natural language processing: Literature review",
                "authors": [
                    {
                        "first": "Tony",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Gaut",
                        "suffix": ""
                    },
                    {
                        "first": "Shirlyn",
                        "middle": [],
                        "last": "Tang",
                        "suffix": ""
                    },
                    {
                        "first": "Yuxin",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Mai",
                        "middle": [],
                        "last": "Elsherief",
                        "suffix": ""
                    },
                    {
                        "first": "Jieyu",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Diba",
                        "middle": [],
                        "last": "Mirza",
                        "suffix": ""
                    },
                    {
                        "first": "Elizabeth",
                        "middle": [
                            "M"
                        ],
                        "last": "Belding",
                        "suffix": ""
                    },
                    {
                        "first": "Kai-Wei",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "William",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Wang",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tony Sun, Andrew Gaut, Shirlyn Tang, Yuxin Huang, Mai ElSherief, Jieyu Zhao, Diba Mirza, Eliza- beth M. Belding, Kai-Wei Chang, and William Yang Wang. 2019. Mitigating gender bias in natural language processing: Literature review. CoRR, abs/1906.08976.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Mind the GAP: A balanced corpus of gendered ambiguous pronouns",
                "authors": [
                    {
                        "first": "Kellie",
                        "middle": [],
                        "last": "Webster",
                        "suffix": ""
                    },
                    {
                        "first": "Marta",
                        "middle": [],
                        "last": "Recasens",
                        "suffix": ""
                    },
                    {
                        "first": "Vera",
                        "middle": [],
                        "last": "Axelrod",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Baldridge",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "TACL",
                "volume": "6",
                "issue": "",
                "pages": "605--617",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kellie Webster, Marta Recasens, Vera Axelrod, and Ja- son Baldridge. 2018. Mind the GAP: A balanced corpus of gendered ambiguous pronouns. TACL, 6:605-617.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "A broad-coverage challenge corpus for sentence understanding through inference",
                "authors": [
                    {
                        "first": "Adina",
                        "middle": [],
                        "last": "Williams",
                        "suffix": ""
                    },
                    {
                        "first": "Nikita",
                        "middle": [],
                        "last": "Nangia",
                        "suffix": ""
                    },
                    {
                        "first": "Samuel",
                        "middle": [],
                        "last": "Bowman",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "volume": "1",
                "issue": "",
                "pages": "1112--1122",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sen- tence understanding through inference. In Proceed- ings of the 2018 Conference of the North American Chapter of the Association for Computational Lin- guistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112-1122. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Transformers: State-of-theart natural language processing",
                "authors": [
                    {
                        "first": "Thomas",
                        "middle": [],
                        "last": "Wolf",
                        "suffix": ""
                    },
                    {
                        "first": "Lysandre",
                        "middle": [],
                        "last": "Debut",
                        "suffix": ""
                    },
                    {
                        "first": "Victor",
                        "middle": [],
                        "last": "Sanh",
                        "suffix": ""
                    },
                    {
                        "first": "Julien",
                        "middle": [],
                        "last": "Chaumond",
                        "suffix": ""
                    },
                    {
                        "first": "Clement",
                        "middle": [],
                        "last": "Delangue",
                        "suffix": ""
                    },
                    {
                        "first": "Anthony",
                        "middle": [],
                        "last": "Moi",
                        "suffix": ""
                    },
                    {
                        "first": "Pierric",
                        "middle": [],
                        "last": "Cistac",
                        "suffix": ""
                    },
                    {
                        "first": "Tim",
                        "middle": [],
                        "last": "Rault",
                        "suffix": ""
                    },
                    {
                        "first": "R\u00e9mi",
                        "middle": [],
                        "last": "Louf",
                        "suffix": ""
                    },
                    {
                        "first": "Morgan",
                        "middle": [],
                        "last": "Funtowicz",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1910.03771"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier- ric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Fun- towicz, et al. 2019. Transformers: State-of-the- art natural language processing. arXiv preprint arXiv:1910.03771.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "Gender bias in contextualized word embeddings",
                "authors": [
                    {
                        "first": "Jieyu",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Tianlu",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Yatskar",
                        "suffix": ""
                    },
                    {
                        "first": "Ryan",
                        "middle": [],
                        "last": "Cotterell",
                        "suffix": ""
                    },
                    {
                        "first": "Vicente",
                        "middle": [],
                        "last": "Ordonez",
                        "suffix": ""
                    },
                    {
                        "first": "Kai-Wei",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "volume": "1",
                "issue": "",
                "pages": "629--634",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/N19-1064"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jieyu Zhao, Tianlu Wang, Mark Yatskar, Ryan Cot- terell, Vicente Ordonez, and Kai-Wei Chang. 2019. Gender bias in contextualized word embeddings. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 629-634, Minneapolis, Minnesota. Association for Computa- tional Linguistics.",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "Men also like shopping: Reducing gender bias amplification using corpus-level constraints",
                "authors": [
                    {
                        "first": "Jieyu",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Tianlu",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Yatskar",
                        "suffix": ""
                    },
                    {
                        "first": "Vicente",
                        "middle": [],
                        "last": "Ordonez",
                        "suffix": ""
                    },
                    {
                        "first": "Kai-Wei",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "2979--2989",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/D17-1323"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or- donez, and Kai-Wei Chang. 2017. Men also like shopping: Reducing gender bias amplification using corpus-level constraints. In Proceedings of the 2017 Conference on Empirical Methods in Natural Lan- guage Processing, pages 2979-2989, Copenhagen, Denmark. Association for Computational Linguis- tics.",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "Learning gender-neutral word embeddings",
                "authors": [
                    {
                        "first": "Jieyu",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Yichao",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Zeyu",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Kai-Wei",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of EMNLP 2018",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/D18-1521"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jieyu Zhao, Yichao Zhou, Zeyu Li, Wei Wang, and Kai- Wei Chang. 2018. Learning gender-neutral word embeddings. In Proceedings of EMNLP 2018.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "A, B) = \u00b5 a\u2208A cos(a, w)-\u00b5 b\u2208B cos(b, w)",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "r O N S I P z L f r m 1 E Y G P p o P P D G n 2 x u t D S 6 q C d A P e S 0 o q e I i c a 4 q G + b 5 D K Y H X F a 2 3 d q o s U u P l u Z a D / P Y e c u p O d X T m E / + D + u W J u 3 0 L B e y N C B Y h S e E U G q m u D Q z r e p y j c j N w Y o 8 p y",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "Figure 1: Illustration of OSCaR operation f (blue) in the occupation\u00d7male-vs-female subspace S (the span of v 1 , v 2 ).",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table><tr><td/><td>Method</td><td colspan=\"4\">WEAT (1) WEAT (2) WEAT (3) ECT</td></tr><tr><td/><td>Baseline</td><td>1.768</td><td>0.535</td><td>0.788</td><td>0.778</td></tr><tr><td>GloVe</td><td>LP HD INLP</td><td>0.618 0.241 0.495</td><td>0.168 0.157 0.117</td><td>0.282 0.273 0.192</td><td>0.982 0.942 0.844</td></tr><tr><td/><td>OSCAR</td><td>0.235</td><td>0.170</td><td>0.307</td><td>0.980</td></tr><tr><td/><td/><td/><td/><td/><td>Male-vs-</td></tr><tr><td colspan=\"6\">female bias contained by embeddings. There are 3</td></tr><tr><td colspan=\"6\">WEAT tests (a score closer to 0 is better) results in this</td></tr><tr><td colspan=\"6\">table : (1) with stereotypically male/female occupation</td></tr><tr><td colspan=\"6\">words, (2) with work versus home related words, and</td></tr></table>",
                "type_str": "table",
                "text": "Intrinsic measurement of bias.",
                "html": null,
                "num": null
            },
            "TABREF1": {
                "content": "<table><tr><td/><td>Method</td><td colspan=\"4\">N. Neu F. Neu Dev F1 Test F1</td></tr><tr><td/><td>Baseline</td><td>32.1</td><td>29.6</td><td>87.9</td><td>87.3</td></tr><tr><td>GloVe</td><td>LP HD INLP</td><td>38.2 34.7 49.9</td><td>39.7 32.7 53.9</td><td>87.9 83.4 86.4</td><td>87.1 83.3 85.9</td></tr><tr><td/><td>OSCAR</td><td>40.0</td><td>41.4</td><td>87.2</td><td>86.9</td></tr><tr><td>RoBERTa</td><td>Baseline LP HD INLP OSCAR</td><td>34.9 48.9 45.0 42.8 56.6</td><td>32.1 41.8 35.6 44.0 58.8</td><td>91.2 91.1 91.1 91.0 91.2</td><td>90.5 90.8 90.5 90.8 90.7</td></tr></table>",
                "type_str": "table",
                "text": "Table2shows scores from NLI models with GloVe/RoBERTa before and after they have been debiased by the various approaches. Most debiasing methods, including OSCAR, improve on the neutrality scores (recall sentences should be objectively neutral) with-out substantially sacrificing the F1 scores on the dev/test sets, except HD on GloVe. INLP appears to be the most successful at debiasing GloVe with multiple steps of projection. OSCAR is always the second best method in each measure. On RoBERTa, OSCAR is significantly more successful at debiasing as compared to the other methods. Extrinsic measurement of bias. Malevs-female bias expressed downstream by GloVE and RoBERTa embeddings using NLI as a probe. N: Net. F: Fraction. Higher neutrality scores imply less biases. OSCAR performs as good as LP on GloVe, and the best on RoBERTa.",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table><tr><td>Larger scores indicate more</td></tr><tr><td>valid associations expressed. The test words in</td></tr><tr><td>columns (1), (2) and (3) respectively, are he and she,</td></tr><tr><td>gendered words, and statistically male and female</td></tr><tr><td>names.</td></tr></table>",
                "type_str": "table",
                "text": "Intrinsic measurement of information retained using WEAT * .",
                "html": null,
                "num": null
            },
            "TABREF4": {
                "content": "<table><tr><td/><td>Method</td><td colspan=\"4\">N. Ent F. Ent N. Con F. Con</td></tr><tr><td/><td>Baseline</td><td>89.5</td><td>96.7</td><td>84.0</td><td>88.8</td></tr><tr><td>GloVe</td><td>LP HD INLP</td><td>81.0 66.8 74.8</td><td>86.5 89.1 79.3</td><td>71.5 54.4 62.4</td><td>71.3 75.2 63.4</td></tr><tr><td/><td>OSCAR</td><td>84.5</td><td>91.1</td><td>74.7</td><td>75.9</td></tr><tr><td>RoBERTa</td><td>Baseline LP HD INLP OSCAR</td><td>94.9 95.9 95.1 92.8 95.1</td><td>98.4 99.7 98.6 97.1 99.0</td><td>97.4 98.9 98.7 95.4 99.4</td><td>97.7 99.4 99.3 96.4 99.7</td></tr><tr><td colspan=\"6\">textual information, is more equipped at retaining</td></tr><tr><td colspan=\"6\">information after being debiased. LP, OSCAR,</td></tr><tr><td colspan=\"6\">and HD perform similar to the baseline. Only</td></tr><tr><td colspan=\"6\">INLP with multiple projections registers a notice-</td></tr><tr><td colspan=\"3\">able drop in SIRT score.</td><td/><td/><td/></tr></table>",
                "type_str": "table",
                "text": "Extrinsic measurement of information retained using SIRT. N: Net. F: Fraction. Higher scores indicate more valid information is retained. Baseline models perform among the best as expected.",
                "html": null,
                "num": null
            }
        }
    }
}