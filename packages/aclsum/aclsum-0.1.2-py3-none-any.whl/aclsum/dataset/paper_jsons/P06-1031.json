{
    "paper_id": "P06-1031",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T11:54:42.124907Z"
    },
    "title": "A Feedback-Augmented Method for Detecting Errors in the Writing of Learners of English",
    "authors": [
        {
            "first": "Ryo",
            "middle": [],
            "last": "Nagata",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Hyogo University of Teacher",
                "location": {
                    "postCode": "6731494",
                    "country": "Japan"
                }
            },
            "email": "rnagata@hyogo-u.ac.jp"
        },
        {
            "first": "Atsuo",
            "middle": [],
            "last": "Kawai",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Mie University",
                "location": {
                    "postCode": "5148507",
                    "country": "Japan"
                }
            },
            "email": ""
        },
        {
            "first": "Koichiro",
            "middle": [],
            "last": "Morihiro",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Hyogo University of Teacher",
                "location": {
                    "postCode": "6731494",
                    "country": "Japan"
                }
            },
            "email": ""
        },
        {
            "first": "Naoki",
            "middle": [],
            "last": "Isu",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Mie University",
                "location": {
                    "postCode": "5148507",
                    "country": "Japan"
                }
            },
            "email": ""
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "This paper proposes a method for detecting errors in article usage and singular plural usage based on the mass count distinction. First, it learns decision lists from training data generated automatically to distinguish mass and count nouns. Then, in order to improve its performance, it is augmented by feedback that is obtained from the writing of learners. Finally, it detects errors by applying rules to the mass count distinction. Experiments show that it achieves a recall of 0.71 and a precision of 0.72 and outperforms other methods used for comparison when augmented by feedback.",
    "pdf_parse": {
        "paper_id": "P06-1031",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "This paper proposes a method for detecting errors in article usage and singular plural usage based on the mass count distinction. First, it learns decision lists from training data generated automatically to distinguish mass and count nouns. Then, in order to improve its performance, it is augmented by feedback that is obtained from the writing of learners. Finally, it detects errors by applying rules to the mass count distinction. Experiments show that it achieves a recall of 0.71 and a precision of 0.72 and outperforms other methods used for comparison when augmented by feedback.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Although several researchers (Kawai et al., 1984; McCoy et al., 1996; Schneider and McCoy, 1998; Tschichold et al., 1997) have shown that rulebased methods are effective to detecting grammatical errors in the writing of learners of English, it has been pointed out that it is hard to write rules for detecting errors concerning the articles and singular plural usage. To be precise, it is hard to write rules for distinguishing mass and count nouns which are particularly important in detecting these errors (Kawai et al., 1984) . The major reason for this is that whether a noun is a mass noun or a count noun greatly depends on its meaning or its surrounding context (refer to Allan (1980) and Bond (2005) for details of the mass count distinction).",
                "cite_spans": [
                    {
                        "start": 29,
                        "end": 49,
                        "text": "(Kawai et al., 1984;",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 50,
                        "end": 69,
                        "text": "McCoy et al., 1996;",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 70,
                        "end": 96,
                        "text": "Schneider and McCoy, 1998;",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 97,
                        "end": 121,
                        "text": "Tschichold et al., 1997)",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 508,
                        "end": 528,
                        "text": "(Kawai et al., 1984)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 679,
                        "end": 691,
                        "text": "Allan (1980)",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 696,
                        "end": 707,
                        "text": "Bond (2005)",
                        "ref_id": "BIBREF1"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The above errors are very common among Japanese learners of English (Kawai et al., 1984; Izumi et al., 2003) . This is perhaps because the Japanese language does not have a mass count distinction system similar to that of English. Thus, it is favorable for error detection systems aiming at Japanese learners to be capable of detecting these errors. In other words, such systems need to somehow distinguish mass and count nouns.",
                "cite_spans": [
                    {
                        "start": 68,
                        "end": 88,
                        "text": "(Kawai et al., 1984;",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 89,
                        "end": 108,
                        "text": "Izumi et al., 2003)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "This paper proposes a method for distinguishing mass and count nouns in context to complement the conventional rules for detecting grammatical errors. In this method, first, training data, which consist of instances of mass and count nouns, are automatically generated from a corpus. Then, decision lists for distinguishing mass and count nouns are learned from the training data. Finally, the decision lists are used with the conventional rules to detect the target errors.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The proposed method requires a corpus to learn decision lists for distinguishing mass and count nouns. General corpora such as newspaper articles can be used for the purpose. However, a drawback to it is that there are differences in character between general corpora and the writing of non-native learners of English (Granger, 1998; Chodorow and Leacock, 2000) . For instance, Chodorow and Leacock (2000) point out that the word concentrate is usually used as a noun in a general corpus whereas it is a verb 91% of the time in essays written by non-native learners of English. Consequently, the differences affect the performance of the proposed method.",
                "cite_spans": [
                    {
                        "start": 318,
                        "end": 333,
                        "text": "(Granger, 1998;",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 334,
                        "end": 361,
                        "text": "Chodorow and Leacock, 2000)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 378,
                        "end": 405,
                        "text": "Chodorow and Leacock (2000)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In order to reduce the drawback, the proposed method is augmented by feedback; it takes as feedback learners' essays whose errors are corrected by a teacher of English (hereafter, referred to as the feedback corpus). In essence, the feedback corpus could be added to a general corpus to generate training data. Or, ideally training data could be generated only from the feedback corpus just as from a general corpus. However, this causes a serious problem in practice since the size of the feedback corpus is normally far smaller than that of a general corpus. To make it practical, this paper discusses the problem and explores its solution.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The rest of this paper is structured as follows. Section 2 describes the method for detecting the target errors based on the mass count distinction. Section 3 explains how the method is augmented by feedback. Section 4 discusses experiments conducted to evaluate the proposed method.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "First, instances of the target noun that head their noun phrase (NP) are collected from a corpus with their surrounding words. This can be simply done by an existing chunker or parser.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generating training data",
                "sec_num": "2.1"
            },
            {
                "text": "Then, the collected instances are tagged with mass or count by the following tagging rules. For example, the underlined chicken:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generating training data",
                "sec_num": "2.1"
            },
            {
                "text": "... are a lot of chickens in the roost ... is tagged as ... are a lot of chickens/count in the roost ... because it is in plural form.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generating training data",
                "sec_num": "2.1"
            },
            {
                "text": "We have made tagging rules based on linguistic knowledge (Huddleston and Pullum, 2002) . Figure 1 and Table 1 represent the tagging rules. Figure 1 shows the framework of the tagging rules. Each node in Figure 1 represents a question applied to the instance in question. For example, the root node reads \"Is the instance in question plural?\". Each leaf represents a result of the classification. For example, if the answer is yes at the root node, the instance in question is tagged with count. Otherwise, the question at the lower node is applied and so on. The tagging rules do not classify instances as mass or count in some cases. These unclassified instances are tagged with the symbol \"?\". Unfortunately, they cannot readily be included in training data. For simplicity of implementation, they are excluded from training data 1 .",
                "cite_spans": [
                    {
                        "start": 57,
                        "end": 86,
                        "text": "(Huddleston and Pullum, 2002)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 96,
                        "end": 97,
                        "text": "1",
                        "ref_id": null
                    },
                    {
                        "start": 108,
                        "end": 109,
                        "text": "1",
                        "ref_id": null
                    },
                    {
                        "start": 146,
                        "end": 147,
                        "text": "1",
                        "ref_id": null
                    },
                    {
                        "start": 210,
                        "end": 211,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Generating training data",
                "sec_num": "2.1"
            },
            {
                "text": "Note that the tagging rules can be used only for generating training data. They cannot be used to distinguish mass and count nouns in the writing of learners of English for the purpose of detecting 1 According to experiments we have conducted, approximately 30% of instances are tagged with \"?\" on average. It is highly possible that performance of the proposed method will improve if these instances are included in the training data.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generating training data",
                "sec_num": "2.1"
            },
            {
                "text": "the target errors since they are based on the articles and the distinction between singular and plural.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generating training data",
                "sec_num": "2.1"
            },
            {
                "text": "Finally, the tagged instances are stored in a file with their surrounding words. Each line of it consists of one of the tagged instances and its surrounding words as in the above chicken example.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generating training data",
                "sec_num": "2.1"
            },
            {
                "text": "In the proposed method, decision lists are used for distinguishing mass and count nouns. One of the reasons for the use of decision lists is that they have been shown to be effective to the word sense disambiguation task and the mass count distinction is highly related to word sense as we will see in this section. Another reason is that rules for distinguishing mass and count nouns are observable in decision lists, which helps understand and improve the proposed method.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Decision Lists",
                "sec_num": "2.2"
            },
            {
                "text": "A decision list consists of a set of rules. Each rule matches the template as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Decision Lists",
                "sec_num": "2.2"
            },
            {
                "text": "If a condition is true, then a decision",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Decision Lists",
                "sec_num": "2.2"
            },
            {
                "text": "(1)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Decision Lists",
                "sec_num": "2.2"
            },
            {
                "text": "To define the template in the proposed method, let us have a look at the following two examples:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Decision Lists",
                "sec_num": "2.2"
            },
            {
                "text": "1. I read the paper.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Decision Lists",
                "sec_num": "2.2"
            },
            {
                "text": "2. The paper is made of hemp pulp.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Decision Lists",
                "sec_num": "2.2"
            },
            {
                "text": "The underlined papers in both sentences cannot simply be classified as mass or count by the tagging rules presented in Section 2.1 because both are singular and modified by the definite article. Nevertheless, we can tell that the former is a count noun and the latter is a mass noun from the contexts. This suggests that the mass count distinction is often determined by words surrounding the target noun. In example 1, we can tell that the paper refers to something that can be read such as a newspaper or a scientific paper from read, and therefore it is a count noun. Likewise, in example 2, we can tell that the paper refers to a certain substance from made and pulp, and therefore it is a mass noun.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Decision Lists",
                "sec_num": "2.2"
            },
            {
                "text": "Taking this observation into account, we define the template based on words surrounding the target noun. To formalize the template, we will use a random variable \u00a1 \u00a3\u00a2 that takes either \u00a4 \u00a6\u00a5 \u00a8 \u00a7 \u00a9 \u00a7 or \u00a9 to denote that the target noun is a mass noun or a count noun, respectively. We will also use and \u00a2 to denote a word and a certain context around the target noun, respectively. We define three types of \u00a2 : \u00a8\" , # %$ , and & '$ that denote the contexts consisting of the noun phrase that the target noun heads, $ words to the left of the noun phrase, and $ words to its right, respectively. Then the template is formalized by: If word appears in context \u00a2 of the target noun, then it is distinguished as \u00a1 \u00a3\u00a2",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Decision Lists",
                "sec_num": "2.2"
            },
            {
                "text": "Hereafter, to keep the notation simple, it will be abbreviated to",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Decision Lists",
                "sec_num": "2.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": ")( 10 \u00a1 \u00a3\u00a2",
                        "eq_num": "(2)"
                    }
                ],
                "section": "Learning Decision Lists",
                "sec_num": "2.2"
            },
            {
                "text": "Now rules that match the template can be obtained from the training data. All we need to do is to collect words in \u00a2 from the training data.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Decision Lists",
                "sec_num": "2.2"
            },
            {
                "text": "Here, the words in Table 1 are excluded. Also, function words (except prepositions), cardinal and quasi-cardinal numerals, and the target noun are excluded. All words are reduced to their morphological stem and converted entirely to lower case when collected. For example, the following tagged instance: She ate fried chicken/mass for dinner.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 25,
                        "end": 26,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Learning Decision Lists",
                "sec_num": "2.2"
            },
            {
                "text": "would give a set of rules that match the template:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Decision Lists",
                "sec_num": "2.2"
            },
            {
                "text": "2 \u00a5 43 5 )0 \u00a4 \u00a6\u00a5 \u00a8 \u00a7 \u00a9 \u00a7 6 87 @9 BA DC 0 \u00a4 \u00a6\u00a5 \u00a8 \u00a7 \u00a9 \u00a7 6 7 FE 5 0 \u00a4 \u00a6\u00a5 \u00a8 \u00a7 F \u00a7 G IH P2 7 FE 5 0 \u00a4 \u00a6\u00a5 \u00a8 \u00a7 \u00a9 \u00a7",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Decision Lists",
                "sec_num": "2.2"
            },
            {
                "text": "for the target noun chicken when $ RQ TS .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Decision Lists",
                "sec_num": "2.2"
            },
            {
                "text": "In addition, a default rule is defined. It is based on the target noun itself and used when no other applicable rules are found in the decision list for the target noun. It is defined by The log-likelihood ratio (Yarowsky, 1995) decides in which order rules are applied to the target noun in novel context. It is defined by",
                "cite_spans": [
                    {
                        "start": 212,
                        "end": 228,
                        "text": "(Yarowsky, 1995)",
                        "ref_id": "BIBREF18"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Decision Lists",
                "sec_num": "2.2"
            },
            {
                "text": "2 W FX \" `Y \u00a1 V\u00a2 ba ( `c \" `Y \u00a1 V\u00a2 ba ( `c (4)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Decision Lists",
                "sec_num": "2.2"
            },
            {
                "text": "where \u00a1 V\u00a2 is the exclusive event of \u00a1 \u00a3\u00a2 and \" dY \u00a1 \u00a3\u00a2 ea ( Uc is the probability that the target noun is used as \u00a1 V\u00a2 when appears in the context \u00a2 .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Decision Lists",
                "sec_num": "2.2"
            },
            {
                "text": "It is important to exercise some care in estimating \" dY \u00a1 V\u00a2 ba )( c . In principle, we could simply count the number of times that appears in the context \u00a2 of the target noun used as \u00a1 \u00a3\u00a2 in the training data. However, this estimate can be unreliable, when does not appear often in the con- text. To solve this problem, using a smoothing parameter r (Yarowsky, 1996), \" `Y \u00a1 V\u00a2 ba ( Uc is esti-",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Decision Lists",
                "sec_num": "2.2"
            },
            {
                "text": "mated by 3 \" `Y \u00a1 V\u00a2 ba ( `c Q 6 Y s )( ut \u00a1 \u00a3\u00a2 c & vr 6 Y s ( `c & w\u00a4 pr (5)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Decision Lists",
                "sec_num": "2.2"
            },
            {
                "text": "where 6 Y s ( Uc and 6 Y s ( t \u00a1 \u00a3\u00a2 c are occurrences of appearing in \u00a2 and those in \u00a2 of the target noun used as \u00a1 V\u00a2 , respectively. The constant \u00a4 is the number of possible classes, that is, \u00a4 xQ \u00a3y (\u00a4 \u00a6\u00a5 \u00a7 \u00a9 \u00a7 or \u00a9 ) in our case, and introduced to satisfy \" dY \u00a1 \u00a3\u00a2 ea ( Uc & \" dY \u00a1 \u00a3\u00a2 ea ( c Q \u00a3 . In this paper, r is set to 1.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Decision Lists",
                "sec_num": "2.2"
            },
            {
                "text": "Rules in a decision list are sorted in descending order by the log-likelihood ratio. They are tested on the target noun in novel context in this order. Rules sorted below the default rule are discarded4 because they are never used as we will see in Section 2.3.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Decision Lists",
                "sec_num": "2.2"
            },
            {
                "text": "Table 2 shows part of a decision list for the target noun chicken that was learned from a subset of the BNC (British National Corpus) (Burnard, 1995) . Note that the rules are divided into two columns for the purpose of illustration in Table 2 ; in practice, they are merged into one. ",
                "cite_spans": [
                    {
                        "start": 134,
                        "end": 149,
                        "text": "(Burnard, 1995)",
                        "ref_id": "BIBREF2"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "2",
                        "ref_id": "TABREF1"
                    },
                    {
                        "start": 242,
                        "end": 243,
                        "text": "2",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Learning Decision Lists",
                "sec_num": "2.2"
            },
            {
                "text": "G IH \u00a7 \u00a9 3 5 1.23 \" H X A C 1.23 \u00a7 F$ H E 5 1.23 7 3 5 1.23 \u00a7 2 7 \u00a9 2 E 5 1.18 2 X X A DC 1.18 target noun: chicken, $ RQ S LLR (Log-Likelihood Ratio)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Decision Lists",
                "sec_num": "2.2"
            },
            {
                "text": "On one hand, we associate the words in the left half with food or cooking. On the other hand, we associate those in the right half with animals or birds. From this observation, we can say that chicken in the sense of an animal or a bird is a count noun but a mass noun when referring to food or cooking, which agrees with the knowledge presented in previous work (Ostler and Atkins, 1991) .",
                "cite_spans": [
                    {
                        "start": 363,
                        "end": 388,
                        "text": "(Ostler and Atkins, 1991)",
                        "ref_id": "BIBREF14"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Decision Lists",
                "sec_num": "2.2"
            },
            {
                "text": "To distinguish the target noun in novel context, each rule in the decision list is tested on it in the sorted order until the first applicable one is found. It is distinguished according to the first applicable one. Ties are broken by the rules below.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Distinguishing mass and count nouns",
                "sec_num": "2.3"
            },
            {
                "text": "It should be noted that rules sorted below the default rule are never used because the default rule is always applicable to the target noun. This is the reason why rules sorted below the default rule are discarded as mentioned in Section 2.2.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Distinguishing mass and count nouns",
                "sec_num": "2.3"
            },
            {
                "text": "The target errors are detected by the following three steps. Rules in each step are examined on each target noun in the target text.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Detecting the target errors",
                "sec_num": "2.4"
            },
            {
                "text": "In the first step, any mass noun in plural form is detected as an error5 . If an error is detected in this step, the rest of the steps are not applied.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Detecting the target errors",
                "sec_num": "2.4"
            },
            {
                "text": "In the second step, errors are detected by the rules described in Table 3 . The symbol \" \" in Ta- ble 3 denotes that the combination of the corresponding row and column is erroneous. For example, the fifth row denotes that singular and plural count nouns modified by much are erroneous. The symbol \"-\" denotes that no error can be detected by the table. If one of the rules in Table 3 is applied to the target noun, the third step is not applied.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 72,
                        "end": 73,
                        "text": "3",
                        "ref_id": "TABREF2"
                    },
                    {
                        "start": 383,
                        "end": 384,
                        "text": "3",
                        "ref_id": "TABREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Detecting the target errors",
                "sec_num": "2.4"
            },
            {
                "text": "In the third step, errors are detected by the rules described in Table 4 . The symbols \" \" and \"-\" are the same as in Table 3 .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 71,
                        "end": 72,
                        "text": "4",
                        "ref_id": "TABREF3"
                    },
                    {
                        "start": 124,
                        "end": 125,
                        "text": "3",
                        "ref_id": "TABREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Detecting the target errors",
                "sec_num": "2.4"
            },
            {
                "text": "In addition, the indefinite article that modifies other than the head noun is judged to be erroneous ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Detecting the target errors",
                "sec_num": "2.4"
            },
            {
                "text": "As mentioned in Section 1, the proposed method takes the feedback corpus6 as feedback to improve its performance. In essence, decision lists could be learned from a corpus consisting of a general corpus and the feedback corpus. However, since the size of the feedback corpus is normally far smaller than that of general corpora, so is the effect of the feedback corpus on \" dY \u00a1 \u00a3\u00a2 ea ( c . This means that the feedback corpus hardly has effect on the performance.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Feedback-augmented method",
                "sec_num": "3"
            },
            {
                "text": "Instead, \" dY \u00a1 \u00a3\u00a2 ea ( Uc can be estimated by in- terpolating the probabilities estimated from the feedback corpus and the general corpus according to confidences of their estimates. It is favorable that the interpolated probability approaches to the probability estimated from the feedback corpus as its confidence increases; the more confident its estimate is, the more effect it has on the interpolated probability. Here, confidence of ratio \"",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Feedback-augmented method",
                "sec_num": "3"
            },
            {
                "text": "is measured by the reciprocal of variance of the ratio (Tanaka, 1977) . Variance is calculated by",
                "cite_spans": [
                    {
                        "start": 55,
                        "end": 69,
                        "text": "(Tanaka, 1977)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Feedback-augmented method",
                "sec_num": "3"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "\" dY # \" c",
                        "eq_num": "(6)"
                    }
                ],
                "section": "Feedback-augmented method",
                "sec_num": "3"
            },
            {
                "text": "where denotes the number of samples used for calculating the ratio. Therefore, confidence of the estimate of the conditional probability used in the proposed method is measured by Q 6 Y s )( c \" dY \u00a1 V\u00a2 ba ( Uc Y # \" dY \u00a1 \u00a3\u00a2 ea ( c c (7) To formalize the interpolated probability, we will use the symbols \" , \" d , , and to de- note the conditional probabilities estimated from the feedback corpus and the general corpus, and their confidences, respectively. Then, the interpolated probability \" 8e is estimated by 7 \" e Q f \" & \u00a3g ih kj g ml Y n\" 8 # \" c t o p q \" 8 qt r p (8) In Equation ( 8), the effect of \" s on \" e becomes large as its confidence increases. It should also be noted that when its confidence exceeds that of \" , the general corpus is no longer used in the interpolated probability.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Feedback-augmented method",
                "sec_num": "3"
            },
            {
                "text": "A problem that arises in Equation ( 8) is that \" P hardly has effect on \" 8e when a much larger general corpus is used than the feedback corpus even if \" t is estimated with a sufficient confidence. For example, \" 8 estimated from 100 samples, which are a relatively large number for estimating a probability, hardly has effect on \" ue when \" is estimated from 10000 samples; roughly, \" s has a v w Bw ef- fect of \" on \" e . One way to prevent this is to limit the effect of to some extent. It can be realized by taking the log of D in Equation ( 8). That is, the interpolated probability is estimated by",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Feedback-augmented method",
                "sec_num": "3"
            },
            {
                "text": "\" e Q f \" & xg ih j y {z | g l Y n\" 8 # \" Fc t } ~o R B % \" 8 qt } ~r R B % q (9)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Feedback-augmented method",
                "sec_num": "3"
            },
            {
                "text": "It is arguable what base of the log should be used.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Feedback-augmented method",
                "sec_num": "3"
            },
            {
                "text": "In this paper, it is set to 2 so that the effect of \" on the interpolated probability becomes large when the confidence of the estimate of the conditional probability estimated from the feedback corpus is small (that is, when there is little data in the feedback corpus for the estimate) 8 . In summary, Equation ( 9) interpolates between the conditional probabilities estimated from the feedback corpus and the general corpus in the feedback-augmented method. The interpolated probability is then used to calculate the loglikelihood ratio. Doing so, the proposed method takes the feedback corpus as feedback to improve its performance.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Feedback-augmented method",
                "sec_num": "3"
            },
            {
                "text": "7 In general, the interpolated probability needs to be normalized to satisfy B s . In our case, however, it is al- ways satisfied without normalization since h j h pi f g ~ h j h i f g and l h pi f g ~ l h i f g are satisfied.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Feedback-augmented method",
                "sec_num": "3"
            },
            {
                "text": "8 We tested several bases in the experiments and found there were little difference in performance between them.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Feedback-augmented method",
                "sec_num": "3"
            },
            {
                "text": "A set of essays 9 written by Japanese learners of English was used as the target essays in the experiments. It consisted of 47 essays (3180 words) on the topic traveling. A native speaker of English who was a professional rewriter of English recognized 105 target errors in it.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Conditions",
                "sec_num": "4.1"
            },
            {
                "text": "The written part of the British National Corpus (BNC) (Burnard, 1995) was used to learn decision lists. Sentences the OAK system 10 , which was used to extract NPs from the corpus, failed to analyze were excluded. After these operations, the size of the corpus approximately amounted to 80 million words. Hereafter, the corpus will be referred to as the BNC.",
                "cite_spans": [
                    {
                        "start": 54,
                        "end": 69,
                        "text": "(Burnard, 1995)",
                        "ref_id": "BIBREF2"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Conditions",
                "sec_num": "4.1"
            },
            {
                "text": "As another corpus, the English concept explication in the EDR English-Japanese Bilingual dictionary and the EDR corpus (1993) were used; it will be referred to as the EDR corpus, hereafter. Its size amounted to about 3 million words.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Conditions",
                "sec_num": "4.1"
            },
            {
                "text": "Performance of the proposed method was evaluated by recall and precision. Recall is defined by No. of target errors detected correctly No. of target errors in the target essays (10) Precision is defined by",
                "cite_spans": [
                    {
                        "start": 177,
                        "end": 181,
                        "text": "(10)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Conditions",
                "sec_num": "4.1"
            },
            {
                "text": "No. of target errors detected correctly No. of detected errors (11)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Conditions",
                "sec_num": "4.1"
            },
            {
                "text": "First, decision lists for each target noun in the target essays were learned from the BNC 11 . To extract noun phrases and their head nouns, the OAK system was used. An optimal value for $ (window size of context) was estimated as follows. For 25 nouns shown in (Huddleston and Pullum, 2002) as examples of nouns used as both mass and count nouns, accuracy on the BNC was calculated using ten-fold cross validation. As a result of setting small ($ wQ S ), medium ($ wQ x w ), and large ($ wQ @w ) window sizes, it turned out that $ Q S maximized the average accuracy. Following this result, $ eQ S was selected in the experiments.",
                "cite_spans": [
                    {
                        "start": 262,
                        "end": 291,
                        "text": "(Huddleston and Pullum, 2002)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Procedures",
                "sec_num": "4.2"
            },
            {
                "text": "Second, the target nouns were distinguished whether they were mass or count by the learned 9 http://www.eng.ritsumei.ac.jp/lcorpus/. 10 OAK System Homepage: http://nlp.cs.nyu.edu/oak/. 11 If no instance of the target noun is found in the general corpora (and also in the feedback corpus in case of the feedback-augmented method), the target noun is ignored in the error detection procedure.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Procedures",
                "sec_num": "4.2"
            },
            {
                "text": "decision lists, and then the target errors were detected by applying the detection rules to the mass count distinction. As a preprocessing, spelling errors were corrected using a spell checker. The results of the detection were compared to those done by the native-speaker of English. From the comparison, recall and precision were calculated.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Procedures",
                "sec_num": "4.2"
            },
            {
                "text": "Then, the feedback-augmented method was evaluated on the same target essays. Each target essay in turn was left out, and all the remaining target essays were used as a feedback corpus. The target errors in the left-out essay were detected using the feedback-augmented method. The results of all 47 detections were integrated into one to calculate overall performance. This way of feedback can be regarded as that one uses revised essays previously written in a class to detect errors in essays on the same topic written in other classes.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Procedures",
                "sec_num": "4.2"
            },
            {
                "text": "Finally, the above two methods were compared with their seven variants shown in Table 5 . \"DL\" in Table 5 refers to the nine decision list based methods (the above two methods and their seven variants). The words in brackets denote the corpora used to learn decision lists; the symbol \"+FB\" means that the feedback corpus was simply added to the general corpus. The subscripts 6 B and 6 D indicate that the feedback was done by using Equation (8) and Equation (9), respectively.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 86,
                        "end": 87,
                        "text": "5",
                        "ref_id": "TABREF4"
                    },
                    {
                        "start": 104,
                        "end": 105,
                        "text": "5",
                        "ref_id": "TABREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Experimental Procedures",
                "sec_num": "4.2"
            },
            {
                "text": "In addition to the seven variants, two kinds of earlier method were used for comparison. One was one (Kawai et al., 1984) of the rule-based methods. It judges singular head nouns with no determiner to be erroneous since missing articles are most common in the writing of Japanese learners of English. In the experiments, this was implemented by treating all nouns as count nouns and applying the same detection rules as in the proposed method to the countability.",
                "cite_spans": [
                    {
                        "start": 101,
                        "end": 121,
                        "text": "(Kawai et al., 1984)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Procedures",
                "sec_num": "4.2"
            },
            {
                "text": "The other was a web-based method (Lapata and Keller, 2005) 12 for generating articles. It retrieves web counts for queries consisting of two words preceding the NP that the target noun head, one of the articles ( a/an, the, ), and the core NP to generate articles. All queries are performed as exact matches using quotation marks and submitted to the Google search engine in lower case. For example, in the case of \"*She is good student.\", it retrieves web counts for \"she is a good student\", \"she is the good student\", and \"she is good student\". Then, it generates the article that maximizes the web counts. We extended it to make it capable of detecting our target errors. First, the singular/plural distinction was taken into account in the queries (e.g., \"she is a good students\", \"she is the good students\", and \"she is good students\" in addition to the above three queries). The one(s) that maximized the web counts was judged to be correct; the rest were judged to be erroneous. Second, if determiners other than the articles modify head nouns, only the distinction between singular and plural was taken into account (e.g., \"he has some book\" vs \"he has some books\"). In the case of \"much/many\", the target noun in singular form modified by \"much\" and that in plural form modified by \"many\" were compared (e.g., \"he has much furniture\" vs \"he has many furnitures). Finally, some rules were used to detect literal errors. For example, plural head nouns modified by \"this\" were judged to be erroneous.",
                "cite_spans": [
                    {
                        "start": 33,
                        "end": 58,
                        "text": "(Lapata and Keller, 2005)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Procedures",
                "sec_num": "4.2"
            },
            {
                "text": "Table 5 shows the experimental results. \"Rulebased\" and \"Web-based\" in Table 5 refer to the rule-based method and the web-based method, respectively. The other symbols are as already explained in Section 4.2.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "5",
                        "ref_id": "TABREF4"
                    },
                    {
                        "start": 77,
                        "end": 78,
                        "text": "5",
                        "ref_id": "TABREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Experimental Results and Discussion",
                "sec_num": "4.3"
            },
            {
                "text": "As we can see from Table 5 , all the decision list based methods outperform the earlier methods. The rule-based method treated all nouns as count nouns, and thus it did not work well at all on mass nouns. This caused a lot of false-positives and false-negatives. The web-based method suffered a lot from other errors than the target errors since it implicitly assumed that there were no errors except the target errors. Contrary to this assumption, not only did the target essays contain the target errors but also other errors since they were written by Japanese learners of English. This indicate that the queries often contained the other errors when web counts were retrieved. These errors made the web counts useless, and thus it did not perform well. By contrast, the decision list based methods did because they distinguished mass and count nouns by one of the words around the target noun that was most likely to be effective according to the log-likelihood ratio 13 ; the best performing decision list based method (DL (EDR)) is sig- nificantly superior to the best performing 14 nondecision list based method (Web-based) in both recall and precision at the 99% confidence level. Table 5 also shows that the feedback-augmented methods benefit from feedback. Only an exception is \"DL (BNC)\". The reason is that the size of BNC is far larger than that of the feedback corpus and thus it did not affect the performance. This also explains that simply adding the feedback corpus to the general corpus achieved little or no improvement as \"DL (EDR+FB)\" and \"DL (BNC+FB)\" show. Unlike these, both \"DL (BNC)\" and \"DL (EDR)\" benefit from feed- back since the effect of the general corpus is limited to some extent by the log function in Equation (9). Because of this, both benefit from feedback despite the differences in size between the feedback corpus and the general corpus.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 25,
                        "end": 26,
                        "text": "5",
                        "ref_id": "TABREF4"
                    },
                    {
                        "start": 1195,
                        "end": 1196,
                        "text": "5",
                        "ref_id": "TABREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Experimental Results and Discussion",
                "sec_num": "4.3"
            },
            {
                "text": "Although the experimental results have shown that the feedback-augmented method is effective to detecting the target errors in the writing of Japanese learners of English, even the best performing method (DL (EDR)) made 30 false- negatives and 29 false-positives. About 70% of the false-negatives were errors that required other sources of information than the mass count distinction to be detected. For example, extra definite articles (e.g., *the traveling) cannot be detected even if the correct mass count distinction is given. Thus, only a little improvement is expected in recall however much feedback corpus data become available. On the other hand, most of the 13 Indeed, words around the target noun were effective. The default rules were used about 60% and 30% of the time in \"DL (EDR)\" and \"DL (BNC)\", respectively; when only the default rules were used, \"DL (EDR)\" (\"DL (BNC)\") achieved 0.66 (0.56) in recall and 0.58 (0.53) in precision.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Results and Discussion",
                "sec_num": "4.3"
            },
            {
                "text": "14 \"Best performing\" here means best performing in terms of -measure.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Results and Discussion",
                "sec_num": "4.3"
            },
            {
                "text": "false-positives were due to the decision lists themselves. Considering this, it is highly possible that precision will improve as the size of the feedback corpus increases.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Results and Discussion",
                "sec_num": "4.3"
            },
            {
                "text": "This paper has proposed a feedback-augmented method for distinguishing mass and count nouns to complement the conventional rules for detecting grammatical errors. The experiments have shown that the proposed method detected 71% of the target errors in the writing of Japanese learners of English with a precision of 72% when it was augmented by feedback. From the results, we conclude that the feedback-augmented method is effective to detecting errors concerning the articles and singular plural usage in the writing of Japanese learners of English. Although it is not taken into account in this paper, the feedback corpus contains further useful information. For example, we can obtain training data consisting of instances of errors by comparing the feedback corpus with its original corpus. Also, comparing it with the results of detection, we can know performance of each rule used in the detection, which make it possible to increase or decrease their log-likelihood ratios according to their performance. We will investigate how to exploit these sources of information in future work.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusions",
                "sec_num": "5"
            },
            {
                "text": "For the default rule, the log-likelihood ratio is defined by replacing f Pg and h pi with q and h pi major , respectively.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "The probability for the default rule is estimated just as the log-likelihood ratio for the default rule above.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "It depends on the target noun how many rules are discarded.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Mass nouns can be used in plural in some cases. However, they are rare especially in the writing of learners of English.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "The feedback corpus refers to learners' essays whose errors are corrected as mentioned in Section 1.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "There are other statistical methods that can be used for comparison includingLee (2004) andMinnen (2000).Lapata and Keller (2005) report that the web-based method is the best performing article generation method.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "The authors would like to thank Sekine Satoshi who has developed the OAK System. The authors also would like to thank three anonymous reviewers for their useful comments on this paper.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgments",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Nouns and countability",
                "authors": [
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Allan",
                        "suffix": ""
                    }
                ],
                "year": 1980,
                "venue": "J. Linguistic Society of America",
                "volume": "56",
                "issue": "3",
                "pages": "541--567",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "K. Allan. 1980. Nouns and countability. J. Linguistic Society of America, 56(3):541-567.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Translating the Untranslatable. CSLI publications",
                "authors": [
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Bond",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "F. Bond. 2005. Translating the Untranslatable. CSLI publications, Stanford.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Users Reference Guide for the British National Corpus. version 1.0. Oxford University Computing Services",
                "authors": [
                    {
                        "first": "L",
                        "middle": [],
                        "last": "Burnard",
                        "suffix": ""
                    }
                ],
                "year": 1995,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "L. Burnard. 1995. Users Reference Guide for the British National Corpus. version 1.0. Oxford Uni- versity Computing Services, Oxford.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "An unsupervised method for detecting grammatical errors",
                "authors": [
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Chodorow",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Leacock",
                        "suffix": ""
                    }
                ],
                "year": 2000,
                "venue": "Proc. of 1st Meeting of the North America Chapter of ACL",
                "volume": "",
                "issue": "",
                "pages": "140--147",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "M. Chodorow and C. Leacock. 2000. An unsupervised method for detecting grammatical errors. In Proc. of 1st Meeting of the North America Chapter of ACL, pages 140-147.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Japan electronic dictionary research institute ltd",
                "authors": [],
                "year": 1993,
                "venue": "EDR electronic dictionary specifications guide. Japan electronic dictionary research institute ltd",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Japan electronic dictionary research institute ltd. 1993. EDR electronic dictionary specifications guide. Japan electronic dictionary research institute ltd, Tokyo.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Prefabricated patterns in advanced EFL writing: collocations and formulae",
                "authors": [
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Granger",
                        "suffix": ""
                    }
                ],
                "year": 1998,
                "venue": "Phraseology: theory, analysis, and applications",
                "volume": "",
                "issue": "",
                "pages": "145--160",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "S. Granger. 1998. Prefabricated patterns in advanced EFL writing: collocations and formulae. In A. P. Cowie, editor, Phraseology: theory, analysis, and applications, pages 145-160. Clarendon Press.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "The Cambridge Grammar of the English Language",
                "authors": [
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Huddleston",
                        "suffix": ""
                    },
                    {
                        "first": "G",
                        "middle": [
                            "K"
                        ],
                        "last": "Pullum",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "R. Huddleston and G.K. Pullum. 2002. The Cam- bridge Grammar of the English Language. Cam- bridge University Press, Cambridge.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Automatic error detection in the Japanese learners' English spoken data",
                "authors": [
                    {
                        "first": "E",
                        "middle": [],
                        "last": "Izumi",
                        "suffix": ""
                    },
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Uchimoto",
                        "suffix": ""
                    },
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Saiga",
                        "suffix": ""
                    },
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Supnithi",
                        "suffix": ""
                    },
                    {
                        "first": "H",
                        "middle": [],
                        "last": "Isahara",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Proc. of 41st Annual Meeting of ACL",
                "volume": "",
                "issue": "",
                "pages": "145--148",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "E. Izumi, K. Uchimoto, T. Saiga, T. Supnithi, and H. Isahara. 2003. Automatic error detection in the Japanese learners' English spoken data. In Proc. of 41st Annual Meeting of ACL, pages 145-148.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "ASPEC-I: An error detection system for English composition",
                "authors": [
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Kawai",
                        "suffix": ""
                    },
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Sugihara",
                        "suffix": ""
                    },
                    {
                        "first": "N",
                        "middle": [],
                        "last": "Sugie",
                        "suffix": ""
                    }
                ],
                "year": 1984,
                "venue": "IPSJ Journal (in Japanese)",
                "volume": "25",
                "issue": "6",
                "pages": "1072--1079",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "A. Kawai, K. Sugihara, and N. Sugie. 1984. ASPEC-I: An error detection system for English composition. IPSJ Journal (in Japanese), 25(6):1072-1079.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Web-based models for natural language processing",
                "authors": [
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Lapata",
                        "suffix": ""
                    },
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Keller",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "ACM Transactions on Speech and Language Processing",
                "volume": "2",
                "issue": "1",
                "pages": "1--31",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "M. Lapata and F. Keller. 2005. Web-based models for natural language processing. ACM Transactions on Speech and Language Processing, 2(1):1-31.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Automatic article restoration",
                "authors": [
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proc. of the Human Language Technology Conference of the North American Chapter of ACL",
                "volume": "",
                "issue": "",
                "pages": "31--36",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "J. Lee. 2004. Automatic article restoration. In Proc. of the Human Language Technology Conference of the North American Chapter of ACL, pages 31-36.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "English error correction: A syntactic user model based on principled \"mal-rule\" scoring",
                "authors": [
                    {
                        "first": "K",
                        "middle": [
                            "F"
                        ],
                        "last": "Mccoy",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [
                            "A"
                        ],
                        "last": "Pennington",
                        "suffix": ""
                    },
                    {
                        "first": "L",
                        "middle": [
                            "Z"
                        ],
                        "last": "Suri",
                        "suffix": ""
                    }
                ],
                "year": 1996,
                "venue": "Proc. of 5th International Conference on User Modeling",
                "volume": "",
                "issue": "",
                "pages": "69--66",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "K.F. McCoy, C.A. Pennington, and L.Z. Suri. 1996. English error correction: A syntactic user model based on principled \"mal-rule\" scoring. In Proc. of 5th International Conference on User Modeling, pages 69-66.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Memory-based learning for article generation",
                "authors": [],
                "year": null,
                "venue": "Proc. of CoNLL-2000 and LLL-2000 workshop",
                "volume": "",
                "issue": "",
                "pages": "43--48",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Memory-based learning for article generation. In Proc. of CoNLL-2000 and LLL-2000 workshop, pages 43-48.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Predictable meaning shift: Some linguistic properties of lexical implication rules",
                "authors": [
                    {
                        "first": "N",
                        "middle": [],
                        "last": "Ostler",
                        "suffix": ""
                    },
                    {
                        "first": "B",
                        "middle": [
                            "T"
                        ],
                        "last": "Atkins",
                        "suffix": ""
                    }
                ],
                "year": 1991,
                "venue": "Proc. of 1st SIGLEX Workshop on Lexical Semantics and Knowledge Representation",
                "volume": "",
                "issue": "",
                "pages": "87--100",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "N. Ostler and B.T.S Atkins. 1991. Predictable mean- ing shift: Some linguistic properties of lexical impli- cation rules. In Proc. of 1st SIGLEX Workshop on Lexical Semantics and Knowledge Representation, pages 87-100.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Recognizing syntactic errors in the writing of second language learners",
                "authors": [
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Schneider",
                        "suffix": ""
                    },
                    {
                        "first": "K",
                        "middle": [
                            "F"
                        ],
                        "last": "Mccoy",
                        "suffix": ""
                    }
                ],
                "year": 1998,
                "venue": "Proc. of 17th International Conference on Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "1198--1205",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "D. Schneider and K.F. McCoy. 1998. Recognizing syntactic errors in the writing of second language learners. In Proc. of 17th International Conference on Computational Linguistics, pages 1198-1205.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Psychological methods",
                "authors": [
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Tanaka",
                        "suffix": ""
                    }
                ],
                "year": 1977,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Y. Tanaka. 1977. Psychological methods (in Japanese). University of Tokyo Press.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Developing a new grammar checker for English as a second language",
                "authors": [
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Tschichold",
                        "suffix": ""
                    },
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Bodmer",
                        "suffix": ""
                    },
                    {
                        "first": "E",
                        "middle": [],
                        "last": "Cornu",
                        "suffix": ""
                    },
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Grosjean",
                        "suffix": ""
                    },
                    {
                        "first": "L",
                        "middle": [],
                        "last": "Grosjean",
                        "suffix": ""
                    },
                    {
                        "first": "N",
                        "middle": [],
                        "last": "Ubler",
                        "suffix": ""
                    },
                    {
                        "first": "N",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Tschumi",
                        "suffix": ""
                    }
                ],
                "year": 1997,
                "venue": "Proc. of the From Research to Commercial Applications Workshop",
                "volume": "",
                "issue": "",
                "pages": "7--12",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "C. Tschichold, F. Bodmer, E. Cornu, F. Grosjean, L. Grosjean, N. K ubler, N. L ewy, and C. Tschumi. 1997. Developing a new grammar checker for En- glish as a second language. In Proc. of the From Re- search to Commercial Applications Workshop, pages 7-12.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Unsupervised word sense disambiguation rivaling supervised methods",
                "authors": [
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Yarowsky",
                        "suffix": ""
                    }
                ],
                "year": 1995,
                "venue": "Proc. of 33rd Annual Meeting of ACL",
                "volume": "",
                "issue": "",
                "pages": "189--196",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "D. Yarowsky. 1995. Unsupervised word sense disam- biguation rivaling supervised methods. In Proc. of 33rd Annual Meeting of ACL, pages 189-196.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Homograph Disambiguation in Speech Synthesis",
                "authors": [
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Yarowsky",
                        "suffix": ""
                    }
                ],
                "year": 1996,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "D. Yarowsky. 1996. Homograph Disambiguation in Speech Synthesis. Springer-Verlag.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "major denote the target noun and the majority of \u00a1 V\u00a2 in the training data, respec- tively. Equation (3) reads \"If the target noun appears, then it is distinguished by the majority\".",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "TABREF1": {
                "content": "<table><tr><td>( \" H 2 2 3 5 1.49 \u00a9 3 5 1.49 LLR ( LLR 6 H  \u00a7 F 3 5 1.28 \" 82 $ E 5 1.32</td></tr></table>",
                "type_str": "table",
                "text": "Rules in a decision list MassCount",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table><tr><td>Count</td><td>Mass</td></tr></table>",
                "type_str": "table",
                "text": "Detection rules (i)",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table><tr><td/><td/><td>Singular</td><td>Plural</td></tr><tr><td/><td colspan=\"3\">a/an the a/an the</td></tr><tr><td>Mass</td><td/><td>--</td><td/></tr><tr><td>Count</td><td>-</td><td>-</td><td>--</td></tr><tr><td colspan=\"4\">(e.g., *an expensive). Likewise, the definite article</td></tr><tr><td colspan=\"4\">that modifies other than the head noun or adjective</td></tr><tr><td colspan=\"4\">is judged to be erroneous (e.g., *the them). Also,</td></tr><tr><td colspan=\"4\">we have made exceptions to the rules. The follow-</td></tr><tr><td colspan=\"4\">ing combinations are excluded from the detection</td></tr><tr><td colspan=\"4\">in the second and third steps: head nouns modified</td></tr><tr><td colspan=\"4\">by interrogative adjectives (e.g., what), possessive</td></tr><tr><td colspan=\"4\">adjectives (e.g., my), 's genitives, \"some\", \"any\",</td></tr><tr><td>or \"no\".</td><td/><td/><td/></tr></table>",
                "type_str": "table",
                "text": "Detection rules (ii)",
                "html": null,
                "num": null
            },
            "TABREF4": {
                "content": "<table><tr><td>Method</td><td colspan=\"2\">Recall Precision</td></tr><tr><td>DL (BNC)</td><td>0.66</td><td>0.65</td></tr><tr><td>DL (BNC+FB)</td><td>0.66</td><td>0.65</td></tr><tr><td colspan=\"3\">DL (BNC) 0.66 0.65</td></tr><tr><td colspan=\"3\">DL (BNC) 0.69 0.70</td></tr><tr><td>DL (EDR)</td><td>0.70</td><td>0.68</td></tr><tr><td>DL (EDR+FB)</td><td>0.71</td><td>0.69</td></tr><tr><td colspan=\"3\">DL (EDR) 0.71 0.70</td></tr><tr><td colspan=\"3\">DL (EDR) 0.71 0.72</td></tr><tr><td>DL (FB)</td><td>0.43</td><td>0.76</td></tr><tr><td>Rule-based</td><td>0.59</td><td>0.39</td></tr><tr><td>Web-based</td><td>0.49</td><td>0.53</td></tr></table>",
                "type_str": "table",
                "text": "Experimental results",
                "html": null,
                "num": null
            }
        }
    }
}