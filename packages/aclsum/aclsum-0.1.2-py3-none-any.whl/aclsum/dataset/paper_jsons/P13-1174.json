{
    "paper_id": "P13-1174",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:03:01.023718Z"
    },
    "title": "Connotation Lexicon: A Dash of Sentiment Beneath the Surface Meaning",
    "authors": [
        {
            "first": "Song",
            "middle": [],
            "last": "Feng",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Jun",
            "middle": [
                "Seok"
            ],
            "last": "Kang",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Polina",
            "middle": [],
            "last": "Kuznetsova",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Yejin",
            "middle": [],
            "last": "Choi",
            "suffix": "",
            "affiliation": {},
            "email": "ychoi@cs.stonybrook.edu"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Understanding the connotation of words plays an important role in interpreting subtle shades of sentiment beyond denotative or surface meaning of text, as seemingly objective statements often allude nuanced sentiment of the writer, and even purposefully conjure emotion from the readers' minds. The focus of this paper is drawing nuanced, connotative sentiments from even those words that are objective on the surface, such as \"intelligence\", \"human\", and \"cheesecake\". We propose induction algorithms encoding a diverse set of linguistic insights (semantic prosody, distributional similarity, semantic parallelism of coordination) and prior knowledge drawn from lexical resources, resulting in the first broad-coverage connotation lexicon.",
    "pdf_parse": {
        "paper_id": "P13-1174",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Understanding the connotation of words plays an important role in interpreting subtle shades of sentiment beyond denotative or surface meaning of text, as seemingly objective statements often allude nuanced sentiment of the writer, and even purposefully conjure emotion from the readers' minds. The focus of this paper is drawing nuanced, connotative sentiments from even those words that are objective on the surface, such as \"intelligence\", \"human\", and \"cheesecake\". We propose induction algorithms encoding a diverse set of linguistic insights (semantic prosody, distributional similarity, semantic parallelism of coordination) and prior knowledge drawn from lexical resources, resulting in the first broad-coverage connotation lexicon.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "There has been a substantial body of research in sentiment analysis over the last decade (Pang and Lee, 2008) , where a considerable amount of work has focused on recognizing sentiment that is generally explicit and pronounced rather than implied and subdued. However in many real-world texts, even seemingly objective statements can be opinion-laden in that they often allude nuanced sentiment of the writer (Greene and Resnik, 2009) , or purposefully conjure emotion from the readers' minds (Mohammad and Turney, 2010) . Although some researchers have explored formal and statistical treatments of those implicit and implied sentiments (e.g. Wiebe et al. (2005) , Esuli and Sebastiani (2006) , Greene and Resnik (2009) , Davidov et al. (2010) ), automatic analysis of them largely remains as a big challenge.",
                "cite_spans": [
                    {
                        "start": 89,
                        "end": 109,
                        "text": "(Pang and Lee, 2008)",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 409,
                        "end": 434,
                        "text": "(Greene and Resnik, 2009)",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 493,
                        "end": 520,
                        "text": "(Mohammad and Turney, 2010)",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 644,
                        "end": 663,
                        "text": "Wiebe et al. (2005)",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 666,
                        "end": 693,
                        "text": "Esuli and Sebastiani (2006)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 696,
                        "end": 720,
                        "text": "Greene and Resnik (2009)",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 723,
                        "end": 744,
                        "text": "Davidov et al. (2010)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this paper, we concentrate on understanding the connotative sentiments of words, as they play an important role in interpreting subtle shades of sentiment beyond denotative or surface meaning of text. For instance, consider the following:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Geothermal replaces oil-heating; it helps reducing greenhouse emissions.1 ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Although this sentence could be considered as a factual statement from the general standpoint, the subtle effect of this sentence may not be entirely objective: this sentence is likely to have an influence on readers' minds in regard to their opinion toward \"geothermal\". In order to sense the subtle overtone of sentiments, one needs to know that the word \"emissions\" has generally negative connotation, which geothermal reduces. In fact, depending on the pragmatic contexts, it could be precisely the intention of the author to transfer his opinion into the readers' minds.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The main contribution of this paper is a broadcoverage connotation lexicon that determines the connotative polarity of even those words with ever so subtle connotation beneath their surface meaning, such as \"Literature\", \"Mediterranean\", and \"wine\". Although there has been a number of previous work that constructed sentiment lexicons (e.g., Esuli and Sebastiani (2006) , Wilson et al. (2005a) , Kaji and Kitsuregawa (2007) , Qiu et al. (2009) ), which seem to be increasingly and inevitably expanding over words with (strongly) connotative sentiments rather than explicit sentiments alone (e.g., \"gun\"), little prior work has directly tackled this problem of learning connotation, 2 and much of the subtle connotation of many seemingly objective words is yet to be determined. POSITIVE NEGATIVE FEMA, Mandela, Intel, Google, Python, Sony, Pulitzer, Harvard, Duke, Einstein, Shakespeare, Elizabeth, Clooney, Hoover, Goldman, Swarovski, Hawaii, Yellowstone Katrina, Monsanto, Halliburton, Enron, Teflon, Hiroshima, Holocaust, Afghanistan, Mugabe, Hutu, Saddam, Osama, Qaeda, Kosovo, Helicobacter, HIV A central premise to our approach is that it is collocational statistics of words that affect and shape the polarity of connotation. Indeed, the etymology of \"connotation\" is from the Latin \"com-\" (\"together or with\") and \"notare\" (\"to mark\"). It is important to clarify, however, that we do not simply assume that words that collocate share the same polarity of connotation. Although such an assumption played a key role in previous work for the analogous task of learning sentiment lexicon (Velikovich et al., 2010) , we expect that the same assumption would be less reliable in drawing subtle connotative sentiments of words. As one example, the predicate \"cure\", which has a positive connotation typically takes arguments with negative connotation, e.g., \"disease\", when used as the \"relieve\" sense. 3 Therefore, in order to attain a broad coverage lexicon while maintaining good precision, we guide the induction algorithm with multiple, carefully selected linguistic insights: [1] distributional similarity, [2] semantic parallelism of coordination, [3] selectional preference, and [4] semantic prosody (e.g., Sinclair (1991) , Louw (1993) , Stubbs (1995) , Stefanowitsch and Gries (2003) )), and also exploit existing lexical resources as an additional inductive bias.",
                "cite_spans": [
                    {
                        "start": 343,
                        "end": 370,
                        "text": "Esuli and Sebastiani (2006)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 373,
                        "end": 394,
                        "text": "Wilson et al. (2005a)",
                        "ref_id": null
                    },
                    {
                        "start": 397,
                        "end": 424,
                        "text": "Kaji and Kitsuregawa (2007)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 427,
                        "end": 444,
                        "text": "Qiu et al. (2009)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 803,
                        "end": 1100,
                        "text": "Mandela, Intel, Google, Python, Sony, Pulitzer, Harvard, Duke, Einstein, Shakespeare, Elizabeth, Clooney, Hoover, Goldman, Swarovski, Hawaii, Yellowstone Katrina, Monsanto, Halliburton, Enron, Teflon, Hiroshima, Holocaust, Afghanistan, Mugabe, Hutu, Saddam, Osama, Qaeda, Kosovo, Helicobacter, HIV",
                        "ref_id": null
                    },
                    {
                        "start": 1593,
                        "end": 1618,
                        "text": "(Velikovich et al., 2010)",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 2217,
                        "end": 2232,
                        "text": "Sinclair (1991)",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 2235,
                        "end": 2246,
                        "text": "Louw (1993)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 2249,
                        "end": 2262,
                        "text": "Stubbs (1995)",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 2265,
                        "end": 2295,
                        "text": "Stefanowitsch and Gries (2003)",
                        "ref_id": "BIBREF24"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "We cast the connotation lexicon induction task as a collective inference problem, and consider approaches based on three distinct types of algorithmic framework that have been shown successful for conventional sentiment lexicon induction:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Random walk based on HITS/PageRank (e.g., Kleinberg (1999) Label/Graph propagation (e.g., Zhu and Ghahra-(2011) but with practical limitations. See \u00a73 for detailed discussion.",
                "cite_spans": [
                    {
                        "start": 42,
                        "end": 58,
                        "text": "Kleinberg (1999)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 90,
                        "end": 111,
                        "text": "Zhu and Ghahra-(2011)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "3 Note that when \"cure\" is used as the \"preserve\" sense, it expects objects with non-negative connotation. Hence wordsense-disambiguation (WSD) presents a challenge, though not unexpectedly. In this work, we assume the general connotation of each word over statistically prevailing senses, leaving a more cautious handling of WSD as future work. mani (2002), Velikovich et al. (2010) )",
                "cite_spans": [
                    {
                        "start": 359,
                        "end": 383,
                        "text": "Velikovich et al. (2010)",
                        "ref_id": "BIBREF29"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Constraint optimization (e.g., Roth and Yih (2004) , Choi and Cardie (2009) , Lu et al. (2011) ).",
                "cite_spans": [
                    {
                        "start": 31,
                        "end": 50,
                        "text": "Roth and Yih (2004)",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 53,
                        "end": 75,
                        "text": "Choi and Cardie (2009)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 78,
                        "end": 94,
                        "text": "Lu et al. (2011)",
                        "ref_id": "BIBREF15"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "We provide comparative empirical results over several variants of these approaches with comprehensive evaluations including lexicon-based, human judgments, and extrinsic evaluations.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "It is worthwhile to note that not all words have connotative meanings that are distinct from denotational meanings, and in some cases, it can be difficult to determine whether the overall sentiment is drawn from denotational or connotative meanings exclusively, or both. Therefore, we encompass any sentiment from either type of meanings into the lexicon, where non-neutral polarity prevails over neutral one if some meanings lead to neutral while others to non-neutral. 4Our work results in the first broad-coverage connotation lexicon,5 significantly improving both the coverage and the precision of Feng et al. (2011) . As an interesting by-product, our algorithm can be also used as a proxy to measure the general connotation of real-world named entities based on their collocational statistics. Table 1 highlights some example proper nouns included in the final lexicon.",
                "cite_spans": [
                    {
                        "start": 602,
                        "end": 620,
                        "text": "Feng et al. (2011)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 806,
                        "end": 807,
                        "text": "1",
                        "ref_id": "TABREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The rest of the paper is structured as follows. In \u00a72 we describe three types of induction algorithms followed by evaluation in \u00a73. Then we revisit the induction algorithms based on constraint optimization in \u00a74 to enhance quality and scalability. \u00a75 presents comprehensive evaluation with human judges and extrinsic evaluations. Related work and conclusion are in \u00a76 and \u00a77. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "We develop induction algorithms based on three distinct types of algorithmic framework that have been shown successful for the analogous task of sentiment lexicon induction: HITS & PageRank ( \u00a72.1), Label/Graph Propagation ( \u00a72.2), and Constraint Optimization via Integer Linear Programming ( \u00a72.3). As will be shown, each of these approaches will incorporate additional, more diverse linguistic insights.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Connotation Induction Algorithms",
                "sec_num": "2"
            },
            {
                "text": "The work of Feng et al. (2011) explored the use of HITS (Kleinberg, 1999) and PageRank (Page et al., 1999) to induce the general connotation of words hinging on the linguistic phenomena of selectional preference and semantic prosody, i.e., connotative predicates influencing the connotation of their arguments. For example, the object of a negative connotative predicate \"cure\" is likely to have negative connotation, e.g., \"disease\" or \"cancer\". The bipartite graph structure for this approach corresponds to the left-most box (labeled as \"pred-arg\") in Figure 1 .",
                "cite_spans": [
                    {
                        "start": 12,
                        "end": 30,
                        "text": "Feng et al. (2011)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 56,
                        "end": 73,
                        "text": "(Kleinberg, 1999)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 87,
                        "end": 106,
                        "text": "(Page et al., 1999)",
                        "ref_id": "BIBREF18"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 562,
                        "end": 563,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "HITS & PageRank",
                "sec_num": "2.1"
            },
            {
                "text": "With the goal of obtaining a broad-coverage lexicon in mind, we find that relying only on the structure of semantic prosody is limiting, due to relatively small sets of connotative predicates available. 6 Therefore, we extend the graph structure as an overlay of two sub-graphs (Figure 1 ) as described below:",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 286,
                        "end": 287,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Label Propagation",
                "sec_num": "2.2"
            },
            {
                "text": "Sub-graph #1: Predicate-Argument Graph",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Label Propagation",
                "sec_num": "2.2"
            },
            {
                "text": "This sub-graph is the bipartite graph that encodes the selectional preference of connotative predicates over their arguments. In this graph, connotative predicates p reside on one side of the graph and their co-occurring arguments a reside on the other side of the graph based on Google Web 1T corpus. 7 The weight on the edges between the predicates p and arguments a are defined using Point-wise Mutual Information (PMI) as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Label Propagation",
                "sec_num": "2.2"
            },
            {
                "text": "w(p \u2192 a) := P M I(p, a) = log 2 P (p, a) P (p)P (a)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Label Propagation",
                "sec_num": "2.2"
            },
            {
                "text": "PMI scores have been widely used in previous studies to measure association between words (e.g., Turney (2001) , Church and Hanks (1990) ).",
                "cite_spans": [
                    {
                        "start": 97,
                        "end": 110,
                        "text": "Turney (2001)",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 113,
                        "end": 136,
                        "text": "Church and Hanks (1990)",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Label Propagation",
                "sec_num": "2.2"
            },
            {
                "text": "The second sub-graph is based on the distributional similarities among the arguments. One possible way of constructing such a graph is simply connecting all nodes and assign edge weights proportionate to the word association scores, such as PMI, or distributional similarity. However, such a completely connected graph can be susceptible to propagating noise, and does not scale well over a very large set of vocabulary. We therefore reduce the graph connectivity by exploiting semantic parallelism of coordination (Bock (1986) (1997), Pickering and Branigan (1998)). In particular, we consider an undirected edge between a pair of arguments a 1 and a 2 only if they occurred together in the \"a 1 and a 2 \" or \"a 2 and a 1 \" coordination, and assign edge weights as:",
                "cite_spans": [
                    {
                        "start": 515,
                        "end": 527,
                        "text": "(Bock (1986)",
                        "ref_id": "BIBREF1"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Sub-graph #2: Argument-Argument Graph",
                "sec_num": null
            },
            {
                "text": "w(a1 -a2) = CosineSim( -\u2192 a1, -\u2192 a2) = -\u2192 a1 \u2022 -\u2192 a2 || -\u2192 a1|| || -\u2192 a2||",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Sub-graph #2: Argument-Argument Graph",
                "sec_num": null
            },
            {
                "text": "where -\u2192 a 1 and -\u2192 a 2 are co-occurrence vectors for a 1 and a 2 respectively. The co-occurrence vector for each word is computed using PMI scores with respect to the top n co-occurring words.8 n (=50) is selected empirically. The edge weights in two sub-graphs are normalized so that they are in the comparable range.9 ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Sub-graph #2: Argument-Argument Graph",
                "sec_num": null
            },
            {
                "text": "Although graph-based algorithms ( \u00a72.1, \u00a72.2) provide an intuitive framework to incorporate various lexical relations, limitations include:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Limitations of Graph-based Algorithms",
                "sec_num": null
            },
            {
                "text": "1. They allow only non-negative edge weights. Therefore, we can encode only positive (supportive) relations among words (e.g., distributionally similar words will endorse each other with the same polarity), while missing on exploiting negative relations (e.g., antonyms may drive each other into the opposite polarity).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Limitations of Graph-based Algorithms",
                "sec_num": null
            },
            {
                "text": "2. They induce positive and negative polarities in isolation via separate graphs. However, we expect that a more effective algorithm should induce both polarities simultaneously.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Limitations of Graph-based Algorithms",
                "sec_num": null
            },
            {
                "text": "3. The framework does not readily allow incorporating a diverse set of soft and hard constraints.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Limitations of Graph-based Algorithms",
                "sec_num": null
            },
            {
                "text": "Addressing limitations of graph-based algorithms ( \u00a72.2), we propose an induction algorithm based on Integer Linear Programming (ILP). Figure 2 provides the pictorial overview. In comparison to Figure 1 , two new components are: (1) dictionarydriven relations targeting enhanced precision, and",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 142,
                        "end": 143,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    },
                    {
                        "start": 201,
                        "end": 202,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Constraint Optimization",
                "sec_num": "2.3"
            },
            {
                "text": "(2) dictionary-driven words (i.e., unseen words with respect to those relations explored in Figure 1 ) targeting enhanced coverage. We formulate insights in Figure 2 using ILP as follows:",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 99,
                        "end": 100,
                        "text": "1",
                        "ref_id": null
                    },
                    {
                        "start": 164,
                        "end": 165,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Constraint Optimization",
                "sec_num": "2.3"
            },
            {
                "text": "Definition of sets of words:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Constraint Optimization",
                "sec_num": "2.3"
            },
            {
                "text": "1. P + : the set of positive seed predicates. P -: the set of negative seed predicates. 2. S: the set of seed sentiment words. 3. R syn : word pairs in synonyms relation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Constraint Optimization",
                "sec_num": "2.3"
            },
            {
                "text": "R ant : word pairs in antonyms relation. R coord : word pairs in coordination relation. R pred : word pairs in pred-arg relation. R pred +(-) : R pred based on P + (P -).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Constraint Optimization",
                "sec_num": "2.3"
            },
            {
                "text": "For each word i, we define binary variables x i , y i , z i \u2208 {0, 1}, where x i = 1 (y i = 1, z i = 1) if and only if i has a positive (negative, neutral) connotation respectively. For every pair of word i and j, we define binary variables d pq ij where p, q \u2208 {+, -, 0} and d pq ij = 1 if and only if the polarity of i and j are p and q respectively.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Definition of variables:",
                "sec_num": null
            },
            {
                "text": "Objective function: We aim to maximize:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Definition of variables:",
                "sec_num": null
            },
            {
                "text": "F = \u03a6 prosody + \u03a6 coord + \u03a6 neu",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Definition of variables:",
                "sec_num": null
            },
            {
                "text": "where \u03a6 prosody is the scores based on semantic prosody, \u03a6 coord captures the distributional similarity over coordination, and \u03a6 neu controls the sensitivity of connotation detection between positive (negative) and neutral. In particular,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Definition of variables:",
                "sec_num": null
            },
            {
                "text": "\u03a6 prosody = R pred i,j w pred i,j (d ++ i,j + d -- i,j -d +- i,j -d -+ i,j ) \u03a6 coord = R coord i,j w coord i,j (d ++ i,j + d -- i,j + d 00 i,j ) \u03a6 neu = \u03b1 R pred i,j w pred i,j",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Definition of variables:",
                "sec_num": null
            },
            {
                "text": "\u2022 zj",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Definition of variables:",
                "sec_num": null
            },
            {
                "text": "The weights in the objective function are set as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Soft constraints (edge weights):",
                "sec_num": null
            },
            {
                "text": "w pred (p, a) = f req(p, a) (p,x)\u2208R pred f req(p, x) w coord (a1, a2) = CosSim( -\u2192 a1, -\u2192 a2) = -\u2192 a1 \u2022 -\u2192 a2 || -\u2192 a1|| || -\u2192 a2||",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Soft constraints (edge weights):",
                "sec_num": null
            },
            {
                "text": "Note that the same w coord (a 1 , a 2 ) has been used in graph propagation described in Section 2.2. \u03b1 controls the sensitivity of connotation detection such that higher value of \u03b1 will promote neutral connotation over polar ones.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Soft constraints (edge weights):",
                "sec_num": null
            },
            {
                "text": "Hard constrains for variable consistency:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Soft constraints (edge weights):",
                "sec_num": null
            },
            {
                "text": "1. Each word i has one of {+, -, \u00f8} as polarity: \u2200i,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Soft constraints (edge weights):",
                "sec_num": null
            },
            {
                "text": "x i + y i + z i = 1",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Soft constraints (edge weights):",
                "sec_num": null
            },
            {
                "text": "2. Variable consistency between d pq ij and x i , y i , z i :",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Soft constraints (edge weights):",
                "sec_num": null
            },
            {
                "text": "x i + x j -1 \u2264 2d ++ i,j \u2264 x i + x j y i + y j -1 \u2264 2d -- i,j \u2264 y i + y j z i + z j -1 \u2264 2d 00 i,j \u2264 z i + z j x i + y j -1 \u2264 2d +- i,j \u2264 x i + y j y i + x j -1 \u2264 2d -+",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Soft constraints (edge weights):",
                "sec_num": null
            },
            {
                "text": "i,j \u2264 y i + x j Hard constrains for WordNet relations:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Soft constraints (edge weights):",
                "sec_num": null
            },
            {
                "text": "1. C ant : Antonym pairs will not have the same positive or negative polarity:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Soft constraints (edge weights):",
                "sec_num": null
            },
            {
                "text": "\u2200(i, j) \u2208 R ant , x i + x j \u2264 1, y i + y j \u2264 1",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Soft constraints (edge weights):",
                "sec_num": null
            },
            {
                "text": "For this constraint, we only consider antonym pairs that share the same root, e.g., \"sufficient\" and \"insufficient\", as those pairs are more likely to have the opposite polarities than pairs without sharing the same root, e.g., \"east\" and \"west\".",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Soft constraints (edge weights):",
                "sec_num": null
            },
            {
                "text": "2. C syn : Synonym pairs will not have the opposite polarity:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Soft constraints (edge weights):",
                "sec_num": null
            },
            {
                "text": "\u2200(i, j) \u2208 R syn , x i + y j \u2264 1, x j + y i \u2264 1 3 Experimental Result I",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Soft constraints (edge weights):",
                "sec_num": null
            },
            {
                "text": "We provide comprehensive comparisons over variants of three types of algorithms proposed in \u00a72.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Soft constraints (edge weights):",
                "sec_num": null
            },
            {
                "text": "We use the Google Web 1T data (Brants and Franz (2006) ), and POS-tagged ngrams using Stanford POS Tagger (Toutanova and Manning (2000) ). We filter out the ngrams with punctuations and other special characters to reduce the noise.",
                "cite_spans": [
                    {
                        "start": 30,
                        "end": 54,
                        "text": "(Brants and Franz (2006)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 106,
                        "end": 135,
                        "text": "(Toutanova and Manning (2000)",
                        "ref_id": "BIBREF27"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Soft constraints (edge weights):",
                "sec_num": null
            },
            {
                "text": "Note that we consider the connotation lexicon to be inclusive of a sentiment lexicon for two practical reasons: first, it is highly unlikely that any word with non-neutral sentiment (i.e., positive or negative) would carry connotation of the opposite, i.e., conflicting10 polarity. Second, for some words with distinct sentiment or strong connotation, it can be difficult or even unnatural to draw a precise distinction between connotation and sentiment, e.g., \"efficient\". Therefore, sentiment lexicons can serve as a surrogate to measure a subset of connotation words induced by the algorithms, as shown in Table 3 with respect to General Inquirer (Stone and Hunt (1963) ) and MPQA (Wilson et al. (2005b) ).11 ",
                "cite_spans": [
                    {
                        "start": 650,
                        "end": 672,
                        "text": "(Stone and Hunt (1963)",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 684,
                        "end": 706,
                        "text": "(Wilson et al. (2005b)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 615,
                        "end": 616,
                        "text": "3",
                        "ref_id": "TABREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Comparison against Conventional Sentiment Lexicon",
                "sec_num": "3.1"
            },
            {
                "text": "Discussion Table 3 shows the agreement statistics with respect to two conventional sentiment lexicons. We find that the use of label propagation alone [PRED-ARG (CP)] improves the performance substantially over the comparable graph construction with different graph analysis algorithms, in particular, HITS and PageRank approaches of Feng et al. (2011) . The two completely connected variants of the graph propagation on the Pred-Arg graph, [ PRED-ARG (PMI)] and [ PRED-ARG (CP)], do not necessarily improve the performance over the simpler and computationally lighter alternative, [PRED-ARG (CP)]. The [OVERLAY] , which is based on both Pred-Arg and Arg-Arg subgraphs ( \u00a72.2), achieves the best performance among graph-based algorithms, significantly improving the precision over all other baselines. This result suggests:",
                "cite_spans": [
                    {
                        "start": 334,
                        "end": 352,
                        "text": "Feng et al. (2011)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 603,
                        "end": 612,
                        "text": "[OVERLAY]",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 17,
                        "end": 18,
                        "text": "3",
                        "ref_id": "TABREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Comparison against Conventional Sentiment Lexicon",
                "sec_num": "3.1"
            },
            {
                "text": "1 The sub-graph #2, based on the semantic parallelism of coordination, is simple and yet very powerful as an inductive bias.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Comparison against Conventional Sentiment Lexicon",
                "sec_num": "3.1"
            },
            {
                "text": "2 The performance of graph propagation varies significantly depending on the graph topology and the corresponding edge weights.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Comparison against Conventional Sentiment Lexicon",
                "sec_num": "3.1"
            },
            {
                "text": "Note that a direct comparison against ILP for top N words is tricky, as ILP does not rank results.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Comparison against Conventional Sentiment Lexicon",
                "sec_num": "3.1"
            },
            {
                "text": "Only for comparison purposes however, we assign ranks based on the frequency of words for ILP. Because of this issue, the performance of top \u223c1k words of ILP should be considered only as a conservative measure. Importantly, when evaluated over more than top 5k words, ILP is overall the top performer considering both precision (shown in Table 3 ) and coverage (omitted for brevity).12 4 Precision, Coverage, and Efficiency",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 344,
                        "end": 345,
                        "text": "3",
                        "ref_id": "TABREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Comparison against Conventional Sentiment Lexicon",
                "sec_num": "3.1"
            },
            {
                "text": "In this section, we address three important aspects of an ideal induction algorithm: precision, coverage, and efficiency. For brevity, the remainder of the paper will focus on the algorithms based on constraint optimization, as it turned out to be the most effective one from the empirical results in \u00a73.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Comparison against Conventional Sentiment Lexicon",
                "sec_num": "3.1"
            },
            {
                "text": "Precision In order to see the effectiveness of the induction algorithms more sharply, we had used a limited set of seed words in \u00a73. However to build a lexicon with substantially enhanced precision, we will use as a large seed set as possible, e.g., entire sentiment lexicons 13 .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Comparison against Conventional Sentiment Lexicon",
                "sec_num": "3.1"
            },
            {
                "text": "Broad coverage Although statistics in Google 1T corpus represent a very large amount of text, words that appear in pred-arg and coordination relations are still limited. To substantially increase the coverage, we will leverage dictionary words (that are not in the corpus) as described in \u00a72.3 and Figure 2 .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 305,
                        "end": 306,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Comparison against Conventional Sentiment Lexicon",
                "sec_num": "3.1"
            },
            {
                "text": "Efficiency One practical problem with ILP is efficiency and scalability. In particular, we found that it becomes nearly impractical to run the ILP formulation including all words in WordNet plus all words in the argument position in Google Web 1T. We therefore explore an alternative approach based on Linear Programming in what follows.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Comparison against Conventional Sentiment Lexicon",
                "sec_num": "3.1"
            },
            {
                "text": "One straightforward option for Linear Programming formulation may seem like using the same Integer Linear Programming formulation introduced in \u00a72.3, only changing the variable definitions to be real values \u2208 [0, 1] rather than integers. However, because the hard constraints in \u00a72.3 are defined based on the assumption that all the variables are binary integers, those constraints are not as meaningful when considered for real numbers. Therefore we revise those hard constraints to encode various semantic relations (WordNet and semantic coordination) more directly.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Induction using Linear Programming",
                "sec_num": "4.1"
            },
            {
                "text": "For each word i, we define variables x i , y i , z i \u2208 [0, 1]. i has a positive (negative) connotation if and only if the x i (y i ) is assigned the greatest value among the three variables; otherwise, i is neutral.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Definition of variables:",
                "sec_num": null
            },
            {
                "text": "Objective function: We aim to maximize:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Definition of variables:",
                "sec_num": null
            },
            {
                "text": "F = \u03a6 prosody + \u03a6 coord + \u03a6 syn + \u03a6 ant + \u03a6 neu \u03a6 prosody = R pred + i,j w pred + i,j \u2022 xj + R pred - i,j w pred - i,j",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Definition of variables:",
                "sec_num": null
            },
            {
                "text": "\u2022 yj",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Definition of variables:",
                "sec_num": null
            },
            {
                "text": "\u03a6 coord = R coord i,j w coord i,j \u2022 (dc ++ i,j + dc -- i,j ) \u03a6 syn = W syn R syn i,j (ds ++ i,j + ds -- i,j ) \u03a6 ant = W ant R ant i,j (da ++ i,j + da -- i,j ) \u03a6 neu = \u03b1 R pred i,j",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Definition of variables:",
                "sec_num": null
            },
            {
                "text": "w pred i,j",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Definition of variables:",
                "sec_num": null
            },
            {
                "text": "\u2022 zj",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Definition of variables:",
                "sec_num": null
            },
            {
                "text": "Hard constraints We add penalties to the objective function if the polarity of a pair of words is not consistent with its corresponding semantic relations. For example, for synonyms i and j, we introduce a penalty W syn (a positive constant) for ds ++ i,j , ds -- i,j \u2208 [-1, 0], where we set the upper bound of ds ++ i,j (ds -- i,j ) as the signed distance of x i and x j (y i and y j ) as shown below:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Definition of variables:",
                "sec_num": null
            },
            {
                "text": "For (i, j) \u2208 R syn , ds ++ i,j \u2264 x i -x j , ds ++ i,j \u2264 x j -x i ds -- i,j \u2264 y i -y j , ds -- i,j \u2264 y j -y i Notice that ds ++ i,j",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Definition of variables:",
                "sec_num": null
            },
            {
                "text": ", ds -- i,j satisfying above inequalities will be always of negative values, hence in order to maximize the objective function, the LP solver will try to minimize the absolute values of ds ++ i,j , ds -- i,j , effectively pushing i and j toward the same polarity. Constraints for semantic coordination R coord can be defined similarly. Lastly, following constraints encode antonym relations:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Definition of variables:",
                "sec_num": null
            },
            {
                "text": "For (i, j) \u2208 R ant , da ++ i,j \u2264 x i -(1 -x j ), da ++ i,j \u2264 (1 -x j ) -x i da -- i,j \u2264 y i -(1 -y j ), da -- i,j \u2264 (1 -y j ) -y i",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Definition of variables:",
                "sec_num": null
            },
            {
                "text": "Interpretation Unlike ILP, some of the variables result in fractional values. We consider a word has positive or negative polarity only if the assignment indicates 1 for the corresponding polarity and 0 for the rest. In other words, we treat all words with fractional assignments over different polarities as neutral. Because the optimal solutions of LP correspond to extreme points in the convex polytope formed by the constraints, we obtain a large portion of words with non-fractional assignments toward non-neutral polarities. Alternatively, one can round up fractional values.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Definition of variables:",
                "sec_num": null
            },
            {
                "text": "To solve the ILP/LP, we run ILOG CPLEX Optimizer (CPLEX, 2009)) on a 3.5GHz 6 core CPU machine with 96GB RAM. Efficiency-wise, LP runs within 10 minutes while ILP takes several hours. Table 4 shows the results evaluated against MPQA for different variations of ILP and LP. We find that LP variants much better recall and F-score, while maintaining comparable precision.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 190,
                        "end": 191,
                        "text": "4",
                        "ref_id": "TABREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Empirical Comparisons: ILP v.s. LP",
                "sec_num": "4.2"
            },
            {
                "text": "Therefore, we choose the connotation lexicon by LP (C-LP) in the following evaluations in \u00a75.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Empirical Comparisons: ILP v.s. LP",
                "sec_num": "4.2"
            },
            {
                "text": "In this section, we present comprehensive intrinsic \u00a75.1 and extrinsic \u00a75.2 evaluations comparing three representative lexicons from \u00a72 & \u00a74: C-LP, OVERLAY, PRED-ARG (CP), and two popular sentiment lexicons: SentiWordNet (Baccianella et al., 2010) and GI+MPQA. 14 Note that C-LP is the largest among all connotation lexicons, including \u223c70,000 polar words. 15",
                "cite_spans": [
                    {
                        "start": 221,
                        "end": 247,
                        "text": "(Baccianella et al., 2010)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Results II",
                "sec_num": "5"
            },
            {
                "text": "We evaluate 4000 words 16 using Amazon Mechanical Turk (AMT). Because we expect that judging a connotation can be dependent on one's cultural background, personality and value systems, we gather judgements from 5 people for each word, from which we hope to draw a more general judgement of connotative polarity. About 300 unique Turkers participated the evaluation tasks. We gather gold standard only for those words for which more than half of the judges agreed on the same polarity. Otherwise we treat them as ambiguous cases. 17 Figure 3 shows a part of the AMT task, where Turkers are presented with questions that help judges to determine the subtle connotative polarity of each word, then asked to rate the degree of connotation on a scale from -5 (most negative) and 5 (most positive). To draw 14 GI+MPQA is the union of General Inquirer and MPQA. The GI, we use words in the \"Positiv\" & \"Negativ\" set. For SentiWordNet, to retrieve the polarity of a given word, we sum over the polarity scores over all senses, where positive (negative) values correspond to positive (negative) polarity.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 539,
                        "end": 540,
                        "text": "3",
                        "ref_id": "FIGREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Intrinsic Evaluation: Human Judgements",
                "sec_num": "5.1"
            },
            {
                "text": "15 \u223c13k adj, \u223c6k verbs, \u223c28k nouns, \u223c22k proper nouns. 16 We choose words that are not already in GI+MPQA and obtain most frequent 10,000 words based on the unigram frequency in Google-Ngram, then randomly select 4000 words. 17 We allow Turkers to mark words that can be used with both positive and negative connotation, which results in about 7% of words that are excluded from the gold standard set. the gold standard, we consider two different voting schemes:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Intrinsic Evaluation: Human Judgements",
                "sec_num": "5.1"
            },
            {
                "text": "\u2022 \u2126 V ote : The judgement of each Turker is mapped to neutral for -1 \u2264 score \u2264 1, positive for score \u2265 2, negative for score \u2264 2, then we take the majority vote.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Intrinsic Evaluation: Human Judgements",
                "sec_num": "5.1"
            },
            {
                "text": "\u2022 \u2126 Score : Let \u03c3(i) be the sum (weighted vote) of the scores given by 5 judges for word i.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Intrinsic Evaluation: Human Judgements",
                "sec_num": "5.1"
            },
            {
                "text": "Then we determine the polarity label l(i) of i as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Intrinsic Evaluation: Human Judgements",
                "sec_num": "5.1"
            },
            {
                "text": "l(i) = \uf8f1 \uf8f2 \uf8f3 positive if \u03c3(i) > 1 negative if \u03c3(i) < -1 neutral if -1 \u2264 \u03c3(i) \u2264 1",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Intrinsic Evaluation: Human Judgements",
                "sec_num": "5.1"
            },
            {
                "text": "The resulting distribution of judgements is shown in Table 5 & 6. Interestingly, we observe that among the relatively frequently used English words, there are overwhelmingly more positively connotative words than negative ones.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 59,
                        "end": 60,
                        "text": "5",
                        "ref_id": "TABREF5"
                    }
                ],
                "eq_spans": [],
                "section": "Intrinsic Evaluation: Human Judgements",
                "sec_num": "5.1"
            },
            {
                "text": "In Table 7 , we show the percentage of words with the same label over the mutual words by the two lexicon. The highest agreement is 77% by C-LP and the gold standard by AMT V ote . How good is this? It depends on what is the natural degree of agreement over subtle connotation among people. Therefore, we also report the degree of agreement among human judges in Table 7 , where we compute the agreement of one Turker with respect to the gold standard drawn from the rest of the Turkers, and take the average across over all five Turkers 18 . Interestingly, the performance of 18 In order to draw the gold standard from the 4 remaining Turkers, we consider adjusted versions of \u2126 V ote and \u2126 Score schemes described above.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 9,
                        "end": 10,
                        "text": "7",
                        "ref_id": "TABREF7"
                    },
                    {
                        "start": 369,
                        "end": 370,
                        "text": "7",
                        "ref_id": "TABREF7"
                    }
                ],
                "eq_spans": [],
                "section": "Intrinsic Evaluation: Human Judgements",
                "sec_num": "5.1"
            },
            {
                "text": "POS NEG NEU UNDETERMINED \u2126 V ote 50.4 14.6 24.1 10.9 \u2126 Score 67.9 20.6 11.5 n/a Turkers is not as good as that of C-LP lexicon. We conjecture that this could be due to generally varying perception of different people on the connotative polarity,19 while the corpus-driven induction algorithms focus on the general connotative polarity corresponding to the most prevalent senses of words in the corpus.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Intrinsic Evaluation: Human Judgements",
                "sec_num": "5.1"
            },
            {
                "text": "We conduct lexicon-based binary sentiment classification on the following two corpora.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Extrinsic Evaluation",
                "sec_num": "5.2"
            },
            {
                "text": "SemEval From the SemEval task, we obtain a set of news headlines with annotated scores (ranging from -100 to 87). The positive/negative scores indicate the degree of positive/negative polarity orientation. We construct several sets of the positive and negative texts by setting thresholds on the scores as shown in Table 8 . \"\u2276 n\" indicates that the positive set consists of the texts with scores \u2265 n and the negative set consists of the texts with scores \u2264 -n.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 321,
                        "end": 322,
                        "text": "8",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Extrinsic Evaluation",
                "sec_num": "5.2"
            },
            {
                "text": "The sentiment Twitter data20 consists of tweets containing either a smiley emoticon (positive sentiment) or a frowny emoticon (negative sentiment). We filter out the tweets with question marks or more than 30 words, and keep the ones with at least two words in the union of all polar words in the five lexicons in Table 8 , and then randomly select 10000 per class.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 320,
                        "end": 321,
                        "text": "8",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Emoticon tweets",
                "sec_num": null
            },
            {
                "text": "We denote the short text (e.g., content of tweets or headline texts from SemEval) by t. w represents the word in t. W + /W -is the set of posi- tive/negative words of the lexicon. We define the weight of w as s(w). If w is adjective, s(w) = 2; otherwise s(w) = 1. Then the polarity of each text is determined as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Emoticon tweets",
                "sec_num": null
            },
            {
                "text": "pol(t) = \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f3 positive if W + w\u2208t s(w) \u2265 W - w\u2208t s(w) negative if W + w\u2208t s(w) < W - w\u2208t s(w)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Emoticon tweets",
                "sec_num": null
            },
            {
                "text": "As shown in Table 8 , C-LP generally performs better than the other lexicons on both corpora. Considering that only very simple classification strategy is applied, the result by the connotation lexicon is quite promising.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 18,
                        "end": 19,
                        "text": "8",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Emoticon tweets",
                "sec_num": null
            },
            {
                "text": "Finally, Table 1 highlights interesting examples of proper nouns with connotative polarity, e.g., \"Mandela\", \"Google\", \"Hawaii\" with positive connotation, and \"Monsanto\", \"Halliburton\", \"Enron\" with negative connotation, suggesting that our algorithms could potentially serve as a proxy to track the general connotation of real world entities. Table 2 shows example common nouns with connotative polarity.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 15,
                        "end": 16,
                        "text": "1",
                        "ref_id": "TABREF0"
                    },
                    {
                        "start": 350,
                        "end": 351,
                        "text": "2",
                        "ref_id": "TABREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Emoticon tweets",
                "sec_num": null
            },
            {
                "text": "In this work we aim to find the polarity of most prevalent senses of each word, in part because it is not easy to perform unsupervised word sense disambiguation (WSD) on a large corpus in a reliable way, especially when the corpus consists primarily of short n-grams. Although the resulting lexicon loses on some of the polysemous words with potentially opposite polarities, per-word connotation (rather than per-sense connotation) does have a practical value: it provides a convenient option for users who wish to avoid the burden of WSD before utilizing the lexicon. Future work includes handling of WSD and multi-word expressions (MWEs), e.g., \"Great Leader\" (for Kim Jong-Il), \"Inglourious Basterds\" (a movie title). ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Practical Remarks on WSD and MWEs",
                "sec_num": "5.3"
            },
            {
                "text": "A very interesting work of Mohammad and Turney (2010) uses Mechanical Turk in order to build the lexicon of emotions evoked by words. In contrast, we present an automatic approach that infers the general connotation of words. Velikovich et al. (2010) use graph propagation algorithms for constructing a web-scale polarity lexicon for sentiment analysis. Although we employ the same graph propagation algorithm, our graph construction is fundamentally different in that we integrate stronger inductive biases into the graph topology and the corresponding edge weights. As shown in our experimental results, we find that judicious construction of graph structure, exploiting multiple complementing linguistic phenomena can enhance both the performance and the efficiency of the algorithm substantially. Other interesting approaches include one based on min-cut (Dong et al., 2012) or LDA (Xie and Li, 2012) . Our proposed approaches are more suitable for encoding a much diverse set of linguistic phenomena however. But our work use a few seed predicates with selectional preference instead of relying on word similarity. Some recent work explored the use of constraint optimization framework for inducing domain-dependent sentiment lexicon (Choi and Cardie (2009) , Lu et al. (2011) ). Our work differs in that we provide comprehensive insights into different formulations of ILP and LP, aiming to learn the much different task of learning the general connotation of words.",
                "cite_spans": [
                    {
                        "start": 27,
                        "end": 53,
                        "text": "Mohammad and Turney (2010)",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 226,
                        "end": 250,
                        "text": "Velikovich et al. (2010)",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 859,
                        "end": 878,
                        "text": "(Dong et al., 2012)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 886,
                        "end": 904,
                        "text": "(Xie and Li, 2012)",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 1239,
                        "end": 1262,
                        "text": "(Choi and Cardie (2009)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 1265,
                        "end": 1281,
                        "text": "Lu et al. (2011)",
                        "ref_id": "BIBREF15"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "6"
            },
            {
                "text": "We presented a broad-coverage connotation lexicon that determines the subtle nuanced sentiment of even those words that are objective on the surface, including the general connotation of realworld named entities. Via a comprehensive evaluation, we provided empirical insights into three different types of induction algorithms, and proposed one with good precision, coverage, and efficiency.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "7"
            },
            {
                "text": "Our learned lexicon correctly assigns negative polarity to emission.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "A notable exception would be the work ofFeng et al.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "In general, polysemous words do not seem to have conflicting non-neutral polarities over different senses, though there are many exceptions, e.g., \"heat\", or \"fine\". We treat each word in each part-of-speech as a separate word to reduce such cases, otherwise aim to learn the most prevalent polarity in the corpus with respect to each part-of-speech of each",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "word.5 Available at http://www.cs.stonybrook.edu/ \u02dcychoi/connotation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "For connotative predicates, we use the seed predicate set ofFeng et al. (2011), which comprises of 20 positive and 20 negative predicates.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "We restrict predicte-argument pairs to verb-object pairs in this study. Note that Google Web 1T dataset consists of n-grams upto n = 5. Since n-gram sequences are too short to apply a parser, we extract verb-object pairs approximately by matching part-of-speech tags. Empirically, when overlaid with the second sub-graph, we found that it is better to keep the connectivity of this sub-graph as uni-directional. That is, we only allow edges to go from a predicate to an argument.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "We discard edges with cosine similarity \u2264 0, as those indicate either independence or the opposite of similarity.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Note that cosine similarity does not make sense for the first sub-graph as there is no reason why a predicate and an argument should be distributionally similar. We experimented with many different variations on the graph structure and edge weights, including ones that include any word pairs that occurred frequently enough together. For brevity, we present the version that achieved the best results here.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "We consider \"positive\" and \"negative\" polarities conflict, but \"neutral\" polarity does not conflict with any.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "In the case of General Inquirer, we use words in POSITIV and NEGATIV sets as words with positive and negative labels respectively.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "In fact, the performance of PRED-ARG variants for top 10K w.r.t. GENINQ is not meaningful as no additional word was matched beyond top 5k words.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Note that doing so will prevent us from evaluating against the same sentiment lexicon used as a seed set.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Pearson correlation coefficient among turkers is 0.28, which corresponds to a positive small to medium correlation. Note that when the annotation of turkers is aggregated, we observe agreement as high as 77% with respect to the learned connotation lexicon.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "http://www.stanford.edu/ \u02dcalecmgo/ cs224n/twitterdata.2009.05.25.c.zip",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "This research was supported in part by the Stony Brook University Office of the Vice President for Research. We thank reviewers for many insightful comments and suggestions, and for providing us with several very inspiring examples to work with.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgments",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining",
                "authors": [
                    {
                        "first": "Stefano",
                        "middle": [],
                        "last": "Baccianella",
                        "suffix": ""
                    },
                    {
                        "first": "Andrea",
                        "middle": [],
                        "last": "Esuli",
                        "suffix": ""
                    },
                    {
                        "first": "Fabrizio",
                        "middle": [],
                        "last": "Sebastiani",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC'10)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas- tiani. 2010. Sentiwordnet 3.0: An enhanced lexi- cal resource for sentiment analysis and opinion min- ing. In Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC'10), Valletta, Malta, may. European Lan- guage Resources Association (ELRA).",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Syntactic persistence in language production",
                "authors": [
                    {
                        "first": "J",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Kathryn",
                        "middle": [],
                        "last": "Bock",
                        "suffix": ""
                    }
                ],
                "year": 1986,
                "venue": "Cognitive psychology",
                "volume": "18",
                "issue": "3",
                "pages": "355--387",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "J. Kathryn Bock. 1986. Syntactic persistence in language production. Cognitive psychology, 18(3):355-387.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "{Web 1T 5-gram Version 1}",
                "authors": [
                    {
                        "first": "Thorsten",
                        "middle": [],
                        "last": "Brants",
                        "suffix": ""
                    },
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Franz",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Thorsten Brants and Alex Franz. 2006. {Web 1T 5- gram Version 1}.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Adapting a polarity lexicon using integer linear programming for domain-specific sentiment classification",
                "authors": [
                    {
                        "first": "Yejin",
                        "middle": [],
                        "last": "Choi",
                        "suffix": ""
                    },
                    {
                        "first": "Claire",
                        "middle": [],
                        "last": "Cardie",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing",
                "volume": "2",
                "issue": "",
                "pages": "590--598",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yejin Choi and Claire Cardie. 2009. Adapting a po- larity lexicon using integer linear programming for domain-specific sentiment classification. In Pro- ceedings of the 2009 Conference on Empirical Meth- ods in Natural Language Processing: Volume 2 - Volume 2, EMNLP '09, pages 590-598, Strouds- burg, PA, USA. Association for Computational Lin- guistics.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Word association norms, mutual information, and lexicography",
                "authors": [
                    {
                        "first": "Kenneth",
                        "middle": [
                            "Ward"
                        ],
                        "last": "Church",
                        "suffix": ""
                    },
                    {
                        "first": "Patrick",
                        "middle": [],
                        "last": "Hanks",
                        "suffix": ""
                    }
                ],
                "year": 1990,
                "venue": "Comput. Linguist",
                "volume": "16",
                "issue": "",
                "pages": "22--29",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kenneth Ward Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicog- raphy. Comput. Linguist., 16:22-29, March.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "High-performance software for mathematical programming and optimization. U RL",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Ilog Cplex",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "ILOG CPLEX. 2009. High-performance software for mathematical programming and optimization. U RL http://www.ilog.com/products/cplex.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Semi-supervised recognition of sarcastic sentences in twitter and amazon",
                "authors": [
                    {
                        "first": "Dmitry",
                        "middle": [],
                        "last": "Davidov",
                        "suffix": ""
                    },
                    {
                        "first": "Oren",
                        "middle": [],
                        "last": "Tsur",
                        "suffix": ""
                    },
                    {
                        "first": "Ari",
                        "middle": [],
                        "last": "Rappoport",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the Fourteenth Conference on Computational Natural Language Learning, CoNLL '10",
                "volume": "",
                "issue": "",
                "pages": "107--116",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010. Semi-supervised recognition of sarcastic sentences in twitter and amazon. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning, CoNLL '10, pages 107-116, Stroudsburg, PA, USA. Association for Computa- tional Linguistics.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Setsimilarity joins based semi-supervised sentiment analysis",
                "authors": [
                    {
                        "first": "Xishuang",
                        "middle": [],
                        "last": "Dong",
                        "suffix": ""
                    },
                    {
                        "first": "Qibo",
                        "middle": [],
                        "last": "Zou",
                        "suffix": ""
                    },
                    {
                        "first": "Yi",
                        "middle": [],
                        "last": "Guan",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Neural Information Processing",
                "volume": "",
                "issue": "",
                "pages": "176--183",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xishuang Dong, Qibo Zou, and Yi Guan. 2012. Set- similarity joins based semi-supervised sentiment analysis. In Neural Information Processing, pages 176-183. Springer.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Sentiwordnet: A publicly available lexical resource for opinion mining",
                "authors": [
                    {
                        "first": "Andrea",
                        "middle": [],
                        "last": "Esuli",
                        "suffix": ""
                    },
                    {
                        "first": "Fabrizio",
                        "middle": [],
                        "last": "Sebastiani",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Proceedings of the 5th Conference on Language Resources and Evaluation (LREC06)",
                "volume": "",
                "issue": "",
                "pages": "417--422",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Andrea Esuli and Fabrizio Sebastiani. 2006. Sen- tiwordnet: A publicly available lexical resource for opinion mining. In In Proceedings of the 5th Conference on Language Resources and Evaluation (LREC06), pages 417-422.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Learning general connotation of words using graph-based algorithms",
                "authors": [
                    {
                        "first": "Song",
                        "middle": [],
                        "last": "Feng",
                        "suffix": ""
                    },
                    {
                        "first": "Ritwik",
                        "middle": [],
                        "last": "Bose",
                        "suffix": ""
                    },
                    {
                        "first": "Yejin",
                        "middle": [],
                        "last": "Choi",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "1092--1103",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Song Feng, Ritwik Bose, and Yejin Choi. 2011. Learn- ing general connotation of words using graph-based algorithms. In Proceedings of the Conference on Empirical Methods in Natural Language Process- ing, pages 1092-1103. Association for Computa- tional Linguistics.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "More than words: Syntactic packaging and implicit sentiment",
                "authors": [
                    {
                        "first": "Stephan",
                        "middle": [],
                        "last": "Greene",
                        "suffix": ""
                    },
                    {
                        "first": "Philip Resnik ; Vasileios",
                        "middle": [],
                        "last": "Hatzivassiloglou",
                        "suffix": ""
                    },
                    {
                        "first": "Kathleen",
                        "middle": [
                            "R"
                        ],
                        "last": "Mckeown",
                        "suffix": ""
                    }
                ],
                "year": 1997,
                "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "174--181",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Stephan Greene and Philip Resnik. 2009. More than words: Syntactic packaging and implicit sentiment. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Lin- guistics, pages 503-511, Boulder, Colorado, June. Association for Computational Linguistics. Vasileios Hatzivassiloglou and Kathleen R McKeown. 1997. Predicting the semantic orientation of adjec- tives. In Proceedings of the eighth conference on European chapter of the Association for Computa- tional Linguistics, pages 174-181. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Sentiment lexicon creation from lexical resources",
                "authors": [
                    {
                        "first": "Bas",
                        "middle": [],
                        "last": "Heerschop",
                        "suffix": ""
                    },
                    {
                        "first": "Alexander",
                        "middle": [],
                        "last": "Hogenboom",
                        "suffix": ""
                    },
                    {
                        "first": "Flavius",
                        "middle": [],
                        "last": "Frasincar",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Business Information Systems",
                "volume": "",
                "issue": "",
                "pages": "185--196",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Bas Heerschop, Alexander Hogenboom, and Flavius Frasincar. 2011. Sentiment lexicon creation from lexical resources. In Business Information Systems, pages 185-196. Springer.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Building lexicon for sentiment analysis from massive collection of html documents",
                "authors": [
                    {
                        "first": "Nobuhiro",
                        "middle": [],
                        "last": "Kaji",
                        "suffix": ""
                    },
                    {
                        "first": "Masaru",
                        "middle": [],
                        "last": "Kitsuregawa",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Nobuhiro Kaji and Masaru Kitsuregawa. 2007. Build- ing lexicon for sentiment analysis from massive col- lection of html documents. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL).",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Authoritative sources in a hyperlinked environment",
                "authors": [
                    {
                        "first": "Jon",
                        "middle": [
                            "M"
                        ],
                        "last": "Kleinberg",
                        "suffix": ""
                    }
                ],
                "year": 1999,
                "venue": "JOURNAL OF THE ACM",
                "volume": "46",
                "issue": "5",
                "pages": "604--632",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jon M. Kleinberg. 1999. Authoritative sources in a hy- perlinked environment. JOURNAL OF THE ACM, 46(5):604-632.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Irony in the text or insincerity in the writer. Text and technology: In honour of John Sinclair",
                "authors": [
                    {
                        "first": "Bill",
                        "middle": [],
                        "last": "Louw",
                        "suffix": ""
                    }
                ],
                "year": 1993,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "157--176",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Bill Louw. 1993. Irony in the text or insincerity in the writer. Text and technology: In honour of John Sinclair, pages 157-176.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Automatic construction of a context-aware sentiment lexicon: an optimization approach",
                "authors": [
                    {
                        "first": "Yue",
                        "middle": [],
                        "last": "Lu",
                        "suffix": ""
                    },
                    {
                        "first": "Malu",
                        "middle": [],
                        "last": "Castellanos",
                        "suffix": ""
                    },
                    {
                        "first": "Umeshwar",
                        "middle": [],
                        "last": "Dayal",
                        "suffix": ""
                    },
                    {
                        "first": "Chengxiang",
                        "middle": [],
                        "last": "Zhai",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Proceedings of the 20th international conference on World wide web",
                "volume": "",
                "issue": "",
                "pages": "347--356",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yue Lu, Malu Castellanos, Umeshwar Dayal, and ChengXiang Zhai. 2011. Automatic construction of a context-aware sentiment lexicon: an optimiza- tion approach. In Proceedings of the 20th interna- tional conference on World wide web, pages 347- 356. ACM.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Emotions evoked by common words and phrases: Using mechanical turk to create an emotion lexicon",
                "authors": [
                    {
                        "first": "Saif",
                        "middle": [],
                        "last": "Mohammad",
                        "suffix": ""
                    },
                    {
                        "first": "Peter",
                        "middle": [],
                        "last": "Turney",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text",
                "volume": "",
                "issue": "",
                "pages": "26--34",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Saif Mohammad and Peter Turney. 2010. Emotions evoked by common words and phrases: Using me- chanical turk to create an emotion lexicon. In Pro- ceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Genera- tion of Emotion in Text, pages 26-34, Los Angeles, CA, June. Association for Computational Linguis- tics.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Random walk weighting over sentiwordnet for sentiment polarity detection on twitter",
                "authors": [
                    {
                        "first": "Arturo",
                        "middle": [],
                        "last": "Montejo-R\u00e1ez",
                        "suffix": ""
                    },
                    {
                        "first": "Eugenio",
                        "middle": [],
                        "last": "Mart\u00ednez-C\u00e1mara",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Teresa Mart\u00edn-Valdivia",
                        "suffix": ""
                    },
                    {
                        "first": "L",
                        "middle": [],
                        "last": "Alfonso Ure\u00f1a L\u00f3pez",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Proceedings of the 3rd Workshop in Computational Approaches to Subjectivity and Sentiment Analysis",
                "volume": "",
                "issue": "",
                "pages": "3--10",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Arturo Montejo-R\u00e1ez, Eugenio Mart\u00ednez-C\u00e1mara, M. Teresa Mart\u00edn-Valdivia, and L. Alfonso Ure\u00f1a L\u00f3pez. 2012. Random walk weighting over sen- tiwordnet for sentiment polarity detection on twit- ter. In Proceedings of the 3rd Workshop in Com- putational Approaches to Subjectivity and Sentiment Analysis, pages 3-10, Jeju, Korea, July. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "The pagerank citation ranking: Bringing order to the web",
                "authors": [
                    {
                        "first": "Lawrence",
                        "middle": [],
                        "last": "Page",
                        "suffix": ""
                    },
                    {
                        "first": "Sergey",
                        "middle": [],
                        "last": "Brin",
                        "suffix": ""
                    },
                    {
                        "first": "Rajeev",
                        "middle": [],
                        "last": "Motwani",
                        "suffix": ""
                    },
                    {
                        "first": "Terry",
                        "middle": [],
                        "last": "Winograd",
                        "suffix": ""
                    }
                ],
                "year": 1999,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1999. The pagerank citation rank- ing: Bringing order to the web. Technical Report 1999-66, Stanford InfoLab, November.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Opinion mining and sentiment analysis",
                "authors": [
                    {
                        "first": "Bo",
                        "middle": [],
                        "last": "Pang",
                        "suffix": ""
                    },
                    {
                        "first": "Lillian",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Found. Trends Inf. Retr",
                "volume": "2",
                "issue": "1-2",
                "pages": "1--135",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Found. Trends Inf. Retr., 2(1- 2):1-135.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "The representation of verbs: Evidence from syntactic priming in language production",
                "authors": [
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Martin",
                        "suffix": ""
                    },
                    {
                        "first": "Holly",
                        "middle": [
                            "P"
                        ],
                        "last": "Pickering",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Branigan",
                        "suffix": ""
                    }
                ],
                "year": 1998,
                "venue": "Journal of Memory and Language",
                "volume": "39",
                "issue": "4",
                "pages": "633--651",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Martin J Pickering and Holly P Branigan. 1998. The representation of verbs: Evidence from syntactic priming in language production. Journal of Mem- ory and Language, 39(4):633-651.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Expanding domain sentiment lexicon through double propagation",
                "authors": [
                    {
                        "first": "Guang",
                        "middle": [],
                        "last": "Qiu",
                        "suffix": ""
                    },
                    {
                        "first": "Bing",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Jiajun",
                        "middle": [],
                        "last": "Bu",
                        "suffix": ""
                    },
                    {
                        "first": "Chun",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of the 21st international jont conference on Artifical intelligence, IJCAI'09",
                "volume": "",
                "issue": "",
                "pages": "1199--1204",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2009. Expanding domain sentiment lexicon through double propagation. In Proceedings of the 21st in- ternational jont conference on Artifical intelligence, IJCAI'09, pages 1199-1204, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "A linear programming formulation for global inference in natural language tasks",
                "authors": [
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Roth",
                        "suffix": ""
                    },
                    {
                        "first": "Wen-Tau",
                        "middle": [],
                        "last": "Yih",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dan Roth and Wen-tau Yih. 2004. A linear program- ming formulation for global inference in natural lan- guage tasks. Defense Technical Information Center.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Corpus, concordance, collocation. Describing English language",
                "authors": [
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Sinclair",
                        "suffix": ""
                    }
                ],
                "year": 1991,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "John Sinclair. 1991. Corpus, concordance, colloca- tion. Describing English language. Oxford Univer- sity Press.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Collostructions: Investigating the interaction of words and constructions",
                "authors": [
                    {
                        "first": "Anatol",
                        "middle": [],
                        "last": "Stefanowitsch",
                        "suffix": ""
                    },
                    {
                        "first": "Stefan",
                        "middle": [],
                        "last": "Th",
                        "suffix": ""
                    },
                    {
                        "first": "Gries",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "International journal of corpus linguistics",
                "volume": "8",
                "issue": "2",
                "pages": "209--243",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Anatol Stefanowitsch and Stefan Th Gries. 2003. Col- lostructions: Investigating the interaction of words and constructions. International journal of corpus linguistics, 8(2):209-243.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "A computer approach to content analysis: studies using the general inquirer system",
                "authors": [
                    {
                        "first": "Philip",
                        "middle": [
                            "J"
                        ],
                        "last": "Stone",
                        "suffix": ""
                    },
                    {
                        "first": "Earl",
                        "middle": [
                            "B"
                        ],
                        "last": "Hunt",
                        "suffix": ""
                    }
                ],
                "year": 1963,
                "venue": "Proceedings of the May 21-23, 1963, spring joint computer conference, AFIPS '63 (Spring)",
                "volume": "",
                "issue": "",
                "pages": "241--256",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Philip J. Stone and Earl B. Hunt. 1963. A computer approach to content analysis: studies using the gen- eral inquirer system. In Proceedings of the May 21- 23, 1963, spring joint computer conference, AFIPS '63 (Spring), pages 241-256, New York, NY, USA. ACM.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Collocations and semantic profiles: on the cause of the trouble with quantitative studies",
                "authors": [
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Stubbs",
                        "suffix": ""
                    }
                ],
                "year": 1995,
                "venue": "Functions of language",
                "volume": "2",
                "issue": "1",
                "pages": "23--55",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Michael Stubbs. 1995. Collocations and semantic pro- files: on the cause of the trouble with quantitative studies. Functions of language, 2(1):23-55.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Enriching the knowledge sources used in a maximum entropy part-of-speech tagger",
                "authors": [
                    {
                        "first": "Kristina",
                        "middle": [],
                        "last": "Toutanova",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2000,
                "venue": "EMNLP/VLC 2000",
                "volume": "",
                "issue": "",
                "pages": "63--70",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kristina Toutanova and Christopher D. Manning. 2000. Enriching the knowledge sources used in a maximum entropy part-of-speech tagger. In In EMNLP/VLC 2000, pages 63-70.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Mining the web for synonyms: Pmi-ir versus lsa on toefl",
                "authors": [
                    {
                        "first": "Peter",
                        "middle": [],
                        "last": "Turney",
                        "suffix": ""
                    }
                ],
                "year": 2001,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Peter Turney. 2001. Mining the web for synonyms: Pmi-ir versus lsa on toefl.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "The viability of web-derived polarity lexicons",
                "authors": [
                    {
                        "first": "Leonid",
                        "middle": [],
                        "last": "Velikovich",
                        "suffix": ""
                    },
                    {
                        "first": "Sasha",
                        "middle": [],
                        "last": "Blair-Goldensohn",
                        "suffix": ""
                    },
                    {
                        "first": "Kerry",
                        "middle": [],
                        "last": "Hannan",
                        "suffix": ""
                    },
                    {
                        "first": "Ryan",
                        "middle": [],
                        "last": "Mcdonald",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Hannan, and Ryan McDonald. 2010. The via- bility of web-derived polarity lexicons. In Human Language Technologies: The 2010 Annual Confer- ence of the North American Chapter of the Associa- tion for Computational Linguistics. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "Annotating expressions of opinions and emotions in language. Language Resources and Evaluation (formerly",
                "authors": [
                    {
                        "first": "Janyce",
                        "middle": [],
                        "last": "Wiebe",
                        "suffix": ""
                    },
                    {
                        "first": "Theresa",
                        "middle": [],
                        "last": "Wilson",
                        "suffix": ""
                    },
                    {
                        "first": "Claire",
                        "middle": [],
                        "last": "Cardie",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Computers and the Humanities)",
                "volume": "39",
                "issue": "2/3",
                "pages": "164--210",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emo- tions in language. Language Resources and Eval- uation (formerly Computers and the Humanities), 39(2/3):164-210.",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "Opinionfinder: a system for subjectivity analysis",
                "authors": [
                    {
                        "first": "Theresa",
                        "middle": [],
                        "last": "Wilson",
                        "suffix": ""
                    },
                    {
                        "first": "Paul",
                        "middle": [],
                        "last": "Hoffmann",
                        "suffix": ""
                    },
                    {
                        "first": "Swapna",
                        "middle": [],
                        "last": "Somasundaran",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Kessler",
                        "suffix": ""
                    },
                    {
                        "first": "Janyce",
                        "middle": [],
                        "last": "Wiebe",
                        "suffix": ""
                    },
                    {
                        "first": "Yejin",
                        "middle": [],
                        "last": "Choi",
                        "suffix": ""
                    },
                    {
                        "first": "Claire",
                        "middle": [],
                        "last": "Cardie",
                        "suffix": ""
                    },
                    {
                        "first": "Ellen",
                        "middle": [],
                        "last": "Riloff",
                        "suffix": ""
                    },
                    {
                        "first": "Siddharth",
                        "middle": [],
                        "last": "Patwardhan",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of HLT/EMNLP on Interactive Demonstrations",
                "volume": "",
                "issue": "",
                "pages": "34--35",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Theresa Wilson, Paul Hoffmann, Swapna Somasun- daran, Jason Kessler, Janyce Wiebe, Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth Patward- han. 2005a. Opinionfinder: a system for subjec- tivity analysis. In Proceedings of HLT/EMNLP on Interactive Demonstrations, pages 34-35, Morris- town, NJ, USA. Association for Computational Lin- guistics.",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "Recognizing contextual polarity in phraselevel sentiment analysis",
                "authors": [
                    {
                        "first": "Theresa",
                        "middle": [],
                        "last": "Wilson",
                        "suffix": ""
                    },
                    {
                        "first": "Janyce",
                        "middle": [],
                        "last": "Wiebe",
                        "suffix": ""
                    },
                    {
                        "first": "Paul",
                        "middle": [],
                        "last": "Hoffmann",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "HLT '05: Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "347--354",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005b. Recognizing contextual polarity in phrase- level sentiment analysis. In HLT '05: Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Pro- cessing, pages 347-354, Morristown, NJ, USA. As- sociation for Computational Linguistics.",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "Lexicon construction: A topic model approach",
                "authors": [
                    {
                        "first": "Rui",
                        "middle": [],
                        "last": "Xie",
                        "suffix": ""
                    },
                    {
                        "first": "Chunping",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Systems and Informatics (ICSAI), 2012 International Conference on",
                "volume": "",
                "issue": "",
                "pages": "2299--2303",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Rui Xie and Chunping Li. 2012. Lexicon construc- tion: A topic model approach. In Systems and Infor- matics (ICSAI), 2012 International Conference on, pages 2299-2303. IEEE.",
                "links": null
            },
            "BIBREF34": {
                "ref_id": "b34",
                "title": "Learning from labeled and unlabeled data with label propagation",
                "authors": [
                    {
                        "first": "Xiaojin",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    },
                    {
                        "first": "Zoubin",
                        "middle": [],
                        "last": "Ghahramani",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xiaojin Zhu and Zoubin Ghahramani. 2002. Learn- ing from labeled and unlabeled data with label prop- agation. In Technical Report CMU-CALD-02-107. CarnegieMellon University.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": ", Page et al. (1999), Feng et al. (2011) Heerschop et al. (2011), Montejo-R\u00e1ez et al. (2012))",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Figure 1: Graph for Graph Propagation ( \u00a72.2).",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "Figure 3: A Part of AMT Task Design.",
                "uris": null,
                "fig_num": "3",
                "type_str": "figure"
            },
            "FIGREF3": {
                "num": null,
                "text": "21",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF4": {
                "num": null,
                "text": "These examples credit to an anonymous reviewer.",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Example Named Entities (Proper Nouns) with Polar Connotation.",
                "html": null,
                "num": null
            },
            "TABREF1": {
                "content": "<table><tr><td>POSITIVE</td><td>NEGATIVE</td><td>NEUTRAL</td></tr><tr><td>n. avatar, adrenaline, keynote, debut, stakeholder, sunshine, cooperation</td><td>unbeliever, delay, shortfall, gun-shot, misdemeanor, mutiny, rigor</td><td>header, mark, clothing, outline, grid, gasoline, course, preview</td></tr><tr><td>v. handcraft, volunteer, party, ac-credit, personalize, nurse, google</td><td>sentence, cough, trap, scratch, de-bunk, rip, misspell, overcharge</td><td>state, edit, send, put, arrive, type, drill, name, stay, echo, register</td></tr><tr><td>a. floral, vegetarian, prepared, age-less, funded, contemporary</td><td>debilitating, impaired, swollen, intentional, jarring, unearned</td><td>same, cerebral, west, uncut, auto-matic, hydrated, unheated, routine</td></tr></table>",
                "type_str": "table",
                "text": ", Hatzivassiloglou and McKeown",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Example Words with Learned Connotation: Nouns(n), Verbs(v), Adjectives(a).",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table><tr><td>GENINQ EVAL 100 1,000 5,000 10,000 ALL ILP 97.6 94.5 84.5 80.8 80.4 98.0 100 1,000 5,000 10,000 ALL MPQA EVAL 89.7 84.6 81.2 78.4 OVERLAY 97.0 95.1 78.8 (78.3) 78.3 98.0 93.4 82.1 77.7 77.7 PRED-ARG (PMI) 91.0 91.4 76.1 (76.1) 76.1 88.0 89.1 78.8 75.1 75.1 PRED-ARG (CP) 88.0 85.4 76.2 (76.2) 76.2 87.0 82.6 78.0 76.3 76.3 PRED-ARG (CP) 91.0 91.0 81.0 (81.0) 81.0 88.0 91.5 80.0 78.3 78.3 HITS-ASYMT 77.0 68.8 --66.5 86.3 81.3 --72.2 PAGERANK-ASYMF 77.0 68.5 --65.7 87.2 80.3 --72.3</td></tr></table>",
                "type_str": "table",
                "text": "Evaluation of Induction Algorithms ( \u00a72) with respect to Sentiment Lexicons (precision%).",
                "html": null,
                "num": null
            },
            "TABREF4": {
                "content": "<table><tr><td>FORMULA</td><td>R</td><td>POSITIVE P</td><td>F</td><td>R</td><td>NEGATIVE P</td><td>F</td><td>R</td><td>ALL P</td><td>F</td></tr><tr><td>ILP \u03a6</td><td/><td/><td/><td/><td/><td/><td/><td/><td/></tr></table>",
                "type_str": "table",
                "text": "prosody + C syn + C ant 51.4 85.7 64.3 44.7 87.9 59.3 48.0 86.8 61.8 \u03a6 prosody + C syn + C ant + C S 61.2 93.3 73.9 52.4 92.2 66.8 56.8 92.8 70.5 \u03a6 prosody + \u03a6 coord + C syn + C ant 67.3 75.0 70.9 53.7 84.4 65.6 60.5 79.7 68.8 \u03a6 prosody + \u03a6 coord + C syn + C ant + C S 62.2 96.0 75.5 51.5 89.5 65.4 56.9 92.8 70.5 LP \u03a6 prosody + \u03a6 syn + \u03a6 ant 24.4 76.0 36.9 23.6 78.8 36.3 24.0 77.4 36.6 \u03a6 prosody + \u03a6 syn + \u03a6 ant + \u03a6 S 71.6 87.8 78.9 68.8 84.6 75.9 70.2 86.2 77.4 \u03a6 prosody + \u03a6 coord + \u03a6 syn + \u03a6 ant 67.9 92.6 78.3 64.6 89.1 74.9 66.3 90.8 76.6 \u03a6 prosody + \u03a6 coord + \u03a6 syn + \u03a6 ant + \u03a6 S 78.6 90.5 84.1 73.3 87.1 79.6 75.9 88.8 81.8 ILP/LP Comparison on MQPA (%).",
                "html": null,
                "num": null
            },
            "TABREF5": {
                "content": "<table><tr><td>YES Avg 43.3 % 2.9 56.7 2.5 \"Respectable / honourable\" 21.0 QUESTION \"Enjoyable or pleasant\" \"Of a good quality\" 3.3 \"Would like to do or have\" 52.5 2.8</td><td>NO 16.3 -2.4 % Avg 6.1 -2.7 14.0 -1.1 11.5 -2.4</td></tr></table>",
                "type_str": "table",
                "text": "Distribution of Answers from AMT.",
                "html": null,
                "num": null
            },
            "TABREF6": {
                "content": "<table><tr><td/><td colspan=\"3\">C-LP SENTIWN HUMAN JUDGES</td></tr><tr><td>\u2126 V ote</td><td>77.0</td><td>71.5</td><td>66.0</td></tr><tr><td>\u2126 Score</td><td>73.0</td><td>69.0</td><td>69.0</td></tr></table>",
                "type_str": "table",
                "text": "Distribution of Connotative Polarity from AMT.",
                "html": null,
                "num": null
            },
            "TABREF7": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Agreement (Accuracy) against AMTdriven Gold Standard.",
                "html": null,
                "num": null
            }
        }
    }
}