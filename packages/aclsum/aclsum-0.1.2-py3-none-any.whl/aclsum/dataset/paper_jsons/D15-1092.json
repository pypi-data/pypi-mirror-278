{
    "paper_id": "D15-1092",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:26:26.390486Z"
    },
    "title": "Sentence Modeling with Gated Recursive Neural Network",
    "authors": [
        {
            "first": "Xinchi",
            "middle": [],
            "last": "Chen",
            "suffix": "",
            "affiliation": {
                "laboratory": "Shanghai Key Laboratory of Intelligent Information Processing",
                "institution": "Fudan University",
                "location": {
                    "addrLine": "825 Zhangheng Road",
                    "settlement": "Shanghai",
                    "country": "China"
                }
            },
            "email": "xinchichen13@fudan.edu.cn"
        },
        {
            "first": "Xipeng",
            "middle": [],
            "last": "Qiu",
            "suffix": "",
            "affiliation": {
                "laboratory": "Shanghai Key Laboratory of Intelligent Information Processing",
                "institution": "Fudan University",
                "location": {
                    "addrLine": "825 Zhangheng Road",
                    "settlement": "Shanghai",
                    "country": "China"
                }
            },
            "email": "xpqiu@fudan.edu.cn"
        },
        {
            "first": "Chenxi",
            "middle": [],
            "last": "Zhu",
            "suffix": "",
            "affiliation": {
                "laboratory": "Shanghai Key Laboratory of Intelligent Information Processing",
                "institution": "Fudan University",
                "location": {
                    "addrLine": "825 Zhangheng Road",
                    "settlement": "Shanghai",
                    "country": "China"
                }
            },
            "email": ""
        },
        {
            "first": "Shiyu",
            "middle": [],
            "last": "Wu",
            "suffix": "",
            "affiliation": {
                "laboratory": "Shanghai Key Laboratory of Intelligent Information Processing",
                "institution": "Fudan University",
                "location": {
                    "addrLine": "825 Zhangheng Road",
                    "settlement": "Shanghai",
                    "country": "China"
                }
            },
            "email": ""
        },
        {
            "first": "Xuanjing",
            "middle": [],
            "last": "Huang",
            "suffix": "",
            "affiliation": {
                "laboratory": "Shanghai Key Laboratory of Intelligent Information Processing",
                "institution": "Fudan University",
                "location": {
                    "addrLine": "825 Zhangheng Road",
                    "settlement": "Shanghai",
                    "country": "China"
                }
            },
            "email": "xjhuang@fudan.edu.cn"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Recently, neural network based sentence modeling methods have achieved great progress. Among these methods, the recursive neural networks (RecNNs) can effectively model the combination of the words in sentence. However, RecNNs need a given external topological structure, like syntactic tree. In this paper, we propose a gated recursive neural network (GRNN) to model sentences, which employs a full binary tree (FBT) structure to control the combinations in recursive structure. By introducing two kinds of gates, our model can better model the complicated combinations of features. Experiments on three text classification datasets show the effectiveness of our model.",
    "pdf_parse": {
        "paper_id": "D15-1092",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Recently, neural network based sentence modeling methods have achieved great progress. Among these methods, the recursive neural networks (RecNNs) can effectively model the combination of the words in sentence. However, RecNNs need a given external topological structure, like syntactic tree. In this paper, we propose a gated recursive neural network (GRNN) to model sentences, which employs a full binary tree (FBT) structure to control the combinations in recursive structure. By introducing two kinds of gates, our model can better model the complicated combinations of features. Experiments on three text classification datasets show the effectiveness of our model.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Recently, neural network based sentence modeling approaches have been increasingly focused on for their ability to minimize the efforts in feature engineering, such as Neural Bag-of-Words (NBoW), Recurrent Neural Network (RNN) (Mikolov et al., 2010) , Recursive Neural Network (RecNN) (Pollack, 1990; Socher et al., 2013b; Socher et al., 2012) and Convolutional Neural Network (CNN) (Kalchbrenner et al., 2014; Hu et al., 2014) .",
                "cite_spans": [
                    {
                        "start": 227,
                        "end": 249,
                        "text": "(Mikolov et al., 2010)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 285,
                        "end": 300,
                        "text": "(Pollack, 1990;",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 301,
                        "end": 322,
                        "text": "Socher et al., 2013b;",
                        "ref_id": null
                    },
                    {
                        "start": 323,
                        "end": 343,
                        "text": "Socher et al., 2012)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 383,
                        "end": 410,
                        "text": "(Kalchbrenner et al., 2014;",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 411,
                        "end": 427,
                        "text": "Hu et al., 2014)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Among these methods, recursive neural networks (RecNNs) have shown their excellent abilities to model the word combinations in sentence. However, RecNNs require a pre-defined topological structure, like parse tree, to encode sentence, which limits the scope of its application. Cho et al. (2014) proposed the gated recursive convolutional neural network (grConv) by utilizing the directed acyclic graph (DAG) structure instead of parse tree * Corresponding author. (GRNNs) . Left is a GRNN using a directed acyclic graph (DAG) structure. Right is a GRNN using a full binary tree (FBT) structure. (The green nodes, gray nodes and white nodes illustrate the positive, negative and neutral sentiments respectively.) to model sentences. However, DAG structure is relatively complicated. The number of the hidden neurons quadraticly increases with the length of sentences so that grConv cannot effectively deal with long sentences.",
                "cite_spans": [
                    {
                        "start": 278,
                        "end": 295,
                        "text": "Cho et al. (2014)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 465,
                        "end": 472,
                        "text": "(GRNNs)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Inspired by grConv, we propose a gated recursive neural network (GRNN) for sentence modeling. Different with grConv, we use the full binary tree (FBT) as the topological structure to recursively model the word combinations, as shown in Figure 1 . The number of the hidden neurons linearly increases with the length of sentences. Another difference is that we introduce two kinds of gates, reset and update gates (Chung et al., 2014) , to control the combinations in recursive structure. With these two gating mechanisms, our model can better model the complicated combinations of features and capture the long dependency interactions.",
                "cite_spans": [
                    {
                        "start": 412,
                        "end": 432,
                        "text": "(Chung et al., 2014)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 243,
                        "end": 244,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In our previous works, we have investigated several different topological structures (tree and directed acyclic graph) to recursively model the semantic composition from the bottom layer to the top layer, and applied them on Chinese word segmentation (Chen et al., 2015a) and dependency parsing (Chen et al., 2015b) tasks. However, these structures are not suitable for modeling sentences.",
                "cite_spans": [
                    {
                        "start": 251,
                        "end": 271,
                        "text": "(Chen et al., 2015a)",
                        "ref_id": null
                    },
                    {
                        "start": 295,
                        "end": 315,
                        "text": "(Chen et al., 2015b)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\u2026 Softmax(W s \u00d7 u i + b s ) u i P(\u2022|x i ;\u03b8) w 2 w 1 (i) (i) \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 w 3 w 4 w 5 (i) (i) (i) 0 0 0 Figure 2: Architecture of Gated Recursive Neural Network (GRNN).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this paper, we adopt the full binary tree as the topological structure to reduce the model complexity.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Experiments on the Stanford Sentiment Treebank dataset (Socher et al., 2013b) and the TREC questions dataset (Li and Roth, 2002) show the effectiveness of our approach.",
                "cite_spans": [
                    {
                        "start": 55,
                        "end": 77,
                        "text": "(Socher et al., 2013b)",
                        "ref_id": null
                    },
                    {
                        "start": 109,
                        "end": 128,
                        "text": "(Li and Roth, 2002)",
                        "ref_id": "BIBREF10"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "2 Gated Recursive Neural Network",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The recursive neural network (RecNN) need a topological structure to model a sentence, such as a syntactic tree. In this paper, we use a full binary tree (FBT), as showing in Figure 2 , to model the combinations of features for a given sentence.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 182,
                        "end": 183,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Architecture",
                "sec_num": "2.1"
            },
            {
                "text": "In fact, the FBT structure can model the combinations of features by continuously mixing the information from the bottom layer to the top layer. Each neuron can be regarded as a complicated feature composition of its governed sub-sentence. When the children nodes combine into their parent node, the combination information of two children nodes is also merged and preserved by their parent node. As shown in Figure 2 , we put all-zero padding vectors after the last word of the sentence until the length of 2 \u2308log n 2 \u2309 , where n is the length of the given sentence.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 416,
                        "end": 417,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Architecture",
                "sec_num": "2.1"
            },
            {
                "text": "Inspired by the success of the gate mechanism of Chung et al. (2014) , we further propose a gated recursive neural network (GRNN) by introducing two kinds of gates, namely \"reset gate\" and \"update gate\". Specifically, there are two reset gates, r L and r R , partially reading the information from",
                "cite_spans": [
                    {
                        "start": 49,
                        "end": 68,
                        "text": "Chung et al. (2014)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Architecture",
                "sec_num": "2.1"
            },
            {
                "text": "Gate rL Gate rR h2j (l-1) h2j+1 (l-1) hj ^(l)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Gate z",
                "sec_num": null
            },
            {
                "text": "hj (l) Figure 3: Our proposed gated recursive unit. left child and right child respectively. And the update gates z N , z L and z R decide what to preserve when combining the children's information. Intuitively, these gates seem to decide how to update and exploit the combination information.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Gate z",
                "sec_num": null
            },
            {
                "text": "In the case of text classification, for each given sentence x i = w (i) 1:N (i) and the corresponding class y i , we first represent each word w",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Gate z",
                "sec_num": null
            },
            {
                "text": "(i) j into its corre- sponding embedding w w (i) j \u2208 R d , where N (i) in-",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Gate z",
                "sec_num": null
            },
            {
                "text": "dicates the length of i-th sentence and d is dimensionality of word embeddings. Then, the embeddings are sent to the first layer of GRNN as inputs, whose outputs are recursively applied to upper layers until it outputs a single fixed-length vector. Next, we receive the class distribution P(\u2022|x i ; \u03b8) for the given sentence x i by a softmax transformation of u i , where u i is the top node of the network (a fixed length vectorial representation):",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Gate z",
                "sec_num": null
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "P(\u2022|x i ; \u03b8) = softmax(W s \u00d7 u i + b s ),",
                        "eq_num": "(1)"
                    }
                ],
                "section": "Gate z",
                "sec_num": null
            },
            {
                "text": "where",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Gate z",
                "sec_num": null
            },
            {
                "text": "b s \u2208 R |T | , W s \u2208 R |T |\u00d7d",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Gate z",
                "sec_num": null
            },
            {
                "text": ". d is the dimensionality of the top node u i , which is same with the word embedding size and T represents the set of possible classes. \u03b8 represents the parameter set.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Gate z",
                "sec_num": null
            },
            {
                "text": "GRNN consists of the minimal structures, gated recursive units, as showing in Figure 3 . By assuming that the length of sentence is n, we will have recursion layer l \u2208 [1, \u2308log n 2 \u2309+1], where symbol \u2308q\u2309 indicates the minimal integer q * \u2265 q. At each recursion layer l, the activation of the j-",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 85,
                        "end": 86,
                        "text": "3",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Gated Recursive Unit",
                "sec_num": "2.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "th (j \u2208 [0, 2 \u2308log n 2 \u2309-l )) hidden node h (l) j \u2208 R d is computed as h (l) j = { zN \u2299 \u0125l j + zL \u2299 h l-1 2j + zR \u2299 h l-1 2j+1 , l > 1, corresponding word embedding, l = 1,",
                        "eq_num": "(2)"
                    }
                ],
                "section": "Gated Recursive Unit",
                "sec_num": "2.2"
            },
            {
                "text": "where z N , z L and z R \u2208 R d are update gates for new activation \u0125l j , left child node h l-1 2j and right child node h l-1 2j+1 respectively, and \u2299 indicates element-wise multiplication.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Gated Recursive Unit",
                "sec_num": "2.2"
            },
            {
                "text": "The update gates can be formalized as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Gated Recursive Unit",
                "sec_num": "2.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "z = \uf8ee \uf8f0 zN zL zR \uf8f9 \uf8fb = \uf8ee \uf8f0 1/Z 1/Z 1/Z \uf8f9 \uf8fb \u2299 exp(U \uf8ee \uf8ef \uf8f0 \u0125l j h l-1 2j h l-1 2j+1 \uf8f9 \uf8fa \uf8fb),",
                        "eq_num": "(3)"
                    }
                ],
                "section": "Gated Recursive Unit",
                "sec_num": "2.2"
            },
            {
                "text": "where U \u2208 R 3d\u00d73d is the coefficient of update gates, and Z \u2208 R d is the vector of the normalization coefficients,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Gated Recursive Unit",
                "sec_num": "2.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "Z k = 3 \u2211 i=1 [exp(U \uf8ee \uf8ef \uf8f0 \u0125l j h l-1 2j h l-1 2j+1 \uf8f9 \uf8fa \uf8fb)] d\u00d7(i-1)+k ,",
                        "eq_num": "(4)"
                    }
                ],
                "section": "Gated Recursive Unit",
                "sec_num": "2.2"
            },
            {
                "text": "where",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Gated Recursive Unit",
                "sec_num": "2.2"
            },
            {
                "text": "1 \u2264 k \u2264 d.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Gated Recursive Unit",
                "sec_num": "2.2"
            },
            {
                "text": "The new activation \u0125l j is computed as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Gated Recursive Unit",
                "sec_num": "2.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "\u0125l j = tanh(W \u0125 [ r L \u2299 h l-1 2j r R \u2299 h l-1 2j+1 ] ),",
                        "eq_num": "(5)"
                    }
                ],
                "section": "Gated Recursive Unit",
                "sec_num": "2.2"
            },
            {
                "text": "where",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Gated Recursive Unit",
                "sec_num": "2.2"
            },
            {
                "text": "W \u0125 \u2208 R d\u00d72d , r L \u2208 R d , r R \u2208 R d . r L and",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Gated Recursive Unit",
                "sec_num": "2.2"
            },
            {
                "text": "r R are the reset gates for left child node h l-1 2j and right child node h l-1 2j+1 respectively, which can be formalized as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Gated Recursive Unit",
                "sec_num": "2.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "[ r L r R ] = \u03c3(G [ h l-1 2j h l-1 2j+1 ] ),",
                        "eq_num": "(6)"
                    }
                ],
                "section": "Gated Recursive Unit",
                "sec_num": "2.2"
            },
            {
                "text": "where G \u2208 R 2d\u00d72d is the coefficient of two reset gates and \u03c3 indicates the sigmoid function.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Gated Recursive Unit",
                "sec_num": "2.2"
            },
            {
                "text": "Intuiativly, the reset gates control how to select the output information of the left and right children, which result to the current new activation \u0125. By the update gates, the activation of a parent neuron can be regarded as a choice among the the current new activation \u0125, the left child, and the right child. This choice allows the overall structure to change adaptively with respect to the inputs.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Gated Recursive Unit",
                "sec_num": "2.2"
            },
            {
                "text": "This gate mechanism is effective to model the combinations of features.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Gated Recursive Unit",
                "sec_num": "2.2"
            },
            {
                "text": "We use the Maximum Likelihood (ML) criterion to train our model. Given training set (x i , y i ) and the parameter set of our model \u03b8, the goal is to minimize the loss function:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training",
                "sec_num": "2.3"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "J(\u03b8) = - 1 m m \u2211 i=1 log P(y i |x i ; \u03b8) + \u03bb 2m \u2225\u03b8\u2225 2 2 ,",
                        "eq_num": "(7)"
                    }
                ],
                "section": "Training",
                "sec_num": "2.3"
            },
            {
                "text": "Initial learning rate \u03b1 = 0.3 Regularization \u03bb = 10 -4 Dropout rate on input layer p = 20% where m is number of training sentences.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training",
                "sec_num": "2.3"
            },
            {
                "text": "Following (Socher et al., 2013a) , we use the diagonal variant of AdaGrad (Duchi et al., 2011) with minibatchs to minimize the objective.",
                "cite_spans": [
                    {
                        "start": 10,
                        "end": 32,
                        "text": "(Socher et al., 2013a)",
                        "ref_id": null
                    },
                    {
                        "start": 74,
                        "end": 94,
                        "text": "(Duchi et al., 2011)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training",
                "sec_num": "2.3"
            },
            {
                "text": "For parameter initialization, we use random initialization within (-0.01, 0.01) for all parameters except the word embeddings. We adopt the pretrained English word embeddings from (Collobert et al., 2011) and fine-tune them during training.",
                "cite_spans": [
                    {
                        "start": 180,
                        "end": 204,
                        "text": "(Collobert et al., 2011)",
                        "ref_id": "BIBREF5"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training",
                "sec_num": "2.3"
            },
            {
                "text": "To evaluate our approach, we test our model on three datasets:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Datasets",
                "sec_num": "3.1"
            },
            {
                "text": "\u2022 SST-1 The movie reviews with five classes in the Stanford Sentiment Treebank 1 (Socher et al., 2013b) : negative, somewhat negative, neutral, somewhat positive, positive.",
                "cite_spans": [
                    {
                        "start": 81,
                        "end": 103,
                        "text": "(Socher et al., 2013b)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Datasets",
                "sec_num": "3.1"
            },
            {
                "text": "\u2022 SST-2 The movie reviews with binary classes in the Stanford Sentiment Treebank 1 (Socher et al., 2013b) : negative, positive.",
                "cite_spans": [
                    {
                        "start": 83,
                        "end": 105,
                        "text": "(Socher et al., 2013b)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Datasets",
                "sec_num": "3.1"
            },
            {
                "text": "\u2022 QC The TREC questions dataset2 (Li and Roth, 2002) involves six different question types.",
                "cite_spans": [
                    {
                        "start": 33,
                        "end": 52,
                        "text": "(Li and Roth, 2002)",
                        "ref_id": "BIBREF10"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Datasets",
                "sec_num": "3.1"
            },
            {
                "text": "Table 1 lists the hyper-parameters of our model. In this paper, we also exploit dropout strategy (Srivastava et al., 2014) to avoid overfitting. In addition, we set the batch size to 20. We set word embedding size d = 50 on the TREC dataset and d = 100 on the Stanford Sentiment Treebank dataset.",
                "cite_spans": [
                    {
                        "start": 97,
                        "end": 122,
                        "text": "(Srivastava et al., 2014)",
                        "ref_id": "BIBREF17"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "1",
                        "ref_id": "TABREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Hyper-parameters",
                "sec_num": "3.2"
            },
            {
                "text": "Table 2 shows the performance of our GRNN on three datasets.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Experiment Results",
                "sec_num": "3.3"
            },
            {
                "text": "SST-1 SST-2 QC NBoW (Kalchbrenner et al., 2014) 42.4 80.5 88.2 PV (Le and Mikolov, 2014) 44.6 * 82.7 * 91.8 * CNN-non-static (Kim, 2014) 48.0 87.2 93.6 CNN-multichannel (Kim, 2014) 47.4 88.1 92.2 MaxTDNN (Collobert and Weston, 2008) 37.4 77.1 84.4 DCNN (Kalchbrenner et al., 2014) 48.5 86.8 93.0 RecNTN (Socher et al., 2013b) 45.7 85.4 -RAE (Socher et al., 2011) 43.2 82.4 -MV-RecNN (Socher et al., 2012) 44.4 82.9 -AdaSent (Zhao et al., 2015) --92.4 GRNN (our approach) 47.5 85.5 93.8",
                "cite_spans": [
                    {
                        "start": 20,
                        "end": 47,
                        "text": "(Kalchbrenner et al., 2014)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 74,
                        "end": 88,
                        "text": "Mikolov, 2014)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 125,
                        "end": 136,
                        "text": "(Kim, 2014)",
                        "ref_id": null
                    },
                    {
                        "start": 152,
                        "end": 180,
                        "text": "CNN-multichannel (Kim, 2014)",
                        "ref_id": null
                    },
                    {
                        "start": 204,
                        "end": 232,
                        "text": "(Collobert and Weston, 2008)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 253,
                        "end": 280,
                        "text": "(Kalchbrenner et al., 2014)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 303,
                        "end": 325,
                        "text": "(Socher et al., 2013b)",
                        "ref_id": null
                    },
                    {
                        "start": 341,
                        "end": 362,
                        "text": "(Socher et al., 2011)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 383,
                        "end": 404,
                        "text": "(Socher et al., 2012)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 424,
                        "end": 443,
                        "text": "(Zhao et al., 2015)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Methods",
                "sec_num": null
            },
            {
                "text": "Table 2 : Performances of the different models. The result of PV is from our own implementation based on Gensim.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Methods",
                "sec_num": null
            },
            {
                "text": "Competitor Models Neural Bag-of-Words (NBOW) model is a simple and intuitive method which ignores the word order. Paragraph Vector (PV) (Le and Mikolov, 2014) learns continuous distributed vector representations for pieces of texts, which can be regarded as a long term memory of sentences as opposed to short memory in recurrent neural network. Here, we use the popular open source implementation of PV in Gensim 1 . Methods in the third block are CNN based models. Kim (2014) reports 4 different CNN models using max-over-time pooling, where CNN-non-static and CNN-multichannel are more sophisticated. MaxTDNN sentence model is based on the architecture of the Time-Delay Neural Network (TDNN) (Waibel et al., 1989; Collobert and Weston, 2008) . Dynamic convolutional neural network (DCNN) (Kalchbrenner et al., 2014) uses the dynamic k-max pooling operator as a non-linear sub-sampling function, in which the choice of k depends on the length of given sentence. Methods in the fourth block are RecNN based models. Recursive Neural Tensor Network (RecNTN) (Socher et al., 2013b ) is an extension of plain RecNN, which also depends on a external syntactic structure. Recursive Autoencoder (RAE) (Socher et al., 2011) learns the representations of sentences by minimizing the reconstruction error. Matrix-Vector Recursive Neural Network (MV-RecNN) (Socher et al., 2012) is a extension of RecNN by assigning a vector and a matrix to every node in the parse tree. AdaSent (Zhao et al., 2015) adopts recursive neural network using DAG structure.",
                "cite_spans": [
                    {
                        "start": 696,
                        "end": 717,
                        "text": "(Waibel et al., 1989;",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 718,
                        "end": 745,
                        "text": "Collobert and Weston, 2008)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 792,
                        "end": 819,
                        "text": "(Kalchbrenner et al., 2014)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 1058,
                        "end": 1079,
                        "text": "(Socher et al., 2013b",
                        "ref_id": null
                    },
                    {
                        "start": 1196,
                        "end": 1217,
                        "text": "(Socher et al., 2011)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 1348,
                        "end": 1369,
                        "text": "(Socher et al., 2012)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 1470,
                        "end": 1489,
                        "text": "(Zhao et al., 2015)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Methods",
                "sec_num": null
            },
            {
                "text": "1 https://github.com/piskvorky/gensim/ Moreover, the plain GRNN which does not incorporate the gate mechanism cannot outperform the GRNN model. Theoretically, the plain GRNN can be regarded as a special case of GRNN, whose parameters are constrained or truncated. As a result, GRNN is a more powerful model which outperforms the plain GRNN. Thus, we mainly focus on the GRNN model in this paper.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Methods",
                "sec_num": null
            },
            {
                "text": "Result Discussion Generally, our model is better than the previous recursive neural network based models (RecNTN, RAE, MV-RecNN and AdaSent), which indicates our model can better model the combinations of features with the FBT and our gating mechanism, even without an external syntactic tree.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Methods",
                "sec_num": null
            },
            {
                "text": "Although we just use the top layer outputs as the feature for classification, our model still outperforms AdaSent.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Methods",
                "sec_num": null
            },
            {
                "text": "Compared with the CNN based methods (MaxTDNN, DCNN and CNNs), our model achieves the comparable performances with much fewer parameters. Although CNN based methods outperform our model on SST-1 and SST-2, the number of parameters2 of GRNN ranges from 40K to 160K while the number of parameters is about 400K in CNN.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Methods",
                "sec_num": null
            },
            {
                "text": "4 Related Work Cho et al. (2014) proposed grConv to model sentences for machine translation. Unlike our model, grConv uses the DAG structure as the topological structure to model sentences. The number of the internal nodes is n 2 /2, where n is the length of the sentence. Zhao et al. (2015) uses the same structure to model sentences (called AdaSent), and utilizes the information of internal nodes to model sentences for text classification. Unlike grConv and AdaSent, our model uses full binary tree as the topological structure. The number of the internal nodes is 2n in our model. Therefore, our model is more efficient for long sentences. In addition, we just use the top layer neurons for text classification.",
                "cite_spans": [
                    {
                        "start": 15,
                        "end": 32,
                        "text": "Cho et al. (2014)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 273,
                        "end": 291,
                        "text": "Zhao et al. (2015)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Methods",
                "sec_num": null
            },
            {
                "text": "Moreover, grConv and AdaSent only exploit one gating mechanism (update gate), which cannot sufficiently model the complicated feature combinations. Unlike them, our model incorporates two kind of gates and can better model the feature combinations. Hu et al. (2014) also proposed a similar architecture for matching problems, but they employed the convolutional neural network which might be coarse in modeling the feature combinations.",
                "cite_spans": [
                    {
                        "start": 249,
                        "end": 265,
                        "text": "Hu et al. (2014)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Methods",
                "sec_num": null
            },
            {
                "text": "In this paper, we propose a gated recursive neural network (GRNN) to recursively summarize the meaning of sentence. GRNN uses full binary tree as the recursive topological structure instead of an external syntactic tree. In addition, we introduce two kinds of gates to model the complicated combinations of features. In future work, we would like to investigate the other gating mechanisms for better modeling the feature combinations.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "5"
            },
            {
                "text": "http://nlp.stanford.edu/sentiment",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "http://cogcomp.cs.illinois.edu/Data/ QA/QC/",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "We only take parameters of network into account, leaving out word embeddings.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "We would like to thank the anonymous reviewers for their valuable comments. This work was partially funded by the National Natural Science Foundation of China (61472088, 61473092), National High Technology Research and Development Program of China (2015AA015408), Shanghai Science and Technology Development Funds (14ZR1403200).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgments",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Chenxi Zhu, and Xuanjing Huang. 2015a. Gated recursive neural network for Chinese word segmentation",
                "authors": [
                    {
                        "first": "Xinchi",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Xipeng",
                        "middle": [],
                        "last": "Qiu",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "Proceedings of Annual Meeting of the",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xinchi Chen, Xipeng Qiu, Chenxi Zhu, and Xuanjing Huang. 2015a. Gated recursive neural network for Chinese word segmentation. In Proceedings of An- nual Meeting of the Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Xipeng Qiu, and Xuanjing Huang. 2015b. Transition-based dependency parsing using two heterogeneous gated recursive neural networks",
                "authors": [
                    {
                        "first": "Xinchi",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Yaqian",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Chenxi",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xinchi Chen, Yaqian Zhou, Chenxi Zhu, Xipeng Qiu, and Xuanjing Huang. 2015b. Transition-based de- pendency parsing using two heterogeneous gated re- cursive neural networks. In Proceedings of the Con- ference on Empirical Methods in Natural Language Processing.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "On the properties of neural machine translation",
                "authors": [
                    {
                        "first": "Kyunghyun",
                        "middle": [],
                        "last": "Cho",
                        "suffix": ""
                    },
                    {
                        "first": "Bart",
                        "middle": [],
                        "last": "Van Merri\u00ebnboer",
                        "suffix": ""
                    },
                    {
                        "first": "Dzmitry",
                        "middle": [],
                        "last": "Bahdanau",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1409.1259"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Kyunghyun Cho, Bart van Merri\u00ebnboer, Dzmitry Bah- danau, and Yoshua Bengio. 2014. On the properties of neural machine translation: Encoder-decoder ap- proaches. arXiv preprint arXiv:1409.1259.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
                "authors": [
                    {
                        "first": "Junyoung",
                        "middle": [],
                        "last": "Chung",
                        "suffix": ""
                    },
                    {
                        "first": "Caglar",
                        "middle": [],
                        "last": "Gulcehre",
                        "suffix": ""
                    },
                    {
                        "first": "Kyunghyun",
                        "middle": [],
                        "last": "Cho",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1412.3555"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014. Empirical evaluation of gated recurrent neural networks on sequence model- ing. arXiv preprint arXiv:1412.3555.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "A unified architecture for natural language processing: Deep neural networks with multitask learning",
                "authors": [
                    {
                        "first": "Ronan",
                        "middle": [],
                        "last": "Collobert",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Weston",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Proceedings of ICML",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Pro- ceedings of ICML.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Natural language processing (almost) from scratch",
                "authors": [
                    {
                        "first": "Ronan",
                        "middle": [],
                        "last": "Collobert",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Weston",
                        "suffix": ""
                    },
                    {
                        "first": "L\u00e9on",
                        "middle": [],
                        "last": "Bottou",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Karlen",
                        "suffix": ""
                    },
                    {
                        "first": "Koray",
                        "middle": [],
                        "last": "Kavukcuoglu",
                        "suffix": ""
                    },
                    {
                        "first": "Pavel",
                        "middle": [],
                        "last": "Kuksa",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "The Journal of Machine Learning Research",
                "volume": "12",
                "issue": "",
                "pages": "2493--2537",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ronan Collobert, Jason Weston, L\u00e9on Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Re- search, 12:2493-2537.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Adaptive subgradient methods for online learning and stochastic optimization",
                "authors": [
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Duchi",
                        "suffix": ""
                    },
                    {
                        "first": "Elad",
                        "middle": [],
                        "last": "Hazan",
                        "suffix": ""
                    },
                    {
                        "first": "Yoram",
                        "middle": [],
                        "last": "Singer",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "The Journal of Machine Learning Research",
                "volume": "12",
                "issue": "",
                "pages": "2121--2159",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. The Journal of Ma- chine Learning Research, 12:2121-2159.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Convolutional neural network architectures for matching natural language sentences",
                "authors": [
                    {
                        "first": "Baotian",
                        "middle": [],
                        "last": "Hu",
                        "suffix": ""
                    },
                    {
                        "first": "Zhengdong",
                        "middle": [],
                        "last": "Lu",
                        "suffix": ""
                    },
                    {
                        "first": "Hang",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Qingcai",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai Chen. 2014. Convolutional neural network archi- tectures for matching natural language sentences. In Advances in Neural Information Processing Sys- tems.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "A convolutional neural network for modelling sentences",
                "authors": [
                    {
                        "first": "Nal",
                        "middle": [],
                        "last": "Kalchbrenner",
                        "suffix": ""
                    },
                    {
                        "first": "Edward",
                        "middle": [],
                        "last": "Grefenstette",
                        "suffix": ""
                    },
                    {
                        "first": "Phil",
                        "middle": [],
                        "last": "Blunsom",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Convolutional neural networks for sentence classification",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1408.5882"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Nal Kalchbrenner, Edward Grefenstette, and Phil Blun- som. 2014. A convolutional neural network for modelling sentences. In Proceedings of ACL. Yoon Kim. 2014. Convolutional neural net- works for sentence classification. arXiv preprint arXiv:1408.5882.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Distributed representations of sentences and documents",
                "authors": [
                    {
                        "first": "V",
                        "middle": [],
                        "last": "Quoc",
                        "suffix": ""
                    },
                    {
                        "first": "Tomas",
                        "middle": [],
                        "last": "Le",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Mikolov",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of ICML",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Quoc V. Le and Tomas Mikolov. 2014. Distributed representations of sentences and documents. In Pro- ceedings of ICML.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Learning question classifiers",
                "authors": [
                    {
                        "first": "Xin",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Roth",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Proceedings of the 19th International Conference on Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "556--562",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xin Li and Dan Roth. 2002. Learning question classi- fiers. In Proceedings of the 19th International Con- ference on Computational Linguistics, pages 556- 562.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Recurrent neural network based language model",
                "authors": [
                    {
                        "first": "Tomas",
                        "middle": [],
                        "last": "Mikolov",
                        "suffix": ""
                    },
                    {
                        "first": "Martin",
                        "middle": [],
                        "last": "Karafi\u00e1t",
                        "suffix": ""
                    },
                    {
                        "first": "Lukas",
                        "middle": [],
                        "last": "Burget",
                        "suffix": ""
                    },
                    {
                        "first": "Jan",
                        "middle": [],
                        "last": "Cernock\u1ef3",
                        "suffix": ""
                    },
                    {
                        "first": "Sanjeev",
                        "middle": [],
                        "last": "Khudanpur",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tomas Mikolov, Martin Karafi\u00e1t, Lukas Burget, Jan Cernock\u1ef3, and Sanjeev Khudanpur. 2010. Recur- rent neural network based language model. In IN- TERSPEECH.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Recursive distributed representations",
                "authors": [
                    {
                        "first": "B",
                        "middle": [],
                        "last": "Jordan",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Pollack",
                        "suffix": ""
                    }
                ],
                "year": 1990,
                "venue": "Artificial Intelligence",
                "volume": "46",
                "issue": "1",
                "pages": "77--105",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jordan B Pollack. 1990. Recursive distributed repre- sentations. Artificial Intelligence, 46(1):77-105.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Semi-supervised recursive autoencoders for predicting sentiment distributions",
                "authors": [
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    },
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Pennington",
                        "suffix": ""
                    },
                    {
                        "first": "Eric",
                        "middle": [
                            "H"
                        ],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [
                            "Y"
                        ],
                        "last": "Ng",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "151--161",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Richard Socher, Jeffrey Pennington, Eric H Huang, Andrew Y Ng, and Christopher D Manning. 2011. Semi-supervised recursive autoencoders for predict- ing sentiment distributions. In Proceedings of the Conference on Empirical Methods in Natural Lan- guage Processing, pages 151-161.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Semantic compositionality through recursive matrix-vector spaces",
                "authors": [
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    },
                    {
                        "first": "Brody",
                        "middle": [],
                        "last": "Huval",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [
                            "Y"
                        ],
                        "last": "Ng",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning",
                "volume": "",
                "issue": "",
                "pages": "1201--1211",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Richard Socher, Brody Huval, Christopher D Manning, and Andrew Y Ng. 2012. Semantic compositional- ity through recursive matrix-vector spaces. In Pro- ceedings of the 2012 Joint Conference on Empiri- cal Methods in Natural Language Processing and Computational Natural Language Learning, pages 1201-1211.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Parsing with compositional vector grammars",
                "authors": [
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    },
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Bauer",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [
                            "Y"
                        ],
                        "last": "Ng",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of the ACL conference",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Richard Socher, John Bauer, Christopher D Manning, and Andrew Y Ng. 2013a. Parsing with compo- sitional vector grammars. In In Proceedings of the ACL conference. Citeseer.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
                "authors": [
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    },
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Perelygin",
                        "suffix": ""
                    },
                    {
                        "first": "Jean",
                        "middle": [
                            "Y"
                        ],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Chuang",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [
                            "Y"
                        ],
                        "last": "Ng",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [],
                        "last": "Potts",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of the conference on empirical methods in natural language processing (EMNLP)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013b. Recursive deep models for semantic compositionality over a senti- ment treebank. In Proceedings of the conference on empirical methods in natural language processing (EMNLP).",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Dropout: A simple way to prevent neural networks from overfitting",
                "authors": [
                    {
                        "first": "Nitish",
                        "middle": [],
                        "last": "Srivastava",
                        "suffix": ""
                    },
                    {
                        "first": "Geoffrey",
                        "middle": [],
                        "last": "Hinton",
                        "suffix": ""
                    },
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Krizhevsky",
                        "suffix": ""
                    },
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    },
                    {
                        "first": "Ruslan",
                        "middle": [],
                        "last": "Salakhutdinov",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "The Journal of Machine Learning Research",
                "volume": "15",
                "issue": "1",
                "pages": "1929--1958",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):1929-1958.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Phoneme recognition using time-delay neural networks. Acoustics, Speech and Signal Processing",
                "authors": [
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Waibel",
                        "suffix": ""
                    },
                    {
                        "first": "Toshiyuki",
                        "middle": [],
                        "last": "Hanazawa",
                        "suffix": ""
                    },
                    {
                        "first": "Geoffrey",
                        "middle": [],
                        "last": "Hinton",
                        "suffix": ""
                    },
                    {
                        "first": "Kiyohiro",
                        "middle": [],
                        "last": "Shikano",
                        "suffix": ""
                    },
                    {
                        "first": "Kevin",
                        "middle": [
                            "J"
                        ],
                        "last": "Lang",
                        "suffix": ""
                    }
                ],
                "year": 1989,
                "venue": "IEEE Transactions on",
                "volume": "37",
                "issue": "3",
                "pages": "328--339",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alex Waibel, Toshiyuki Hanazawa, Geoffrey Hin- ton, Kiyohiro Shikano, and Kevin J Lang. 1989. Phoneme recognition using time-delay neural net- works. Acoustics, Speech and Signal Processing, IEEE Transactions on, 37(3):328-339.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Self-adaptive hierarchical sentence model",
                "authors": [
                    {
                        "first": "Han",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Zhengdong",
                        "middle": [],
                        "last": "Lu",
                        "suffix": ""
                    },
                    {
                        "first": "Pascal",
                        "middle": [],
                        "last": "Poupart",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1504.05070"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Han Zhao, Zhengdong Lu, and Pascal Poupart. 2015. Self-adaptive hierarchical sentence model. arXiv preprint arXiv:1504.05070.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure1: Example of Gated Recursive Neural Networks (GRNNs). Left is a GRNN using a directed acyclic graph (DAG) structure. Right is a GRNN using a full binary tree (FBT) structure. (The green nodes, gray nodes and white nodes illustrate the positive, negative and neutral sentiments respectively.)",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Hyper-parameter settings.",
                "html": null,
                "num": null
            }
        }
    }
}