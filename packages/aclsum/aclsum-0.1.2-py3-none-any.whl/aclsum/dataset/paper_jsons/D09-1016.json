{
    "paper_id": "D09-1016",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:01:08.393910Z"
    },
    "title": "A Unified Model of Phrasal and Sentential Evidence for Information Extraction",
    "authors": [
        {
            "first": "Siddharth",
            "middle": [],
            "last": "Patwardhan",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Utah",
                "location": {
                    "postCode": "84112",
                    "settlement": "Salt Lake City",
                    "region": "UT"
                }
            },
            "email": ""
        },
        {
            "first": "Ellen",
            "middle": [],
            "last": "Riloff",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Utah",
                "location": {
                    "postCode": "84112",
                    "settlement": "Salt Lake City",
                    "region": "UT"
                }
            },
            "email": "riloff@cs.utah.edu"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Information Extraction (IE) systems that extract role fillers for events typically look at the local context surrounding a phrase when deciding whether to extract it. Often, however, role fillers occur in clauses that are not directly linked to an event word. We present a new model for event extraction that jointly considers both the local context around a phrase along with the wider sentential context in a probabilistic framework. Our approach uses a sentential event recognizer and a plausible role-filler recognizer that is conditioned on event sentences. We evaluate our system on two IE data sets and show that our model performs well in comparison to existing IE systems that rely on local phrasal context.",
    "pdf_parse": {
        "paper_id": "D09-1016",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Information Extraction (IE) systems that extract role fillers for events typically look at the local context surrounding a phrase when deciding whether to extract it. Often, however, role fillers occur in clauses that are not directly linked to an event word. We present a new model for event extraction that jointly considers both the local context around a phrase along with the wider sentential context in a probabilistic framework. Our approach uses a sentential event recognizer and a plausible role-filler recognizer that is conditioned on event sentences. We evaluate our system on two IE data sets and show that our model performs well in comparison to existing IE systems that rely on local phrasal context.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Information Extraction (IE) systems typically use extraction patterns (e.g., Soderland et al. (1995) , Riloff (1996) , Yangarber et al. (2000) , Califf and Mooney (2003) ) or classifiers (e.g., Freitag (1998) , Freitag and McCallum (2000) , Chieu et al. (2003) , Bunescu and Mooney (2004) ) to extract role fillers for events. Most IE systems consider only the immediate context surrounding a phrase when deciding whether to extract it. For tasks such as named entity recognition, immediate context is usually sufficient. But for more complex tasks, such as event extraction, a larger field of view is often needed to understand how facts tie together.",
                "cite_spans": [
                    {
                        "start": 77,
                        "end": 100,
                        "text": "Soderland et al. (1995)",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 103,
                        "end": 116,
                        "text": "Riloff (1996)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 119,
                        "end": 142,
                        "text": "Yangarber et al. (2000)",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 145,
                        "end": 169,
                        "text": "Califf and Mooney (2003)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 194,
                        "end": 208,
                        "text": "Freitag (1998)",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 211,
                        "end": 238,
                        "text": "Freitag and McCallum (2000)",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 241,
                        "end": 260,
                        "text": "Chieu et al. (2003)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 263,
                        "end": 288,
                        "text": "Bunescu and Mooney (2004)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Most IE systems are designed to identify role fillers that appear as arguments to event verbs or nouns, either explicitly via syntactic relations or implicitly via proximity (e.g., John murdered Tom or the murder of Tom by John). But many facts are presented in clauses that do not contain event words, requiring discourse relations or deep structural analysis to associate the facts with event roles. For example, consider the sentences below:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Seven people have died . . . and 30 were injured in India after terrorists launched an attack on the Taj Hotel.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": ". . . in Mexico City and its surrounding suburbs in a Swine Flu outbreak.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": ". . . after a tractor-trailer collided with a bus in Arkansas.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Two bridges were destroyed . . . in Baghdad last night in a resurgence of bomb attacks in the capital city.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": ". . . and $50 million in damage was caused by a hurricane that hit Miami on Friday.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": ". . . to make way for modern, safer bridges that will be constructed early next year.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "These examples illustrate a common phenomenon in text where information is not explicitly stated as filling an event role, but readers have no trouble making this inference. The role fillers above (seven people, two bridges) occur as arguments to verbs that reveal state information (death, destruction) but are not event-specific (i.e., death and destruction can result from a wide variety of incident types). IE systems often fail to extract these role fillers because these systems do not recognize the immediate context as being relevant to the specific type of event that they are looking for.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "We propose a new model for information extraction that incorporates both phrasal and sentential evidence in a unified framework. Our unified probabilistic model, called GLACIER, consists of two components: a model for sentential event recognition and a model for recognizing plausible role fillers. The Sentential Event Recognizer offers a probabilistic assessment of whether a sentence is discussing a domain-relevant event. The Plausible Role-Filler Recognizer is then conditioned to identify phrases as role fillers based upon the assumption that the surrounding context is discussing a relevant event. This unified probabilistic model allows the two components to jointly make decisions based upon both the local evidence surrounding each phrase and the \"peripheral vision\" afforded by the sentential event recognizer.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "This paper is organized as follows. Section 2 positions our research with respect to related work. Section 3 presents our unified probabilistic model for information extraction. Section 4 shows experimental results on two IE data sets, and Section 5 discusses directions for future work.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Many event extraction systems rely heavily on the local context around words or phrases that are candidates for extraction. Some systems use extraction patterns (Soderland et al., 1995; Riloff, 1996; Yangarber et al., 2000; Califf and Mooney, 2003) , which represent the immediate contexts surrounding candidate extractions. Similarly, classifierbased approaches (Freitag, 1998; Freitag and Mc-Callum, 2000; Chieu et al., 2003; Bunescu and Mooney, 2004) rely on features in the immediate context of the candidate extractions. Our work seeks to incorporate additional context into IE.",
                "cite_spans": [
                    {
                        "start": 161,
                        "end": 185,
                        "text": "(Soderland et al., 1995;",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 186,
                        "end": 199,
                        "text": "Riloff, 1996;",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 200,
                        "end": 223,
                        "text": "Yangarber et al., 2000;",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 224,
                        "end": 248,
                        "text": "Califf and Mooney, 2003)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 363,
                        "end": 378,
                        "text": "(Freitag, 1998;",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 379,
                        "end": 407,
                        "text": "Freitag and Mc-Callum, 2000;",
                        "ref_id": null
                    },
                    {
                        "start": 408,
                        "end": 427,
                        "text": "Chieu et al., 2003;",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 428,
                        "end": 453,
                        "text": "Bunescu and Mooney, 2004)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "Indeed, several recent approaches have shown the need for global information to improve IE performance. Maslennikov and Chua (2007) use discourse trees and local syntactic dependencies in a pattern-based framework to incorporate wider context. Finkel et al. (2005) and Ji and Grishman (2008) incorporate global information by enforcing event role or label consistency over a document or across related documents. In contrast, our approach simply creates a richer IE model for individual extractions by expanding the \"field of view\" to include the surrounding sentence.",
                "cite_spans": [
                    {
                        "start": 104,
                        "end": 131,
                        "text": "Maslennikov and Chua (2007)",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 244,
                        "end": 264,
                        "text": "Finkel et al. (2005)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 269,
                        "end": 291,
                        "text": "Ji and Grishman (2008)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "The two components of the unified model presented in this paper are somewhat similar to our previous work (Patwardhan and Riloff, 2007) , where we employ a relevant region identification phase prior to pattern-based extraction. In that work we adopted a pipeline paradigm, where a classifier identifies relevant sentences and only those sentences are fed to the extraction module. Our unified probabilistic model described in this paper does not draw a hard line between relevant and irrelevant sentences, but gently balances the influence of both local and sentential contexts through probability estimates.",
                "cite_spans": [
                    {
                        "start": 106,
                        "end": 135,
                        "text": "(Patwardhan and Riloff, 2007)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "We introduce a probabilistic model for eventbased IE that balances the influence of two kinds of contextual information. Our goal is to create a model that has the flexibility to make extraction decisions based upon strong evidence from the local context, or strong evidence from the wider context coupled with a more general local context. For example, some phrases explicitly refer to an event, so they almost certainly warrant extraction regardless of the wider context (e.g., terrorists launched an attack). 1 In contrast, some phrases are potentially relevant but too general to warrant extraction on their own (e.g., people died could be the result of different incident types). If we are confident that the sentence discusses an event of interest, however, then such phrases could be reliably extracted.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Unified IE Model that Combines Phrasal and Sentential Evidence",
                "sec_num": "3"
            },
            {
                "text": "Our unified model for IE (GLACIER) combines two types of contextual information by incorporating it into a probabilistic framework. To determine whether a noun phrase instance NP i should be extracted as a filler for an event role, GLACIER computes the joint probability that NP i :",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Unified IE Model that Combines Phrasal and Sentential Evidence",
                "sec_num": "3"
            },
            {
                "text": "(1) appears in an event sentence, and (2) is a legitimate filler for the event role. Thus, GLACIER is designed for noun phrase extraction and, mathematically, its decisions are based on the following joint probability:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Unified IE Model that Combines Phrasal and Sentential Evidence",
                "sec_num": "3"
            },
            {
                "text": "P (EvSent(S NP i ), PlausFillr (NP i ))",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Unified IE Model that Combines Phrasal and Sentential Evidence",
                "sec_num": "3"
            },
            {
                "text": "where S NP i is the sentence containing noun phrase NP i . This probability estimate is based on contextual features F appearing within S NP i and in the local context of NP i . Including F in the joint probability, and applying the product rule, we can split our probability into two components:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Unified IE Model that Combines Phrasal and Sentential Evidence",
                "sec_num": "3"
            },
            {
                "text": "P (EvSent(S NP i ), PlausFillr (NP i )|F ) = P (EvSent(S NP i )|F ) * P (PlausFillr (NP i )|EvSent(S NP i ), F )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Unified IE Model that Combines Phrasal and Sentential Evidence",
                "sec_num": "3"
            },
            {
                "text": "These two probability components, in the expression above, form the basis of the two modules in our IE system -the sentential event recognizer and the plausible role-filler recognizer. In arriving at a decision to extract a noun phrase, our unified model for IE uses these modules to estimate the two probabilities based on the set of contextual features F . Note that having these two probability components allows the system to gently balance the influence from the sentential and phrasal contexts, without having to make hard decisions about sentence relevance or phrases in isolation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Unified IE Model that Combines Phrasal and Sentential Evidence",
                "sec_num": "3"
            },
            {
                "text": "In this system, the sentential event recognizer is embodied in the probability component P (EvSent(S NP i )|F ). This is essentially the probability of a sentence describing a relevant event.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Unified IE Model that Combines Phrasal and Sentential Evidence",
                "sec_num": "3"
            },
            {
                "text": "Similarly, the plausible rolefiller recognizer is embodied by the probability P (PlausFillr (NP i )|EvSent(S NP i ), F ). This component, therefore, estimates the probability that a noun phrase fills a specific event role, assuming that the noun phrase occurs in an event sentence. Many different techniques could be used to produce these probability estimates. In the rest of this section, we present the specific models that we used for each of these components.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Unified IE Model that Combines Phrasal and Sentential Evidence",
                "sec_num": "3"
            },
            {
                "text": "The plausible role-filler recognizer is similar to most traditional IE systems, where the goal is to determine whether a noun phrase can be a legitimate filler for a specific type of event role based on its local context. Pattern-based approaches match the context surrounding a phrase using lexicosyntactic patterns or rules. However, most of these approaches do not produce probability estimates for the extractions. Classifier-based approaches use machine learning classifiers to make extraction decisions, based on features associated with the local context. Any classifier that can generate probability estimates, or similar confidence values, could be plugged into our model.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Plausible Role-Filler Recognizer",
                "sec_num": "3.1"
            },
            {
                "text": "In our work, we use a Na\u00efve Bayes classifier as our plausible role-filler recognizer. The probabilities are computed using a generative Na\u00efve Bayes framework, based on local contextual features surrounding a noun phrase. These clues include lexical matches, semantic features, and syntactic relations, and will be described in more detail in Section 3.3. The Na\u00efve Bayes (NB) plausible rolefiller recognizer is defined as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Plausible Role-Filler Recognizer",
                "sec_num": "3.1"
            },
            {
                "text": "P (PlausFillr (NP i )|EvSent(S NP i ), F ) = 1 Z P (PlausFillr (NP i )|EvSent(S NP i )) * f i \u2208F P (f i |PlausFillr (NP i ), EvSent(S NP i ))",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Plausible Role-Filler Recognizer",
                "sec_num": "3.1"
            },
            {
                "text": "where F is the set of local contextual features and Z is the normalizing constant. The prior P (PlausFillr (NP i )|EvSent(S NP i )) is estimated from the fraction of role fillers in the training data.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Plausible Role-Filler Recognizer",
                "sec_num": "3.1"
            },
            {
                "text": "The product term in the equation is the likelihood, which makes the simplifying assumption that all of the features in F are independent of one another. It is important to note that these probabilities are conditioned on the noun phrase NP i appearing in an event sentence.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Plausible Role-Filler Recognizer",
                "sec_num": "3.1"
            },
            {
                "text": "Most IE systems need to extract several different types of role fillers for each event. For instance, to extract information about terrorist incidents a system may extract the names of perpetrators, victims, targets, and weapons. We create a separate IE model for each type of event role. To construct a unified IE model for an event role, we must specifically create a plausible role-filler recognizer for that event role, but we can use a single sentential event recognizer for all of the role filler types.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Plausible Role-Filler Recognizer",
                "sec_num": "3.1"
            },
            {
                "text": "The task at hand for the sentential event recognizer is to analyze features in a sentence and estimate the probability that the sentence is discussing a relevant event. This is very similar to the task performed by text classification systems, with some minor differences. Firstly, we are dealing with the classification of sentences, as opposed to entire documents. Secondly, we need to generate a probability estimate of the \"class\", and not just a class label. Like the plausible role-filler recognizer, here too we employ machine learning classifiers to estimate the desired probabilities.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Sentential Event Recognizer",
                "sec_num": "3.2"
            },
            {
                "text": "Since Na\u00efve Bayes classifiers estimate class probabilities, we employ such a classifier to create a sentential event recognizer:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Na\u00efve Bayes Event Recognizer",
                "sec_num": "3.2.1"
            },
            {
                "text": "P (EvSent(S NP i )|F ) = 1 Z P (EvSent(S NP i )) * f i \u2208F P (f i |EvSent(S NP i ))",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Na\u00efve Bayes Event Recognizer",
                "sec_num": "3.2.1"
            },
            {
                "text": "where Z is the normalizing constant and F is the set of contextual features in the sentence. The prior P (EvSent S(NP i ) ) is obtained from the ratio of event and non-event sentences in the training data. The product term in the equation is the likelihood, which makes the simplifying assumption that the features in F are independent of one another. The features used by the model will be described in Section 3.3. A known issue with Na\u00efve Bayes classifiers is that, even though their classification accuracy is often quite reasonable, their probability estimates are often poor (Domingos and Pazzani, 1996; Zadrozny and Elkan, 2001; Manning et al., 2008) . The problem is that these classifiers tend to overestimate the probability of the predicted class, resulting in a situation where most probability estimates from the classifier tend to be either extremely close to 0.0 or extremely close to 1.0. We observed this problem in our classifier too, so we decided to explore an additional model to estimate probabilities for the sentential event recognizer. This second model, based on SVMs, is described next.",
                "cite_spans": [
                    {
                        "start": 581,
                        "end": 609,
                        "text": "(Domingos and Pazzani, 1996;",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 610,
                        "end": 635,
                        "text": "Zadrozny and Elkan, 2001;",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 636,
                        "end": 657,
                        "text": "Manning et al., 2008)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Na\u00efve Bayes Event Recognizer",
                "sec_num": "3.2.1"
            },
            {
                "text": "Given the all-or-nothing nature of the probability estimates that we observed from the Na\u00efve Bayes model, we decided to try using a Support Vector Machine (SVM) (Vapnik, 1995; Joachims, 1998) classifier as an alternative to Na\u00efve Bayes. One of the issues with doing this is that SVMs are not probabilistic classifiers. SVMs make classification decisions using on a decision boundary defined by support vectors identified during training. A decision function is applied to unseen test examples to determine which side of the decision boundary those examples lie. While the values obtained from the decision function only indicate class assignments for the examples, we used these values to produce confidence scores for our sentential event recognizer.",
                "cite_spans": [
                    {
                        "start": 161,
                        "end": 175,
                        "text": "(Vapnik, 1995;",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 176,
                        "end": 191,
                        "text": "Joachims, 1998)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "SVM Event Recognizer",
                "sec_num": "3.2.2"
            },
            {
                "text": "To produce a confidence score from the SVM classifier, we take the values generated by the decision function for each test instance and normalize them based on the minimum and maximum values produced across all of the test instances. This normalization process produces values between 0 and 1 that we use as a rough indicator of the confidence in the SVM's classification. We observed that we could effect a consistent recall/precision trade-off by using these values as thresholds for classification decisions, which suggests that this approach worked reasonably well for our task.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "SVM Event Recognizer",
                "sec_num": "3.2.2"
            },
            {
                "text": "We used a variety of contextual features in both components of our system. The plausible rolefiller recognizer uses the following types of features for each candidate noun phrase NP i : lexical head of NP i , semantic class of NP i 's lexical head, named entity tags associated with NP i and lexicosyntactic patterns that represent the local context surrounding NP i . The feature set is automatically generated from the texts. Each feature is assigned a binary value for each instance, indicating either the presence or absence of the feature.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Contextual Features",
                "sec_num": "3.3"
            },
            {
                "text": "The named-entity features are generated by the freely available Stanford NER tagger (Finkel et al., 2005) . We use the pre-trained NER model that comes with the software to identify person, organization and location names. The syntactic and semantic features are generated by the Sundance/AutoSlog system (Riloff and Phillips, 2004) . We use the Sundance shallow parser to identify lexical heads, and use its semantic dictionaries to assign semantic features to words. The AutoSlog pattern generator (Riloff, 1996) is used to create the lexico-syntactic pattern features that capture local context around each noun phrase.",
                "cite_spans": [
                    {
                        "start": 84,
                        "end": 105,
                        "text": "(Finkel et al., 2005)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 305,
                        "end": 332,
                        "text": "(Riloff and Phillips, 2004)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 500,
                        "end": 514,
                        "text": "(Riloff, 1996)",
                        "ref_id": "BIBREF14"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Contextual Features",
                "sec_num": "3.3"
            },
            {
                "text": "Our training sets produce a very large number of features, which initially bogged down our classifiers. Consequently, we reduced the size of the feature set by discarding all features that appeared four times or less in the training set.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Contextual Features",
                "sec_num": "3.3"
            },
            {
                "text": "Our sentential event recognizer uses the same contextual features as the plausible role-filler recognizer, except that features are generated for every NP in the sentence. In addition, it uses three types of sentence-level features: sentence length, bag of words, and verb tense, which are also binary features. We have two binary sentence length features indicating that the sentence is long (greater than 35 words) or is short (shorter than 5 words). Additionally, all of the words in each sentence in the training data are generated as bag of words features for the sentential model. Finally, we generate verb tense features from all verbs appearing in each sentence. Here too we apply a frequency cutoff and eliminate all features that appear four times or less in the training data.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Contextual Features",
                "sec_num": "3.3"
            },
            {
                "text": "We evaluated the performance of our IE system on two data sets: the MUC-4 terrorism corpus (Sund-heim, 1992) , and a ProMed disease outbreaks corpus (Phillips and Riloff, 2007; Patwardhan and Riloff, 2007) . The MUC-4 data set is a standard IE benchmark collection of news stories about terrorist events. It contains 1700 documents divided into 1300 development (DEV) texts, and four test sets of 100 texts each (TST1, TST2, TST3, and TST4). Unless otherwise stated, our experiments adopted the same training/test split used in previous research: the 1300 DEV texts for training, 200 texts (TST1+TST2) for tuning, and 200 texts (TST3+TST4) as the blind test set. We evaluated our system on five MUC-4 string roles: perpetrator individuals, perpetrator organizations, physical targets, victims, and weapons.",
                "cite_spans": [
                    {
                        "start": 91,
                        "end": 108,
                        "text": "(Sund-heim, 1992)",
                        "ref_id": null
                    },
                    {
                        "start": 149,
                        "end": 176,
                        "text": "(Phillips and Riloff, 2007;",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 177,
                        "end": 205,
                        "text": "Patwardhan and Riloff, 2007)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data Sets",
                "sec_num": "4.1"
            },
            {
                "text": "The ProMed corpus consists of 120 documents obtained from ProMed-mail2 , a freely accessible global electronic reporting system for outbreaks of diseases. These 120 documents are paired with corresponding answer key templates. Unless otherwise noted, all of our experiments on this data set used 5-fold cross validation. We extracted two types of event roles: diseases and victims3 .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data Sets",
                "sec_num": "4.1"
            },
            {
                "text": "Unlike some other IE data sets, many of the texts in these collections do not describe a relevant event. Only about half of the MUC-4 articles describe a specific terrorist incident4 , and only about 80% of the ProMed articles describe a disease outbreak. The answer keys for the irrelevant documents are therefore empty. IE systems are especially susceptible to false hits when they can be given texts that contain no relevant events.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data Sets",
                "sec_num": "4.1"
            },
            {
                "text": "The complete IE task involves the creation of answer key templates, one template per incident (many documents in our data sets describe multiple events). Our work focuses on accurately extracting the facts from the text and not on template generation per se (e.g., we are not concerned with coreference resolution or which extraction belongs in which template). Consequently, our experiments evaluate the accuracy of the extractions individually. We used head noun scoring, where an extraction is considered to be correct if its head noun matches the head noun in the answer key.5 ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data Sets",
                "sec_num": "4.1"
            },
            {
                "text": "We generated three baselines to use as comparisons with our IE system. As our first baseline, we used AutoSlog-TS (Riloff, 1996) , which is a weakly-supervised, pattern-based IE system available as part of the Sundance/AutoSlog software package (Riloff and Phillips, 2004) . Our previous work in event-based IE (Patwardhan and Riloff, 2007 ) also used a pattern-based approach that applied semantic affinity patterns to relevant regions in text. We use this system as our second baseline. As a third baseline, we trained a Na\u00efve Bayes IE classifier that is analogous to the plausible rolefiller recognizer in our unified IE model, except that this baseline system is not conditioned on the assumption of having an event sentence. Consequently, this baseline NB classifier is akin to a traditional supervised learning-based IE system that uses only local contextual features to make extraction decisions. Formally, the baseline NB classifier uses the formula:",
                "cite_spans": [
                    {
                        "start": 114,
                        "end": 128,
                        "text": "(Riloff, 1996)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 245,
                        "end": 272,
                        "text": "(Riloff and Phillips, 2004)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 311,
                        "end": 339,
                        "text": "(Patwardhan and Riloff, 2007",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baselines",
                "sec_num": "4.2"
            },
            {
                "text": "P (PlausFillr (NP i )|F ) = 1 Z P (PlausFillr (NP i )) * f i \u2208F P (f i |PlausFillr (NP i ))",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baselines",
                "sec_num": "4.2"
            },
            {
                "text": "where F is the set of local features, P (PlausFillr (NP i )) is the prior probability, and Z is the normalizing constant. We used the Weka (Witten and Frank, 2005) implementation of Na\u00efve Bayes for this baseline NB system.",
                "cite_spans": [
                    {
                        "start": 139,
                        "end": 163,
                        "text": "(Witten and Frank, 2005)",
                        "ref_id": "BIBREF18"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baselines",
                "sec_num": "4.2"
            },
            {
                "text": "New Jersey, February, 26. An outbreak of Ebola has been confirmed in Mercer County, New Jersey. Five teenage boys appear to have contracted the deadly virus from an unknown source. The CDC is investigating the cases and is taking measures to prevent the spread. . .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baselines",
                "sec_num": "4.2"
            },
            {
                "text": "Five teenage boys Location:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Disease: Ebola Victims:",
                "sec_num": null
            },
            {
                "text": "Mercer County, New Jersey Date:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Disease: Ebola Victims:",
                "sec_num": null
            },
            {
                "text": "February 26 Both the MUC-4 and ProMed data sets have separate answer keys rather than annotated source documents. Figure 1 shows an example of a document and its corresponding answer key template. To train the baseline NB system, we identify all instances of each answer key string in the source document and consider every instance a positive training example. This produces noisy training data, however, because some instances occur in TS .33 .49 .40 .52 .33 .41 .54 .59 .56 .49 .54 .51 .38 .44 .41 Sem Affinity .48 .39 .43 .36 .58 .45 .56 .46 .50 .46 .44 .45 .53 .46 .50 NB .50 .36 .34 .35 .35 .46 .40 .53 .49 .51 .50 .50 .50 1.00 .05 .10 NB .70 .41 .25 .31 .43 .31 .36 .58 .42 .48 .58 .37 .45 1.00 .04 .07 NB .90 .51 .17 .25 .56 .15 .24 .67 .30 .41 .75 .23 .36 1.00 .02 .04 Table 1 : Baseline Results on MUC-4 TS .33 .60 .43 .36 .49 .41 Sem Affinity .31 .49 .38 .41 .47 .44 NB .50 .20 .73 .31 .29 .56 .39 NB .70 .23 .67 .34 .37 .52 .44 NB .90 .34 .59 .43 .47 .39 .43 Table 2 : Baseline Results on ProMed undesirable contexts. For example, if the string \"man\" appears in an answer key as a victim, one instance of \"man\" may refer to the actual victim in an event sentence, while another instance of \"man\" may occur in a non-event context (e.g., background information) or may refer to a completely different person.",
                "cite_spans": [
                    {
                        "start": 438,
                        "end": 777,
                        "text": "TS .33 .49 .40 .52 .33 .41 .54 .59 .56 .49 .54 .51 .38 .44 .41 Sem Affinity .48 .39 .43 .36 .58 .45 .56 .46 .50 .46 .44 .45 .53 .46 .50 NB .50 .36 .34 .35 .35 .46 .40 .53 .49 .51 .50 .50 .50 1.00 .05 .10 NB .70 .41 .25 .31 .43 .31 .36 .58 .42 .48 .58 .37 .45 1.00 .04 .07 NB .90 .51 .17 .25 .56 .15 .24 .67 .30 .41 .75 .23 .36 1.00 .02 .04",
                        "ref_id": null
                    },
                    {
                        "start": 814,
                        "end": 970,
                        "text": "TS .33 .60 .43 .36 .49 .41 Sem Affinity .31 .49 .38 .41 .47 .44 NB .50 .20 .73 .31 .29 .56 .39 NB .70 .23 .67 .34 .37 .52 .44 NB .90 .34 .59 .43 .47 .39 .43",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 121,
                        "end": 122,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    },
                    {
                        "start": 784,
                        "end": 785,
                        "text": "1",
                        "ref_id": null
                    },
                    {
                        "start": 977,
                        "end": 978,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Disease: Ebola Victims:",
                "sec_num": null
            },
            {
                "text": "PerpInd PerpOrg Target Victim Weapon P R F P R F P R F P R F P R F AutoSlog-",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Disease: Ebola Victims:",
                "sec_num": null
            },
            {
                "text": "Disease Victim P R F P R F AutoSlog-",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Disease: Ebola Victims:",
                "sec_num": null
            },
            {
                "text": "We report three evaluation metrics in our experiments: precision (P), recall (R), and F-score (F), where recall and precision are equally weighted. For the Na\u00efve Bayes classifier, the natural threshold for distinguishing between positive and negative classes is 0.5, but we also evaluated this classifier with thresholds of 0.7 and 0.9 to see if we could effect a recall/precision trade-off. Tables 1 and 2 present the results of our three baseline systems. The NB classifier performs comparably to AutoSlog-TS and Semantic Affinity on most event roles, although a threshold of 0.90 is needed to reach comparable performance on ProMed. The relatively low numbers across the board indicate that these corpora are challenging, but these results suggest that our plausible role-filler recognizer is competitive with other existing IE systems. In Section 4.4 we will show how our unified IE model compares to these baselines. But before that (in the next section) we evaluate the quality of the second component of our IE system: the sentential event recognizer.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 399,
                        "end": 400,
                        "text": "1",
                        "ref_id": null
                    },
                    {
                        "start": 405,
                        "end": 406,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Disease: Ebola Victims:",
                "sec_num": null
            },
            {
                "text": "The sentential event recognizer is one of the core contributions of this research, so in this section we evaluate it by itself, before we employ it within the unified framework. The purpose of the sentential event recognizer is to determine whether a sentence is discussing a domain-relevant event. For our data sets, the classifier must decide whether a sentence is discussing a terrorist incident (MUC-4) or a disease outbreak (ProMed). Ideally, we want such a classifier to operate independently from the answer keys and the extraction task per se. For example, a terrorism IE system could be designed to extract only perpetrators and victims of terrorist events, or it could be designed to extract only targets and locations. The job of the sentential event recognizer remains the same: to identify sentences that discuss a terrorist event. How to train and evaluate such a system is a difficult question. In this section, we present two approaches that we explored to generate the training data: (a) using the IE answer keys, and (b) using human judgements.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Sentential Event Recognizer Models",
                "sec_num": "4.3"
            },
            {
                "text": "We have argued that the event relevance of a sentence should not be tied to a specific set of event roles. However, the IE answer keys can be used to identify some sentences that describe an event, because they contain an answer string. So we can map the answer strings back to sentences in the source documents to automatically generate event sentence annotations.6 These annotations will be noisy, though, because an answer string can appear in a non-event sentence, and some event sentences may not contain any answer strings. The alternative, however, is sentence annotations by humans, which (as we will discuss in Section 4.3.2) is challenging.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Sentence Annotation via Answer Keys",
                "sec_num": "4.3.1"
            },
            {
                "text": "For many sentences there is a clear consensus among people that an event is being discussed. For example, most readers would agree that sentence (1) below is describing a terrorist event, while sen- 2) is not. However it is difficult to draw a clear line. Sentence (3), for example, describes an action taken in response to a terrorist event. Is this a terrorist event sentence? Precisely how to define an event sentence is not obvious.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Sentence Annotation via Human Judgements",
                "sec_num": "4.3.2"
            },
            {
                "text": "(1) Al Qaeda operatives launched an attack on the Madrid subway system.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Sentence Annotation via Human Judgements",
                "sec_num": "4.3.2"
            },
            {
                "text": "(2) Madrid has a population of about 3.2 million people.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Sentence Annotation via Human Judgements",
                "sec_num": "4.3.2"
            },
            {
                "text": "(3) City officials stepped up security in response to the attacks.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Sentence Annotation via Human Judgements",
                "sec_num": "4.3.2"
            },
            {
                "text": "We tackled this issue by creating detailed annotation guidelines to define the notion of an event sentence, and conducting a human annotation study. The guidelines delineated a general time frame for the beginning and end of an event, and constrained the task to focus on specific incidents that were reported in the IE answer key. We gave the annotators a brief description (e.g., murder in Peru) of each event that had a filled answer key in the data set. They only labeled sentences that discussed those particular events.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Sentence Annotation via Human Judgements",
                "sec_num": "4.3.2"
            },
            {
                "text": "We employed two human judges, who annotated 120 documents from the ProMed test set, and 100 documents from the MUC-4 test set. We asked both judges to label 30 of the same documents from each data set so that we could compute inter-annotator agreement. The annotators had an agreement of 0.72 Cohen's \u03ba on the ProMed data, and 0.77 Cohen's \u03ba on the MUC-4 data. Given the difficulty of this task, we were satisfied that this task is reasonably well-defined and the annotations are of good quality.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Sentence Annotation via Human Judgements",
                "sec_num": "4.3.2"
            },
            {
                "text": "We evaluated the two sentential event recognizer models described in Section 3.2 in two ways:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Event Recognizer Results",
                "sec_num": "4.3.3"
            },
            {
                "text": "(1) using the answer key sentence annotations for training/testing, and (2) using the human annotations for training/testing. Table 3 shows the results for all combinations of training/testing data. Since we only have human annotations for 100 MUC-4 texts and 120 ProMed texts, we performed 5-fold cross-validation on these documents. For our classifiers, we used the Weka (Witten and Frank, 2005) implementation of Na\u00efve Bayes and the SVMLight (Joachims, 1998) implementation of the SVM. For each classifier we report overall accuracy, and precision, recall and F-scores with respect to both the positive and negative classes (event vs. non-event sentences).",
                "cite_spans": [
                    {
                        "start": 373,
                        "end": 397,
                        "text": "(Witten and Frank, 2005)",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 445,
                        "end": 461,
                        "text": "(Joachims, 1998)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 132,
                        "end": 133,
                        "text": "3",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Event Recognizer Results",
                "sec_num": "4.3.3"
            },
            {
                "text": "The rows labeled Ans show the results for models trained via answer keys, and the rows labeled Hum show the results for the models trained with human annotations. The left side of the table shows the results using the answer key annotations for evaluation, and the right side of the table shows the results using the human annotations for evaluation. One expects classifiers to perform best when they are trained and tested on the same type of data, and our results bear this out -the classifiers that were trained and tested on the same kind of annotations do best. The boldfaced numbers represent the best accuracies achieved for each domain. As we would expect, the classifiers that are both trained and tested with human annotations (Hum) show the best performance, with the Na\u00efve Bayes achieving the best accuracy of 85% on the event are often discussed later in a document, far removed from the main event description. For example, a statement that Al Qaeda is believed to be responsible for an attack would typically appear after the event description. As a result, the sentential event recognizer tends to generate low probabilities for such sentences. We believe that addressing this issue would require the use of discourse relations or the use of even larger context sizes. We intend to explore these avenues of research in future work.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Event Recognizer Results",
                "sec_num": "4.3.3"
            },
            {
                "text": "On the ProMed data, GLACIER produces results that are similar to the baselines for the Victim role, but it outperforms the baselines for the Disease role. We find that for this domain, the unified IE model with the Na\u00efve Bayes sentential event recognizer is superior to the unified IE model with the SVM classifier. For the Disease role, the Fscore jumped 6%, from 0.43 for the best baseline systems (AutoSlog-TS and the NB baseline) to 0.49 for GLACIER NB/NB . In contrast to the MUC-4 data, this improvement was mostly due to an increase in precision (up to 0.41), indicating that our unified IE model was effective at eliminating many false hits. For the Victim role, the performance of the unified model is comparable to the baselines. On this event role, the F-score of GLACIER NB/NB (0.44) matches that of the best baseline system (Sem Affinity, with 0.44). However, note that GLACIER NB/NB can achieve a 5% gain in recall over this baseline, at the cost of a 3% precision loss.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Event Recognizer Results",
                "sec_num": "4.3.3"
            },
            {
                "text": "Figure 2 presents some specific examples of extractions that are failed to be extracted by the baseline models, but are correctly identified by GLACIER because of its use of sentential evidence. Observe that in each of these examples, GLACIER correctly extracts the underlined phrases, in spite of the inconclusive evidence in the local contexts around them. In the last sentence in Figure 2 , for example, GLACIER correctly makes the inference that the policemen in the bus (which was traveling on the bridge) are likely the victims of the terrorist event. Thus, we see that our system manages to balance the influence of the two probability components to make extraction decisions that would be impossible to make by relying only on the local phrasal context. In addition, the sentential event recognizer can also help improve precision by pre- ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "2",
                        "ref_id": null
                    },
                    {
                        "start": 390,
                        "end": 391,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Specific Examples",
                "sec_num": "4.5"
            },
            {
                "text": "We presented a unified model for IE that balances the influence of sentential context with local contextual evidence to improve the performance of event-based IE. Our experimental results showed that using sentential contexts indeed produced better results on two IE data sets. Our current model uses supervised learning, so one direction for future work is to adapt the model for weakly supervised learning. We also plan to incorporate discourse features and investigate even wider contexts to capture broader discourse effects.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusions",
                "sec_num": "5"
            },
            {
                "text": "There are always exceptions of course, such as hypothetical statements, but they are relatively uncommon.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "http://www.promedmail.org",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "The \"victims\" can be people, animals, or plants.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "With respect to the definition of terrorist incidents in the MUC-4 guidelines(Sundheim, 1992).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Pronouns were discarded from both the system responses and the answer keys since we do not perform coreference resolution. Duplicate extractions (e.g., the same string extracted multiple times from the same document) were conflated before being scored, so they count as just one hit or one miss.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "A similar strategy was used in previous work(Patwardhan and Riloff, 2007) to generate a test set for the evaluation of a relevant region classifier.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "This work has been supported in part by the Department of Homeland Security Grant N0014-07-1-0152. We are grateful to Nathan Gilbert and Adam Teichert for their help with the annotation of event sentences.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgments",
                "sec_num": null
            },
            {
                "text": " .33 .49 .40 .52 .33 .41 .54 .59 .56 .49 .54 .51 .38 .44 .41 Sem Affinity .48 .39 .43 .36 .58 .45 .56 .46 .50 .46 .44 .45 .53 .46 .50 NB (baseline) . 36 .34 .35 .35 .46 .40 .53 .49 .51 .50 .50 .50 1.00 .05 .10 GLACIER NB/NB . 90 .39 .59 .47 .33 .51 .40 .39 .72 .51 .52 .54 .53 .47 .55 The recall and precision for non-event sentences is much higher than for event sentences. This classifier is forced to draw a hard line between the event and non-event sentences, which is a difficult task even for people. One of the advantages of our unified IE model, which will be described in the next section, is that it does not require hard decisions but instead uses a probabilistic estimate of how \"event-ish\" a sentence is.Table 3 showed that models trained on human annotations outperform models trained on answer key annotations. But with the MUC-4 data, we have the luxury of 1300 training documents with answer keys, while we only have 100 documents with human annotations. Even though the answer key annotations are noisier, we have 13 times as much training data.So we trained another sentential event recognizer using the entire MUC-4 training set. These results are shown in Table 4 . Observe that using this larger (albeit noisy) training data does not appear to affect the Na\u00efve Bayes model very much. Compared with the model trained on 100 manually annotated documents, its accuracy decreases by 2% from 85% to 83%. The SVM model, on the other hand, achieves an 89% accuracy when trained with the larger MUC-4 training data, compared to 84% accuracy for the model trained from the 100 manually labeled documents. Consequently, the sentential event recognizer models used in our unified IE framework (described in Section 4.4) are trained with this 1300 document training set.",
                "cite_spans": [
                    {
                        "start": 1,
                        "end": 147,
                        "text": ".33 .49 .40 .52 .33 .41 .54 .59 .56 .49 .54 .51 .38 .44 .41 Sem Affinity .48 .39 .43 .36 .58 .45 .56 .46 .50 .46 .44 .45 .53 .46 .50 NB (baseline)",
                        "ref_id": null
                    },
                    {
                        "start": 150,
                        "end": 192,
                        "text": "36 .34 .35 .35 .46 .40 .53 .49 .51 .50 .50",
                        "ref_id": null
                    },
                    {
                        "start": 226,
                        "end": 284,
                        "text": "90 .39 .59 .47 .33 .51 .40 .39 .72 .51 .52 .54 .53 .47 .55",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 723,
                        "end": 724,
                        "text": "3",
                        "ref_id": null
                    },
                    {
                        "start": 1183,
                        "end": 1184,
                        "text": "4",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "annex",
                "sec_num": null
            },
            {
                "text": "We now evaluate the performance of our unified IE model, GLACIER, which allows a plausible rolefiller recognizer and a sentential event recognizer to make joint decisions about phrase extractions. Tables 5 and 6 present the results of the unified The NB/NB systems use Na\u00efve Bayes models for both components, while the NB/SVM systems use a Na\u00efve Bayes model for the plausible role-filler recognizer and an SVM for the sentential event recognizer. As with our baseline system, we obtain good results using a threshold of 0.90 for our NB/NB model (i.e., only NPs with probability \u2265 0.90 are extracted). For our NB/SVM models, we evaluated using the default threshold (0.50) but observed that recall was sometimes low. So we also use a threshold of 0.40, which produces superior results. Here too, we used the Weka (Witten and Frank, 2005) implementation of the Na\u00efve Bayes model and the SVMLight (Joachims, 1998) implementation of the SVM.For the MUC-4 data, our unified IE model using the SVM (0.40) outperforms all 3 baselines on three roles (PerpInd, Victim, Weapon) and outperforms 2 of the 3 baselines on the Target role. When GLACIER outperforms the other systems it is often by a wide margin: the F-score for PerpInd jumped from 0.43 for the best baseline (Sem Affinity) to 0.54 for GLACIER, and the F-scores for Victim and Weapon each improved by 5% over the best baseline. These gains came from both increased recall and increased precision, demonstrating that GLACIER extracts some information that was missed by the other systems and is also less prone to false hits.Only the PerpOrg role shows inferior performance. Organizations perpetrating a terrorist",
                "cite_spans": [
                    {
                        "start": 812,
                        "end": 836,
                        "text": "(Witten and Frank, 2005)",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 894,
                        "end": 910,
                        "text": "(Joachims, 1998)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 204,
                        "end": 205,
                        "text": "5",
                        "ref_id": null
                    },
                    {
                        "start": 210,
                        "end": 211,
                        "text": "6",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Evaluation of the Unified IE Model",
                "sec_num": "4.4"
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Collective Information Extraction with Relational Markov Networks",
                "authors": [
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Bunescu",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Mooney",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "438--445",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "R. Bunescu and R. Mooney. 2004. Collective In- formation Extraction with Relational Markov Net- works. In Proceedings of the 42nd Annual Meet- ing of the Association for Computational Linguis- tics, pages 438-445, Barcelona, Spain, July.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Bottom-Up Relational Learning of Pattern Matching Rules for Information Extraction",
                "authors": [
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Califf",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Mooney",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Journal of Machine Learning Research",
                "volume": "4",
                "issue": "",
                "pages": "177--210",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "M. Califf and R. Mooney. 2003. Bottom-Up Rela- tional Learning of Pattern Matching Rules for In- formation Extraction. Journal of Machine Learning Research, 4:177-210, December.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Closing the Gap: Learning-Based Information Extraction Rivaling Knowledge-Engineering Methods",
                "authors": [
                    {
                        "first": "H",
                        "middle": [],
                        "last": "Chieu",
                        "suffix": ""
                    },
                    {
                        "first": "H",
                        "middle": [],
                        "last": "Ng",
                        "suffix": ""
                    },
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "216--223",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "H. Chieu, H. Ng, and Y. Lee. 2003. Closing the Gap: Learning-Based Information Extraction Rival- ing Knowledge-Engineering Methods. In Proceed- ings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 216-223, Sap- poro, Japan, July.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Beyond Independence: Conditions for the Optimality of the Simple Bayesian Classifier",
                "authors": [
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Domingos",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Pazzani",
                        "suffix": ""
                    }
                ],
                "year": 1996,
                "venue": "Proceedings of the Thirteenth International Conference on Machine Learning",
                "volume": "",
                "issue": "",
                "pages": "105--112",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "P. Domingos and M. Pazzani. 1996. Beyond Inde- pendence: Conditions for the Optimality of the Sim- ple Bayesian Classifier. In Proceedings of the Thir- teenth International Conference on Machine Learn- ing, pages 105-112, Bari, Italy, July.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling",
                "authors": [
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Finkel",
                        "suffix": ""
                    },
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Grenager",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "363--370",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "J. Finkel, T. Grenager, and C. Manning. 2005. In- corporating Non-local Information into Information Extraction Systems by Gibbs Sampling. In Proceed- ings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 363-370, Ann Arbor, MI, June.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Information Extraction with HMM Structures Learned by Stochastic Optimization",
                "authors": [
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Freitag",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Mccallum",
                        "suffix": ""
                    }
                ],
                "year": 2000,
                "venue": "Proceedings of the Seventeenth National Conference on Artificial Intelligence",
                "volume": "",
                "issue": "",
                "pages": "584--589",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "D. Freitag and A. McCallum. 2000. Informa- tion Extraction with HMM Structures Learned by Stochastic Optimization. In Proceedings of the Sev- enteenth National Conference on Artificial Intelli- gence, pages 584-589, Austin, TX, August.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Toward General-Purpose Learning for Information Extraction",
                "authors": [
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Freitag",
                        "suffix": ""
                    }
                ],
                "year": 1998,
                "venue": "Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "404--408",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "D. Freitag. 1998. Toward General-Purpose Learning for Information Extraction. In Proceedings of the 36th Annual Meeting of the Association for Compu- tational Linguistics and 17th International Confer- ence on Computational Linguistics, pages 404-408, Montreal, Quebec, August.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Refining Event Extraction through Cross-Document Inference",
                "authors": [
                    {
                        "first": "H",
                        "middle": [],
                        "last": "Ji",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Grishman",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Proceedings of ACL-08: HLT",
                "volume": "",
                "issue": "",
                "pages": "254--262",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "H. Ji and R. Grishman. 2008. Refining Event Ex- traction through Cross-Document Inference. In Pro- ceedings of ACL-08: HLT, pages 254-262, Colum- bus, OH, June.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Text Categorization with Support Vector Machines: Learning with Many Relevant Features",
                "authors": [
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Joachims",
                        "suffix": ""
                    }
                ],
                "year": 1998,
                "venue": "Proceedings of the Tenth European Conference on Machine Learning",
                "volume": "",
                "issue": "",
                "pages": "137--142",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "T. Joachims. 1998. Text Categorization with Sup- port Vector Machines: Learning with Many Rele- vant Features. In Proceedings of the Tenth European Conference on Machine Learning, pages 137-142, April.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Introduction to Information Retrieval",
                "authors": [
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Manning",
                        "suffix": ""
                    },
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Raghavan",
                        "suffix": ""
                    },
                    {
                        "first": "H",
                        "middle": [],
                        "last": "Sch\u00fctze",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "C. Manning, P. Raghavan, and H Sch\u00fctze. 2008. Intro- duction to Information Retrieval. Cambridge Uni- versity Press, New York, NY.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "A Multiresolution Framework for Information Extraction from Free Text",
                "authors": [
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Maslennikov",
                        "suffix": ""
                    },
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Chua",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "592--599",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "M. Maslennikov and T. Chua. 2007. A Multi- resolution Framework for Information Extraction from Free Text. In Proceedings of the 45th Annual Meeting of the Association of Computational Lin- guistics, pages 592-599, Prague, Czech Republic, June.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Effective Information Extraction with Semantic Affinity Patterns and Relevant Regions",
                "authors": [
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Patwardhan",
                        "suffix": ""
                    },
                    {
                        "first": "E",
                        "middle": [],
                        "last": "Riloff",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning",
                "volume": "",
                "issue": "",
                "pages": "717--727",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "S. Patwardhan and E. Riloff. 2007. Effective Informa- tion Extraction with Semantic Affinity Patterns and Relevant Regions. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Lan- guage Processing and Computational Natural Lan- guage Learning, pages 717-727, Prague, Czech Re- public, June.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Exploiting Role-Identifying Nouns and Expressions for Information Extraction",
                "authors": [
                    {
                        "first": "W",
                        "middle": [],
                        "last": "Phillips",
                        "suffix": ""
                    },
                    {
                        "first": "E",
                        "middle": [],
                        "last": "Riloff",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Proceedings of International Conference on Recent Advances in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "165--172",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "W. Phillips and E. Riloff. 2007. Exploiting Role- Identifying Nouns and Expressions for Informa- tion Extraction. In Proceedings of International Conference on Recent Advances in Natural Lan- guage Processing, pages 165-172, Borovets, Bul- garia, September.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "An Introduction to the Sundance and AutoSlog Systems",
                "authors": [
                    {
                        "first": "E",
                        "middle": [],
                        "last": "Riloff",
                        "suffix": ""
                    },
                    {
                        "first": "W",
                        "middle": [],
                        "last": "Phillips",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "E. Riloff and W. Phillips. 2004. An Introduction to the Sundance and AutoSlog Systems. Technical Report UUCS-04-015, School of Computing, University of Utah.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Automatically Generating Extraction Patterns from Untagged Text",
                "authors": [
                    {
                        "first": "E",
                        "middle": [],
                        "last": "Riloff",
                        "suffix": ""
                    }
                ],
                "year": 1996,
                "venue": "Proceedings of the Thirteenth National Conference on Articial Intelligence",
                "volume": "",
                "issue": "",
                "pages": "1044--1049",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "E. Riloff. 1996. Automatically Generating Extraction Patterns from Untagged Text. In Proceedings of the Thirteenth National Conference on Articial Intelli- gence, pages 1044-1049, Portland, OR, August.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "CRYSTAL: Inducing a Conceptual Dictionary",
                "authors": [
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Soderland",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Fisher",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Aseltine",
                        "suffix": ""
                    },
                    {
                        "first": "W",
                        "middle": [],
                        "last": "Lehnert",
                        "suffix": ""
                    }
                ],
                "year": 1995,
                "venue": "Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence",
                "volume": "",
                "issue": "",
                "pages": "1314--1319",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "S. Soderland, D. Fisher, J. Aseltine, and W. Lehnert. 1995. CRYSTAL: Inducing a Conceptual Dictio- nary. In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, pages 1314-1319, Montreal, Canada, August.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Overview of the Fourth Message Understanding Evaluation and Conference",
                "authors": [
                    {
                        "first": "B",
                        "middle": [],
                        "last": "Sundheim",
                        "suffix": ""
                    }
                ],
                "year": 1992,
                "venue": "Proceedings of the Fourth Message Understanding Conference (MUC-4)",
                "volume": "",
                "issue": "",
                "pages": "3--21",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "B. Sundheim. 1992. Overview of the Fourth Message Understanding Evaluation and Conference. In Pro- ceedings of the Fourth Message Understanding Con- ference (MUC-4), pages 3-21, McLean, VA, June.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "The Nature of Statistical Learning Theory",
                "authors": [
                    {
                        "first": "V",
                        "middle": [],
                        "last": "Vapnik",
                        "suffix": ""
                    }
                ],
                "year": 1995,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "V. Vapnik. 1995. The Nature of Statistical Learning Theory. Springer, New York, NY.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Data Mining -Practical Machine Learning Tools and Techniques",
                "authors": [
                    {
                        "first": "I",
                        "middle": [],
                        "last": "Witten",
                        "suffix": ""
                    },
                    {
                        "first": "E",
                        "middle": [],
                        "last": "Frank",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "I. Witten and E. Frank. 2005. Data Mining -Practical Machine Learning Tools and Techniques. Morgan- Kaufmann, San Francisco, CA.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Automatic Acquisition of Domain Knowledge for Information Extraction",
                "authors": [
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Yangarber",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Grishman",
                        "suffix": ""
                    },
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Tapanainen",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Huttunen",
                        "suffix": ""
                    }
                ],
                "year": 2000,
                "venue": "Proceedings of the 18th International Conference on Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "940--946",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "R. Yangarber, R. Grishman, P. Tapanainen, and S. Hut- tunen. 2000. Automatic Acquisition of Domain Knowledge for Information Extraction. In Proceed- ings of the 18th International Conference on Com- putational Linguistics, pages 940-946, Saarbr\u00fccken, Germany, August.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Obtaining Calibrated Probability Estimates from Decision Trees and Na\u00efve Bayesian Classiers",
                "authors": [
                    {
                        "first": "B",
                        "middle": [],
                        "last": "Zadrozny",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Elkan",
                        "suffix": ""
                    }
                ],
                "year": 2001,
                "venue": "Proceedings of the Eighteenth International Conference on Machine Learning",
                "volume": "",
                "issue": "",
                "pages": "609--616",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "B. Zadrozny and C. Elkan. 2001. Obtaining Cal- ibrated Probability Estimates from Decision Trees and Na\u00efve Bayesian Classiers. In Proceedings of the Eighteenth International Conference on Machine Learning, pages 609-616, Williamstown, MA, June.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 1: A Disease Outbreak Event Template",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Figure 2: Examples of GLACIER Extractions",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            }
        }
    }
}