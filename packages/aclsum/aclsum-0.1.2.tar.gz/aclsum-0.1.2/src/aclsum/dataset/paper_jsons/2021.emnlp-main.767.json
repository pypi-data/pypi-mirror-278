{
    "paper_id": "2021",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:36:35.835885Z"
    },
    "title": "A Role-Selected Sharing Network for Joint Machine-Human Chatting Handoff and Service Satisfaction Analysis",
    "authors": [
        {
            "first": "Jiawei",
            "middle": [],
            "last": "Liu",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Wuhan University",
                "location": {
                    "settlement": "Wuhan",
                    "country": "China"
                }
            },
            "email": ""
        },
        {
            "first": "Kaisong",
            "middle": [],
            "last": "Song",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Northeastern University",
                "location": {
                    "settlement": "Shenyang",
                    "country": "China"
                }
            },
            "email": ""
        },
        {
            "first": "Yangyang",
            "middle": [],
            "last": "Kang",
            "suffix": "",
            "affiliation": {},
            "email": "yangyang.kangyy@alibaba-inc.com"
        },
        {
            "first": "Guoxiu",
            "middle": [],
            "last": "He",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "East China Normal University",
                "location": {
                    "settlement": "Shanghai",
                    "country": "China"
                }
            },
            "email": "gxhe@fem.ecnu.edu.cn"
        },
        {
            "first": "Zhuoren",
            "middle": [],
            "last": "Jiang",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Zhejiang University",
                "location": {
                    "settlement": "Hangzhou",
                    "country": "China"
                }
            },
            "email": "jiangzhuoren@zju.edu.cn"
        },
        {
            "first": "Changlong",
            "middle": [],
            "last": "Sun",
            "suffix": "",
            "affiliation": {},
            "email": "changlong.scl@taobao.com"
        },
        {
            "first": "Wei",
            "middle": [],
            "last": "Lu",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Wuhan University",
                "location": {
                    "settlement": "Wuhan",
                    "country": "China"
                }
            },
            "email": "weilu@whu.edu.cn"
        },
        {
            "first": "Xiaozhong",
            "middle": [],
            "last": "Liu",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Worcester Polytechnic Institute",
                "location": {
                    "settlement": "Worcester",
                    "country": "USA"
                }
            },
            "email": ""
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Chatbot is increasingly thriving in different domains, however, because of unexpected discourse complexity and training data sparseness, its potential distrust hatches vital apprehension. Recently, Machine-Human Chatting Handoff (MHCH), predicting chatbot failure and enabling human-algorithm collaboration to enhance chatbot quality, has attracted increasing attention from industry and academia. In this study, we propose a novel model, Role-Selected Sharing Network (RSSN), which integrates both dialogue satisfaction estimation and handoff prediction in one multi-task learning framework. Unlike prior efforts in dialog mining, by utilizing local user satisfaction as a bridge, global satisfaction detector and handoff predictor can effectively exchange critical information. Specifically, we decouple the relation and interaction between the two tasks by the role information after the shared encoder. Extensive experiments on two public datasets demonstrate the effectiveness of our model.",
    "pdf_parse": {
        "paper_id": "2021",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Chatbot is increasingly thriving in different domains, however, because of unexpected discourse complexity and training data sparseness, its potential distrust hatches vital apprehension. Recently, Machine-Human Chatting Handoff (MHCH), predicting chatbot failure and enabling human-algorithm collaboration to enhance chatbot quality, has attracted increasing attention from industry and academia. In this study, we propose a novel model, Role-Selected Sharing Network (RSSN), which integrates both dialogue satisfaction estimation and handoff prediction in one multi-task learning framework. Unlike prior efforts in dialog mining, by utilizing local user satisfaction as a bridge, global satisfaction detector and handoff predictor can effectively exchange critical information. Specifically, we decouple the relation and interaction between the two tasks by the role information after the shared encoder. Extensive experiments on two public datasets demonstrate the effectiveness of our model.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Chatbot, as one of the recent palpable AI excitements, has been widely adopted to reduce the cost of customer service (Qiu et al., 2017; Ram et al., 2018; Zhou et al., 2020) . However, due to the complexity of human conversation, auto-chatbot can hardly meet all users' needs, while its potential failure perceives skepticism. AI-enabled customer service, for instance, may trigger unexpected business losses because of chatbot failures (Radziwill and Benton, 2017; Rajendran et al., 2019) . Moreover, for chatbot adoption in sensitive areas, such as healthcare (Chung and Park, 2019) and criminal justice (Wang et al., 2020a) , any subtle statistical miscalculation may trigger serious health and legal * Corresponding authors.",
                "cite_spans": [
                    {
                        "start": 118,
                        "end": 136,
                        "text": "(Qiu et al., 2017;",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 137,
                        "end": 154,
                        "text": "Ram et al., 2018;",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 155,
                        "end": 173,
                        "text": "Zhou et al., 2020)",
                        "ref_id": "BIBREF41"
                    },
                    {
                        "start": 437,
                        "end": 465,
                        "text": "(Radziwill and Benton, 2017;",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 466,
                        "end": 489,
                        "text": "Rajendran et al., 2019)",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 562,
                        "end": 584,
                        "text": "(Chung and Park, 2019)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 606,
                        "end": 626,
                        "text": "(Wang et al., 2020a)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "What a business! It has been a week! utter 3",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "We will ship goods about a week after placing the order. Please be patient.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "utter 2 Sorry, my dear customer. We have pushed the warehouse to ship as soon as possible, and we will compensate the freight for you. utter 7",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "We will try our best to improve your shopping experience. Thank you for your understanding and patience. consequences. To address this problem, recently, scholars proposed new dialog mining tasks to autoassess dialogue satisfaction, a.k.a. Service Satisfaction Analysis (SSA) at dialogue-level (Song et al., 2019) , and to predict potential chatbot failure via machine-human chatting handoff (MHCH) at utterance-level (Huang et al., 2018; Liu et al., 2021) . In a MHCH context, algorithm can transfer an ongoing auto-dialogue to the human agent when the current utterance is confusing.",
                "cite_spans": [
                    {
                        "start": 294,
                        "end": 313,
                        "text": "(Song et al., 2019)",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 418,
                        "end": 438,
                        "text": "(Huang et al., 2018;",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 439,
                        "end": 456,
                        "text": "Liu et al., 2021)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Figure 1 depicts an exemplar dialogue of online customer service. In this dialogue, the chatbot gives an unsatisfied answer about shipping, thus causing the customer's complaint (local dissatisfaction utter 2 and utter 3 ). Ideally, chatbot should be able to detect the negative (local) emotion (utter 3 ) and tries to appease complaints, but this problem remains unresolved. If chatbot continues, the customer may cancel the deal and give a negative rating (dialogue global dissatisfaction). With MHCH (detects the risks of utter 2 and utter 3 ), the dialogue can be transferred to the human agent, who is better at handling, compensating, and comforting the customer and enhance customer satisfaction. This example illustrates the cross-impact between handoff and dialogue (local+global) satisfaction. Intuitively, MHCH and SSA tasks can be compatible and complementary given a dialogue discourse, i.e., the local satisfaction is related to the quality of the conversation (Bodigutla et al., 2019a (Bodigutla et al., , 2020)) , which can support the handoff judgment and ultimately affect the overall satisfaction. On the one hand, handoff labels of utterances are highly pertinent to local satisfaction, e.g., one can utilize single handoff information to enhance local satisfaction prediction, which ultimately contributes to the overall satisfaction estimation. On the other hand, the overall satisfaction is obtained by combining local satisfactions, which reflects the quality in terms of answer generation, language understanding, and emotion perception, and subsequently helps to facilitate handoff judgment.",
                "cite_spans": [
                    {
                        "start": 975,
                        "end": 999,
                        "text": "(Bodigutla et al., 2019a",
                        "ref_id": null
                    },
                    {
                        "start": 1000,
                        "end": 1027,
                        "text": "(Bodigutla et al., , 2020))",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In recent years, researchers (Bodigutla et al., 2019a,b; Ultes, 2019; Bodigutla et al., 2020) explore joint evaluation of turn and dialogue level qualities in spoken dialogue systems. In terms of general dialogue system, to improve the efficiency of dialogue management, Qin et al. (2020) propose a co-interactive relation layer to explicitly examine the cross-impact and model the interaction between sentiment classification and dialog act recognition, which are relevant tasks at the same level (utterancelevel). However, MHCH (utterance-level) and SSA (dialogue-level) target satisfaction at different levels. More importantly, handoff labels of utterances are more comprehensive and pertinent to local satisfaction than sentiment polarities. Meanwhile, customer utterances have significant impacts on the overall satisfaction (Song et al., 2019) , which motivates us that the role information can be critical for knowledge transfer of these two tasks.",
                "cite_spans": [
                    {
                        "start": 29,
                        "end": 56,
                        "text": "(Bodigutla et al., 2019a,b;",
                        "ref_id": null
                    },
                    {
                        "start": 57,
                        "end": 69,
                        "text": "Ultes, 2019;",
                        "ref_id": "BIBREF34"
                    },
                    {
                        "start": 70,
                        "end": 93,
                        "text": "Bodigutla et al., 2020)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 271,
                        "end": 288,
                        "text": "Qin et al. (2020)",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 831,
                        "end": 850,
                        "text": "(Song et al., 2019)",
                        "ref_id": "BIBREF32"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "To address the aforementioned issues, we propose an innovative Role-Selected Sharing Network (RSSN) for handoff prediction and dialogue satisfaction estimation, which utilizes role information to selectively characterize complex relations and interactions between two tasks. To the best of our knowledge, it is the pioneer investigation to leverage the multi-task learning approach for integrating MHCH and SSA. In practice, we first adopt a shared encoder to obtain the shared representations of utterances. Inspired by the co-attention mechanism (Xiong et al., 2016; Qin et al., 2020) , the shared representations are then fed into the roleselected sharing module, which consists of two directional interactions: MHCH to SSA and SSA to MHCH. This module is used to get the fusion of MHCH and SSA representations. We propose the role-selected sharing module based on the hypothesis that the role information can benefit the tasks' performances. The satisfaction distributions of utterances from different roles (agent and customer) are different, and the effects for the tasks are also different. Specifically, the satisfaction of agent is non-negative. The utterances from agent can enrich the context of customer's utterances and indirectly affect satisfaction polarity. Thus, directly employing local satisfaction of agent into the interaction with handoff may introduce noise. In the proposed role-selected sharing module, we adopt local satisfaction based on the role information: only the local satisfaction from customer can be adopted to interact with handoff information. By this means, we can control knowledge transfer for both tasks and make our framework more explainable. The final integrated outputs are then fed to separate decoders for handoff and satisfaction predictions.",
                "cite_spans": [
                    {
                        "start": 548,
                        "end": 568,
                        "text": "(Xiong et al., 2016;",
                        "ref_id": "BIBREF39"
                    },
                    {
                        "start": 569,
                        "end": 586,
                        "text": "Qin et al., 2020)",
                        "ref_id": "BIBREF23"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "To summarize, our contributions are mainly as follows: (1) We introduce a novel multi-task learning framework for combining machine-human chatting handoff and service satisfaction analysis.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "(2) We propose a Role-Selected Sharing Network for handoff prediction and satisfaction rating estimation, which can utilize different role information to control knowledge transfer for both tasks and enhance model performance and explainability. (3) The experimental results demonstrate that our model outperforms a series of baselines that consists of the state-of-the-art (SOTA) models on each task and multi-task learning models for both tasks. To assist other scholars in reproducing the experiment outcomes, we release the codes and the annotated dataset1 .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Due to the complexity of human conversation, current automatic chatbots are not mature enough and still fail to meet users' expectations (Brandtzaeg and F\u00f8lstad, 2018; Jain et al., 2018; Chaves and Gerosa, 2020) . Besides exploring novel dialogue models, dialogue quality estimation, service satis-faction analysis, and human intervention are vital strategies to enhance chatbot performance.",
                "cite_spans": [
                    {
                        "start": 137,
                        "end": 167,
                        "text": "(Brandtzaeg and F\u00f8lstad, 2018;",
                        "ref_id": null
                    },
                    {
                        "start": 168,
                        "end": 186,
                        "text": "Jain et al., 2018;",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 187,
                        "end": 211,
                        "text": "Chaves and Gerosa, 2020)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "Dialogue Quality and Service Satisfaction Analysis. Interaction Quality (IQ) (Schmitt et al., 2012) and Response Quality (RQ) (Bodigutla et al., 2019b) are dialogue quality evaluation metrics for spoken dialogue systems. Automated models to estimate IQ (Ultes et al., 2014; El Asri et al., 2014) and RQ (Bodigutla et al., 2019a (Bodigutla et al., ,b, 2020) ) utilize various features derived from the dialogue content and output from spoken language understanding components. For chat-oriented dialogue system, Higashinaka et al. (2015a,b) introduce Dialogue Breakdown Detection task to detect a system's inappropriate utterances that lead to dialogue breakdowns. To efficiently analyze dialogue satisfaction, Song et al. (2019) introduce the task of service satisfaction analysis (SSA) based on multi-turn customer service dialogues. The proposed CAMIL model can predict the sentiment of all the customer utterances and aggregate those sentiments into overall service satisfaction polarity. Nevertheless, the sentiment of customer utterance is only one of the factors that influence service satisfaction.",
                "cite_spans": [
                    {
                        "start": 77,
                        "end": 99,
                        "text": "(Schmitt et al., 2012)",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 126,
                        "end": 151,
                        "text": "(Bodigutla et al., 2019b)",
                        "ref_id": null
                    },
                    {
                        "start": 253,
                        "end": 273,
                        "text": "(Ultes et al., 2014;",
                        "ref_id": "BIBREF35"
                    },
                    {
                        "start": 274,
                        "end": 295,
                        "text": "El Asri et al., 2014)",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 303,
                        "end": 327,
                        "text": "(Bodigutla et al., 2019a",
                        "ref_id": null
                    },
                    {
                        "start": 328,
                        "end": 358,
                        "text": "(Bodigutla et al., ,b, 2020) )",
                        "ref_id": null
                    },
                    {
                        "start": 511,
                        "end": 539,
                        "text": "Higashinaka et al. (2015a,b)",
                        "ref_id": null
                    },
                    {
                        "start": 710,
                        "end": 728,
                        "text": "Song et al. (2019)",
                        "ref_id": "BIBREF32"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "Machine Human Chatting Handoff. Another perspective of further enhancing the chatbot's performance is to combine chatbots with human agent. Recently, there are several works about humanmachine cooperation for chatbots. Huang et al. (2018) propose the crowd-powered conversational assist architecture, namely Evorus, which integrates crowds with multiple chatbots and a voting system. Rajendran et al. (2019) utilize reinforce learning framework to transfer conversations to human agents once encountered new user behaviors. Different from them, Liu et al. (2021) mainly focus on detecting transferable utterances which are one of the keys to improve user satisfaction. They propose a DAMI network that utilizes difficultyassisted encoding and matching inference mechanisms to predict the transferable utterance.",
                "cite_spans": [
                    {
                        "start": 219,
                        "end": 238,
                        "text": "Huang et al. (2018)",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 384,
                        "end": 407,
                        "text": "Rajendran et al. (2019)",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 545,
                        "end": 562,
                        "text": "Liu et al. (2021)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "Multi-task learning in dialogue system. For satisfaction estimation, Bodigutla et al. (2020) propose to jointly predict turn-level RQ labels and dialogue-level ratings. They utilize features from spoken dialogue system and BiLSTM (Hochreiter and Schmidhuber, 1997) based model to automatically weight each turn's contribution towards the rating. Ma et al. (2018) propose a joint framework that unifies two highly pertinent tasks. Both tasks are trained jointly using weight sharing to extract the common and task-invariant features while each task can still learn its task-specific features. To learn the correlation between two tasks, Qin et al. (2020) propose a DCR-Net. It adopts a stacked co-interactive relation layer to incorporate mutual knowledge explicitly. This model ignores the contextual information and isolated two types of information when performing interaction.",
                "cite_spans": [
                    {
                        "start": 69,
                        "end": 92,
                        "text": "Bodigutla et al. (2020)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 230,
                        "end": 264,
                        "text": "(Hochreiter and Schmidhuber, 1997)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 346,
                        "end": 362,
                        "text": "Ma et al. (2018)",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 636,
                        "end": 653,
                        "text": "Qin et al. (2020)",
                        "ref_id": "BIBREF23"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "Figure 2 shows the overall architecture of RSSN, which consists of three parts: Shared Utterance and Matching Encoder, Role-Selected Interaction Layer, and Decoder for MHCH and SSA. In this section, we will describe them in detail.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "2",
                        "ref_id": "FIGREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Methodology",
                "sec_num": "3"
            },
            {
                "text": "Given a dialogue D = [u 1 , ..., u L ], it consists of a sequence of L utterances with corresponding handoff labels [y h 1 , ..., y h L ], where t = {1 \u2264 t \u2264 L|t \u2208 N}, y h t \u2208 \u03a8 and \u03a8 = {normal, transfer-able}. Transferable indicates the dialogue should be transferred to the human agent, whereas normal indicates there is no need to transfer. The satisfaction polarity of dialogue D is noted as y s , where y s \u2208 \u2126 and \u2126 = {well satisfied, met, unsatisfied}. Note that we perform the multi-task learning with the supervision of handoff labels and dialogue's satisfaction only. The local satisfaction distributions of utterances are only the latent estimation, which helps to predict the dialogue's satisfaction.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Methodology",
                "sec_num": "3"
            },
            {
                "text": "The shared encoder consists of a bidirectional LSTM (BiLSTM) to learn the utterance representation and a masked matching layer to capture the contextual matching information.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Shared Utterance and Matching Encoder",
                "sec_num": "3.1"
            },
            {
                "text": "Suppose u t = [w 1 , ..., w |ut| ] represents a sequence of words in the t-th utterance. These words are mapped into corresponding word embeddings E ut \u2208 R n\u00d7|ut| , where n is the word embedding dimension. By adopting semantic composition models with word embeddings, we can learn the utterance representation. In this work, we adopt a BiLSTM model and concatenate hidden states of forward and backward LSTM to learn the contextsensitive utterance representation v t \u2208 R 2k , where k is the number of hidden units of LSTM cell. Formally, we have v t = BiLSTM(E ut ).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Shared Utterance and Matching Encoder",
                "sec_num": "3.1"
            },
            {
                "text": "In a dialogue, preceding utterances for each utterance provide helpful context information to estimate local satisfaction. Thus, within a dialogue, there is a high probability of inter-dependency with respect to their context clues. To encapsulate the contextual matching and information flow in the dialogue, we feed the utterance representation into a unidirectional matching mechanism:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Shared Utterance and Matching Encoder",
                "sec_num": "3.1"
            },
            {
                "text": "MHCH Label \ud835\udc89 ! \ud835\udc89 \"#! \ud835\udc89 \" \ud835\udc89 $ \" \ud835\udc9a ! % \u2026 \" \ud835\udc9a $ % \" \ud835\udc9a \"#! % \" \ud835\udc9a \" % \u2112 ! \u2112 $ Transformer Encoder \ud835\udc60\ud835\udc5c\ud835\udc53\ud835\udc61\ud835\udc5a\ud835\udc4e\ud835\udc65 \ud835\udf36 SSA Label \ud835\udc37\ud835\udc52\ud835\udc5b\ud835\udc60\ud835\udc52 \ud835\udc40\ud835\udc4e\ud835\udc60\ud835\udc58\ud835\udc52\ud835\udc51 \ud835\udc40\ud835\udc4e\ud835\udc61\ud835\udc5f\ud835\udc56\ud835\udc65 \u2297 \u2297 \u2295 \ud835\udc60\ud835\udc5c\ud835\udc53\ud835\udc61\ud835\udc5a\ud835\udc4e\ud835\udc65 \ud835\udc34\ud835\udc59\ud835\udc59 SSA to MHCH \u2026 \ud835\udc97 ! \ud835\udc97 ! \" \ud835\udc97 #$! \" \ud835\udc97 # \" \ud835\udc97 #$! \ud835\udc97 # \ud835\udc97 % \ud835\udc97 % \" \u2026 \u2a01 \u2a01 \u2a01 \u2a01 \ud835\udc36\ud835\udc62\ud835\udc60\ud835\udc61\ud835\udc5c\ud835\udc5a\ud835\udc52\ud835\udc5f \u2297 \u2297 \ud835\udc60\ud835\udc5c\ud835\udc53\ud835\udc61\ud835\udc5a\ud835\udc4e\ud835\udc65 MHCH to",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Shared Utterance and Matching Encoder",
                "sec_num": "3.1"
            },
            {
                "text": "\u2112 * \u03b7 \ud835\udc37\ud835\udc52\ud835\udc5b\ud835\udc60\ud835\udc52 \ud835\udc3f\ud835\udc4e\ud835\udc66\ud835\udc52\ud835\udc5f \ud835\udc41\ud835\udc5c\ud835\udc5f\ud835\udc5a \ud835\udc35\ud835\udc56\ud835\udc3f\ud835\udc46\ud835\udc47\ud835\udc40 \ud835\udc35\ud835\udc56\ud835\udc3f\ud835\udc46\ud835\udc47\ud835\udc40 \ud835\udc35\ud835\udc56\ud835\udc3f\ud835\udc46\ud835\udc47\ud835\udc40 \ud835\udc35\ud835\udc56\ud835\udc3f\ud835\udc46\ud835\udc47\ud835\udc40 \ud835\udc6c 3 ! \ud835\udc6c 3 \" \ud835\udc6c 3 #$! \ud835\udc6c 3 # 7 \ud835\udc97 ! 7 \ud835\udc97 % 7 \ud835\udc97 #$! 7 \ud835\udc97 # \ud835\udc6f \ud835\udc7a \ud835\udc6f \" \ud835\udc7a \" \ud835\udc74 \ud835\udf36 4 \ud835\udf36 5 \ud835\udc78 \ud835\udc54\ud835\udc59\ud835\udc5c\ud835\udc4f\ud835\udc4e\ud835\udc59 \ud835\udc60\ud835\udc4e\ud835\udc61\ud835\udc56\ud835\udc60\ud835\udc53\ud835\udc4e\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b \" \ud835\udc9a 7 \ud835\udeaa \ud835\udc59\ud835\udc5c\ud835\udc50\ud835\udc4e\ud835\udc59 \ud835\udc60\ud835\udc4e\ud835\udc61\ud835\udc56\ud835\udc60\ud835\udc53\ud835\udc4e\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b \ud835\udd03 !",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Shared Utterance and Matching Encoder",
                "sec_num": "3.1"
            },
            {
                "text": "v t = v t [v 1 , v 2 , ..., v t-1 ]",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Shared Utterance and Matching Encoder",
                "sec_num": "3.1"
            },
            {
                "text": "(1)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Shared Utterance and Matching Encoder",
                "sec_num": "3.1"
            },
            {
                "text": "After masking out the future information of the present utterance, the matching features of dialogue D is a lower triangular matrix with the diagonal values removed. Then we concatenate the matching features with utterance representation to get vt = [v t ; v t ]. Finally, we obtain the initial shared utterances representations of MHCH H = [v 1 , ..., vL ] and SSA S = [v 1 , ..., vL ].",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Shared Utterance and Matching Encoder",
                "sec_num": "3.1"
            },
            {
                "text": "In customer service dialogue, the roles of different participants would exhibit different characteristics (Song et al., 2019) . Besides, we conjecture that MHCH and SSA have different impacts on each other. These two tasks indirectly establish a connection through various factors such as dialogue quality, satisfaction, and sentiment. At the same time, role information also plays an important role in both tasks. On the one hand, the utterances from agent can enrich the context of customer utterances and indirectly affect satisfaction polarity. In contrast, customer utterances tend to have a more direct impact on the dominating satisfaction polarity. On the other hand, the utterances of any participants can trigger machine-human chatting handoff. Thus, we propose the Role-Selected Interaction Layer, which contains two interaction directions: SSA to MHCH and MHCH to SSA, to model the relations and interactions between the two tasks separately.",
                "cite_spans": [
                    {
                        "start": 106,
                        "end": 125,
                        "text": "(Song et al., 2019)",
                        "ref_id": "BIBREF32"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Role-Selected Interaction Layer",
                "sec_num": "3.2"
            },
            {
                "text": "We first apply two Dense layers over the handoff information and satisfaction information respectively to make them more task-specific, which can be noted as H = Dense(H) and S = Dense(S), where H \u2208 R L\u00d7d and S \u2208 R L\u00d7d . Note that d is the number of hidden units of the Dense layer.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Role-Selected Interaction Layer",
                "sec_num": "3.2"
            },
            {
                "text": "SSA to MHCH. Co-attention is an effective and widely used method to capture the mutual knowledge among the correlated tasks (Xiong et al., 2016; Qin et al., 2020) . Inspired by the basic co-attention mechanism, we design the interaction mechanism separately according to the characteristics of tasks. In this way, task-relevant knowledge can be transferred mutually between two tasks. Specifically, the SSA to MHCH module produces comprehensive handoff representations incorporating the local satisfaction information. Since the agent utterances indirectly affect satisfaction polarity, directly employing local satisfaction of agent into the interaction with handoff may introduce noise. As a consequence, we only adopt the local satisfaction information of customer to interact with handoff information. The process can be defined as follows:",
                "cite_spans": [
                    {
                        "start": 124,
                        "end": 144,
                        "text": "(Xiong et al., 2016;",
                        "ref_id": "BIBREF39"
                    },
                    {
                        "start": 145,
                        "end": 162,
                        "text": "Qin et al., 2020)",
                        "ref_id": "BIBREF23"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Role-Selected Interaction Layer",
                "sec_num": "3.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "\u03b1 s = softmax(Mask c (H (S ) )) (2) M = Dense([\u03b1 s S ; H ])",
                        "eq_num": "(3)"
                    }
                ],
                "section": "Role-Selected Interaction Layer",
                "sec_num": "3.2"
            },
            {
                "text": "where M \u2208 R L\u00d7d and Mask c denotes that we mask out (setting to -\u221e) all values of the future information and agent utterances.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Role-Selected Interaction Layer",
                "sec_num": "3.2"
            },
            {
                "text": "MHCH to SSA. As shown in Figure 3 , we observe that the dialogue satisfaction rating is related to the handoff position. Intuitively, a handoff can be triggered by the local unsatisfied attitude of the customer, and the later handoff means users are unsatisfied before the end of the conversation. Prior study, Song et al. (2019) , also found that user satisfaction at the dialogue level is usually determined by the attitudes of the last few utterances. We can derive that handoff at the later period of the conversation may result in a lower satisfaction rating. Thus, we adjust the interactive attention by positional weights, which can be computed as below:",
                "cite_spans": [
                    {
                        "start": 311,
                        "end": 329,
                        "text": "Song et al. (2019)",
                        "ref_id": "BIBREF32"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 32,
                        "end": 33,
                        "text": "3",
                        "ref_id": "FIGREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Role-Selected Interaction Layer",
                "sec_num": "3.2"
            },
            {
                "text": "\u03b2 t = softmax([ 1 L , ..., t L , ..., 1] I p (u t )) (4)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Role-Selected Interaction Layer",
                "sec_num": "3.2"
            },
            {
                "text": "where is element-wise product and I p (\u2022) denotes a zero masking identity matrix to mask out future information. Finally, the positional weights \u0393 = [\u03b2 1 ; ...; \u03b2 L ], where \u0393 \u2208 R L\u00d7L . The mechanism gives more weight to the later handoff information.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Role-Selected Interaction Layer",
                "sec_num": "3.2"
            },
            {
                "text": "We apply the positional weights to the interaction:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Role-Selected Interaction Layer",
                "sec_num": "3.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "\u03b1 m = softmax(Mask(S \u2022 (H ) \u2022 \u0393)) (5) Q = LayerNorm(\u03b1 m \u2022 H + S )",
                        "eq_num": "(6)"
                    }
                ],
                "section": "Role-Selected Interaction Layer",
                "sec_num": "3.2"
            },
            {
                "text": "where Mask denotes that we mask out the future information (setting to -\u221e), and LayerNorm denotes the layer normalization (Ba et al., 2016) .",
                "cite_spans": [
                    {
                        "start": 122,
                        "end": 139,
                        "text": "(Ba et al., 2016)",
                        "ref_id": "BIBREF1"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Role-Selected Interaction Layer",
                "sec_num": "3.2"
            },
            {
                "text": "After the role-selected interaction layer, we can get the outputs M = [m 1 , ..., m L ] and Q = [q 1 , ..., q L ]. Then we adopt separate decoders to predict handoff and satisfaction rating.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Decoder for MHCH and SSA",
                "sec_num": "3.3"
            },
            {
                "text": "In terms of machine-human chatting handoff, the tendency of handoff also depends on the dialogue context. Thus, we feed the outputs of the interaction layer into an LSTM to connect the sequential information flow in the dialogue:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Decoder for MHCH and SSA",
                "sec_num": "3.3"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "h t = LSTM(m t , h t-1 )",
                        "eq_num": "(7)"
                    }
                ],
                "section": "Decoder for MHCH and SSA",
                "sec_num": "3.3"
            },
            {
                "text": "where h t \u2208 R k is the hidden state for u t . Since there are no dependencies among labels, we simply use a softmax classifier for handoff prediction:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Decoder for MHCH and SSA",
                "sec_num": "3.3"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "\u0177h t = softmax(W h t + b )",
                        "eq_num": "(8)"
                    }
                ],
                "section": "Decoder for MHCH and SSA",
                "sec_num": "3.3"
            },
            {
                "text": "where",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Decoder for MHCH and SSA",
                "sec_num": "3.3"
            },
            {
                "text": "W \u2208 R |\u03a8|\u00d7k and b \u2208 R |\u03a8| . \u0177h t \u2208 R |\u03a8| is the predicted handoff probability distribution of u t .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Decoder for MHCH and SSA",
                "sec_num": "3.3"
            },
            {
                "text": "For service satisfaction analysis, we first apply a transformer block (Vaswani et al., 2017) to model the long-range context of the dialogue further. Formally, we have",
                "cite_spans": [
                    {
                        "start": 70,
                        "end": 92,
                        "text": "(Vaswani et al., 2017)",
                        "ref_id": "BIBREF36"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Decoder for MHCH and SSA",
                "sec_num": "3.3"
            },
            {
                "text": "Q = Transformer(Q), where Q = {q 1 , ..., qL |q t \u2208 R k }.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Decoder for MHCH and SSA",
                "sec_num": "3.3"
            },
            {
                "text": "Then we utilize a softmax function for estimating local satisfaction distribution z t \u2208 R |\u2126| of u t :",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Decoder for MHCH and SSA",
                "sec_num": "3.3"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "z t = softmax(W \u03be qt + b \u03be )",
                        "eq_num": "(9)"
                    }
                ],
                "section": "Decoder for MHCH and SSA",
                "sec_num": "3.3"
            },
            {
                "text": "where",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Decoder for MHCH and SSA",
                "sec_num": "3.3"
            },
            {
                "text": "W \u03be \u2208 R |\u2126|\u00d7k and b \u03be \u2208 R |\u2126| .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Decoder for MHCH and SSA",
                "sec_num": "3.3"
            },
            {
                "text": "Since only a fraction of customer utterances can contribute to the final satisfaction rating, we introduce an attention strategy that enables our model to attend to customer utterances of different importance when merging the local satisfaction distribution. Formally, we measure the importance of each customer utterances as below:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Decoder for MHCH and SSA",
                "sec_num": "3.3"
            },
            {
                "text": "\u03b1 = softmax(Mask c (g tanh(W \u00b5 Q + b \u00b5 ))) (10) where \u03b1 \u2208 R L . W \u00b5 \u2208 R z\u00d7k , b \u00b5 \u2208 R z",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Decoder for MHCH and SSA",
                "sec_num": "3.3"
            },
            {
                "text": ", and g \u2208 R z are trainable parameters. z is the number of attention units. Mask c denotes the masking function used to reserve customer utterances. g can be perceived as a high-level representation of a fixed query \"Which is the critical utterance?\". Finally, we obtain the overall satisfaction distribution \u0177s \u2208 R |\u2126| as the weighted sum of local customer satisfaction distribution:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Decoder for MHCH and SSA",
                "sec_num": "3.3"
            },
            {
                "text": "\u0177s = L t=1 \u03b1 t z t (11)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Decoder for MHCH and SSA",
                "sec_num": "3.3"
            },
            {
                "text": "where \u03b1 t is the t-th weight of utterance u t in \u03b1. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Decoder for MHCH and SSA",
                "sec_num": "3.3"
            },
            {
                "text": "The objective function of MHCH is formulated as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Joint Training",
                "sec_num": "3.4"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "L 1 = - 1 L L t=1 |\u03a8| i=1 y h i,t log(\u0177 h i,t )",
                        "eq_num": "(12)"
                    }
                ],
                "section": "Joint Training",
                "sec_num": "3.4"
            },
            {
                "text": "The objective function of SSA is formulated as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Joint Training",
                "sec_num": "3.4"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "L 2 = - |\u2126| i=1 y s i log(\u0177 s i )",
                        "eq_num": "(13)"
                    }
                ],
                "section": "Joint Training",
                "sec_num": "3.4"
            },
            {
                "text": "Finally, we minimize the joint cross-entropy loss L, which is obtained as follow:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Joint Training",
                "sec_num": "3.4"
            },
            {
                "text": "L(\u0398) = L 1 + \u03b7 * L 2 + \u03b4 \u0398 2 2 (14)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Joint Training",
                "sec_num": "3.4"
            },
            {
                "text": "where \u03b7 \u2208 R + denotes the trade-off parameter, \u03b4 denotes the L 2 regularization weight, and \u0398 denotes all the trainable parameters of model. We use backpropagation to compute the gradients of the parameters, and update them with Adam (Kingma and Ba, 2015) optimizer.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Joint Training",
                "sec_num": "3.4"
            },
            {
                "text": "Our experiments are conducted based on two publicly available Chinese customer service dialogue datasets, namely Clothes and Makeup 2 , collected by Song et al. (2019) from Taobao 3 . Both datasets have service satisfaction ratings from customer feedbacks and annotated sentiment labels of utterances. Note that the sentiment labels do not participate in our training process and are only used for test. Meanwhile, we also annotate the transferable/normal labels for both datasets according to the existing specifications (Liu et al., 2021) . Two 2 https://github.com/songkaisong/ssa 3 https://www.taobao.com annotators with professional linguistics knowledge participated in the annotation task.",
                "cite_spans": [
                    {
                        "start": 149,
                        "end": 167,
                        "text": "Song et al. (2019)",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 522,
                        "end": 540,
                        "text": "(Liu et al., 2021)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dataset and Experimental Settings",
                "sec_num": "4.1"
            },
            {
                "text": "A summary of statistics, including Kappa value (Snow et al., 2008) for both datasets are given in Table 1 . Clothes is a corpus with 10K dialogues in the Clothes domain, which has an imbalanced satisfaction distribution at dialogue level. Makeup is a corpus with 3,540 dialogues in the Makeup domain, which has a balanced satisfaction distribution dialogue level. Note that we do not adopt the original word segmentation. Figure 3 shows the relative handoff position distributions in different satisfaction ratings, where we take explicit request, negative emotion, and unsatisfactory answer handoffs into consideration. It indicates that handoff at the later phase of the conversation is more likely to get a lower service satisfaction rating.",
                "cite_spans": [
                    {
                        "start": 47,
                        "end": 66,
                        "text": "(Snow et al., 2008)",
                        "ref_id": "BIBREF31"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 104,
                        "end": 105,
                        "text": "1",
                        "ref_id": "TABREF1"
                    },
                    {
                        "start": 429,
                        "end": 430,
                        "text": "3",
                        "ref_id": "FIGREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Dataset and Experimental Settings",
                "sec_num": "4.1"
            },
            {
                "text": "Except BERT-based model, all texts are tokenized by a popular Chinese word segmentation utility called jieba4 . The datasets are partitioned for training, validation, and test with an 80/10/10 split. For the BERT-based methods, we fine-tune the pre-trained model. For the other methods, we apply the pre-trained word vectors initially trained on Clothes and Makeup corpora by using CBOW (Mikolov et al., 2013) . The dimension of word embedding is set as 200. Other trainable model parameters are initialized by sampling values from the Glorot uniform initializer (Glorot and Bengio, 2010) . The sizes of hidden state k, Dense units d, attention units z, and batch size are selected from {32, 64, 128, 256, 512}. The dropout (Srivastava et al., 2014) rate and the loss weight \u03b7 are selected from (0, 1) by grid search. Finally, we train the models with an initial learning rate of 1.5 \u00d7 10 -3 and 2\u00d710 -5 for regular baselines and BERT-based models. All the methods run on a server configured with a Tesla V100, 32 CPU, and 32G memory.",
                "cite_spans": [
                    {
                        "start": 387,
                        "end": 409,
                        "text": "(Mikolov et al., 2013)",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 563,
                        "end": 588,
                        "text": "(Glorot and Bengio, 2010)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 724,
                        "end": 749,
                        "text": "(Srivastava et al., 2014)",
                        "ref_id": "BIBREF33"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dataset and Experimental Settings",
                "sec_num": "4.1"
            },
            {
                "text": "We compare our model with 14 strong dialogue classification baseline models, which come from MHCH, SSA, and other similar tasks.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baselines",
                "sec_num": "4.2"
            },
            {
                "text": "Generic Baselines: HAN (Yang et al., 2016) and BERT (Devlin et al., 2019) +LSTM. We adopt outputs and the last hidden of RNN to predict handoff labels and the satisfaction rating, respectively.",
                "cite_spans": [
                    {
                        "start": 23,
                        "end": 42,
                        "text": "(Yang et al., 2016)",
                        "ref_id": "BIBREF40"
                    },
                    {
                        "start": 52,
                        "end": 73,
                        "text": "(Devlin et al., 2019)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baselines",
                "sec_num": "4.2"
            },
            {
                "text": "Baselines for the MHCH task: HEC (Kumar et al., 2018), DialogueRNN (Majumder et al., 2019) , CASA (Raheja and Tetreault, 2019) 3 corresponding to GT-I, GT-II, and GT-III. The results of comparisons are shown in Table 2 .",
                "cite_spans": [
                    {
                        "start": 67,
                        "end": 90,
                        "text": "(Majumder et al., 2019)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 98,
                        "end": 126,
                        "text": "(Raheja and Tetreault, 2019)",
                        "ref_id": "BIBREF26"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 217,
                        "end": 218,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Baselines",
                "sec_num": "4.2"
            },
            {
                "text": "We can observe that: (1) The proposed method outperforms all state-of-the-art models specific to one task in terms of all metrics on two datasets. This indicates that our proposed model can effectively capture useful information in both tasks by utilizing role and positional information to explicitly control the interaction between the two tasks. Hence, the performance of the two tasks can be boosted mutually. (2) By integrating MHCH with SSA, the multi-task learning model can obtain further improvements. Specifically, we find that the MHCH task has a positive influence on detecting the unsatisfied dialogue. Overall, DCR-Net and our model perform better than standalone models on US F1 of satisfaction prediction. Intuitively, it is mainly because the interaction with handoff can more comprehensively reflect the local dissatisfaction dialogues than solely sentiment polarity analysis, which helps the joint model better identify dissatisfied dialogues for the SSA task. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baselines",
                "sec_num": "4.2"
            },
            {
                "text": "We perform several ablation tests in our model on two datasets and the results are recorded in Table 3 . The results demonstrate the effectiveness of different components of our model. w/o Interact: We modify the full version of our model by only sharing parameters of the Utterance and Matching Encoder. The performance degradation demonstrates the effectiveness of modeling the relations between two tasks with interaction. w/o Select: We remove the Role-Select mechanism to ignore the role information during the interaction process. The performance degradation indicates that straightforward interaction may bring noisy information for both tasks. w/o Position: We remove the positional weights in the MHCH to SSA sub-module. It performs well but worse than Full Model since the position information provide prior knowledge for controlling context interaction. Average, Voting, and Last: Average takes the average of the local satisfaction distributions of customer utterances for classification. Voting directly maps the majority local satisfaction distributions of customer into satisfaction prediction. Last takes the last customer's satisfaction distribution as classification result. Average, Voting and Last are sub-optimal choices and perform worse than the Full Model. This is because the local satisfaction distributions contribute unequally to the overall satisfaction polarity. Also, the majority satisfaction polarity does not directly correlate with the overall satisfaction.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 101,
                        "end": 102,
                        "text": "3",
                        "ref_id": "TABREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Ablation",
                "sec_num": "4.4"
            },
            {
                "text": "Figure 4 illustrates our prediction results with an example dialogue, which is translated from Chinese text. In this case, three utterances (A 4 , C 5 and C 6 ) are labeled as transferable, and two of them (C 5 and C 6 ) are labeled as \"negative emotion\". Among them, A 4 is an unsatisfactory response, which arouses negative emotions of the customer. DAMI only predicts C 5 and C 6 as transferable utterances. However our model successfully detects all the transferable utterances. By mapping local satisfaction distribution of utterances to sentiment of utterances, our model is able to predict reasonable sentiment polarities for customer utterances (detailed analysis is in Subsection 4.6). Considering the context, the customer describes his/her skin problem at C 3 and asks for a recommendation. However, the chatbot does not give any recommendations and returns an irrelevant answer at A 4 . We provide the attention distributions of the utterances on the right side of the example dialog. \u03b1 s 5 and \u03b1 s 6 are the SSA to MHCH attention distributions of C 5 and C 6 ; \u03b1 m 5 and \u03b1 m 6 are the MHCH to SSA attention distributions of C 5 and C 6 . We can observe that attention distributions are concentrated on A 4 rather than other utterances. It is because A 4 is the main cause of negative emotion and dissatisfaction. This again demonstrates that our model can capture the mutual influence between local satisfaction and handoff, which is useful for prediction. In terms of final satisfaction rating, although CAMIL correctly predicts the sentiments of customer utterances, it gives a wrong prediction of satisfaction rating. Our model correctly predicts the satisfaction rating as Unsatisfied by considering the negative emotions and its cause of the unsatisfied response. of the dialogue's satisfaction labels only during the training process. Similarly, our satisfaction prediction is based on the estimation of local satisfaction distributions while the utterance sentiment or satisfaction labels are unobserved. To compare and analyze the performance of utterance-level sentiment classification, we map these distributions into sentiments of utterances as the sentiment prediction results according to the distribution polarities, i.e., unsatisfied \u2192 negative (NG), met \u2192 neutral (NE), well-satisfied \u2192 positive (PO).",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "4",
                        "ref_id": "FIGREF5"
                    }
                ],
                "eq_spans": [],
                "section": "Case Study",
                "sec_num": "4.5"
            },
            {
                "text": "In Table 4 , we compare the sentiment prediction results of MILNET, CAMIL, and our model. On Clothes dataset, our RSSN performs better than other baselines, while it performs worse than CAMIL on Makeup dataset. It is worth noting that our model achieves the best performance on both Clothes and Makeup datasets in terms of NG F1 metric. It indicates that MHCH task is sensitive to negative emotion and contributes more to negative emotion recognition than separate SSA models. From Table 2 , we can also see that our model performs better than separate SSA models in terms of US F1, which is consistent with the findings of sentiment classification.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 9,
                        "end": 10,
                        "text": "4",
                        "ref_id": "TABREF3"
                    },
                    {
                        "start": 488,
                        "end": 489,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Results on Sentiment Classification",
                "sec_num": "4.6"
            },
            {
                "text": "In this paper, we propose an innovative multi-task framework for service satisfaction analysis and machine-human chatting handoff, which deliberately establishes the mutual interrelation for each other. Specifically, we propose a Role-Selected Sharing Network for joint handoff prediction and satisfaction estimation, utilizing role and positional information to control knowledge transfer for both tasks. Extensive experiments and analyses reveal that explicitly modeling the interrelation between the two tasks can boost the performance mutually.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusions and Future works",
                "sec_num": "5"
            },
            {
                "text": "However, our model has not been calibrated to account for user preferences and biases, which we plan to address in future work. Moreover, we will further explore how to adjust the handoff priority with the assistance of personalized information.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusions and Future works",
                "sec_num": "5"
            },
            {
                "text": "https://github.com/WeijiaLau/RSSN",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://pypi.org/project/jieba",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://github.com/WeijiaLau/MHCH-DAMI",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://github.com/senticnet/conv-emotion",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://github.com/google-research/bert",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "We thank the anonymous reviewers for their valuable comments and suggestions. This work is supported by the National Natural Science Foundation of China (61876003, 62106039), the National Key R&D Program of China (2020YFC0832505), the Fundamental Research Funds for the Central Universities, and Alibaba Group through Alibaba Research Intern Program and Alibaba Research Fellowship Program.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgments",
                "sec_num": null
            },
            {
                "text": "LSTMLCA (Dai et al., 2020) , CESTa (Wang et al., 2020b) , and DAMI (Liu et al., 2021) .Baselines for the SSA task: MILNET (Angelidis and Lapata, 2018), HMN (Shen et al., 2018) , and CAMIL (Song et al., 2019) .Multi-task baselines: MT-ES (Ma et al., 2018) , JointBiLSTM (Bodigutla et al., 2020), and DCR-Net (Qin et al., 2020) . Specifically, We modify DCR-Net for our tasks by keeping the core selfattention and co-interactive relation layer.For DAMI, we adopt the open-sourced code 5 to get the results. For DialogueRNN, we adapt the open-sourced code 6 to MHCH by keeping the core component unchanged. For HAN, MILNET, HMN, and CAMIL of SSA, we adopt the reported results from Song et al. (2019) . We re-implement the other models. For BERT+LSTM, we adopt Chinese BERT-base model 7 .",
                "cite_spans": [
                    {
                        "start": 8,
                        "end": 26,
                        "text": "(Dai et al., 2020)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 35,
                        "end": 55,
                        "text": "(Wang et al., 2020b)",
                        "ref_id": null
                    },
                    {
                        "start": 67,
                        "end": 85,
                        "text": "(Liu et al., 2021)",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 156,
                        "end": 175,
                        "text": "(Shen et al., 2018)",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 188,
                        "end": 207,
                        "text": "(Song et al., 2019)",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 237,
                        "end": 254,
                        "text": "(Ma et al., 2018)",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 307,
                        "end": 325,
                        "text": "(Qin et al., 2020)",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 679,
                        "end": 697,
                        "text": "Song et al. (2019)",
                        "ref_id": "BIBREF32"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "annex",
                "sec_num": null
            },
            {
                "text": "Following Song et al. (2019) , we adopt Macro F1 (Mac. F1) and Accuracy (Acc.) for evaluating the SSA task. For evaluating the MHCH task, we adopt F1, Macro F1 (Mac. F1), and Golden Transfer within Tolerance (GT-T) (Liu et al., 2021) . GT-T considers the tolerance property of the MHCH task by the tolerance range T , which allows a \"biased\" prediction within it. The adjustment coefficient \u03bb of GT-T penalizes early or delayed handoff. Likewise, we set \u03bb as 0, and set T to range from 1 to",
                "cite_spans": [
                    {
                        "start": 10,
                        "end": 28,
                        "text": "Song et al. (2019)",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 215,
                        "end": 233,
                        "text": "(Liu et al., 2021)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Comparative Study",
                "sec_num": "4.3"
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Multiple instance learning networks for fine-grained sentiment analysis",
                "authors": [
                    {
                        "first": "Stefanos",
                        "middle": [],
                        "last": "Angelidis",
                        "suffix": ""
                    },
                    {
                        "first": "Mirella",
                        "middle": [],
                        "last": "Lapata",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "TACL",
                "volume": "6",
                "issue": "",
                "pages": "17--31",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Stefanos Angelidis and Mirella Lapata. 2018. Multi- ple instance learning networks for fine-grained sen- timent analysis. TACL, 6:17-31.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Layer normalization",
                "authors": [
                    {
                        "first": "Jimmy",
                        "middle": [],
                        "last": "Lei Ba",
                        "suffix": ""
                    },
                    {
                        "first": "Jamie",
                        "middle": [],
                        "last": "Ryan Kiros",
                        "suffix": ""
                    },
                    {
                        "first": "Geoffrey",
                        "middle": [
                            "E"
                        ],
                        "last": "Hinton",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1607.06450"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin- ton. 2016. Layer normalization. arXiv preprint arXiv:1607.06450.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Multi-domain conversation quality evaluation via user satisfaction estimation",
                "authors": [
                    {
                        "first": "Praveen",
                        "middle": [],
                        "last": "Kumar",
                        "suffix": ""
                    },
                    {
                        "first": "Bodigutla",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Lazaros Polymenakos, and Spyros Matsoukas",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1911.08567"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Praveen Kumar Bodigutla, Lazaros Polymenakos, and Spyros Matsoukas. 2019a. Multi-domain conversa- tion quality evaluation via user satisfaction estima- tion. arXiv preprint arXiv:1911.08567.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Joint turn and dialogue level user satisfaction estimation on mulit-domain conversations",
                "authors": [
                    {
                        "first": "Praveen",
                        "middle": [],
                        "last": "Kumar Bodigutla",
                        "suffix": ""
                    },
                    {
                        "first": "Aditya",
                        "middle": [],
                        "last": "Tiwari",
                        "suffix": ""
                    },
                    {
                        "first": "Spyros",
                        "middle": [],
                        "last": "Matsoukas",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proc. of EMNLP: Findings",
                "volume": "",
                "issue": "",
                "pages": "3897--3909",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Praveen Kumar Bodigutla, Aditya Tiwari, Spyros Matsoukas, Josep Valls-Vargas, and Lazaros Poly- menakos. 2020. Joint turn and dialogue level user satisfaction estimation on mulit-domain conversa- tions. In Proc. of EMNLP: Findings, pages 3897- 3909.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Domain-independent turn-level dialogue quality evaluation via user satisfaction estimation",
                "authors": [
                    {
                        "first": "Praveen",
                        "middle": [],
                        "last": "Kumar Bodigutla",
                        "suffix": ""
                    },
                    {
                        "first": "Longshaokan",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Kate",
                        "middle": [],
                        "last": "Ridgeway",
                        "suffix": ""
                    },
                    {
                        "first": "Joshua",
                        "middle": [],
                        "last": "Levy",
                        "suffix": ""
                    },
                    {
                        "first": "Swanand",
                        "middle": [],
                        "last": "Joshi",
                        "suffix": ""
                    },
                    {
                        "first": "Alborz",
                        "middle": [],
                        "last": "Geramifard",
                        "suffix": ""
                    },
                    {
                        "first": "Spyros",
                        "middle": [],
                        "last": "Matsoukas",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1908.07064"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Praveen Kumar Bodigutla, Longshaokan Wang, Kate Ridgeway, Joshua Levy, Swanand Joshi, Al- borz Geramifard, and Spyros Matsoukas. 2019b. Domain-independent turn-level dialogue quality evaluation via user satisfaction estimation. arXiv preprint arXiv:1908.07064.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Chatbots: Changing user needs and motivations",
                "authors": [],
                "year": 2018,
                "venue": "Interactions",
                "volume": "25",
                "issue": "5",
                "pages": "38--43",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Petter Bae Brandtzaeg and Asbj\u00f8rn F\u00f8lstad. 2018. Chatbots: Changing user needs and motivations. In- teractions, 25(5):38-43.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "How should my chatbot interact? A survey on social characteristics in human-chatbot interaction design",
                "authors": [
                    {
                        "first": "Ana",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Paula",
                        "middle": [],
                        "last": "Chaves",
                        "suffix": ""
                    },
                    {
                        "first": "Marco",
                        "middle": [],
                        "last": "Aurelio",
                        "suffix": ""
                    },
                    {
                        "first": "Gerosa",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "IJHCI",
                "volume": "",
                "issue": "",
                "pages": "1--30",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ana Paula Chaves and Marco Aurelio Gerosa. 2020. How should my chatbot interact? A survey on social characteristics in human-chatbot interaction design. IJHCI, pages 1-30.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Chatbotbased heathcare service with a knowledge base for cloud computing",
                "authors": [
                    {
                        "first": "Kyungyong",
                        "middle": [],
                        "last": "Chung",
                        "suffix": ""
                    },
                    {
                        "first": "Roy",
                        "middle": [
                            "C"
                        ],
                        "last": "Park",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Cluster Computing",
                "volume": "22",
                "issue": "1",
                "pages": "1925--1937",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kyungyong Chung and Roy C Park. 2019. Chatbot- based heathcare service with a knowledge base for cloud computing. Cluster Computing, 22(1):1925- 1937.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Local contextual attention with hierarchical structure for dialogue act recognition",
                "authors": [
                    {
                        "first": "Zhigang",
                        "middle": [],
                        "last": "Dai",
                        "suffix": ""
                    },
                    {
                        "first": "Jinhua",
                        "middle": [],
                        "last": "Fu",
                        "suffix": ""
                    },
                    {
                        "first": "Qile",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    },
                    {
                        "first": "Hengbin",
                        "middle": [],
                        "last": "Cui",
                        "suffix": ""
                    },
                    {
                        "first": "Yuan",
                        "middle": [],
                        "last": "Qi",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2003.06044"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Zhigang Dai, Jinhua Fu, Qile Zhu, Hengbin Cui, Yuan Qi, et al. 2020. Local contextual attention with hier- archical structure for dialogue act recognition. arXiv preprint arXiv:2003.06044.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
                "authors": [
                    {
                        "first": "Jacob",
                        "middle": [],
                        "last": "Devlin",
                        "suffix": ""
                    },
                    {
                        "first": "Ming-Wei",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "Kenton",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Kristina",
                        "middle": [],
                        "last": "Toutanova",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proc. of NAACL",
                "volume": "",
                "issue": "",
                "pages": "4171--4186",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proc. of NAACL, pages 4171-4186.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Ordinal regression for interaction quality prediction",
                "authors": [
                    {
                        "first": "Layla",
                        "middle": [
                            "El"
                        ],
                        "last": "Asri",
                        "suffix": ""
                    },
                    {
                        "first": "Hatim",
                        "middle": [],
                        "last": "Khouzaimi",
                        "suffix": ""
                    },
                    {
                        "first": "Romain",
                        "middle": [],
                        "last": "Laroche",
                        "suffix": ""
                    },
                    {
                        "first": "Olivier",
                        "middle": [],
                        "last": "Pietquin",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proc. of ICASSP",
                "volume": "",
                "issue": "",
                "pages": "3221--3225",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Layla El Asri, Hatim Khouzaimi, Romain Laroche, and Olivier Pietquin. 2014. Ordinal regression for inter- action quality prediction. In Proc. of ICASSP, pages 3221-3225.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Understanding the difficulty of training deep feedforward neural networks",
                "authors": [
                    {
                        "first": "Xavier",
                        "middle": [],
                        "last": "Glorot",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proc of AISTATS",
                "volume": "",
                "issue": "",
                "pages": "249--256",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xavier Glorot and Yoshua Bengio. 2010. Understand- ing the difficulty of training deep feedforward neural networks. In Proc of AISTATS, pages 249-256.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Towards taxonomy of errors in chat-oriented dialogue systems",
                "authors": [
                    {
                        "first": "Ryuichiro",
                        "middle": [],
                        "last": "Higashinaka",
                        "suffix": ""
                    },
                    {
                        "first": "Kotaro",
                        "middle": [],
                        "last": "Funakoshi",
                        "suffix": ""
                    },
                    {
                        "first": "Masahiro",
                        "middle": [],
                        "last": "Araki",
                        "suffix": ""
                    },
                    {
                        "first": "Hiroshi",
                        "middle": [],
                        "last": "Tsukahara",
                        "suffix": ""
                    },
                    {
                        "first": "Yuka",
                        "middle": [],
                        "last": "Kobayashi",
                        "suffix": ""
                    },
                    {
                        "first": "Masahiro",
                        "middle": [],
                        "last": "Mizukami",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proc. of SIGDIAL",
                "volume": "",
                "issue": "",
                "pages": "87--95",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ryuichiro Higashinaka, Kotaro Funakoshi, Masahiro Araki, Hiroshi Tsukahara, Yuka Kobayashi, and Masahiro Mizukami. 2015a. Towards taxonomy of errors in chat-oriented dialogue systems. In Proc. of SIGDIAL, pages 87-95.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Fatal or not? Finding errors that lead to dialogue breakdowns in chatoriented dialogue systems",
                "authors": [
                    {
                        "first": "Ryuichiro",
                        "middle": [],
                        "last": "Higashinaka",
                        "suffix": ""
                    },
                    {
                        "first": "Masahiro",
                        "middle": [],
                        "last": "Mizukami",
                        "suffix": ""
                    },
                    {
                        "first": "Kotaro",
                        "middle": [],
                        "last": "Funakoshi",
                        "suffix": ""
                    },
                    {
                        "first": "Masahiro",
                        "middle": [],
                        "last": "Araki",
                        "suffix": ""
                    },
                    {
                        "first": "Hiroshi",
                        "middle": [],
                        "last": "Tsukahara",
                        "suffix": ""
                    },
                    {
                        "first": "Yuka",
                        "middle": [],
                        "last": "Kobayashi",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proc. of EMNLP",
                "volume": "",
                "issue": "",
                "pages": "2243--2248",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ryuichiro Higashinaka, Masahiro Mizukami, Kotaro Funakoshi, Masahiro Araki, Hiroshi Tsukahara, and Yuka Kobayashi. 2015b. Fatal or not? Finding errors that lead to dialogue breakdowns in chat- oriented dialogue systems. In Proc. of EMNLP, pages 2243-2248.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Long short-term memory",
                "authors": [
                    {
                        "first": "Sepp",
                        "middle": [],
                        "last": "Hochreiter",
                        "suffix": ""
                    },
                    {
                        "first": "J\u00fcrgen",
                        "middle": [],
                        "last": "Schmidhuber",
                        "suffix": ""
                    }
                ],
                "year": 1997,
                "venue": "Neural computation",
                "volume": "9",
                "issue": "8",
                "pages": "1735--1780",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735-1780.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Evorus: A crowd-powered conversational assistant built to automate itself over time",
                "authors": [
                    {
                        "first": "Ting-Hao Kenneth",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Joseph",
                        "middle": [
                            "Chee"
                        ],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "Jeffrey",
                        "middle": [
                            "P"
                        ],
                        "last": "Bigham",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proc. of CHI",
                "volume": "",
                "issue": "",
                "pages": "1--13",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ting-Hao Kenneth Huang, Joseph Chee Chang, and Jef- frey P. Bigham. 2018. Evorus: A crowd-powered conversational assistant built to automate itself over time. In Proc. of CHI, pages 1-13.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Evaluating and informing the design of chatbots",
                "authors": [
                    {
                        "first": "Mohit",
                        "middle": [],
                        "last": "Jain",
                        "suffix": ""
                    },
                    {
                        "first": "Pratyush",
                        "middle": [],
                        "last": "Kumar",
                        "suffix": ""
                    },
                    {
                        "first": "Ramachandra",
                        "middle": [],
                        "last": "Kota",
                        "suffix": ""
                    },
                    {
                        "first": "Shwetak N",
                        "middle": [],
                        "last": "Patel",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proc. of DIS",
                "volume": "",
                "issue": "",
                "pages": "895--906",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mohit Jain, Pratyush Kumar, Ramachandra Kota, and Shwetak N Patel. 2018. Evaluating and informing the design of chatbots. In Proc. of DIS, pages 895- 906.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Adam: A method for stochastic optimization",
                "authors": [
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Diederik",
                        "suffix": ""
                    },
                    {
                        "first": "Jimmy",
                        "middle": [],
                        "last": "Kingma",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Ba",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proc. of ICLR (Poster)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In Proc. of ICLR (Poster).",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Dialogue act sequence labeling using hierarchical encoder with CRF",
                "authors": [
                    {
                        "first": "Harshit",
                        "middle": [],
                        "last": "Kumar",
                        "suffix": ""
                    },
                    {
                        "first": "Arvind",
                        "middle": [],
                        "last": "Agarwal",
                        "suffix": ""
                    },
                    {
                        "first": "Riddhiman",
                        "middle": [],
                        "last": "Dasgupta",
                        "suffix": ""
                    },
                    {
                        "first": "Sachindra",
                        "middle": [],
                        "last": "Joshi",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proc. of AAAI",
                "volume": "",
                "issue": "",
                "pages": "3440--3447",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Harshit Kumar, Arvind Agarwal, Riddhiman Dasgupta, and Sachindra Joshi. 2018. Dialogue act sequence labeling using hierarchical encoder with CRF. In Proc. of AAAI, pages 3440-3447.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Time to transfer: Predicting and evaluating machine-human chatting handoff",
                "authors": [
                    {
                        "first": "Jiawei",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Zhe",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Yangyang",
                        "middle": [],
                        "last": "Kang",
                        "suffix": ""
                    },
                    {
                        "first": "Zhuoren",
                        "middle": [],
                        "last": "Jiang",
                        "suffix": ""
                    },
                    {
                        "first": "Guoxiu",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Changlong",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaozhong",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Lu",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proc. of AAAI",
                "volume": "",
                "issue": "",
                "pages": "5841--5849",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jiawei Liu, Zhe Gao, Yangyang Kang, Zhuoren Jiang, Guoxiu He, Changlong Sun, Xiaozhong Liu, and Wei Lu. 2021. Time to transfer: Predicting and eval- uating machine-human chatting handoff. In Proc. of AAAI, pages 5841-5849.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Detect rumor and stance jointly by neural multi-task learning",
                "authors": [
                    {
                        "first": "Jing",
                        "middle": [],
                        "last": "Ma",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Kam-Fai",
                        "middle": [],
                        "last": "Wong",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proc. of WWW",
                "volume": "",
                "issue": "",
                "pages": "585--593",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jing Ma, Wei Gao, and Kam-Fai Wong. 2018. Detect rumor and stance jointly by neural multi-task learn- ing. In Proc. of WWW, pages 585-593.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Dialoguernn: An attentive RNN for emotion detection in conversations",
                "authors": [
                    {
                        "first": "Navonil",
                        "middle": [],
                        "last": "Majumder",
                        "suffix": ""
                    },
                    {
                        "first": "Soujanya",
                        "middle": [],
                        "last": "Poria",
                        "suffix": ""
                    },
                    {
                        "first": "Devamanyu",
                        "middle": [],
                        "last": "Hazarika",
                        "suffix": ""
                    },
                    {
                        "first": "Rada",
                        "middle": [],
                        "last": "Mihalcea",
                        "suffix": ""
                    },
                    {
                        "first": "Alexander",
                        "middle": [
                            "F"
                        ],
                        "last": "Gelbukh",
                        "suffix": ""
                    },
                    {
                        "first": "Erik",
                        "middle": [],
                        "last": "Cambria",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proc. of AAAI",
                "volume": "",
                "issue": "",
                "pages": "6818--6825",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Navonil Majumder, Soujanya Poria, Devamanyu Haz- arika, Rada Mihalcea, Alexander F. Gelbukh, and Erik Cambria. 2019. Dialoguernn: An attentive RNN for emotion detection in conversations. In Proc. of AAAI, pages 6818-6825.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Efficient estimation of word representations in vector space",
                "authors": [
                    {
                        "first": "Tomas",
                        "middle": [],
                        "last": "Mikolov",
                        "suffix": ""
                    },
                    {
                        "first": "Kai",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Greg",
                        "middle": [],
                        "last": "Corrado",
                        "suffix": ""
                    },
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Dean",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1301.3781"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Tomas Mikolov, Kai Chen, Greg Corrado, and Jef- frey Dean. 2013. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "DCR-Net: A deep co-interactive relation network for joint dialog act recognition and sentiment classification",
                "authors": [
                    {
                        "first": "Libo",
                        "middle": [],
                        "last": "Qin",
                        "suffix": ""
                    },
                    {
                        "first": "Wanxiang",
                        "middle": [],
                        "last": "Che",
                        "suffix": ""
                    },
                    {
                        "first": "Yangming",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Minheng",
                        "middle": [],
                        "last": "Ni",
                        "suffix": ""
                    },
                    {
                        "first": "Ting",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proc. of AAAI",
                "volume": "",
                "issue": "",
                "pages": "8665--8672",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Libo Qin, Wanxiang Che, Yangming Li, Minheng Ni, and Ting Liu. 2020. DCR-Net: A deep co-interactive relation network for joint dialog act recognition and sentiment classification. In Proc. of AAAI, pages 8665-8672.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "AliMe chat: A sequence to sequence and rerank based chatbot engine",
                "authors": [
                    {
                        "first": "Minghui",
                        "middle": [],
                        "last": "Qiu",
                        "suffix": ""
                    },
                    {
                        "first": "Feng-Lin",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Siyu",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Xing",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Yan",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Weipeng",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Haiqing",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Jun",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Chu",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proc. of ACL",
                "volume": "",
                "issue": "",
                "pages": "498--503",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Minghui Qiu, Feng-Lin Li, Siyu Wang, Xing Gao, Yan Chen, Weipeng Zhao, Haiqing Chen, Jun Huang, and Wei Chu. 2017. AliMe chat: A sequence to se- quence and rerank based chatbot engine. In Proc. of ACL, pages 498-503.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Evaluating quality of chatbots and intelligent conversational agents",
                "authors": [
                    {
                        "first": "Nicole",
                        "middle": [],
                        "last": "Radziwill",
                        "suffix": ""
                    },
                    {
                        "first": "Morgan",
                        "middle": [],
                        "last": "Benton",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Software Quality Professional",
                "volume": "19",
                "issue": "3",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Nicole Radziwill and Morgan Benton. 2017. Evaluat- ing quality of chatbots and intelligent conversational agents. Software Quality Professional, 19(3):25.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Dialogue Act Classification with Context-Aware Self-Attention",
                "authors": [
                    {
                        "first": "Vipul",
                        "middle": [],
                        "last": "Raheja",
                        "suffix": ""
                    },
                    {
                        "first": "Joel",
                        "middle": [],
                        "last": "Tetreault",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proc. of NAACL",
                "volume": "",
                "issue": "",
                "pages": "3727--3733",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Vipul Raheja and Joel Tetreault. 2019. Dialogue Act Classification with Context-Aware Self-Attention. In Proc. of NAACL, pages 3727-3733.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Learning end-to-end goaloriented dialog with maximal user task success and minimal human agent use",
                "authors": [
                    {
                        "first": "Janarthanan",
                        "middle": [],
                        "last": "Rajendran",
                        "suffix": ""
                    },
                    {
                        "first": "Jatin",
                        "middle": [],
                        "last": "Ganhotra",
                        "suffix": ""
                    },
                    {
                        "first": "Lazaros",
                        "middle": [
                            "C"
                        ],
                        "last": "Polymenakos",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "TACL",
                "volume": "7",
                "issue": "",
                "pages": "375--386",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Janarthanan Rajendran, Jatin Ganhotra, and Lazaros C. Polymenakos. 2019. Learning end-to-end goal- oriented dialog with maximal user task success and minimal human agent use. TACL, 7:375-386.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Conversational AI: The science behind the alexa prize",
                "authors": [
                    {
                        "first": "Ashwin",
                        "middle": [],
                        "last": "Ram",
                        "suffix": ""
                    },
                    {
                        "first": "Rohit",
                        "middle": [],
                        "last": "Prasad",
                        "suffix": ""
                    },
                    {
                        "first": "Chandra",
                        "middle": [],
                        "last": "Khatri",
                        "suffix": ""
                    },
                    {
                        "first": "Anu",
                        "middle": [],
                        "last": "Venkatesh",
                        "suffix": ""
                    },
                    {
                        "first": "Raefer",
                        "middle": [],
                        "last": "Gabriel",
                        "suffix": ""
                    },
                    {
                        "first": "Qing",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Jeff",
                        "middle": [],
                        "last": "Nunn",
                        "suffix": ""
                    },
                    {
                        "first": "Behnam",
                        "middle": [],
                        "last": "Hedayatnia",
                        "suffix": ""
                    },
                    {
                        "first": "Ming",
                        "middle": [],
                        "last": "Cheng",
                        "suffix": ""
                    },
                    {
                        "first": "Ashish",
                        "middle": [],
                        "last": "Nagar",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1801.03604"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ashwin Ram, Rohit Prasad, Chandra Khatri, Anu Venkatesh, Raefer Gabriel, Qing Liu, Jeff Nunn, Behnam Hedayatnia, Ming Cheng, Ashish Nagar, et al. 2018. Conversational AI: The science behind the alexa prize. arXiv preprint arXiv:1801.03604.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "A parameterized and annotated spoken dialog corpus of the CMU let's go bus information system",
                "authors": [
                    {
                        "first": "Alexander",
                        "middle": [],
                        "last": "Schmitt",
                        "suffix": ""
                    },
                    {
                        "first": "Stefan",
                        "middle": [],
                        "last": "Ultes",
                        "suffix": ""
                    },
                    {
                        "first": "Wolfgang",
                        "middle": [],
                        "last": "Minker",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Proc. of LREC",
                "volume": "",
                "issue": "",
                "pages": "3369--3373",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alexander Schmitt, Stefan Ultes, and Wolfgang Minker. 2012. A parameterized and annotated spoken dialog corpus of the CMU let's go bus information system. In Proc. of LREC, pages 3369-3373.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "Sentiment classification towards question-answering with hierarchical matching network",
                "authors": [
                    {
                        "first": "Chenlin",
                        "middle": [],
                        "last": "Shen",
                        "suffix": ""
                    },
                    {
                        "first": "Changlong",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Jingjing",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Yangyang",
                        "middle": [],
                        "last": "Kang",
                        "suffix": ""
                    },
                    {
                        "first": "Shoushan",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaozhong",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Luo",
                        "middle": [],
                        "last": "Si",
                        "suffix": ""
                    },
                    {
                        "first": "Min",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Guodong",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proc. of EMNLP",
                "volume": "",
                "issue": "",
                "pages": "3654--3663",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chenlin Shen, Changlong Sun, Jingjing Wang, Yangyang Kang, Shoushan Li, Xiaozhong Liu, Luo Si, Min Zhang, and Guodong Zhou. 2018. Senti- ment classification towards question-answering with hierarchical matching network. In Proc. of EMNLP, pages 3654-3663.",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "Cheap and fast -but is it good? evaluating non-expert annotations for natural language tasks",
                "authors": [
                    {
                        "first": "Rion",
                        "middle": [],
                        "last": "Snow",
                        "suffix": ""
                    },
                    {
                        "first": "O'",
                        "middle": [],
                        "last": "Brendan",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Connor",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Jurafsky",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Ng",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Proc. of EMNLP",
                "volume": "",
                "issue": "",
                "pages": "254--263",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Rion Snow, Brendan O'Connor, Daniel Jurafsky, and Andrew Ng. 2008. Cheap and fast -but is it good? evaluating non-expert annotations for natural lan- guage tasks. In Proc. of EMNLP, pages 254-263.",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "Using customer service dialogues for satisfaction analysis with contextassisted multiple instance learning",
                "authors": [
                    {
                        "first": "Kaisong",
                        "middle": [],
                        "last": "Song",
                        "suffix": ""
                    },
                    {
                        "first": "Lidong",
                        "middle": [],
                        "last": "Bing",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Jun",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Lujun",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Jiancheng",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Changlong",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaozhong",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Qiong",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proc. of EMNLP",
                "volume": "",
                "issue": "",
                "pages": "198--207",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kaisong Song, Lidong Bing, Wei Gao, Jun Lin, Lujun Zhao, Jiancheng Wang, Changlong Sun, Xiaozhong Liu, and Qiong Zhang. 2019. Using customer ser- vice dialogues for satisfaction analysis with context- assisted multiple instance learning. In Proc. of EMNLP, pages 198-207.",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "Dropout: a simple way to prevent neural networks from overfitting",
                "authors": [
                    {
                        "first": "Nitish",
                        "middle": [],
                        "last": "Srivastava",
                        "suffix": ""
                    },
                    {
                        "first": "Geoffrey",
                        "middle": [],
                        "last": "Hinton",
                        "suffix": ""
                    },
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Krizhevsky",
                        "suffix": ""
                    },
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    },
                    {
                        "first": "Ruslan",
                        "middle": [],
                        "last": "Salakhutdinov",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "JMLR",
                "volume": "15",
                "issue": "1",
                "pages": "1929--1958",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from overfitting. JMLR, 15(1):1929-1958.",
                "links": null
            },
            "BIBREF34": {
                "ref_id": "b34",
                "title": "Improving interaction quality estimation with BiLSTMs and the impact on dialogue policy learning",
                "authors": [
                    {
                        "first": "Stefan",
                        "middle": [],
                        "last": "Ultes",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proc. of SIGDIAL",
                "volume": "",
                "issue": "",
                "pages": "11--20",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Stefan Ultes. 2019. Improving interaction quality esti- mation with BiLSTMs and the impact on dialogue policy learning. In Proc. of SIGDIAL, pages 11-20.",
                "links": null
            },
            "BIBREF35": {
                "ref_id": "b35",
                "title": "Application and evaluation of a conditioned hidden markov model for estimating interaction quality of spoken dialogue systems",
                "authors": [
                    {
                        "first": "Stefan",
                        "middle": [],
                        "last": "Ultes",
                        "suffix": ""
                    },
                    {
                        "first": "Robert",
                        "middle": [],
                        "last": "Elchab",
                        "suffix": ""
                    },
                    {
                        "first": "Wolfgang",
                        "middle": [],
                        "last": "Minker",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Natural Interaction with Robots, Knowbots and Smartphones",
                "volume": "",
                "issue": "",
                "pages": "303--312",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Stefan Ultes, Robert ElChab, and Wolfgang Minker. 2014. Application and evaluation of a conditioned hidden markov model for estimating interaction quality of spoken dialogue systems. Natural Interac- tion with Robots, Knowbots and Smartphones, pages 303-312.",
                "links": null
            },
            "BIBREF36": {
                "ref_id": "b36",
                "title": "Attention is all you need",
                "authors": [
                    {
                        "first": "Ashish",
                        "middle": [],
                        "last": "Vaswani",
                        "suffix": ""
                    },
                    {
                        "first": "Noam",
                        "middle": [],
                        "last": "Shazeer",
                        "suffix": ""
                    },
                    {
                        "first": "Niki",
                        "middle": [],
                        "last": "Parmar",
                        "suffix": ""
                    },
                    {
                        "first": "Jakob",
                        "middle": [],
                        "last": "Uszkoreit",
                        "suffix": ""
                    },
                    {
                        "first": "Llion",
                        "middle": [],
                        "last": "Jones",
                        "suffix": ""
                    },
                    {
                        "first": "Aidan",
                        "middle": [
                            "N"
                        ],
                        "last": "Gomez",
                        "suffix": ""
                    },
                    {
                        "first": "Lukasz",
                        "middle": [],
                        "last": "Kaiser",
                        "suffix": ""
                    },
                    {
                        "first": "Illia",
                        "middle": [],
                        "last": "Polosukhin",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proc. of NIPS",
                "volume": "",
                "issue": "",
                "pages": "5998--6008",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proc. of NIPS, pages 5998-6008.",
                "links": null
            },
            "BIBREF37": {
                "ref_id": "b37",
                "title": "Masking orchestration: Multi-task pretraining for multi-role dialogue representation learning",
                "authors": [
                    {
                        "first": "Tianyi",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Yating",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaozhong",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Changlong",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Qiong",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proc. of AAAI",
                "volume": "",
                "issue": "",
                "pages": "9217--9224",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tianyi Wang, Yating Zhang, Xiaozhong Liu, Chang- long Sun, and Qiong Zhang. 2020a. Masking or- chestration: Multi-task pretraining for multi-role di- alogue representation learning. In Proc. of AAAI, pages 9217-9224.",
                "links": null
            },
            "BIBREF38": {
                "ref_id": "b38",
                "title": "Contextualized emotion recognition in conversation as sequence tagging",
                "authors": [
                    {
                        "first": "Yan",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Jiayu",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Jun",
                        "middle": [],
                        "last": "Ma",
                        "suffix": ""
                    },
                    {
                        "first": "Shaojun",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Jing",
                        "middle": [],
                        "last": "Xiao",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proc. of SIGDIAL",
                "volume": "",
                "issue": "",
                "pages": "186--195",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yan Wang, Jiayu Zhang, Jun Ma, Shaojun Wang, and Jing Xiao. 2020b. Contextualized emotion recogni- tion in conversation as sequence tagging. In Proc. of SIGDIAL, pages 186-195.",
                "links": null
            },
            "BIBREF39": {
                "ref_id": "b39",
                "title": "Dynamic memory networks for visual and textual question answering",
                "authors": [
                    {
                        "first": "Caiming",
                        "middle": [],
                        "last": "Xiong",
                        "suffix": ""
                    },
                    {
                        "first": "Stephen",
                        "middle": [],
                        "last": "Merity",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proc. of ICML",
                "volume": "",
                "issue": "",
                "pages": "2397--2406",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Caiming Xiong, Stephen Merity, and Richard Socher. 2016. Dynamic memory networks for visual and textual question answering. In Proc. of ICML, pages 2397-2406.",
                "links": null
            },
            "BIBREF40": {
                "ref_id": "b40",
                "title": "Hierarchical attention networks for document classification",
                "authors": [
                    {
                        "first": "Zichao",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Diyi",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Dyer",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaodong",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Smola",
                        "suffix": ""
                    },
                    {
                        "first": "Eduard",
                        "middle": [],
                        "last": "Hovy",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proc. of NAACL",
                "volume": "",
                "issue": "",
                "pages": "1480--1489",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, and Eduard Hovy. 2016. Hierarchical attention networks for document classification. In Proc. of NAACL, pages 1480-1489.",
                "links": null
            },
            "BIBREF41": {
                "ref_id": "b41",
                "title": "The design and implementation of XiaoIce, an empathetic social chatbot",
                "authors": [
                    {
                        "first": "Li",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Jianfeng",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Di",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Heung-Yeung",
                        "middle": [],
                        "last": "Shum",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Computational Linguistics",
                "volume": "46",
                "issue": "1",
                "pages": "53--93",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Li Zhou, Jianfeng Gao, Di Li, and Heung-Yeung Shum. 2020. The design and implementation of XiaoIce, an empathetic social chatbot. Computational Lin- guistics, 46(1):53-93.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "utter 5 Fine, hope I can receive it soon.",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "placed my order about one week ago. When can it be shipped? I'm sorry for the inconvenience.",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "Figure 1: A snippet of a moderately satisfied customer service dialogue. There is a satisfaction rating at the end of the conversation. The utterance with an orange background color denotes a transferable utterance.",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF3": {
                "num": null,
                "text": "Figure 2: The architecture of our Role-Selected Sharing Network (RSSN) Network.",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "FIGREF4": {
                "num": null,
                "text": "Figure 3: The relative handoff position distributions in three different service satisfaction ratings.",
                "uris": null,
                "fig_num": "3",
                "type_str": "figure"
            },
            "FIGREF5": {
                "num": null,
                "text": "Figure 4: An example dialogue with predictions and attention distribution. C i /A i denotes Customer/Chatbot utterance, followed by true labels. The sentiment labels of Customer utterances are also given along with the handoff labels. The other columns are the predictions of our model, CAMIL and DAMI, respectively. The satisfaction ratings of ground truth and predictions are in the last row of the table. N/T denotes Normal/Transferable.",
                "uris": null,
                "fig_num": "4",
                "type_str": "figure"
            },
            "FIGREF6": {
                "num": null,
                "text": "Song et al. (2019) utilize multiple instance learning to predict the satisfaction rating and the sentiment of customer utterances with the supervision",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table/>",
                "type_str": "table",
                "text": "",
                "html": null,
                "num": null
            },
            "TABREF1": {
                "content": "<table><tr><td>Statistics items</td><td colspan=\"2\">Clothes Makeup</td></tr><tr><td># Dialogues</td><td>10,000</td><td>3,540</td></tr><tr><td># US (unsatisfied)</td><td>2,302</td><td>1,180</td></tr><tr><td># MT (met)</td><td>6,399</td><td>1,180</td></tr><tr><td># WS (well satisfied)</td><td>1,299</td><td>1,180</td></tr><tr><td># Transferable Utterances</td><td>16,921</td><td>7,668</td></tr><tr><td># Normal Utterances</td><td>237,891</td><td>86,778</td></tr><tr><td>Avg # Utterances</td><td>25.48</td><td>26.67</td></tr><tr><td>Avg # Tokens</td><td>7.64</td><td>7.87</td></tr><tr><td>Kappa</td><td>0.85</td><td>0.88</td></tr></table>",
                "type_str": "table",
                "text": "Statistics of the datasets.",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table><tr><td/><td>Clothes</td><td/><td>Makeup</td><td/></tr><tr><td>Models</td><td>MHCH</td><td>SSA</td><td>MHCH</td><td>SSA</td></tr><tr><td/><td colspan=\"4\">F1 Mac. F1 GT-I Mac. F1 Acc. F1 Mac. F1 GT-I Mac. F1 Acc.</td></tr><tr><td>Average</td><td colspan=\"4\">65.3 81.6 74.6 63.8 73.7 64.1 80.4 73.2 73.6 73.7</td></tr><tr><td>Voting</td><td colspan=\"4\">61.7 79.7 71.7 27.5 61.2 62.0 79.4 68.3 34.6 42.1</td></tr><tr><td>Last</td><td colspan=\"4\">66.4 82.1 74.6 67.0 75.6 62.8 79.8 71.5 76.6 76.8</td></tr><tr><td colspan=\"5\">w/o Interact 65.6 81.6 70.6 65.5 72.3 62.0 79.5 71.4 74.8 74.6</td></tr><tr><td colspan=\"5\">w/o Select 64.4 81.0 73.6 67.0 74.3 61.7 79.2 71.7 72.9 72.9</td></tr><tr><td colspan=\"5\">w/o Position 66.1 81.9 73.2 68.4 76.0 64.7 80.8 73.5 76.8 76.8</td></tr><tr><td colspan=\"5\">Full Model 69.2 83.6 78.4 71.7 79.5 65.9 81.5 75.1 80.8 80.8</td></tr></table>",
                "type_str": "table",
                "text": ", Ablation study performance (%) on Clothes and Makeup test datasets. w/o denotes \"without\".",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table><tr><td/><td>Clothes</td><td>Makeup</td><td/></tr><tr><td>Models</td><td colspan=\"3\">PO F1 NE F1 NG F1 Mac. F1 Acc. PO F1 NE F1 NG F1 Mac. F1 Acc.</td></tr><tr><td colspan=\"2\">MILNET 44.1 81.4 40.4</td><td>55.3 71.3 44.7 38.7 41.6</td><td>41.7 41.0</td></tr><tr><td colspan=\"2\">CAMIL 48.4 89.3 55.5</td><td>64.4 82.4 54.4 72.5 51.6</td><td>59.5 64.7</td></tr><tr><td>RSSN</td><td>63.5 90.1 58.4</td><td>70.7 83.8 51.3 67.8 54.6</td><td>57.9 61.6</td></tr></table>",
                "type_str": "table",
                "text": "Results of sentiment classification by different models on Clothes and Makeup test datasets.",
                "html": null,
                "num": null
            }
        }
    }
}