{
    "paper_id": "P12-1103",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T11:56:34.719362Z"
    },
    "title": "Improve SMT Quality with Automatically Extracted Paraphrase Rules",
    "authors": [
        {
            "first": "Wei",
            "middle": [],
            "last": "He",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Harbin Institute of Technology",
                "location": {}
            },
            "email": ""
        },
        {
            "first": "Hua",
            "middle": [],
            "last": "Wu",
            "suffix": "",
            "affiliation": {},
            "email": "wu_hua@baidu.com"
        },
        {
            "first": "Haifeng",
            "middle": [],
            "last": "Wang",
            "suffix": "",
            "affiliation": {},
            "email": "wanghaifeng@baidu.com"
        },
        {
            "first": "Ting",
            "middle": [],
            "last": "Liu",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Harbin Institute of Technology",
                "location": {}
            },
            "email": "tliu@ir.hit.edu.cn"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "We propose a novel approach to improve SMT via paraphrase rules which are automatically extracted from the bilingual training data. Without using extra paraphrase resources, we acquire the rules by comparing the source side of the parallel corpus with the target-to-source translations of the target side. Besides the word and phrase paraphrases, the acquired paraphrase rules mainly cover the structured paraphrases on the sentence level. These rules are employed to enrich the SMT inputs for translation quality improvement. The experimental results show that our proposed approach achieves significant improvements of 1.6~3.6 points of BLEU in the oral domain and 0.5~1 points in the news domain.\nThis work was done when the first author was visiting Baidu.",
    "pdf_parse": {
        "paper_id": "P12-1103",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "We propose a novel approach to improve SMT via paraphrase rules which are automatically extracted from the bilingual training data. Without using extra paraphrase resources, we acquire the rules by comparing the source side of the parallel corpus with the target-to-source translations of the target side. Besides the word and phrase paraphrases, the acquired paraphrase rules mainly cover the structured paraphrases on the sentence level. These rules are employed to enrich the SMT inputs for translation quality improvement. The experimental results show that our proposed approach achieves significant improvements of 1.6~3.6 points of BLEU in the oral domain and 0.5~1 points in the news domain.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            },
            {
                "text": "This work was done when the first author was visiting Baidu.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "The translation quality of the SMT system is highly related to the coverage of translation models. However, no matter how much data is used for training, it is still impossible to completely cover the unlimited input sentences. This problem is more serious for online SMT systems in real-world applications. Naturally, a solution to the coverage problem is to bridge the gaps between the input sentences and the translation models, either from the input side, which targets on rewriting the input sentences to the MT-favored expressions, or from the side of translation models, which tries to enrich the translation models to cover more expressions.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In recent years, paraphrasing has been proven useful for improving SMT quality. The proposed methods can be classified into two categories according to the paraphrase targets: (1) enrich translation models to cover more bilingual expressions; (2) paraphrase the input sentences to reduce OOVs or generate multiple inputs. In the first category, He et al. (2011) , Bond et al. (2008) and Nakov (2008) enriched the SMT models via paraphrasing the training corpora. Kuhn et al. (2010) and Max (2010) used paraphrases to smooth translation models. For the second category, previous studies mainly focus on finding translations for unknown terms using phrasal paraphrases. Callison-Burch et al. (2006) and Marton et al. (2009) paraphrase unknown terms in the input sentences using phrasal paraphrases extracted from bilingual and monolingual corpora. Mirkin et al. (2009) rewrite OOVs with entailments and paraphrases acquired from WordNet. Onishi et al. (2010) and Du et al. (2010) use phrasal paraphrases to build a word lattice to get multiple input candidates. In the above methods, only word or phrasal paraphrases are used for input sentence rewriting. No structured paraphrases on the sentence level have been investigated. However, the information in the sentence level is very important for disambiguation. For example, we can only substitute play with drama in a context related to stage or theatre. Phrasal paraphrase substitutions can hardly solve such kind of problems.",
                "cite_spans": [
                    {
                        "start": 345,
                        "end": 361,
                        "text": "He et al. (2011)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 364,
                        "end": 382,
                        "text": "Bond et al. (2008)",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 387,
                        "end": 399,
                        "text": "Nakov (2008)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 463,
                        "end": 481,
                        "text": "Kuhn et al. (2010)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 486,
                        "end": 496,
                        "text": "Max (2010)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 668,
                        "end": 696,
                        "text": "Callison-Burch et al. (2006)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 701,
                        "end": 721,
                        "text": "Marton et al. (2009)",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 846,
                        "end": 866,
                        "text": "Mirkin et al. (2009)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 936,
                        "end": 956,
                        "text": "Onishi et al. (2010)",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 961,
                        "end": 977,
                        "text": "Du et al. (2010)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this paper, we propose a method that rewrites the input sentences of the SMT system using automatically extracted paraphrase rules which can capture structures on sentence level in addition to paraphrases on the word or phrase level. Without extra paraphrase resources, a novel approach is proposed to acquire paraphrase rules from the bilingual training corpus based on the results of Forward-Translation and Back-Translation. The rules target on rewriting the input sentences to an MT-favored expression to ensure a better translation. The paraphrase rules cover all kinds of paraphrases on the word, phrase and sentence levels, enabling structure reordering, word or phrase insertion, deletion and substitution. The experimental results show that our proposed approach achieves significant improvements of 1.6~3.6 points of BLEU in the oral domain and 0.5~1 points in the news domain.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The remainder of the paper is organized as follows: Section 2 makes a comparison between the Forward-Translation and Back-Translation. Section 3 introduces our methods that extract paraphrase rules from the bilingual corpus of SMT. Section 4 describes the strategies for constructing word lattice with paraphrase rules. The experimental results and some discussions are presented in Section 5 and Section 6. Section 7 compares our work to the previous researches. Finally, Section 8 concludes the paper and suggests directions for future work.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "vs.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Forward-Translation",
                "sec_num": "2"
            },
            {
                "text": "The Back-Translation method is mainly used for automatic MT evaluation (Rapp 2009 ). This approach is very helpful when no target language reference is available. The only requirement is that the MT system needs to be bidirectional. The procedure includes translating a text into certain foreign language with the MT system (Forward-Translation), and translating it back into the original language with the same system (Back-Translation). Finally the translation quality of Back-Translation is evaluated by using the original source texts as references. Sun et al. (2010) reported an interesting phenomenon: given a bilingual text, the Back-Translation results of the target sentences is better than the Forward-Translation results of the source sentences. Clearly, let (S 0 , T 0 ) be the initial pair of bilingual text. A source-to-target translation system SYS_ST and a target-to-source translation system SYS_TS are trained using the bilingual corpus.",
                "cite_spans": [
                    {
                        "start": 71,
                        "end": 81,
                        "text": "(Rapp 2009",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 554,
                        "end": 571,
                        "text": "Sun et al. (2010)",
                        "ref_id": "BIBREF20"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Back-Translation",
                "sec_num": null
            },
            {
                "text": "\u2022 is a Forward-Translation function, and \u2022 is a function of Back-Translation which can be deduced with two rounds of translations: _ _ . In the first round of translation, S 0 and T 0 are fed into SYS_ST and SYS_TS, and we get T 1 and S 1 as translation results. In the second round, we translate S 1 back into the target side with SYS_ST, and get the translation T 2 . The procedure is illustrated in Figure 1 , which can also formally be described as:",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 409,
                        "end": 410,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Back-Translation",
                "sec_num": null
            },
            {
                "text": "1. T 1 = FT(S 0 ) = SYS_ST(S 0 ). 2. T 2 = BT(T 0 ), which can be decomposed into two steps: S 1 = SYS_TS(T 0 ), T 2 = SYS_ST(S 1 ). Using T 0 as reference, an interesting result is reported in Sun et al. (2010) that T 2 achieves a higher score than T 1 in automatic MT evaluation. This outcome is important because T 2 is translated ",
                "cite_spans": [
                    {
                        "start": 194,
                        "end": 211,
                        "text": "Sun et al. (2010)",
                        "ref_id": "BIBREF20"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Back-Translation",
                "sec_num": null
            },
            {
                "text": "Back-Translation from a machine-generated text S 1 , but T 1 is translated from a human-write text S 0 . Why the machine-generated text results in a better translation than the human-write text? Two possible reasons may explain this phenomenon: (1) in the first round of translation T 0 \uf0e0 S 1 , some target word orders are reserved due to the reordering failure, and these reserved orders lead to a better result in the second round of translation; (2) the text generated by an MT system is more likely to be matched by the reversed but homologous MT system.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Forward-Translation",
                "sec_num": null
            },
            {
                "text": "Note that all the texts of S 0 , S 1 , S 2 , T 0 and T 1 are sentence aligned because the initial parallel corpus (S 0 , T 0 ) is aligned in the sentence level. The aligned sentence pairs in (S 0 , S 1 ) can be considered as paraphrases. Since S 1 has some MT-favored structures which may result in a better translation, an intuitive idea is whether we can learn these structures by comparing S 1 with S 0 . This is the main assumption of this paper. Taking (S 0 , S 1 ) as paraphrase resource, we propose a method that automatically extracts paraphrase rules to capture the MT-favored structures.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Forward-Translation",
                "sec_num": null
            },
            {
                "text": "We define a paraphrase rule as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Definition of Paraphrase Rules",
                "sec_num": "3.1"
            },
            {
                "text": "1. A paraphrase rule consists of two parts, lefthand-side (LHS) and right-hand-side (RHS).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Definition of Paraphrase Rules",
                "sec_num": "3.1"
            },
            {
                "text": "Both of LHS and RHS consist of nonterminals (slot) and terminals (words). 2. LHS must start/end with a terminal. 3. There must be at least one terminal between two non-terminals in LHS. A paraphrase rule in the format of:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Definition of Paraphrase Rules",
                "sec_num": "3.1"
            },
            {
                "text": "LHS \uf0e0 RHS which means the words matched by LHS can be paraphrased to RHS. Taking Chinese as a case study, some examples of paraphrase rules are shown in Table 1 .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 159,
                        "end": 160,
                        "text": "1",
                        "ref_id": "TABREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Definition of Paraphrase Rules",
                "sec_num": "3.1"
            },
            {
                "text": "Following the methods in Section 2, the initial bilingual corpus is (S 0 , T 0 ). We train a source-totarget PBMT system (SYS_ST) and a target-tosource PBMT system (SYS_TS) on the parallel corpus. Then a Forward-Translation is performed on S 0 using SYS_ST, and a Back-Translation is performed on T 0 using SYS_TS and SYS_ST. As mentioned above, the detailed procedure is:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Selecting Paraphrase Sentence Pairs",
                "sec_num": "3.2"
            },
            {
                "text": "T 1 = SYS_ST(S 0 ), S 1 = SYS_TS(T 0 ), T 2 = SYS_ST(S 1 ).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Selecting Paraphrase Sentence Pairs",
                "sec_num": "3.2"
            },
            {
                "text": "Finally we compute BLEU (Papineni et al. 2002) score for every sentence in T 2 and T 1 , using the corresponding sentence in T 0 as reference. If the sentence in T 2 has a higher BLEU score than the aligned sentence in T 1 , the corresponding sentences in S 0 and S 1 are selected as candidate paraphrase sentence pairs, which are used in the following steps of paraphrase extractions.",
                "cite_spans": [
                    {
                        "start": 24,
                        "end": 46,
                        "text": "(Papineni et al. 2002)",
                        "ref_id": "BIBREF17"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Selecting Paraphrase Sentence Pairs",
                "sec_num": "3.2"
            },
            {
                "text": "We can construct word alignment between S 0 and S 1 through T 0 . On the initial corpus of (S 0 , T 0 ), we conduct word alignment with Giza++ (Och and Ney, 2000) in both directions and then apply the grow-diag-final heuristic (Koehn et al., 2005) for symmetrization. Because S 1 is generated by feeding T 0 into the PBMT system SYS_TS, the word alignment between T 0 and S 1 can be acquired from the verbose information of the decoder.",
                "cite_spans": [
                    {
                        "start": 136,
                        "end": 162,
                        "text": "Giza++ (Och and Ney, 2000)",
                        "ref_id": null
                    },
                    {
                        "start": 227,
                        "end": 247,
                        "text": "(Koehn et al., 2005)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word Alignments Filtering",
                "sec_num": "3.3"
            },
            {
                "text": "The word alignments of S 0 and S 1 contain noises which are produced by either wrong alignment of GIZA++ or translation errors of SYS_TS. To ensure the alignment quality, we use some heuristics to filter the alignment between S 0 and S 1 :",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word Alignments Filtering",
                "sec_num": "3.3"
            },
            {
                "text": "1. If two identical words are aligned in S 0 and S 1 , then remove all the other links to the two words. 2 illustrates an example of using the heuristics to filter alignment.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 105,
                        "end": 106,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Word Alignments Filtering",
                "sec_num": "3.3"
            },
            {
                "text": "From the word-aligned sentence pairs, we then extract a set of rules that are consistent with the word alignments. We use the rule extracting methods of Chiang (2005) . Take the sentence pair in Figure 2 as an example, two initial phrase pairs PP 1 = \"\u90a3 \u53ea \u84dd\u8272 \u624b\u63d0\u5305 ||| \u90a3 \u4e2a \u84dd\u8272 \u624b\u63d0\u5305\" and PP 2 = \"\u5bf9 \u90a3 \u53ea \u84dd\u8272 \u624b\u63d0\u5305 \u6709 \u5174\u8da3 ||| \u5f88 \u611f \u5174\u8da3 \u90a3 \u4e2a \u84dd\u8272 \u624b\u63d0\u5305\" are identified, and PP 1 is contained by PP 2 , then we could form the rule:",
                "cite_spans": [
                    {
                        "start": 153,
                        "end": 166,
                        "text": "Chiang (2005)",
                        "ref_id": "BIBREF2"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 202,
                        "end": 203,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Extracting Paraphrase Rules",
                "sec_num": "3.4"
            },
            {
                "text": "\u5bf9 X 1 \u6709 \u5174\u8da3 \uf0e0 \u5f88 \u611f \u5174\u8da3 X 1 to have interest very feel interest",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Extracting Paraphrase Rules",
                "sec_num": "3.4"
            },
            {
                "text": "The extracted paraphrase rules aim to rewrite the input sentences to an MT-favored form which may lead to a better translation. However, it is risky to directly replace the input sentence with a paraphrased sentence, since the errors in automatic paraphrase substitution may jeopardize the translation result seriously. To avoid such damage, for a given input sentence, we first transform all paraphrase rules that match the input sentences to phrasal paraphrases, and then build a word lattice for SMT decoder using the phrasal paraphrases. In this case, the decoder can search for the best result among all the possible paths.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Paraphrasing the Input Sentences",
                "sec_num": "4"
            },
            {
                "text": "The input sentences are first segmented into subsentences by punctuations. Then for each subsentence, the matched paraphrase rules are ranked according to: (1) the number of matched words; (2) the frequency of the paraphrase rule in the training data. Actually, the ranking strategy tends to select paraphrase rules that have more matched words (therefore less ambiguity) and higher frequency (therefore more reliable).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Paraphrasing the Input Sentences",
                "sec_num": "4"
            },
            {
                "text": "Given an input sentence S and a paraphrase rule R <R LHS , R RHS >, if S matches R LHS , then the matched part can be replaced by R RHS . An example for applying the paraphrase rules is illustrated in Figure 3 .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 208,
                        "end": 209,
                        "text": "3",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Applying Paraphrase Rules",
                "sec_num": "4.1"
            },
            {
                "text": "From Figure 3 , we can see that the words of position 1~3 are replaced to \"\u4e58\u5750 10 \u8def \u5df4\u58eb\". Actually, only the words at position 3 and 4 are paraphrased to the word \"\u5df4\u58eb\", other words are left unchanged. Therefore, we can use a triple, <MIN_RP_TEXT, COVER_START, COVER_LEN> (< \u5df4 \u58eb , 3, 1> in this example) to denote the paraphrase rule, which means the minimal text to replace is \"\u5df4\u58eb\", and the paraphrasing starts at position 3 and covers 1 words.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 12,
                        "end": 13,
                        "text": "3",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Applying Paraphrase Rules",
                "sec_num": "4.1"
            },
            {
                "text": "In this manner, all the paraphrase rules matched for a certain sentence can be converted to the format of <MIN_RP_TEXT, COVER_START, COVER_LEN>, which can also be considered as phrasal paraphrases. Then the methods of building phrasal paraphrases into word lattice for SMT inputs can be used in our approaches. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Applying Paraphrase Rules",
                "sec_num": "4.1"
            },
            {
                "text": "The weights of new edges in the lattices are estimated by an empirical method base on ranking positions. Following Du et al. (2010) , supposing that E = {e 1 ,\u2026,e k } are a set of new edges constructed from k paraphrase rules, which are sorted in a descending order. Then the weight for an edge e i is calculated as:",
                "cite_spans": [
                    {
                        "start": 115,
                        "end": 131,
                        "text": "Du et al. (2010)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Weight Estimation",
                "sec_num": "4.3"
            },
            {
                "text": "e 1 1",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Weight Estimation",
                "sec_num": "4.3"
            },
            {
                "text": "where k is a predefined tradeoff parameter between decoding speed and the number of potential paraphrases being considered.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Weight Estimation",
                "sec_num": "4.3"
            },
            {
                "text": "In our experiments, we used Moses (Koehn et al., 2007) as the baseline system which can support lattice decoding. The alignment was obtained using GIZA++ (Och and Ney, 2003) and then we symmetrized the word alignment using the growdiag-final heuristic. Parameters were tuned using Minimum Error Rate Training (Och, 2003) . To comprehensively evaluate the proposed methods in different domains, two groups of experiments were carried out, namely, the oral group (G oral ) and the news group (G news ). The experiments were conducted in both Chinese-English and English-Chinese directions for the oral group, and Chinese-English direction for the news group. The English sentences were all tokenized and lowercased, and the Chinese sentences were segmented into words by Language Technology Platform (LTP)1 . We used SRILM2 for the training of language models (5-gram in all the experiments). The metrics for automatic evaluation were BLEU3 and TER4 (Snover et al., 2005) .",
                "cite_spans": [
                    {
                        "start": 34,
                        "end": 54,
                        "text": "(Koehn et al., 2007)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 309,
                        "end": 320,
                        "text": "(Och, 2003)",
                        "ref_id": null
                    },
                    {
                        "start": 948,
                        "end": 969,
                        "text": "(Snover et al., 2005)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Data",
                "sec_num": "5.1"
            },
            {
                "text": "The detailed statistics of the training data in G oral are showed in Table 2 . For the bilingual corpus, we used the BTEC and PIVOT data of IWSLT 2008, HIT corpus5 and other Chinese LDC (CLDC) 3 .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 75,
                        "end": 76,
                        "text": "2",
                        "ref_id": "TABREF2"
                    },
                    {
                        "start": 193,
                        "end": 194,
                        "text": "3",
                        "ref_id": "TABREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Experimental Data",
                "sec_num": "5.1"
            },
            {
                "text": "In detail, we chose CSTAR03-test and IWSLT06-dev as the development set; and used IWSLT04-test, IWSLT05-test, IWSLT06-dev and IWSLT07-test for testing. For English-Chinese evaluation, we used IWSLT English-Chinese MT evaluation 2005 as the test set. Due to the lacking of development set, we did not tune parameters on English-Chinese side, instead, we just used the default parameters of Moses.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Data",
                "sec_num": "5.1"
            },
            {
                "text": "In the experiments of the news group, we used the Sinorama and FBIS corpora (LDC2005T10 and LDC2003E14) for bilingual corpus. After tokenization and filtering, this bilingual corpus contained 319,694 sentence pairs (7.9M tokens on Chinese side and 9.2M tokens on English side). We trained a 5-gram language model on the English side of the bi-text. The system was tested using the Chinese-English MT evaluation sets of NIST 2004 , NIST 2006 and NIST 2008. For development, we used the Chinese-English MT evaluation sets of NIST 2002 and NIST 2005. Table 4 shows the statistics of test/development sets used in the news group.",
                "cite_spans": [
                    {
                        "start": 419,
                        "end": 428,
                        "text": "NIST 2004",
                        "ref_id": null
                    },
                    {
                        "start": 429,
                        "end": 440,
                        "text": ", NIST 2006",
                        "ref_id": null
                    },
                    {
                        "start": 441,
                        "end": 455,
                        "text": "and NIST 2008.",
                        "ref_id": null
                    },
                    {
                        "start": 523,
                        "end": 536,
                        "text": "NIST 2002 and",
                        "ref_id": null
                    },
                    {
                        "start": 537,
                        "end": 547,
                        "text": "NIST 2005.",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 554,
                        "end": 555,
                        "text": "4",
                        "ref_id": "TABREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Experimental Data",
                "sec_num": "5.1"
            },
            {
                "text": "We extract both Chinese and English rules in G oral , and Chinese paraphrase rules in G news by comparing the results of Forward-Translation and Back-Translation as described in Section 3. During the extraction, some heuristics are used to ensure the quality of paraphrase rules. Take the extraction of Chinese paraphrase rules in G oral as a case study. Suppose (C 0 , E 0 ) are the initial bilingual corpus of G oral . A Chinese-English and an English-Chinese MT system are trained on (C 0 , E 0 ). We perform Back-Translation on E 0 ( ), and Forward-Translation on C 0 ( ). Suppose e 1i and e 2i are two aligned sentences in E 1 and E 2 , c 0i and c 1i are the corresponding sentences in C 0 and C 1 . (c 0i , c 1i ) are selected for the extraction of paraphrase rules if two conditions are satisfied: (1) BLEU(e 2i ) -BLEU(e 1i ) > \u03b8 1 , and (2) BLEU(e 2i ) > \u03b8 2 , where BLEU \u2022 is a function for computing BLEU score; \u03b8 1 and \u03b8 2 are thresholds for balancing the rules number and the quality of paraphrase rules. In our experiment, \u03b8 1 and \u03b8 2 are empirically set to 0.1 and 0.3.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "5.2"
            },
            {
                "text": "As a result, we extract 912,625 Chinese and 1,116,375 English paraphrase rules for G oral , and for G news the number of Chinese paraphrase rules is 2,877,960. Then we use the extracted paraphrase rules to improve SMT by building word lattices for the input sentences.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "5.2"
            },
            {
                "text": "The Chinese-English experimental results of G oral and G news are shown in Table 5 and Table 6 , respectively. It can be seen that our method outperforms the baselines in both oral and news domains.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 81,
                        "end": 82,
                        "text": "5",
                        "ref_id": "TABREF7"
                    },
                    {
                        "start": 87,
                        "end": 94,
                        "text": "Table 6",
                        "ref_id": "TABREF8"
                    }
                ],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "5.2"
            },
            {
                "text": "Our system gains significant improvements of 1.6~3.6 points of BLEU in the oral domain, and 0.5~1 points of BLEU in the news domain. Figure 5 shows the effect of considered paraphrases (k) in the step of building ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 140,
                        "end": 141,
                        "text": "5",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "5.2"
            },
            {
                "text": "We make a detailed analysis on the Chinese-English translation results that are affected by our paraphrase rules. The aim is to investigate what kinds of paraphrases have been captured in the rules. Firstly the input path is recovered from the translation results according to the tracing information of the decoder, and therefore we can examine which path is selected by the SMT decoder from the paraphrase lattice. A human annotator is asked to judge whether the recovered paraphrase sentence keeps the same meaning as the original input. Then the annotator compares the baseline translation with the translations proposed by our approach. The analysis is carried out on the IWSLT 2007 Chinese-English test set, 84 out of 489 input sentences have been affected by paraphrases, and the statistic of human evaluation is shown in Table 8 . It can be seen in Table 8 that the paraphrases achieve a relatively high accuracy, 60 (71.4%) paraphrased sentences retain the same meaning, and the other 24 (28.6%) are incorrect. Among the 60 correct paraphrases, 36 sentences finally result in an improved translation. We further analyze these paraphrases and the translation results to investigate what kinds of transformation finally lead to the translation quality improvement. The paraphrase variations can be classified into four categories:",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 835,
                        "end": 836,
                        "text": "8",
                        "ref_id": "TABREF6"
                    },
                    {
                        "start": 863,
                        "end": 864,
                        "text": "8",
                        "ref_id": "TABREF6"
                    }
                ],
                "eq_spans": [],
                "section": "Discussion",
                "sec_num": "6"
            },
            {
                "text": "(1) Reordering: The original source sentences are reordered to be similar to the order of the target language. (2) Word substitution: A phrase with multiword translations is replaced by a phrase with a single-word translation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discussion",
                "sec_num": "6"
            },
            {
                "text": "(3) Recovering omitted words: Ellipsis occurs frequently in spoken language. Recovering the omitted words often leads to a better translation. (4) Removing redundant words: Mostly, translating redundant words may confuse the SMT system and would be unnecessary.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discussion",
                "sec_num": "6"
            },
            {
                "text": "Removing redundant words can mitigate this problem. 2), ( 3) and ( 4) are shown in Table 9 , respectively. The numbers in the second column indicates the number of the sentences affected by the rules, among the 36 sentences with improved paraphrasing and translation. A sentence can be classified into multiple categories. Except category (2), the other three categories cannot be detected by the previous approaches, which verify our statement that our rules can capture structured paraphrases on the sentence level in addition to the paraphrases on the word or phrase level.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 89,
                        "end": 90,
                        "text": "9",
                        "ref_id": "TABREF10"
                    }
                ],
                "eq_spans": [],
                "section": "Discussion",
                "sec_num": "6"
            },
            {
                "text": "Not all the paraphrased results are correct. Sometimes an ill paraphrased sentence can produce better translations. Take the first line of Table 9 as an example, the paraphrased sentence \"\u591a\u5c11/How many \u9999\u70df/cigarettes \u53ef\u4ee5/can \u514d\u7a0e/duty-free \u5e26 /take \u652f/NULL\" is not a fluent Chinese sentence, however, the rearranged word order is closer to English, which finally results in a much better translation.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 145,
                        "end": 146,
                        "text": "9",
                        "ref_id": "TABREF10"
                    }
                ],
                "eq_spans": [],
                "section": "Discussion",
                "sec_num": "6"
            },
            {
                "text": "Previous studies on improving SMT using paraphrase rules focus on hand-crafted rules. Nakov (2008) employs six rules for paraphrasing the training corpus. Bond et al. (2008) use grammars to paraphrase the source side of training data, covering aspects like word order and minor lexical variations (tenses etc.) but not content words. The paraphrases are added to the source side of the corpus and the corresponding target sentences are duplicated.",
                "cite_spans": [
                    {
                        "start": 86,
                        "end": 98,
                        "text": "Nakov (2008)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 155,
                        "end": 173,
                        "text": "Bond et al. (2008)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "7"
            },
            {
                "text": "A disadvantage for hand-crafted paraphrase rules is that it is language dependent. In contrast, our method that automatically extracted paraphrase rules from bilingual corpus is flexible and suitable for any language pairs. Our work is similar to Sun et al. (2010) . Both tried to capture the MT-favored structures from bilingual corpus. However, a clear difference is that Sun et al. (2010) captures the structures implicitly by training an MT system on (S 0 , S 1 ) and \"translates\" the SMT input to an MT-favored expression. Actually, the rewriting process is considered as a black box in Sun et al. (2010) . In this paper, the MT-favored expressions are captured explicitly by automatically extracted paraphrase rules. The advantages of the paraphrase rules are: (1) Our method can explicitly capture the structure information in the sentence level, enabling global reordering, which is impossible in Sun et al. (2010) . (2) For each rule, we can control its quality automatically or manually.",
                "cite_spans": [
                    {
                        "start": 247,
                        "end": 264,
                        "text": "Sun et al. (2010)",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 374,
                        "end": 391,
                        "text": "Sun et al. (2010)",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 592,
                        "end": 609,
                        "text": "Sun et al. (2010)",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 905,
                        "end": 922,
                        "text": "Sun et al. (2010)",
                        "ref_id": "BIBREF20"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "7"
            },
            {
                "text": "In this paper, we propose a novel method for extracting paraphrase rules by comparing the source side of bilingual corpus to the target-tosource translation of the target side. The acquired paraphrase rules are employed to enrich the SMT inputs, which target on rewriting the input sentences to an MT-favored form. The paraphrase rules cover all kinds of paraphrases on the word, phrase and sentence levels, enabling structure reordering, word or phrase insertion, deletion and substitution. Experimental results show that the paraphrase rules can improve SMT quality in both the oral and news domains. The manual investigation on oral translation results indicate that the paraphrase rules capture four kinds of MTfavored transformation to ensure translation quality improvement. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "8"
            },
            {
                "text": "http://ir.hit.edu.cn/ltp/",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "http://www.speech.sri.com/projects/srilm/",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13a.pl",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "http://www.umiacs.umd.edu/~snover/terp/",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "The HIT corpus contains the CLDC Olympic corpus(2004- 863-008) and the other HIT corpora available at http://mitlab.hit.edu.cn/index.php/resources/29-theresource/111-share-bilingual-corpus.html.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "This work was supported by National Natural Science Foundation of China (NSFC) (61073126, 61133012), 863 High Technology Program (2011AA01A207).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgement",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Improving Statistical Machine Translation by Paraphrasing the Training Data",
                "authors": [
                    {
                        "first": "Francis",
                        "middle": [],
                        "last": "Bond",
                        "suffix": ""
                    },
                    {
                        "first": "Eric",
                        "middle": [],
                        "last": "Nichols",
                        "suffix": ""
                    },
                    {
                        "first": "Darren",
                        "middle": [],
                        "last": "Scott Appling",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Paul",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Proceedings of the IWSLT",
                "volume": "",
                "issue": "",
                "pages": "150--157",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Francis Bond, Eric Nichols, Darren Scott Appling, and Michael Paul. 2008. Improving Statistical Machine Translation by Paraphrasing the Training Data. In Proceedings of the IWSLT, pages 150-157.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Improved Statistical Machine Translation Using Paraphrases",
                "authors": [
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Callison-Burch",
                        "suffix": ""
                    },
                    {
                        "first": "Philipp",
                        "middle": [],
                        "last": "Koehn",
                        "suffix": ""
                    },
                    {
                        "first": "Miles",
                        "middle": [],
                        "last": "Osborne",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Proceedings of NAACL",
                "volume": "",
                "issue": "",
                "pages": "17--24",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chris Callison-Burch, Philipp Koehn, and Miles Osborne. 2006. Improved Statistical Machine Translation Using Paraphrases. In Proceedings of NAACL, pages 17-24.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "A hierarchical phrase-based model for statistical machine translation",
                "authors": [
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Chiang",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of ACL",
                "volume": "",
                "issue": "",
                "pages": "263--270",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of ACL, pages 263-270.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Facilitating Translation Using Source Language Paraphrase Lattices",
                "authors": [
                    {
                        "first": "Jinhua",
                        "middle": [],
                        "last": "Du",
                        "suffix": ""
                    },
                    {
                        "first": "Jie",
                        "middle": [],
                        "last": "Jiang",
                        "suffix": ""
                    },
                    {
                        "first": "Andy",
                        "middle": [],
                        "last": "Way",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of EMNLP",
                "volume": "",
                "issue": "",
                "pages": "420--429",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jinhua Du, Jie Jiang, Andy Way. 2010. Facilitating Translation Using Source Language Paraphrase Lattices. In Proceedings of EMNLP, pages 420-429.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Enriching SMT Training Data via Paraphrasing",
                "authors": [
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Shiqi",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Haifeng",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Ting",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Proceedings of IJCNLP",
                "volume": "",
                "issue": "",
                "pages": "803--810",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Wei He, Shiqi Zhao, Haifeng Wang and Ting Liu. 2011. Enriching SMT Training Data via Paraphrasing. In Proceedings of IJCNLP, pages 803-810.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Statistical Phrase-Based Translation",
                "authors": [
                    {
                        "first": "Philipp",
                        "middle": [],
                        "last": "Koehn",
                        "suffix": ""
                    },
                    {
                        "first": "Franz",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Josef",
                        "middle": [],
                        "last": "Och",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Marcu",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Proceedings of HLT/NAACL",
                "volume": "",
                "issue": "",
                "pages": "48--54",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical Phrase-Based Translation. In Proceedings of HLT/NAACL, pages 48-54",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Statistical significance tests for machine translation evaluation",
                "authors": [
                    {
                        "first": "Philipp",
                        "middle": [],
                        "last": "Koehn",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proceedings of EMNLP",
                "volume": "",
                "issue": "",
                "pages": "388--395",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of EMNLP, pages 388-395.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Edinburgh System Description for the 2005 IWSLT Speech Translation Evaluation",
                "authors": [
                    {
                        "first": "Philipp",
                        "middle": [],
                        "last": "Koehn",
                        "suffix": ""
                    },
                    {
                        "first": "Amittai",
                        "middle": [],
                        "last": "Axelrod",
                        "suffix": ""
                    },
                    {
                        "first": "Alexandra",
                        "middle": [
                            "Birch"
                        ],
                        "last": "Mayne",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Callison-Burch",
                        "suffix": ""
                    },
                    {
                        "first": "Miles",
                        "middle": [],
                        "last": "Osborne",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Talbot",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of IWSLT",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, Chris Callison-Burch, Miles Osborne, and David Talbot. 2005. Edinburgh System Description for the 2005 IWSLT Speech Translation Evaluation. In Proceedings of IWSLT.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Moses: Open source toolkit for statistical machine translation",
                "authors": [
                    {
                        "first": "Philipp",
                        "middle": [],
                        "last": "Koehn",
                        "suffix": ""
                    },
                    {
                        "first": "Hieu",
                        "middle": [],
                        "last": "Hoang",
                        "suffix": ""
                    },
                    {
                        "first": "Alexandra",
                        "middle": [],
                        "last": "Birch",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Callison-Burch",
                        "suffix": ""
                    },
                    {
                        "first": "Marcello",
                        "middle": [],
                        "last": "Federico",
                        "suffix": ""
                    },
                    {
                        "first": "Nicola",
                        "middle": [],
                        "last": "Bertoldi",
                        "suffix": ""
                    },
                    {
                        "first": "Brooke",
                        "middle": [],
                        "last": "Cowan",
                        "suffix": ""
                    },
                    {
                        "first": "Wade",
                        "middle": [],
                        "last": "Shen",
                        "suffix": ""
                    },
                    {
                        "first": "Christine",
                        "middle": [],
                        "last": "Moran",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Zens",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Dyer",
                        "suffix": ""
                    },
                    {
                        "first": "Ondrej",
                        "middle": [],
                        "last": "Bojar",
                        "suffix": ""
                    },
                    {
                        "first": "Alexandra",
                        "middle": [],
                        "last": "Constantin",
                        "suffix": ""
                    },
                    {
                        "first": "Evan",
                        "middle": [],
                        "last": "Herbst",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Proceedings of the ACL Demo and Poster Sessions",
                "volume": "",
                "issue": "",
                "pages": "177--180",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the ACL Demo and Poster Sessions, pages 177-180.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Phrase Clustering for Smoothing TM Probabilities-or, How to Extract Paraphrases from Phrase Tables",
                "authors": [
                    {
                        "first": "Roland",
                        "middle": [],
                        "last": "Kuhn",
                        "suffix": ""
                    },
                    {
                        "first": "Boxing",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "George",
                        "middle": [],
                        "last": "Foster",
                        "suffix": ""
                    },
                    {
                        "first": "Evan",
                        "middle": [],
                        "last": "Stratford",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of COLING",
                "volume": "",
                "issue": "",
                "pages": "608--616",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Roland Kuhn, Boxing Chen, George Foster and Evan Stratford. 2010. Phrase Clustering for Smoothing TM Probabilities-or, How to Extract Paraphrases from Phrase Tables. In Proceedings of COLING, pages 608-616.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Improved Statistical Machine Translation Using Monolingually-Dervied Paraphrases",
                "authors": [
                    {
                        "first": "Yuval",
                        "middle": [],
                        "last": "Marton",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Callison-Burch",
                        "suffix": ""
                    },
                    {
                        "first": "Philip",
                        "middle": [],
                        "last": "Resnik",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of EMNLP",
                "volume": "",
                "issue": "",
                "pages": "381--390",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yuval Marton, Chris Callison-Burch, and Philip Resnik. 2009. Improved Statistical Machine Translation Using Monolingually-Dervied Paraphrases. In Proceedings of EMNLP, pages 381-390.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Example-Based Paraphrasing for Improved Phrase-Based Statistical Machine TranslationIn Proceedings of EMNLP",
                "authors": [
                    {
                        "first": "Aur\u00e9lien",
                        "middle": [],
                        "last": "Max",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "656--666",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Aur\u00e9lien Max. 2010. Example-Based Paraphrasing for Improved Phrase-Based Statistical Machine TranslationIn Proceedings of EMNLP, pages 656- 666.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Source-Language Entailment Modeling for Translation Unknown Terms",
                "authors": [
                    {
                        "first": "Lucia",
                        "middle": [],
                        "last": "Shachar Mirkin",
                        "suffix": ""
                    },
                    {
                        "first": "Nicola",
                        "middle": [],
                        "last": "Specia",
                        "suffix": ""
                    },
                    {
                        "first": "Ido",
                        "middle": [],
                        "last": "Cancedda",
                        "suffix": ""
                    },
                    {
                        "first": "Marc",
                        "middle": [],
                        "last": "Dagan",
                        "suffix": ""
                    },
                    {
                        "first": "Idan",
                        "middle": [],
                        "last": "Dymetman",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Szpektor",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of ACL",
                "volume": "",
                "issue": "",
                "pages": "791--799",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Shachar Mirkin, Lucia Specia, Nicola Cancedda, Ido Dagan, Marc Dymetman, Idan Szpektor. 2009. Source-Language Entailment Modeling for Translation Unknown Terms. In Proceedings of ACL, pages 791-799.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Improved Statistical Machine Translation Using Monolingual Paraphrases",
                "authors": [
                    {
                        "first": "Preslav",
                        "middle": [],
                        "last": "Nakov",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Proceedings of ECAI",
                "volume": "",
                "issue": "",
                "pages": "338--342",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Preslav Nakov. 2008. Improved Statistical Machine Translation Using Monolingual Paraphrases. In Proceedings of ECAI, pages 338-342.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Improved Statistical Alignment Models",
                "authors": [
                    {
                        "first": "Josef",
                        "middle": [],
                        "last": "Franz",
                        "suffix": ""
                    },
                    {
                        "first": "Hermann",
                        "middle": [],
                        "last": "Och",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Ney",
                        "suffix": ""
                    }
                ],
                "year": 2000,
                "venue": "Proceedings of ACL",
                "volume": "",
                "issue": "",
                "pages": "440--447",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Franz Josef Och and Hermann Ney. 2000. Improved Statistical Alignment Models. In Proceedings of ACL, pages 440-447.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Minimum Error Rate Training in Statistical Machine Translation",
                "authors": [
                    {
                        "first": "Josef",
                        "middle": [],
                        "last": "Franz",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Och",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Proceedings of ACL",
                "volume": "",
                "issue": "",
                "pages": "160--167",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Franz Josef Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Proceedings of ACL, pages 160-167.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Paraphrase Lattice for Statistical Machine Translation",
                "authors": [
                    {
                        "first": "Takashi",
                        "middle": [],
                        "last": "Onishi",
                        "suffix": ""
                    },
                    {
                        "first": "Masao",
                        "middle": [],
                        "last": "Utiyama",
                        "suffix": ""
                    },
                    {
                        "first": "Eiichiro",
                        "middle": [],
                        "last": "Sumita",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of ACL",
                "volume": "",
                "issue": "",
                "pages": "1--5",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Takashi Onishi, Masao Utiyama, Eiichiro Sumita. 2010. Paraphrase Lattice for Statistical Machine Translation. In Proceedings of ACL, pages 1-5.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "BLEU: a Method for Automatic Evaluation of Machine Translation",
                "authors": [
                    {
                        "first": "Kishore",
                        "middle": [],
                        "last": "Papineni",
                        "suffix": ""
                    },
                    {
                        "first": "Salim",
                        "middle": [],
                        "last": "Roukos",
                        "suffix": ""
                    },
                    {
                        "first": "Todd",
                        "middle": [],
                        "last": "Ward",
                        "suffix": ""
                    },
                    {
                        "first": "Wei-Jing",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Proceedings of ACL",
                "volume": "",
                "issue": "",
                "pages": "311--318",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. In Proceedings of ACL, pages 311-318.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "The Back-translation Score: Automatic MT Evaluation at the Sentence Level without Reference Translations",
                "authors": [
                    {
                        "first": "Reinhard",
                        "middle": [],
                        "last": "Rapp",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of ACL-IJCNLP",
                "volume": "",
                "issue": "",
                "pages": "133--136",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Reinhard Rapp. 2009. The Back-translation Score: Automatic MT Evaluation at the Sentence Level without Reference Translations. In Proceedings of ACL-IJCNLP, pages 133-136.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "A study of translation error rate with targeted human annotation",
                "authors": [
                    {
                        "first": "Matthew",
                        "middle": [],
                        "last": "Snover",
                        "suffix": ""
                    },
                    {
                        "first": "Bonnie",
                        "middle": [
                            "J"
                        ],
                        "last": "Dorr",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Schwartz",
                        "suffix": ""
                    },
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Makhoul",
                        "suffix": ""
                    },
                    {
                        "first": "Linnea",
                        "middle": [],
                        "last": "Micciulla",
                        "suffix": ""
                    },
                    {
                        "first": "Ralph",
                        "middle": [],
                        "last": "Weischedel",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Matthew Snover, Bonnie J. Dorr, Richard Schwartz, John Makhoul, Linnea Micciulla, and Ralph Weischedel. 2005. A study of translation error rate with targeted human annotation. Technical Report LAMP-TR-126, CS-TR-4755, UMIACS-TR-2005- 58, University of Maryland, July, 2005.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "A Novel Statistical Pre-Processing Model for Rule-Based Machine Translation System",
                "authors": [
                    {
                        "first": "Yanli",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "O'",
                        "middle": [],
                        "last": "Sharon",
                        "suffix": ""
                    },
                    {
                        "first": "Minako",
                        "middle": [],
                        "last": "Brien",
                        "suffix": ""
                    },
                    {
                        "first": "O'",
                        "middle": [],
                        "last": "Hagan",
                        "suffix": ""
                    },
                    {
                        "first": "Fred",
                        "middle": [],
                        "last": "Hollowood",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of EAMT",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yanli Sun, Sharon O'Brien, Minako O'Hagan and Fred Hollowood. 2010. A Novel Statistical Pre-Processing Model for Rule-Based Machine Translation System. In Proceedings of EAMT, 8pp.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 1: Procedure of Forward-Translation and Back-Translation.",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Figure 3: Example for Applying Paraphrase Rules",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "Figure 4: An example of lattice-based paraphrases for an input sentence.",
                "uris": null,
                "fig_num": "4",
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table><tr><td>No.</td><td>LHS</td><td>RHS</td></tr><tr><td>1</td><td>\u4e58\u5750/ride X 1 \u516c\u5171\u6c7d\u8f66/bus</td><td>\u4e58\u5750/ride X 1 \u5df4\u58eb/bus</td></tr><tr><td>2</td><td>\u5728/at X 1 \u5904/location \u5411\u5de6\u62d0/turn left</td><td>\u5411\u5de6\u62d0/turn left \u5728/at X 1 \u5904/location</td></tr><tr><td>3</td><td>\u628a/NULL X 1 \u7ed9/give \u6211/me</td><td>\u7ed9/give \u6211/me X 1</td></tr><tr><td>4</td><td>\u4ece/from X 1 \u5230/to X 2 \u8981/need \u591a\u957f/how long \u65f6\u95f4/time</td><td>\u8981/need \u82b1/spend \u591a\u957f/how long \u65f6\u95f4/time \u4ece/from X 1 \u5230/to X 2</td></tr></table>",
                "type_str": "table",
                "text": "Examples of Chinese Paraphrase rules, together with English translations for every word 2. Stop words (including some function words and punctuations) can only be aligned to either stop words or null. Figure",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table><tr><td/><td>Corpus</td><td colspan=\"2\">#Sen. #Ref.</td></tr><tr><td>develop</td><td>CSTAR03 test set IWSLT06 dev set</td><td>506 489</td><td>16 7</td></tr><tr><td/><td>IWSLT04 test set</td><td>500</td><td>16</td></tr><tr><td>test</td><td>IWSLT05 test set IWSLT06 test set</td><td>506 500</td><td>16 7</td></tr><tr><td/><td>IWSLT07 test set</td><td>489</td><td>6</td></tr></table>",
                "type_str": "table",
                "text": "Statistics of training data in G oral",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table><tr><td/><td>Corpus</td><td>#Sen.</td><td>#Ref.</td></tr><tr><td>develop</td><td>NIST 2002 NIST 2005</td><td>878 1,082</td><td>10 4</td></tr><tr><td/><td>NIST 2004</td><td>1,788</td><td>5</td></tr><tr><td>test</td><td>NIST 2006</td><td>1,664</td><td>4</td></tr><tr><td/><td>NIST 2008</td><td>1,357</td><td>4</td></tr></table>",
                "type_str": "table",
                "text": "Statistics of test/develop sets in G oral",
                "html": null,
                "num": null
            },
            "TABREF4": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Statistics of test/develop sets in G news word lattices. The result of English-Chinese experiments in G oral is shown in Table7.",
                "html": null,
                "num": null
            },
            "TABREF5": {
                "content": "<table><tr><td/><td>Model</td><td colspan=\"7\">BLEU iwslt 04 iwslt 05 iwslt 06 iwslt 07 iwslt 04 iwslt 05 iwslt 06 iwslt 07 TER</td></tr><tr><td colspan=\"2\">baseline</td><td colspan=\"7\">0.5353 0.5887 0.2765 0.3977 0.3279 0.2874 0.5559 0.4390</td></tr><tr><td colspan=\"3\">para. improved 0Model</td><td>nist 04</td><td/><td>BLEU nist 06</td><td colspan=\"2\">nist 08</td><td>nist 04</td><td>TER nist 06</td><td>nist 08</td></tr><tr><td/><td>baseline</td><td/><td>0.2795</td><td/><td>0.2389</td><td colspan=\"2\">0.1933</td><td>0.6554</td><td>0.6515</td><td>0.6652</td></tr><tr><td colspan=\"2\">para. improved</td><td/><td>0.2891</td><td/><td>0.2485</td><td colspan=\"2\">0.1978</td><td>0.6451</td><td>0.6407</td><td>0.6582</td></tr><tr><td colspan=\"2\">model baseline para. improved</td><td/><td colspan=\"3\">IWSLT 2005 BLEU TER 0.4644 0.4164 0.4853 0.3883</td><td>BLEU score (%)</td><td>44.4 44.6 44.8 45.0 45.2 45.4</td></tr><tr><td>trans. para.</td><td colspan=\"5\">improve comparable worsen total</td><td/><td>44.2</td><td>0</td><td>1 0</td><td>2 0</td><td>3 0</td><td>4 0</td></tr><tr><td>correct incorrect</td><td>36 1</td><td colspan=\"2\">20 9</td><td>4 14</td><td>60 24</td><td/><td colspan=\"2\">Considered paraprhases (k) Figure 5: Effect of considered paraphrases (k)</td></tr><tr><td/><td/><td/><td/><td/><td/><td/><td/><td>on BLEU score</td></tr></table>",
                "type_str": "table",
                "text": "",
                "html": null,
                "num": null
            },
            "TABREF6": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Human analysis of the paraphrasing results in IWSLT 2007 CE translation",
                "html": null,
                "num": null
            },
            "TABREF7": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Experimental results of G oral in Chinese-English direction",
                "html": null,
                "num": null
            },
            "TABREF8": {
                "content": "<table><tr><td>Table 7: Experimental results of G oral in English-Chinese direction</td></tr></table>",
                "type_str": "table",
                "text": "Experimental results of G news in Chinese-English direction",
                "html": null,
                "num": null
            },
            "TABREF9": {
                "content": "<table><tr><td colspan=\"2\">Cate. Num</td><td>Original Sentence/Translation</td><td>Paraphrased Sentence/Translation</td></tr><tr><td>(1)</td><td>11</td><td/><td/></tr><tr><td/><td/><td>\u4f60/you \u6709/have \u591a\u4e45/how long \u7684/N/A</td><td>\u4f60/you \u6709/have \u591a\u5c11/how much \u6559\u5b66/teaching</td></tr><tr><td>(2)</td><td>18</td><td>\u6559\u5b66/teaching \u7ecf\u9a8c/experience \uff1f</td><td>\u7ecf\u9a8c/experience \uff1f</td></tr><tr><td/><td/><td>you have how long teaching experience ?</td><td>how much teaching experience you have ?</td></tr><tr><td>(3)</td><td>10</td><td>\u9700\u8981/need \u62bc\u91d1/deposit \u5417/N/A ? you need a deposit ?</td><td>\u4f60/you \u9700\u8981/need \u62bc\u91d1/deposit \u5417/N/A ? do you need a deposit ?</td></tr><tr><td/><td/><td>\u6212\u6307/ring \u6389/fall \u8fdb/into \u6d17\u8138\u6c60/washbasin</td><td>\u6212\u6307/ring \u6389/fall \u8fdb/into \u6d17\u8138\u6c60/washbasin \u4e86</td></tr><tr><td>(4)</td><td>4</td><td>\u91cc/in \u4e86/N/A \u3002</td><td>/N/A \u3002</td></tr><tr><td/><td/><td>ring off into the washbasin is in .</td><td>ring off into the washbasin .</td></tr></table>",
                "type_str": "table",
                "text": "\u9999\u70df/cigarette \u53ef\u4ee5/can \u514d\u7a0e/duty-free \u5e26 /take \u591a\u5c11/how much \u652f/N/A ? \u591a\u5c11/how much \u9999\u70df/cigarettes \u53ef\u4ee5/can \u514d\u7a0e /duty-free \u5e26/take \u652f/N/A ? what a cigarette can i take duty-free ? how many cigarettes can i take duty-free one ?",
                "html": null,
                "num": null
            },
            "TABREF10": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Examples for classification of paraphrase rules",
                "html": null,
                "num": null
            }
        }
    }
}