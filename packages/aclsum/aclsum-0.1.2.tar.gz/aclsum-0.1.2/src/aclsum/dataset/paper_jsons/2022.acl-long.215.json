{
    "paper_id": "2022",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T11:59:51.102320Z"
    },
    "title": "Cross-Modal Discrete Representation Learning",
    "authors": [
        {
            "first": "Alexander",
            "middle": [
                "H Liu"
            ],
            "last": "Souyoung",
            "suffix": "",
            "affiliation": {},
            "email": "souyoung@mit.edu"
        },
        {
            "first": "Jin",
            "middle": [],
            "last": "Cheng-I",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Jeff",
            "middle": [],
            "last": "Lai",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Andrew",
            "middle": [],
            "last": "Rouditchenko",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Aude",
            "middle": [],
            "last": "Oliva",
            "suffix": "",
            "affiliation": {},
            "email": "oliva@mit.edu"
        },
        {
            "first": "James",
            "middle": [],
            "last": "Glass",
            "suffix": "",
            "affiliation": {},
            "email": "glass@mit.edu"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "In contrast to recent advances focusing on highlevel representation learning across modalities, in this work we present a self-supervised learning framework that is able to learn a representation that captures finer levels of granularity across different modalities such as concepts or events represented by visual objects or spoken words. Our framework relies on a discretized embedding space created via vector quantization that is shared across different modalities. Beyond the shared embedding space, we propose a Cross-Modal Code Matching objective that forces the representations from different views (modalities) to have a similar distribution over the discrete embedding space such that cross-modal objects/actions localization can be performed without direct supervision. We show that the proposed discretized multi-modal finegrained representation (e.g., pixel/word/frame) can complement high-level summary representations (e.g., video/sentence/waveform) for improved performance on cross-modal retrieval tasks. We also observe that the discretized representation uses individual clusters to represent the same semantic concept across modalities.",
    "pdf_parse": {
        "paper_id": "2022",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "In contrast to recent advances focusing on highlevel representation learning across modalities, in this work we present a self-supervised learning framework that is able to learn a representation that captures finer levels of granularity across different modalities such as concepts or events represented by visual objects or spoken words. Our framework relies on a discretized embedding space created via vector quantization that is shared across different modalities. Beyond the shared embedding space, we propose a Cross-Modal Code Matching objective that forces the representations from different views (modalities) to have a similar distribution over the discrete embedding space such that cross-modal objects/actions localization can be performed without direct supervision. We show that the proposed discretized multi-modal finegrained representation (e.g., pixel/word/frame) can complement high-level summary representations (e.g., video/sentence/waveform) for improved performance on cross-modal retrieval tasks. We also observe that the discretized representation uses individual clusters to represent the same semantic concept across modalities.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Toddlers acquire much of their knowledge through grounded learning -visual concepts can be acquired through language, and language acquisition emerges through visual interaction. Inspired by this type of grounded learning, a rich body of representation learning research (Harwath et al., 2018; Miech et al., 2020; Alayrac et al., 2020; Monfort et al., 2021; Luo et al., 2021) has been exploring the potential to learn from multi-modal data such as video-text, video-audio, and image-audio pairs. These works typically focus on learning a joint embedding space between different modalities, in which high-level summary representations are extracted as embedding vectors. These embedding vectors often represent entire video clips, spoken utterances, or sentences as single vectors, and can be useful on tasks such as cross-modal data retrieval, e.g., finding the most similar visual scene according to a spoken language description. The predominant approach to learning these embedding vectors is to use modality-independent encoders, and while this has been successful for downstream retrieval tasks, it makes it difficult to compare the activations of the encoders from different modalities. Further, the space of continuous embedding vectors is unbounded, which makes interpreting the learned representations challenging.",
                "cite_spans": [
                    {
                        "start": 271,
                        "end": 293,
                        "text": "(Harwath et al., 2018;",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 294,
                        "end": 313,
                        "text": "Miech et al., 2020;",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 314,
                        "end": 335,
                        "text": "Alayrac et al., 2020;",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 336,
                        "end": 357,
                        "text": "Monfort et al., 2021;",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 358,
                        "end": 375,
                        "text": "Luo et al., 2021)",
                        "ref_id": "BIBREF25"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "To this end, we propose to jointly learn highlevel embedding vector representations with a finegrained discrete embedding space that is shared across different modalities. The discrete embedding space enables model interpretability since there are a finite number of embedding vectors which are shared across modalities. Besides the shared embedding space, we propose a Cross-Modal Code Matching (CMCM) objective that guides the embedding space to capture cross-modal correspondences of concepts, actions, and words. This not only improves downstream performance on retrieval, but also allows us to better interpret what the model recognized through cross-modal grounded learning.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "To verify the effectiveness of our proposed learning framework, we conducted experiments in several cross-modal domains, including video-text, video-audio, and image-audio. We found consistent improvements over baseline models, verifying that the gain was not restricted to the particular choice of network architecture, input modalities, or dataset. We also demonstrate the interpretability of the fine-grained discrete representations by showing the cross-modal relations between the embedding vectors and semantic concepts appearing in the input modalities. Our approach also enables cross-modal concept localization without requiring any labels during training.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Figure 1 : An overview of the proposed framework. The proposed shared discrete embedding space (green region, described in Section 2.2) is based on a cross-modal representation learning paradigm (blue/yellow regions, described in Section 2.1). The proposed Cross-Modal Code Matching L CMCM objective is detailed in Section 2.3 and Figure 2 .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "1",
                        "ref_id": null
                    },
                    {
                        "start": 338,
                        "end": 339,
                        "text": "2",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Figure 1 provides an overview of the proposed framework. We begin by describing the two-branch cross-modal representation learning paradigm in Section 2.1 (the blue and yellow regions). Next, we introduce our shared discrete embedding space in Section 2.2 (the green region). Finally, in Section 2.3 and Figure 2 , we introduce the Cross-Modal Code Matching objective which guides the model to learn semantically meaningful representations through the shared discrete embedding space.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "1",
                        "ref_id": null
                    },
                    {
                        "start": 311,
                        "end": 312,
                        "text": "2",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Methodology",
                "sec_num": "2"
            },
            {
                "text": "Given a set of data X = {(x A i , x B i )} N i=1 of size N where each instance x i is instantiated in different modalities A and B (e.g. video and its corresponding caption), the goal is to derive high-level representative vectors (z A i , z B i ) for each instance (x A i , x B i ) that capture the cross-modal relation measured by a choice of similarity function S(\u2022, \u2022).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Cross-Modal Learning Paradigm",
                "sec_num": "2.1"
            },
            {
                "text": "For a specific modality M \u2208 {A, B}, a common first step is to encode raw data x M i into a sequence of \"fine-grained\" latent features H M i with a modality-specific neural network f M fine , i.e.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Cross-Modal Learning Paradigm",
                "sec_num": "2.1"
            },
            {
                "text": "H M i = f M fine (x M i ).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Cross-Modal Learning Paradigm",
                "sec_num": "2.1"
            },
            {
                "text": "The fine-grained representations H M i can express different kinds of raw data, such as video, audio, or sentences, as a sequence of vectors {h M i,1 , ..., h M i,L } of length L. In the second step, a \"high-level\" representation z M i can be derived by summarizing the fine-grained latent features H M i with another encoding function f M high that reduces the sequence into a single vector, i.e. z M i = f M high (H M i ). For example, with modality A being video, raw data x A i can be treated as a sequence along time and space and encoded into fine-grained represen-tations H A i = {h A i,l } L l=1 by choosing f A fine to be a Residual Network (He et al., 2016) . For the second step, a natural choice for f A high to derive the highlevel representation z A i would be a mean pooling function over the time and spatial axes (arranged along l).",
                "cite_spans": [
                    {
                        "start": 650,
                        "end": 667,
                        "text": "(He et al., 2016)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Cross-Modal Learning Paradigm",
                "sec_num": "2.1"
            },
            {
                "text": "With the sets of high-level representations {z A i } N i=1 and {z B j } N j=1 from different modalities, we can measure the cross-modal relation between any pair of representations (z A i , z B j ) with some similarity function1 S(\u2022, \u2022). The final step in this paradigm is to adopt an objective function that maximizes the similarity score between \"positive\" pairs (where i = j, and thus the true pairs) and minimizes the similarity score between \"negative\" pairs (where i \u0338 = j, and thus imposter pairs).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Cross-Modal Learning Paradigm",
                "sec_num": "2.1"
            },
            {
                "text": "While different objective functions, such as Semi-Hard Negative Mining (Schroff et al., 2015) (SHN) and Noise Constrastive Estimation (Gutmann and Hyv\u00e4rinen, 2010) (NCE), have been studied in prior work, we focused on the Masked Margin Softmax (Ilharco et al., 2019) ",
                "cite_spans": [
                    {
                        "start": 71,
                        "end": 93,
                        "text": "(Schroff et al., 2015)",
                        "ref_id": "BIBREF36"
                    },
                    {
                        "start": 244,
                        "end": 266,
                        "text": "(Ilharco et al., 2019)",
                        "ref_id": "BIBREF17"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Cross-Modal Learning Paradigm",
                "sec_num": "2.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "(MMS) loss LMMS = - 1 N N i=1 log e S(z A i ,z B i )-M e S(z A i ,z B i )-M + N j=1 I i\u0338 =j e S(z A i ,z B j ) ,",
                        "eq_num": "(1)"
                    }
                ],
                "section": "Cross-Modal Learning Paradigm",
                "sec_num": "2.1"
            },
            {
                "text": "where the margin M is a hyperparameter to encourage a higher similarity for positive pairs. The MMS loss L MMS can be seen as an application of the InfoNCE (Oord et al., 2018) loss with a margin. The effectiveness of the described cross-modal learning paradigm has been shown by recent works that achieved state-of-the-art results on benchmark datasets in different cross-modal scenarios such as video-text (Luo et al., 2021) , video-audio (Monfort et al., 2021; Rouditchenko et al., 2020) , and imagetext (Radford et al., 2021) .",
                "cite_spans": [
                    {
                        "start": 148,
                        "end": 175,
                        "text": "InfoNCE (Oord et al., 2018)",
                        "ref_id": null
                    },
                    {
                        "start": 407,
                        "end": 425,
                        "text": "(Luo et al., 2021)",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 440,
                        "end": 462,
                        "text": "(Monfort et al., 2021;",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 463,
                        "end": 489,
                        "text": "Rouditchenko et al., 2020)",
                        "ref_id": "BIBREF35"
                    },
                    {
                        "start": 506,
                        "end": 528,
                        "text": "(Radford et al., 2021)",
                        "ref_id": "BIBREF33"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Cross-Modal Learning Paradigm",
                "sec_num": "2.1"
            },
            {
                "text": "While the high-level representations (z A i , z B i ) given by the cross-modal learning paradigm benefit end tasks such as data retrieval, the representations cannot be easily interpreted by humans. To obtain fine-grained representations that are more interpretable, we introduce a Vector Quantization (Oord et al., 2017) (VQ) mechanism after obtaining the H M i representations. Formally, with an auxiliary embedding table E = {e 1 , e 2 , ..., e V } of size V , which we refer to as the codebook, vector quantization is performed on each fine-grained representation",
                "cite_spans": [
                    {
                        "start": 302,
                        "end": 321,
                        "text": "(Oord et al., 2017)",
                        "ref_id": "BIBREF31"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Shared Discrete Embedding Space",
                "sec_num": "2.2"
            },
            {
                "text": "h M i,l \u2208 H M i of modality M \u2208 {A, B} with hM i,l = f M (h M i,l ) + sg(e v -f M (h M i,l ))",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Shared Discrete Embedding Space",
                "sec_num": "2.2"
            },
            {
                "text": ", where f M is a modality specific projection network to project the input to the shared embedding space, v = arg min k\u2208V \u2225h M i,l -e k \u2225 2 , and sg(\u2022) is the stopgradient operator proposed in straight-through gradient estimation (Bengio et al., 2013) that treats the input as constant during backpropagation. In other words, each vector h M i,l will be replaced by its nearest neighbor e v , which we refer to as the codeword, in the codebook E. The codebook is randomly initialized and updated with the exponential moving average (Oord et al., 2017) given the fine-grained representations (more details in Section A of the Appendix).",
                "cite_spans": [
                    {
                        "start": 230,
                        "end": 251,
                        "text": "(Bengio et al., 2013)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 532,
                        "end": 551,
                        "text": "(Oord et al., 2017)",
                        "ref_id": "BIBREF31"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Shared Discrete Embedding Space",
                "sec_num": "2.2"
            },
            {
                "text": "We trained the shared embedding space jointly with the rest of the framework by modifying the high-level representations z M i to include the discretized fine-grained representations as",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Shared Discrete Embedding Space",
                "sec_num": "2.2"
            },
            {
                "text": "z M i = f M high (H M i ) + f M code ( HM i )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Shared Discrete Embedding Space",
                "sec_num": "2.2"
            },
            {
                "text": ", where f M code is, similar to f M high , the encoding function for summarizing the sequence of quantized fine-grained representations (e.g., an average pooling function over l). Having such a discrete embedding space allows humans to better interpret the learned embeddings since they are shared across modalities and there are a finite number of them.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Shared Discrete Embedding Space",
                "sec_num": "2.2"
            },
            {
                "text": "Ideally, the codebook should be shared across different modalities since the quantization method is independent to the input modality. However, as we demonstrate in Section F of the Appendix, the model will learn to partition the codebook into modality-specific subspaces due to the significant difference between fine-grained representations from different modalities. To learn a shared embedding space that is invariant to input modality, we propose the Cross-Modal Code Matching objective which encourages the model to focus more on the semantic aspect of the input, as illustrated in Figure 2 . For each vector h M i,l in the fine-grained representation sequence H M i encoded from an instance x M i of modality M , we first define the probability of h M i,l belonging to the codeword e v as the Softmin function of their Euclidean distance,",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 595,
                        "end": 596,
                        "text": "2",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Cross-Modal Code Matching",
                "sec_num": "2.3"
            },
            {
                "text": "P (e v |h M i,l ) = exp(-\u2225f M (h M i,l )-ev\u2225 2 ) k\u2208V exp(-\u2225f M (h M i,l )-e k \u2225 2",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Cross-Modal Code Matching",
                "sec_num": "2.3"
            },
            {
                "text": ") . Note that this definition assigns higher a probability to codewords that are closer to the fine-grained representation, where the closest codeword is used to perform vector quantization. We can then define the sequence-level probability distribution over the codebook as the average of the fine-grained distribution,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Cross-Modal Code Matching",
                "sec_num": "2.3"
            },
            {
                "text": "P (e v |H M i ) = 1 L l P (e v |h M i,l",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Cross-Modal Code Matching",
                "sec_num": "2.3"
            },
            {
                "text": "), which is the normalized frequency of codeword usage for a given sequence of fine-grained representations. Next, for a pair of cross-modal data (x A i , x B j ), we define their code similar-ity as the negative symmetric cross entropy of probability distribution over the codebook",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Cross-Modal Code Matching",
                "sec_num": "2.3"
            },
            {
                "text": "S code (x A i , x B j ) = v P (e v |H A i ) log P (e v |H B j ) + v P (e v |H B j ) log P (e v |H A i ).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Cross-Modal Code Matching",
                "sec_num": "2.3"
            },
            {
                "text": "Finally, we define the Cross-Modal Code Matching (CMCM) objective using code similarity as",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Cross-Modal Code Matching",
                "sec_num": "2.3"
            },
            {
                "text": "LCMCM = - 1 N N i=1 log e S code (x A i ,x B i ) e S code (x A i ,x B i ) + j\u0338 =i e S code (x A i ,x B j )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Cross-Modal Code Matching",
                "sec_num": "2.3"
            },
            {
                "text": ".",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Cross-Modal Code Matching",
                "sec_num": "2.3"
            },
            {
                "text": "(2)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Cross-Modal Code Matching",
                "sec_num": "2.3"
            },
            {
                "text": "Intuitively, the proposed objective encourages the model to represent the input (x A i , x B j ) with similar codewords for positive pairs (i = j) and nonmatching codewords for negative pairs (i \u0338 = j). As a consequence, each codeword is expected to be a modality invariant representation of a more fine-grained concept, action, or word that can be discovered from cross-modal data. For example, a codeword could correspond to both the visual scene of a man juggling, and also the spoken word \"juggling,\" as we demonstrate in our experimental results in Table 2 and Figure 4 .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 560,
                        "end": 561,
                        "text": "2",
                        "ref_id": "TABREF2"
                    },
                    {
                        "start": 566,
                        "end": 574,
                        "text": "Figure 4",
                        "ref_id": "TABREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Cross-Modal Code Matching",
                "sec_num": "2.3"
            },
            {
                "text": "The full objective of our proposed cross-modal representation learning framework is the combination of objectives at different levels L = L MMS + \u03b1L CMCM , where \u03b1 controls the weight between the two terms. Empirically, we found \u03b1 = 0.1 worked well across different settings. Please refer to Section C and D in Appendix for ablation study and comparison to possible alternatives to our method.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Cross-Modal Code Matching",
                "sec_num": "2.3"
            },
            {
                "text": "Examples of the cross-modal learning paradigm. As described in Section 2.1, many of the existing methods for cross-modal learning fit into the paradigm where encoders are modalityindependent. This paradigm has been shown to be effective by achieving state-of-the-art retrieval performance on benchmark datasets with the modality pairs that we considered in this work: videotext (Bain et al., 2021; Luo et al., 2021) , videoaudio (Monfort et al., 2021; Rouditchenko et al., 2020) , and image-audio (Harwath et al., 2018 (Harwath et al., , 2020)) . While these prior works relied on different pre-training datasets, model architectures, and objective functions, they all leverage modalityindependent encoders. One of the most important features of this paradigm is the fixed inference time for retrieval. Since the encoders are modalityindependent, embedding vectors for samples in a given modality can be computed without using any samples from the other modality. Thus retrieval only involves computing the dot product between embedding vectors from two different modalities. As a consequence, these models are more flexible for large-scale retrieval, and the embedding vectors from each modality can be used independently for other downstream tasks.",
                "cite_spans": [
                    {
                        "start": 378,
                        "end": 397,
                        "text": "(Bain et al., 2021;",
                        "ref_id": "BIBREF44"
                    },
                    {
                        "start": 398,
                        "end": 415,
                        "text": "Luo et al., 2021)",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 429,
                        "end": 451,
                        "text": "(Monfort et al., 2021;",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 452,
                        "end": 478,
                        "text": "Rouditchenko et al., 2020)",
                        "ref_id": "BIBREF35"
                    },
                    {
                        "start": 497,
                        "end": 518,
                        "text": "(Harwath et al., 2018",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 519,
                        "end": 544,
                        "text": "(Harwath et al., , 2020))",
                        "ref_id": "BIBREF13"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related work",
                "sec_num": "3"
            },
            {
                "text": "Other cross-modal learning frameworks. In contrast to the aforementioned works, some methods leverage cross-modal relations within the encoders instead of using modality-independent encoders. This has been done with both cross-modal encoders (Lei et al., 2021; Luo et al., 2021) and cross-modal attention mechanisms (Miech et al., 2018; Liu et al., 2019b,a; Gabeur et al., 2020) . However, the cross-modal interactions increase the complexity for retrieval since every instance of a specific modality must be used as input with every instance of another modality to obtain the embedding vectors. With m and n samples in the modalities respectively, this increases the complexity from the modality-independent approach from O(m + n) to O(mn). Further, it also makes analysis of the embedding vectors from any individual modality challenging and inhibits single-modality downstream tasks. Our proposed framework builds on the modality-independent approach to enable light-weight retrieval, but it also enables crossmodal interaction through our proposed codebook and Cross-Modal Code Matching objective.",
                "cite_spans": [
                    {
                        "start": 242,
                        "end": 260,
                        "text": "(Lei et al., 2021;",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 261,
                        "end": 278,
                        "text": "Luo et al., 2021)",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 316,
                        "end": 336,
                        "text": "(Miech et al., 2018;",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 337,
                        "end": 357,
                        "text": "Liu et al., 2019b,a;",
                        "ref_id": null
                    },
                    {
                        "start": 358,
                        "end": 378,
                        "text": "Gabeur et al., 2020)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related work",
                "sec_num": "3"
            },
            {
                "text": "Uncovering semantic-level correspondences. Image-audio models have been shown to discover spoken words and visual objects without supervision through retrieval tasks (Synnaeve et al., 2014; Harwath and Glass, 2015; Harwath et al., 2017; Kamper et al., 2018) , and the audio embedding vectors have been shown to cluster into word-like speech units (Harwath and Glass, 2017; Wang and Hasegawa-Johnson, 2019; Harwath et al., 2020) . Some work has studied the ability of video-audio models to relate spoken words to visual objects and actions in videos (Boggust et al., 2019; Rouditchenko et al., 2020) . However, none of these models incorporated a shared embedding space that enabled modality-invariant representations. VQ units have been used in the audio encoder of an image-audio model (Harwath et al., 2020) , which allowed it to capture the hierarchical structure of spoken language. While our proposed framework is similar in that it also discretizes the audio sequence with VQ units, our work differs significantly by capturing the cross-modal interactions between visual and audio inputs in the shared embedding space rather than solely capturing the tree structure of speech. Further, besides image-audio data, our proposed framework can handle video-audio and video-text data.",
                "cite_spans": [
                    {
                        "start": 166,
                        "end": 189,
                        "text": "(Synnaeve et al., 2014;",
                        "ref_id": "BIBREF37"
                    },
                    {
                        "start": 190,
                        "end": 214,
                        "text": "Harwath and Glass, 2015;",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 215,
                        "end": 236,
                        "text": "Harwath et al., 2017;",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 237,
                        "end": 257,
                        "text": "Kamper et al., 2018)",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 347,
                        "end": 372,
                        "text": "(Harwath and Glass, 2017;",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 373,
                        "end": 405,
                        "text": "Wang and Hasegawa-Johnson, 2019;",
                        "ref_id": "BIBREF39"
                    },
                    {
                        "start": 406,
                        "end": 427,
                        "text": "Harwath et al., 2020)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 549,
                        "end": 571,
                        "text": "(Boggust et al., 2019;",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 572,
                        "end": 598,
                        "text": "Rouditchenko et al., 2020)",
                        "ref_id": "BIBREF35"
                    },
                    {
                        "start": 787,
                        "end": 809,
                        "text": "(Harwath et al., 2020)",
                        "ref_id": "BIBREF13"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related work",
                "sec_num": "3"
            },
            {
                "text": "(B \u2192 A) (A \u2192 B) R@1 \u2191 R@5 \u2191 R@10 \u2191 MnR \u2193 R@1 \u2191 R@5 \u2191 R@10 \u2191 MnR \u2193 Video-Audio / S-",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related work",
                "sec_num": "3"
            },
            {
                "text": "To demonstrate the generalizability of the proposed method, we tested our framework on different cross-modal datasets and baseline models that fit into the cross-modal learning paradigm. All setups are listed below and summarized in Table 3 of the Appendix. For training the proposed model, we randomly initialized all the modules related to the discrete shared embedding space and trained them jointly with the rest of the framework (see Figure 1 ). Unless otherwise specified, (1) we \"warm-started\" our proposed framework by initializing it with the modality-specific encoders (namely, f M fine and f M high ) from the baseline models;",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 239,
                        "end": 240,
                        "text": "3",
                        "ref_id": "TABREF3"
                    },
                    {
                        "start": 446,
                        "end": 447,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Experiments 4.1 Setup",
                "sec_num": "4"
            },
            {
                "text": "(2) both the projection network f M and the encoder network f M code are single linear layers; (3) the codebook size is set to 1024. Please refer to Section B in the Appendix for more implementation details. Video-Audio: S-MiT (Monfort et al., 2021) contains over 500k pairs of 3-second video and corre-sponding spoken audio captions averaging 8 seconds. We followed the official protocol to train on the training set of 500k pairs, use the validation set of 10k pairs for development and analysis, and report the retrieval result on a 1k search space over 5 runs randomly sampled from a held-out test set. We selected the same baseline model used on the dataset (Monfort et al., 2021) , which contains a visual encoder composed of a ResNet-152 pre-trained on ImageNet (Deng et al., 2009) and TSM ResNet-50 (Lin et al., 2019) pre-trained on M-MiT (Monfort et al., 2019) . The audio encoder is a randomly initialized 1D-ResNet (Harwath et al., 2018) designed specifically for spectrograms. The shared embedding space has the dimension of 4096, matching the encoders in the baseline model.",
                "cite_spans": [
                    {
                        "start": 227,
                        "end": 249,
                        "text": "(Monfort et al., 2021)",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 663,
                        "end": 685,
                        "text": "(Monfort et al., 2021)",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 769,
                        "end": 788,
                        "text": "(Deng et al., 2009)",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 807,
                        "end": 825,
                        "text": "(Lin et al., 2019)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 847,
                        "end": 869,
                        "text": "(Monfort et al., 2019)",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 926,
                        "end": 948,
                        "text": "(Harwath et al., 2018)",
                        "ref_id": "BIBREF14"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments 4.1 Setup",
                "sec_num": "4"
            },
            {
                "text": "Image-Audio: Places (Harwath et al., 2017) contains over 400k pairs of images from the Places 205 dataset (Zhou et al., 2014) and corresponding spoken audio captions averaging 10 seconds. We followed the previous works (Harwath et al., 2018 (Harwath et al., , 2020) ) to use the training set of 400k pairs and report results on the validation set of 1k pairs. We select ResDAVEnet (Harwath et al., 2018) as the baseline model where the visual encoder is a ResNet-50 pre-trained on ImageNet (Deng et al., 2009) and the audio encoder is a randomly initialized 1D-ResNet (Harwath et al., 2018) designed specifically for spectrograms. The shared embedding space has the dimension of 1024. Video-Text: MSR-VTT (Xu et al., 2016) contains 10k video clips with length varying from 10 to 32 seconds. While each video is provided with 20 related captions for training, we followed the evaluation protocol from previous works (Luo et al., 2021; Gabeur et al., 2020; Yu et al., 2018) to use the training-9k / test 1k-A splits for training and testing respectively. CLIP4Clip (Luo et al., 2021) , the current state-of-the-art on MSR-VTT, is selected as the baseline model. Following the crossmodal learning paradigm described in Section 2.1, CLIP4Clip is composed of a pair of encoders: a Visual Transformer (Dosovitskiy et al., 2020) and a Text Transformer (Vaswani et al., 2017) . Both encoders are initialized from the CLIP model (Radford et al., 2021) , which is pre-trained on the textimage dataset WIT (Radford et al., 2021) and optimized in the end-to-end manner from pixel/text input. For training the proposed framework on top of CLIP4Clip, we freeze the transformers from CLIP4Clip and update only the modules related to the discrete shared embedding space. Both the projection network f M and the encoder network f M code are 4D-Convolutions for video with a depth of 3 and BiLSTMs for text, also with a depth of 3. While CLIP4Clip provided different options for the high-level visual encoder f M high , we adopted the vanilla mean-pooling model. Following CLIP4Clip, the shared embedding space has a dimension of 512.",
                "cite_spans": [
                    {
                        "start": 20,
                        "end": 42,
                        "text": "(Harwath et al., 2017)",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 106,
                        "end": 125,
                        "text": "(Zhou et al., 2014)",
                        "ref_id": "BIBREF42"
                    },
                    {
                        "start": 219,
                        "end": 240,
                        "text": "(Harwath et al., 2018",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 241,
                        "end": 267,
                        "text": "(Harwath et al., , 2020) )",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 381,
                        "end": 403,
                        "text": "(Harwath et al., 2018)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 490,
                        "end": 509,
                        "text": "(Deng et al., 2009)",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 568,
                        "end": 590,
                        "text": "(Harwath et al., 2018)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 705,
                        "end": 722,
                        "text": "(Xu et al., 2016)",
                        "ref_id": "BIBREF40"
                    },
                    {
                        "start": 915,
                        "end": 933,
                        "text": "(Luo et al., 2021;",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 934,
                        "end": 954,
                        "text": "Gabeur et al., 2020;",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 955,
                        "end": 971,
                        "text": "Yu et al., 2018)",
                        "ref_id": "BIBREF41"
                    },
                    {
                        "start": 1063,
                        "end": 1081,
                        "text": "(Luo et al., 2021)",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 1295,
                        "end": 1321,
                        "text": "(Dosovitskiy et al., 2020)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 1345,
                        "end": 1367,
                        "text": "(Vaswani et al., 2017)",
                        "ref_id": "BIBREF38"
                    },
                    {
                        "start": 1420,
                        "end": 1442,
                        "text": "(Radford et al., 2021)",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 1495,
                        "end": 1517,
                        "text": "(Radford et al., 2021)",
                        "ref_id": "BIBREF33"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments 4.1 Setup",
                "sec_num": "4"
            },
            {
                "text": "Data retrieval is one of the most common evaluations for cross-modal representation learning. For example, in video retrieval with input query text, videos in the search space will be ranked by the similarity between the representation of each video and the query. We report the standard retrieval metrics recall at rank K (R@K) and median rank (MdR) in Table 1 . We show the performance on both visual retrieval, where input language queries are used to retrieve videos or images, and language retrieval, where input visual queries are used to retrieve spoken or text captions. Video-Audio Retrieval. Video-Audio retrieval on S-MiT (Monfort et al., 2021 ) is a challenging task since videos are paired with raw speech audio, which is untranscribed, unsegmented, and can contain background noise and speaker variation. However, our proposed framework that leverages cross-modal connections between visual actions and spoken words is able to improve the baseline model by a margin. We further analyze our framework's ability to relate visual actions and spoken words in Section 4.3. Image-Audio Retrieval. Comparing the baseline model, ResDAVEnet (Harwath et al., 2018) , and the current state-of-the-art ResDAVEnet-VQ (Harwath et al., 2020) , the latter model introduces VQ units into the audio encoder, allowing it to model the hierarchical structure of speech and achieve better retrieval results. With our framework, we introduce our shared VQ embedding space into the ResDAVEnet model to capture cross-modal interactions. This improves the performance over both ResDAVEnet and ResDAVEnet-VQ. Video-Text Retrieval.",
                "cite_spans": [
                    {
                        "start": 633,
                        "end": 654,
                        "text": "(Monfort et al., 2021",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 1146,
                        "end": 1168,
                        "text": "(Harwath et al., 2018)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 1218,
                        "end": 1240,
                        "text": "(Harwath et al., 2020)",
                        "ref_id": "BIBREF13"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 360,
                        "end": 361,
                        "text": "1",
                        "ref_id": "TABREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Cross-Modal Retrieval",
                "sec_num": "4.2"
            },
            {
                "text": "On the benchmark MSR-VTT dataset, we compared our proposed 2 ) are highlighted in red.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 59,
                        "end": 60,
                        "text": "2",
                        "ref_id": "TABREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Cross-Modal Retrieval",
                "sec_num": "4.2"
            },
            {
                "text": "method against recent works achieving state-ofthe-art (Bain et al., 2021; Liu et al., 2021; Luo et al., 2021) and provide a full comparison against more prior work (Liu et al., 2019b; Rouditchenko et al., 2020; Gabeur et al., 2020; Patrick et al., 2020; Dzabraev et al., 2021; Croitoru et al., 2021) in Section E of the Appendix. Frozen-in-Time (Bain et al., 2021) and CLIP4Clip (Luo et al., 2021) are similar methods that employ a Visual Transformer (Dosovitskiy et al., 2020) to encode video as sequence of images. The key differences between them is the choice of summarizing function (i.e. f M high ) for video and the pre-training procedure. We also note that the CLIP4Clip with tight transformer encoder (Luo et al., 2021 ) (CLIP4Clip-tightT) relied on cross-modal reference via selfattention encoders to derive representations, which has a higher time complexity as mentioned in Section 3. With the shared codebook and Cross-Modal Code Matching objection, our proposed framework also enables cross-modal reference and gives an improvement over the baseline model without increasing the time complexity.",
                "cite_spans": [
                    {
                        "start": 54,
                        "end": 73,
                        "text": "(Bain et al., 2021;",
                        "ref_id": "BIBREF44"
                    },
                    {
                        "start": 74,
                        "end": 91,
                        "text": "Liu et al., 2021;",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 92,
                        "end": 109,
                        "text": "Luo et al., 2021)",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 164,
                        "end": 183,
                        "text": "(Liu et al., 2019b;",
                        "ref_id": "BIBREF43"
                    },
                    {
                        "start": 184,
                        "end": 210,
                        "text": "Rouditchenko et al., 2020;",
                        "ref_id": "BIBREF35"
                    },
                    {
                        "start": 211,
                        "end": 231,
                        "text": "Gabeur et al., 2020;",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 232,
                        "end": 253,
                        "text": "Patrick et al., 2020;",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 254,
                        "end": 276,
                        "text": "Dzabraev et al., 2021;",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 277,
                        "end": 299,
                        "text": "Croitoru et al., 2021)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 345,
                        "end": 364,
                        "text": "(Bain et al., 2021)",
                        "ref_id": "BIBREF44"
                    },
                    {
                        "start": 379,
                        "end": 397,
                        "text": "(Luo et al., 2021)",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 451,
                        "end": 477,
                        "text": "(Dosovitskiy et al., 2020)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 710,
                        "end": 727,
                        "text": "(Luo et al., 2021",
                        "ref_id": "BIBREF25"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Cross-Modal Retrieval",
                "sec_num": "4.2"
            },
            {
                "text": "Overall, our proposed method enables consistent improvements regardless of the data modalities and baseline architectures, demonstrating its effectiveness and generalizability.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Cross-Modal Retrieval",
                "sec_num": "4.2"
            },
            {
                "text": "One of the important motivations of introducing the discrete cross-modal embedding space is better model interpretability. In this section, we take a closer look into the codewords learned through our proposed framework. For the evaluation, we chose the video-audio setup on S-MiT (Monfort et al., 2021) . We used video-audio pairs from the development set, where each pair is labeled with an action out of 332 categories. Note that we only used labels for analysis, labels are never used for training.",
                "cite_spans": [
                    {
                        "start": 281,
                        "end": 303,
                        "text": "(Monfort et al., 2021)",
                        "ref_id": "BIBREF29"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discrete Representation Analysis",
                "sec_num": "4.3"
            },
            {
                "text": "Codeword. First, we compute the conditional probability distributions of action labels given the codewords over the video inputs. Each video input is fixed-length and represented by 27 codewords (3 frames each represented by 3\u00d73 codewords), and we labeled all these codewords with the video's action label. By accumulating codeword labels through the whole development set, we can compute the conditional probability of each action given any codeword, i.e. P (action|codeword). Results are visualized in the upper part of Figure 3 . Similarly, we computed the conditional probabilities based on the audio input where each utterance is represented by up to 32 codewords depending on the utterance length. We selected the most frequent codewords used by the video inputs and plot the conditional probabilities based on the audio input in the lower part of Figure 3 . We can observe that both matrices have similar patterns, i.e., when a codeword is activated, there is a high chance of a specific action appearing in the input regardless if it is video or audio. This suggests that our model is able to learn cross-modal representations for actions grounded by either visual or spoken language input. The codewords are not only modality invariant, but more importantly, they also capture the semantic relations of the labels. e.g., codewords with the highest chance to represent \"autographing\" typically have the second highest chance of representing \"signing\"; codewords for \"surfing\" are less likely to represent other actions as all of them are very different from \"surfing\". We also note that without the Cross-Modal Code Matching objective, semantically related video and audio inputs no longer use the same codewords, which we illustrate in Section F of the Appendix.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 529,
                        "end": 530,
                        "text": "3",
                        "ref_id": "FIGREF1"
                    },
                    {
                        "start": 861,
                        "end": 862,
                        "text": "3",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Conditional Probability of Action Labels Given",
                "sec_num": null
            },
            {
                "text": "Cross-Modal Correspondences. Next, we analyze the connections captured by the codewords between action labels and spoken words. With the same label accumulation method described previously, we compute the precision of action prediction with codewords (i.e. code-action co-occurrence code occurrence",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conditional Probability of Action Labels Given",
                "sec_num": null
            },
            {
                "text": ").",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conditional Probability of Action Labels Given",
                "sec_num": null
            },
            {
                "text": "For the audio, we used word-level transcriptions (from Google's speech-to-text API) to assign a spoken word to each codeword when it is activated by the input utterance. This results in a hypothesis set including around 7k words for each codeword, and we listed the top 2 hypotheses for each codeword with the highest F1 score (instead of precision to avoid domination of high-frequency words). Results are listed in Table 2 . For the codewords that have the highest precision on predicting the action label, we found the top hypotheses for spoken words are often the action label itself. E.g., the codeword (rank 1st) for the visual action \"juggling\" maps to the spoken word \"juggling\" perfectly. As precision on visual action prediction decreases, we observed fewer perfect mappings, but the spoken word hypotheses remained semantically related to the visual action hypotheses. E.g., the codeword (rank 35th) for the visual action \"dunking\" with lower precision now maps to the spoken word \"basketball.\" Surprisingly, even the codewords with the lowest precision capture relationships between visual actions and spoken words to some extent. E.g., codeword (rank 743th) that is most related to the action \"baking\" has the top and second word hypotheses \"cupcake\" and \"peanut.\"",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 423,
                        "end": 424,
                        "text": "2",
                        "ref_id": "TABREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Conditional Probability of Action Labels Given",
                "sec_num": null
            },
            {
                "text": "Codeword Localization. Finally, to visualize the relation between codewords and the input data, we localize the segments of both the video and audio input that are assigned to certain codewords. This is possible because quantization in our shared embedding space is done at the fine-grained level, so that the time and spatial axes are preserved. Examples are shown in Figure 4 , where the regions assigned to the given code are highlighted. Interestingly, we see the codewords being aligned to both the visual actions and the corresponding spoken words. This supports our claim of having a more interpretable representation at the fine-grained level.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 376,
                        "end": 377,
                        "text": "4",
                        "ref_id": "FIGREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Conditional Probability of Action Labels Given",
                "sec_num": null
            },
            {
                "text": "In this paper, we proposed a framework for crossmodal representation learning with a discrete embedding space that is shared amongst different modalities and enables model interpretability. We also propose a Cross-Modal Code Matching objective that encourages models to represent crossmodel semantic concepts in the embedding space. Combining our discrete embedding space and objective with existing cross-modal representation learning models improves retrieval performance on video-text, video-audio, and image-audio datasets.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "5"
            },
            {
                "text": "We also analyze the shared embedding space and find that semantically related video and audio inputs tend to use the same codewords.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "5"
            },
            {
                "text": "The codebook with d-dimensional codewords is initialized with",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Appendix A Codebook Update Policy",
                "sec_num": null
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "N (0) v = 1 m (0) v \u223c N d (0, 1) e (0) v = m (0) v ,",
                        "eq_num": "(3)"
                    }
                ],
                "section": "Appendix A Codebook Update Policy",
                "sec_num": null
            },
            {
                "text": "and updated with each codeword e v being the exponential moving average (EMA) of all the fine-grained representations H = f M (h M i,l ) hM i,l = e v that was replaced by e v for every training step t:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Appendix A Codebook Update Policy",
                "sec_num": null
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "N (t) v \u2190 \u03b3 N (t-1) v + (1 -\u03b3) |H| m (t) v \u2190 \u03b3 m (t-1) v + (1 -\u03b3) h\u2208H h e (t) v \u2190 m (t) v N (t) v ,",
                        "eq_num": "(4)"
                    }
                ],
                "section": "Appendix A Codebook Update Policy",
                "sec_num": null
            },
            {
                "text": "where the decay factor \u03b3 is set to 0.99 throughout this work. To improve the overall usage of the codebook, the input fine-grained representations are modality-wise batch normalized. In addition, codewords that are not activated (i.e. |H| = 0) for 100 consecutive steps are re-initialized during codebook update. The reset value is randomly chosen from activated codewords.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Appendix A Codebook Update Policy",
                "sec_num": null
            },
            {
                "text": "For each dataset and modality pair considered in this work, we selected baseline models that follow the cross-modal learning paradigm (as described in Section 2.1). Baseline models with different finegrained and high-level encoders (f M fine and f M high ) are summarized in Table 3 . The links to the official implementation of these baseline models are also provided in the table. For a fair comparison, we retrained the models with the L MMS (margin set to 1e-3) as our baseline models. S-MiT. The input audio feature is a 40 dimensional mel-spectrogram with a window size of 25 ms and a hop size of 10 ms. The baseline is trained with a batch size of 2048 and a learning rate of 1e-3. To train the shared discrete embedding space, we warm-started from the baseline model with a learning rate of 1e-4. Each video is encoded into 27 codewords (3 \u00d7 3 \u00d7 3 for time, height, width) and every 16 consecutive frames from the spectrogram is encoded into 1 codeword. The baseline model is trained for 4 hours on 4 V100 GPUs; and it takes an additional 1 hour to train the proposed framework. For both baseline model and our proposed model, we followed the previous work (Monfort et al., 2021) to perform a second round training with a learning rate of 1e-5 and a batch size of 128. The second round training fine-tunes the TSM video encoder (which is frozen in the first round training) on S-MiT jointly with the rest of the components, which takes 2 days on 8 Titan RTX GPUs. Places. The input audio feature is a 40 dimentional mel-spectrogram with a window size of 25 ms and a hop size of 10 ms. The baseline is trained with a batch size of 256 and a learning rate of 1e-3. To train the shared discrete embedding space, we warm-started from the baseline model with a learning rate of 1e-4. Each image is encoded into 49 codewords (7 \u00d7 7 for height, width) and every 16 consecutive frames from the spectrogram is encoded into 1 codeword. The baseline model is trained for 36 hours on 1 V100 GPU; and it takes an additional 4 hours to train the proposed framework. MSR-VTT. For our baseline model, we did not reproduce CLIP4Clip's post-pretraining stage, which trained CLIP4Clip on the subset of HowTo100M (Miech et al., 2019) before adapting to MSR-VTT, since this stage is not necessary for the best results on MSR-VTT and the subset is not released. We used all of the hyper-parameters of the official implementation except the batch size is reduced from 128 to 64 to meet our hardware restriction. To train the shared discrete embedding space, we warm-started from the baseline model with a learning rate of 1e-5. Each video is encoded into 8 codewords (2 \u00d7 2 \u00d7 2 for time, height, width) and each subword unit in the sentence is encoded into 1 codeword. The baseline model is trained for 12 hours on 8 2080Ti GPUs; and it takes an additional 6 hours to train the proposed framework.",
                "cite_spans": [
                    {
                        "start": 1165,
                        "end": 1187,
                        "text": "(Monfort et al., 2021)",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 2201,
                        "end": 2221,
                        "text": "(Miech et al., 2019)",
                        "ref_id": "BIBREF28"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 281,
                        "end": 282,
                        "text": "3",
                        "ref_id": "TABREF3"
                    }
                ],
                "eq_spans": [],
                "section": "B Implementation Details",
                "sec_num": null
            },
            {
                "text": "To justify our framework design and choice of hyperparameters, we conducted an ablation study on the image-audio setting and report the results in Table 4 . Impact of the shared embedding space. For the codebook size, 1024 codewords worked well across different datasets. Halving and doubling the number of codewords (row(b) & (c)) both decreased Removing the proposed Cross-Modal Code Matching objective (setting \u03b1 = 0, row(e)), however, hurts the performance. Furthermore, without the objective, the codebook no longer captures crossmodal correspondences, as illustrated in Section F of the Appendix. We also observed that disabling the VQ layer together with the Cross-Modal Code Matching objective slightly recovers performance (row(f) v.s. row(e)). All of these observations serve as evidence that the proposed discrete embedding space is most beneficial to the retrieval task with the guidance from the Cross-Modal Code Matching objective.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 153,
                        "end": 154,
                        "text": "4",
                        "ref_id": "TABREF4"
                    }
                ],
                "eq_spans": [],
                "section": "C Ablation Study",
                "sec_num": null
            },
            {
                "text": "Importance of baseline models in the crossmodal learning paradigm. As mentioned in Section 4.1, the discrete shared embedding space is learned with \"warm-starting\" from a baseline model. We note that warm-starting is important for getting more refined representations that yield better retrieval results (row(a) v.s. row(g)). Without warm-starting, our framework can only perform similar to the baseline (row(g) v.s. row(i)).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "C Ablation Study",
                "sec_num": null
            },
            {
                "text": "This finding aligns with previous work (Harwath et al., 2020) that used VQ layers in the audio encoder and used warm-starting to learn acoustic units. Moreover, removing the continuous representations (row(h)) originally used in the cross-modal learning paradigm and using only the codeword representations significantly decreases performance. This exposes the trade-off between interpretability and end-task performance by imposing a discrete embedding space. Hence, we choose to integrate both discrete and continuous embedding space for retrieval.",
                "cite_spans": [
                    {
                        "start": 39,
                        "end": 61,
                        "text": "(Harwath et al., 2020)",
                        "ref_id": "BIBREF13"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "C Ablation Study",
                "sec_num": null
            },
            {
                "text": "As shown in our experiments and ablation study, the key to improve model interpretability and highlevel retrieval performance by our proposed method is learning domain-invariant discrete representation. To show the necessity of the proposed Cross-Modal Code Matching, we also provide a list of methods we already tried but failed that eventually helped us derive domain-invariant representation with the proposed Cross-Modal Code Matching:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "D Failure Attempts with Possible Alternatives",
                "sec_num": null
            },
            {
                "text": "Domain adversarial training is a common technique to learn domain-invariant representation. The method introduces an auxiliary classifier to classify the source domain of the representation in the latent space. To train domain-invariant encoders (f M fine ), a gradient reversal layer (Ganin and Lempitsky, 2015) is introduced between the classifier and the encoder and the whole system is trained in an end-to-end manner. In practice, this method results in mode collapse at the fine-grained representation level, i.e., the model neglects input and produces a constant vector. This leads to adding noise with small variance to the high-level representation and results in slightly worse retrieval scores.",
                "cite_spans": [
                    {
                        "start": 285,
                        "end": 312,
                        "text": "(Ganin and Lempitsky, 2015)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Domain adversarial training over continuous representation",
                "sec_num": "1."
            },
            {
                "text": "As an alternative, domain adversarial training can also be performed in the discrete embedding space. In practice, we observed code collapse at the fine-grained representation level, i.e., only 1 codeword is active out of the entire codebook. This leads to adding constant noise to the high-level representation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Domain adversarial training over distribution over codebook",
                "sec_num": "2."
            },
            {
                "text": "Another common technique to enforce domain-invariant latent space is to adapt identical prior distributions over representation from different domains. This can be implemented by adding a regularization term during training which minimizes the KL-divergence between representation from model and the desired prior distribution as shown in Variational Auto-Encoder (Kingma and Welling, 2013). In practice, we tried both Gaussian and uniform prior, and both resulted in mode collapse, similar to method 1.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "VAE-like prior distribution regularization over continuous representation",
                "sec_num": "3."
            },
            {
                "text": "With the distribution over codebook defined in Section 2.3, we can also adapt prior distribution regularization in the discrete latent space instead of the continuous space. However, we observed code collapse where only certain codewords will be utilized by the model in our experiments, similar to method 2.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "VAE-like Prior distribution regularization over distribution over codebook",
                "sec_num": "4."
            },
            {
                "text": "5. Minimizing the cross-entropy/JSD/KLD between distribution over codebook of each positive pair (i.e. no negative sampling in the CMCM loss)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "VAE-like Prior distribution regularization over distribution over codebook",
                "sec_num": "4."
            },
            {
                "text": "Besides the proposed Cross-Modal Code Matching , we also experimented with different substitutes that might be able to encourage codeword sharing across domains, including cross-entropy, JS-divergence, and KLdivergence. While cross-entropy leads to code collapse similar to method 2., JSD and KLD lead to uniform code distribution for each input instance, making fine-grained representation uninformative. A possible explanation is that both measurements include the negative entropy term. Minimizing them encourages uniform distribution over the codebook.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "VAE-like Prior distribution regularization over distribution over codebook",
                "sec_num": "4."
            },
            {
                "text": "Note that unlike all the objectives above, the proposed Cross-Modal Code Matching loss not only enforces the model to learn domaininvariant representation but also introduces contrastive learning simultaneously. To be more specific, different view (modality) of the same instance is encouraged to share similar codeword combinations while different instance should be encoded into different codeword combinations. This key difference allows our method to learn informative discrete codewords that align to the goal of the high-level objective function.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "VAE-like Prior distribution regularization over distribution over codebook",
                "sec_num": "4."
            },
            {
                "text": "In addition to the comparison against recent stateof-the-art methods in Table 1 for video retrieval on MSR-VTT, in Table 5 we show the complete comparison to prior work and summarize the models here. Collaborative Experts (Liu et al., 2019b) leverages \"expert\" features that can be obtained from the raw video from different off-the-shelf models (such as object detection, scene classification, and speech recognition models) to build representations. Instead of summarizing the expert features into a compact video representation and computing similarity with the text representation, the Multi-Modal Transformer (Gabeur et al., 2020) computes similarity between different expert features and the text representation with a proposed variation of the Transformer (Vaswani et al., 2017) . Based on the Multi-Modal Transformer, Multidomain Multi-Modal Transformer (Dzabraev et al., 2021) explored an additional motion feature and the combination of different training datasets to further improve the result. Support-Set Bottlenecks (Patrick et al., 2020) studies the benefit that cross-instance captioning can bring by generating text based on the combination of all representations of similar videos. Similar to our framework, Hierarchical Transformer with Momentum Contrast (Liu et al., 2021) divided representations from different layer of the encoders into fine-grained (which they referred to feature-level) and high-level (which they reffered to semantic-level) representations. While our work focused on learning discrete representations in the fine-grained embedding space, they performed momentum-based representation matching across the two levels that encourages the two embedding spaces to be more similar. TeachText (Croitoru et al., 2021) leverages distillation learning where multiple captions describing the same video can be considered by different teacher models that jointly guide the student network. Frozen-in-Time (Bain et al., 2021) and CLIP4Clip (Luo et al., 2021) both found the recent proposed Visual Transformer (Dosovitskiy et al., 2020) can significantly improve retrieval results while they differ in the choice of summarizing function for video (i.e. f M high ) and the pre-training procedure. Moreover, CLIP4Clip also introduces different choice of the summarizing function f M high including RNNs (CLIP4Clip-seqLSTM) and Transformers (CLIP4Clip-seqTransformer) that replaces the mean-pooling function (CLIP4Clip-meanPooling) at the cost of higher time complexity and computational cost. Note that while our work is based on the vanilla mean-pooling function, we achieved comparable or better performance with the proposed discrete embedding representations. As described in Section 3, CLIP4Clip also introduced a cross-modal transformer network (CLIP4Clip-tightTransformer) that allows cross-modal reference for deriving representations.",
                "cite_spans": [
                    {
                        "start": 222,
                        "end": 241,
                        "text": "(Liu et al., 2019b)",
                        "ref_id": "BIBREF43"
                    },
                    {
                        "start": 614,
                        "end": 635,
                        "text": "(Gabeur et al., 2020)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 763,
                        "end": 785,
                        "text": "(Vaswani et al., 2017)",
                        "ref_id": "BIBREF38"
                    },
                    {
                        "start": 862,
                        "end": 885,
                        "text": "(Dzabraev et al., 2021)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 1030,
                        "end": 1052,
                        "text": "(Patrick et al., 2020)",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 1274,
                        "end": 1292,
                        "text": "(Liu et al., 2021)",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 1727,
                        "end": 1750,
                        "text": "(Croitoru et al., 2021)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 1934,
                        "end": 1953,
                        "text": "(Bain et al., 2021)",
                        "ref_id": "BIBREF44"
                    },
                    {
                        "start": 1968,
                        "end": 1986,
                        "text": "(Luo et al., 2021)",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 2037,
                        "end": 2063,
                        "text": "(Dosovitskiy et al., 2020)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 78,
                        "end": 79,
                        "text": "1",
                        "ref_id": "TABREF0"
                    },
                    {
                        "start": 121,
                        "end": 122,
                        "text": "5",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "E MSR-VTT Video Retreival Full Comparison",
                "sec_num": null
            },
            {
                "text": "To demonstrate the importance of our proposed Cross-Modal Code Matching objective, Figure 5 illustrates the conditional probability matrix (described in Section 4.3 and Figure 3 ) when the proposed objective is deactived (setting \u03b1 = 0). Unsurprisingly, we see that the correlation between codewords and action labels are gone, indicating that the assignment of codewords are now dominated by the input modality instead of the underlying action label. This can also be verified by visualizing the discrete embedding space in a lower dimension as plotted in Figure 6 . This evidence suggests that the proposed Cross-Modal Code Matching Objective is effective for learning modality-invariant representations.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 90,
                        "end": 91,
                        "text": "5",
                        "ref_id": "FIGREF4"
                    },
                    {
                        "start": 176,
                        "end": 177,
                        "text": "3",
                        "ref_id": "FIGREF1"
                    },
                    {
                        "start": 564,
                        "end": 565,
                        "text": "6",
                        "ref_id": "FIGREF6"
                    }
                ],
                "eq_spans": [],
                "section": "F Results Without Cross-Modal Code Matching",
                "sec_num": null
            },
            {
                "text": "An extension of Table 2 showing the correspondence between codewords, visual actions, and spoken words are provided in Table 6 . We also provide more examples for codeword localization in Figure 7 . ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 22,
                        "end": 23,
                        "text": "2",
                        "ref_id": "TABREF2"
                    },
                    {
                        "start": 125,
                        "end": 126,
                        "text": "6",
                        "ref_id": "TABREF5"
                    },
                    {
                        "start": 195,
                        "end": 196,
                        "text": "7",
                        "ref_id": "FIGREF7"
                    }
                ],
                "eq_spans": [],
                "section": "G Additional Codeword Correspondence and Localization Examples",
                "sec_num": null
            },
            {
                "text": "While we used dot product throughtout this work, we also found euclidean distance works well in practice.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "This research was supported in part by the MIT-IBM Watson AI Lab and its member companies, Nexplore and Woodside, and by MIT Lincoln Laboratory.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgements",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Self-supervised multimodal versatile networks",
                "authors": [
                    {
                        "first": "Jean-Baptiste",
                        "middle": [],
                        "last": "Alayrac",
                        "suffix": ""
                    },
                    {
                        "first": "Adri\u00e0",
                        "middle": [],
                        "last": "Recasens",
                        "suffix": ""
                    },
                    {
                        "first": "Rosalia",
                        "middle": [],
                        "last": "Schneider",
                        "suffix": ""
                    },
                    {
                        "first": "Relja",
                        "middle": [],
                        "last": "Arandjelovi\u0107",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Ramapuram",
                        "suffix": ""
                    },
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "De Fauw",
                        "suffix": ""
                    },
                    {
                        "first": "Lucas",
                        "middle": [],
                        "last": "Smaira",
                        "suffix": ""
                    },
                    {
                        "first": "Sander",
                        "middle": [],
                        "last": "Dieleman",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Zisserman",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2006.16228"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jean-Baptiste Alayrac, Adri\u00e0 Recasens, Rosalia Schnei- der, Relja Arandjelovi\u0107, Jason Ramapuram, Jeffrey De Fauw, Lucas Smaira, Sander Dieleman, and An- drew Zisserman. 2020. Self-supervised multimodal versatile networks. arXiv preprint arXiv:2006.16228.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "G\u00fcl Varol, and Andrew Zisserman. 2021",
                "authors": [
                    {
                        "first": "Max",
                        "middle": [],
                        "last": "Bain",
                        "suffix": ""
                    },
                    {
                        "first": "Arsha",
                        "middle": [],
                        "last": "Nagrani",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "Frozen in time: A joint video and image encoder for end-to-end retrieval",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2104.00650"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zis- serman. 2021. Frozen in time: A joint video and im- age encoder for end-to-end retrieval. arXiv preprint arXiv:2104.00650.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Estimating or propagating gradients through stochastic neurons for conditional computation",
                "authors": [
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    },
                    {
                        "first": "Nicholas",
                        "middle": [],
                        "last": "L\u00e9onard",
                        "suffix": ""
                    },
                    {
                        "first": "Aaron",
                        "middle": [],
                        "last": "Courville",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1308.3432"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yoshua Bengio, Nicholas L\u00e9onard, and Aaron Courville. 2013. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Grounding spoken words in unlabeled video",
                "authors": [
                    {
                        "first": "Angie",
                        "middle": [
                            "W"
                        ],
                        "last": "Boggust",
                        "suffix": ""
                    },
                    {
                        "first": "Kartik",
                        "middle": [],
                        "last": "Audhkhasi",
                        "suffix": ""
                    },
                    {
                        "first": "Dhiraj",
                        "middle": [],
                        "last": "Joshi",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Harwath",
                        "suffix": ""
                    },
                    {
                        "first": "Samuel",
                        "middle": [],
                        "last": "Thomas",
                        "suffix": ""
                    },
                    {
                        "first": "Rog\u00e9rio",
                        "middle": [],
                        "last": "Schmidt Feris",
                        "suffix": ""
                    },
                    {
                        "first": "Danny",
                        "middle": [],
                        "last": "Gutfreund",
                        "suffix": ""
                    },
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Antonio",
                        "middle": [],
                        "last": "Torralba",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Picheny",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "CVPR Workshops",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Angie W Boggust, Kartik Audhkhasi, Dhiraj Joshi, David Harwath, Samuel Thomas, Rog\u00e9rio Schmidt Feris, Danny Gutfreund, Yang Zhang, Antonio Tor- ralba, Michael Picheny, et al. 2019. Grounding spo- ken words in unlabeled video. In CVPR Workshops.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Teachtext: Crossmodal generalized distillation for text-video retrieval",
                "authors": [
                    {
                        "first": "Ioana",
                        "middle": [],
                        "last": "Croitoru",
                        "suffix": ""
                    },
                    {
                        "first": "Simion-Vlad",
                        "middle": [],
                        "last": "Bogolin",
                        "suffix": ""
                    },
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Samuel",
                        "middle": [],
                        "last": "Albanie",
                        "suffix": ""
                    },
                    {
                        "first": "Marius",
                        "middle": [],
                        "last": "Leordeanu",
                        "suffix": ""
                    },
                    {
                        "first": "Jin",
                        "middle": [],
                        "last": "Hailin",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Zisserman",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2104.08271"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ioana Croitoru, Simion-Vlad Bogolin, Yang Liu, Samuel Albanie, Marius Leordeanu, Hailin Jin, and Andrew Zisserman. 2021. Teachtext: Crossmodal generalized distillation for text-video retrieval. arXiv preprint arXiv:2104.08271.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Language modeling with gated convolutional networks",
                "authors": [
                    {
                        "first": "Angela",
                        "middle": [],
                        "last": "Yann N Dauphin",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Fan",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Auli",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Grangier",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "International conference on machine learning",
                "volume": "",
                "issue": "",
                "pages": "933--941",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. 2017. Language modeling with gated con- volutional networks. In International conference on machine learning, pages 933-941. PMLR.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Imagenet: A large-scale hierarchical image database",
                "authors": [
                    {
                        "first": "Jia",
                        "middle": [],
                        "last": "Deng",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Dong",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    },
                    {
                        "first": "Li-Jia",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Kai",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Li",
                        "middle": [],
                        "last": "Fei-Fei",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "2009 IEEE conference on computer vision and pattern recognition",
                "volume": "",
                "issue": "",
                "pages": "248--255",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: A large-scale hier- archical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248-255. Ieee.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
                "authors": [
                    {
                        "first": "Alexey",
                        "middle": [],
                        "last": "Dosovitskiy",
                        "suffix": ""
                    },
                    {
                        "first": "Lucas",
                        "middle": [],
                        "last": "Beyer",
                        "suffix": ""
                    },
                    {
                        "first": "Alexander",
                        "middle": [],
                        "last": "Kolesnikov",
                        "suffix": ""
                    },
                    {
                        "first": "Dirk",
                        "middle": [],
                        "last": "Weissenborn",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaohua",
                        "middle": [],
                        "last": "Zhai",
                        "suffix": ""
                    },
                    {
                        "first": "Thomas",
                        "middle": [],
                        "last": "Unterthiner",
                        "suffix": ""
                    },
                    {
                        "first": "Mostafa",
                        "middle": [],
                        "last": "Dehghani",
                        "suffix": ""
                    },
                    {
                        "first": "Matthias",
                        "middle": [],
                        "last": "Minderer",
                        "suffix": ""
                    },
                    {
                        "first": "Georg",
                        "middle": [],
                        "last": "Heigold",
                        "suffix": ""
                    },
                    {
                        "first": "Sylvain",
                        "middle": [],
                        "last": "Gelly",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2010.11929"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Mdmmt: Multidomain multimodal transformer for video retrieval",
                "authors": [
                    {
                        "first": "Maksim",
                        "middle": [],
                        "last": "Dzabraev",
                        "suffix": ""
                    },
                    {
                        "first": "Maksim",
                        "middle": [],
                        "last": "Kalashnikov",
                        "suffix": ""
                    },
                    {
                        "first": "Stepan",
                        "middle": [],
                        "last": "Komkov",
                        "suffix": ""
                    },
                    {
                        "first": "Aleksandr",
                        "middle": [],
                        "last": "Petiushko",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2103.10699"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Maksim Dzabraev, Maksim Kalashnikov, Stepan Komkov, and Aleksandr Petiushko. 2021. Mdmmt: Multidomain multimodal transformer for video re- trieval. arXiv preprint arXiv:2103.10699.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Yaroslav Ganin and Victor Lempitsky. 2015. Unsupervised domain adaptation by backpropagation",
                "authors": [
                    {
                        "first": "Valentin",
                        "middle": [],
                        "last": "Gabeur",
                        "suffix": ""
                    },
                    {
                        "first": "Chen",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Karteek",
                        "middle": [],
                        "last": "Alahari",
                        "suffix": ""
                    },
                    {
                        "first": "Cordelia",
                        "middle": [],
                        "last": "Schmid",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "European Conference on Computer Vision (ECCV)",
                "volume": "5",
                "issue": "",
                "pages": "1180--1189",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia Schmid. 2020. Multi-modal transformer for video retrieval. In European Conference on Com- puter Vision (ECCV), volume 5. Springer. Yaroslav Ganin and Victor Lempitsky. 2015. Unsu- pervised domain adaptation by backpropagation. In International conference on machine learning, pages 1180-1189. PMLR.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Noisecontrastive estimation: A new estimation principle for unnormalized statistical models",
                "authors": [
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Gutmann",
                        "suffix": ""
                    },
                    {
                        "first": "Aapo",
                        "middle": [],
                        "last": "Hyv\u00e4rinen",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics",
                "volume": "9",
                "issue": "",
                "pages": "297--304",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Michael Gutmann and Aapo Hyv\u00e4rinen. 2010. Noise- contrastive estimation: A new estimation principle for unnormalized statistical models. In Proceedings of the Thirteenth International Conference on Artifi- cial Intelligence and Statistics, volume 9 of Proceed- ings of Machine Learning Research, pages 297-304, Chia Laguna Resort, Sardinia, Italy. PMLR.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Deep multimodal semantic embeddings for speech and images",
                "authors": [
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Harwath",
                        "suffix": ""
                    },
                    {
                        "first": "James",
                        "middle": [],
                        "last": "Glass",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)",
                "volume": "",
                "issue": "",
                "pages": "237--244",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "David Harwath and James Glass. 2015. Deep multi- modal semantic embeddings for speech and images. In 2015 IEEE Workshop on Automatic Speech Recog- nition and Understanding (ASRU), pages 237-244.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Learning word-like units from joint audio-visual analysis",
                "authors": [
                    {
                        "first": "Ieee",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Harwath",
                        "suffix": ""
                    },
                    {
                        "first": "James",
                        "middle": [
                            "R"
                        ],
                        "last": "Glass",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1701.07481"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "IEEE. David Harwath and James R Glass. 2017. Learn- ing word-like units from joint audio-visual analysis. arXiv preprint arXiv:1701.07481.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Learning hierarchical discrete linguistic units from visually-grounded speech",
                "authors": [
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Harwath",
                        "suffix": ""
                    },
                    {
                        "first": "Wei-Ning",
                        "middle": [],
                        "last": "Hsu",
                        "suffix": ""
                    },
                    {
                        "first": "James",
                        "middle": [],
                        "last": "Glass",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "International Conference on Learning Representations",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "David Harwath, Wei-Ning Hsu, and James Glass. 2020. Learning hierarchical discrete linguistic units from visually-grounded speech. In International Confer- ence on Learning Representations.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Jointly discovering visual objects and spoken words from raw sensory input",
                "authors": [
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Harwath",
                        "suffix": ""
                    },
                    {
                        "first": "Adria",
                        "middle": [],
                        "last": "Recasens",
                        "suffix": ""
                    },
                    {
                        "first": "D\u00eddac",
                        "middle": [],
                        "last": "Sur\u00eds",
                        "suffix": ""
                    },
                    {
                        "first": "Galen",
                        "middle": [],
                        "last": "Chuang",
                        "suffix": ""
                    },
                    {
                        "first": "Antonio",
                        "middle": [],
                        "last": "Torralba",
                        "suffix": ""
                    },
                    {
                        "first": "James",
                        "middle": [],
                        "last": "Glass",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the European conference on computer vision (ECCV)",
                "volume": "",
                "issue": "",
                "pages": "649--665",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "David Harwath, Adria Recasens, D\u00eddac Sur\u00eds, Galen Chuang, Antonio Torralba, and James Glass. 2018. Jointly discovering visual objects and spoken words from raw sensory input. In Proceedings of the Euro- pean conference on computer vision (ECCV), pages 649-665.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Unsupervised learning of spoken language with visual context",
                "authors": [
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Harwath",
                        "suffix": ""
                    },
                    {
                        "first": "Antonio",
                        "middle": [],
                        "last": "Torralba",
                        "suffix": ""
                    },
                    {
                        "first": "James",
                        "middle": [
                            "R"
                        ],
                        "last": "Glass",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "David Harwath, Antonio Torralba, and James R Glass. 2017. Unsupervised learning of spoken language with visual context. In Neural Information Process- ing Systems.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Deep residual learning for image recognition",
                "authors": [
                    {
                        "first": "Kaiming",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Xiangyu",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Shaoqing",
                        "middle": [],
                        "last": "Ren",
                        "suffix": ""
                    },
                    {
                        "first": "Jian",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recog- nition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Large-scale representation learning from visually grounded untranscribed speech",
                "authors": [
                    {
                        "first": "Gabriel",
                        "middle": [],
                        "last": "Ilharco",
                        "suffix": ""
                    },
                    {
                        "first": "Yuan",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Baldridge",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1909.08782"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Gabriel Ilharco, Yuan Zhang, and Jason Baldridge. 2019. Large-scale representation learning from visu- ally grounded untranscribed speech. arXiv preprint arXiv:1909.08782.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Semantic speech retrieval with a visually grounded model of untranscribed speech",
                "authors": [
                    {
                        "first": "Herman",
                        "middle": [],
                        "last": "Kamper",
                        "suffix": ""
                    },
                    {
                        "first": "Gregory",
                        "middle": [],
                        "last": "Shakhnarovich",
                        "suffix": ""
                    },
                    {
                        "first": "Karen",
                        "middle": [],
                        "last": "Livescu",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops",
                "volume": "",
                "issue": "",
                "pages": "2514--2517",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Herman Kamper, Gregory Shakhnarovich, and Karen Livescu. 2018. Semantic speech retrieval with a vi- sually grounded model of untranscribed speech. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 2514-2517.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Autoencoding variational bayes",
                "authors": [
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Diederik",
                        "suffix": ""
                    },
                    {
                        "first": "Max",
                        "middle": [],
                        "last": "Kingma",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Welling",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1312.6114"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Diederik P Kingma and Max Welling. 2013. Auto- encoding variational bayes. arXiv preprint arXiv:1312.6114.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Less is more: Clipbert for video-and-language learning via sparse sampling",
                "authors": [
                    {
                        "first": "Jie",
                        "middle": [],
                        "last": "Lei",
                        "suffix": ""
                    },
                    {
                        "first": "Linjie",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Luowei",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Zhe",
                        "middle": [],
                        "last": "Gan",
                        "suffix": ""
                    },
                    {
                        "first": "Tamara",
                        "middle": [
                            "L"
                        ],
                        "last": "Berg",
                        "suffix": ""
                    },
                    {
                        "first": "Mohit",
                        "middle": [],
                        "last": "Bansal",
                        "suffix": ""
                    },
                    {
                        "first": "Jingjing",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2102.06183"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L Berg, Mohit Bansal, and Jingjing Liu. 2021. Less is more: Clipbert for video-and-language learning via sparse sampling. arXiv preprint arXiv:2102.06183.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Tsm: Temporal shift module for efficient video understanding",
                "authors": [
                    {
                        "first": "Ji",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Chuang",
                        "middle": [],
                        "last": "Gan",
                        "suffix": ""
                    },
                    {
                        "first": "Song",
                        "middle": [],
                        "last": "Han",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision",
                "volume": "",
                "issue": "",
                "pages": "7083--7093",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ji Lin, Chuang Gan, and Song Han. 2019. Tsm: Tem- poral shift module for efficient video understanding. In Proceedings of the IEEE/CVF International Con- ference on Computer Vision, pages 7083-7093.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Hit: Hierarchical transformer with momentum contrast for video-text retrieval",
                "authors": [
                    {
                        "first": "Song",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Haoqi",
                        "middle": [],
                        "last": "Fan",
                        "suffix": ""
                    },
                    {
                        "first": "Shengsheng",
                        "middle": [],
                        "last": "Qian",
                        "suffix": ""
                    },
                    {
                        "first": "Yiru",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Wenkui",
                        "middle": [],
                        "last": "Ding",
                        "suffix": ""
                    },
                    {
                        "first": "Zhongyuan",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2103.15049"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Song Liu, Haoqi Fan, Shengsheng Qian, Yiru Chen, Wenkui Ding, and Zhongyuan Wang. 2021. Hit: Hierarchical transformer with momentum contrast for video-text retrieval. arXiv preprint arXiv:2103.15049.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Improving referring expression grounding with cross-modal attention-guided erasing",
                "authors": [
                    {
                        "first": "Xihui",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Zihao",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Jing",
                        "middle": [],
                        "last": "Shao",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaogang",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Hongsheng",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xihui Liu, Zihao Wang, Jing Shao, Xiaogang Wang, and Hongsheng Li. 2019a. Improving referring expres- sion grounding with cross-modal attention-guided erasing. In Proceedings of the IEEE/CVF Confer- ence on Computer Vision and Pattern Recognition (CVPR).",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Use what you have: Video retrieval using representations from collaborative experts",
                "authors": [
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Samuel",
                        "middle": [],
                        "last": "Albanie",
                        "suffix": ""
                    },
                    {
                        "first": "Arsha",
                        "middle": [],
                        "last": "Nagrani",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Zisserman",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1907.13487"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yang Liu, Samuel Albanie, Arsha Nagrani, and An- drew Zisserman. 2019b. Use what you have: Video retrieval using representations from collaborative ex- perts. arXiv preprint arXiv:1907.13487.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Clip4clip: An empirical study of clip for end to end video clip retrieval",
                "authors": [
                    {
                        "first": "Huaishao",
                        "middle": [],
                        "last": "Luo",
                        "suffix": ""
                    },
                    {
                        "first": "Lei",
                        "middle": [],
                        "last": "Ji",
                        "suffix": ""
                    },
                    {
                        "first": "Ming",
                        "middle": [],
                        "last": "Zhong",
                        "suffix": ""
                    },
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Wen",
                        "middle": [],
                        "last": "Lei",
                        "suffix": ""
                    },
                    {
                        "first": "Nan",
                        "middle": [],
                        "last": "Duan",
                        "suffix": ""
                    },
                    {
                        "first": "Tianrui",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2104.08860"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li. 2021. Clip4clip: An empirical study of clip for end to end video clip retrieval. arXiv preprint arXiv:2104.08860.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "End-to-end learning of visual representations from uncurated instructional videos",
                "authors": [
                    {
                        "first": "Antoine",
                        "middle": [],
                        "last": "Miech",
                        "suffix": ""
                    },
                    {
                        "first": "Jean-Baptiste",
                        "middle": [],
                        "last": "Alayrac",
                        "suffix": ""
                    },
                    {
                        "first": "Lucas",
                        "middle": [],
                        "last": "Smaira",
                        "suffix": ""
                    },
                    {
                        "first": "Ivan",
                        "middle": [],
                        "last": "Laptev",
                        "suffix": ""
                    },
                    {
                        "first": "Josef",
                        "middle": [],
                        "last": "Sivic",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Zisserman",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
                "volume": "",
                "issue": "",
                "pages": "9879--9889",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan Laptev, Josef Sivic, and Andrew Zisserman. 2020. End-to-end learning of visual representations from uncurated instructional videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9879-9889.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Learning a text-video embedding from incomplete and heterogeneous data",
                "authors": [
                    {
                        "first": "Antoine",
                        "middle": [],
                        "last": "Miech",
                        "suffix": ""
                    },
                    {
                        "first": "Ivan",
                        "middle": [],
                        "last": "Laptev",
                        "suffix": ""
                    },
                    {
                        "first": "Josef",
                        "middle": [],
                        "last": "Sivic",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1804.02516"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Antoine Miech, Ivan Laptev, and Josef Sivic. 2018. Learning a text-video embedding from incom- plete and heterogeneous data. arXiv preprint arXiv:1804.02516.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Howto100m: Learning a text-video embedding by watching hundred million narrated video clips",
                "authors": [
                    {
                        "first": "Antoine",
                        "middle": [],
                        "last": "Miech",
                        "suffix": ""
                    },
                    {
                        "first": "Dimitri",
                        "middle": [],
                        "last": "Zhukov",
                        "suffix": ""
                    },
                    {
                        "first": "Jean-Baptiste",
                        "middle": [],
                        "last": "Alayrac",
                        "suffix": ""
                    },
                    {
                        "first": "Makarand",
                        "middle": [],
                        "last": "Tapaswi",
                        "suffix": ""
                    },
                    {
                        "first": "Ivan",
                        "middle": [],
                        "last": "Laptev",
                        "suffix": ""
                    },
                    {
                        "first": "Josef",
                        "middle": [],
                        "last": "Sivic",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision",
                "volume": "",
                "issue": "",
                "pages": "2630--2640",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. 2019. Howto100m: Learning a text-video embed- ding by watching hundred million narrated video clips. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2630-2640.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Spoken moments: Learning joint audiovisual representations from video descriptions",
                "authors": [
                    {
                        "first": "Mathew",
                        "middle": [],
                        "last": "Monfort",
                        "suffix": ""
                    },
                    {
                        "first": "Souyoung",
                        "middle": [],
                        "last": "Jin",
                        "suffix": ""
                    },
                    {
                        "first": "Alexander",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Harwath",
                        "suffix": ""
                    },
                    {
                        "first": "Rogerio",
                        "middle": [],
                        "last": "Feris",
                        "suffix": ""
                    },
                    {
                        "first": "James",
                        "middle": [],
                        "last": "Glass",
                        "suffix": ""
                    },
                    {
                        "first": "Aude",
                        "middle": [],
                        "last": "Oliva",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2105.04489"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Mathew Monfort, SouYoung Jin, Alexander Liu, David Harwath, Rogerio Feris, James Glass, and Aude Oliva. 2021. Spoken moments: Learning joint audio- visual representations from video descriptions. arXiv preprint arXiv:2105.04489.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "Multi-moments in time: Learning and interpreting models for multi-action video understanding",
                "authors": [
                    {
                        "first": "Mathew",
                        "middle": [],
                        "last": "Monfort",
                        "suffix": ""
                    },
                    {
                        "first": "Kandan",
                        "middle": [],
                        "last": "Ramakrishnan",
                        "suffix": ""
                    },
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Andonian",
                        "suffix": ""
                    },
                    {
                        "first": "Barry",
                        "middle": [
                            "A"
                        ],
                        "last": "Mcnamara",
                        "suffix": ""
                    },
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Lascelles",
                        "suffix": ""
                    },
                    {
                        "first": "Quanfu",
                        "middle": [],
                        "last": "Bowen Pan",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Fan",
                        "suffix": ""
                    },
                    {
                        "first": "Rogerio",
                        "middle": [],
                        "last": "Gutfreund",
                        "suffix": ""
                    },
                    {
                        "first": "Aude",
                        "middle": [],
                        "last": "Feris",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Oliva",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1911.00232"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Mathew Monfort, Kandan Ramakrishnan, Alex Ando- nian, Barry A McNamara, Alex Lascelles, Bowen Pan, Quanfu Fan, Dan Gutfreund, Rogerio Feris, and Aude Oliva. 2019. Multi-moments in time: Learn- ing and interpreting models for multi-action video understanding. arXiv preprint arXiv:1911.00232.",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu",
                "authors": [
                    {
                        "first": "Aaron",
                        "middle": [],
                        "last": "Van Den Oord",
                        "suffix": ""
                    },
                    {
                        "first": "Yazhe",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Oriol",
                        "middle": [],
                        "last": "Vinyals",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Representation learning with contrastive predictive coding",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1807.03748"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748. Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. 2017. Neural discrete representation learning. arXiv preprint arXiv:1711.00937.",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "Support-set bottlenecks for video-text representation learning",
                "authors": [
                    {
                        "first": "Mandela",
                        "middle": [],
                        "last": "Patrick",
                        "suffix": ""
                    },
                    {
                        "first": "Po-Yao",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Yuki",
                        "middle": [],
                        "last": "Asano",
                        "suffix": ""
                    },
                    {
                        "first": "Florian",
                        "middle": [],
                        "last": "Metze",
                        "suffix": ""
                    },
                    {
                        "first": "Alexander",
                        "middle": [],
                        "last": "Hauptmann",
                        "suffix": ""
                    },
                    {
                        "first": "Jo\u00e3o",
                        "middle": [],
                        "last": "Henriques",
                        "suffix": ""
                    },
                    {
                        "first": "Andrea",
                        "middle": [],
                        "last": "Vedaldi",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2010.02824"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Mandela Patrick, Po-Yao Huang, Yuki Asano, Florian Metze, Alexander Hauptmann, Jo\u00e3o Henriques, and Andrea Vedaldi. 2020. Support-set bottlenecks for video-text representation learning. arXiv preprint arXiv:2010.02824.",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "Learning transferable visual models from natural language supervision",
                "authors": [
                    {
                        "first": "Alec",
                        "middle": [],
                        "last": "Radford",
                        "suffix": ""
                    },
                    {
                        "first": "Jong",
                        "middle": [
                            "Wook"
                        ],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Hallacy",
                        "suffix": ""
                    },
                    {
                        "first": "Aditya",
                        "middle": [],
                        "last": "Ramesh",
                        "suffix": ""
                    },
                    {
                        "first": "Gabriel",
                        "middle": [],
                        "last": "Goh",
                        "suffix": ""
                    },
                    {
                        "first": "Sandhini",
                        "middle": [],
                        "last": "Agarwal",
                        "suffix": ""
                    },
                    {
                        "first": "Girish",
                        "middle": [],
                        "last": "Sastry",
                        "suffix": ""
                    },
                    {
                        "first": "Amanda",
                        "middle": [],
                        "last": "Askell",
                        "suffix": ""
                    },
                    {
                        "first": "Pamela",
                        "middle": [],
                        "last": "Mishkin",
                        "suffix": ""
                    },
                    {
                        "first": "Jack",
                        "middle": [],
                        "last": "Clark",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2103.00020"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sas- try, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020.",
                "links": null
            },
            "BIBREF34": {
                "ref_id": "b34",
                "title": "Language models are unsupervised multitask learners",
                "authors": [
                    {
                        "first": "Alec",
                        "middle": [],
                        "last": "Radford",
                        "suffix": ""
                    },
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Rewon",
                        "middle": [],
                        "last": "Child",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Luan",
                        "suffix": ""
                    },
                    {
                        "first": "Dario",
                        "middle": [],
                        "last": "Amodei",
                        "suffix": ""
                    },
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "OpenAI blog",
                "volume": "1",
                "issue": "8",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.",
                "links": null
            },
            "BIBREF35": {
                "ref_id": "b35",
                "title": "Avlnet: Learning audio-visual language representations from instructional videos",
                "authors": [
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Rouditchenko",
                        "suffix": ""
                    },
                    {
                        "first": "Angie",
                        "middle": [],
                        "last": "Boggust",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Harwath",
                        "suffix": ""
                    },
                    {
                        "first": "Brian",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Dhiraj",
                        "middle": [],
                        "last": "Joshi",
                        "suffix": ""
                    },
                    {
                        "first": "Samuel",
                        "middle": [],
                        "last": "Thomas",
                        "suffix": ""
                    },
                    {
                        "first": "Kartik",
                        "middle": [],
                        "last": "Audhkhasi",
                        "suffix": ""
                    },
                    {
                        "first": "Hilde",
                        "middle": [],
                        "last": "Kuehne",
                        "suffix": ""
                    },
                    {
                        "first": "Rameswar",
                        "middle": [],
                        "last": "Panda",
                        "suffix": ""
                    },
                    {
                        "first": "Rogerio",
                        "middle": [],
                        "last": "Feris",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2006.09199"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Andrew Rouditchenko, Angie Boggust, David Harwath, Brian Chen, Dhiraj Joshi, Samuel Thomas, Kartik Audhkhasi, Hilde Kuehne, Rameswar Panda, Rogerio Feris, et al. 2020. Avlnet: Learning audio-visual language representations from instructional videos. arXiv preprint arXiv:2006.09199.",
                "links": null
            },
            "BIBREF36": {
                "ref_id": "b36",
                "title": "Facenet: A unified embedding for face recognition and clustering",
                "authors": [
                    {
                        "first": "Florian",
                        "middle": [],
                        "last": "Schroff",
                        "suffix": ""
                    },
                    {
                        "first": "Dmitry",
                        "middle": [],
                        "last": "Kalenichenko",
                        "suffix": ""
                    },
                    {
                        "first": "James",
                        "middle": [],
                        "last": "Philbin",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
                "volume": "",
                "issue": "",
                "pages": "815--823",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Florian Schroff, Dmitry Kalenichenko, and James Philbin. 2015. Facenet: A unified embedding for face recognition and clustering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 815-823.",
                "links": null
            },
            "BIBREF37": {
                "ref_id": "b37",
                "title": "Learning words from images and speech",
                "authors": [
                    {
                        "first": "Maarten",
                        "middle": [],
                        "last": "Gabriel Synnaeve",
                        "suffix": ""
                    },
                    {
                        "first": "Emmanuel",
                        "middle": [],
                        "last": "Versteegh",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Dupoux",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "NIPS Workshop Learn. Semantics. Citeseer",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Gabriel Synnaeve, Maarten Versteegh, and Emmanuel Dupoux. 2014. Learning words from images and speech. In NIPS Workshop Learn. Semantics. Cite- seer.",
                "links": null
            },
            "BIBREF38": {
                "ref_id": "b38",
                "title": "Attention is all you need",
                "authors": [
                    {
                        "first": "Ashish",
                        "middle": [],
                        "last": "Vaswani",
                        "suffix": ""
                    },
                    {
                        "first": "Noam",
                        "middle": [],
                        "last": "Shazeer",
                        "suffix": ""
                    },
                    {
                        "first": "Niki",
                        "middle": [],
                        "last": "Parmar",
                        "suffix": ""
                    },
                    {
                        "first": "Jakob",
                        "middle": [],
                        "last": "Uszkoreit",
                        "suffix": ""
                    },
                    {
                        "first": "Llion",
                        "middle": [],
                        "last": "Jones",
                        "suffix": ""
                    },
                    {
                        "first": "Aidan",
                        "middle": [
                            "N"
                        ],
                        "last": "Gomez",
                        "suffix": ""
                    },
                    {
                        "first": "Lukasz",
                        "middle": [],
                        "last": "Kaiser",
                        "suffix": ""
                    },
                    {
                        "first": "Illia",
                        "middle": [],
                        "last": "Polosukhin",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1706.03762"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. arXiv preprint arXiv:1706.03762.",
                "links": null
            },
            "BIBREF39": {
                "ref_id": "b39",
                "title": "Multimodal word discovery and retrieval with phone sequence and image concepts",
                "authors": [
                    {
                        "first": "Liming",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Mark",
                        "middle": [
                            "A"
                        ],
                        "last": "Hasegawa-Johnson",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Liming Wang and Mark A Hasegawa-Johnson. 2019. Multimodal word discovery and retrieval with phone sequence and image concepts. In INTERSPEECH.",
                "links": null
            },
            "BIBREF40": {
                "ref_id": "b40",
                "title": "Msrvtt: A large video description dataset for bridging video and language",
                "authors": [
                    {
                        "first": "Jun",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Tao",
                        "middle": [],
                        "last": "Mei",
                        "suffix": ""
                    },
                    {
                        "first": "Ting",
                        "middle": [],
                        "last": "Yao",
                        "suffix": ""
                    },
                    {
                        "first": "Yong",
                        "middle": [],
                        "last": "Rui",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
                "volume": "",
                "issue": "",
                "pages": "5288--5296",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jun Xu, Tao Mei, Ting Yao, and Yong Rui. 2016. Msr- vtt: A large video description dataset for bridging video and language. In Proceedings of the IEEE con- ference on computer vision and pattern recognition, pages 5288-5296.",
                "links": null
            },
            "BIBREF41": {
                "ref_id": "b41",
                "title": "A joint sequence fusion model for video question answering and retrieval",
                "authors": [
                    {
                        "first": "Youngjae",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Jongseok",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "Gunhee",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the European Conference on Computer Vision (ECCV)",
                "volume": "",
                "issue": "",
                "pages": "471--487",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Youngjae Yu, Jongseok Kim, and Gunhee Kim. 2018. A joint sequence fusion model for video question answering and retrieval. In Proceedings of the Euro- pean Conference on Computer Vision (ECCV), pages 471-487.",
                "links": null
            },
            "BIBREF42": {
                "ref_id": "b42",
                "title": "Learning deep features for scene recognition using places database",
                "authors": [
                    {
                        "first": "Bolei",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Agata",
                        "middle": [],
                        "last": "Lapedriza",
                        "suffix": ""
                    },
                    {
                        "first": "Jianxiong",
                        "middle": [],
                        "last": "Xiao",
                        "suffix": ""
                    },
                    {
                        "first": "Antonio",
                        "middle": [],
                        "last": "Torralba",
                        "suffix": ""
                    },
                    {
                        "first": "Aude",
                        "middle": [],
                        "last": "Oliva",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "NeurIPS. Table 5: Full comparison against prior works on MSR-VTT text-to-video retrieval. Method Video Retrieval",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, and Aude Oliva. 2014. Learning deep fea- tures for scene recognition using places database. In NeurIPS. Table 5: Full comparison against prior works on MSR-VTT text-to-video retrieval. Method Video Retrieval (Text \u2192 Video)",
                "links": null
            },
            "BIBREF43": {
                "ref_id": "b43",
                "title": "R@1 \u2191 R@5 \u2191 R@10 \u2191 MnR \u2193 Collaborative Experts",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "",
                "volume": "8",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "R@1 \u2191 R@5 \u2191 R@10 \u2191 MnR \u2193 Collaborative Experts (Liu et al., 2019b) 20.9 48.8 62.4 28.2 Multi-Modal Transformer (Gabeur et al., 2020)",
                "links": null
            },
            "BIBREF44": {
                "ref_id": "b44",
                "title": "Hierarchical Transformer with Momentum Contrast",
                "authors": [
                    {
                        "first": "Support-Set",
                        "middle": [],
                        "last": "Bottlenecks",
                        "suffix": ""
                    },
                    {
                        "first": "(",
                        "middle": [],
                        "last": "Patrick",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "5",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Support-Set Bottlenecks (Patrick et al., 2020) 30.1 58.5 69.3 - Multidomain Multimodal Transformer (Dzabraev et al., 2021) 38.9 69.0 79.7 16.5 Frozen-in-Time (Bain et al., 2021) 31.0 59.5 70.5 - Hierarchical Transformer with Momentum Contrast (Liu et al., 2021) 30.7 60.9 73.2 - TeachText (Croitoru et al., 2021) 29.6 61.6 74.2 - CLIP4Clip-meanPooling (Luo et al., 2021) 43.1 70.4 80.8 16.2 CLIP4Clip-seqLSTM (Luo et al., 2021) 42.5 70.8 80.7 16.7 CLIP4Clip-seqTransformer (Luo et al., 2021) 44.5 71.4 81.6 15.3 CLIP4Clip-tightTransformer (Luo et al., 2021) 40.2 71.5 80.5 13.4 Our Baseline (based on CLIP4Clip-meanPooling)",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 2: Our proposed Cross-Modal Code Matching objective (described in Section 2.3), which encourages the model to use similar codewords for matching cross-modal pairs.",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Figure 3: Conditional probability matrix illustrating P (action|codeword) on the S-MiT development set. Y-axis is action label, showing only the top 20 most frequent labels for simplicity. X-axis is the indices of the top 100 most frequent codewords.",
                "uris": null,
                "fig_num": "3",
                "type_str": "figure"
            },
            "FIGREF3": {
                "num": null,
                "text": "Figure 4: Codeword cross-modal localization. Input regions that are encoded by the codeword (selected from Table2) are highlighted in red.",
                "uris": null,
                "fig_num": "4",
                "type_str": "figure"
            },
            "FIGREF4": {
                "num": null,
                "text": "Figure 5: Conditional probability matrix between codewords and action labels learned by our proposed method when the Cross-Modal Code Matching objective is excluded.",
                "uris": null,
                "fig_num": "5",
                "type_str": "figure"
            },
            "FIGREF6": {
                "num": null,
                "text": "Figure 6: T-SNE visualization of the codebook with and without the proposed Cross-Modal Code Matching Objective.Each point corresponds to a codeword colored with respect to the input modality that utilized it the most. Codewords without high (> 90%) usage from single modality are labeled as \"jointly used\".",
                "uris": null,
                "fig_num": "6",
                "type_str": "figure"
            },
            "FIGREF7": {
                "num": null,
                "text": "Figure 7: More examples for codeword cross-modal localization.",
                "uris": null,
                "fig_num": "7",
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table><tr><td>Modality A-B / Dataset</td><td>Visual Retrieval</td><td>Language Retrieval</td></tr><tr><td>Method</td><td/><td/></tr></table>",
                "type_str": "table",
                "text": "Cross-Modal retrieval results on S-MiT, Places, and MSR-VTT.",
                "html": null,
                "num": null
            },
            "TABREF1": {
                "content": "<table><tr><td>MiT (Monfort et al., 2021)</td><td/><td/><td/><td/><td/><td/><td/><td/></tr><tr><td>S-MiT (Monfort et al., 2021)</td><td>32.1</td><td>58.9</td><td>68.6</td><td>-</td><td>32.3</td><td>57.9</td><td>68.1</td><td>-</td></tr><tr><td>Our Baseline \u2020</td><td>30.2</td><td>57.3</td><td>68.5</td><td>41.9</td><td>29.7</td><td>57.2</td><td>68.7</td><td>28.5</td></tr><tr><td>Proposed</td><td>34.3</td><td>61.3</td><td>72.0</td><td>33.5</td><td>34.0</td><td>61.6</td><td>71.7</td><td>22.5</td></tr><tr><td>Image-Audio / Places (Harwath et al., 2017)</td><td/><td/><td/><td/><td/><td/><td/><td/></tr><tr><td>ResDAVEnet (Harwath et al., 2018)*</td><td>30.9</td><td>63.6</td><td>74.2</td><td>20.2</td><td>26.4</td><td>58.5</td><td>71.2</td><td>21.6</td></tr><tr><td>ResDAVEnet-VQ (Harwath et al., 2020)*</td><td>34.9</td><td>70.2</td><td>79.4</td><td>15.0</td><td>32.7</td><td>65.6</td><td>77.0</td><td>18.0</td></tr><tr><td>Our Baseline \u2020</td><td>43.8</td><td>74.1</td><td>82.4</td><td>15.8</td><td>40.4</td><td>73.3</td><td>82.5</td><td>10.9</td></tr><tr><td>Proposed</td><td>46.5</td><td>77.4</td><td>85.8</td><td>13.7</td><td>45.4</td><td>77.7</td><td>85.9</td><td>8.9</td></tr><tr><td>Video-Text / MSR-VTT (Xu et al., 2016)</td><td/><td/><td/><td/><td/><td/><td/><td/></tr><tr><td>Frozen-in-Time (Bain et al., 2021)</td><td>31.0</td><td>59.5</td><td>70.5</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>CLIP4Clip-meanP (Luo et al., 2021)</td><td>43.1</td><td>70.4</td><td>80.8</td><td>16.2</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>CLIP4Clip-tightT (Luo et al., 2021)</td><td>40.2</td><td>71.5</td><td>80.5</td><td>13.4</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Our Baseline \u2020</td><td>42.6</td><td>71.2</td><td>80.8</td><td>15.5</td><td>43.0</td><td>70.9</td><td>80.9</td><td>12.5</td></tr><tr><td>Proposed</td><td>43.4</td><td>72.3</td><td>81.2</td><td>14.8</td><td>42.5</td><td>71.2</td><td>81.1</td><td>12.0</td></tr><tr><td/><td/><td/><td/><td/><td/><td/><td/><td/></tr></table>",
                "type_str": "table",
                "text": "\u2020 Existing model reproduced with LMMS for fair comparison, see Table3in the Appendix for more detail. * Results obtained by running the official code and pre-trained models, see Appendix for more details.",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table><tr><td/><td/><td/><td/><td colspan=\"2\">Visual Action</td><td/><td/><td colspan=\"2\">Spoken word</td><td/></tr><tr><td colspan=\"3\">Rank Code Occ.</td><td colspan=\"4\">Top Hypothesis Second Hypothesis</td><td colspan=\"2\">Top Hypothesis</td><td colspan=\"2\">Second Hypothesis</td></tr><tr><td/><td/><td/><td>Label</td><td>Prc.</td><td>Label</td><td>Prc.</td><td>Word</td><td>F1</td><td>Word</td><td>F1</td></tr><tr><td>1</td><td>201</td><td>147</td><td>juggling</td><td>97.5</td><td>kicking</td><td>1.2</td><td>juggling</td><td>36.7</td><td>juggles</td><td>8.3</td></tr><tr><td>2</td><td>349</td><td>112</td><td>flossing</td><td>96.0</td><td>licking</td><td>0.7</td><td>floss</td><td>15.8</td><td>flossing</td><td>14.0</td></tr><tr><td>3</td><td>145</td><td>49</td><td>surfing</td><td>95.6</td><td>snowing</td><td>2.9</td><td>surfboard</td><td>23.7</td><td>waves</td><td>7.3</td></tr><tr><td>4</td><td>29</td><td>64</td><td colspan=\"2\">tattooing 94.6</td><td>injecting</td><td>2.2</td><td>tattoo</td><td>15.8</td><td>tattooed</td><td>4.2</td></tr><tr><td>5</td><td>233</td><td>25</td><td>ironing</td><td colspan=\"3\">93.8 hammering 6.2</td><td>ironing</td><td>20.5</td><td>iron</td><td>4.7</td></tr><tr><td/><td/><td/><td/><td/><td>...</td><td/><td/><td/><td/><td/></tr><tr><td>32</td><td>500</td><td>89</td><td>dialing</td><td>60.0</td><td>texting</td><td>10.0</td><td>dialing</td><td>13.8</td><td>phone</td><td>9.8</td></tr><tr><td>33</td><td>536</td><td>28</td><td colspan=\"2\">cheering 60.0</td><td>shouting</td><td colspan=\"5\">10.0 cheerleaders 26.8 cheerleading 10.3</td></tr><tr><td>34</td><td>50</td><td>203</td><td>rafting</td><td>58.6</td><td>paddling</td><td>25.7</td><td>rafting</td><td>16.7</td><td>raft</td><td>8.5</td></tr><tr><td>35</td><td>664</td><td>78</td><td>dunking</td><td>58.0</td><td>leaping</td><td>9.1</td><td>basketball</td><td>11.0</td><td>dunking</td><td>5.2</td></tr><tr><td/><td/><td/><td/><td/><td>...</td><td/><td/><td/><td/><td/></tr><tr><td>742</td><td>733</td><td colspan=\"4\">188 discussing 6.5 applauding</td><td>4.6</td><td>men</td><td>7.3</td><td>two</td><td>6.4</td></tr><tr><td>743</td><td>542</td><td>58</td><td>baking</td><td>6.5</td><td>peeling</td><td>5.2</td><td>cupcake</td><td>9.2</td><td>peanut</td><td>6.2</td></tr></table>",
                "type_str": "table",
                "text": "Correspondence between codewords, visual actions, and spoken words. Ranking is based on the precision (Prc.) of the top hypothesis of the visual action label. Occurrence (Occ.) indicates the number of times the codeword was activated throughout the development set. Around 750 codewords were activated on the development set. An extended table is available in Section G of the Appendix.",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table><tr><td>Setup</td><td>Modality</td><td colspan=\"2\">Encoders from baseline model</td></tr><tr><td>Dataset</td><td>A</td><td>f A fine</td><td>f A high</td></tr><tr><td>-Baseline model</td><td>B</td><td>f B fine</td><td>f B high</td></tr><tr><td>S-MiT (Monfort et al., 2021)</td><td>video</td><td>ResNet-152 4 (He et al., 2016) + TSM 5 (Lin et al., 2019)</td><td>Max Pooling + GLU (Dauphin et al., 2017)</td></tr><tr><td>-AVLnet (Rouditchenko et al., 2020)</td><td>audio</td><td>Spectrogram+1D-ResNet (Harwath et al., 2018)</td><td>Avg. Pooling + GLU (Dauphin et al., 2017)</td></tr><tr><td>Places (Harwath et al., 2017)</td><td>image</td><td>ResNet-50 4 (He et al., 2016)</td><td>Avg. Pooling + GLU (Dauphin et al., 2017)</td></tr><tr><td>-ResDAVEnet 2 (Harwath et al., 2018)</td><td>audio</td><td>Spectrogram+1D-ResNet (Harwath et al., 2018)</td><td>Avg. Pooling + GLU (Dauphin et al., 2017)</td></tr><tr><td>MSR-VTT (Xu et al., 2016)</td><td>video</td><td>Vision Transformer 3 (Dosovitskiy et al., 2020)</td><td>Avg. Pooling + Linear</td></tr><tr><td>-CLIP4Clip 1 (Luo et al., 2021)</td><td>text</td><td colspan=\"2\">Transformer 3 (Vaswani et al., 2017; Radford et al., 2019) [EOT] token + Linear</td></tr><tr><td colspan=\"3\">1 https://github.com/ArrowLuo/CLIP4Clip</td><td/></tr><tr><td colspan=\"3\">2 https://github.com/wnhsu/ResDAVEnet-VQ (under BSD license)</td><td/></tr><tr><td>3</td><td/><td/><td/></tr></table>",
                "type_str": "table",
                "text": "Experiment setup on S-MiT, Places, and MSR-VTT. Initialized from CLIP model pretrained on WebImageText dataset(Radford et al., 2021).4 Pretrained on ImageNet(Deng et al., 2009).5 Pretrained on Multi-MiT(Monfort et al., 2019).",
                "html": null,
                "num": null
            },
            "TABREF4": {
                "content": "<table><tr><td>Method</td><td colspan=\"4\">Averaged 2-way Retrieval R@1 \u2191 R@5 \u2191 R@10 \u2191 MnR \u2193</td></tr><tr><td>(a) Proposed</td><td>46.0</td><td>77.6</td><td>85.9</td><td>11.3</td></tr><tr><td>(b) codebook size = 512</td><td>46.2</td><td>77.4</td><td>85.2</td><td>11.5</td></tr><tr><td>(c) codebook size = 2048</td><td>46.1</td><td>76.6</td><td>84.7</td><td>12.1</td></tr><tr><td>(d) \u03b1 = 1.0</td><td>45.6</td><td>76.6</td><td>85.5</td><td>11.6</td></tr><tr><td>(e) \u03b1 = 0.0 (w/o CMCM)</td><td>45.2</td><td>75.5</td><td>84.2</td><td>12.8</td></tr><tr><td>(f) w/o VQ &amp; w/o CMCM</td><td>45.7</td><td>75.9</td><td>84.7</td><td>12.6</td></tr><tr><td>(g) w/o warm-start</td><td>41.6</td><td>73.4</td><td>82.5</td><td>16.0</td></tr><tr><td>(h) w/o cont. repr. (f M high (H M i ))</td><td>29.0</td><td>63.0</td><td>74.7</td><td>19.4</td></tr><tr><td>(i) Our Baseline</td><td>42.1</td><td>73.7</td><td>82.5</td><td>13.4</td></tr></table>",
                "type_str": "table",
                "text": "Ablation study on Places(Harwath et al., 2017), scores are averaged over audio and image retrieval.",
                "html": null,
                "num": null
            },
            "TABREF5": {
                "content": "<table><tr><td/><td/><td colspan=\"2\">Visual Action</td><td>Spoken word</td><td/><td/><td/><td>Visual Action</td><td/><td colspan=\"2\">Spoken word</td></tr><tr><td colspan=\"2\">Rank Code</td><td colspan=\"2\">Top Hypothesis</td><td colspan=\"2\">Top Hypothesis</td><td colspan=\"2\">Rank Code</td><td colspan=\"2\">Top Hypothesis</td><td colspan=\"2\">Top Hypothesis</td></tr><tr><td/><td/><td>label</td><td>Prc.</td><td>word</td><td>F1</td><td/><td/><td>label</td><td>Prc.</td><td>word</td><td>F1</td></tr><tr><td>1</td><td>201</td><td>juggling</td><td>97.5</td><td>juggling</td><td>36.7</td><td>61</td><td>940</td><td>landing</td><td>44.9</td><td>airplane</td><td>19.6</td></tr><tr><td>2</td><td>349</td><td>flossing</td><td>96.0</td><td>floss</td><td>15.8</td><td>62</td><td>262</td><td>sewing</td><td>44.7</td><td>sewing</td><td>13.2</td></tr><tr><td>3</td><td>145</td><td>surfing</td><td>95.6</td><td>surfboard</td><td>23.7</td><td>63</td><td>532</td><td>autographing</td><td>44.4</td><td>selfie</td><td>22.2</td></tr><tr><td>4</td><td>29</td><td>tattooing</td><td>94.6</td><td>tattoo</td><td>15.8</td><td>64</td><td>928</td><td>stirring</td><td>44.1</td><td>boiling</td><td>27.3</td></tr><tr><td>5</td><td>233</td><td>ironing</td><td>93.8</td><td>ironing</td><td>20.5</td><td>65</td><td>747</td><td>applauding</td><td>43.8</td><td>clapping</td><td>23.8</td></tr><tr><td>6</td><td>766</td><td>surfing</td><td>93.2</td><td>surfing</td><td>22.1</td><td>66</td><td>447</td><td>paddling</td><td>43.1</td><td>boat</td><td>8.3</td></tr><tr><td>7</td><td>191</td><td>juggling</td><td>90.2</td><td>juggling</td><td>29.1</td><td>67</td><td>823</td><td>skipping</td><td>43.0</td><td>jump</td><td>17.1</td></tr><tr><td>8</td><td>753</td><td colspan=\"2\">autographing 85.0</td><td>autographs</td><td>26.4</td><td>68</td><td>308</td><td>shaving</td><td>42.5</td><td>comb</td><td>10.0</td></tr><tr><td>9</td><td>606</td><td colspan=\"2\">autographing 83.7</td><td>signing</td><td>16.2</td><td>69</td><td>518</td><td>skiing</td><td>41.8</td><td>skiing</td><td>11.4</td></tr><tr><td>10</td><td>640</td><td>drumming</td><td>81.6</td><td>drums</td><td>19.5</td><td>70</td><td>860</td><td>bulldozing</td><td>41.7</td><td>bulldozer</td><td>25.7</td></tr><tr><td>11</td><td>436</td><td>injecting</td><td>81.6</td><td>injected</td><td>13.2</td><td>71</td><td>61</td><td>extinguishing</td><td>41.3</td><td>sting</td><td>9.1</td></tr><tr><td>12</td><td>109</td><td>peeling</td><td>80.9</td><td>peeling</td><td>21.2</td><td>72</td><td>296</td><td>combing</td><td>40.9</td><td>brushes</td><td>5.9</td></tr><tr><td>13</td><td>551</td><td>shaving</td><td>80.2</td><td>shaving</td><td>18.0</td><td>73</td><td>435</td><td>screwing</td><td>40.8</td><td>drill</td><td>25.0</td></tr><tr><td>14</td><td>137</td><td>paddling</td><td>80.0</td><td>canoe</td><td>25.8</td><td>74</td><td>705</td><td>surfing</td><td>40.6</td><td>ocean</td><td>27.0</td></tr><tr><td>15</td><td>327</td><td>crying</td><td>78.8</td><td>crying</td><td>29.5</td><td>75</td><td>760</td><td>hammering</td><td>40.0</td><td>hammering</td><td>23.3</td></tr><tr><td>16</td><td>593</td><td>surfing</td><td>77.7</td><td>surfboard</td><td>10.9</td><td>76</td><td>926</td><td>paddling</td><td>40.0</td><td>lake</td><td>6.8</td></tr><tr><td>17</td><td>687</td><td>drumming</td><td>77.3</td><td>drums</td><td>14.4</td><td>77</td><td>888</td><td>paddling</td><td>39.6</td><td>lake</td><td>7.4</td></tr><tr><td>18</td><td>883</td><td>tattooing</td><td>77.2</td><td>tattoo</td><td>13.6</td><td>78</td><td>169</td><td>dunking</td><td>39.3</td><td>nba</td><td>7.5</td></tr><tr><td>19</td><td>1000</td><td>inflating</td><td>74.5</td><td>inflatable</td><td>12.8</td><td>79</td><td>681</td><td>manicuring</td><td>38.7</td><td>nails</td><td>13.2</td></tr><tr><td>20</td><td>222</td><td>boxing</td><td>71.3</td><td>boxing</td><td>13.2</td><td>80</td><td>685</td><td>signing</td><td>38.6</td><td>writing</td><td>8.3</td></tr><tr><td>21</td><td>243</td><td>shredding</td><td>70.0</td><td>shredding</td><td>28.6</td><td>81</td><td>631</td><td>paddling</td><td>38.5</td><td>clouds</td><td>12.2</td></tr><tr><td>22</td><td>157</td><td>paddling</td><td>69.9</td><td>kayak</td><td>21.3</td><td>82</td><td>800</td><td>dropping</td><td>38.3</td><td>beans</td><td>12.9</td></tr><tr><td>23</td><td>427</td><td>boxing</td><td>69.8</td><td>boxers</td><td>16.2</td><td>83</td><td>556</td><td>drumming</td><td>38.3</td><td>marching</td><td>11.6</td></tr><tr><td>24</td><td>774</td><td>surfing</td><td>69.2</td><td>waves</td><td>23.0</td><td>84</td><td>758</td><td>wrapping</td><td>38.1</td><td>wrapping</td><td>22.2</td></tr><tr><td>25</td><td>613</td><td>manicuring</td><td>67.9</td><td>nails</td><td>24.5</td><td>85</td><td>368</td><td>texting</td><td>38.0</td><td>texting</td><td>16.7</td></tr><tr><td>26</td><td>952</td><td>leaping</td><td>66.0</td><td>dolphins</td><td>10.7</td><td>86</td><td>625</td><td>combing</td><td>37.9</td><td>hair</td><td>4.9</td></tr><tr><td>27</td><td>196</td><td>boxing</td><td>64.1</td><td>boxer</td><td>13.9</td><td>87</td><td>166</td><td>boxing</td><td>37.8</td><td>boxing</td><td>7.2</td></tr><tr><td>28</td><td>706</td><td>sailing</td><td>63.4</td><td>sailboat</td><td>18.8</td><td>88</td><td>539</td><td>paddling</td><td>37.5</td><td>helmet</td><td>13.0</td></tr><tr><td>29</td><td>58</td><td>shaving</td><td>62.8</td><td>shaving</td><td>10.9</td><td>89</td><td>139</td><td>leaping</td><td>37.5</td><td>jumping</td><td>16.6</td></tr><tr><td>30</td><td>759</td><td>paddling</td><td>60.7</td><td>paddling</td><td>12.4</td><td>90</td><td>123</td><td>drumming</td><td>37.1</td><td>playing</td><td>8.7</td></tr><tr><td>31</td><td>868</td><td>boxing</td><td>60.0</td><td>boxer</td><td>11.2</td><td>91</td><td>577</td><td>drumming</td><td>37.0</td><td>musical</td><td>8.1</td></tr><tr><td>32</td><td>500</td><td>dialing</td><td>60.0</td><td>dialing</td><td>13.8</td><td>92</td><td>780</td><td>screwing</td><td>36.9</td><td>drill</td><td>15.8</td></tr><tr><td>33</td><td>536</td><td>cheering</td><td colspan=\"3\">60.0 cheerleaders 26.8</td><td>93</td><td>621</td><td>leaping</td><td>36.6</td><td>jumps</td><td>9.7</td></tr><tr><td>34</td><td>50</td><td>rafting</td><td>58.6</td><td>rafting</td><td>16.7</td><td>94</td><td>154</td><td>boxing</td><td>36.0</td><td>referee</td><td>14.7</td></tr><tr><td>35</td><td>664</td><td>dunking</td><td>58.0</td><td>basketball</td><td>11.0</td><td>95</td><td>415</td><td>grilling</td><td>35.7</td><td>grill</td><td>15.7</td></tr><tr><td>36</td><td>103</td><td colspan=\"2\">autographing 57.8</td><td>carpet</td><td>8.2</td><td>96</td><td>345</td><td>autographing</td><td>35.5</td><td>pictures</td><td>19.3</td></tr><tr><td>37</td><td>990</td><td>wrestling</td><td>56.1</td><td>wrestling</td><td>25.9</td><td>97</td><td>694</td><td>sailing</td><td>34.9</td><td>sailing</td><td>7.0</td></tr><tr><td>38</td><td>880</td><td>sleeping</td><td>56.0</td><td>sleeping</td><td>21.1</td><td>98</td><td>973</td><td>leaping</td><td>34.4</td><td>tale</td><td>8.0</td></tr><tr><td>39</td><td>48</td><td>paddling</td><td>55.1</td><td>rowing</td><td>18.2</td><td>99</td><td>957</td><td>shrugging</td><td>34.4</td><td>lifting</td><td>10.3</td></tr><tr><td>40</td><td>292</td><td>skiing</td><td>54.2</td><td>skiing</td><td>20.0</td><td colspan=\"2\">100 713</td><td>paddling</td><td>34.3</td><td>sunset</td><td>25.3</td></tr><tr><td>41</td><td>602</td><td>ironing</td><td>52.5</td><td>ironing</td><td>7.1</td><td colspan=\"2\">101 697</td><td>injecting</td><td>34.1</td><td>doctor</td><td>18.8</td></tr><tr><td>42</td><td>954</td><td>dropping</td><td>52.4</td><td>dropped</td><td>8.2</td><td colspan=\"2\">102 431</td><td>peeling</td><td>33.9</td><td>apple</td><td>20.0</td></tr><tr><td>43</td><td>735</td><td>applauding</td><td>52.1</td><td>clapping</td><td>23.4</td><td colspan=\"2\">103 164</td><td>typing</td><td>33.8</td><td>laptop</td><td>20.6</td></tr><tr><td>44</td><td>816</td><td colspan=\"2\">autographing 51.0</td><td>carpet</td><td>22.5</td><td colspan=\"2\">104 776</td><td>juggling</td><td>33.6</td><td>balls</td><td>16.5</td></tr><tr><td>45</td><td>516</td><td>swinging</td><td>50.0</td><td>swing</td><td>20.4</td><td>105</td><td>73</td><td>shrugging</td><td>32.9</td><td>weight</td><td>14.6</td></tr><tr><td>46</td><td>421</td><td>carving</td><td>50.0</td><td>carving</td><td>27.2</td><td colspan=\"2\">106 846</td><td>injecting</td><td>32.8</td><td>gloves</td><td>7.8</td></tr><tr><td>47</td><td>168</td><td>drumming</td><td>49.3</td><td>marching</td><td>17.5</td><td colspan=\"2\">107 395</td><td>juggling</td><td>32.7</td><td>balls</td><td>10.1</td></tr><tr><td>48</td><td>561</td><td>flossing</td><td>48.0</td><td>mouse</td><td>10.0</td><td colspan=\"2\">108 273</td><td>dusting</td><td>32.6</td><td>clean</td><td>11.5</td></tr><tr><td>49</td><td>970</td><td>marrying</td><td>47.8</td><td>bride</td><td>22.2</td><td colspan=\"2\">109 737</td><td>paddling</td><td>32.5</td><td>mountains</td><td>14.0</td></tr><tr><td>50</td><td>610</td><td>dunking</td><td>47.4</td><td>basketball</td><td>19.5</td><td colspan=\"2\">110 291</td><td>coughing</td><td>32.4</td><td>sneezes</td><td>15.6</td></tr><tr><td>51</td><td>105</td><td>paddling</td><td>47.2</td><td>river</td><td>23.7</td><td colspan=\"2\">111 375</td><td>colliding</td><td>32.4</td><td>crashing</td><td>14.5</td></tr><tr><td>52</td><td>150</td><td>waxing</td><td>47.2</td><td>wax</td><td>20.3</td><td colspan=\"2\">112 693</td><td>sleeping</td><td>32.3</td><td>baby</td><td>28.9</td></tr><tr><td>53</td><td>92</td><td>howling</td><td>46.7</td><td>barking</td><td>15.1</td><td colspan=\"2\">113 111</td><td>baking</td><td>32.3</td><td>baker</td><td>13.8</td></tr><tr><td>54</td><td>929</td><td>typing</td><td>46.3</td><td>typing</td><td>22.4</td><td colspan=\"2\">114 805</td><td>massaging</td><td>32.0</td><td>squatted</td><td>8.7</td></tr><tr><td>55</td><td>844</td><td>drumming</td><td>46.2</td><td>band</td><td>14.5</td><td colspan=\"2\">115 134</td><td>autographing</td><td>31.7</td><td>obama</td><td>7.5</td></tr><tr><td>56</td><td>497</td><td>cheering</td><td colspan=\"3\">45.8 cheerleaders 34.8</td><td colspan=\"2\">116 923</td><td>wrapping</td><td>31.6</td><td>tape</td><td>16.7</td></tr><tr><td>57</td><td>322</td><td>paddling</td><td>45.8</td><td>kayak</td><td>7.2</td><td colspan=\"2\">117 698</td><td>surfing</td><td>31.5</td><td>beach</td><td>9.8</td></tr><tr><td>58</td><td>672</td><td>boxing</td><td>45.6</td><td>fighting</td><td>28.8</td><td colspan=\"2\">118 362</td><td>paddling</td><td>31.5</td><td>water</td><td>8.2</td></tr><tr><td>59</td><td>97</td><td>barbecuing</td><td>45.6</td><td>grill</td><td>26.4</td><td colspan=\"2\">119 505</td><td>drumming</td><td>31.0</td><td>guitar</td><td>13.1</td></tr><tr><td>60</td><td>216</td><td>inflating</td><td>45.3</td><td>balloon</td><td>10.3</td><td colspan=\"2\">120 215</td><td>shaving</td><td>31.0</td><td>vent</td><td>12.1</td></tr></table>",
                "type_str": "table",
                "text": "Correspondence between codewords, visual actions, and spoken words (Extended Table2). The second hypothesis and the occurrence are omitted for simplicity. All codewords activated on S-MiT's development set are listed.",
                "html": null,
                "num": null
            },
            "TABREF6": {
                "content": "<table><tr><td>: continued</td></tr></table>",
                "type_str": "table",
                "text": "",
                "html": null,
                "num": null
            },
            "TABREF7": {
                "content": "<table><tr><td>: continued</td></tr></table>",
                "type_str": "table",
                "text": "",
                "html": null,
                "num": null
            }
        }
    }
}