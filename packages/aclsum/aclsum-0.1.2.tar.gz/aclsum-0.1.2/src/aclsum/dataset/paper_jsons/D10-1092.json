{
    "paper_id": "D10-1092",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T11:55:56.961311Z"
    },
    "title": "Automatic Evaluation of Translation Quality for Distant Language Pairs",
    "authors": [
        {
            "first": "Hideki",
            "middle": [],
            "last": "Isozaki",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "NTT Corporation",
                "location": {
                    "addrLine": "2-4 Hikaridai",
                    "postCode": "619-0237",
                    "settlement": "Seikacho, Sorakugun, Kyoto",
                    "country": "Japan"
                }
            },
            "email": "isozaki@cslab.kecl.ntt.co.jp"
        },
        {
            "first": "Tsutomu",
            "middle": [],
            "last": "Hirao",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "NTT Corporation",
                "location": {
                    "addrLine": "2-4 Hikaridai",
                    "postCode": "619-0237",
                    "settlement": "Seikacho, Sorakugun, Kyoto",
                    "country": "Japan"
                }
            },
            "email": "hirao@cslab.kecl.ntt.co.jp"
        },
        {
            "first": "Kevin",
            "middle": [],
            "last": "Duh",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "NTT Corporation",
                "location": {
                    "addrLine": "2-4 Hikaridai",
                    "postCode": "619-0237",
                    "settlement": "Seikacho, Sorakugun, Kyoto",
                    "country": "Japan"
                }
            },
            "email": "kevinduh@cslab.kecl.ntt.co.jp"
        },
        {
            "first": "Katsuhito",
            "middle": [],
            "last": "Sudoh",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "NTT Corporation",
                "location": {
                    "addrLine": "2-4 Hikaridai",
                    "postCode": "619-0237",
                    "settlement": "Seikacho, Sorakugun, Kyoto",
                    "country": "Japan"
                }
            },
            "email": "sudoh@cslab.kecl.ntt.co.jp"
        },
        {
            "first": "Hajime",
            "middle": [],
            "last": "Tsukada",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "NTT Corporation",
                "location": {
                    "addrLine": "2-4 Hikaridai",
                    "postCode": "619-0237",
                    "settlement": "Seikacho, Sorakugun, Kyoto",
                    "country": "Japan"
                }
            },
            "email": "tsukada@cslab.kecl.ntt.co.jp"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Automatic evaluation of Machine Translation (MT) quality is essential to developing highquality MT systems. Various evaluation metrics have been proposed, and BLEU is now used as the de facto standard metric. However, when we consider translation between distant language pairs such as Japanese and English, most popular metrics (e.g., BLEU, NIST, PER, and TER) do not work well. It is well known that Japanese and English have completely different word orders, and special care must be paid to word order in translation. Otherwise, translations with wrong word order often lead to misunderstanding and incomprehensibility. For instance, SMT-based Japanese-to-English translators tend to translate 'A because B' as 'B because A.' Thus, word order is the most important problem for distant language translation. However, conventional evaluation metrics do not significantly penalize such word order mistakes. Therefore, locally optimizing these metrics leads to inadequate translations. In this paper, we propose an automatic evaluation metric based on rank correlation coefficients modified with precision. Our meta-evaluation of the NTCIR-7 PATMT JE task data shows that this metric outperforms conventional metrics. SMT systems thus fail to find (R0). Consequently, the global word order is essential for translation between distant language pairs, and wrong word order can easily lead to misunderstanding or incomprehensibility. Perhaps, some readers do not understand why we emphasize word order from this example alone. A few more examples will clarify what happens when SMT is applied to Japanese-to-English translation. Even the most famous SMT service available on the web failed to translate the following very simple sentence at the time of writing this paper.\nJapanese: meari wa jon wo koroshita. Reference: Mary killed John. SMT output: John killed Mary.\nSince it cannot translate such a simple sentence, it obviously cannot translate more complex sentences correctly.\nJapanese: bobu ga katta hon wo jon wa yonda. Reference: John read a book that Bob bought. SMT output: Bob read the book John bought.\nJapanese: bobu wa meari ni yubiwa wo kau tameni, jon no mise ni itta. Reference: Bob went to John's store to buy a ring for Mary.\nSMT output: Bob Mary to buy the ring, John went to the store.",
    "pdf_parse": {
        "paper_id": "D10-1092",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Automatic evaluation of Machine Translation (MT) quality is essential to developing highquality MT systems. Various evaluation metrics have been proposed, and BLEU is now used as the de facto standard metric. However, when we consider translation between distant language pairs such as Japanese and English, most popular metrics (e.g., BLEU, NIST, PER, and TER) do not work well. It is well known that Japanese and English have completely different word orders, and special care must be paid to word order in translation. Otherwise, translations with wrong word order often lead to misunderstanding and incomprehensibility. For instance, SMT-based Japanese-to-English translators tend to translate 'A because B' as 'B because A.' Thus, word order is the most important problem for distant language translation. However, conventional evaluation metrics do not significantly penalize such word order mistakes. Therefore, locally optimizing these metrics leads to inadequate translations. In this paper, we propose an automatic evaluation metric based on rank correlation coefficients modified with precision. Our meta-evaluation of the NTCIR-7 PATMT JE task data shows that this metric outperforms conventional metrics. SMT systems thus fail to find (R0). Consequently, the global word order is essential for translation between distant language pairs, and wrong word order can easily lead to misunderstanding or incomprehensibility. Perhaps, some readers do not understand why we emphasize word order from this example alone. A few more examples will clarify what happens when SMT is applied to Japanese-to-English translation. Even the most famous SMT service available on the web failed to translate the following very simple sentence at the time of writing this paper.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            },
            {
                "text": "Japanese: meari wa jon wo koroshita. Reference: Mary killed John. SMT output: John killed Mary.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            },
            {
                "text": "Since it cannot translate such a simple sentence, it obviously cannot translate more complex sentences correctly.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            },
            {
                "text": "Japanese: bobu ga katta hon wo jon wa yonda. Reference: John read a book that Bob bought. SMT output: Bob read the book John bought.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            },
            {
                "text": "Japanese: bobu wa meari ni yubiwa wo kau tameni, jon no mise ni itta. Reference: Bob went to John's store to buy a ring for Mary.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            },
            {
                "text": "SMT output: Bob Mary to buy the ring, John went to the store.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Automatic evaluation of machine translation (MT) quality is essential to developing high-quality machine translation systems because human evaluation is time consuming, expensive, and irreproducible. If we have a perfect automatic evaluation metric, we can tune our translation system for the metric.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "BLEU (Papineni et al., 2002b; Papineni et al., 2002a) showed high correlation with human judgments and is still used as the de facto standard automatic evaluation metric. However, Callison-Burch et al. (2006) argued that the MT community is overly reliant on BLEU by showing examples of poor performance. For Japanese-to-English (JE) translation, Echizen-ya et al. (2009) showed that the popular BLEU and NIST do not work well by using the system outputs of the NTCIR-7 PATMT (patent translation) JE task (Fujii et al., 2008) . On the other hand, ROUGE-L (Lin and Hovy, 2003) , Word Error Rate (WER), and IMPACT (Echizen-ya and Araki, 2007) worked better.",
                "cite_spans": [
                    {
                        "start": 5,
                        "end": 29,
                        "text": "(Papineni et al., 2002b;",
                        "ref_id": null
                    },
                    {
                        "start": 30,
                        "end": 53,
                        "text": "Papineni et al., 2002a)",
                        "ref_id": null
                    },
                    {
                        "start": 180,
                        "end": 208,
                        "text": "Callison-Burch et al. (2006)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 347,
                        "end": 371,
                        "text": "Echizen-ya et al. (2009)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 505,
                        "end": 525,
                        "text": "(Fujii et al., 2008)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 555,
                        "end": 575,
                        "text": "(Lin and Hovy, 2003)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 612,
                        "end": 640,
                        "text": "(Echizen-ya and Araki, 2007)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In these studies, Pearson's correlation coefficient and Spearman's rank correlation \u03c1 with human evaluation scores are used to measure how closely an automatic evaluation method correlates with human evaluation. This evaluation of automatic evaluation methods is called meta-evaluation. In human evaluation, people judge the adequacy and the fluency of each translation. Denoual and Lepage (2005) pointed out that BLEU assumes word boundaries, which is ambiguous in Japanese and Chinese. Here, we assume the word boundaries given by ChaSen, one of the standard morphological analyzers (http://chasenlegacy.sourceforge.jp/) following Fujii et al. (2008) In JE translation, most Statistical Machine Translation (SMT) systems translate the Japanese sentence (J0) kare wa sono hon wo yonda node sekaishi ni kyoumi ga atta which means (R0) he was interested in world history because he read the book into an English sentence such as (H0) he read the book because he was interested in world history in which the cause and the effect are swapped. Why does this happen? The former half of (J0) means \"He read the book,\" and the latter half means \"(he) was interested in world history.\" The middle word \"node\" between them corresponds to \"because.\" Therefore, SMT systems output sentences like (H0). On the other hand, Rule-based Machine Translation (RBMT) systems correctly give (R0).",
                "cite_spans": [
                    {
                        "start": 371,
                        "end": 396,
                        "text": "Denoual and Lepage (2005)",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 633,
                        "end": 652,
                        "text": "Fujii et al. (2008)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In order to find (R0), SMT systems have to search a very large space because we cannot restrict its search space with a small distortion limit. Most",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this way, this SMT service usually gives incomprehensible or misleading translations, and thus people prefer RBMT services. Other SMT systems also tend to make similar word order mistakes, and special care should be paid to the translation between distant language pairs such as Japanese and English.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Even Japanese people cannot solve this word order problem easily: It is well known that Japanese people are not good at speaking English.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "From this point of view, conventional automatic evaluation metrics of translation quality disregard word order mistakes too much. Single-reference BLEU is defined by a geometrical mean of n-gram precisions p n and is modified by Brevity Penalty (BP) min(1, exp(1 -r/h)), where r is the length of the reference and h is the length of the hypothesis.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "BLEU = BP \u00d7 (p 1 p 2 p 3 p 4 ) 1/4 .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Its range is [0, 1]. The BLEU score of (H0) with reference (R0) is 1.0\u00d7(11/11\u00d79/10\u00d76/9\u00d74/8) 1/4 = 0.740. Therefore, BLEU gives a very good score to this inadequate translation because it checks only ngrams and does not regard global word order.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Since (R0) and (H0) look similar in terms of fluency, adequacy is more important than fluency in the translation between distant language pairs. Similarly, other popular scores such as NIST, PER, and TER (Snover et al., 2006) also give relatively good scores to this translation. NIST also considers only local word orders (n-grams). PER (Position-Independent Word Error Rate) was designed to disregard word order completely. TER (Snover et al., 2006) was designed to allow phrase movements without large penalties. Therefore, these standard metrics are not optimal for evaluating translation between distant language pairs. In this paper, we propose an alternative automatic evaluation metric appropriate for distant language pairs. Our method is based on rank correlation coefficients. We use them to compare the word ranks in the reference with those in the hypothesis.",
                "cite_spans": [
                    {
                        "start": 204,
                        "end": 225,
                        "text": "(Snover et al., 2006)",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 430,
                        "end": 451,
                        "text": "(Snover et al., 2006)",
                        "ref_id": "BIBREF15"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "There are two popular rank correlation coefficients: Spearman's \u03c1 and Kendall's \u03c4 (Kendall, 1975) . In Isozaki et al. (2010) , we used Kendall's \u03c4 to measure the effectiveness of our Head Finalization rule as a preprocessor for English-to-Japanese translation, but we measured the quality of translation by using conventional metrics.",
                "cite_spans": [
                    {
                        "start": 82,
                        "end": 97,
                        "text": "(Kendall, 1975)",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 103,
                        "end": 124,
                        "text": "Isozaki et al. (2010)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "It is not clear how well \u03c4 works as an automatic evaluation metric of translation quality. Moreover, Spearman's \u03c1 might work better than Kendall's \u03c4 . As we discuss later, \u03c4 considers only the direction of the rank change, whereas \u03c1 considers the distance of the change.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The first objective of this paper is to examine which is the better metric for distant language pairs. The second objective is to find improvements of these rank correlation-metrics.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Spearman's \u03c1 is based on Pearson's correlation coefficients. Suppose we have two lists of numbers x = [0.1, 0.4, 0.2, 0.6], y = [0.9, 0.6, 0.2, 0.7].",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "To obtain Pearson's coefficients between x and y, we use the raw values in these lists. If we substitute their ranks for their raw values, we get",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "x = [1, 3, 2, 4] and y = [4, 2, 1, 3].",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Then, Spearman's \u03c1 between x and y is given by Pearson's coefficients between x and y . This \u03c1 can be rewritten as follows when there is no tie:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\u03c1 = 1 -i d 2 i n+1 C 3 .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Here, d i indicates the difference in the ranks of the i-th element. Rank distances are squared in this formula. Because of this square, we expect that \u03c1 decreases drastically when there is an element that significantly changes in rank. But we are also afraid that \u03c1 may be too severe for alternative good translations.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Since Pearson's correlation metric assumes linearity, nonlinear monotonic functions can change its score. On the other hand, Spearman's \u03c1 and Kendall's \u03c4 uses ranks instead of raw evaluation scores, and simple application of monotonic functions cannot change them (use of other operations such as averaging sentence scores can change them).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "We have to determine word ranks to obtain rank correlation coefficients. Suppose we have:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word alignment for rank correlations",
                "sec_num": "2.1"
            },
            {
                "text": "(R1) John hit Bob yesterday (H1) Bob hit John yesterday The 1st word \"John\" in R1 becomes the 3rd word in H1. The 2nd word \"hit\" in R1 becomes the 2nd word in H1. The 3rd word \"Bob\" in R1 becomes the 1st word in H1. The 4th word \"yesterday\" in R1 becomes the 4th word in H1. Thus, we get H1's word order list [3, 2, 1, 4 ]. The number of all pairs of integers in this list is 4 C 2 = 6. It has three increasing pairs: (3,4), (2,4), and (1,4 ). Since Kendall's \u03c4 is given by:",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 309,
                        "end": 312,
                        "text": "[3,",
                        "ref_id": "TABREF3"
                    },
                    {
                        "start": 313,
                        "end": 315,
                        "text": "2,",
                        "ref_id": "TABREF2"
                    },
                    {
                        "start": 316,
                        "end": 318,
                        "text": "1,",
                        "ref_id": "TABREF1"
                    },
                    {
                        "start": 319,
                        "end": 320,
                        "text": "4",
                        "ref_id": null
                    },
                    {
                        "start": 411,
                        "end": 431,
                        "text": "pairs: (3,4), (2,4),",
                        "ref_id": "TABREF3"
                    },
                    {
                        "start": 436,
                        "end": 440,
                        "text": "(1,4",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Word alignment for rank correlations",
                "sec_num": "2.1"
            },
            {
                "text": "\u03c4 = 2 \u00d7",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word alignment for rank correlations",
                "sec_num": "2.1"
            },
            {
                "text": "the number of increasing pairs the number of all pairs -1,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word alignment for rank correlations",
                "sec_num": "2.1"
            },
            {
                "text": "H1's \u03c4 is 2 \u00d7 3/6 -1 = 0.0.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word alignment for rank correlations",
                "sec_num": "2.1"
            },
            {
                "text": "In this case, we can obtain Spearman's \u03c1 as follows: \"John\" moved by d 1 = 2 words, \"hit\" moved by d 2 = 0 words, \"Bob\" moved by d 3 = 2 words, and \"yesterday\" moved by",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word alignment for rank correlations",
                "sec_num": "2.1"
            },
            {
                "text": "d 4 = 0 words. Therefore, H1's \u03c1 is 1 -(2 2 + 0 2 + 2 2 + 0 2 )/ 5 C 3 = 0.2.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word alignment for rank correlations",
                "sec_num": "2.1"
            },
            {
                "text": "Thus, \u03c4 considers only the direction of the movement, whereas \u03c1 considers the distance of the movement. Both \u03c1 and \u03c4 have the same range [-1, 1]. The main objective of this paper is to clarify which rank correlation is closer to human evaluation scores.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word alignment for rank correlations",
                "sec_num": "2.1"
            },
            {
                "text": "We have to consider the limitation of the rank correlation metrics. They are defined only when there is one-to-one correspondence. However, a reference sentence and a hypothesis sentence may have different numbers of words. They may have two or more occurrences of the same word in one sentence. Sometimes, a word in the reference does not appear in the hypothesis, or a word in the hypothesis does not appear in the reference. Therefore, we cannot calculate \u03c4 and \u03c1 following the above definitions in general.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word alignment for rank correlations",
                "sec_num": "2.1"
            },
            {
                "text": "Here, we determine the correspondence of words between hypotheses and references as follows. First, we find one-to-one corresponding words. That is, we find words that appear in both sentences and only once in each sentence. Suppose we have:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word alignment for rank correlations",
                "sec_num": "2.1"
            },
            {
                "text": "(R2) the boy read the book (H2) the book was read by the boy By removing non-aligned words by one-to-one correspondence, we get: (R3) boy read book (H3) book read boy Thus, we lost \"the.\" We relax this one-to-one correspondence constraint by using one-to-one corresponding bigrams. (R2) and (H2) share \"the boy\" and \"the book,\" and we can align these instances of \"the\" correctly. (R4) the1 boy2 read3 the4 book5  (H4) the4 book5 read3 the1 boy2 Now, we have five aligned words, and H4's word order is represented by [4, 5, 3, 1, 2] .",
                "cite_spans": [
                    {
                        "start": 517,
                        "end": 520,
                        "text": "[4,",
                        "ref_id": null
                    },
                    {
                        "start": 521,
                        "end": 523,
                        "text": "5,",
                        "ref_id": null
                    },
                    {
                        "start": 524,
                        "end": 526,
                        "text": "3,",
                        "ref_id": null
                    },
                    {
                        "start": 527,
                        "end": 529,
                        "text": "1,",
                        "ref_id": null
                    },
                    {
                        "start": 530,
                        "end": 532,
                        "text": "2]",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 381,
                        "end": 445,
                        "text": "(R4) the1 boy2 read3 the4 book5  (H4) the4 book5 read3 the1 boy2",
                        "ref_id": "TABREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Word alignment for rank correlations",
                "sec_num": "2.1"
            },
            {
                "text": "In returning to H0 and R0, we find that each of these sentences has eleven words. Almost all words are aligned by one-to-one correspondence but \"he\" is not aligned because it appears twice in each sentence. By considering one-to-one corresponding bigrams (\"he was\" and \"he read\"), \"he\" is aligned as follows.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word alignment for rank correlations",
                "sec_num": "2.1"
            },
            {
                "text": "(R5) he1 was2 interested3 in4 world5 history6 because7 he8 read9 the10 book11 (H5) he8 read9 the10 book11 because7 he1 was2 interested3 in4 world5 history6 H5's word order is [8, 9, 10, 11, 7, 1, 2, 3, 4, 5, 6] . The number of increasing pairs is: 4 C 2 = 6 pairs in [8, 9, 10, 11] and 6 C 2 = 15 pairs in [1, 2, 3, 4, 5, 6] . Then we obtain \u03c4 = 2 \u00d7 (6 + 15)/ 11 C 2 -1 = -0.236. On the other hand, i d 2 i = 5 2 \u00d7 6 + 2 2 + 7 2 \u00d7 4 = 350, and we obtain \u03c1 = 1 -350/ 12 C 3 = -0.591.",
                "cite_spans": [
                    {
                        "start": 175,
                        "end": 178,
                        "text": "[8,",
                        "ref_id": null
                    },
                    {
                        "start": 179,
                        "end": 181,
                        "text": "9,",
                        "ref_id": null
                    },
                    {
                        "start": 182,
                        "end": 185,
                        "text": "10,",
                        "ref_id": null
                    },
                    {
                        "start": 186,
                        "end": 189,
                        "text": "11,",
                        "ref_id": null
                    },
                    {
                        "start": 190,
                        "end": 192,
                        "text": "7,",
                        "ref_id": null
                    },
                    {
                        "start": 193,
                        "end": 195,
                        "text": "1,",
                        "ref_id": null
                    },
                    {
                        "start": 196,
                        "end": 198,
                        "text": "2,",
                        "ref_id": null
                    },
                    {
                        "start": 199,
                        "end": 201,
                        "text": "3,",
                        "ref_id": null
                    },
                    {
                        "start": 202,
                        "end": 204,
                        "text": "4,",
                        "ref_id": null
                    },
                    {
                        "start": 205,
                        "end": 207,
                        "text": "5,",
                        "ref_id": null
                    },
                    {
                        "start": 208,
                        "end": 210,
                        "text": "6]",
                        "ref_id": null
                    },
                    {
                        "start": 267,
                        "end": 270,
                        "text": "[8,",
                        "ref_id": null
                    },
                    {
                        "start": 271,
                        "end": 273,
                        "text": "9,",
                        "ref_id": null
                    },
                    {
                        "start": 274,
                        "end": 277,
                        "text": "10,",
                        "ref_id": null
                    },
                    {
                        "start": 278,
                        "end": 281,
                        "text": "11]",
                        "ref_id": null
                    },
                    {
                        "start": 306,
                        "end": 309,
                        "text": "[1,",
                        "ref_id": null
                    },
                    {
                        "start": 310,
                        "end": 312,
                        "text": "2,",
                        "ref_id": null
                    },
                    {
                        "start": 313,
                        "end": 315,
                        "text": "3,",
                        "ref_id": null
                    },
                    {
                        "start": 316,
                        "end": 318,
                        "text": "4,",
                        "ref_id": null
                    },
                    {
                        "start": 319,
                        "end": 321,
                        "text": "5,",
                        "ref_id": null
                    },
                    {
                        "start": 322,
                        "end": 324,
                        "text": "6]",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word alignment for rank correlations",
                "sec_num": "2.1"
            },
            {
                "text": "Therefore, both Spearman's \u03c1 and Kendall's \u03c4 give very bad scores to the misleading translation H0. This fact implies they are much better metrics than BLEU, which gave a good score to it. \u03c1 is much lower than \u03c4 as we expected.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word alignment for rank correlations",
                "sec_num": "2.1"
            },
            {
                "text": "In general, we can use higher-order n-grams for this alignment, but here we use only unigrams and bigrams for simplicity. This algnment algorithm is given in Figure 1 . Since some hypothesis words do not have corresponding reference words, the output integer list worder is sometimes shorter than the evaluated sentence. Therefore, we should not use worder[i] -i as d i directly. We have to renumber the list by rank as we did in Section 1.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 165,
                        "end": 166,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Word alignment for rank correlations",
                "sec_num": "2.1"
            },
            {
                "text": "Read a hypothesis sentence h = h 1 h 2 . . . h m and its reference sentence r = r 1 r 2 . . . r n .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word alignment for rank correlations",
                "sec_num": "2.1"
            },
            {
                "text": "Initialize worder with an empty list.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word alignment for rank correlations",
                "sec_num": "2.1"
            },
            {
                "text": "For each word h i in h:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word alignment for rank correlations",
                "sec_num": "2.1"
            },
            {
                "text": "\u2022 If h i appears only once each in h and r, append j s.t. r j = h i to worder.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word alignment for rank correlations",
                "sec_num": "2.1"
            },
            {
                "text": "\u2022 Otherwise, if the bigram h i h i+1 appears only once each in h and r, append j s.t. r j r j+1 = h i h i+1 to worder.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word alignment for rank correlations",
                "sec_num": "2.1"
            },
            {
                "text": "\u2022 Otherwise, if the bigram h i-1 h i appears only once each in h and r, append j s.t. r j-1 r j = h i-1 h i to worder.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word alignment for rank correlations",
                "sec_num": "2.1"
            },
            {
                "text": "Return worder. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word alignment for rank correlations",
                "sec_num": "2.1"
            },
            {
                "text": "These rank correlation metrics sometimes have negative values. In order to make them just like other automatic evaluation metrics, we normalize them as follows.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word order metrics and meta-evaluation metrics",
                "sec_num": "2.2"
            },
            {
                "text": "\u2022 Normalized Kendall's \u03c4 : NKT = (\u03c4 + 1)/2.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word order metrics and meta-evaluation metrics",
                "sec_num": "2.2"
            },
            {
                "text": "\u2022 Normalized Spearman's \u03c1: NSR = (\u03c1 + 1)/2.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word order metrics and meta-evaluation metrics",
                "sec_num": "2.2"
            },
            {
                "text": "Accordingly, NKT is 0.382 and NSR is 0.205. These metrics are defined only when the number of aligned words is two or more. We define both NKT and NSR as zero when the number is one or less. Consequently, these normalized metrics have the same range [0, 1].",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word order metrics and meta-evaluation metrics",
                "sec_num": "2.2"
            },
            {
                "text": "In order to avoid confusion, we use these abbreviations (NKT and NSR) when we use rank correlations as word order metrics, because these correlation metrics are also used in the machine translation community for meta-evaluation. For metaevaluation, we use Spearman's \u03c1 and Pearson's correlation coefficient and call them \"Spearman\" and \"Pearson,\" respectively.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word order metrics and meta-evaluation metrics",
                "sec_num": "2.2"
            },
            {
                "text": "Since we measure the rank correlation of only corresponding words, these metrics will overestimate the correlation. For instance, a hypothesis sentence might have only two corresponding words among (Each corresponds to one sentence generated by one MT system) dozens of words. In this case, these two words determine the score of the whole sentence. If the two words appear in their order in the reference, the whole sentence obtains the best score, NSR = NKT = 1.0, in spite of the fact that only two words matched.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overestimation problem",
                "sec_num": "2.3"
            },
            {
                "text": "Solving this overestimation problem is the second objective of this paper. BLEU uses \"Brevity Penalty (BP)\" (Section 1) to reduce the scores of too-short sentences. We can combine the above word order metrics with BP, e.g., NKT \u00d7 BP and NSR \u00d7 BP.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overestimation problem",
                "sec_num": "2.3"
            },
            {
                "text": "However, we cannot very much expect from this solution because BP scores do not correlate with human judgments well. The left graph of Figure 2 shows a scatter plot of BP and \"normalized average adequacy.\" This graph has 15 (systems) \u00d7 100 (sentences) dots. Each dot ( ) corresponds to one sentence from one translation system.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 142,
                        "end": 143,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Overestimation problem",
                "sec_num": "2.3"
            },
            {
                "text": "In the NTCIR-7 data, three human judges gave five-point scores (1, 2, 3, 4, 5) for \"adequacy\" and \"fluency\" of each translated sentence. Although each system translated 1,381 sentences, only 100 sentences were evaluated by the judges.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overestimation problem",
                "sec_num": "2.3"
            },
            {
                "text": "For each translated sentence, we averaged three judges' adequacy scores and normalized this average x by (x -1)/4. This is our \"normalized average adequacy,\" and the dots appears only at multiples of 1/3 \u00d7 1/4.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overestimation problem",
                "sec_num": "2.3"
            },
            {
                "text": "This graph shows that BP has very little correlation with adequacy, and we cannot expect BP to improve the meta-evaluation performance very much. Perhaps, BP's poor performance was caused by the fact that most MT systems output almost the same number of words, and if the number exceeds the length of the reference, BP=1.0 holds.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overestimation problem",
                "sec_num": "2.3"
            },
            {
                "text": "Therefore, we have to consider other modifiers for this overestimation problem. We can use other common metrics such as precision, recall, and Fmeasure to reduce the overestimation of NSR and NKT.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overestimation problem",
                "sec_num": "2.3"
            },
            {
                "text": "\u2022 Precision: P = c/|h|, where c is the number of corresponding words and |h| is the number of words in the hypothesis sentence h.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overestimation problem",
                "sec_num": "2.3"
            },
            {
                "text": "\u2022 Recall: R = c/r, where |r| is the number of words in the reference sentence r.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overestimation problem",
                "sec_num": "2.3"
            },
            {
                "text": "\u2022 F-measure:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overestimation problem",
                "sec_num": "2.3"
            },
            {
                "text": "F \u03b2 = (1 + \u03b2 2 )P R/(\u03b2 2 P + R),",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overestimation problem",
                "sec_num": "2.3"
            },
            {
                "text": "where \u03b2 is a parameter.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overestimation problem",
                "sec_num": "2.3"
            },
            {
                "text": "In (R2)&(H2)'s case, precision is 5/7 = 0.714 and recall is 5/5 = 1.000.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overestimation problem",
                "sec_num": "2.3"
            },
            {
                "text": "Which metric should we use? Our preliminary experiments with NTCIR-7 data showed that precision correlated best with adequacy among these three metrics (P , R, and F \u03b2=1 ). In addition, BLEU is essentially made for precision. Therefore, precision seems the most promising modifier.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overestimation problem",
                "sec_num": "2.3"
            },
            {
                "text": "The right graph of Figure 2 shows a scatter plot of precision and normalized average adequacy. The graph shows that precision has more correlation with adequacy than BP. We can observe that sentences with very small P values usually obtain very low adequacy scores but those with mediocre P values often obtain good adequacy scores.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 26,
                        "end": 27,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Overestimation problem",
                "sec_num": "2.3"
            },
            {
                "text": "If we multiply P directly by NSR or NKT, those sentences with mediocre P values will lose too much of their scores. The use of \u221a x will mitigate this problem. Since \u221a P is closer to 1.0 than P itself, multiplication of \u221a P instead of P itself will save these sentences. If we apply \u221a x twice ( \u221a P = 4 \u221a P ), it will further save them. Therefore, we expect \u00d7 \u221a P and \u00d7 4 \u221a P to work better than \u00d7P . Now, we propose two new metrics: NSRP \u03b1 and NKTP \u03b1 , where \u03b1 is a parameter (0 \u2264 \u03b1 \u2264 1).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overestimation problem",
                "sec_num": "2.3"
            },
            {
                "text": "In order to compare automatic translation evaluation methods, we use submissions to the NTCIR-7 Patent Translation (PATMT) task (Fujii et al., 2008) . Fourteen MT systems participated in the Japanese-English intrinsic evaluation. There were two Rule-Based MT (RMBT) systems and one Examplebased MT (EBMT) system. All other systems were Statistical MT (SMT) systems. The task organizers provided a baseline SMT system. These 15 systems translated 1,381 Japanese sentences into English. The organizers evaluated these translations by using BLEU and human judgments. In the human judgements, three experts independently evaluated 100 selected sentences in terms of 'adequacy' and 'fluency.'",
                "cite_spans": [
                    {
                        "start": 128,
                        "end": 148,
                        "text": "(Fujii et al., 2008)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Meta-evaluation with NTCIR-7 data",
                "sec_num": "3.1"
            },
            {
                "text": "For automatic evaluation, we used a single reference sentence for each of these 100 manually evaluated sentences. Echizen-ya et al. (2009) used multireference data, but their data is not publicly available yet.",
                "cite_spans": [
                    {
                        "start": 114,
                        "end": 138,
                        "text": "Echizen-ya et al. (2009)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Meta-evaluation with NTCIR-7 data",
                "sec_num": "3.1"
            },
            {
                "text": "For this meta-evaluation, we measured the corpus-level correlation between the human evaluation scores and the automatic evaluation scores. We simply averaged scores of 100 sentences for the proposed metrics. For existing metrics such as BLEU, we followed their definitions for corpus-level evaluation instead of simple averages of sentence-level scores. We used default settings for conventional metrics, but we tuned GTM (Melamed et al., 2007) with -e option. This option controls preferences on longer word runs. We also used the paraphrase database TERp (http://www.umiacs.umd.",
                "cite_spans": [
                    {
                        "start": 423,
                        "end": 445,
                        "text": "(Melamed et al., 2007)",
                        "ref_id": "BIBREF12"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Meta-evaluation with NTCIR-7 data",
                "sec_num": "3.1"
            },
            {
                "text": "edu/\u02dcsnover/terp) for METEOR (Banerjee and Lavie, 2005) .",
                "cite_spans": [
                    {
                        "start": 29,
                        "end": 55,
                        "text": "(Banerjee and Lavie, 2005)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Meta-evaluation with NTCIR-7 data",
                "sec_num": "3.1"
            },
            {
                "text": "We developed our metric mainly for automatic evaluation of translation quality for distant language pairs such as Japanese-English, but we also want to know how well the metric works for similar language pairs. Therefore, we also use the WMT-07 data (Callison-Burch et al., 2007) This data has different language pairs: Spanish, French, German \u21d2 English. We exclude Czech-English because there were so few systems (See the footnote of p. 146 in their paper).",
                "cite_spans": [
                    {
                        "start": 250,
                        "end": 279,
                        "text": "(Callison-Burch et al., 2007)",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Meta-evaluation with WMT-07 data",
                "sec_num": "3.2"
            },
            {
                "text": "Table 1 shows the main results of this paper. The left part has corpus-level meta-evaluation with adequacy. Error metrics, WER, PER, and TER, have negative correlation coefficients, but we did not show their minus signs here.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "1",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Meta-evaluation with NTCIR-7 data",
                "sec_num": "4.1"
            },
            {
                "text": "Both NSR-based metrics and NKT-based metrics perform better than conventional metrics for this NT-CIR PATMT JE translation data. As we expected, \u00d7BP and \u00d7P (1/1) performed badly. Spearman of BP itself is zero.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Meta-evaluation with NTCIR-7 data",
                "sec_num": "4.1"
            },
            {
                "text": "NKT performed slightly better than NSR. Perhaps, NSR penalized alternative good translations too much. However, one of the NSR-based metrics, NSRP 1/4 , gave the best Spearman score of 0.947, and the difference between NSRP \u03b1 and NKTP \u03b1 was small. Modification with P led to this improvement.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Meta-evaluation with NTCIR-7 data",
                "sec_num": "4.1"
            },
            {
                "text": "NKT gave the best Pearson score of 0.922. However, Pearson measures linearity and we can change its score through a nonlinear monotonic function without changing Spearman very much. For instance, (NSRP 1/4 ) 1.5 also has Spearman of 0.947 but its Pearson is 0.931, which is better than NKT's 0.922. Thus, we think Spearman is a better metaevaluation metric than Pearson. The right part of Table 1 shows correlation with fluency, but adequacy is more important, because our motivation is to provide a metric that is useful to reduce incomprehensible or misunderstanding outputs of MT systems. Again, the correlation-based metrics gave better scores than conventional metrics, and BP performed badly. NSR-based metrics proved to be as good as NKT-based metrics.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 395,
                        "end": 396,
                        "text": "1",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Meta-evaluation with NTCIR-7 data",
                "sec_num": "4.1"
            },
            {
                "text": "Meta-evaluation scores of the de facto standard BLEU is much lower than those of other metrics. Echizen-ya et al. (2009) reported that IMPACT performed very well for sentence-level evaluation of NTCIR-7 PATMT JE data. This corpus-level result also shows that IMPACT works better than BLEU, but ROUGE-L, WER, and our methods give better scores than IMPACT. 2007) have performed different human evaluation methods for different language pairs and different corpora. Their Table 5 shows inter-annotator agreements for the human evaluation methods. According to their table, the \"sentence ranking\" (or \"rank\") method obtained better agreement than \"adequacy.\" Therefore, we show Spearman's \u03c1 for \"rank.\" We used the scores given in their Tables 9, 10, and 11. (The \"constituent\" methods obtained the best inter-annotator agreement, but these methods focus on local translation quality and have nothing to do with global word order, which we are discussing here.)",
                "cite_spans": [
                    {
                        "start": 96,
                        "end": 120,
                        "text": "Echizen-ya et al. (2009)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 476,
                        "end": 477,
                        "text": "5",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Meta-evaluation with NTCIR-7 data",
                "sec_num": "4.1"
            },
            {
                "text": "Table 2 shows that our metrics designed for distant language pairs are comparable to conventional methods even for similar language pairs, but ROUGE-L and ROUGE-S performed better than ours for French News Corpus and German Europarl. BLEU scores in this table agree with those in Table 17 of Callison-Burch et al. (2007) within rounding errors.",
                "cite_spans": [
                    {
                        "start": 292,
                        "end": 320,
                        "text": "Callison-Burch et al. (2007)",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "2",
                        "ref_id": "TABREF2"
                    },
                    {
                        "start": 286,
                        "end": 288,
                        "text": "17",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Meta-evaluation with NTCIR-7 data",
                "sec_num": "4.1"
            },
            {
                "text": "After some experiments, we noticed that the use of R instead of P often gives better scores for WMT-07, but it degrades NTCIR-7 scores. We can extend our metric by F \u03b2 , weighted harmonic mean of P and R, or any other interpolation, but the introduction of new parameters into our metric makes it difficult to control. Improvement without new parameters is beyond the scope of this paper.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Meta-evaluation with NTCIR-7 data",
                "sec_num": "4.1"
            },
            {
                "text": "It has come to our attention that Birch et al. (2010) has independently proposed an automatic evaluation method based on Kendall's \u03c4 . First, they started with Kendall's \u03c4 distance, which can be written as \"1 -NKT\" in our terminology, and then subtracted it from one. Thus, their metric is nothing but NKT.",
                "cite_spans": [
                    {
                        "start": 34,
                        "end": 53,
                        "text": "Birch et al. (2010)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discussion",
                "sec_num": "5"
            },
            {
                "text": "Then, they proposed application of the square root to get better Pearson by improving \"the sensitivity to small reorderings.\" Since they used \"Kendall's \u03c4 \" and \"Kendall's \u03c4 distance\" interchangeably, it is not clear what they mean by \" \u221a Kendall's \u03c4 ,\" but perhaps they mean 1 -\u221a 1 -NKT because \u221a NKT is more insensitive to small reorderings. Table 3 shows the performance of these metrics for NTCIR-7 data. Pearson's correlation coefficient with adequacy was improved by 1 -\u221a 1 -NKT, but other scores were degraded in this experiment.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 350,
                        "end": 351,
                        "text": "3",
                        "ref_id": "TABREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Discussion",
                "sec_num": "5"
            },
            {
                "text": "The difference between our method and Birch et al. ( 2010)'s method comes from the fact that we used Japanese-English translation data and Spearman's correlation for meta-evaluation, whereas they used Chinese-English translation data and only Pearson's correlation for meta-evaluation. Chinese word order is different from English, but Chinese is a Subject-Verb-Object (SVO) language and thus is much closer to English word order than Japanese, a typical SOV language.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discussion",
                "sec_num": "5"
            },
            {
                "text": "We preferred NSR because it penalizes global word order mistakes much more than does NKT, and as discussed above, global word order mistakes often lead to incomprehensibility and misunderstanding.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discussion",
                "sec_num": "5"
            },
            {
                "text": "On the other hand, they also tried Hamming distance, and summarized their experiments as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discussion",
                "sec_num": "5"
            },
            {
                "text": "However, the Hamming distance seems to be more informative than Kendall's tau for small amounts of reordering. This sentence and the introduction of the square root to NKT imply that Chinese word order is close to that of English, and they have to measure subtle word order mistakes. In spite of these differences, the two groups independently recognized the usefulness of rank correlations for automatic evaluation of translation quality for distant language pairs.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discussion",
                "sec_num": "5"
            },
            {
                "text": "In their WMT-2010 paper (Birch and Osborne, 2010) , they multiplied NKT with the brevity penalty and interpolated it with BLEU for the WMT-2010 shared task. This fact implies that incomprehensible or misleading word order mistakes are rare in translation among European languages.",
                "cite_spans": [
                    {
                        "start": 24,
                        "end": 49,
                        "text": "(Birch and Osborne, 2010)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discussion",
                "sec_num": "5"
            },
            {
                "text": "When Statistical Machine Translation is applied to distant language pairs such as Japanese and English, word order becomes an important problem. SMT systems often fail to find an appropriate translation because of a large search space. Therefore, they often output misleading or incomprehensible sentences such as \"A because B\" vs. \"B because A.\" To penalize such inadequate translations, we presented an automatic evaluation method based on rank correlation. There were two questions for this approach. First, which correlation coefficient should we use: Spearman's \u03c1 or Kendall's \u03c4 ? Second, how should we solve the overestimation problem caused by the nature of one-to-one correspondence?",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusions",
                "sec_num": "6"
            },
            {
                "text": "We answered these questions through our experiments using the NTCIR-7 PATMT JE translation data. For the first question, \u03c4 was slightly better than \u03c1, but \u03c1 was improved by precision. For the second question, it turned out that BLEU's Brevity Penalty was counter-productive. A precision-based penalty gave a better solution. With this precisionbased penalty, both \u03c1 and \u03c4 worked well and they outperformed conventional methods for NTCIR-7 data. For similar language pairs, our method was comparable to conventional evaluation methods. Fu-",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusions",
                "sec_num": "6"
            }
        ],
        "back_matter": [
            {
                "text": "ture work includes extension of the method so that it can outperform conventional methods even for similar language pairs.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "acknowledgement",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Meteor: An automatic metric for MT evaluation with improved correlation with human judgements",
                "authors": [
                    {
                        "first": "Satanjeev",
                        "middle": [],
                        "last": "Banerjee",
                        "suffix": ""
                    },
                    {
                        "first": "Alon",
                        "middle": [],
                        "last": "Lavie",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proc. of ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and Summarization",
                "volume": "",
                "issue": "",
                "pages": "65--72",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for MT evaluation with improved correlation with human judgements. In Proc. of ACL Workshop on Intrinsic and Extrinsic Evaluation Mea- sures for MT and Summarization, pages 65-72.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "LRscore for evaluating lexical and reordering quality in MT",
                "authors": [
                    {
                        "first": "Alexandra",
                        "middle": [],
                        "last": "Birch",
                        "suffix": ""
                    },
                    {
                        "first": "Miles",
                        "middle": [],
                        "last": "Osborne",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR",
                "volume": "",
                "issue": "",
                "pages": "327--332",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alexandra Birch and Miles Osborne. 2010. LRscore for evaluating lexical and reordering quality in MT. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 327- 332.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Metrics for MT evaluation: evaluating reordering",
                "authors": [
                    {
                        "first": "Alexandra",
                        "middle": [],
                        "last": "Birch",
                        "suffix": ""
                    },
                    {
                        "first": "Miles",
                        "middle": [],
                        "last": "Osborne",
                        "suffix": ""
                    },
                    {
                        "first": "Phil",
                        "middle": [],
                        "last": "Blunsom",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Machine Translation",
                "volume": "24",
                "issue": "1",
                "pages": "15--26",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alexandra Birch, Miles Osborne, and Phil Blunsom. 2010. Metrics for MT evaluation: evaluating reorder- ing. Machine Translation, 24(1):15-26.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Re-evaluatiing the role of Bleu in machine translation research",
                "authors": [
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Callison-Burch",
                        "suffix": ""
                    },
                    {
                        "first": "Miles",
                        "middle": [],
                        "last": "Osborne",
                        "suffix": ""
                    },
                    {
                        "first": "Philipp",
                        "middle": [],
                        "last": "Koehn",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Proc. of the Conference of the European Chapter of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "249--256",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chris Callison-Burch, Miles Osborne, and Philipp Koehn. 2006. Re-evaluatiing the role of Bleu in ma- chine translation research. In Proc. of the Conference of the European Chapter of the Association for Com- putational Linguistics, pages 249-256.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Evaluation of machine translation",
                "authors": [
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Callison-Burch",
                        "suffix": ""
                    },
                    {
                        "first": "Cameron",
                        "middle": [],
                        "last": "Fordyce",
                        "suffix": ""
                    },
                    {
                        "first": "Philipp",
                        "middle": [],
                        "last": "Koehn",
                        "suffix": ""
                    },
                    {
                        "first": "Chrstof",
                        "middle": [],
                        "last": "Monz",
                        "suffix": ""
                    },
                    {
                        "first": "Josh",
                        "middle": [],
                        "last": "Schroeder",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Proc. of the Workshop on Machine Translation (WMT)",
                "volume": "",
                "issue": "",
                "pages": "136--158",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Chrstof Monz, and Josh Schroeder. 2007. (Meta-)Evaluation of machine translation. In Proc. of the Workshop on Machine Translation (WMT), pages 136-158.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "BLEU in characters: towards automatic MT evaluation in languages without word delimiters",
                "authors": [
                    {
                        "first": "Etienne",
                        "middle": [],
                        "last": "Denoual",
                        "suffix": ""
                    },
                    {
                        "first": "Yves",
                        "middle": [],
                        "last": "Lepage",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Companion Volume to the Proceedings of the Second International Joint Conference on Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "81--86",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Etienne Denoual and Yves Lepage. 2005. BLEU in char- acters: towards automatic MT evaluation in languages without word delimiters. In Companion Volume to the Proceedings of the Second International Joint Confer- ence on Natural Language Processing, pages 81-86.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Automatic evaluation of machine translation based on recursive acquisition of an intuitive common parts continuum",
                "authors": [
                    {
                        "first": "Hiroshi",
                        "middle": [],
                        "last": "Echizen",
                        "suffix": ""
                    },
                    {
                        "first": "-Ya",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Kenji",
                        "middle": [],
                        "last": "Araki",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Proceedings of MT Summit XII Workshop on Patent Translation",
                "volume": "",
                "issue": "",
                "pages": "151--158",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Hiroshi Echizen-ya and Kenji Araki. 2007. Automatic evaluation of machine translation based on recursive acquisition of an intuitive common parts continuum. In Proceedings of MT Summit XII Workshop on Patent Translation, pages 151-158.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Metaevaluation of automatic evaluation methods for machine translation using patent translation data in ntcir-7",
                "authors": [
                    {
                        "first": "Terumasa",
                        "middle": [],
                        "last": "Hiroshi Echizen-Ya",
                        "suffix": ""
                    },
                    {
                        "first": "Sayori",
                        "middle": [],
                        "last": "Ehara",
                        "suffix": ""
                    },
                    {
                        "first": "Atsushi",
                        "middle": [],
                        "last": "Shimohata",
                        "suffix": ""
                    },
                    {
                        "first": "Masao",
                        "middle": [],
                        "last": "Fujii",
                        "suffix": ""
                    },
                    {
                        "first": "Mikio",
                        "middle": [],
                        "last": "Utiyama",
                        "suffix": ""
                    },
                    {
                        "first": "Takehito",
                        "middle": [],
                        "last": "Yamamoto",
                        "suffix": ""
                    },
                    {
                        "first": "Noriko",
                        "middle": [],
                        "last": "Utsuro",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Kando",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of the 3rd Workshop on Patent Translation",
                "volume": "",
                "issue": "",
                "pages": "9--16",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Hiroshi Echizen-ya, Terumasa Ehara, Sayori Shimohata, Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, Takehito Utsuro, and Noriko Kando. 2009. Meta- evaluation of automatic evaluation methods for ma- chine translation using patent translation data in ntcir- 7. In Proceedings of the 3rd Workshop on Patent Translation, pages 9-16.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Overview of the patent translation task at the NTCIR-7 workshop",
                "authors": [
                    {
                        "first": "Atsushi",
                        "middle": [],
                        "last": "Fujii",
                        "suffix": ""
                    },
                    {
                        "first": "Masao",
                        "middle": [],
                        "last": "Utiyama",
                        "suffix": ""
                    },
                    {
                        "first": "Mikio",
                        "middle": [],
                        "last": "Yamamoto",
                        "suffix": ""
                    },
                    {
                        "first": "Takehito",
                        "middle": [],
                        "last": "Utsuro",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Working Notes of the NTCIR Workshop Meeting (NTCIR)",
                "volume": "",
                "issue": "",
                "pages": "389--400",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and Takehito Utsuro. 2008. Overview of the patent translation task at the NTCIR-7 workshop. In Work- ing Notes of the NTCIR Workshop Meeting (NTCIR), pages 389-400.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Head Finalization: A simple reordering rule for SOV languages",
                "authors": [
                    {
                        "first": "Hideki",
                        "middle": [],
                        "last": "Isozaki",
                        "suffix": ""
                    },
                    {
                        "first": "Katsuhito",
                        "middle": [],
                        "last": "Sudoh",
                        "suffix": ""
                    },
                    {
                        "first": "Hajime",
                        "middle": [],
                        "last": "Tsukada",
                        "suffix": ""
                    },
                    {
                        "first": "Kevin",
                        "middle": [],
                        "last": "Duh",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR",
                "volume": "",
                "issue": "",
                "pages": "250--257",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and Kevin Duh. 2010. Head Finalization: A simple re- ordering rule for SOV languages. In Proceedings of the Joint Fifth Workshop on Statistical Machine Trans- lation and MetricsMATR, pages 250-257.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Rank Correlation Methods",
                "authors": [
                    {
                        "first": "Maurice",
                        "middle": [
                            "G"
                        ],
                        "last": "Kendall",
                        "suffix": ""
                    }
                ],
                "year": 1975,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Maurice G. Kendall. 1975. Rank Correlation Methods. Charles Griffin.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Automatic evaluation of summaries using n-gram co-occurrence statistics",
                "authors": [
                    {
                        "first": "Chin-Yew",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Eduard",
                        "middle": [],
                        "last": "Hovy",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Proc. of the North American Chapter of the Association of Computational Linguistics (NAACL)",
                "volume": "",
                "issue": "",
                "pages": "71--78",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chin-Yew Lin and Eduard Hovy. 2003. Automatic evalu- ation of summaries using n-gram co-occurrence statis- tics. In Proc. of the North American Chapter of the Association of Computational Linguistics (NAACL), pages 71-78.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Precision and recall of machine translation",
                "authors": [
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Melamed",
                        "suffix": ""
                    },
                    {
                        "first": "Ryan",
                        "middle": [],
                        "last": "Green",
                        "suffix": ""
                    },
                    {
                        "first": "Joseph",
                        "middle": [
                            "P"
                        ],
                        "last": "Turian",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Proc. of NAACL-HLT",
                "volume": "",
                "issue": "",
                "pages": "61--63",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dan Melamed, Ryan Green, and Joseph P. Turian. 2007. Precision and recall of machine translation. In Proc. of NAACL-HLT, pages 61-63.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "John Henderson, and Florence Reeder. 2002a. Corpus-based comprehensive and diagnostic MT evaluation: Initial Arabic, Chinese, French, and Spanish Results",
                "authors": [
                    {
                        "first": "Kishore",
                        "middle": [],
                        "last": "Papineni",
                        "suffix": ""
                    },
                    {
                        "first": "Salim",
                        "middle": [],
                        "last": "Roukos",
                        "suffix": ""
                    },
                    {
                        "first": "Todd",
                        "middle": [],
                        "last": "Ward",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "Proc. of the International Conference on Human Language Technology Research (HLT)",
                "volume": "",
                "issue": "",
                "pages": "132--136",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kishore Papineni, Salim Roukos, Todd Ward, John Hen- derson, and Florence Reeder. 2002a. Corpus-based comprehensive and diagnostic MT evaluation: Initial Arabic, Chinese, French, and Spanish Results. In Proc. of the International Conference on Human Lan- guage Technology Research (HLT), pages 132-136.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "BLEU: a method for automatic evaluation of machine translation",
                "authors": [
                    {
                        "first": "Kishore",
                        "middle": [],
                        "last": "Papineni",
                        "suffix": ""
                    },
                    {
                        "first": "Salim",
                        "middle": [],
                        "last": "Roukos",
                        "suffix": ""
                    },
                    {
                        "first": "Todd",
                        "middle": [],
                        "last": "Ward",
                        "suffix": ""
                    },
                    {
                        "first": "Wei-Jing",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Proc. of the Annual Meeting of the Association of Computational Linguistics (ACL)",
                "volume": "",
                "issue": "",
                "pages": "311--318",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002b. BLEU: a method for automatic eval- uation of machine translation. In Proc. of the Annual Meeting of the Association of Computational Linguis- tics (ACL), pages 311-318.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "A study of translation edit rate with targeted human annotation",
                "authors": [
                    {
                        "first": "Matthew",
                        "middle": [],
                        "last": "Snover",
                        "suffix": ""
                    },
                    {
                        "first": "Bonnie",
                        "middle": [],
                        "last": "Dorr",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Schwartz",
                        "suffix": ""
                    },
                    {
                        "first": "Linnea",
                        "middle": [],
                        "last": "Micciulla",
                        "suffix": ""
                    },
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Makhoul",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Proceedings of Association for Machine Translation in the Americas",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin- nea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings of Association for Machine Translation in the Americas.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 1: Word alignment algorithm for rank correlation",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Figure 2: Scatter plots of normalized average adequacy with brevity penalty (left) and precision (right).(Each corresponds to one sentence generated by one MT system)",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table/>",
                "type_str": "table",
                "text": "that covers only European language pairs. Callison-Burch et al. (2007) tried different human evaluation methods and showed detailed evaluation scores. The Europarl test set has 2,000 sentences, and The News Commentary test set has 2,007 sentences.",
                "html": null,
                "num": null
            },
            "TABREF1": {
                "content": "<table><tr><td>human judge</td><td colspan=\"2\">Adequacy</td><td>Fluency</td><td/></tr><tr><td>eval\\ meta-eval</td><td>Spm</td><td>Prs</td><td>Spm</td><td>Prs</td></tr><tr><td>P</td><td colspan=\"2\">0.615 0.704</td><td colspan=\"2\">0.672 0.876</td></tr><tr><td>R</td><td colspan=\"2\">0.436 0.669</td><td colspan=\"2\">0.461 0.854</td></tr><tr><td>F \u03b2=1</td><td colspan=\"2\">0.525 0.692</td><td colspan=\"2\">0.543 0.871</td></tr><tr><td>BP</td><td colspan=\"4\">0.000 0.515 -0.007 0.742</td></tr><tr><td>NSR</td><td colspan=\"2\">0.904 0.906</td><td colspan=\"2\">0.869 0.910</td></tr><tr><td>NSRP 1/8</td><td colspan=\"2\">0.937 0.905</td><td colspan=\"2\">0.890 0.934</td></tr><tr><td>NSRP 1/4</td><td colspan=\"2\">0.947 0.900</td><td colspan=\"2\">0.901 0.944</td></tr><tr><td>NSRP 1/2</td><td colspan=\"2\">0.937 0.890</td><td colspan=\"2\">0.926 0.949</td></tr><tr><td>NSRP 1/1</td><td colspan=\"2\">0.883 0.872</td><td colspan=\"2\">0.883 0.939</td></tr><tr><td>NSR \u00d7 BP</td><td colspan=\"2\">0.851 0.874</td><td colspan=\"2\">0.769 0.910</td></tr><tr><td>NKT</td><td colspan=\"2\">0.940 0.922</td><td colspan=\"2\">0.887 0.931</td></tr><tr><td>NKTP 1/8</td><td colspan=\"2\">0.940 0.913</td><td colspan=\"2\">0.908 0.944</td></tr><tr><td>NKTP 1/4</td><td colspan=\"2\">0.940 0.904</td><td colspan=\"2\">0.908 0.949</td></tr><tr><td>NKTP 1/2</td><td colspan=\"2\">0.929 0.890</td><td colspan=\"2\">0.897 0.949</td></tr><tr><td>NKTP 1/1</td><td colspan=\"2\">0.897 0.869</td><td colspan=\"2\">0.879 0.936</td></tr><tr><td>NKT \u00d7 BP</td><td colspan=\"2\">0.829 0.878</td><td colspan=\"2\">0.726 0.918</td></tr><tr><td>ROUGE-L</td><td colspan=\"2\">0.903 0.874</td><td colspan=\"2\">0.889 0.932</td></tr><tr><td>ROUGE-S(4)</td><td colspan=\"2\">0.593 0.757</td><td colspan=\"2\">0.640 0.869</td></tr><tr><td>IMPACT</td><td colspan=\"2\">0.797 0.813</td><td colspan=\"2\">0.751 0.932</td></tr><tr><td>WER</td><td colspan=\"2\">0.894 0.822</td><td colspan=\"2\">0.836 0.926</td></tr><tr><td>TER</td><td colspan=\"2\">0.854 0.806</td><td colspan=\"2\">0.372 0.856</td></tr><tr><td>PER</td><td colspan=\"2\">0.375 0.642</td><td colspan=\"2\">0.393 0.842</td></tr><tr><td colspan=\"3\">METEOR(TERp) 0.490 0.708</td><td colspan=\"2\">0.508 0.878</td></tr><tr><td>GTM(-e 12)</td><td colspan=\"2\">0.618 0.723</td><td colspan=\"2\">0.601 0.850</td></tr><tr><td>NIST</td><td colspan=\"2\">0.343 0.661</td><td colspan=\"2\">0.372 0.856</td></tr><tr><td>BLEU</td><td colspan=\"2\">0.515 0.653</td><td colspan=\"2\">0.500 0.795</td></tr></table>",
                "type_str": "table",
                "text": "NTCIR-7 Meta-evaluation: correlation with human judgments (Spm = Spearman, Prs = Pearson)",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table><tr><td/><td colspan=\"2\">Spearman's \u03c1 with human \"rank\"</td><td/></tr><tr><td>source</td><td>French</td><td>Spanish</td><td>German</td></tr><tr><td>NSR</td><td colspan=\"3\">0.775 0.837 0.523 0.766 0.700 0.593</td></tr><tr><td colspan=\"4\">NSRP 1/8 0.821 0.857 0.786 0.595 0.400 0.685</td></tr><tr><td colspan=\"4\">NSRP 1/4 0.821 0.857 0.786 0.455 0.400 0.714</td></tr><tr><td colspan=\"4\">NSRP 1/2 0.821 0.857 0.786 0.347 0.400 0.714</td></tr><tr><td>NKT</td><td colspan=\"3\">0.845 0.857 0.607 0.838 0.700 0.630</td></tr><tr><td colspan=\"4\">NKTP 1/8 0.793 0.857 0.786 0.595 0.400 0.714</td></tr><tr><td colspan=\"4\">NKTP 1/4 0.793 0.857 0.786 0.524 0.400 0.714</td></tr><tr><td colspan=\"4\">NKTP 1/2 0.793 0.857 0.786 0.347 0.400 0.714</td></tr><tr><td>BLEU</td><td colspan=\"3\">0.786 0.679 0.750 0.595 0.400 0.821</td></tr><tr><td>WER</td><td colspan=\"3\">0.607 0.857 0.750 0.429 0.000 0.500</td></tr><tr><td colspan=\"4\">ROUGEL 0.893 0.739 0.786 0.707 0.700 0.857</td></tr><tr><td colspan=\"4\">ROUGES 0.883 0.679 0.786 0.690 0.400 0.929</td></tr><tr><td colspan=\"4\">4.2 Meta-evaluation with WMT-07 data</td></tr><tr><td colspan=\"2\">Callison-Burch et al. (</td><td/><td/></tr></table>",
                "type_str": "table",
                "text": "WMT-07 meta-evaluation: Each source language has two columns: the left one is News Corpus and the right one is Europarl.",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table><tr><td/><td>NKT</td><td>\u221a</td><td colspan=\"2\">NKT b(NKT)</td></tr><tr><td colspan=\"2\">Spearman w/ adequacy 0.940</td><td colspan=\"2\">0.940</td><td>0.922</td></tr><tr><td>Pearson w/ adequacy</td><td>0.922</td><td colspan=\"2\">0.817</td><td>0.941</td></tr><tr><td>Spearman w/ fluency</td><td>0.887</td><td colspan=\"2\">0.865</td><td>0.858</td></tr><tr><td>Pearson w/ fluency</td><td>0.931</td><td colspan=\"2\">0.917</td><td>0.833</td></tr></table>",
                "type_str": "table",
                "text": "NTCIR-7 meta-evaluation: Effects of square root (b(x) = 1 -\u221a 1 -x)",
                "html": null,
                "num": null
            }
        }
    }
}