{
    "paper_id": "D14-1007",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:01:06.754944Z"
    },
    "title": "Policy Learning for Domain Selection in an Extensible Multi-domain Spoken Dialogue System",
    "authors": [
        {
            "first": "Zhuoran",
            "middle": [],
            "last": "Wang",
            "suffix": "",
            "affiliation": {},
            "email": "zhuoran.wang@hw.ac.uk"
        },
        {
            "first": "Hongliang",
            "middle": [],
            "last": "Chen",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Guanchun",
            "middle": [],
            "last": "Wang",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Hao",
            "middle": [],
            "last": "Tian",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Hua",
            "middle": [],
            "last": "Wu",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Haifeng",
            "middle": [],
            "last": "Wang",
            "suffix": "",
            "affiliation": {},
            "email": ""
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "This paper proposes a Markov Decision Process and reinforcement learning based approach for domain selection in a multidomain Spoken Dialogue System built on a distributed architecture. In the proposed framework, the domain selection problem is treated as sequential planning instead of classification, such that confirmation and clarification interaction mechanisms are supported. In addition, it is shown that by using a model parameter tying trick, the extensibility of the system can be preserved, where dialogue components in new domains can be easily plugged in, without re-training the domain selection policy. The experimental results based on human subjects suggest that the proposed model marginally outperforms a non-trivial baseline.",
    "pdf_parse": {
        "paper_id": "D14-1007",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "This paper proposes a Markov Decision Process and reinforcement learning based approach for domain selection in a multidomain Spoken Dialogue System built on a distributed architecture. In the proposed framework, the domain selection problem is treated as sequential planning instead of classification, such that confirmation and clarification interaction mechanisms are supported. In addition, it is shown that by using a model parameter tying trick, the extensibility of the system can be preserved, where dialogue components in new domains can be easily plugged in, without re-training the domain selection policy. The experimental results based on human subjects suggest that the proposed model marginally outperforms a non-trivial baseline.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Due to growing demand for natural humanmachine interaction, over the last decade Spoken Dialogue Systems (SDS) have been increasingly deployed in various commercial applications ranging from traditional call centre automation (e.g. AT&T \"Lets Go!\" bus information system (Williams et al., 2010) ) to mobile personal assistants and knowledge navigators (e.g. Apple's Siri R , Google Now TM , Microsoft Cortana, etc.) or voice interaction for smart household appliance control (e.g. Samsung Evolution Kit for Smart TVs). Furthermore, latest progress in openvocabulary Automatic Speech Recognition (ASR) is pushing SDS from traditional single-domain information systems towards more complex multidomain speech applications, of which typical examples are those voice assistant mobile applications.",
                "cite_spans": [
                    {
                        "start": 271,
                        "end": 294,
                        "text": "(Williams et al., 2010)",
                        "ref_id": "BIBREF22"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Recent advances in SDS have shown that statistical approaches to dialogue management can result in marginal improvement in both the naturalness and the task success rate for domainspecific dialogues (Lemon and Pietquin, 2012; Young et al., 2013) . State-of-the-art statistical SDS treat the dialogue problem as a sequential decision making process, and employ established planning models, such as Markov Decision Processes (MDPs) (Singh et al., 2002) or Partially Observable Markov Decision Processes (POMDPs) (Thomson and Young, 2010; Young et al., 2010; Williams and Young, 2007) , in conjunction with reinforcement learning techniques (Jur\u010d\u00ed\u010dek et al., 2011; Jur\u010d\u00ed\u010dek et al., 2012; Ga\u0161i\u0107 et al., 2013a) to seek optimal dialogue policies that maximise long-term expected (discounted) rewards and are robust to ASR errors.",
                "cite_spans": [
                    {
                        "start": 199,
                        "end": 225,
                        "text": "(Lemon and Pietquin, 2012;",
                        "ref_id": null
                    },
                    {
                        "start": 226,
                        "end": 245,
                        "text": "Young et al., 2013)",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 430,
                        "end": 450,
                        "text": "(Singh et al., 2002)",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 510,
                        "end": 535,
                        "text": "(Thomson and Young, 2010;",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 536,
                        "end": 555,
                        "text": "Young et al., 2010;",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 556,
                        "end": 581,
                        "text": "Williams and Young, 2007)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 638,
                        "end": 661,
                        "text": "(Jur\u010d\u00ed\u010dek et al., 2011;",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 662,
                        "end": 684,
                        "text": "Jur\u010d\u00ed\u010dek et al., 2012;",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 685,
                        "end": 705,
                        "text": "Ga\u0161i\u0107 et al., 2013a)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "However, to the best of our knowledge, most of the existing multi-domain SDS in public use are rule-based (e.g. (Gruber et al., 2012; Mirkovic and Cavedon, 2006) ). The application of statistical models in multi-domain dialogue systems is still preliminary. Komatani et al. (2006) and Nakano et al. (2011) utilised a distributed architecture (Lin et al., 1999) to integrate expert dialogue systems in different domains into a unified framework, where a central controller trained as a data-driven classifier selects a domain expert at each turn to address user's query. Alternatively, Hakkani-T\u00fcr et al. (2012) adopted the well-known Information State mechanism (Traum and Larsson, 2003) to construct a multi-domain SDS and proposed a discriminative classification model for more accurate state updates. More recently, Ga\u0161i\u0107 et al. (2013b) proposed that by a simple expansion of the kernel function in Gaussian Process (GP) reinforcement learning (Engel et al., 2005; Ga\u0161i\u0107 et al., 2013a) , one can adapt pre-trained dialogue policies to handle unseen slots for SDS in extended domains.",
                "cite_spans": [
                    {
                        "start": 112,
                        "end": 133,
                        "text": "(Gruber et al., 2012;",
                        "ref_id": null
                    },
                    {
                        "start": 134,
                        "end": 161,
                        "text": "Mirkovic and Cavedon, 2006)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 258,
                        "end": 280,
                        "text": "Komatani et al. (2006)",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 285,
                        "end": 305,
                        "text": "Nakano et al. (2011)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 342,
                        "end": 360,
                        "text": "(Lin et al., 1999)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 585,
                        "end": 610,
                        "text": "Hakkani-T\u00fcr et al. (2012)",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 662,
                        "end": 687,
                        "text": "(Traum and Larsson, 2003)",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 819,
                        "end": 839,
                        "text": "Ga\u0161i\u0107 et al. (2013b)",
                        "ref_id": null
                    },
                    {
                        "start": 947,
                        "end": 967,
                        "text": "(Engel et al., 2005;",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 968,
                        "end": 988,
                        "text": "Ga\u0161i\u0107 et al., 2013a)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this paper, we use a voice assistant applica- Figure 1 : The distributed architecture of the voice assistant system (a simplified illustration).",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 56,
                        "end": 57,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "tion (similar to Apple's Siri but in Chinese language) as an example to demonstrate a novel MDP-based approach for central interaction management in a complex multi-domain dialogue system. The voice assistant employs a distributed architecture similar to (Lin et al., 1999; Komatani et al., 2006; Nakano et al., 2011) , and handles mixed interactions of multi-turn dialogues across different domains and single-turn queries powered by a collection of information access services (such as web search, Question Answering (QA), etc.).",
                "cite_spans": [
                    {
                        "start": 255,
                        "end": 273,
                        "text": "(Lin et al., 1999;",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 274,
                        "end": 296,
                        "text": "Komatani et al., 2006;",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 297,
                        "end": 317,
                        "text": "Nakano et al., 2011)",
                        "ref_id": "BIBREF14"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In our system, the dialogues in each domain are managed by an individual domain expert SDS, and the single-turn services are used to handle those so-called out-of-domain requests. We use featurised representations to summarise the current dialogue states in each domain (see Section 3 for more details), and let the central controller (the MDP model) choose one of the following system actions at each turn: (1) addressing user's query based on a domain expert, (2) treating it as an out-of-domain request, (3) asking user to confirm whether he/she wants to continue a domain expert's dialogue or to switch to out-of-domain services, and (4) clarifying user's intention between two domains. The Gaussian Process Temporal Difference (GPTD) algorithm (Engel et al., 2005; Ga\u0161i\u0107 et al., 2013a ) is adopted here for policy optimisation based on human subjects, where a parameter tying trick is applied to preserve the extensibility of the system, such that new domain experts (dialogue systems) can be flexibly plugged in without the need of re-training the central controller.",
                "cite_spans": [
                    {
                        "start": 749,
                        "end": 769,
                        "text": "(Engel et al., 2005;",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 770,
                        "end": 789,
                        "text": "Ga\u0161i\u0107 et al., 2013a",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Comparing to the previous classification-based methods (Komatani et al., 2006; Nakano et al., 2011) , the proposed approach not only has the advantage of action selection in consideration of long-term rewards, it can also yield more robust policies that allow clarifications and confirmations to mitigate ASR and Spoken Language Understanding (SLU) errors. Our human evaluation results show that the proposed system with a trained MDP policy achieves significantly better naturalness in domain switching tasks than a non-trivial baseline with a hand-crafted policy.",
                "cite_spans": [
                    {
                        "start": 55,
                        "end": 78,
                        "text": "(Komatani et al., 2006;",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 79,
                        "end": 99,
                        "text": "Nakano et al., 2011)",
                        "ref_id": "BIBREF14"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The remainder of this paper is organised as follows. Section 2 defines the terminology used throughout the paper. Section 3 briefly overviews the distributed architecture of our system. The MDP model and the policy optimisation algorithm are introduced in Section 4 and Section 5, respectively. After this, experimental settings and evaluation results are described in Section 6. Finally, we discuss some possible improvements in Section 7 and conclude ourselves in Section 8.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "A voice assistant application provides a unified speech interface to a collection of individual information access systems. It aims to collect and satisfy user requests in an interactive manner, where different types of interactions can be involved. Here we focus ourselves on two interaction scenarios, i.e. task-oriented (multi-turn) dialogues and single-turn queries.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Terminology",
                "sec_num": "2"
            },
            {
                "text": "According to user intentions, the dialogue interactions in our voice assistant system can further be categorised into different domains, of which each is handled by a separate dialogue manager, namely a domain expert. Example domains include travel information, restaurant search, etc. In addition, some domains in our system can be further decomposed into sub-domains, e.g. the travel information domain consists of three sub-domains: flight ticket booking, train ticket booking and hotel reservation. We use an integrated domain expert to address queries in all its sub-domains, so that relevant information can be shared across those subdomains to allow intelligent induction in the dialogue flow.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Terminology",
                "sec_num": "2"
            },
            {
                "text": "For convenience of future reference, we call those single-turn information access systems outof-domain services or simply services for short. The services integrated in our system include web search, semantic search, QA, system command execution, weather report, chat-bot, and many more.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Terminology",
                "sec_num": "2"
            },
            {
                "text": "The voice assistant system introduced in this paper is built on a distributed architecture (Lin et al., 1999) , as shown in Figure 1 , where the dialogue flow is processed as follows. Firstly, a user's query (either an ASR utterance or directly typed in text) is passed to a user intention identifier, which labels the raw query with a list of intention hypotheses with confidence scores. Here an intention label could be either a domain name or a service name. After this, the central controller distributes the raw query together with its intention labels and confidence scores to all the domain experts and the service modules, which will attempt to process the query and return their results to the central controller.",
                "cite_spans": [
                    {
                        "start": 91,
                        "end": 109,
                        "text": "(Lin et al., 1999)",
                        "ref_id": "BIBREF12"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 131,
                        "end": 132,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "System Architecture",
                "sec_num": "3"
            },
            {
                "text": "The domain experts in the current implementation of our system are all rule-based SDS following the RavenClaw framework proposed in (Bohus and Rudnicky, 2009) . When receiving a query, a domain expert will use its own SLU module to parse the utterance or text input and try to update its dialogue state in consideration of both the SLU output and the intention labels. If the dialogue state in the domain expert can be updated given the query, it will return its output, internal session record and a confidence score to the central controller, where the output can be either a natural language utterance realised by its Natural Language Generation (NLG) module or a set of data records obtained from its database (if a database search operation is triggered), or both. If the domain expert cannot update its state using the current query, it will just return an empty result with a low confidence score. Similar procedures apply to those out-of-domain services as well, but there are no session records or confidence scores returned. Finally, given all the returned information, the central controller chooses, according to its policy, the module (either a domain expert or a service) whose results will be provided to the user.",
                "cite_spans": [
                    {
                        "start": 132,
                        "end": 158,
                        "text": "(Bohus and Rudnicky, 2009)",
                        "ref_id": "BIBREF2"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "System Architecture",
                "sec_num": "3"
            },
            {
                "text": "When the central controller decides to pass a domain expert's output to the user, we regard the domain expert as being activated. Also note here, the updated state of a domain expert in a turn will not be physically stored, unless the domain expert is activated in that turn. This is a necessary mechanism to prevent an inactive domain expert being misled by ambiguous queries in other domains.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "System Architecture",
                "sec_num": "3"
            },
            {
                "text": "In addition, we use a well-engineered priority ranker to rank the services based on the numbers of results they returned as well as some prior knowledge about the quality of their data sources. When the central controller decides to show user the results from an out-of-domain service, it will choose the top one from the ranked list.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "System Architecture",
                "sec_num": "3"
            },
            {
                "text": "The main focus of this paper is to seek a policy for robustly switching the control flow among those domain experts and services (the service ranker in practice) during a dialogue, where the user may have multiple or compound goals (e.g. booking a flight ticket, booking a restaurant in the destination city and checking the weather report of the departure or destination city).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "MDP Modelling of the Central Control Process",
                "sec_num": "4"
            },
            {
                "text": "In order to make the system robust to ASR errors or ambiguous queries, the central controller should also have basic dialogue abilities for confirmation and clarification purposes. Here we define the confirmation as an action of asking whether a user wants to continue the dialogue in a certain domain. If the system receives a negative response at this point, it will switch to out-of-domain services. On the other hand, the clarification action is de-fined between domains, in which case, the system will explicitly ask the user to choose between two domain candidates before continuing the dialogue.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "MDP Modelling of the Central Control Process",
                "sec_num": "4"
            },
            {
                "text": "Due to the confirmation and clarification mechanisms defined above, the central controller becomes a sequential decision maker that must take the overall smoothness of the dialogue into account. Therefore, we propose an MDP-based approach for learning an optimal central control policy in this section.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "MDP Modelling of the Central Control Process",
                "sec_num": "4"
            },
            {
                "text": "The potential state space of our MDP is huge, which in principle consists of the combinations of all possible situations of the domain experts and the out-of-domain services, therefore function approximation techniques must be employed to enable tractable computations. However, when developing such a complex application as the voice assistant here, one also needs to take the extensibility of the system into account, so that new domain experts can be easily integrated into the system without major re-training or re-engineering of the existing components. Essentially, it requires the state featurisation and the central control policy learnt here to be independent of the number of domain experts. In Section 4.3, we show that such a property can be achieved by a parameter tying trick in the definition of the MDP.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "MDP Modelling of the Central Control Process",
                "sec_num": "4"
            },
            {
                "text": "Let P X denote the set of probability distributions over a set X. An MDP is defined as a five tuple S, A, T, R, \u03b3 , where the components are defined as follows. S and A are the sets of system states and actions, respectively. T : S \u00d7 A \u2192 P S is the transition function, and T (s |s, a) defines the conditional probability of the system transiting from state s \u2208 S to state s \u2208 S after taking action a \u2208 A. R : S \u00d7 A \u2192 P R is the reward function with R(s, a) specifying the distribution of the immediate rewards for the system taking action a at state s. In addition, 0 \u2264 \u03b3 \u2264 1 is the discount factor on the summed sequence of rewards.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "MDP Preliminaries",
                "sec_num": "4.1"
            },
            {
                "text": "A finite-horizon MDP operates as follows. The system occupies a state s and takes an action a, which then will make it transit to a next state s \u223c T (\u2022|s, a) and receive a reward r \u223c R(s, a). This process repeats until a terminal state is reached.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "MDP Preliminaries",
                "sec_num": "4.1"
            },
            {
                "text": "For a given policy \u03c0 : S \u2192 A, the value function V \u03c0 is defined to be the expected cumulative reward, as",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "MDP Preliminaries",
                "sec_num": "4.1"
            },
            {
                "text": "V \u03c0 (s 0 ) = E n t=0 \u03b3 t r t | st,\u03c0(st)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "MDP Preliminaries",
                "sec_num": "4.1"
            },
            {
                "text": ", where s 0 is the starting state and n is the plan-ning horizon. The aim of policy optimisation is to seek an optimal policy \u03c0 * that maximises the value function. If T and R are given, in conjunction with a Q-function, the optimal value V * can be expressed by recursive equations as Q(s, a) = R(s, a) + \u03b3 s \u2208S T (s |s, a)V * (s ) and V * (s) = max a\u2208A Q(s, a) (here we assume R(s, a) is deterministic), which can be solved by dynamic programming (Bellman, 1957) . For problems with unknown T or R, such as dialogue systems, the Q-values are usually estimated via reinforcement learning (Sutton and Barto, 1998) .",
                "cite_spans": [
                    {
                        "start": 449,
                        "end": 464,
                        "text": "(Bellman, 1957)",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 589,
                        "end": 613,
                        "text": "(Sutton and Barto, 1998)",
                        "ref_id": "BIBREF18"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "MDP Preliminaries",
                "sec_num": "4.1"
            },
            {
                "text": "Let D denote the set of the domain experts in our voice assistant system, and s d be the current dialogue state of domain expert d \u2208 D at a certain timestamp. We also define s o as an abstract state to describe the current status of those out-of-domain services. Then mathematically we can represent the central control process as an MDP, where its state s is a joint set of the states of all the domain experts and the services, as s = {s d } d\u2208D \u222a {s o }. Four types of system actions are defined as follows.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Problem Definition",
                "sec_num": "4.2"
            },
            {
                "text": "\u2022 present(d): presenting the output of domain expert d to user;",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Problem Definition",
                "sec_num": "4.2"
            },
            {
                "text": "\u2022 present ood(null): presenting the results of the top-ranked out-of-domain service given by the service ranker;",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Problem Definition",
                "sec_num": "4.2"
            },
            {
                "text": "\u2022 confirm(d): confirming whether user wants to continue with domain expert d (or to switch to out-of-domain services); ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Problem Definition",
                "sec_num": "4.2"
            },
            {
                "text": "Function approximation is a commonly used technique to estimate the Q-values when the state space of the MDP is huge. Concretely, in our case, we assume that:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Function Approximation",
                "sec_num": "4.3"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "Q(s, a(x)) = f (\u03c6(s, a(x)); \u03b8)",
                        "eq_num": "(1)"
                    }
                ],
                "section": "Function Approximation",
                "sec_num": "4.3"
            },
            {
                "text": "where \u03c6 : S \u00d7 A \u2192 R K is a feature function that maps a state-action pair to an K-dimensional feature vector, and f : R K \u2192 R is a function of \u03c6(s, a(x)) parameterised by \u03b8. A frequent choice of f is the linear function, as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Function Approximation",
                "sec_num": "4.3"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "Q(s, a(x)) = \u03b8 \u03c6(s, a(x))",
                        "eq_num": "(2)"
                    }
                ],
                "section": "Function Approximation",
                "sec_num": "4.3"
            },
            {
                "text": "After this, the policy optimisation problem becomes learning the parameter \u03b8 to approximate the Q-values based on example dialogue trajectories. However, a crucial problem with the standard formulation in Eq. ( 2) is that the feature function \u03c6 is defined over the entire state and action spaces. In this case, when a new domain expert is integrated into the system, both the state space and the action space will be changed, therefore one will have to re-define the feature function and consequentially re-train the model. In order to achieve an extensible system, we make some simplification assumptions and decompose the feature function as follows. Firstly, we let:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Function Approximation",
                "sec_num": "4.3"
            },
            {
                "text": "\u03c6(s, a(x)) = \u03c6 a (s x ) (3) = \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 \u03c6 pr (s d ) if a(x) =present(d) \u03c6 ood (s o ) if a(x) =present ood() \u03c6 cf (s d ) if a(x) =confirm(d) \u03c6 cl (s d , s d ) if a(x) =clarify(d,d )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Function Approximation",
                "sec_num": "4.3"
            },
            {
                "text": "where the feature function is reduced to only depend on the state of the action's operand, instead of the entire system state. Then, we make those actions a(x) that have a same action type (a) but operate different domain experts (x) share the same parameter, i.e.:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Function Approximation",
                "sec_num": "4.3"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "Q(s, a(x)) = \u03b8 a \u03c6 a (s x )",
                        "eq_num": "(4)"
                    }
                ],
                "section": "Function Approximation",
                "sec_num": "4.3"
            },
            {
                "text": "This decomposition and parameter tying trick preserves the extensibility of the system, because both \u03b8 a and \u03c6 a are independent of x, when there is a new domain expert d, we can directly substitute its state s d into Eq. ( 3) and ( 4) to compute its corresponding Q-values.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Function Approximation",
                "sec_num": "4.3"
            },
            {
                "text": "Based on the problem formulation in Eq. ( 3) and (4), we shall only select high-level summary features to sketch the dialogue state and dialogue history of each domain expert, which must be applicable to all domain experts, regardless of their domain-specific characteristics or implementation differences. Suppose that the dialogue states of the M and L respectively denote the maximum numbers of required and optional slots for the domain experts. N is the maximum number of hypotheses that the intention identifier can return. Z + stands for the non-negative integer set.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Features",
                "sec_num": "4.4"
            },
            {
                "text": "domain experts can be represented as slot-value pairs1 , and for each domain there are required slots and optional slots, where all required slots must be filled before the domain expert can execute a database search operation. The features investigated in the proposed framework are listed in Table 1. Detailed featurisation in Eq. ( 3) is explained as follows. For \u03c6 pr , we choose the first 8 features plus a bias dimension that is always set to -1. Whilst, feature #9 plus a bias is used to define \u03c6 ood . All the features are used in \u03c6 cf , as to do a confirmation, one needs to consider the joint situation in and out of the domain. Finally, the feature function for a clarification action between two domains d and d is defined as \u03c6 cl (s d , s d ) = exp{-|\u03c6 pr (s d ) -\u03c6 pr (s d )|}, where we use | \u2022 | to denote the element-wise absolute of a vector operand. The intuition here is that the more distinguishable the (featurised) states of two domain experts are, the less we tend to clarify them.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Features",
                "sec_num": "4.4"
            },
            {
                "text": "For those domain experts that have multiple sub-domains with different numbers of required and optional slots, the feature extraction procedure only applies to the latest active sub-domain.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Features",
                "sec_num": "4.4"
            },
            {
                "text": "In addition, note that, the confidence scores provided by the user intention identifier are only used as features for out-of-domain services. This is because in the current version of our system, the confidence estimation of the intention identifier for domain-dependent dialogue queries is less reliable due to the lack of context information. In contrast, the confidence scores returned by the domain experts will be more informative at this point.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Features",
                "sec_num": "4.4"
            },
            {
                "text": "In traditional statistical SDS, dialogue policies are usually trained using reinforcement learning based on simulated dialogue trajectories (Schatzmann et al., 2007; Keizer et al., 2010; Thomson and Young, 2010; Young et al., 2010) . Although the evaluation of the simulators themselves could be an arguable issue, there are various advantages, e.g. hundreds of thousands of data examples can be easily generated for training and initial policy evaluation purposes, and different reinforcement learning models can be compared without incurring notable extra costs.",
                "cite_spans": [
                    {
                        "start": 140,
                        "end": 165,
                        "text": "(Schatzmann et al., 2007;",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 166,
                        "end": 186,
                        "text": "Keizer et al., 2010;",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 187,
                        "end": 211,
                        "text": "Thomson and Young, 2010;",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 212,
                        "end": 231,
                        "text": "Young et al., 2010)",
                        "ref_id": "BIBREF23"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Policy Learning with GPTD",
                "sec_num": "5"
            },
            {
                "text": "However, for more complex multi-domain SDS, particularly a voice assistant application like ours that aims at handling very complicated (ideally open-domain) dialogue scenarios, it would be difficult to develop a proper simulator that can reasonably mimic real human behaviours. Therefore, in this work, we learn the central control policy directly with human subjects, for which the following properties of the learning algorithm are required. Firstly and most importantly, the learner must be sample-efficient as the data collection procedure is costly. Secondly, the algorithm should support batch reinforcement learning. This is because when using function approximation, the learning process may not strictly converge, and the quality of the sequence of generated policies tends to oscillate after a certain number of improving steps at the beginning (Bertsekas and Tsitsiklis, 1996) . If online reinforcement learning is used, we will be unable to evaluate the generated policy after each update, and hence will not know which policy to keep for the final evaluation. Therefore, we do a batch policy update and iterate the learning process for a number of batches, such that the data collection phase in a new iteration yields an evaluation of the policy obtained from the previous iteration at the same time.",
                "cite_spans": [
                    {
                        "start": 856,
                        "end": 888,
                        "text": "(Bertsekas and Tsitsiklis, 1996)",
                        "ref_id": "BIBREF1"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Policy Learning with GPTD",
                "sec_num": "5"
            },
            {
                "text": "To fulfill the above two requirements, the Gaussian Process Temporal Difference (GPTD) algorithm (Engel et al., 2005) is a proper choice, due to its sample efficiency (Fard et al., 2011) and batch learning ability (Engel et al., 2005) , as well as its previous success in dialogue policy learning with human subjects (Ga\u0161i\u0107 et al., 2013a) . Note that, GPTD can also admit recursive (online) computations, but here we focus ourselves on the batch version.",
                "cite_spans": [
                    {
                        "start": 97,
                        "end": 117,
                        "text": "(Engel et al., 2005)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 167,
                        "end": 186,
                        "text": "(Fard et al., 2011)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 214,
                        "end": 234,
                        "text": "(Engel et al., 2005)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 317,
                        "end": 338,
                        "text": "(Ga\u0161i\u0107 et al., 2013a)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Policy Learning with GPTD",
                "sec_num": "5"
            },
            {
                "text": "A Gaussian Process (GP) is a generative model of Bayesian inference that can be used for function regression, and has the superiority of obtaining good posterior estimates with just a few observations (Rasmussen and Williams, 2006) . GPTD models the Q-function as a zero mean GP which defines correlations in different parts of the featurised state and action spaces through a kernel function \u03ba, as:",
                "cite_spans": [
                    {
                        "start": 216,
                        "end": 231,
                        "text": "Williams, 2006)",
                        "ref_id": "BIBREF15"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Policy Learning with GPTD",
                "sec_num": "5"
            },
            {
                "text": "Q(s, a(x)) \u223c GP(0, \u03ba((s x , a), (s x , a))) (5)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Policy Learning with GPTD",
                "sec_num": "5"
            },
            {
                "text": "Given a sequence of t state-action pairs X t = [(s 0 , a 0 (x 0 )), . . . , (s t , a t (x t ))] from a collection of dialogues and their corresponding immediate rewards r t = [r 0 , . . . , r t ], the posterior of Q(s, a(x)) for an arbitrary new state-action pair (s, a(x)) can be computed as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Policy Learning with GPTD",
                "sec_num": "5"
            },
            {
                "text": "Q(s, a(x))| Xt,rt \u223c N Q(s, a(x)), cov (s, a(x)) (6) Q(s, a(x)) = k t (s x , a) H t G -1 t r t (7) cov (s, a(x)) = \u03ba((s x , a), (s x , a)) -k t (s x , a) H t G -1 t H t k t (s x , a) (8) G t = H t K t H t + \u03c3 2 H t H t (9) H t = \uf8ee \uf8ef \uf8ef \uf8ef \uf8f0 1 -\u03b3 \u2022 \u2022 \u2022 0 0 0 1 \u2022 \u2022 \u2022 0 0 . . . . . . . . . . . . . . . 0 \u2022 \u2022 \u2022 0 1 -\u03b3 \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fb (10)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Policy Learning with GPTD",
                "sec_num": "5"
            },
            {
                "text": "where K t is the Gram matrix with elements K t (i, j) = \u03ba((s i x i , a i ), (s j x j , a j )), k t (s x , a) = [\u03ba((s i x i , a i ), (s x , a))] t i=0 is a vector, and \u03c3 is a hyperparameter specifying the diagonal covariance values of the zero-mean Gaussian noise. In addition, we use cov (s, a(x)) to denote (for short) the self-covariance cov (s, a(x), s, a(x)).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Policy Learning with GPTD",
                "sec_num": "5"
            },
            {
                "text": "In our case, as different feature functions \u03c6 a are defined for different action types, the kernel function is defined to be:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Policy Learning with GPTD",
                "sec_num": "5"
            },
            {
                "text": "\u03ba((s x , a), (s x , a )) = [[a = a ]]\u03ba a (s x , s x ) (11)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Policy Learning with GPTD",
                "sec_num": "5"
            },
            {
                "text": "where [[\u2022] ] is an indicator function and \u03ba a is the kernel function defined corresponding to the feature function \u03c6 a .",
                "cite_spans": [
                    {
                        "start": 6,
                        "end": 10,
                        "text": "[[\u2022]",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Policy Learning with GPTD",
                "sec_num": "5"
            },
            {
                "text": "Given a state, a most straightforward policy is to select the action that corresponds to the maximum mean Q-value estimated by the GP. However, since the objective is to learn the Q-function associated with the optimal policy by interacting directly with users, the policy must exhibit some form of stochastic behaviour in order to explore alternatives during the process of learning. In this work, the strategy employed for the explorationexploitation trade-off is that, during exploration, actions are chosen according to the variance of the GP estimate for the Q-function, and during exploitation, actions are chosen according to the mean. That is:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Policy Learning with GPTD",
                "sec_num": "5"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "\u03c0(s) = arg max a(x) Q(s, a(x)) : w.p. 1 - arg max a(x) cov (s, a(x)) : w.p. (",
                        "eq_num": "12"
                    }
                ],
                "section": "Policy Learning with GPTD",
                "sec_num": "5"
            },
            {
                "text": ") where 0 < < 1 is a pre-defined exploration rate, and will be exponentially reduced at each batch iteration during our learning process.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Policy Learning with GPTD",
                "sec_num": "5"
            },
            {
                "text": "Note that, in practice, not all the actions are valid at every possible state. For example, if a domain expert d has never been activated during a dialogue and can neither process the user's current query, the actions with an operand d will be regarded as invalid at this state. When executing the policy, we only consider those valid actions for a given state. 6 Experimental Results",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Policy Learning with GPTD",
                "sec_num": "5"
            },
            {
                "text": "We use the batch version of GPTD as described in Section 5 to learn the central control policy with human subjects. There are three domain experts available in our current system, but during the training only two domains are used, which are the travel information domain and the restaurant search domain. We reserve a movie search domain for evaluating the generalisation property of the learnt policy (see Section 6.2). The learning process started from a hand-crafted policy. Then 15 experienced users2 volunteered to contribute dialogue examples with multiple or compound goals (see Figure 4 for an instance), from whom we collected around 50\u223c70 dialogues per day for 5 days3 . After each dialogue, the users were asked to score the system from 5 to 1 according to a scoring standard shown in Table 2 . The scores were taken as the (delayed) rewards to train the GPTD model, where we set the rewards for intermediate turns to 0. The working policy was updated daily based on the data obtained up to that day. The data collected on the first day was used for preliminary experiments to choose the hyperparame- ters of the model, such as the kernel function, the kernel parameters (if applicable), and \u03c3 and \u03b3 in the GPTD model. We initially experimented with linear, polynomial and Gaussian kernels, with different configurations of \u03c3 and \u03b3 values, as well as kernel parameter values. It was found that the linear kernel in combination with \u03c3 = 5 and \u03b3 = 0.99 works more appropriate than the other settings. This configuration was then fixed for the rest iterations.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 593,
                        "end": 594,
                        "text": "4",
                        "ref_id": "FIGREF4"
                    },
                    {
                        "start": 802,
                        "end": 803,
                        "text": "2",
                        "ref_id": "TABREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Training",
                "sec_num": "6.1"
            },
            {
                "text": "The learning process was iterated for 4 days after the first one. On each day, we computed the mean and standard deviation of the user scores as an evaluation of the policy learnt on the previous day. The learning curve is illustrated in Figure 2 . Note here, as we were actually executing a stochastic policy according to Eq. ( 12), to calculate the values in Figure 2 we ignored those dialogues that contain any actions selected due to the exploration. We also manually labelled the correctness of domain selection at every turn of the dialogues. The domain selection accuracies of the obtained policy sequence are shown in Figure 3 , where similarly, those exploration actions as well as the clarification and confirmation actions were excluded from the calculations. Although the domain selection accuracy is not the target that our learning algorithm aims to optimise, it reflects the quality of the learnt policies from a different angle of view.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 245,
                        "end": 246,
                        "text": "2",
                        "ref_id": null
                    },
                    {
                        "start": 368,
                        "end": 369,
                        "text": "2",
                        "ref_id": null
                    },
                    {
                        "start": 633,
                        "end": 634,
                        "text": "3",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Training",
                "sec_num": "6.1"
            },
            {
                "text": "It can be found in Figure 2 that the best policy was obtained in the third iteration, and after that the policy quality oscillated. The same finding is indicated in Figure 3 as well, when we use the domain selection accuracy as the evaluation metric. Therefore, we kept the policy corresponding to the peak point of the learning curve for the comparison experiments below.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 26,
                        "end": 27,
                        "text": "2",
                        "ref_id": null
                    },
                    {
                        "start": 172,
                        "end": 173,
                        "text": "3",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Training",
                "sec_num": "6.1"
            },
            {
                "text": "We conducted paired comparison experiments in four scenarios to compare between the system with the GPTD-learnt central control policy and a non-trivial baseline. The baseline is a publicly deployed version of the voice assistant application. The central control policy of the baseline system is handcrafted, which has a separate list of semantic matching rules for each domain to enable domain switching.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Comparison Experiments",
                "sec_num": "6.2"
            },
            {
                "text": "The first two scenarios aim to test the performance of the two systems on (i) switching between a domain expert and out-of-domain services, and (ii) switching between two domain experts, where only the two training domains (travel information and restaurant search) were considered. Scenarios (iii) and (iv) are similar to scenarios (i) and (ii) respectively, but at this time, the users were required to carry out the tests surrounding the movie search domain (which is addressed by a new domain expert not used in the training phase). There were 13 users who participated this experiment. In each scenario, every user was required to test the two systems with an identical goal and similar queries. After each test, the users were asked to score the two systems separately according to the scoring standard in Table 2 .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 818,
                        "end": 819,
                        "text": "2",
                        "ref_id": "TABREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Comparison Experiments",
                "sec_num": "6.2"
            },
            {
                "text": "The average scores received by the two systems are shown in Table 3 , where we also compute the statistical significance (the p-values) of the results based on paired t-tests. It can be found that the learnt policy works significantly better than the rule-based policy in scenarios (ii) and (iv), but in scenarios (i) and (iii) the differences between two systems are statistically insignificant. Moreover, the learnt policy preserves the extensibility of the entire system as expected, of which strong evidences are given by the results in scenarios (iii) and (iv).",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 66,
                        "end": 67,
                        "text": "3",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Comparison Experiments",
                "sec_num": "6.2"
            },
            {
                "text": "To better understand the policy learnt by the GPTD model, we look into the obtained weight vectors, as shown in Table 4 . It can be found that confidence score (#5) is an informative feature for all the system actions, while the relative turn of a domain being last activated (#7) is an additional strong evidence for a confirmation decision. In addition, the similarity between the dialogue completion status (#1 & #2) of two ambiguous domain experts and the relative turns of them being last confirmed (#8) tend to be extra dominating features for clarification decisions, besides the closeness of the confidence scores returned by the two domain experts.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 118,
                        "end": 119,
                        "text": "4",
                        "ref_id": "TABREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Policy Analysis",
                "sec_num": "6.3"
            },
            {
                "text": "A less noticeable but important phenomenon is observed for feature #6, i.e. the total number of active turns of a domain expert during a dialogue. Concretely, feature #6 has a small negative effect on presentation actions but a small positive contribution to confirmation actions. Such weights could correspond to the discount factor's penalty to long dialogues in the value function. However, it implies an unexpected effect in extreme cases, which we explain in detail as follows. Although the absolute weights for feature #6 are tiny for both presentation and confirmation actions, the feature value will grow linearly during a dialogue. Therefore, when a dialogue in a certain domain last rather long, there tend to be very frequent confirmations. A possible solution to this problem could be either ignoring feature #6 or twisting it to some nonlinear function, such that its value stops increasing at a certain threshold point. In addition, to cover sufficient amount of those \"extreme\" examples in the training phase could also be an alter- ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Policy Analysis",
                "sec_num": "6.3"
            },
            {
                "text": "The proposed approach is a rather general framework to learn extensible central control policies for multi-domain SDS based on distributed architectures. It does not rely on any internal representations of those individual domain experts, as long as a unified featurisation of their dialogue states can be achieved. However, from the entire system point of view, the current implementation is still preliminary. Particularly, the confirmation and clarification mechanisms are isolated, for which the surface realisations sometimes may sound stiff. This phenomenon explains one of the reasons that make the proposed system slightly less preferred by the users than the baseline in scenario (i), when the interaction flows are relative simple. A possible improvement here could be associating the confirmation and clarification actions in the central controller to the error handling mechanisms within each domain expert, and letting domain experts generate their own utterances on receiving a confirmation/clarification request from the central controller.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Further Discussions",
                "sec_num": "7"
            },
            {
                "text": "Online reinforcement learning with real user cases will be another undoubted direction of further improvement of our system. The key challenge here is to automatically estimate user's satisfactions, which will be transformed to the rewards for the reinforcement learners. Strong feedbacks such as clicks or actual order placements can be collected. But to regress user's true satisfaction, other environment features must also be taken into account. Practical solutions are still an open issue at this stage, and are left to our future work.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Further Discussions",
                "sec_num": "7"
            },
            {
                "text": "In this paper, we introduce an MDP framework for learning domain selection policies in a complex multi-domain SDS. Standard problem formulation is modified with tied model parameters, so that the entire system is extensible and new domain experts can be easily integrated without re-training the policy. This expectation is confirmed by empirical experiments with human subjects, where the proposed system marginally beats a non-trivial baseline and demonstrates proper extensibility. Several possible improvements are discussed, which will be the central arc of our future research. Thomas Robert Gruber, Adam John Cheyer, Dag",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "8"
            },
            {
                "text": "This is a rather general assumption. Informally speaking, for most task-oriented SDS, one can extract a slot-value representation from their dialogue models, of which examples include the RavenClaw architecture(Bohus and Rud-",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "nicky, 2009), the Information State dialogue engine(Traum and Larsson, 2003), MDP-SDS(Singh et al., 2002) or POMDP-SDS(Thomson and Young, 2010;Young et al., 2010;Williams and Young, 2007).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Overall user satisfactions may rely on various aspects of the entire system, e.g. the data source quality of the services, the performance of each domain expert, etc. It will be difficult to make non-experienced users to score the central controller",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "isolatedly.3 Not all the users participated the experiments everyday. There were 311 valid dialogues received in total, with an average length of 9 turns.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "The research in this paper is supported by China's 973 Programme no. 2014CB340505. The first author is partially funded by the EC FP7 programme under grant agreement no. 287615 (PARLANCE) and a SICSA PECE grant. The authors would also like to thank Qiaoqiao She, Duo Cai and the HCI-APP group at Baidu for volunteering to participate in the human subject experiments.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgements",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Dynamic Programming",
                "authors": [
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Ernest",
                        "suffix": ""
                    },
                    {
                        "first": "Bellman",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 1957,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Richard Ernest Bellman. 1957. Dynamic Program- ming. Princeton University Press, Princeton, NJ.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Neuro-Dynamic Programming",
                "authors": [
                    {
                        "first": "Dimitri",
                        "middle": [
                            "P"
                        ],
                        "last": "Bertsekas",
                        "suffix": ""
                    },
                    {
                        "first": "John",
                        "middle": [
                            "N"
                        ],
                        "last": "Tsitsiklis",
                        "suffix": ""
                    }
                ],
                "year": 1996,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dimitri P. Bertsekas and John N. Tsitsiklis. 1996. Neuro-Dynamic Programming. Athena Scientific, Belmont, MA.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "The RavenClaw dialog management framework: Architecture and systems",
                "authors": [
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Bohus",
                        "suffix": ""
                    },
                    {
                        "first": "Alexander",
                        "middle": [
                            "I"
                        ],
                        "last": "Rudnicky",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Computer Speech and Language",
                "volume": "23",
                "issue": "3",
                "pages": "332--361",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dan Bohus and Alexander I. Rudnicky. 2009. The RavenClaw dialog management framework: Archi- tecture and systems. Computer Speech and Lan- guage, 23(3):332-361.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Reinforcement learning with Gaussian processes",
                "authors": [
                    {
                        "first": "Yaakov",
                        "middle": [],
                        "last": "Engel",
                        "suffix": ""
                    },
                    {
                        "first": "Shie",
                        "middle": [],
                        "last": "Mannor",
                        "suffix": ""
                    },
                    {
                        "first": "Ron",
                        "middle": [],
                        "last": "Meir",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of the 22nd International Conference on Machine Learning (ICML)",
                "volume": "",
                "issue": "",
                "pages": "201--208",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yaakov Engel, Shie Mannor, and Ron Meir. 2005. Re- inforcement learning with Gaussian processes. In Proceedings of the 22nd International Conference on Machine Learning (ICML), pages 201-208.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "PAC-Bayesian policy evaluation for reinforcement learning",
                "authors": [
                    {
                        "first": "Mahdi",
                        "middle": [],
                        "last": "Milani Fard",
                        "suffix": ""
                    },
                    {
                        "first": "Joelle",
                        "middle": [],
                        "last": "Pineau",
                        "suffix": ""
                    },
                    {
                        "first": "Csaba",
                        "middle": [],
                        "last": "Szepesv\u00e1ri",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Proceedings of the 27th Conference on Uncertainty in Artificial Intelligence (UAI)",
                "volume": "",
                "issue": "",
                "pages": "195--202",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mahdi Milani Fard, Joelle Pineau, and Csaba Szepesv\u00e1ri. 2011. PAC-Bayesian policy evaluation for reinforcement learning. In Proceedings of the 27th Conference on Uncertainty in Artificial Intelli- gence (UAI), pages 195-202.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "A discriminative classification-based approach to information state updates for a multi-domain dialog system",
                "authors": [
                    {
                        "first": "Z",
                        "middle": [],
                        "last": "Dilek",
                        "suffix": ""
                    },
                    {
                        "first": "Gokhan",
                        "middle": [],
                        "last": "Hakkani-T\u00fcr",
                        "suffix": ""
                    },
                    {
                        "first": "Larry",
                        "middle": [
                            "P"
                        ],
                        "last": "T\u00fcr",
                        "suffix": ""
                    },
                    {
                        "first": "Ashley",
                        "middle": [],
                        "last": "Heck",
                        "suffix": ""
                    },
                    {
                        "first": "Asli",
                        "middle": [
                            "C"
                        ],
                        "last": "Fidler",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "\u00b8elikyilmaz",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Proceedings of the 13th Annual Conference of the International Speech Communication Association",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dilek Z. Hakkani-T\u00fcr, Gokhan T\u00fcr, Larry P. Heck, Ashley Fidler, and Asli C \u00b8elikyilmaz. 2012. A dis- criminative classification-based approach to infor- mation state updates for a multi-domain dialog sys- tem. In Proceedings of the 13th Annual Conference of the International Speech Communication Associ- ation (INTERSPEECH).",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Natural actor and belief critic: Reinforcement algorithm for learning parameters of dialogue systems modelled as POMDPs",
                "authors": [
                    {
                        "first": "Filip",
                        "middle": [],
                        "last": "Jur\u010d\u00ed\u010dek",
                        "suffix": ""
                    },
                    {
                        "first": "Blaise",
                        "middle": [],
                        "last": "Thomson",
                        "suffix": ""
                    },
                    {
                        "first": "Steve",
                        "middle": [],
                        "last": "Young",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "ACM Transactions on Speech and Language Processing",
                "volume": "7",
                "issue": "3",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Filip Jur\u010d\u00ed\u010dek, Blaise Thomson, and Steve Young. 2011. Natural actor and belief critic: Reinforcement algorithm for learning parameters of dialogue sys- tems modelled as POMDPs. ACM Transactions on Speech and Language Processing, 7(3):6:1-6:25.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Reinforcement learning for parameter estimation in statistical spoken dialogue systems",
                "authors": [
                    {
                        "first": "Filip",
                        "middle": [],
                        "last": "Jur\u010d\u00ed\u010dek",
                        "suffix": ""
                    },
                    {
                        "first": "Blaise",
                        "middle": [],
                        "last": "Thomson",
                        "suffix": ""
                    },
                    {
                        "first": "Steve",
                        "middle": [],
                        "last": "Young",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Computer Speech & Language",
                "volume": "26",
                "issue": "3",
                "pages": "168--192",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Filip Jur\u010d\u00ed\u010dek, Blaise Thomson, and Steve Young. 2012. Reinforcement learning for parameter esti- mation in statistical spoken dialogue systems. Com- puter Speech & Language, 26(3):168-192.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Parameter estimation for agendabased user simulation",
                "authors": [
                    {
                        "first": "Simon",
                        "middle": [],
                        "last": "Keizer",
                        "suffix": ""
                    },
                    {
                        "first": "Milica",
                        "middle": [],
                        "last": "Ga\u0161i\u0107",
                        "suffix": ""
                    },
                    {
                        "first": "Filip",
                        "middle": [],
                        "last": "Jur\u010d\u00ed\u010dek",
                        "suffix": ""
                    },
                    {
                        "first": "Blaise",
                        "middle": [],
                        "last": "Franc \u00b8ois Mairesse",
                        "suffix": ""
                    },
                    {
                        "first": "Kai",
                        "middle": [],
                        "last": "Thomson",
                        "suffix": ""
                    },
                    {
                        "first": "Steve",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Young",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the 11th annual SIGdial Meeting on Discourse and Dialogue",
                "volume": "",
                "issue": "",
                "pages": "116--123",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Simon Keizer, Milica Ga\u0161i\u0107, Filip Jur\u010d\u00ed\u010dek, Franc \u00b8ois Mairesse, Blaise Thomson, Kai Yu, and Steve Young. 2010. Parameter estimation for agenda- based user simulation. In Proceedings of the 11th annual SIGdial Meeting on Discourse and Dialogue, pages 116-123.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Multi-domain spoken dialogue system with extensibility and robustness against speech recognition errors",
                "authors": [
                    {
                        "first": "Kazunori",
                        "middle": [],
                        "last": "Komatani",
                        "suffix": ""
                    },
                    {
                        "first": "Naoyuki",
                        "middle": [],
                        "last": "Kanda",
                        "suffix": ""
                    },
                    {
                        "first": "Mikio",
                        "middle": [],
                        "last": "Nakano",
                        "suffix": ""
                    },
                    {
                        "first": "Kazuhiro",
                        "middle": [],
                        "last": "Nakadai",
                        "suffix": ""
                    },
                    {
                        "first": "Hiroshi",
                        "middle": [],
                        "last": "Tsujino",
                        "suffix": ""
                    },
                    {
                        "first": "Tetsuya",
                        "middle": [],
                        "last": "Ogata",
                        "suffix": ""
                    },
                    {
                        "first": "Hiroshi",
                        "middle": [
                            "G"
                        ],
                        "last": "Okuno",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue",
                "volume": "",
                "issue": "",
                "pages": "9--17",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kazunori Komatani, Naoyuki Kanda, Mikio Nakano, Kazuhiro Nakadai, Hiroshi Tsujino, Tetsuya Ogata, and Hiroshi G. Okuno. 2006. Multi-domain spo- ken dialogue system with extensibility and robust- ness against speech recognition errors. In Proceed- ings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 9-17.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "2012. Data-Driven Methods for Adaptive Spoken Dialogue Systems: Computational Learning for Conversational Interfaces",
                "authors": [
                    {
                        "first": "Oliver",
                        "middle": [],
                        "last": "Lemon",
                        "suffix": ""
                    },
                    {
                        "first": "Olivier",
                        "middle": [],
                        "last": "Pietquin",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Oliver Lemon and Olivier Pietquin, editors. 2012. Data-Driven Methods for Adaptive Spoken Dia- logue Systems: Computational Learning for Conver- sational Interfaces. Springer.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "A distributed architecture for cooperative spoken dialogue agents with coherent dialogue state and history",
                "authors": [
                    {
                        "first": "Hsin-Min",
                        "middle": [],
                        "last": "Bor-Shen Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Lin-Shan",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    }
                ],
                "year": 1999,
                "venue": "Proceedings of the IEEE Automatic Speech Recognition and Understanding Workshop",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Bor-shen Lin, Hsin-min Wang, and Lin-Shan Lee. 1999. A distributed architecture for cooperative spoken dialogue agents with coherent dialogue state and history. In Proceedings of the IEEE Automatic Speech Recognition and Understanding Workshop (ASRU).",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Dialogue management using scripts",
                "authors": [
                    {
                        "first": "Danilo",
                        "middle": [],
                        "last": "Mirkovic",
                        "suffix": ""
                    },
                    {
                        "first": "Lawrence",
                        "middle": [],
                        "last": "Cavedon",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Danilo Mirkovic and Lawrence Cavedon. 2006. Di- alogue management using scripts. United States Patent No. US 20060271351 A1.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "A two-stage domain selection framework for extensible multi-domain spoken dialogue systems",
                "authors": [
                    {
                        "first": "Mikio",
                        "middle": [],
                        "last": "Nakano",
                        "suffix": ""
                    },
                    {
                        "first": "Shun",
                        "middle": [],
                        "last": "Sato",
                        "suffix": ""
                    },
                    {
                        "first": "Kazunori",
                        "middle": [],
                        "last": "Komatani",
                        "suffix": ""
                    },
                    {
                        "first": "Kyoko",
                        "middle": [],
                        "last": "Matsuyama",
                        "suffix": ""
                    },
                    {
                        "first": "Kotaro",
                        "middle": [],
                        "last": "Funakoshi",
                        "suffix": ""
                    },
                    {
                        "first": "Hiroshi",
                        "middle": [
                            "G"
                        ],
                        "last": "Okuno",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Proceedings of the 12th annual SIGdial Meeting on Discourse and Dialogue",
                "volume": "",
                "issue": "",
                "pages": "18--29",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mikio Nakano, Shun Sato, Kazunori Komatani, Kyoko Matsuyama, Kotaro Funakoshi, and Hiroshi G. Okuno. 2011. A two-stage domain selection frame- work for extensible multi-domain spoken dialogue systems. In Proceedings of the 12th annual SIGdial Meeting on Discourse and Dialogue, pages 18-29.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Gaussian Processes for Machine Learning",
                "authors": [
                    {
                        "first": "Carl",
                        "middle": [],
                        "last": "Edward Rasmussen",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "K I"
                        ],
                        "last": "Williams",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Carl Edward Rasmussen and Christopher K. I. Williams, editors. 2006. Gaussian Processes for Machine Learning. MIT Press.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Agenda-based user simulation for bootstrapping a POMDP dialogue system",
                "authors": [
                    {
                        "first": "Jost",
                        "middle": [],
                        "last": "Schatzmann",
                        "suffix": ""
                    },
                    {
                        "first": "Blaise",
                        "middle": [],
                        "last": "Thomson",
                        "suffix": ""
                    },
                    {
                        "first": "Karl",
                        "middle": [],
                        "last": "Weilhammer",
                        "suffix": ""
                    },
                    {
                        "first": "Hui",
                        "middle": [],
                        "last": "Ye",
                        "suffix": ""
                    },
                    {
                        "first": "Steve",
                        "middle": [],
                        "last": "Young",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Proceedings of Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Companion Volume",
                "volume": "",
                "issue": "",
                "pages": "149--152",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jost Schatzmann, Blaise Thomson, Karl Weilhammer, Hui Ye, and Steve Young. 2007. Agenda-based user simulation for bootstrapping a POMDP dia- logue system. In Proceedings of Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Compu- tational Linguistics; Companion Volume, Short Pa- pers, pages 149-152.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Optimizing dialogue management with reinforcement learning: Experiments with the NJFun system",
                "authors": [
                    {
                        "first": "Satinder",
                        "middle": [],
                        "last": "Singh",
                        "suffix": ""
                    },
                    {
                        "first": "Diane",
                        "middle": [],
                        "last": "Litman",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Kearns",
                        "suffix": ""
                    },
                    {
                        "first": "Marilyn",
                        "middle": [],
                        "last": "Walker",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Journal of Artificial Intelligence Research",
                "volume": "16",
                "issue": "1",
                "pages": "105--133",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Satinder Singh, Diane Litman, Michael Kearns, and Marilyn Walker. 2002. Optimizing dialogue man- agement with reinforcement learning: Experiments with the NJFun system. Journal of Artificial Intelli- gence Research, 16(1):105-133.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Reinforcement Learning: An Introduction",
                "authors": [
                    {
                        "first": "Richard",
                        "middle": [
                            "S"
                        ],
                        "last": "Sutton",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [
                            "G"
                        ],
                        "last": "Barto",
                        "suffix": ""
                    }
                ],
                "year": 1998,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Richard S. Sutton and Andrew G. Barto. 1998. Re- inforcement Learning: An Introduction. MIT Press, Cambridge, MA.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Bayesian update of dialogue state: A POMDP framework for spoken dialogue systems",
                "authors": [
                    {
                        "first": "Blaise",
                        "middle": [],
                        "last": "Thomson",
                        "suffix": ""
                    },
                    {
                        "first": "Steve",
                        "middle": [],
                        "last": "Young",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Computer Speech and Language",
                "volume": "24",
                "issue": "4",
                "pages": "562--588",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Blaise Thomson and Steve Young. 2010. Bayesian update of dialogue state: A POMDP framework for spoken dialogue systems. Computer Speech and Language, 24(4):562-588.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "The Information State approach to dialogue management",
                "authors": [
                    {
                        "first": "David",
                        "middle": [
                            "R"
                        ],
                        "last": "Traum",
                        "suffix": ""
                    },
                    {
                        "first": "Staffan",
                        "middle": [],
                        "last": "Larsson",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Current and New Directions in Discourse and Dialogue",
                "volume": "",
                "issue": "",
                "pages": "325--353",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "David R. Traum and Staffan Larsson. 2003. The In- formation State approach to dialogue management. In Jan van Kuppevelt and Ronnie W. Smith, editors, Current and New Directions in Discourse and Dia- logue, pages 325-353. Springer.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Partially observable Markov decision processes for spoken dialog systems",
                "authors": [
                    {
                        "first": "Jason",
                        "middle": [
                            "D"
                        ],
                        "last": "Williams",
                        "suffix": ""
                    },
                    {
                        "first": "Steve",
                        "middle": [],
                        "last": "Young",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Computer Speech and Language",
                "volume": "21",
                "issue": "2",
                "pages": "393--422",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jason D. Williams and Steve Young. 2007. Partially observable Markov decision processes for spoken dialog systems. Computer Speech and Language, 21(2):393-422.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Let's Go\": A production-grade statistical spoken dialog system",
                "authors": [
                    {
                        "first": "Jason",
                        "middle": [
                            "D"
                        ],
                        "last": "Williams",
                        "suffix": ""
                    },
                    {
                        "first": "Iker",
                        "middle": [],
                        "last": "Arizmendi",
                        "suffix": ""
                    },
                    {
                        "first": "Alistair",
                        "middle": [],
                        "last": "Conkie",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the 3rd IEEE Workshop on Spoken Language Technology (SLT)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jason D. Williams, Iker Arizmendi, and Alistair Conkie. 2010. Demonstration of AT&T \"Let's Go\": A production-grade statistical spoken dialog system. In Proceedings of the 3rd IEEE Workshop on Spoken Language Technology (SLT).",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "The Hidden Information State model: a practical framework for POMDP-based spoken dialogue management",
                "authors": [
                    {
                        "first": "Steve",
                        "middle": [],
                        "last": "Young",
                        "suffix": ""
                    },
                    {
                        "first": "Milica",
                        "middle": [],
                        "last": "Ga\u0161i\u0107",
                        "suffix": ""
                    },
                    {
                        "first": "Simon",
                        "middle": [],
                        "last": "Keizer",
                        "suffix": ""
                    },
                    {
                        "first": "Jost",
                        "middle": [],
                        "last": "Franc \u00b8ois Mairesse",
                        "suffix": ""
                    },
                    {
                        "first": "Blaise",
                        "middle": [],
                        "last": "Schatzmann",
                        "suffix": ""
                    },
                    {
                        "first": "Kai",
                        "middle": [],
                        "last": "Thomson",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Computer Speech and Language",
                "volume": "24",
                "issue": "2",
                "pages": "150--174",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Steve Young, Milica Ga\u0161i\u0107, Simon Keizer, Franc \u00b8ois Mairesse, Jost Schatzmann, Blaise Thomson, and Kai Yu. 2010. The Hidden Information State model: a practical framework for POMDP-based spoken di- alogue management. Computer Speech and Lan- guage, 24(2):150-174.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "POMDP-based statistical spoken dialogue systems: a review",
                "authors": [
                    {
                        "first": "Steve",
                        "middle": [],
                        "last": "Young",
                        "suffix": ""
                    },
                    {
                        "first": "Milica",
                        "middle": [],
                        "last": "Ga\u0161i\u0107",
                        "suffix": ""
                    },
                    {
                        "first": "Blaise",
                        "middle": [],
                        "last": "Thomson",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [
                            "D"
                        ],
                        "last": "Williams",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of the IEEE",
                "volume": "",
                "issue": "",
                "pages": "1--20",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Steve Young, Milica Ga\u0161i\u0107, Blaise Thomson, and Ja- son D. Williams. 2013. POMDP-based statistical spoken dialogue systems: a review. Proceedings of the IEEE, PP(99):1-20.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF1": {
                "num": null,
                "text": "Figure 2: Average scores and standard deviations during policy iteration.",
                "uris": null,
                "fig_num": "3",
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "Paired comparison experiments between the system with a trained GPTD policy and the rule-based baseline.",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF3": {
                "num": null,
                "text": "USER/ASR: \u53bb\u9752\u5c9b\u7684\u673a\u7968 (Flight ticket to Qingdao.) SYSTEM: \u597d\u7684\uff0c\u5230\u9752\u5c9b\u7684\u98de\u673a\u7968\u3002\u8bf7\u95ee\u60a8\u4ec0\u4e48\u65f6\u5019\u79bb\u5f00\uff1f \u81ea\u54ea\u513f\u51fa\u53d1\u554a\uff1f (Okay, flight ticket to Qingdao. When are you departing, and where are you departing from?) [Travel Info. Domain Expert] USER/ASR: \u660e\u5929\u51fa\u53d1 (I'm leaving tomorrow.) SYSTEM: \u597d\u7684\uff0c\u660e\u5929\u5230\u9752\u5c9b\u7684\u98de\u673a\u7968\u3002\u60a8\u9884\u8ba1\u7531\u54ea\u513f\u51fa\u53d1\u554a\uff1f (Okay, flight ticket to Qingdao, leaving tomorrow. Where are you departing from?want to continue with flight ticket booking?) [Confirmation -Central Controller] USER: \u4e0d\u662f (No) SYSTEM: [Web Search Service: \"\u542c\u5230\u6709\u597d\u5403\u7684\u9178\u7684\u5ddd\u83dc\u5417\" ] USER/ASR: \u660e\u5929\u53bb\u9752\u5c9b (I'm going to Qingdao tomorrow.) SYSTEM: [Results Displayed -Travel Info. Domain Expert] USER/ASR: \u9752\u5c9b\u673a\u573a\u9644\u8fd1\u7684\u9910\u5385 (Restaurants near Qingdao Airport) SYSTEM: \u4e3a\u60a8\u627e\u5230\u4ee5\u4e0b\u9910\u5385 (I found the following restaurants for you.) [Results Displayed -Restaurant Search Domain Expert] USER/ASR: \u9752\u5c9b\u660e\u5929\u4f1a\u4e0b\u96e8\u5417? (Will it rain in Qingdao tomorrow?) SYSTEM: \u9752\u5c9b\u660e\u5929\u9635\u96e8\uff0c\u6e29\u5ea616\u2103\u81f320\u2103\uff0c\u4e1c\u98ce5-6\u7ea7 (Qingdao is rainy tomorrow. The temperature is 16 to 20 degree centigrade with moderate east wind.) [Results Displayed -Weather Report Service]",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF4": {
                "num": null,
                "text": "Figure 4: An example dialogue containing multiple user goals. The icons stand for graphical user interface based nonverbal outputs.",
                "uris": null,
                "fig_num": "4",
                "type_str": "figure"
            },
            "TABREF2": {
                "content": "<table><tr><td colspan=\"2\"># Feature</td><td>Range</td></tr><tr><td/><td>the number of unfilled</td><td/></tr><tr><td>1</td><td>required slots of a domain</td><td>{0, . . . , M }</td></tr><tr><td/><td>expert</td><td/></tr><tr><td>2</td><td>the number of filled required slots of a domain expert</td><td>{0, . . . , M }</td></tr><tr><td>3</td><td>the number of filled optional slots of a domain expert</td><td>{0, . . . , L}</td></tr><tr><td>4</td><td>whether a domain expert has executed a database search</td><td>{0, 1}</td></tr><tr><td>5</td><td>the confidence score returned by a domain expert</td><td>[0, 1.2]</td></tr><tr><td/><td>the total number of turns that</td><td/></tr><tr><td>6</td><td>a domain expert has been</td><td>Z +</td></tr><tr><td/><td>activated during a dialogue</td><td/></tr><tr><td/><td>e -ta where t a denotes the</td><td/></tr><tr><td>7</td><td>relative turn of a domain expert being last activated,</td><td>[0, 1]</td></tr><tr><td/><td>or 0 if not applicable</td><td/></tr><tr><td/><td>e -tc where t c denotes the</td><td/></tr><tr><td>8</td><td>relative turn of a domain expert being last confirmed,</td><td>[0, 1]</td></tr><tr><td/><td>or 0 if not applicable</td><td/></tr><tr><td/><td>the summed confidence</td><td/></tr><tr><td>9</td><td>score from the user intention identifier of a query being</td><td>[0, 1.2N ]</td></tr><tr><td/><td>for out-of-domain services</td><td/></tr></table>",
                "type_str": "table",
                "text": "A list of all features used in our model.",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table/>",
                "type_str": "table",
                "text": "The scoring standard in our experiments.",
                "html": null,
                "num": null
            },
            "TABREF4": {
                "content": "<table><tr><td/><td/><td colspan=\"2\">Feature Weights</td><td/></tr><tr><td>#</td><td colspan=\"3\">present confirm clarify</td><td/></tr><tr><td>1</td><td>0.09</td><td>0.02</td><td>0.60</td><td/></tr><tr><td>2 3 4 5 6</td><td>0.20 0.18 -0.10 0.75 -0.02</td><td>0.29 0.29 0.16 0.57 0.11</td><td>0.53 0.16 0.25 0.54 0.13</td><td>present ood</td></tr><tr><td>7</td><td>0.25</td><td>1.19</td><td>0.36</td><td/></tr><tr><td>8</td><td>-0.22</td><td>-0.19</td><td>0.69</td><td/></tr><tr><td>9</td><td>-</td><td>0.20</td><td>-</td><td>0.47</td></tr><tr><td>Bias</td><td>-1.79</td><td>-</td><td>-</td><td>-2.42</td></tr><tr><td colspan=\"5\">native solution, as our current training set contains</td></tr><tr><td colspan=\"5\">very few examples that exhibit extraordinary long</td></tr><tr><td colspan=\"2\">domain persistence.</td><td/><td/><td/></tr></table>",
                "type_str": "table",
                "text": "Feature weights learnt by GPTD. See Table 1 for the meanings of the features.",
                "html": null,
                "num": null
            }
        }
    }
}