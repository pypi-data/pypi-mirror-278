{
    "paper_id": "2020",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:36:05.513185Z"
    },
    "title": "Language Models as an Alternative Evaluator of Word Order Hypotheses: A Case Study in Japanese",
    "authors": [
        {
            "first": "Tatsuki",
            "middle": [],
            "last": "Kuribayashi",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Tohoku University",
                "location": {}
            },
            "email": "kuribayashi@ecei.tohoku.ac.jp"
        },
        {
            "first": "Takumi",
            "middle": [],
            "last": "Ito",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Tohoku University",
                "location": {}
            },
            "email": "t-ito@ecei.tohoku.ac.jp"
        },
        {
            "first": "Jun",
            "middle": [],
            "last": "Suzuki",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Tohoku University",
                "location": {}
            },
            "email": "jun.suzuki@ecei.tohoku.ac.jp"
        },
        {
            "first": "Kentaro",
            "middle": [],
            "last": "Inui",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Tohoku University",
                "location": {}
            },
            "email": "inui@ecei.tohoku.ac.jp"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "We examine a methodology using neural language models (LMs) for analyzing the word order of language. This LM-based method has the potential to overcome the difficulties existing methods face, such as the propagation of preprocessor errors in count-based methods. In this study, we explore whether the LMbased method is valid for analyzing the word order. As a case study, this study focuses on Japanese due to its complex and flexible word order. To validate the LM-based method, we test (i) parallels between LMs and human word order preference, and (ii) consistency of the results obtained using the LM-based method with previous linguistic studies. Through our experiments, we tentatively conclude that LMs display sufficient word order knowledge for usage as an analysis tool. Finally, using the LMbased method, we demonstrate the relationship between the canonical word order and topicalization, which had yet to be analyzed by largescale experiments.\nModel Verbs whose type-A examples prefer the ACC-DAT order\nVerbs whose type-B examples prefer the ACC-DAT order CLM \"\u9810\u3051\u308b\" (deposit), \"\u7f6e\u304f\" (place), \"\u6301\u3064\" (have), \"\u5165 \u308c\u308b\" (put in), \"\u7d0d\u3081\u308b\" (pay), \"\u90f5\u9001\" (mail), \"\u4f9b \u7d66\" (supply), \"\u51fa\u3059\" (put out), \"\u904b\u3076\" (transport), \"\u6d41 \u3059\" (shed), \"\u639b\u3051\u308b\" (hang), \"\u98fe\u308b\" (decorate), \"\u5e83 \u3052\u308b\" (spread), \"\u79fb\u3059\" (transfer), \"\u6b8b\u3059\" (leave), \"\u914d \u9001\" (deliver), \"\u9001\u308b\" (send), \"\u6295\u3052\u308b\" (throw), \"\u9001 \u4ed8\" (send), \"\u8fd4\u5374\" (return), \"\u5c4a\u3051\u308b\" (send), \"\u623b\u3059\" (return), \"\u7740\u3051\u308b\" (wear), \"\u4e0a\u3052\u308b\" (increase), \"\u843d\u3068 \u3059\" (drop), \"\u8f09\u305b\u308b\" (publish), \"\u5909\u66f4\" (change), \"\u7d0d \u5165\" (deliver), \"\u5378\u3059\" (unload), \"\u63b2\u8f09\" (publish), \"\u901a \u3059\" (get X through)\n\"\u914d \u5e03\" (distribute), \"\u6e21 \u3059\" (pass), \"\u30d7 \u30ec \u30bc \u30f3 \u30c8\" (present), \"\u5408\u308f\u305b\u308b\" (match), \"\u898b\u305b\u308b\" (show), \"\u63d0 \u4f9b\" (offer), \"\u4e0e\u3048\u308b\" (give), \"\u5f53\u3066\u308b\" (hit), \"\u56de\u3059\" (turn), \"\u8ffd\u52a0\" (add), \"\u8cb8\u3059\" (lend), \"\u5c55\u793a\" (exhibit), \"\u636e\u3048\u308b\" (lay), \"\u4f9d\u983c\" (request), \"\u633f\u5165\" (insert), \"\u7e8f \u3081\u308b\" (collect), \"\u8acb\u6c42\" (claim) SLM \"\u9810\u3051\u308b\" (deposit), \"\u7f6e\u304f\" (place), \"\u983c\u3080\" (ask), \"\u5165 \u308c\u308b\" (put in), \"\u7d0d\u3081\u308b\" (pay), \"\u90f5\u9001\" (mail), \"\u51fa \u3059\" (put out), \"\u904b\u3076\" (transport), \"\u6d41\u3059\" (shed), \"\u639b\u3051 \u308b\" (hang), \"\u5e83\u3052\u308b\" (spread), \"\u79fb\u3059\" (transfer), \"\u6b8b \u3059\" (leave), \"\u30ea\u30af\u30a8\u30b9\u30c8\" (request), \"\u914d\u9001\" (deliver), \"\u9001\u308b\" (send), \"\u6295\u3052\u308b\" (throw), \"\u9001\u4ed8\" (send), \"\u6c42 \u3081\u308b\" (ask), \"\u63d0\u51fa\" (submit), \"\u5c4a\u3051\u308b\" (deliver), \"\u8981 \u6c42\" (request), \"\u623b\u3059\" (return), \"\u5bc4\u4ed8\" (donate), \"\u5bc4\u8d08\" (donation), \"\u7740\u3051\u308b\" (wear), \"\u4e57\u305b\u308b\" (place), \"\u4e0a\u3052 \u308b\" (increase), \"\u843d\u3068\u3059\" (drop), \"\u8cbc\u308b\" (stick), \"\u5206\u3051 \u308b\" (divide), \"\u3070\u3089\u307e\u304f\" (scatter), \"\u306f\u3081\u308b\" (fit), \"\u652f \u6255\u3046\" (pay), \"\u914d\u9054\" (deliver), \"\u5378\u3059\" (unload), \"\u7e8f\u3081 \u308b\" (collect), \"\u901a\u3059\" (get X through)\n\"\u30d7\u30ec\u30bc\u30f3\u30c8\" (present), \"\u6301\u3064\" (have), \"\u5408\u308f\u305b\u308b\" (match), \"\u898b\u305b\u308b\" (show), \"\u5411\u3051\u308b\" (point), \"\u63d0\u4f9b\" (offer), \"\u88c5\u5099\" (equip), \"\u8ffd\u52a0\" (add), \"\u5c55\u793a\" (exhibit), \"\u636e\u3048\u308b\" (lay), \"\u63a1\u7528\" (adopt)",
    "pdf_parse": {
        "paper_id": "2020",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "We examine a methodology using neural language models (LMs) for analyzing the word order of language. This LM-based method has the potential to overcome the difficulties existing methods face, such as the propagation of preprocessor errors in count-based methods. In this study, we explore whether the LMbased method is valid for analyzing the word order. As a case study, this study focuses on Japanese due to its complex and flexible word order. To validate the LM-based method, we test (i) parallels between LMs and human word order preference, and (ii) consistency of the results obtained using the LM-based method with previous linguistic studies. Through our experiments, we tentatively conclude that LMs display sufficient word order knowledge for usage as an analysis tool. Finally, using the LMbased method, we demonstrate the relationship between the canonical word order and topicalization, which had yet to be analyzed by largescale experiments.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            },
            {
                "text": "Model Verbs whose type-A examples prefer the ACC-DAT order",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            },
            {
                "text": "Verbs whose type-B examples prefer the ACC-DAT order CLM \"\u9810\u3051\u308b\" (deposit), \"\u7f6e\u304f\" (place), \"\u6301\u3064\" (have), \"\u5165 \u308c\u308b\" (put in), \"\u7d0d\u3081\u308b\" (pay), \"\u90f5\u9001\" (mail), \"\u4f9b \u7d66\" (supply), \"\u51fa\u3059\" (put out), \"\u904b\u3076\" (transport), \"\u6d41 \u3059\" (shed), \"\u639b\u3051\u308b\" (hang), \"\u98fe\u308b\" (decorate), \"\u5e83 \u3052\u308b\" (spread), \"\u79fb\u3059\" (transfer), \"\u6b8b\u3059\" (leave), \"\u914d \u9001\" (deliver), \"\u9001\u308b\" (send), \"\u6295\u3052\u308b\" (throw), \"\u9001 \u4ed8\" (send), \"\u8fd4\u5374\" (return), \"\u5c4a\u3051\u308b\" (send), \"\u623b\u3059\" (return), \"\u7740\u3051\u308b\" (wear), \"\u4e0a\u3052\u308b\" (increase), \"\u843d\u3068 \u3059\" (drop), \"\u8f09\u305b\u308b\" (publish), \"\u5909\u66f4\" (change), \"\u7d0d \u5165\" (deliver), \"\u5378\u3059\" (unload), \"\u63b2\u8f09\" (publish), \"\u901a \u3059\" (get X through)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            },
            {
                "text": "\"\u914d \u5e03\" (distribute), \"\u6e21 \u3059\" (pass), \"\u30d7 \u30ec \u30bc \u30f3 \u30c8\" (present), \"\u5408\u308f\u305b\u308b\" (match), \"\u898b\u305b\u308b\" (show), \"\u63d0 \u4f9b\" (offer), \"\u4e0e\u3048\u308b\" (give), \"\u5f53\u3066\u308b\" (hit), \"\u56de\u3059\" (turn), \"\u8ffd\u52a0\" (add), \"\u8cb8\u3059\" (lend), \"\u5c55\u793a\" (exhibit), \"\u636e\u3048\u308b\" (lay), \"\u4f9d\u983c\" (request), \"\u633f\u5165\" (insert), \"\u7e8f \u3081\u308b\" (collect), \"\u8acb\u6c42\" (claim) SLM \"\u9810\u3051\u308b\" (deposit), \"\u7f6e\u304f\" (place), \"\u983c\u3080\" (ask), \"\u5165 \u308c\u308b\" (put in), \"\u7d0d\u3081\u308b\" (pay), \"\u90f5\u9001\" (mail), \"\u51fa \u3059\" (put out), \"\u904b\u3076\" (transport), \"\u6d41\u3059\" (shed), \"\u639b\u3051 \u308b\" (hang), \"\u5e83\u3052\u308b\" (spread), \"\u79fb\u3059\" (transfer), \"\u6b8b \u3059\" (leave), \"\u30ea\u30af\u30a8\u30b9\u30c8\" (request), \"\u914d\u9001\" (deliver), \"\u9001\u308b\" (send), \"\u6295\u3052\u308b\" (throw), \"\u9001\u4ed8\" (send), \"\u6c42 \u3081\u308b\" (ask), \"\u63d0\u51fa\" (submit), \"\u5c4a\u3051\u308b\" (deliver), \"\u8981 \u6c42\" (request), \"\u623b\u3059\" (return), \"\u5bc4\u4ed8\" (donate), \"\u5bc4\u8d08\" (donation), \"\u7740\u3051\u308b\" (wear), \"\u4e57\u305b\u308b\" (place), \"\u4e0a\u3052 \u308b\" (increase), \"\u843d\u3068\u3059\" (drop), \"\u8cbc\u308b\" (stick), \"\u5206\u3051 \u308b\" (divide), \"\u3070\u3089\u307e\u304f\" (scatter), \"\u306f\u3081\u308b\" (fit), \"\u652f \u6255\u3046\" (pay), \"\u914d\u9054\" (deliver), \"\u5378\u3059\" (unload), \"\u7e8f\u3081 \u308b\" (collect), \"\u901a\u3059\" (get X through)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            },
            {
                "text": "\"\u30d7\u30ec\u30bc\u30f3\u30c8\" (present), \"\u6301\u3064\" (have), \"\u5408\u308f\u305b\u308b\" (match), \"\u898b\u305b\u308b\" (show), \"\u5411\u3051\u308b\" (point), \"\u63d0\u4f9b\" (offer), \"\u88c5\u5099\" (equip), \"\u8ffd\u52a0\" (add), \"\u5c55\u793a\" (exhibit), \"\u636e\u3048\u308b\" (lay), \"\u63a1\u7528\" (adopt)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Speakers sometimes have a range of options for word order in conveying a similar meaning. A typical case in English is dative alternation:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "(1) a. A teacher gave a student a book.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "b. A teacher gave a book to a student.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Even for such a particular alternation, several studies (Bresnan et al., 2007; Hovav and Levin, 2008; Colleman, 2009) investigated the factors determining this word order and found that the choice is not random. For analyzing such linguistic phenomena, linguists repeat the cycle of constructing hypotheses and testing their validity, usually through psychological experiments or count-based methods. However, these approaches sometimes face difficulties, such as scalability issues in psychological experiments and the propagation of preprocessor errors in count-based methods.",
                "cite_spans": [
                    {
                        "start": 56,
                        "end": 78,
                        "text": "(Bresnan et al., 2007;",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 79,
                        "end": 101,
                        "text": "Hovav and Levin, 2008;",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 102,
                        "end": 117,
                        "text": "Colleman, 2009)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Compared to the typical approaches for evaluating linguistic hypotheses, approaches using LMs have potential advantages (Section 3.2). In this study, we examine the methodology of using LMs for analyzing word order (Figure 1 ). To validate the LM-based method, we first examine if there is a parallel between canonical word order and generation probability of LMs for each word order. Futrell and Levy (2019) reported that English LMs have human-like word order preferences, which can be one piece of evidence for validating the LM-based method. However, it is not clear whether the above assumption is valid even in languages with more flexible word order.",
                "cite_spans": [
                    {
                        "start": 385,
                        "end": 408,
                        "text": "Futrell and Levy (2019)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 223,
                        "end": 224,
                        "text": "1",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this study, we specifically focus on the Japanese language due to its complex and flexible word order. There are many claims on the canonical word order of Japanese, and it has attracted considerable attention from linguists and natural language processing (NLP) researchers for decades (Hoji, 1985; Saeki, 1998; Miyamoto, 2002; Matsuoka, 2003; Koizumi and Tamaoka, 2004; Nakamoto et al., 2006; Shigenaga, 2014; Sasano and Okumura, 2016; Orita, 2017; Asahara et al., 2018) .",
                "cite_spans": [
                    {
                        "start": 290,
                        "end": 302,
                        "text": "(Hoji, 1985;",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 303,
                        "end": 315,
                        "text": "Saeki, 1998;",
                        "ref_id": "BIBREF31"
                    },
                    {
                        "start": 316,
                        "end": 331,
                        "text": "Miyamoto, 2002;",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 332,
                        "end": 347,
                        "text": "Matsuoka, 2003;",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 348,
                        "end": 374,
                        "text": "Koizumi and Tamaoka, 2004;",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 375,
                        "end": 397,
                        "text": "Nakamoto et al., 2006;",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 398,
                        "end": 414,
                        "text": "Shigenaga, 2014;",
                        "ref_id": "BIBREF34"
                    },
                    {
                        "start": 415,
                        "end": 440,
                        "text": "Sasano and Okumura, 2016;",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 441,
                        "end": 453,
                        "text": "Orita, 2017;",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 454,
                        "end": 475,
                        "text": "Asahara et al., 2018)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "We investigated the validity of using Japanese LMs for canonical word order analysis by conducting two sets of experiments: (i) comparing word order preference in LMs to that in Japanese speakers (Section 4), and (ii) checking the consistency Topic Time Location Subject (Adverb) Indirect object Direct object Verb Notation TOP TIM LOC NOM -DAT ACC -Typical particle \"\u306f\" (wa) \"\u306b\" (ni) \"\u3067\" (de) \"\u304c\" (ga) -\"\u306b\" (ni) \"\u3092\" (o) -Related section 6 5.2 5.2 5.2 5.3 5.1 5.1 5.1",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Table 1 : Overview of the typical cases in Japanese, their typical particles, and the sections where the corresponding case is analyzed. The well-known canonical word order of Japanese is listed from left to right.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "between the preference of LMs with previous linguistic studies (Section 5). From our experiments, we tentatively conclude that LMs display sufficient word order knowledge for usage as an analysis tool, and further explore potential applications. Finally, we analyzed the relationship between topicalization and word order of Japanese by taking advantage of the LM-based method (Section 6).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In summary, we:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\u2022 Discuss and validate the use of LMs as a tool for word order analysis as well as investigate the sensitivity of LMs against different word orders in non-European language (Section 3); \u2022 Find encouraging parallels between the results obtained with the LM-based method and those with the previously established method on various hypotheses of canonical word order of Japanese (Sections 4 and 5); and \u2022 Showcase the advantages of an LM-based method through analyzing linguistic phenomena that is difficult to explore with the previous data-driven methods (Section 6).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "This section provides a brief overview of the linguistic background of canonical word order, some basics of Japanese grammar, and common methods of linguistic analysis.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Linguistic background",
                "sec_num": "2"
            },
            {
                "text": "Every language is assumed to have a canonical word order, even those with flexible word order (Comrie, 1989) . There has been a significant linguistic effort to reveal the factors determining the canonical word order (Bresnan et al., 2007; Hoji, 1985) . The motivations for revealing the canonical word order range from linguistic interests to those involved in various other fields-it relates to language acquisition and production in psycholinguistics (Slobin and Bever, 1982; Akhtar, 1999) , second language education (Alonso Belmonte et al., 2000) , and natural language generation (Visweswariah et al., 2011) or error cor-rection (Cheng et al., 2014) in NLP. In Japanese, there are also many studies on its canonical word order (Hoji, 1985; Saeki, 1998; Koizumi and Tamaoka, 2004; Sasano and Okumura, 2016) .",
                "cite_spans": [
                    {
                        "start": 94,
                        "end": 108,
                        "text": "(Comrie, 1989)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 217,
                        "end": 239,
                        "text": "(Bresnan et al., 2007;",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 240,
                        "end": 251,
                        "text": "Hoji, 1985)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 454,
                        "end": 478,
                        "text": "(Slobin and Bever, 1982;",
                        "ref_id": "BIBREF35"
                    },
                    {
                        "start": 479,
                        "end": 492,
                        "text": "Akhtar, 1999)",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 521,
                        "end": 551,
                        "text": "(Alonso Belmonte et al., 2000)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 586,
                        "end": 613,
                        "text": "(Visweswariah et al., 2011)",
                        "ref_id": "BIBREF37"
                    },
                    {
                        "start": 635,
                        "end": 655,
                        "text": "(Cheng et al., 2014)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 733,
                        "end": 745,
                        "text": "(Hoji, 1985;",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 746,
                        "end": 758,
                        "text": "Saeki, 1998;",
                        "ref_id": "BIBREF31"
                    },
                    {
                        "start": 759,
                        "end": 785,
                        "text": "Koizumi and Tamaoka, 2004;",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 786,
                        "end": 811,
                        "text": "Sasano and Okumura, 2016)",
                        "ref_id": "BIBREF32"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "On canonical word order",
                "sec_num": "2.1"
            },
            {
                "text": "Japanese canonical word order The word order of Japanese is basically subject-object-verb (SOV) order, but there is no strict rule except placing the verb at the end of the sentence (Tsujimura, 2013) . For example, the following three sentences have the same denotational meaning (\"A teacher gave a student a book.\"):",
                "cite_spans": [
                    {
                        "start": 182,
                        "end": 199,
                        "text": "(Tsujimura, 2013)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "On canonical word order",
                "sec_num": "2.1"
            },
            {
                "text": "(2) a. \u5148\u751f\u304c .............. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "On canonical word order",
                "sec_num": "2.1"
            },
            {
                "text": "book-ACC student-DAT teacher-NOM gave.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "\u3042\u3052\u305f.",
                "sec_num": null
            },
            {
                "text": "This order-free nature suggests that the position of each constituent does not represent its semantic role (case). Instead, postpositional case particles indicate the roles. Table 1 shows typical constituents in a Japanese sentence, their postpositional particles, their canonical order, and the sections of this paper where each of them is analyzed. Note that postpositional case particles are sometimes omitted or replaced with other particles such as adverbial particles (Section 6). These characteristics complicate the factors determining word order, which renders the automatic analysis of Japanese word order difficult.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 180,
                        "end": 181,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "\u3042\u3052\u305f.",
                "sec_num": null
            },
            {
                "text": "There are two main methods in linguistic research: human-based methods, which observe human reactions, and data-driven methods, which analyze text corpora.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "On typical methods for evaluating word order hypotheses and their difficulties",
                "sec_num": "2.2"
            },
            {
                "text": "Human-based methods A typical approach of testing word order hypotheses is observing the reaction (e.g., reading time) of humans to each word order (Shigenaga, 2014; Bahlmann et al., 2007) . These approaches are based on the direct observation of humans, but this method has scalability issues. There are also concerns that the participants may be biased, and that the experiments may not be replicable.",
                "cite_spans": [
                    {
                        "start": 148,
                        "end": 165,
                        "text": "(Shigenaga, 2014;",
                        "ref_id": "BIBREF34"
                    },
                    {
                        "start": 166,
                        "end": 188,
                        "text": "Bahlmann et al., 2007)",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "On typical methods for evaluating word order hypotheses and their difficulties",
                "sec_num": "2.2"
            },
            {
                "text": "Data-driven methods Another typical approach is counting the occurrence frequencies of the targeted phenomena in a large corpus. This countbased method is based on the assumption that there are parallels between the canonical word order and the frequency of each word order in a large corpus.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "On typical methods for evaluating word order hypotheses and their difficulties",
                "sec_num": "2.2"
            },
            {
                "text": "The parallel has been widely discussed (Arnon and Snider, 2010; Bresnan et al., 2007) , and many studies rely on this assumption (Sasano and Okumura, 2016; Kempen and Harbusch, 2004) . One of the advantages of this approach is suitability for largescale experiments. This enables considering a large number of examples.",
                "cite_spans": [
                    {
                        "start": 39,
                        "end": 63,
                        "text": "(Arnon and Snider, 2010;",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 64,
                        "end": 85,
                        "text": "Bresnan et al., 2007)",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 129,
                        "end": 155,
                        "text": "(Sasano and Okumura, 2016;",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 156,
                        "end": 182,
                        "text": "Kempen and Harbusch, 2004)",
                        "ref_id": "BIBREF17"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "On typical methods for evaluating word order hypotheses and their difficulties",
                "sec_num": "2.2"
            },
            {
                "text": "In this method, researchers often have to identify the phenomena of interest with preprocessors (e.g., the predicate-argument structure parser used by Sasano and Okumura (2016) ) in order to count them. However, sometimes, identification of the targeted phenomena is difficult for the preprocessors, which limits the possibilities of analysis. For example, Sasano and Okumura (2016) focused only on simple examples where case markers appear explicitly, and only extract the head noun of the argument to avoid preprocessor errors. Thus, they could not analyze the phenomena in which the above conditions were not met. The above issue becomes more serious in low-resource languages, where the necessary preprocessors are often unavailable.",
                "cite_spans": [
                    {
                        "start": 151,
                        "end": 176,
                        "text": "Sasano and Okumura (2016)",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 357,
                        "end": 382,
                        "text": "Sasano and Okumura (2016)",
                        "ref_id": "BIBREF32"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "On typical methods for evaluating word order hypotheses and their difficulties",
                "sec_num": "2.2"
            },
            {
                "text": "In this count-based direction, Bloem (2016) used n-gram LMs to test the claims on the German twoverb clusters. This method is closest to our proposed approach, but the general validity of using LMs is out of focus. This LM-based method also relies on the assumption of the parallels between the canonical word order and the frequency.",
                "cite_spans": [
                    {
                        "start": 31,
                        "end": 43,
                        "text": "Bloem (2016)",
                        "ref_id": "BIBREF5"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "On typical methods for evaluating word order hypotheses and their difficulties",
                "sec_num": "2.2"
            },
            {
                "text": "Another common data-driven approach is to train an interpretable model (e.g., Bayesian linear mixed models) to predict the targeted linguistic phenomena and analyze the inner workings of the model (e.g., slope parameters) (Bresnan et al., 2007; Asahara et al., 2018) . Through this approach, researchers can obtain richer statistics, such as the strength of each factor's effect on the targeted phenomena, but creating labeled data and designing features for supervised learning can be costly.",
                "cite_spans": [
                    {
                        "start": 222,
                        "end": 244,
                        "text": "(Bresnan et al., 2007;",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 245,
                        "end": 266,
                        "text": "Asahara et al., 2018)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "On typical methods for evaluating word order hypotheses and their difficulties",
                "sec_num": "2.2"
            },
            {
                "text": "In the NLP field, LMs are widely used to estimate the acceptability of text (Olteanu et al., 2006; Kann et al., 2018) . An overview of the LM-based method is shown in Figure 1 . After preparing several word orders considering the targeted linguistic hypothesis, we compare their generation probabilities in LMs. We assume that the word order with the highest generation probability follows their canonical word order.",
                "cite_spans": [
                    {
                        "start": 76,
                        "end": 98,
                        "text": "(Olteanu et al., 2006;",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 99,
                        "end": 117,
                        "text": "Kann et al., 2018)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 174,
                        "end": 175,
                        "text": "1",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Overview of the LM-based method",
                "sec_num": "3.1"
            },
            {
                "text": "In the count-based methods mentioned in Section 2.2, researchers often require preprocessors to identify the occurrence of the phenomena of interest in a large corpus. On the other hand, researchers need to prepare data to be scored by LMs to evaluate hypothesis in the LM-based method. Whether it is easier to prepare the preprocessor or the evaluation data depends on the situation. For example, the data preparation is easier in the situation where one wants to analyze the word order trends when a specific postpositional particle is omitted. The question is whether Japanese speakers prefer the word order like in Example (3)-a or (3)-b. Thus, in such situation, the LM-based method can be suitable. The human-based method is more reliable given an example. However, it can be prohibitively costly. While the human-based method requires an evaluation data and human subjects, the LM-based method only requires the evaluation data. Thus, the LM-based method can be more suitable for estimating the validity of hypotheses and considering many examples as exhaustively as possible. In addition, the LM-based method can be replicable. The suitable approach can be different in a situation, and broadening the choice of alternative methodologies may be beneficial to linguistic research.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Advantages of the LM-based method",
                "sec_num": "3.2"
            },
            {
                "text": "Nowadays, various useful frameworks, language resources, and machine resources required to train LMs are available,2 which support the ease of implementing the LM-based method. Moreover, we make the LMs used in this study available.3 ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Advantages of the LM-based method",
                "sec_num": "3.2"
            },
            {
                "text": "The goal of this study is to validate the use of LMs for analyzing the canonical word order. The canonical word order itself is still a subject of research, and the community does not know all about it. Thus, it is ultimately impossible to enumerate the requirements on what LMs should know about the canonical word order and probe the knowledge of LMs. Instead, we demonstrate the validity of the LM-based method by showcasing two types of parallels: (i) word order preference of LMs showing parallels with that of humans, and (ii) the results obtained with the LM-based method and those with previous methods being consistent on various claims on canonical word order. If the results of LMs are consistent with those of existing methods, the possibility that LMs and existing methods have the same ability to evaluate the hypotheses is supported. If the LM-based method is assumed to be valid, the method has the potential to streamline the research on unevaluated claims on word order. In the experiment sections, we examine the properties of Japanese LMs on (i) and (ii).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Strategies to validate the use of LMs to analyze the word order",
                "sec_num": "3.3"
            },
            {
                "text": "Even if LMs satisfy the criteria described in 3.3, there is no exact guarantee that LM scores will reflect the effectiveness of human processing of specific constructions in general. Thus, there seems to be a danger of confusing LM artifacts with language facts. Based on this, we hope that researchers use LMs as a tool just to limit the hypothesis space. LM supported hypotheses should then be re-verified with a human-based approach.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "CAUTION -when using LMs for evaluating linguistic hypotheses",
                "sec_num": "3.4"
            },
            {
                "text": "Furthermore, since there is a lot of hypotheses and corresponding research, we cannot check all the properties of LMs in this study. This study focuses on intra-sentential factors of Japanese case order, and it is still unclear whether the LM-based method works properly in linguistic phenomena which are far from being the focus of this study. This is the first study where evidence is collected on the validity of using LMs for word order analysis and encourages further research on collecting such evidence and examining under what conditions this validity is guaranteed.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "CAUTION -when using LMs for evaluating linguistic hypotheses",
                "sec_num": "3.4"
            },
            {
                "text": "We used auto-regressive, unidirectional LMs with Transformer (Vaswani et al., 2017) . We used two variants of LMs, a character-based LM (CLM) and a subword-based LM (SLM). In training SLM, the input sentences are once divided into morphemes by MeCab (Kudo, 2006) with a UniDic dictionary,4 and then these morphemes are split into subword units by byte-pair-encoding. (Sennrich et al., 2016) 5 . 160M sentences6 randomly selected from 3B web pages were used to train the LMs. Hyperparameters are shown in Appendix A.",
                "cite_spans": [
                    {
                        "start": 61,
                        "end": 83,
                        "text": "(Vaswani et al., 2017)",
                        "ref_id": "BIBREF36"
                    },
                    {
                        "start": 250,
                        "end": 262,
                        "text": "(Kudo, 2006)",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 367,
                        "end": 390,
                        "text": "(Sennrich et al., 2016)",
                        "ref_id": "BIBREF33"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "LMs settings",
                "sec_num": "3.5"
            },
            {
                "text": "Given a sentence s, we calculate its generation probability p(s) = -\u2192 p (s) \u2022 \u2190p (s), where -\u2192 p (\u2022) and \u2190p (\u2022) are generation probabilities calculated by a left-to-right LM and a right-to-left LM, respectively. Depending on the hypothesis, we compare the generation probabilities of various variants of s with different word orders. We assume that the word order with the highest generation probability follows their canonical word order.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "LMs settings",
                "sec_num": "3.5"
            },
            {
                "text": "To examine the validity of using LMs for canonical word order analysis, we examined the parallels between the LMs and humans on the task determining the canonicality of the word order (Figure 2 ). First, we created data for this task (Section 4.1). We then compared the word order preference of LMs and that of humans (Section 4.2). ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 192,
                        "end": 193,
                        "text": "2",
                        "ref_id": "FIGREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Experiment1: comparing human and LMs word order preference",
                "sec_num": "4"
            },
            {
                "text": "Data We randomly collected 10k sentences from 3B web pages, which are not overlapped with the LM training data. To remove overly complex sentences, we extracted sentences that must: (i) have less than or equal to five clauses and one verb, (ii) have clauses with a sibling relationship in its dependency tree, and they accompany a particle or adverb, (iii) not have special symbols such as parentheses, and (iv) not have a backward dependency path. For each sentence, we created its scrambled version. 7 The scrambling process is as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Human annotation",
                "sec_num": "4.1"
            },
            {
                "text": "1. Identify the dependency structure by using JUMAN8 and KNP9 . 2. Randomly select a clause with several children. 3. Shuffle the position of its children along with their descendants. Annotation We used the crowdsourcing platform Yahoo Japan!10 . For our task, we showed crowdworkers a pair of sentences (order 1 , order 2 ), where one sentence has the original word order, and the other sentence has a scrambled word order.11 Each annotator was instructed to label the pair with one of the following choices: (1) order 1 is better, (2) order 2 is better, or (3) the pair contains a semantically broken sentence. Only the sentences (order 1 , order 2 ) were shown to the annotators, and they were instructed not to imagine a specific context for the sentences. We filtered unmotivated workers by using check questions. 12 For each pair instance, we employed 10 crowdworkers. In total, 756 unique, motivated crowdworkers participated in our task.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Human annotation",
                "sec_num": "4.1"
            },
            {
                "text": "From the annotated data, we collected only the pairs satisfying the following conditions for our experiments: (i) none of 10 annotators determined that the pair contains a semantically broken sentence, and (ii) nine or more annotators preferred the same order. The majority decision is labeled in each pair; the task is binary classification. We assume that if many workers prefer a certain word order, then it follows its canonical word order, and the other one deviates from it. We collected 2.6k pair instances of sentences.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Human annotation",
                "sec_num": "4.1"
            },
            {
                "text": "We compared the word order preference of LMs and that of the workers by using the 2.6K pairs created in Section 4.1. We calculated the correlation of the decisions between the LMs and the workers; which word order is more appropriate order 1 or order 2 . The word orders supported by CLM and SLM are highly correlated with workers, with the Pearson correlation coefficient of 0.89 and 0.90, respectively. This supports the assumption that the generation probability of LMs can determine the canonical word order as accurately as humans do. Note that such a direct comparison of word order is difficult with the count-based methods because of the sparsity of the corpus.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "4.2"
            },
            {
                "text": "This section examines whether LMs show word order preference consistent with previous linguistic studies. The results are entirely consistent, which support the validity of the LM-based methods in Japanese. Each subsection focuses on a specific component of Japanese sentences.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiment2: consistency with previous studies",
                "sec_num": "5"
            },
            {
                "text": "The order of double objects is one of the most controversial topics in Japanese word order. Examples of the possible order are as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Double objects",
                "sec_num": "5.1"
            },
            {
                "text": "( (c) Relationship between the degree of co-occurrence of verb and arguments, and the ACC-DAT rate in each example. For the results of LMs, the ACC-DAT rate of each example is regarded as 1 if LMs prefer ACC-DAT order, otherwise we regard the example as 0. claims Sasano and Okumura (2016) focused on with the data they collected. 13Word order for each verb First, we analyzed the trend of the double object order for each verb. We analyzed 620 verbs following Sasano and Okumura (2016). 14 For each set of examples S v corresponding to a verb v, we: (i) created an instance with the swapped order of ACC and DAT for each example, and (ii) compared the generation probabilities of the original and swapped instance. \u015cv is the set of examples preferred by LMs. R v ACC-DAT is calculated as follows:",
                "cite_spans": [
                    {
                        "start": 264,
                        "end": 289,
                        "text": "Sasano and Okumura (2016)",
                        "ref_id": "BIBREF32"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Double objects",
                "sec_num": "5.1"
            },
            {
                "text": "R v ACC-DAT = N v ACC-DAT N v ACC-DAT + N v DAT-ACC",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Double objects",
                "sec_num": "5.1"
            },
            {
                "text": ", where N v ACC-DAT / N v DAT-ACC is the number of examples with the ACC-DAT / DAT-ACC order in \u015cv .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Double objects",
                "sec_num": "5.1"
            },
            {
                "text": "Figure 3-(a) shows the relationship between R v ACC-DAT determined by LMs and one reported in a previous count-based study (Sasano and Okumura, 2016) . These results strongly correlate with the Pearson correlation coefficient of 0.91 and 0.88, in CLM and SLM, respectively. In addition, \"canonical word order is DAT-ACC\" (Hoji, 1985) is unlikely to be valid because there are verbs where R v ACC-DAT is very high (details in Appendix B.1). This conclusion is consistent with Sasano and Okumura (2016) .",
                "cite_spans": [
                    {
                        "start": 123,
                        "end": 149,
                        "text": "(Sasano and Okumura, 2016)",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 321,
                        "end": 333,
                        "text": "(Hoji, 1985)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 475,
                        "end": 500,
                        "text": "Sasano and Okumura (2016)",
                        "ref_id": "BIBREF32"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 12,
                        "text": "3-(a)",
                        "ref_id": "FIGREF5"
                    }
                ],
                "eq_spans": [],
                "section": "Double objects",
                "sec_num": "5.1"
            },
            {
                "text": "Word order and verb types In Japanese, there are show-type and pass-type verbs (details in Appendix B.2). Matsuoka (2003) claimed that the order of double objects differs depending on these verb types. Following Sasano and Okumura (2016) , we analyzed this trends.",
                "cite_spans": [
                    {
                        "start": 106,
                        "end": 121,
                        "text": "Matsuoka (2003)",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 212,
                        "end": 237,
                        "text": "Sasano and Okumura (2016)",
                        "ref_id": "BIBREF32"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Double objects",
                "sec_num": "5.1"
            },
            {
                "text": "We applied the Wilcoxon rank-sum test between the distributions of R v ACC-DAT determined by LMs in the two groups (show-type and passtype verbs). The results show no significant difference between the two groups (p-value is 0.17 and 0.12 in the experiments using CLM and SLM, respectively). These results are consistent with the count-based (Sasano and Okumura, 2016) and the human-based (Miyamoto, 2002; Koizumi and Tamaoka, 2004) DAT-only and R v ACC-DAT is 0.404 for CLM and 0.374 for SLM. The results are consistent with Sasano and Okumura (2016) , where they reported that the correlation coefficient was 0.391.",
                "cite_spans": [
                    {
                        "start": 342,
                        "end": 368,
                        "text": "(Sasano and Okumura, 2016)",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 389,
                        "end": 405,
                        "text": "(Miyamoto, 2002;",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 406,
                        "end": 432,
                        "text": "Koizumi and Tamaoka, 2004)",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 526,
                        "end": 551,
                        "text": "Sasano and Okumura (2016)",
                        "ref_id": "BIBREF32"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Double objects",
                "sec_num": "5.1"
            },
            {
                "text": "Word order and semantic role of the dative argument Matsuoka (2003) claimed that the canonical word order differs depending on the semantic role of the dative argument. Sasano and Okumura Type-A has an inanimate goal (school) as the DAT argument, while Type-B has an animate processor (teacher). It was reported that Type-A is likely to be the ACC-DAT order, while Type-B is likely to be the DAT-ACC order. Following Sasano and Okumura (2016), we analyzed 113 verbs. 15 For each verb, we compared the ACC-DAT rate in its type-A examples and the rate in its type-B examples.",
                "cite_spans": [
                    {
                        "start": 52,
                        "end": 67,
                        "text": "Matsuoka (2003)",
                        "ref_id": "BIBREF22"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Double objects",
                "sec_num": "5.1"
            },
            {
                "text": "The number of verbs where the ACC-DAT order is preferred in Type-A examples to Type-B examples is significantly larger (a two-sided sign test p < 0.05). This result is consistent with that of Sasano and Okumura (2016) ; Matsuoka (2003) and implies that the LMs capture the animacy of the nouns. Details are in Appendix B.3.",
                "cite_spans": [
                    {
                        "start": 192,
                        "end": 217,
                        "text": "Sasano and Okumura (2016)",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 220,
                        "end": 235,
                        "text": "Matsuoka (2003)",
                        "ref_id": "BIBREF22"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Double objects",
                "sec_num": "5.1"
            },
            {
                "text": "Word order and co-occurrence of verb and arguments Sasano and Okumura (2016) claimed that an argument that frequently co-occurs with the verb tends to be placed near the verb. For each example, the LMs determine which word order (DAT-ACC or ACC-DAT) is appropriate. Each example also has a score \u2206NPMI (definition in Appendix B.4). Higher \u2206NPMI means that the DAT noun in the example more strongly co-occurs with the verb in the example than the ACC noun.",
                "cite_spans": [
                    {
                        "start": 51,
                        "end": 76,
                        "text": "Sasano and Okumura (2016)",
                        "ref_id": "BIBREF32"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Double objects",
                "sec_num": "5.1"
            },
            {
                "text": "Figure 3-(c ) shows the relationship between \u2206NPMI and the ACC-DAT rate in each example. \u2206NPMI and the ACC-DAT rate are correlated with the Pearson correlation coefficient of 0.517 and 0.521 in CLM and SLM, respectively. These results are consistent with Sasano and Okumura (2016) . Table 3 : The scores denote the rank correlation between the preference of each adverb position in LMs and that reported in (Koizumi and Tamaoka, 2006) .",
                "cite_spans": [
                    {
                        "start": 255,
                        "end": 280,
                        "text": "Sasano and Okumura (2016)",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 407,
                        "end": 434,
                        "text": "(Koizumi and Tamaoka, 2006)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 11,
                        "text": "3-(c",
                        "ref_id": "FIGREF5"
                    },
                    {
                        "start": 289,
                        "end": 290,
                        "text": "3",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Double objects",
                "sec_num": "5.1"
            },
            {
                "text": "Our focus moves to the cases closer to the beginning of the sentences. The following claim is a well-known property of Japanese word order: \"The case representing time information (TIM) is placed before the case representing location information (LOC), and the TIM and LOC cases are placed before the NOM case\" (Saeki, 1960 (Saeki, , 1998)) . We examined a parallel between the result obtained with the LM-based and count-based methods on this claim. We randomly collected 81k examples from 3B web pages. 16 To create the examples, we identified the case components by KNP, and the TIM and LOC cases were categorized with JUMAN (details in Appendix C). For each example s, we created all possible word orders and obtained the word order with the highest generation probability (\u015d). Given \u015c a set of \u015d, we calculated a score o(a < b) for cases a and b as follows:",
                "cite_spans": [
                    {
                        "start": 311,
                        "end": 323,
                        "text": "(Saeki, 1960",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 324,
                        "end": 340,
                        "text": "(Saeki, , 1998))",
                        "ref_id": "BIBREF31"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Order of constituents representing time, location, and subject information",
                "sec_num": "5.2"
            },
            {
                "text": "o(a < b) = N a<b N a<b + N b<a ,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Order of constituents representing time, location, and subject information",
                "sec_num": "5.2"
            },
            {
                "text": "where N k<l is the number of examples where the case k precedes the case l in \u015c. Higher o(a < b) indicates that the case a is more likely to be placed before the case b. The results with the LM-based methods and the count-based method are consistent (Table 2 ). Both results show that o(TIM < LOC) is significantly larger than o(TIM > LOC) (p < 0.05 with a two-sided signed test), which indicates that the TIM case usually precedes the LOC case.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 257,
                        "end": 258,
                        "text": "2",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Order of constituents representing time, location, and subject information",
                "sec_num": "5.2"
            },
            {
                "text": "Similarly, the results indicate that the TIM case and the LOC case precedes the NOM case.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Order of constituents representing time, location, and subject information",
                "sec_num": "5.2"
            },
            {
                "text": "We checked the preference of the adverb position in LMs. The position of the adverb has no restriction except that it must be before the verb, which is similar to the trend of the case position. However, Koizumi and Tamaoka (2006) claimed that \"There is a canonical position of an adverb depend- We used the same examples as Koizumi and Tamaoka (2006) . For each example s, we created its three variants with a different adverb position as follows (\"A friend handled the tools roughly.\"): where the sequence of the alphabet such as \"ASOV\" denote the word order of its corresponding sentences. For example, \"ASOV\" indicates the order: adverb < subject < object < verb. \"A,\" \"S,\" \"O,\" and \"V\" denote \"adverb,\" \"subject,\" \"object,\" and \"verb,\" respectively. Then, we obtained the preferred adverb position by comparing their generation probabilities. Finally, for each adverb type and its examples, we ranked the preference of the possible adverb positions: \"ASOV,\" \"SAOV,\" and \"SOAV.\" Table 3 shows the rank correlation of the preference of the position of each adverb type. The results show similar trends of LMs with that of the human-based method (Koizumi and Tamaoka, 2006) .",
                "cite_spans": [
                    {
                        "start": 204,
                        "end": 230,
                        "text": "Koizumi and Tamaoka (2006)",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 325,
                        "end": 351,
                        "text": "Koizumi and Tamaoka (2006)",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 1148,
                        "end": 1175,
                        "text": "(Koizumi and Tamaoka, 2006)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 989,
                        "end": 990,
                        "text": "3",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Adverb position",
                "sec_num": "5.3"
            },
            {
                "text": "The effects of \"long-before-short,\" the trend that a long constituent precedes a short one, has been reported in several studies (Asahara et al., 2018; Orita, 2017) \uff0e We checked whether this effect can be captured with the LM-based method. Among the examples used in Section 5.2, we analyzed about 9.5k examples in which the position of the constituent with the largest number of chunks17 differed between its canonical case order18 and the order supported by LMs.",
                "cite_spans": [
                    {
                        "start": 129,
                        "end": 151,
                        "text": "(Asahara et al., 2018;",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 152,
                        "end": 164,
                        "text": "Orita, 2017)",
                        "ref_id": "BIBREF28"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Long-before-short effect",
                "sec_num": "5.4"
            },
            {
                "text": "Table 4 shows that there are significantly (p < 0.05 with a two-sided signed test) large numbers of examples where the longest constituent moves closer to the beginning of the sentence. This result is consistent with existing studies and supports the tendency for longer constituents to appear before shorter ones.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "4",
                        "ref_id": "TABREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Long-before-short effect",
                "sec_num": "5.4"
            },
            {
                "text": "We found parallels between the results with the LM-based method and that with the previously established method on various properties of canonical word order. These results support the use of LMs for analyzing Japanese canonical word order.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Summary of the results",
                "sec_num": "5.5"
            },
            {
                "text": "6 Analysis: word order and topicalization",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Summary of the results",
                "sec_num": "5.5"
            },
            {
                "text": "In the previous section, we tentatively concluded that LMs can be used for analyzing the intrasentential properties on the canonical word order.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Summary of the results",
                "sec_num": "5.5"
            },
            {
                "text": "Based on this finding, in this section, we demonstrate the analysis of additional claims on the properties of the canonical word order with the LMbased method, which has been less explored by large-scale experiments. This section shows the analysis of the relationship between topicalization and the canonical word order. Additional analyses on the effect of various adverbial particles for the word order are shown in Appendix F.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Summary of the results",
                "sec_num": "5.5"
            },
            {
                "text": "The adverbial particle \"\u306f\" (TOP) is usually used as a postpositional particle when a specific constituent represents the topic of the sentence (Heycock, 1993; Noda, 1996; Fry, 2003) . When a case component is topicalized, the constituent moves to the beginning of the sentence, and the particle \"\u306f\" (TOP) is added (Noda, 1996) . Additionally, the original case particle is sometimes omitted, 19which makes the case of the constituent difficult to identify. For example, to topicalize \"\u672c\u3092\" (book-ACC) in Example (8)-a, the constituent moves to the beginning of the sentence, and the original accusative case particle \"\u3092\" (ACC) is omitted. Similarly, \"\u5148\u751f\u304c\" (teacher-NOM) is topicalized in Example (8)-b. The original sentence is enclosed in the square brackets in Example ( 8). With the above process, we can easily create a sentence with a topicalized constituent. On the other hand, identifying the original case of the topicalized case components is error-prone. Thus, the LM-based method can be suitable for empirically evaluating the claims related to the topicalization.",
                "cite_spans": [
                    {
                        "start": 143,
                        "end": 158,
                        "text": "(Heycock, 1993;",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 159,
                        "end": 170,
                        "text": "Noda, 1996;",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 171,
                        "end": 181,
                        "text": "Fry, 2003)",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 314,
                        "end": 326,
                        "text": "(Noda, 1996)",
                        "ref_id": "BIBREF26"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Topicalization in Japanese",
                "sec_num": "6.1"
            },
            {
                "text": "By using the LM-based method, we evaluate the following two claims:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments and results",
                "sec_num": "6.2"
            },
            {
                "text": "(i) The more anterior the case is in the canonical word order, the more likely its component is topicalized (Noda, 1996) . (ii) The more the verb prefers the ACC-DAT order, the more likely the ACC case is topicalized than the DAT case.",
                "cite_spans": [
                    {
                        "start": 108,
                        "end": 120,
                        "text": "(Noda, 1996)",
                        "ref_id": "BIBREF26"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments and results",
                "sec_num": "6.2"
            },
            {
                "text": "The claim (i) suggests that, for example, the NOM case is more likely to be topicalized than the ACC case because the NOM case is before the ACC case in the canonical word order of Japanese. The claim (ii) is based on our observation. It can be regarded as an extension of the claim (i) considering the effect of the verb on its argument order. We assume that the canonical word order of Japanese is TIM < LOC < NOM < DAT < ACC in this section.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments and results",
                "sec_num": "6.2"
            },
            {
                "text": "We examine which case is more likely to be topicalized. We collected 81k examples from Japanese Wikipedia (Details are in Appendix C).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Claim (i)",
                "sec_num": null
            },
            {
                "text": "For each example, a set of candidates was created by topicalizing each case, as shown in Example (8). Then, we selected the sentences with the highest score by LMs in each candidate set. We denote the obtained sentences as \u015ctopic . We calculated a score t a|b for pairs of cases a and b.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Claim (i)",
                "sec_num": null
            },
            {
                "text": "t a|b = N a|b N a|b + N b|a",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Claim (i)",
                "sec_num": null
            },
            {
                "text": "where N a|b is the examples where the case a and b appear, and case a is a topic of the sentence in \u015ctopic . The higher the score is, the more the case a is likely to be topicalized than the case b is.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Claim (i)",
                "sec_num": null
            },
            {
                "text": "We compared t a|b and t b|a among the pairs of cases a and b, where the case a precedes the case b in the canonical word order. Through our experiments, t a|b was significantly larger than t b|a (p < 0.05 with a paired t-test) in CLM and SLM results, which supports the claim (i) (Noda, 1996) . Detailed results are shown in Appendix E.",
                "cite_spans": [
                    {
                        "start": 280,
                        "end": 292,
                        "text": "(Noda, 1996)",
                        "ref_id": "BIBREF26"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Claim (i)",
                "sec_num": null
            },
            {
                "text": "The canonical word order of double objects is different for each verb (Section 5.1). Based on this assumption and the claim (i), we hypothesized that the more the verb prefers the ACC-DAT order, the more likely the ACC case of the verb is topicalized than the DAT case.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Claim (ii)",
                "sec_num": null
            },
            {
                "text": "We used the same data as in Section 5.1. For each example, we created two sentences by topicalizing the ACC or DAT argument. Then we compared their generation probabilities. In each set of examples corresponding to a verb v, we calculated the rate that the sentence with the topicalized ACC argument is preferred rather than that with the topicalized DAT argument. This rate and R v ACC-DAT is significantly correlated with the Pearson correlation coefficient of 0.89 and 0.84 in CLM and SLM, respectively. This results support the claim (ii). Detailed results are shown in Appendix E.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Claim (ii)",
                "sec_num": null
            },
            {
                "text": "We have proposed to use LMs as a tool for analyzing word order in Japanese. Our experimental results support the validity of using Japanese LMs for canonical word order analysis, which has the potential to broaden the possibilities of linguistic research. From an engineering view, this study supports the use of LMs for scoring Japanese word order automatically. From the viewpoint of the linguistic field, we provide additional empirical evidence to various word order hypotheses as well as demonstrate the validity of the LM-based method.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion and Future work",
                "sec_num": "7"
            },
            {
                "text": "We plan to further explore the capability of LMs on other linguistic phenomena related to word order, such as \"given new ordering\" (Nakagawa, 2016; Asahara et al., 2018) . Since LMs are language-agnostic, analyzing word order in another language with the LM-based method would also be an interesting direction to investigate. Furthermore, we would like to extend a comparison between machine and human language processing beyond the perspective of word order.",
                "cite_spans": [
                    {
                        "start": 131,
                        "end": 147,
                        "text": "(Nakagawa, 2016;",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 148,
                        "end": 169,
                        "text": "Asahara et al., 2018)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion and Future work",
                "sec_num": "7"
            },
            {
                "text": "We used the Transformer (Vaswani et al., 2017) LMs implemented in fairseq (Ott et al., 2019) . Table 5 shows the hyperparameters of the LMs. The adaptive softmax cutoff (Grave et al., 2017) is only applied to SLM. We split 10K sentences for dev set.",
                "cite_spans": [
                    {
                        "start": 24,
                        "end": 46,
                        "text": "(Vaswani et al., 2017)",
                        "ref_id": "BIBREF36"
                    },
                    {
                        "start": 74,
                        "end": 92,
                        "text": "(Ott et al., 2019)",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 169,
                        "end": 189,
                        "text": "(Grave et al., 2017)",
                        "ref_id": "BIBREF12"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 101,
                        "end": 102,
                        "text": "5",
                        "ref_id": "TABREF4"
                    }
                ],
                "eq_spans": [],
                "section": "A Hyperparameters and implementation of the LMs",
                "sec_num": null
            },
            {
                "text": "The left-to-right and right-to-left CLMs achieved a perplexity of 11.05 and 11.08, respectively. The left-to-right and right-to-left SLMs achieved a perplexity of 28.51 and 28.25, respectively. Note that the difference in the perplexities between CLM and SLM is due to the difference in the vocabulary size.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Hyperparameters and implementation of the LMs",
                "sec_num": null
            },
            {
                "text": "B Details on Section 5.1 (double objects)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Hyperparameters and implementation of the LMs",
                "sec_num": null
            },
            {
                "text": "It is considered that different verbs have different preferences in the order of their object. For example, while the verb \"\u4f8b\u3048\u308b\" (compare) prefers the ACC-DAT order (Example (9)-a), the verb \"\u8868\u3059 \u308b\" (express) prefers the DAT-ACC order (Example (9)-b). Table 6 shows the verbs with the top five and the five worst R v ACC-DAT .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 257,
                        "end": 258,
                        "text": "6",
                        "ref_id": "TABREF5"
                    }
                ],
                "eq_spans": [],
                "section": "B.1 Word order for each verb",
                "sec_num": null
            },
            {
                "text": "There are two types of causative-inchoative alternating verbs in Japanese: show-type verbs and passtype verbs. The verb types are determined by the subject of the sentence where the corresponding inchoative verb is used. For the show-type verbs, the DAT argument of a causative sentence becomes the subject in its corresponding inchoative sentence (Example (10)). On the other hand, the ACC argument of a causative sentence becomes the subject in its corresponding inchoative sentence for the pass-type verbs (Example (11)). Matsuoka (2003) claims that the show-type verb prefers the DAT-ACC order, while the pass-type verb prefers the ACC-DAT order.",
                "cite_spans": [
                    {
                        "start": 525,
                        "end": 540,
                        "text": "Matsuoka (2003)",
                        "ref_id": "BIBREF22"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.2 Word order and verb types",
                "sec_num": null
            },
            {
                "text": "Table 7 shows R v ACC-DAT of the show-type and pass-type verbs. The results show no significant difference in word order trends between show-type and pass-type verbs, which are consistent with that of Sasano and Okumura (2016) .",
                "cite_spans": [
                    {
                        "start": 201,
                        "end": 226,
                        "text": "Sasano and Okumura (2016)",
                        "ref_id": "BIBREF32"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "7",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "B.2 Word order and verb types",
                "sec_num": null
            },
            {
                "text": "As described in Section 5.1, Sasano and Okumura (2016) reported that type-A examples prefer the ACC-DAT order and type-B examples prefer the DAT-ACC order. We used the same examples as Sasano and Okumura (2016) used. We analyzed the difference in the trend of argument order between type-A and type-B examples in each verb.",
                "cite_spans": [
                    {
                        "start": 29,
                        "end": 54,
                        "text": "Sasano and Okumura (2016)",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 185,
                        "end": 210,
                        "text": "Sasano and Okumura (2016)",
                        "ref_id": "BIBREF32"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.3 Word order and semantic role of the dative argument",
                "sec_num": null
            },
            {
                "text": "Table 8 shows the verbs, which show a significant change in the argument order between type-A and type-B examples (p < 0.05 in a two-proportion z-test). In the experiment using CLM, 31 verbs show the trend that type-A examples more prefer the ACC-DAT order to type-B, and 17 verbs show contrary trends. In the experiment using SLM, 38 verbs show the trend that type-A examples more prefer the ACC-DAT order to type-B, and 11 verbs show contrary trends. These results show that the number of verbs, where the ACC-DAT order is preferred by type-A examples rather than type-B, is significantly larger (p < 0.05 with a two-sided sign test). This experimental design follows Sasano and Okumura (2016) .",
                "cite_spans": [
                    {
                        "start": 670,
                        "end": 695,
                        "text": "Sasano and Okumura (2016)",
                        "ref_id": "BIBREF32"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "8",
                        "ref_id": "TABREF7"
                    }
                ],
                "eq_spans": [],
                "section": "B.3 Word order and semantic role of the dative argument",
                "sec_num": null
            },
            {
                "text": "We evaluate the claim that an argument frequently co-occurring with the verb tends to be placed near the verb. We examine the relationship between each example's word order trend and \u2206NPMI. \u2206NPMI is calculated as follows: Sasano and Okumura (2016) .",
                "cite_spans": [
                    {
                        "start": 222,
                        "end": 247,
                        "text": "Sasano and Okumura (2016)",
                        "ref_id": "BIBREF32"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.4 Word order and co-occurrence of verb and arguments",
                "sec_num": null
            },
            {
                "text": "\u2206NPMI = NPMI(n DAT , v) -NPMI(n ACC , v) , where NPMI(n c , v) = PMI(n c , v) -log(p(n c , v)) , PMI(n c , v) = log p(n c , v) p(n c )p(v) ,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.4 Word order and co-occurrence of verb and arguments",
                "sec_num": null
            },
            {
                "text": "where, v is a verb and n c (c \u2208 DAT, ACC) is its argument.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.4 Word order and co-occurrence of verb and arguments",
                "sec_num": null
            },
            {
                "text": "C Data used in Section 5.2, Section 6, and Appendix F First, we randomly collected 50M sentences from 3B web pages. Note that there is no overlap between the collected sentences and the training data of LMs. Next, we obtained the sentences that satisfy the following criteria:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.4 Word order and co-occurrence of verb and arguments",
                "sec_num": null
            },
            {
                "text": "\u2022 There is a verb (placed at the end of the sentence) with more than two arguments (accompanying the case particle ga, o, ni, or de), where dependency distance between the verb and arguments is one. \u2022 Each argument (with its descendant) has fewer than 11 morphemes in the argument.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.4 Word order and co-occurrence of verb and arguments",
                "sec_num": null
            },
            {
                "text": "In each example, the verb (satisfying the above condition), its arguments, and the descendants of the arguments are extracted. Example sentences are created by concatenating the verb, its argument, and the descendants of the arguments with preserving their order in the original sentences.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.4 Word order and co-occurrence of verb and arguments",
                "sec_num": null
            },
            {
                "text": "In the experiments in Section 5.2, we analyzed the word order trend of the TIM and LOC constituents. We regard the constituent (argument and its descendants) satisfying the following condition as the TIM constituent:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.4 Word order and co-occurrence of verb and arguments",
                "sec_num": null
            },
            {
                "text": "\u2022 Accompanying the postpositional case particle \"\u306b\" (DAT). Table 7 : Overlap of the results of LMs and that of Sasano and Okumura (2016) on the relationship of the ACC-DAT rate and verb types. Each score corresponding to a verb denotes its DAT-ACC rate. The \"S&O\" columns show the ACC-DAT rate reported in Sasano and Okumura (2016) . There is no significant difference between the distributions of the DAT-ACC rate in two verb types.",
                "cite_spans": [
                    {
                        "start": 111,
                        "end": 136,
                        "text": "Sasano and Okumura (2016)",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 306,
                        "end": 331,
                        "text": "Sasano and Okumura (2016)",
                        "ref_id": "BIBREF32"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 65,
                        "end": 66,
                        "text": "7",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "B.4 Word order and co-occurrence of verb and arguments",
                "sec_num": null
            },
            {
                "text": "\u2022 Containing time category morphemes 20 .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.4 Word order and co-occurrence of verb and arguments",
                "sec_num": null
            },
            {
                "text": "We regard the constituent (argument and its descendants) satisfying the following condition as the LOC constituent:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.4 Word order and co-occurrence of verb and arguments",
                "sec_num": null
            },
            {
                "text": "\u2022 Accompanying the postpositional case particle \"\u3067\". \u2022 Containing location category morphemes 20 . 81k examples were created. The averaged number of characters in a sentence was 45.1 characters. The number of occurrences of each case is shown in Table 9 . The scrambling process conducted in the experiments (Sections 5.2 and 6) is the same as described in Section 4.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 252,
                        "end": 253,
                        "text": "9",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "B.4 Word order and co-occurrence of verb and arguments",
                "sec_num": null
            },
            {
                "text": "Table 10 shows the correlation between the result of LMs and that of Koizumi and Tamaoka (2006) . The column \"Canonical\" shows the position, which is significantly preferred over the other positions. \"A,\" \"S,\" \"O,\" and \"V\" denote \"adverb,\" \"subject,\" \"object,\" and \"verb,\" respectively. The sequence of the alphabets corresponds to their order; for example, \"ASOV\" indicates the order: adverb < subject < object < verb. Following Koizumi and Tamaoka (2006) , we examined the three candidate positions of the adverb: \"ASOV,\" \"SAOV,\" and \"SOAV.\" The score r denotes the Pearson correlation coefficient of the preferred ranks of each adverb position to that reported in Koizumi and Tamaoka (2006) .",
                "cite_spans": [
                    {
                        "start": 69,
                        "end": 95,
                        "text": "Koizumi and Tamaoka (2006)",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 430,
                        "end": 456,
                        "text": "Koizumi and Tamaoka (2006)",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 667,
                        "end": 693,
                        "text": "Koizumi and Tamaoka (2006)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 8,
                        "text": "10",
                        "ref_id": "TABREF8"
                    }
                ],
                "eq_spans": [],
                "section": "D Details on Section 5.3 (adverb)",
                "sec_num": null
            },
            {
                "text": "We topicalized a specific constituent by moving the constituent to the beginning of the sentence and 20 identified by JUMAN adding the adverbial particle \"\u306f\" (TOP). Strictly speaking, conjunctions are preferentially placed at the beginning of the sentence rather than topicalized constituents. The examples we used do not include the conjunctions at the beginning of the sentence. The adverbial particle was added according to the rules shown in Table 12 .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 452,
                        "end": 454,
                        "text": "12",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "E Details on Section 6.2 (topicalization)",
                "sec_num": null
            },
            {
                "text": "Claim (i): Table 11 shows the t a|b for each pair of the case a (row) and b (column). The results show that the more anterior the case a is and the more posterior the case b is in the canonical word order, the larger the t a|b is.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 17,
                        "end": 19,
                        "text": "11",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "E Details on Section 6.2 (topicalization)",
                "sec_num": null
            },
            {
                "text": "Claim (ii): Figure 4 shows that the more a verb prefers the ACC-DAT order, the more ACC case tends to be topicalized. The X-axis denotes the ACC-DAT rate of the verb, and the Y-axis denotes the trend that ACC is more likely to be topicalized than DAT. Table 9 : The number of occurrence for each case in the data used in Section 5.2, Section 6, and Appendix F F Additional analysis: adverbial particles and their effect for word order",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 19,
                        "end": 20,
                        "text": "4",
                        "ref_id": "FIGREF12"
                    },
                    {
                        "start": 258,
                        "end": 259,
                        "text": "9",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "E Details on Section 6.2 (topicalization)",
                "sec_num": null
            },
            {
                "text": "The adverbial particles We can add supplementary information with adverbial particles. The adverbial particle \"\u306f\" (TOP) is the typical one. In Example (12), the adverbial particle \"\u3082\" (also), instead of \"\u3092\" (ACC), implies that there is another thing the teacher gave to the student (\"a teacher gave not only \u03c6 but also a book to a student.\").",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E Details on Section 6.2 (topicalization)",
                "sec_num": null
            },
            {
                "text": "(12) ::::: \u751f\u5f92\u306b \u672c\u3092 \u3082 \u3042\u3052\u305f. student-DAT also book-ACC gave.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E Details on Section 6.2 (topicalization)",
                "sec_num": null
            },
            {
                "text": "Experiments A constituent accompanying the adverbial particle \"\u306f\" (TOP) is moved to the beginning of the sentence (Noda, 1996) . However, it is not clear whether other adverbial particles also have the above property. In this section, we evaluate the following claim: a different adverbial particle shows different degrees of the effects for the word order.",
                "cite_spans": [
                    {
                        "start": 114,
                        "end": 126,
                        "text": "(Noda, 1996)",
                        "ref_id": "BIBREF26"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E Details on Section 6.2 (topicalization)",
                "sec_num": null
            },
            {
                "text": "For each example s \u2208 S collected from Japanese Wikipedia, we replaced the postpositional particle with a specific adverbial particle, following the rules in Table 12 . We used four typical adverbial particles: \"\u306f\" (TOP), \"\u3053\u305d\" (emphasis), \"\u3082\" (also), and \"\u3060\u3051\" (only). Two variants of word order, Non-moved, and Moved were created for each example. Example ( 13) is an example focusing on the ACC case with the particle \"\u3082\" (also). We compared the generation probabilities between the Non-moved and Moved orders. We calculated the rate that the Moved order is preferred in each combination of the case types and the adverbial particles. Koizumi and Tamaoka (2006) . The column \"Canonical\" shows the adverb position, which is significantly preferred over the other positions. The score r denotes the Pearson correlation coefficient of the preferred rank of three possible adverb positions obtained from LMs to that of Koizumi and Tamaoka (2006) .",
                "cite_spans": [
                    {
                        "start": 635,
                        "end": 661,
                        "text": "Koizumi and Tamaoka (2006)",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 915,
                        "end": 941,
                        "text": "Koizumi and Tamaoka (2006)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 163,
                        "end": 165,
                        "text": "12",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "E Details on Section 6.2 (topicalization)",
                "sec_num": null
            },
            {
                "text": "Omitted characters are crossed out. (e.g., \u3092)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "For example, one can train LMs with fairseq(Ott et al., 2019) and Wikipedia data on cloud computing platforms.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://github.com/kuribayashi4/LM_ as_Word_Order_Evaluator.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://unidic.ninjal.ac.jp/",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Implemented in sentencepiece(Kudo and Richardson, 2018) We set character coverage to 0.9995\uff0cand vocab size to 100,000.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "14GB in UTF-8 encoding. For reference, Japanese Wikipedia has around 2.5 GB of text. Because the focus of this study has context-independent nature, the sentences order is shuffled to prevent learning the inter-sentential characteristics of the language.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "When several scrambled versions were possible for a given sentence, we randomly selected one of them.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "http://nlp.ist.i.kyoto-u.ac.jp/EN/ index.php?JUMAN",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "http://nlp.ist.i.kyoto-u.ac.jp/EN/ index.php?KNP",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://crowdsourcing.yahoo.co.jp/",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Crowdworkers did not know which sentence was the original sentence.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "12 We manually created check questions considering the Japanese speakers' preference in trial experiments in advance.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "We filtered the examples overlapping with the training data of LMs in advance. As a result, we collected 4.5M examples.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "We removed verbs for which all examples overlap with the data for training the LMs.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Among the 126 verbs used inSasano and Okumura (2016), 113 verbs with data that do not overlap with the LM training data were selected.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "chunks were identified by KNP.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "In this section, canonical case order is assumed to be TOM<LOC<NOM<DAT<ACC.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "The particles \"\u3092\" (ACC) and \"\u304c\" (NOM) are omitted.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "We would like to offer our gratitude to Kaori Uchiyama for taking the time to discuss our paper and Ana Brassard for her sharp feedback on English. We also would like to show our appreciation to the Tohoku NLP lab members for their valuable advice. We are particularly grateful to Ryohei Sasano for sharing the data for double objects order analyses. This work was supported by JST CREST Grant Number JPMJCR1513, JSPS KAK-ENHI Grant Number JP19H04162, and Grant-in-Aid for JSPS Fellows Grant Number JP20J22697.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgments",
                "sec_num": "8"
            },
            {
                "text": "The results are shown in Table 13 . When using \"\u306f\" (TOP) as a postpositional particle, the Moved order is preferred to Non-moved, which is consistent with the well-known characteristics of topicalization described in Section 6. In addition, the degree of preference between Moved and Nonmoved differs depending on the adverbial particles. Furthermore, the results indicate that the anterior case in the canonical word order is likely to move to the beginning of the sentence by the effect of the adverbial particle.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 31,
                        "end": 33,
                        "text": "13",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Results",
                "sec_num": null
            },
            {
                "text": "We analyzed the trend of double object order when a specific case accompanies an adverbial particle. Figure 5 shows the result when the ACC argument accompanies an adverbial particle, and Figure 6 shows the result when the DAT argument accompanies an adverbial particle. The left parts of these figures show the result of CLM, and the right part of these figures shows the result of SLM. The Xaxis denotes the ACC-DAT / DAT-ACC rate of the verb when both of the arguments do not accom-Original case particle After the adverbial particle \"\u306f\" (TOP) is addedTable 12 : Rules of deleting the original case particle when the adverbial particle \"\u306f\" (TOP) is added. This rule is also applied when adding the other adverbial particles (Appendix F).pany an adverbial particle. The Y-axis denotes the ACC-DAT / DAT-ACC rate when a specific case accompanies an adverbial particle. The results show that the case accompanying an adverbial particle is likely to be placed near the beginning of the sentence. In addition, the degree of the above trend depends on the adverbial particles. These results suggest that some adverbial particles have a effect for word order. Table 13 : The scores denote that the Moved order is preferred over the Non-moved order when the corresponding case (column) accompanies the corresponding particle (row). The trend is different depending on the case and particle. indicate that the ACC argument with an adverbial particle (ACC adv ) is more likely to be placed before the DAT argument. In addition, this trend differs for each particle. indicate that the DAT argument with an adverbial particle (DAT adv ) is more likely to be placed before the ACC argument. In addition, this trend differs for each particle.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 108,
                        "end": 109,
                        "text": "5",
                        "ref_id": null
                    },
                    {
                        "start": 195,
                        "end": 196,
                        "text": "6",
                        "ref_id": null
                    },
                    {
                        "start": 561,
                        "end": 563,
                        "text": "12",
                        "ref_id": null
                    },
                    {
                        "start": 1162,
                        "end": 1164,
                        "text": "13",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Additional experiments and results",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Acquiring basic word order: Evidence for data-driven learning of syntactic structure",
                "authors": [
                    {
                        "first": "Nameera",
                        "middle": [],
                        "last": "Akhtar",
                        "suffix": ""
                    }
                ],
                "year": 1999,
                "venue": "Journal of child language",
                "volume": "26",
                "issue": "2",
                "pages": "339--356",
                "other_ids": {
                    "DOI": [
                        "10.1017/S030500099900375X"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Nameera Akhtar. 1999. Acquiring basic word order: Evidence for data-driven learning of syntactic struc- ture. Journal of child language, 26(2):339-356.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Teaching English Word Order to ESL Spanish Students. A Functional Perspective",
                "authors": [
                    {
                        "first": "Isabel",
                        "middle": [],
                        "last": "Alonso",
                        "suffix": ""
                    },
                    {
                        "first": "Belmonte",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 2000,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Isabel Alonso Belmonte et al. 2000. Teaching English Word Order to ESL Spanish Students. A Functional Perspective.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "More than words: Frequency effects for multi-word phrases",
                "authors": [
                    {
                        "first": "Inbal",
                        "middle": [],
                        "last": "Arnon",
                        "suffix": ""
                    },
                    {
                        "first": "Neal",
                        "middle": [],
                        "last": "Snider",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Journal of Memory and Language",
                "volume": "62",
                "issue": "1",
                "pages": "67--82",
                "other_ids": {
                    "DOI": [
                        "10.1016/j.jml.2009.09.005"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Inbal Arnon and Neal Snider. 2010. More than words: Frequency effects for multi-word phrases. Journal of Memory and Language, 62(1):67-82.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Predicting Japanese Word Order in Double Object Constructions",
                "authors": [
                    {
                        "first": "Masayuki",
                        "middle": [],
                        "last": "Asahara",
                        "suffix": ""
                    },
                    {
                        "first": "Satoshi",
                        "middle": [],
                        "last": "Nambu",
                        "suffix": ""
                    },
                    {
                        "first": "Shin-Ichiro",
                        "middle": [],
                        "last": "Sano",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the Eight Workshop on Cognitive Aspects of Computational Language Learning and Processing",
                "volume": "",
                "issue": "",
                "pages": "36--40",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/W18-2805"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Masayuki Asahara, Satoshi Nambu, and Shin-Ichiro Sano. 2018. Predicting Japanese Word Order in Double Object Constructions. In Proceedings of the Eight Workshop on Cognitive Aspects of Compu- tational Language Learning and Processing, pages 36-40, Melbourne. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "An fMRI study of canonical and noncanonical word order in German",
                "authors": [
                    {
                        "first": "J\u00f6rg",
                        "middle": [],
                        "last": "Bahlmann",
                        "suffix": ""
                    },
                    {
                        "first": "Antoni",
                        "middle": [],
                        "last": "Rodriguez-Fornells",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Rotte",
                        "suffix": ""
                    },
                    {
                        "first": "Thomas",
                        "middle": [
                            "F"
                        ],
                        "last": "M\u00fcnte",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Human brain mapping",
                "volume": "28",
                "issue": "10",
                "pages": "940--949",
                "other_ids": {
                    "DOI": [
                        "10.1002/hbm.20318"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "J\u00f6rg Bahlmann, Antoni Rodriguez-Fornells, Michael Rotte, and Thomas F M\u00fcnte. 2007. An fMRI study of canonical and noncanonical word order in Ger- man. Human brain mapping, 28(10):940-949.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Testing the Processing Hypothesis of word order variation using a probabilistic language model",
                "authors": [
                    {
                        "first": "Jelke",
                        "middle": [],
                        "last": "Bloem",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the Workshop on Computational Linguistics for Linguistic Complexity (CL4LC)",
                "volume": "",
                "issue": "",
                "pages": "174--185",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jelke Bloem. 2016. Testing the Processing Hypoth- esis of word order variation using a probabilistic language model. In Proceedings of the Workshop on Computational Linguistics for Linguistic Com- plexity (CL4LC), pages 174-185, Osaka, Japan. The COLING 2016 Organizing Committee.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Predicting the dative alternation",
                "authors": [
                    {
                        "first": "Joan",
                        "middle": [],
                        "last": "Bresnan",
                        "suffix": ""
                    },
                    {
                        "first": "Anna",
                        "middle": [],
                        "last": "Cueni",
                        "suffix": ""
                    },
                    {
                        "first": "Tatiana",
                        "middle": [],
                        "last": "Nikitina",
                        "suffix": ""
                    },
                    {
                        "first": "Harald",
                        "middle": [],
                        "last": "Baayen",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Cognitive foundations of interpretation",
                "volume": "",
                "issue": "",
                "pages": "69--94",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Joan Bresnan, Anna Cueni, Tatiana Nikitina, and R Harald Baayen. 2007. Predicting the dative alter- nation. In Cognitive foundations of interpretation, pages 69-94. KNAW.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Chinese Word Ordering Errors Detection and Correction for Non-Native Chinese Language Learners",
                "authors": [
                    {
                        "first": "Shuk-Man",
                        "middle": [],
                        "last": "Cheng",
                        "suffix": ""
                    },
                    {
                        "first": "Chi-Hsin",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Hsin-Hsi",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers",
                "volume": "",
                "issue": "",
                "pages": "279--289",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Shuk-Man Cheng, Chi-Hsin Yu, and Hsin-Hsi Chen. 2014. Chinese Word Ordering Errors Detection and Correction for Non-Native Chinese Language Learn- ers. In Proceedings of COLING 2014, the 25th Inter- national Conference on Computational Linguistics: Technical Papers, pages 279-289, Dublin, Ireland. Dublin City University and Association for Compu- tational Linguistics.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Verb disposition in argument structure alternations: a corpus study of the dative alternation in Dutch",
                "authors": [
                    {
                        "first": "Timothy",
                        "middle": [],
                        "last": "Colleman",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Language Sciences",
                "volume": "31",
                "issue": "5",
                "pages": "593--611",
                "other_ids": {
                    "DOI": [
                        "10.1016/j.langsci.2008.01.001"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Timothy Colleman. 2009. Verb disposition in argu- ment structure alternations: a corpus study of the dative alternation in Dutch. Language Sciences, 31(5):593-611.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Language universals and linguistic typology: Syntax and morphology",
                "authors": [
                    {
                        "first": "Bernard",
                        "middle": [],
                        "last": "Comrie",
                        "suffix": ""
                    }
                ],
                "year": 1989,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Bernard Comrie. 1989. Language universals and lin- guistic typology: Syntax and morphology. Univer- sity of Chicago press.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Ellipsis and wa-marking in Japanese conversation",
                "authors": [
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Fry",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.4324/9780203484036"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "John Fry. 2003. Ellipsis and wa-marking in Japanese conversation. Taylor & Francis.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Do RNNs learn human-like abstract word order preferences?",
                "authors": [
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Futrell",
                        "suffix": ""
                    },
                    {
                        "first": "Roger",
                        "middle": [
                            "P"
                        ],
                        "last": "Levy",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the Society for Computation in Linguistics (SCiL) 2019",
                "volume": "",
                "issue": "",
                "pages": "50--59",
                "other_ids": {
                    "DOI": [
                        "10.7275/jb34-9986"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Richard Futrell and Roger P Levy. 2019. Do RNNs learn human-like abstract word order preferences? In Proceedings of the Society for Computation in Linguistics (SCiL) 2019, pages 50-59.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Efficient softmax approximation for GPUs",
                "authors": [
                    {
                        "first": "\u00c9douard",
                        "middle": [],
                        "last": "Grave",
                        "suffix": ""
                    },
                    {
                        "first": "Armand",
                        "middle": [],
                        "last": "Joulin",
                        "suffix": ""
                    },
                    {
                        "first": "Moustapha",
                        "middle": [],
                        "last": "Ciss\u00e9",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Grangier",
                        "suffix": ""
                    },
                    {
                        "first": "Herv\u00e9",
                        "middle": [],
                        "last": "J\u00e9gou",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the 34th International Conference on Machine Learning",
                "volume": "70",
                "issue": "",
                "pages": "1302--1310",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "\u00c9douard Grave, Armand Joulin, Moustapha Ciss\u00e9, David Grangier, and Herv\u00e9 J\u00e9gou. 2017. Efficient softmax approximation for GPUs. In Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 1302-1310, International Convention Centre, Sydney, Australia. PMLR.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Syntactic predication in Japanese",
                "authors": [
                    {
                        "first": "Caroline",
                        "middle": [],
                        "last": "Heycock",
                        "suffix": ""
                    }
                ],
                "year": 1993,
                "venue": "Journal of East Asian Linguistics",
                "volume": "2",
                "issue": "2",
                "pages": "167--211",
                "other_ids": {
                    "DOI": [
                        "10.1007/BF01732503"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Caroline Heycock. 1993. Syntactic predication in Japanese. Journal of East Asian Linguistics, 2(2):167-211.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Logical form constraints and configurational structures in Japanese",
                "authors": [
                    {
                        "first": "Hajime",
                        "middle": [],
                        "last": "Hoji",
                        "suffix": ""
                    }
                ],
                "year": 1985,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Hajime Hoji. 1985. Logical form constraints and con- figurational structures in Japanese. PHD Thesis. University of Washington.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "The English dative alternation: The case for verb sensitivity",
                "authors": [
                    {
                        "first": "Malka",
                        "middle": [],
                        "last": "Rappaport",
                        "suffix": ""
                    },
                    {
                        "first": "Hovav",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Beth",
                        "middle": [],
                        "last": "Levin",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Journal of linguistics",
                "volume": "44",
                "issue": "1",
                "pages": "129--167",
                "other_ids": {
                    "DOI": [
                        "10.1017/S0022226707004975"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Malka Rappaport Hovav and Beth Levin. 2008. The English dative alternation: The case for verb sensi- tivity. Journal of linguistics, 44(1):129-167.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Sentence-Level Fluency Evaluation: References Help, But Can Be Spared! In Proceedings of the 22nd Conference on Computational Natural Language Learning",
                "authors": [
                    {
                        "first": "Katharina",
                        "middle": [],
                        "last": "Kann",
                        "suffix": ""
                    },
                    {
                        "first": "Sascha",
                        "middle": [],
                        "last": "Rothe",
                        "suffix": ""
                    },
                    {
                        "first": "Katja",
                        "middle": [],
                        "last": "Filippova",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "313--323",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/K18-1031"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Katharina Kann, Sascha Rothe, and Katja Filippova. 2018. Sentence-Level Fluency Evaluation: Refer- ences Help, But Can Be Spared! In Proceedings of the 22nd Conference on Computational Natural Lan- guage Learning, pages 313-323, Brussels, Belgium. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "A corpus study into word order variation in German subordinate clauses: Animacy affects. Multidisciplinary approaches to language production",
                "authors": [
                    {
                        "first": "Gerard",
                        "middle": [],
                        "last": "Kempen",
                        "suffix": ""
                    },
                    {
                        "first": "Karin",
                        "middle": [],
                        "last": "Harbusch",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "173--181",
                "other_ids": {
                    "DOI": [
                        "10.1515/9783110894028.173"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Gerard Kempen and Karin Harbusch. 2004. A corpus study into word order variation in German subordi- nate clauses: Animacy affects. Multidisciplinary ap- proaches to language production, pages 173-181.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Cognitive processing of Japanese sentences with ditransitive verbs",
                "authors": [
                    {
                        "first": "Masatoshi",
                        "middle": [],
                        "last": "Koizumi",
                        "suffix": ""
                    },
                    {
                        "first": "Katsuo",
                        "middle": [],
                        "last": "Tamaoka",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Gengo Kenkyu (Journal of the Linguistic Society of Japan)",
                "volume": "",
                "issue": "125",
                "pages": "173--190",
                "other_ids": {
                    "DOI": [
                        "10.11435/gengo1939.2004.125_173"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Masatoshi Koizumi and Katsuo Tamaoka. 2004. Cog- nitive processing of Japanese sentences with ditran- sitive verbs. Gengo Kenkyu (Journal of the Linguis- tic Society of Japan), 2004(125):173-190.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "The Canonical Positions of Adjuncts in the Processing of Japanese Sentence",
                "authors": [
                    {
                        "first": "Masatoshi",
                        "middle": [],
                        "last": "Koizumi",
                        "suffix": ""
                    },
                    {
                        "first": "Katsuo",
                        "middle": [],
                        "last": "Tamaoka",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Cognitive Studies: Bulletin of the Japanese Cognitive Science Society",
                "volume": "13",
                "issue": "3",
                "pages": "392--403",
                "other_ids": {
                    "DOI": [
                        "10.11225/jcss.13.392"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Masatoshi Koizumi and Katsuo Tamaoka. 2006. The Canonical Positions of Adjuncts in the Processing of Japanese Sentence. Cognitive Studies: Bulletin of the Japanese Cognitive Science Society, 13(3):392- 403.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Mecab: Yet another part-of-speech and morphological analyzer",
                "authors": [
                    {
                        "first": "Taku",
                        "middle": [],
                        "last": "Kudo",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Taku Kudo. 2006. Mecab: Yet another part-of-speech and morphological analyzer. http://mecab.sourceforge.jp.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing",
                "authors": [
                    {
                        "first": "Taku",
                        "middle": [],
                        "last": "Kudo",
                        "suffix": ""
                    },
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Richardson",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
                "volume": "",
                "issue": "",
                "pages": "66--71",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/D18-2012"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Taku Kudo and John Richardson. 2018. SentencePiece: A simple and language independent subword tok- enizer and detokenizer for Neural Text Processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 66-71, Brussels, Belgium. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Two Types of Ditransitive Consturctions in Japanese",
                "authors": [
                    {
                        "first": "Mikinari",
                        "middle": [],
                        "last": "Matsuoka",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Journal of East Asian Linguistics",
                "volume": "12",
                "issue": "2",
                "pages": "171--203",
                "other_ids": {
                    "DOI": [
                        "10.1023/A:1022472109327"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Mikinari Matsuoka. 2003. Two Types of Ditransitive Consturctions in Japanese. Journal of East Asian Linguistics, 12(2):171-203.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Sources of difficulty in the processing of scrambling in Japanese. Sentence processing in East Asian languages",
                "authors": [
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Edson",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Miyamoto",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "167--188",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Edson T Miyamoto. 2002. Sources of difficulty in the processing of scrambling in Japanese. Sentence pro- cessing in East Asian languages, pages 167-188.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Information structure in spoken japanese: Particles, word order, and intonation",
                "authors": [
                    {
                        "first": "Natsuko",
                        "middle": [],
                        "last": "Nakagawa",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Natsuko Nakagawa. 2016. Information structure in spoken japanese: Particles, word order, and intona- tion.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Preferred Word Orders Correlate with \"Sentential\" Meanings That Cannot Be Reduced to Verb Meanings: A New Perspective on \"Construction Effects",
                "authors": [
                    {
                        "first": "Keiko",
                        "middle": [],
                        "last": "Nakamoto",
                        "suffix": ""
                    },
                    {
                        "first": "Jae-Ho",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Kow",
                        "middle": [],
                        "last": "Kuroda",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Cognitive Studies: Bulletin of the Japanese Cognitive Science Society",
                "volume": "13",
                "issue": "3",
                "pages": "334--352",
                "other_ids": {
                    "DOI": [
                        "10.11225/jcss.13.334"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Keiko Nakamoto, Jae-ho Lee, and Kow Kuroda. 2006. Preferred Word Orders Correlate with \"Sentential\" Meanings That Cannot Be Reduced to Verb Mean- ings: A New Perspective on \"Construction Ef- fects\" in Japanese. Cognitive Studies: Bulletin of the Japanese Cognitive Science Society, 13(3):334- 352.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Wa to ga",
                "authors": [
                    {
                        "first": "Hisashi",
                        "middle": [],
                        "last": "Noda",
                        "suffix": ""
                    }
                ],
                "year": 1996,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Hisashi Noda. 1996. Wa to ga [Wa and ga].",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Language models and reranking for machine translation",
                "authors": [
                    {
                        "first": "Marian",
                        "middle": [],
                        "last": "Olteanu",
                        "suffix": ""
                    },
                    {
                        "first": "Pasin",
                        "middle": [],
                        "last": "Suriyentrakorn",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Moldovan",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Proceedings of the Workshop on Statistical Machine Translation",
                "volume": "",
                "issue": "",
                "pages": "150--153",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Marian Olteanu, Pasin Suriyentrakorn, and Dan Moldovan. 2006. Language models and reranking for machine translation. In Proceedings of the Work- shop on Statistical Machine Translation, pages 150- 153, New York City. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Predicting japanese scrambling in the wild",
                "authors": [
                    {
                        "first": "Naho",
                        "middle": [],
                        "last": "Orita",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the 7th Workshop on Cognitive Modeling and Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "41--45",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Naho Orita. 2017. Predicting japanese scrambling in the wild. In Proceedings of the 7th Workshop on Cognitive Modeling and Computational Linguistics, pages 41-45.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling",
                "authors": [
                    {
                        "first": "Myle",
                        "middle": [],
                        "last": "Ott",
                        "suffix": ""
                    },
                    {
                        "first": "Sergey",
                        "middle": [],
                        "last": "Edunov",
                        "suffix": ""
                    },
                    {
                        "first": "Alexei",
                        "middle": [],
                        "last": "Baevski",
                        "suffix": ""
                    },
                    {
                        "first": "Angela",
                        "middle": [],
                        "last": "Fan",
                        "suffix": ""
                    },
                    {
                        "first": "Sam",
                        "middle": [],
                        "last": "Gross",
                        "suffix": ""
                    },
                    {
                        "first": "Nathan",
                        "middle": [],
                        "last": "Ng",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Grangier",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Auli",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)",
                "volume": "",
                "issue": "",
                "pages": "48--53",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/N19-4009"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. 2019. fairseq: A Fast, Extensible Toolkit for Sequence Modeling. In Proceedings of the 2019 Conference of the North American Chap- ter of the Association for Computational Linguistics (Demonstrations), pages 48-53, Minneapolis, Min- nesota. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "Gendaigo ni okeru gojun no keik\u014d -iwayuru hogo no baai [The trend of word order in modern writing-in so-called complements]",
                "authors": [
                    {
                        "first": "Tetsuo",
                        "middle": [],
                        "last": "Saeki",
                        "suffix": ""
                    }
                ],
                "year": 1960,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "56--63",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tetsuo Saeki. 1960. Gendaigo ni okeru gojun no keik\u014d -iwayuru hogo no baai [The trend of word order in modern writing-in so-called complements]. Gengo seikatsu [Language life], (111):56-63.",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "Y\u014dsetsu Nihongo no Gojun [Essentials of Japanese word order]",
                "authors": [
                    {
                        "first": "Tetsuo",
                        "middle": [],
                        "last": "Saeki",
                        "suffix": ""
                    }
                ],
                "year": 1998,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tetsuo Saeki. 1998. Y\u014dsetsu Nihongo no Gojun [Essen- tials of Japanese word order]. Kurosio Publishers.",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "A Corpus-Based Analysis of Canonical Word Order of Japanese Double Object Constructions",
                "authors": [
                    {
                        "first": "Ryohei",
                        "middle": [],
                        "last": "Sasano",
                        "suffix": ""
                    },
                    {
                        "first": "Manabu",
                        "middle": [],
                        "last": "Okumura",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics",
                "volume": "1",
                "issue": "",
                "pages": "2236--2244",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/P16-1211"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ryohei Sasano and Manabu Okumura. 2016. A Corpus-Based Analysis of Canonical Word Order of Japanese Double Object Constructions. In Proceed- ings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa- pers), pages 2236-2244, Berlin, Germany. Associa- tion for Computational Linguistics.",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "Neural Machine Translation of Rare Words with Subword Units",
                "authors": [
                    {
                        "first": "Rico",
                        "middle": [],
                        "last": "Sennrich",
                        "suffix": ""
                    },
                    {
                        "first": "Barry",
                        "middle": [],
                        "last": "Haddow",
                        "suffix": ""
                    },
                    {
                        "first": "Alexandra",
                        "middle": [],
                        "last": "Birch",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics",
                "volume": "1",
                "issue": "",
                "pages": "1715--1725",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/P16-1162"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural Machine Translation of Rare Words with Subword Units. In Proceedings of the 54th An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715- 1725, Berlin, Germany. Association for Computa- tional Linguistics.",
                "links": null
            },
            "BIBREF34": {
                "ref_id": "b34",
                "title": "Canonical Word Order of Japanese Ditransitive Sentences: A Preliminary Investigation through a Grammaticality Judgment Survey",
                "authors": [
                    {
                        "first": "Yasumasa",
                        "middle": [],
                        "last": "Shigenaga",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Advances in Language and Literary Studies",
                "volume": "5",
                "issue": "2",
                "pages": "35--45",
                "other_ids": {
                    "DOI": [
                        "10.7575/aiac.alls.v.5n.2p.35"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yasumasa Shigenaga. 2014. Canonical Word Order of Japanese Ditransitive Sentences: A Preliminary In- vestigation through a Grammaticality Judgment Sur- vey. Advances in Language and Literary Studies, 5(2):35-45.",
                "links": null
            },
            "BIBREF35": {
                "ref_id": "b35",
                "title": "Children use canonical sentence schemas: A crosslinguistic study of word order and inflections",
                "authors": [
                    {
                        "first": "I",
                        "middle": [],
                        "last": "Dan",
                        "suffix": ""
                    },
                    {
                        "first": "Thomas",
                        "middle": [
                            "G"
                        ],
                        "last": "Slobin",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Bever",
                        "suffix": ""
                    }
                ],
                "year": 1982,
                "venue": "An introduction to Japanese linguistics",
                "volume": "12",
                "issue": "",
                "pages": "229--265",
                "other_ids": {
                    "DOI": [
                        "10.1016/0010-0277(82)90033-6"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Dan I Slobin and Thomas G Bever. 1982. Children use canonical sentence schemas: A crosslinguistic study of word order and inflections. Cognition, 12(3):229- 265. Natsuko Tsujimura. 2013. An introduction to Japanese linguistics. John Wiley & Sons.",
                "links": null
            },
            "BIBREF36": {
                "ref_id": "b36",
                "title": "Attention is All you Need",
                "authors": [
                    {
                        "first": "Ashish",
                        "middle": [],
                        "last": "Vaswani",
                        "suffix": ""
                    },
                    {
                        "first": "Noam",
                        "middle": [],
                        "last": "Shazeer",
                        "suffix": ""
                    },
                    {
                        "first": "Niki",
                        "middle": [],
                        "last": "Parmar",
                        "suffix": ""
                    },
                    {
                        "first": "Jakob",
                        "middle": [],
                        "last": "Uszkoreit",
                        "suffix": ""
                    },
                    {
                        "first": "Llion",
                        "middle": [],
                        "last": "Jones",
                        "suffix": ""
                    },
                    {
                        "first": "Aidan",
                        "middle": [
                            "N"
                        ],
                        "last": "Gomez",
                        "suffix": ""
                    },
                    {
                        "first": "\u0141 Ukasz",
                        "middle": [],
                        "last": "Kaiser",
                        "suffix": ""
                    },
                    {
                        "first": "Illia",
                        "middle": [],
                        "last": "Polosukhin",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "30",
                "issue": "",
                "pages": "5998--6008",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar- nett, editors, Advances in Neural Information Pro- cessing Systems 30, pages 5998-6008. Curran Asso- ciates, Inc.",
                "links": null
            },
            "BIBREF37": {
                "ref_id": "b37",
                "title": "A Word Reordering Model for Improved Machine Translation",
                "authors": [
                    {
                        "first": "Karthik",
                        "middle": [],
                        "last": "Visweswariah",
                        "suffix": ""
                    },
                    {
                        "first": "Rajakrishnan",
                        "middle": [],
                        "last": "Rajkumar",
                        "suffix": ""
                    },
                    {
                        "first": "Ankur",
                        "middle": [],
                        "last": "Gandhe",
                        "suffix": ""
                    },
                    {
                        "first": "Ananthakrishnan",
                        "middle": [],
                        "last": "Ramanathan",
                        "suffix": ""
                    },
                    {
                        "first": "Jiri",
                        "middle": [],
                        "last": "Navratil",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "486--496",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Karthik Visweswariah, Rajakrishnan Rajkumar, Ankur Gandhe, Ananthakrishnan Ramanathan, and Jiri Navratil. 2011. A Word Reordering Model for Im- proved Machine Translation. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 486-496, Edinburgh, Scotland, UK. Association for Computational Lin- guistics.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF1": {
                "num": null,
                "text": "Figure1: LM-based method for evaluating the canonicality of each word order considering their generation probabilities.",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "FIGREF3": {
                "num": null,
                "text": "Figure 2: Overview of the experiment of comparing human and LMs word order preference. First, we created data for the task of comparing the appropriateness of the word order (left part), then we compare the preference of LMs and humans through this task (right part).",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "FIGREF4": {
                "num": null,
                "text": "5) DAT-ACC: \u751f\u5f92\u306b student-DAT \u672c\u3092 book-ACC \u3042\u3052\u305f gave. ACC-DAT: \u672c\u3092 book-ACC ::::: \u751f\u5f92\u306b student-DAT \u3042\u3052\u305f gave. Henceforth, DAT-ACC / ACC-DAT denotes the word order in which the DAT / ACC argument precedes the ACC / DAT argument. We evaluate the rate (our results) ACC-DAT rate (S&O 2016) (a) Each verb's ACC-DAT rate. between each verb's R v DAT-only and the ACC-DAT rate.",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF5": {
                "num": null,
                "text": "Figure 3: Overlap of the results of Sasano and Okumura (2016) and that of LMs. In figures (a) and (b), each plot corresponds to each verb. In figure (c), each plot corresponds to each example. The legend of figure (a) and (b) is the same as in figure (c). \"S&O 2016\" refers to Sasano and Okumura (2016).",
                "uris": null,
                "fig_num": "3",
                "type_str": "figure"
            },
            "FIGREF6": {
                "num": null,
                "text": "methods.Word order and argument omissionSasano and Okumura (2016) claimed that the frequently omitted case is placed near the verb. First, we calculated R v DAT-only for each verb v as follows: DAT-only / N v ACC-only denotes the number of examples in which the DAT / ACC case appears, and the other case does not in S v . A large R v DAT-only score indicates that the DAT argument is less frequently omitted than the ACC argument in S v . We analyzed the relationship between R v DAT-only and R v ACC-DAT for each verb. Figure 3-(b) shows that the regression lines from the LM-based method and Sasano and Okumura (2016) corroborate similar trends. The Pearson correlation coefficient between R v",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF9": {
                "num": null,
                "text": "book-TOP teacher-NOM book-ACC gave. b. \u5148\u751f\u304c\u306f [\u5148\u751f\u304c \u672c\u3092 \u3042\u3052\u305f.] teacher-TOP teacher-NOM book-ACC gave.",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF10": {
                "num": null,
                "text": "color-DAT compared. (\u03c6I compared a person to color.) b. \u5e97\u4e3b\u306b \u656c\u610f\u3092 \u8868\u3057\u305f. shopkeeper-DAT respect-ACC expressed.(\u03c6I expressed a respect to a shopkeeper.)",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF11": {
                "num": null,
                "text": "Causative: \u751f\u5f92\u306b student-DAT \u672c\u3092 book-ACC \u898b\u305b\u305f showed. (\u03c6I showed a student a book.) Inchoative: \u751f\u5f92\u304c student-NOM \u898b\u305f saw. (A student saw \u03c6something.) passed to \u03c6something.)",
                "uris": null,
                "fig_num": "10",
                "type_str": "figure"
            },
            "FIGREF12": {
                "num": null,
                "text": "Figure 4: Correlation between the ACC-DAT rate and the rate that the ACC argument is more likely to be topicalized than DAT for each verb. Each plot corresponds to the result of each verb.",
                "uris": null,
                "fig_num": "4",
                "type_str": "figure"
            },
            "TABREF1": {
                "content": "<table><tr><td/><td colspan=\"4\">TIM&lt;LOC TIM&lt;NOM LOC&lt;NOM</td></tr><tr><td>CLM</td><td>.757</td><td/><td>.642</td><td>.604</td></tr><tr><td>SLM</td><td>.708</td><td/><td>.632</td><td>.615</td></tr><tr><td>Count</td><td>.686</td><td/><td>.666</td><td>.681</td></tr><tr><td colspan=\"5\">(2016) evaluated this claim by analyzing the trend</td></tr><tr><td colspan=\"5\">in the following two types of examples:</td></tr><tr><td colspan=\"2\">(6) Type-A: \u672c\u3092 book-ACC</td><td colspan=\"2\">\u5b66\u6821\u306b ::::: school-DAT</td><td>\u8fd4\u3057\u305f returned.</td></tr><tr><td colspan=\"3\">Type-B: ::::: \u5148\u751f\u306b teacher-DAT</td><td>\u672c\u3092 book-ACC</td><td>\u8fd4\u3057\u305f returned.</td></tr></table>",
                "type_str": "table",
                "text": "The columns a < b show the score o(a < b), which indicates the rate of case a being more likely to be placed before b. The row \"Count\" shows the countbased results in the dataset we used.",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table><tr><td>ing on its type.\" They focus on four types of ad-</td></tr><tr><td>verbs: MODAL, TIME, MANNER, and RESULTIVE.</td></tr></table>",
                "type_str": "table",
                "text": "Changes in the position of a constituent with the largest number of chunks.",
                "html": null,
                "num": null
            },
            "TABREF4": {
                "content": "<table><tr><td colspan=\"2\">Fairseq model</td><td colspan=\"2\">architecture adaptive softmax cut off</td><td colspan=\"2\">transformer lm 50,000, 140,000</td></tr><tr><td/><td/><td>algorithm</td><td/><td colspan=\"3\">Nesterov accelerated gradient (nag)</td></tr><tr><td/><td/><td>learning rates</td><td/><td/><td>1e-5</td></tr><tr><td>Optimizer</td><td/><td>momentum</td><td/><td/><td>0.99</td></tr><tr><td/><td/><td>weight decay</td><td/><td/><td>0</td></tr><tr><td/><td/><td>clip norm</td><td/><td/><td>0.1</td></tr><tr><td/><td/><td>type</td><td/><td/><td>cosine</td></tr><tr><td/><td/><td colspan=\"2\">warmup updates</td><td/><td>16,000</td></tr><tr><td/><td/><td colspan=\"2\">warmup init lrarning rate</td><td/><td>1e-7</td></tr><tr><td/><td/><td colspan=\"2\">max learning rate</td><td/><td>0.1</td></tr><tr><td colspan=\"2\">Learning rate scheduler</td><td colspan=\"2\">min learning rate</td><td/><td>1e-9</td></tr><tr><td/><td/><td colspan=\"3\">t mult (factor to grow the length of each period)</td><td>2</td></tr><tr><td/><td/><td colspan=\"3\">learning rate period updates</td><td>270,000</td></tr><tr><td/><td/><td colspan=\"2\">learning rate shrink</td><td/><td>0.75</td></tr><tr><td>Training</td><td/><td>batch size epochs</td><td/><td/><td>4608 tokens 3</td></tr><tr><td/><td/><td colspan=\"2\">ACC-DAT is preferred</td><td colspan=\"2\">DAT-ACC is preferred</td></tr><tr><td colspan=\"2\">Model Verb</td><td/><td>R v ACC-DAT</td><td>S&amp;O Verb</td><td>R v ACC-DAT</td><td>S&amp;O</td></tr><tr><td/><td colspan=\"2\">\"\u4f8b\u3048\u308b\" (compare)</td><td>0.993</td><td>0.945 \"\u8868\u3059\u308b\" (to table)</td><td>0.001</td><td>0.013</td></tr><tr><td/><td colspan=\"2\">\"\u63db\u7b97\u3059\u308b\" (converted)</td><td>0.992</td><td>0.935 \"\u6f84\u307e\u3059\" (put on airs)</td><td>0.000</td><td>0.017</td></tr><tr><td>CLM</td><td colspan=\"2\">\"\u62bc\u3057\u51fa\u3059\" (extruded)</td><td>0.979</td><td>0.923 \"\u716e\u3084\u3059\" (cook inside)</td><td>0.000</td><td>0.019</td></tr><tr><td/><td colspan=\"2\">\"\u898b\u7acb\u3066\u308b\" (mitateru)</td><td>0.994</td><td>0.919 \"\u7791\u308b\" (close the eyes)</td><td>0.001</td><td>0.021</td></tr><tr><td/><td colspan=\"2\">\"\u5909\u63db\" (conversion)</td><td>0.975</td><td>0.898 \"\u7ae6\u3081\u308b\" (shrug)</td><td>0.002</td><td>0.022</td></tr><tr><td/><td colspan=\"2\">\"\u4f8b\u3048\u308b\" (compare)</td><td>0.993</td><td>0.926 \"\u55ab\u3059\u308b\" (kissuru)</td><td>0.003</td><td>0.018</td></tr><tr><td/><td colspan=\"2\">\"\u62bc\u3057\u51fa\u3059\" (extruded)</td><td>0.979</td><td>0.914 \"\u8868\u3059\u308b\" (to table)</td><td>0.001</td><td>0.018</td></tr><tr><td>SLM</td><td colspan=\"2\">\"\u76e3\u7981\" (confinement)</td><td>0.885</td><td>0.912 \"\u6f84\u307e\u3059\" (put on airs)</td><td>0.000</td><td>0.021</td></tr><tr><td/><td colspan=\"2\">\"\u5f79\u7acb\u3066\u308b\" (help)</td><td>0.933</td><td>0.904 \"\u629c\u304b\u3059\" (leave out)</td><td>0.002</td><td>0.022</td></tr><tr><td/><td colspan=\"2\">\"\u5e30\u3059\" (attributable)</td><td>0.838</td><td>0.903 \"\u8e0f\u307f\u5165\u308c\u308b\" (step into)</td><td>0.002</td><td>0.025</td></tr></table>",
                "type_str": "table",
                "text": "Hyperparameters of the LMs.",
                "html": null,
                "num": null
            },
            "TABREF5": {
                "content": "<table/>",
                "type_str": "table",
                "text": "The verbs with the top five and the worst five R v ACC-DAT in each LM. The \"S&O\" columns show the ACC-DAT rate reported in",
                "html": null,
                "num": null
            },
            "TABREF7": {
                "content": "<table><tr><td colspan=\"2\">Case #occurrence</td></tr><tr><td>TIM</td><td>11,780</td></tr><tr><td>LOC</td><td>15,544</td></tr><tr><td>NOM</td><td>55,230</td></tr><tr><td>DAT</td><td>56,243</td></tr><tr><td>ACC</td><td>57,823</td></tr></table>",
                "type_str": "table",
                "text": "The verbs which show a significant change in the argument order trend depending on the semantic role of its dative argument. The scores denote the DAT-ACC rate. Type-A corresponds to the examples with an inanimate goal dative argument. Type-B corresponds to the examples with an animate processor dative argument. The number of type-A verbs is significantly larger than that of type-B verbs.",
                "html": null,
                "num": null
            },
            "TABREF8": {
                "content": "<table><tr><td>Model</td><td colspan=\"2\">MODAL Canonical r</td><td>TIME Canonical</td><td>r</td><td>MANNER Canonical</td><td>r</td><td>RESULTIVE Canonical</td><td>r</td></tr><tr><td>CLM</td><td>ASOV</td><td colspan=\"2\">1. ASOV, SAOV</td><td>1.</td><td colspan=\"3\">SAOV, SOAV 0.5 SAOV, SOAV</td><td>1.</td></tr><tr><td>SLM</td><td>ASOV</td><td>1.</td><td>SAOV</td><td colspan=\"2\">0.5 SAOV, SOAV</td><td>1.</td><td>SOAV</td><td>0.5</td></tr><tr><td>Koizumi(2016)</td><td>ASOV</td><td>-</td><td>ASOV, SAOV</td><td>-</td><td>SAOV, SOAV</td><td>-</td><td>SAOV, SOAV</td><td>-</td></tr></table>",
                "type_str": "table",
                "text": "Overlap of the preference of the adverb position of LMs and that of",
                "html": null,
                "num": null
            }
        }
    }
}