{
    "paper_id": "P10-1079",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:07:48.208637Z"
    },
    "title": "A hybrid rule/model-based finite-state framework for normalizing SMS messages",
    "authors": [
        {
            "first": "Richard",
            "middle": [],
            "last": "Beaufort",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Universit\u00e9 catholique de Louvain",
                "location": {
                    "postCode": "1348",
                    "settlement": "Louvain-la-Neuve",
                    "country": "Belgium"
                }
            },
            "email": "richard.beaufort@uclouvain.be"
        },
        {
            "first": "Sophie",
            "middle": [],
            "last": "Roekhaut",
            "suffix": "",
            "affiliation": {},
            "email": "sophie.roekhaut@umons.ac.be"
        },
        {
            "first": "Louise-Am\u00e9lie",
            "middle": [],
            "last": "Cougnon",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Universit\u00e9 catholique de Louvain",
                "location": {
                    "postCode": "1348",
                    "settlement": "Louvain-la-Neuve",
                    "country": "Belgium"
                }
            },
            "email": "louise-amelie.cougnon@uclouvain.be"
        },
        {
            "first": "C\u00e9drick",
            "middle": [],
            "last": "Fairon",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Universit\u00e9 catholique de Louvain",
                "location": {
                    "postCode": "1348",
                    "settlement": "Louvain-la-Neuve",
                    "country": "Belgium"
                }
            },
            "email": "cedrick.fairon@uclouvain.be"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "In recent years, research in natural language processing has increasingly focused on normalizing SMS messages. Different well-defined approaches have been proposed, but the problem remains far from being solved: best systems achieve a 11% Word Error Rate. This paper presents a method that shares similarities with both spell checking and machine translation approaches. The normalization part of the system is entirely based on models trained from a corpus. Evaluated in French by 10-fold-cross validation, the system achieves a 9.3% Word Error Rate and a 0.83 BLEU score.",
    "pdf_parse": {
        "paper_id": "P10-1079",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "In recent years, research in natural language processing has increasingly focused on normalizing SMS messages. Different well-defined approaches have been proposed, but the problem remains far from being solved: best systems achieve a 11% Word Error Rate. This paper presents a method that shares similarities with both spell checking and machine translation approaches. The normalization part of the system is entirely based on models trained from a corpus. Evaluated in French by 10-fold-cross validation, the system achieves a 9.3% Word Error Rate and a 0.83 BLEU score.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Introduced a few years ago, Short Message Service (SMS) offers the possibility of exchanging written messages between mobile phones. SMS has quickly been adopted by users. These messages often greatly deviate from traditional spelling conventions. As shown by specialists (Thurlow and Brown, 2003; Fairon et al., 2006; Bieswanger, 2007) , this variability is due to the simultaneous use of numerous coding strategies, like phonetic plays (2m1 read 'demain', \"tomorrow\"), phonetic transcriptions (kom instead of 'comme', \"like\"), consonant skeletons (tjrs for 'toujours', \"always\"), misapplied, missing or incorrect separators (j esper for 'j'esp\u00e8re', \"I hope\"; j'croibi1k, instead of 'je crois bien que', \"I am pretty sure that\"), etc. These deviations are due to three main factors: the small number of characters allowed per text message by the service (140 bytes), the constraints of the small phones' keypads and, last but not least, the fact that people mostly communicate between friends and relatives in an informal register.",
                "cite_spans": [
                    {
                        "start": 272,
                        "end": 297,
                        "text": "(Thurlow and Brown, 2003;",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 298,
                        "end": 318,
                        "text": "Fairon et al., 2006;",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 319,
                        "end": 336,
                        "text": "Bieswanger, 2007)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Whatever their causes, these deviations considerably hamper any standard natural language processing (NLP) system, which stumbles against so many Out-Of-Vocabulary words. For this reason, as noted by Sproat et al. (2001) , an SMS normalization must be performed before a more conventional NLP process can be applied. As defined by Yvon (2008) , \"SMS normalization consists in rewriting an SMS text using a more conventional spelling, in order to make it more readable for a human or for a machine.\"",
                "cite_spans": [
                    {
                        "start": 200,
                        "end": 220,
                        "text": "Sproat et al. (2001)",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 331,
                        "end": 342,
                        "text": "Yvon (2008)",
                        "ref_id": "BIBREF27"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The SMS normalization we present here was developed in the general framework of an SMSto-speech synthesis system1 . This paper, however, only focuses on the normalization process.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Evaluated in French, our method shares similarities with both spell checking and machine translation. The machine translation-like module of the system performs the true normalization task. It is entirely based on models learned from an SMS corpus and its transcription, aligned at the character-level in order to get parallel corpora.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Two spell checking-like modules surround the normalization module. The first one detects unambiguous tokens, like URLs or phone numbers, to keep them out of the normalization. The second one, applied on the normalized parts only, identifies non-alphabetic sequences, like punctuations, and labels them with the corresponding token. This greatly helps the system's print module to follow the basic rules of typography.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "This paper is organized as follows. Section 2 proposes an overview of the state of the art. Section 3 presents the general architecture of our system, while Section 4 focuses on how we learn and combine our normalization models. Section 5 evaluates the system and compares it to previous works. Section 6 draws conclusions and considers some future possible improvements of the method.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "As highlighted by Kobus et al. (2008b) , SMS normalization, up to now, has been handled through three well-known NLP metaphors: spell checking, machine translation and automatic speech recognition. In this section, we only present the pros and cons of these approaches. Their results are given in Section 5, focused on our evaluation.",
                "cite_spans": [
                    {
                        "start": 18,
                        "end": 38,
                        "text": "Kobus et al. (2008b)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related work",
                "sec_num": "2"
            },
            {
                "text": "The spell checking metaphor (Guimier de Neef et al., 2007; Choudhury et al., 2007; Cook and Stevenson, 2009) performs the normalization task on a word-per-word basis. On the assumption that most words should be correct for the purpose of communication, its principle is to keep In-Vocabulary words out of the correction process. Guimier de Neef et al. (2007) proposed a rulebased system that uses only a few linguistic resources dedicated to SMS, like specific lexicons of abbreviations. Choudhury et al. (2007) and Cook and Stevenson (2009) preferred to implement the noisy channel metaphor (Shannon, 1948) , which assumes a communication process in which a sender emits the intended message W through an imperfect (noisy) communication channel, such that the sequence O observed by the recipient is a noisy version of the original message. On this basis, the idea is to recover the intended message W hidden behind the sequences of observations O, by maximizing:",
                "cite_spans": [
                    {
                        "start": 40,
                        "end": 58,
                        "text": "Neef et al., 2007;",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 59,
                        "end": 82,
                        "text": "Choudhury et al., 2007;",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 83,
                        "end": 108,
                        "text": "Cook and Stevenson, 2009)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 340,
                        "end": 358,
                        "text": "Neef et al. (2007)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 488,
                        "end": 511,
                        "text": "Choudhury et al. (2007)",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 516,
                        "end": 541,
                        "text": "Cook and Stevenson (2009)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 592,
                        "end": 607,
                        "text": "(Shannon, 1948)",
                        "ref_id": "BIBREF23"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related work",
                "sec_num": "2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "W max = arg max P (W |O)",
                        "eq_num": "(1)"
                    }
                ],
                "section": "Related work",
                "sec_num": "2"
            },
            {
                "text": "= arg max P (O|W ) P (W ) P (O)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related work",
                "sec_num": "2"
            },
            {
                "text": "where P (O) is ignored because constant, P (O|W ) models the channel's noise, and P (W ) models the language of the source. Choudhury et al. (2007) implemented the noisy channel through a Hidden-Markov Model (HMM) able to handle both graphemic variants and phonetic plays as proposed by (Toutanova and Moore, 2002) , while Cook and Stevenson (2009) enhanced the model by adapting the channel's noise P (O|W, wf) according to a list of predefined observed word formations {wf}: stylistic variation, word clipping, phonetic abbreviations, etc. Whatever the system, the main limitation of the spell checking approach is the excessive confidence it places in word boundaries.",
                "cite_spans": [
                    {
                        "start": 124,
                        "end": 147,
                        "text": "Choudhury et al. (2007)",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 287,
                        "end": 314,
                        "text": "(Toutanova and Moore, 2002)",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 323,
                        "end": 348,
                        "text": "Cook and Stevenson (2009)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related work",
                "sec_num": "2"
            },
            {
                "text": "The machine translation metaphor, which is historically the first proposed (Bangalore et al., 2002; Aw et al., 2006) , considers the process of normalizing SMS as a translation task from a source language (the SMS) to a target language (its standard written form). This standpoint is based on the observation that, on the one side, SMS messages greatly differ from their standard written forms, and that, on the other side, most of the errors cross word boundaries and require a wide context to be handled. On this basis, Aw et al. (2006) proposed a statistical machine translation model working at the phrase-level, by splitting sentences into their k most probable phrases. While this approach achieves really good results, Kobus et al. (2008b) make the assertion that a phrase-based translation can hardly capture the lexical creativity observed in SMS messages. Moreover, the translation framework, which can handle many-to-many correspondences between sources and targets, exceeds the needs of SMS normalization, where the normalization task is almost deterministic.",
                "cite_spans": [
                    {
                        "start": 75,
                        "end": 99,
                        "text": "(Bangalore et al., 2002;",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 100,
                        "end": 116,
                        "text": "Aw et al., 2006)",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 522,
                        "end": 538,
                        "text": "Aw et al. (2006)",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 726,
                        "end": 746,
                        "text": "Kobus et al. (2008b)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related work",
                "sec_num": "2"
            },
            {
                "text": "Based on this analysis, Kobus et al. (2008b) proposed to handle SMS normalization through an automatic speech recognition (ASR) metaphor. The starting point of this approach is the observation that SMS messages present a lot of phonetic plays that sometimes make the SMS word (sr\u00e9, mwa) closer to its phonetic representation ([sKe], [mwa]) than to its standard written form (serai, \"will be\", moi, \"me\"). Typically, an ASR system tries to discover the best word sequence within a lattice of weighted phonetic sequences.",
                "cite_spans": [
                    {
                        "start": 24,
                        "end": 44,
                        "text": "Kobus et al. (2008b)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related work",
                "sec_num": "2"
            },
            {
                "text": "Applied to the SMS normalization task, the ASR metaphor consists in first converting the SMS message into a phone lattice, before turning it into a word-based lattice using a phoneme-to-grapheme dictionary. A language model is then applied on the word lattice, and the most probable word sequence is finally chosen by applying a best-path algorithm on the lattice. One of the advantages of the grapheme-to-phoneme conversion is its intrinsic ability to handle word boundaries. However, this step also presents an important drawback, raised by the authors themselves: it prevents next normalization steps from knowing what graphemes were in the initial sequence.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related work",
                "sec_num": "2"
            },
            {
                "text": "Our approach, which is detailed in Sections 3 and 4, shares similarities with both the spell checking approach and the machine translation principles, trying to combine the advantages of these methods, while leaving aside their drawbacks: like in spell checking systems, we detect unambiguous units of text as soon as possible and try to rely on word boundaries when they seem reliable enough; but like in the machine translation task, our method intrinsically handles word boundaries in the normalization process if needed.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related work",
                "sec_num": "2"
            },
            {
                "text": "3 Overview of the system 3.1 Tools in use",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related work",
                "sec_num": "2"
            },
            {
                "text": "In our system, all lexicons, language models and sets of rules are compiled into finite-state machines (FSMs) and combined with the input text by composition (\u2022). The reader who is not familiar with FSMs and their fundamental theoretical properties, like composition, is urged to consult the state-of-the-art literature (Roche and Schabes, 1997; Mohri and Riley, 1997; Mohri et al., 2000; Mohri et al., 2001) .",
                "cite_spans": [
                    {
                        "start": 346,
                        "end": 368,
                        "text": "Mohri and Riley, 1997;",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 369,
                        "end": 388,
                        "text": "Mohri et al., 2000;",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 389,
                        "end": 408,
                        "text": "Mohri et al., 2001)",
                        "ref_id": "BIBREF20"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related work",
                "sec_num": "2"
            },
            {
                "text": "We used our own finite-state tools: a finite-state machine library and its associated compiler (Beaufort, 2008) . In conformance with the format of the library, the compiler builds finite-state machines from weighted rewrite rules, weighted regular expressions and n-gram models.",
                "cite_spans": [
                    {
                        "start": 95,
                        "end": 111,
                        "text": "(Beaufort, 2008)",
                        "ref_id": "BIBREF2"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related work",
                "sec_num": "2"
            },
            {
                "text": "We formulated four constraints before fixing the system's architecture. First, special tokens, like URLs, phones or currencies, should be identified as soon as possible, to keep them out of the normalization process.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Aims",
                "sec_num": "3.2"
            },
            {
                "text": "Second, word boundaries should be taken into account, as far as they seem reliable enough. The idea, here, is to base the decision on a learning able to catch frequent SMS sequences to include in a dedicated In-Vocabulary (IV) lexicon.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Aims",
                "sec_num": "3.2"
            },
            {
                "text": "Third, any other SMS sequence should be considered as Out-Of-Vocabulary (OOV), on which in-depth rewritings may be applied.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Aims",
                "sec_num": "3.2"
            },
            {
                "text": "Fourth, the basic rules of typography and typesettings should be applied on the normalized version of the SMS message.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Aims",
                "sec_num": "3.2"
            },
            {
                "text": "The architecture depicted in Figure 1 directly relies on these considerations. In short, an SMS message first goes through three SMS modules, which normalize its noisy parts. Then, two standard NLP modules produce a morphosyntactic analysis of the normalized text. A last module, finally, takes advantage of this linguistic analysis either to print a text that follows the basic rules of typography, or to synthesize the corresponding speech signal.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 36,
                        "end": 37,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Architecture",
                "sec_num": "3.3"
            },
            {
                "text": "Because this paper focuses on the normalization task, the rest of this section only presents the SMS modules and the \"smart print\" output. The morphosyntactic analysis, made of state-of-the-art algorithms, is described in (Beaufort, 2008) , and the text-to-speech synthesis system we use is presented in (Colotte and Beaufort, 2005) . unambiguous tokens: URLs, phone numbers, dates, times, currencies, units of measurement and, last but not least in the context of SMS, smileys 2 . These tokens are kept out of the normalization process, while any other sequence of characters is considered -and labelled -as noisy.",
                "cite_spans": [
                    {
                        "start": 222,
                        "end": 238,
                        "text": "(Beaufort, 2008)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 317,
                        "end": 332,
                        "text": "Beaufort, 2005)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Architecture",
                "sec_num": "3.3"
            },
            {
                "text": "SMS normalization. This module only uses models learned from a training corpus (cf. Section 4). It involves three steps. First, an SMSdedicated lexicon look-up, which differentiates between known and unknown parts of a noisy token. Second, a rewrite process, which creates a lattice of weighted solutions. The rewrite model differs depending on whether the part to rewrite is known or not. Third, a combination of the lattice of solutions with a language model, and the choice of the best sequence of lexical units. At this stage, the normalization as such is completed.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Architecture",
                "sec_num": "3.3"
            },
            {
                "text": "SMS postprocessing. Like the preprocessor, the postprocessor relies on a set of manuallytuned rewrite rules. The module is only applied on the normalized version of the noisy tokens, with the intention to identify any non-alphabetic sequence and to isolate it in a distinct token. At this stage, for instance, a point becomes a 'strong punctuation'. Apart from the list of tokens already managed by the preprocessor, the postprocessor handles as well numeric and alphanumeric strings, fields of data (like bank account numbers), punctuations and symbols.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Architecture",
                "sec_num": "3.3"
            },
            {
                "text": "The smart print module, based on manually-tuned rules, checks either the kind of token (chosen by the SMS pre-/post-processing modules) or the grammatical category (chosen by the morphosyntactic analysis) to make the right typography choices, such as the insertion of a space after certain tokens (URLs, phone numbers), the insertion of two spaces after a strong punctuation (point, question mark, exclamation mark), the insertion of two carriage returns at the end of a paragraph, or the upper case of the initial letter at the beginning of the sentence.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Smart print",
                "sec_num": "3.3.2"
            },
            {
                "text": "2 Our list contains about 680 smileys.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Smart print",
                "sec_num": "3.3.2"
            },
            {
                "text": "4 The normalization models",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Smart print",
                "sec_num": "3.3.2"
            },
            {
                "text": "Our approach is an approximation of the noisy channel metaphor (cf. Section 2). It differs from this general framework, because we adapt the model of the channel's noise depending on whether the noisy token (our sequence of observations) is In-Vocabulary or Out-Of-Vocabulary:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overview of the normalization algorithm",
                "sec_num": "4.1"
            },
            {
                "text": "P (O|W ) = \uf8f1 \uf8f2 \uf8f3 P IV (O|W ) if O \u2208 IV P OOV (O|W ) else (2)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overview of the normalization algorithm",
                "sec_num": "4.1"
            },
            {
                "text": "Indeed, our algorithm is based on the assumption that applying different normalization models to IV and OOV words should both improve the results and reduce the processing time.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overview of the normalization algorithm",
                "sec_num": "4.1"
            },
            {
                "text": "For this purpose, the first step of the algorithm consists in composing a noisy token T with an FST Sp whose task is to differentiate between sequences of IV words and sequences of OOV words, by labelling them with a special IV or OOV marker. The token is then split in n segments sg i according to these markers:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overview of the normalization algorithm",
                "sec_num": "4.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "{sg} = Split(T \u2022 Sp)",
                        "eq_num": "(3)"
                    }
                ],
                "section": "Overview of the normalization algorithm",
                "sec_num": "4.1"
            },
            {
                "text": "In a second step, each segment is composed with a rewrite model according to its kind: the IV rewrite model R IV for sequences of IV words, and the OOV rewrite model R OOV for sequences of OOV words:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overview of the normalization algorithm",
                "sec_num": "4.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "sg i = \uf8f1 \uf8f2 \uf8f3 sg i \u2022 R IV if sg i \u2208 IV sg i \u2022 R OOV else",
                        "eq_num": "(4)"
                    }
                ],
                "section": "Overview of the normalization algorithm",
                "sec_num": "4.1"
            },
            {
                "text": "All rewritten segments are then concatenated together in order to get back the complete token:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overview of the normalization algorithm",
                "sec_num": "4.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "T = n i=1 (sg i )",
                        "eq_num": "(5)"
                    }
                ],
                "section": "Overview of the normalization algorithm",
                "sec_num": "4.1"
            },
            {
                "text": "where is the concatenation operator.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overview of the normalization algorithm",
                "sec_num": "4.1"
            },
            {
                "text": "The third and last normalization step is applied on a complete sentence S. All tokens T j of S are concatenated together and composed with the lexical language model LM . The result of this composition is a word lattice, of which we take the most probable word sequence S by applying a best-path algorithm:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overview of the normalization algorithm",
                "sec_num": "4.1"
            },
            {
                "text": "S = BestPath( ( m j=1 T j ) \u2022 LM ) (6)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overview of the normalization algorithm",
                "sec_num": "4.1"
            },
            {
                "text": "where m is the number of tokens of S. In S , each noisy token T j of S is mapped onto its most probable normalization.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overview of the normalization algorithm",
                "sec_num": "4.1"
            },
            {
                "text": "Our normalization models were trained on a French SMS corpus of 30,000 messages, gathered in Belgium, semi-automatically anonymized and manually normalized by the Catholic University of Louvain (Fairon and Paumier, 2006) . Together, the SMS corpus and its transcription constitute parallel corpora aligned at the message-level. However, in order to learn pieces of knowledge from these corpora, we needed a string alignment at the character-level.",
                "cite_spans": [
                    {
                        "start": 194,
                        "end": 220,
                        "text": "(Fairon and Paumier, 2006)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The corpus alignment",
                "sec_num": "4.2"
            },
            {
                "text": "One way of implementing this string alignment is to compute the edit-distance of two strings, which measures the minimum number of operations (substitutions, insertions, deletions) required to transform one string into the other (Levenshtein, 1966) .",
                "cite_spans": [
                    {
                        "start": 229,
                        "end": 248,
                        "text": "(Levenshtein, 1966)",
                        "ref_id": "BIBREF15"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The corpus alignment",
                "sec_num": "4.2"
            },
            {
                "text": "Using this algorithm, in which each operation gets a cost of 1, two strings may be aligned in different ways with the same global cost. This is the case, for instance, for the SMS form kozer ([koze]) and its standard transcription caus\u00e9 (\"talked\"), as illustrated by Figure 2 . However, from a linguistic standpoint, alignment (1) is preferable, because corresponding graphemes are aligned on their first character.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 274,
                        "end": 275,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "The corpus alignment",
                "sec_num": "4.2"
            },
            {
                "text": "In order to automatically choose this preferred alignment, we had to distinguish the three editoperations, according to the characters to be aligned. For that purpose, probabilities were required.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The corpus alignment",
                "sec_num": "4.2"
            },
            {
                "text": "Computing probabilities for each operation according to the characters to be aligned was performed through an iterative algorithm described in (Cougnon and Beaufort, 2009) . In short, this algorithm gradually learns the best way of aligning strings. On our parallel corpora, it converged after 7 iterations and provided us with a result from which the learning could start.",
                "cite_spans": [
                    {
                        "start": 143,
                        "end": 171,
                        "text": "(Cougnon and Beaufort, 2009)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The corpus alignment",
                "sec_num": "4.2"
            },
            {
                "text": "(1) ko_ser",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The corpus alignment",
                "sec_num": "4.2"
            },
            {
                "text": "(2) k_oser caus\u00e9_ caus\u00e9_",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The corpus alignment",
                "sec_num": "4.2"
            },
            {
                "text": "(3) ko_ser (4) k_oser caus_\u00e9 caus_\u00e9",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The corpus alignment",
                "sec_num": "4.2"
            },
            {
                "text": "Figure 2 : Different equidistant alignments, using a standard edit-cost of 1. Underscores ('_') mean insertion in the upper string, and deletion in the lower string.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "The corpus alignment",
                "sec_num": "4.2"
            },
            {
                "text": "In natural language processing, a word is commonly defined as \"a sequence of alphabetic characters between separators\", and an IV word is simply a word that belongs to the lexicon in use.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The split model Sp",
                "sec_num": "4.3"
            },
            {
                "text": "In SMS messages however, separators are surely indicative, but not reliable. For this reason, our definition of the word is far from the previous one, and originates from the string alignment. After examining our parallel corpora aligned at the character-level, we decided to consider as a word \"the longest sequence of characters parsed without meeting the same separator on both sides of the alignment\". For instance, the following alignment J esper_ k___tu va_ J'esp\u00e8re que tu vas (I hope that you will) is split as follows according to our definition: J esper_ k___tu va_ J'esp\u00e8re que tu vas since the separator in \"J esper\" is different from its transcription, and \"ktu\" does not contain any separator. Thus, this SMS sequence corresponds to 3 SMS words: [J esper], [ktu] and [va] .",
                "cite_spans": [
                    {
                        "start": 781,
                        "end": 785,
                        "text": "[va]",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The split model Sp",
                "sec_num": "4.3"
            },
            {
                "text": "A first parsing of our parallel corpora provided us with a list of SMS sequences corresponding to our IV lexicon. The FST Sp is built on this basis:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The split model Sp",
                "sec_num": "4.3"
            },
            {
                "text": "Sp = ( S * (I|O) ( S + (I|O) ) * S * ) \u2022 G (7)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The split model Sp",
                "sec_num": "4.3"
            },
            {
                "text": "where:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The split model Sp",
                "sec_num": "4.3"
            },
            {
                "text": "\u2022 I is an FST corresponding to the lexicon, in which IV words are mapped onto the IV marker.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The split model Sp",
                "sec_num": "4.3"
            },
            {
                "text": "\u2022 O is the complement of I3 . In this OOV lexicon, OOV sequences are mapped onto the OOV marker.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The split model Sp",
                "sec_num": "4.3"
            },
            {
                "text": "\u2022 S is an FST corresponding to the list of separators (any non-alphabetic and nonnumeric character), mapped onto a SEP marker.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The split model Sp",
                "sec_num": "4.3"
            },
            {
                "text": "\u2022 G is an FST able to detect consecutive sequences of IV (resp. OOV) words, and to group them under a unique IV (resp. OOV) marker. By gathering sequences of IVs and OOVs, SEP markers disappear from Sp.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The split model Sp",
                "sec_num": "4.3"
            },
            {
                "text": "Figure 3 illustrates the composition of Sp with the SMS sequence J esper kcv b1 (J'esp\u00e8re que \u00e7a va bien, \"I hope you are well\"). For the example, we make the assumption that kcv was never seen during the training. ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "3",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "The split model Sp",
                "sec_num": "4.3"
            },
            {
                "text": "This model is built during a second parsing of our parallel corpora. In short, the parsing simply gathers all possible normalizations for each SMS sequence put, by the first parsing, in the IV lexicon. Contrary to the first parsing, this second one processes the corpus without taking separators into account, in order to make sure that all possible normalizations are collected.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The IV rewrite model R IV",
                "sec_num": "4.4"
            },
            {
                "text": "Each normalization w for a given SMS sequence w is weighted as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The IV rewrite model R IV",
                "sec_num": "4.4"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "p( w|w) = Occ( w, w) Occ(w)",
                        "eq_num": "(8)"
                    }
                ],
                "section": "The IV rewrite model R IV",
                "sec_num": "4.4"
            },
            {
                "text": "where Occ(x) is the number of occurrences of x in the corpus. The FST R IV is then built as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The IV rewrite model R IV",
                "sec_num": "4.4"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "R IV = S IV * IV R ( S IV + IV R ) * S IV *",
                        "eq_num": "(9)"
                    }
                ],
                "section": "The IV rewrite model R IV",
                "sec_num": "4.4"
            },
            {
                "text": "where:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The IV rewrite model R IV",
                "sec_num": "4.4"
            },
            {
                "text": "\u2022 IV R is a weighted lexicon compiled into an FST, in which each IV sequence is mapped onto the list of its possible normalizations.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The IV rewrite model R IV",
                "sec_num": "4.4"
            },
            {
                "text": "\u2022 S IV is a weighted lexicon of separators, in which each separator is mapped onto the list of its possible normalizations. The deletion is often one of the possible normalization of a separator. Otherwise, the deletion is added and is weighted by the following smoothed probability:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The IV rewrite model R IV",
                "sec_num": "4.4"
            },
            {
                "text": "p(DEL|w) = 0.1 Occ(w) + 0.1 (10)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The IV rewrite model R IV",
                "sec_num": "4.4"
            },
            {
                "text": "4.5 The OOV rewrite model R OOV",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The IV rewrite model R IV",
                "sec_num": "4.4"
            },
            {
                "text": "In contrast to the other models, this one is not a regular expression made of weighted lexicons. It corresponds to a set of weighted rewrite rules (Chomsky and Halle, 1968; Johnson, 1972; Mohri and Sproat, 1996) learned from the alignment. Developed in the framework of generative phonology, rules take the form 11) which means that the replacement \u03c6 \u2192 \u03c8 is only performed when \u03c6 is surrounded by \u03bb on the left and \u03c1 on the right, and gets the weight w.",
                "cite_spans": [
                    {
                        "start": 147,
                        "end": 172,
                        "text": "(Chomsky and Halle, 1968;",
                        "ref_id": null
                    },
                    {
                        "start": 173,
                        "end": 187,
                        "text": "Johnson, 1972;",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 188,
                        "end": 211,
                        "text": "Mohri and Sproat, 1996)",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 312,
                        "end": 315,
                        "text": "11)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The IV rewrite model R IV",
                "sec_num": "4.4"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "\u03c6 \u2192 \u03c8 : \u03bb _ \u03c1 / w",
                        "eq_num": "("
                    }
                ],
                "section": "The IV rewrite model R IV",
                "sec_num": "4.4"
            },
            {
                "text": "However, in our case, rules take the simpler form",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The IV rewrite model R IV",
                "sec_num": "4.4"
            },
            {
                "text": "\u03c6 \u2192 \u03c8 / w (12)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The IV rewrite model R IV",
                "sec_num": "4.4"
            },
            {
                "text": "which means that the replacement \u03c6 \u2192 \u03c8 is always performed, whatever the context. Inputs of our rules (\u03c6) are sequences of 1 to 5 characters taken from the SMS side of the alignment, while outputs (\u03c8) are their corresponding normalizations. Our rules are sorted in the reverse order of the length of their inputs: rules with longer inputs come first in the list.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The IV rewrite model R IV",
                "sec_num": "4.4"
            },
            {
                "text": "Long-to-short rule ordering reduces the number of proposed normalizations for a given SMS sequence for two reasons:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The IV rewrite model R IV",
                "sec_num": "4.4"
            },
            {
                "text": "1. the firing of a rule with a longer input blocks the firing of any shorter sub-rule. This is due to a constraint expressed on lists of rewrite rules: a given rule may be applied only if no more specific and relevant rule has been met higher in the list;",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The IV rewrite model R IV",
                "sec_num": "4.4"
            },
            {
                "text": "2. a rule with a longer input usually has fewer alternative normalizations than a rule with a shorter input does, because the longer SMS sequence likely occurred paired with fewer alternative normalizations in the training corpus than did the shorter SMS sequence.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The IV rewrite model R IV",
                "sec_num": "4.4"
            },
            {
                "text": "Among the wide set of possible sequences of 2 to 5 characters gathered from the corpus, we only kept in our list of rules the sequences that allowed at least one normalization solely made of IV words. It is important to notice that here, we refer to the standard notion of IV word: while gathering the candidate sequences from the corpus, we systematically checked each word of the normalizations against a lexicon of French standard written forms. The lexicon we used contains about 430,000 inflected forms and is derived from Morlex 4 , a French lexical database.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The IV rewrite model R IV",
                "sec_num": "4.4"
            },
            {
                "text": "Figure 4 illustrates these principles by focusing on 3 input sequences: aussi, au and a. As shown by the Figure, all rules of a set dedicated to the same input sequence (for instance, aussi) are optional (?\u2192), except the last one, which is obligatory (\u2192). In our finite-state compiler, this convention allows the application of all concurrent normalizations on the same input sequence, as depicted in Figure 5 .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "4",
                        "ref_id": null
                    },
                    {
                        "start": 408,
                        "end": 409,
                        "text": "5",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "The IV rewrite model R IV",
                "sec_num": "4.4"
            },
            {
                "text": "In our real list of OOV rules, the input sequence a corresponds to 231 normalizations, while au accepts 43 normalizations and aussi, only 3. This highlights the interest, in terms of efficiency, of the long-to-short rule ordering.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The IV rewrite model R IV",
                "sec_num": "4.4"
            },
            {
                "text": "Our language model is an n-gram of lexical forms, smoothed by linear interpolation (Chen and Goodman, 1998) , estimated on the normalized part of our training corpus and compiled into a weighted FST LM w .",
                "cite_spans": [
                    {
                        "start": 83,
                        "end": 107,
                        "text": "(Chen and Goodman, 1998)",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The language model",
                "sec_num": "4.6"
            },
            {
                "text": "At this point, this FST cannot be combined with our other models, because it works on lexical units and not on characters. This problem is solved by composing LM w with another FST L, which represents a lexicon mapping each input word, considered as a string of characters, onto the same output words, but considered here as a lexical unit. Lexical units are then permanently removed from the language model by keeping only the first projection (the input side) of the composition:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The language model",
                "sec_num": "4.6"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "LM = FirstProjection( L \u2022 LM w )",
                        "eq_num": "(13)"
                    }
                ],
                "section": "The language model",
                "sec_num": "4.6"
            },
            {
                "text": "In this model, special characters, like punctuations or symbols, are represented by their categories (light, medium and strong punctuations, question mark, symbol, etc.), while special tokens, like URLs or phone numbers, are handled as token values (URL, phone, etc.) instead of as sequences of characters. This reduces the complexity of the model.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The language model",
                "sec_num": "4.6"
            },
            {
                "text": "As we explained earlier, tokens of a same sentence S are concatenated together at the end of the second normalization step. During this concatenation process, sequences corresponding to special tokens are automatically replaced by their token values. Special characters, however, 4 See http://bach.arts.kuleuven.be/pmertens/.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The language model",
                "sec_num": "4.6"
            },
            {
                "text": "\"aussi\" ?-> \"au si\" / 8.4113 ( * ) \"aussi\" ?-> \"ou si\" / 6.6743 ( * ) \"aussi\" -> \"aussi\" / 0.0189 ( * ) ... ... \"au\" ?-> \"ow\" / 14.1787 ... \"au\" ?-> \"\u00f4t\" / 12.5938 \"au\" ?-> \"du\" / 12.1787 ( * ) \"au\" ?-> \"o\" / 11.8568 ... \"au\" ?-> \"on\" / 10.8568 ( * ) ... \"au\" ?-> \"aud\" / 9.9308 \"au\" ?-> \"aux\" / 6.1731 ( * ) \"au\" -> \"au\" / 0.0611 ( * ) ... ... \"a\" ?-> \"a d\" / 17.8624 \"a\" ?-> \"ation\" / 17.8624 \"a\" ?-> \"\u00e2ts\" / 17.8624 ... \"a\" ?-> \"ablement\" / 16.8624 \"a\" ?-> \"anisation\" / 16.8624 ... \"a\" ?-> \"u\" / 15.5404 \"a\" ?-> \"y a\" / 15.5404 ... \"a\" ?-> \"abilit\u00e9\" / 13.4029 \"a\" ?-> \"\u00e0-\" / 12.1899 \"a\" ?-> \"ar\" / 11.5225 \"a\" ?-> \\DEL / 9.1175 \"a\" ?-> \"\u00e7a\" / 6.2019 \"a\" ?-> \"\u00e0\" / 3.5013 \"a\" -> \"a\" / 0.3012 are still present in S. For this reason, S is first composed with an FST Reduce, which maps each special character onto its corresponding category:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The language model",
                "sec_num": "4.6"
            },
            {
                "text": "S \u2022 Reduce \u2022 LM (14)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The language model",
                "sec_num": "4.6"
            },
            {
                "text": "The performance and the efficiency of our system were evaluated on a MacBook Pro with a 2.4 GHz Intel Core 2 Duo CPU, 4 GB 667 MHz DDR2 SDRAM, running Mac OS X version 10.5.8. The evaluation was performed on the corpus of 30,000 French SMS presented in Section 4.2, by ten-fold cross-validation (Kohavi, 1995) . The principle of this method of evaluation is to split the initial corpus into 10 subsets of equal size. The system is then trained 10 times, each time leaving out one of the subsets from the training corpus, but using only this omitted subset as test corpus.",
                "cite_spans": [
                    {
                        "start": 295,
                        "end": 309,
                        "text": "(Kohavi, 1995)",
                        "ref_id": "BIBREF14"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation",
                "sec_num": "5"
            },
            {
                "text": "The language model of the evaluation is a 3-gram. We did not try a 4-gram. This choice was motivated by the experiments of Kobus et al. (2008a) , who showed on a French corpus comparable to ours that, if using a larger language model is always rewarded, the improvement quickly decreases with every higher level and is already quite small between 2-gram and 3-gram.",
                "cite_spans": [
                    {
                        "start": 123,
                        "end": 143,
                        "text": "Kobus et al. (2008a)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation",
                "sec_num": "5"
            },
            {
                "text": "Table 1 presents the results in terms of efficiency. The system seems efficient, while we cannot compare it with other methods, which did not provide us with this information.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "1",
                        "ref_id": "TABREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Evaluation",
                "sec_num": "5"
            },
            {
                "text": "Table 2 , part 1, presents the performance of our approach (Hybrid) and compares it to a trivial copy-paste (Copy). The system was evaluated in terms of BLEU score (Papineni et al., 2001) , Word Error Rate (WER) and Sentence Error Rate (SER). Concerning WER, the table presents the distribution between substitutions (Sub), deletions (Del) and insertions (Ins). The copy-paste results just inform about the real deviation of our corpus from the traditional spelling conventions, and highlight the fact that our system is still at pains to significantly reduce the SER, while results in terms of WER and BLEU score are quite encouraging.",
                "cite_spans": [
                    {
                        "start": 164,
                        "end": 187,
                        "text": "(Papineni et al., 2001)",
                        "ref_id": "BIBREF21"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "2",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Evaluation",
                "sec_num": "5"
            },
            {
                "text": "Table 2 , part 2, provides the results of the state-of-the-art approaches. The only results truly comparable to ours are those of Guimier de Neef et al. (2007) , who evaluated their approach on the same corpus as ours 5 ; clearly, our method 5 They performed an evaluation without ten-fold cross- The analysis of the normalizations produced by our system pointed out that, most often, errors are contextual and concern the gender (quel(le), \"what\"), the number (bisou(s), \"kiss\"), the person ([tu t']inqui\u00e8te(s), \"you are worried\") or the tense (arriv\u00e9/arriver, \"arrived\"/\"to arrive\"). That contextual errors are frequent is not surprising. In French, as mentioned by Kobus et al. (2008b) , ngram models are unable to catch this information, as it is generally out of their scope.",
                "cite_spans": [
                    {
                        "start": 141,
                        "end": 159,
                        "text": "Neef et al. (2007)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 668,
                        "end": 688,
                        "text": "Kobus et al. (2008b)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "2",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Evaluation",
                "sec_num": "5"
            },
            {
                "text": "On the other hand, this analysis confirmed our initial assumptions. First, special tokens (URLs, phones, etc.) are not modified. Second, agglutinated words are generally split (Pensa ms \u2192 Pense \u00e0 mes, \"think to my\"), while misapplied separators tend to be deleted (G t \u2192 J'\u00e9tais, \"I was\"). Of course, we also found some errors at word boundaries ([il] l'arrange \u2192 [il] la range, \"[he] arranges\" \u2192 \"[he] pits in order\"), but they were fairly rare.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation",
                "sec_num": "5"
            },
            {
                "text": "In this paper, we presented an SMS normalization framework based on finite-state machines and developed in the context of an SMS-to-speech synthesis system. With the intention to avoid wrong modifications of special tokens and to handle word boundaries as easily as possible, we designed a method that shares similarities with both spell checking and machine translation. Our Evaluated by ten-fold cross-validation, the system seems efficient, and the performance in terms of BLEU score and WER are quite encouraging. However, the SER remains too high, which emphasizes the fact that the system needs several improvements.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion and perspectives",
                "sec_num": "6"
            },
            {
                "text": "First of all, the model should take phonetic similarities into account, because SMS messages contain a lot of phonetic plays. The phonetic model, for instance, should know that o, au, eau, . . . , aux can all be pronounced [o], while \u00e8, ais, ait, . . . , aient are often pronounced [E]. However, unlike Kobus et al. (2008a) , we feel that this model must avoid the normalization step in which the graphemic sequence is converted into phonemes, because this conversion prevents the next steps from knowing which graphemes were in the initial sequence. Instead, we propose to learn phonetic similarities from a dictionary of words with phonemic transcriptions, and to build graphemes-to-graphemes rules. These rules could then be automatically weighted, by learning their frequencies from our aligned corpora. Furthermore, this model should be able to allow for timbre variation, like [e]-[E], in order to allow similarities between graphemes frequently confused in French, like ai ([e]) and ais/ait/aient ([E]). Last but not least, the graphemes-tographemes rules should be contextualized, in order to reduce the complexity of the model. It would also be interesting to test the impact of another lexical language model, learned on non-SMS sentences. Indeed, the lexical model must be learned from sequences of standard written forms, an obvious prerequisite that involves a major drawback when the corpus is made of SMS sentences: the corpus must first be transcribed, an expensive process that reduces the amount of data on which the model will be trained. For this reason, we propose to learn a lexical model from non-SMS sentences. However, the corpus of external sentences should still share two important features with the SMS language: it should mimic the oral language and be as spontaneous as possible. With this in mind, our intention is to gather sentences from Internet forums. But not just any forum, because often forums share another feature with the SMS language: their language is noisy. Thus, the idea is to choose a forum asking its members to pay attention to spelling mistakes and grammatical errors, and to avoid the use of the SMS language.",
                "cite_spans": [
                    {
                        "start": 303,
                        "end": 323,
                        "text": "Kobus et al. (2008a)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion and perspectives",
                "sec_num": "6"
            },
            {
                "text": "The Vocalise project. See cental.fltr.ucl.ac.be/team/projects/vocalise/.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Actually, the true complement of I accepts sequences with separators, while these sequences were removed from O.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "This research was funded by grants no. 716619 and 616422 from the Walloon Region of Belgium, and supported by the Multitel research centre.We sincerely thank our anonymous reviewers for their insightful and helpful comments on the first version of this paper.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgments",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "A phrase-based statistical model for SMS text normalization",
                "authors": [
                    {
                        "first": "Aiti",
                        "middle": [],
                        "last": "Aw",
                        "suffix": ""
                    },
                    {
                        "first": "Min",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Juan",
                        "middle": [],
                        "last": "Xiao",
                        "suffix": ""
                    },
                    {
                        "first": "Jian",
                        "middle": [],
                        "last": "Su",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Proc. COLING/ACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A phrase-based statistical model for SMS text normalization. In Proc. COLING/ACL 2006.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Bootstrapping bilingual data using consensus translation for a multilingual instant messaging system",
                "authors": [
                    {
                        "first": "Srinivas",
                        "middle": [],
                        "last": "Bangalore",
                        "suffix": ""
                    },
                    {
                        "first": "Vanessa",
                        "middle": [],
                        "last": "Murdock",
                        "suffix": ""
                    },
                    {
                        "first": "Giuseppe",
                        "middle": [],
                        "last": "Riccardi",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Proc. the 19th international conference on Computational linguistics",
                "volume": "",
                "issue": "",
                "pages": "1--7",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Srinivas Bangalore, Vanessa Murdock, and Giuseppe Riccardi. 2002. Bootstrapping bilingual data using consensus translation for a multilingual instant messaging system. In Proc. the 19th international conference on Computational linguistics, pages 1- 7, Morristown, NJ, USA.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Application des machines \u00e0 etats finis en synth\u00e8se de la parole. S\u00e9lection d'unit\u00e9s non uniformes et correction orthographique",
                "authors": [
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Beaufort",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "FUNDP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Richard Beaufort. 2008. Application des machines \u00e0 etats finis en synth\u00e8se de la parole. S\u00e9lection d'unit\u00e9s non uniformes et correction orthographique. Ph.D. thesis, FUNDP, Namur, Belgium, March. 605 pages.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "abbrevi8 or not 2 abbrevi8: A contrastive analysis of different space and timesaving strategies in English and German text messages",
                "authors": [
                    {
                        "first": "Markus",
                        "middle": [],
                        "last": "Bieswanger",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Texas Linguistics Forum",
                "volume": "50",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Markus Bieswanger. 2007. abbrevi8 or not 2 abbrevi8: A contrastive analysis of different space and time- saving strategies in English and German text messages. In Texas Linguistics Forum, volume 50.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "An empirical study of smoothing techniques for language modeling",
                "authors": [
                    {
                        "first": "Stanley",
                        "middle": [
                            "F"
                        ],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Joshua",
                        "middle": [],
                        "last": "Goodman",
                        "suffix": ""
                    }
                ],
                "year": 1968,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Stanley F. Chen and Joshua Goodman. 1998. An empirical study of smoothing techniques for language modeling. Technical Report 10-98, Computer Science Group, Harvard University. Noam Chomsky and Morris Halle. 1968. The sound pattern of English. Harper and Row, New York, NY.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Linguistic features weighting for a text-to-speech system without prosody model",
                "authors": [
                    {
                        "first": "Monojit",
                        "middle": [],
                        "last": "Choudhury",
                        "suffix": ""
                    },
                    {
                        "first": "Rahul",
                        "middle": [],
                        "last": "Saraf",
                        "suffix": ""
                    },
                    {
                        "first": "Vijit",
                        "middle": [],
                        "last": "Jain",
                        "suffix": ""
                    },
                    {
                        "first": "Animesh",
                        "middle": [],
                        "last": "Mukherjee",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proc",
                "volume": "10",
                "issue": "",
                "pages": "157--174",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh Mukherjee, Sudeshna Sarkar1, and Anupam Basu. 2007. Investigation and modeling of the structure of texting language. International Journal on Document Analysis and Recognition, 10(3):157- 174. Vincent Colotte and Richard Beaufort. 2005. Linguistic features weighting for a text-to-speech system without prosody model. In Proc.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Interspeech'05",
                "authors": [],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "2549--2552",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Interspeech'05, pages 2549-2552.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "An unsupervised model for text message normalization",
                "authors": [
                    {
                        "first": "Paul",
                        "middle": [],
                        "last": "Cook",
                        "suffix": ""
                    },
                    {
                        "first": "Suzanne",
                        "middle": [],
                        "last": "Stevenson",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proc. Workshop on Computational Approaches to Linguistic Creativity",
                "volume": "",
                "issue": "",
                "pages": "71--78",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Paul Cook and Suzanne Stevenson. 2009. An unsupervised model for text message normalization. In Proc. Workshop on Computational Approaches to Linguistic Creativity, pages 71-78.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "SSLD: a French SMS to standard language dictionary",
                "authors": [
                    {
                        "first": "Louise-Am\u00e9lie",
                        "middle": [],
                        "last": "Cougnon",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Beaufort",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proc. eLexicography in the 21st century: New applications, new challenges",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Louise-Am\u00e9lie Cougnon and Richard Beaufort. 2009. SSLD: a French SMS to standard language dictionary. In Sylviane Granger and Magali Paquot, editors, Proc. eLexicography in the 21st century: New applications, new challenges (eLEX 2009). Presses Universitaires de Louvain. To appear.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "A translated corpus of 30,000 French SMS",
                "authors": [
                    {
                        "first": "C\u00e9drick",
                        "middle": [],
                        "last": "Fairon",
                        "suffix": ""
                    },
                    {
                        "first": "S\u00e9bastien",
                        "middle": [],
                        "last": "Paumier",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Proc. LREC",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "C\u00e9drick Fairon and S\u00e9bastien Paumier. 2006. A translated corpus of 30,000 French SMS. In Proc. LREC 2006, May.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Le langage SMS: \u00e9tude d'un corpus informatis\u00e9 \u00e0 partir de l'enqu\u00eate Faites don de vos SMS \u00e0 la science. Presses Universitaires de Louvain",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "C\u00e9crick",
                        "suffix": ""
                    },
                    {
                        "first": "Jean",
                        "middle": [
                            "R"
                        ],
                        "last": "Fairon",
                        "suffix": ""
                    },
                    {
                        "first": "S\u00e9bastien",
                        "middle": [],
                        "last": "Klein",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Paumier",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "",
                "volume": "136",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "C\u00e9crick. Fairon, Jean R. Klein, and S\u00e9bastien Paumier. 2006. Le langage SMS: \u00e9tude d'un corpus informatis\u00e9 \u00e0 partir de l'enqu\u00eate Faites don de vos SMS \u00e0 la science. Presses Universitaires de Louvain. 136 pages.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "TILT correcteur de SMS: \u00e9valuation et bilan quantitatif",
                "authors": [
                    {
                        "first": "Emilie",
                        "middle": [],
                        "last": "Guimier De Neef",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Actes de TALN 2007",
                "volume": "",
                "issue": "",
                "pages": "123--132",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Emilie Guimier de Neef, Arnaud Debeurme, and Jungyeul Park. 2007. TILT correcteur de SMS: \u00e9valuation et bilan quantitatif. In Actes de TALN 2007, pages 123-132, Toulouse, France.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Formal aspects of phonological description. Mouton, The Hague. Catherine Kobus, Fran\u00e7ois Yvon, and G\u00e9raldine Damnati. 2008a. Normalizing SMS: are two metaphors better than one?",
                "authors": [
                    {
                        "first": "C",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Douglas",
                        "middle": [],
                        "last": "Johnson",
                        "suffix": ""
                    }
                ],
                "year": 1972,
                "venue": "Proc. COLING 2008",
                "volume": "",
                "issue": "",
                "pages": "441--448",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "C. Douglas Johnson. 1972. Formal aspects of phonological description. Mouton, The Hague. Catherine Kobus, Fran\u00e7ois Yvon, and G\u00e9raldine Damnati. 2008a. Normalizing SMS: are two metaphors better than one? In Proc. COLING 2008, pages 441-448, Manchester, UK.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Transcrire les SMS comme on reconna\u00eet la parole",
                "authors": [
                    {
                        "first": "Catherine",
                        "middle": [],
                        "last": "Kobus",
                        "suffix": ""
                    },
                    {
                        "first": "Fran\u00e7ois",
                        "middle": [],
                        "last": "Yvon",
                        "suffix": ""
                    },
                    {
                        "first": "G\u00e9raldine",
                        "middle": [],
                        "last": "Damnati",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Actes de la Conf\u00e9rence sur le Traitement Automatique des Langues (TALN'08)",
                "volume": "",
                "issue": "",
                "pages": "128--138",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Catherine Kobus, Fran\u00e7ois Yvon, and G\u00e9raldine Damnati. 2008b. Transcrire les SMS comme on reconna\u00eet la parole. In Actes de la Conf\u00e9rence sur le Traitement Automatique des Langues (TALN'08), pages 128-138, Avignon, France.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "A study of cross-validation and bootstrap for accuracy estimation and model selection",
                "authors": [
                    {
                        "first": "Ron",
                        "middle": [],
                        "last": "Kohavi",
                        "suffix": ""
                    }
                ],
                "year": 1995,
                "venue": "Proc. IJCAI'95",
                "volume": "",
                "issue": "",
                "pages": "1137--1143",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ron Kohavi. 1995. A study of cross-validation and bootstrap for accuracy estimation and model selection. In Proc. IJCAI'95, pages 1137-1143.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Binary codes capable of correcting deletions, insertions and reversals",
                "authors": [
                    {
                        "first": "Vladimir",
                        "middle": [],
                        "last": "Levenshtein",
                        "suffix": ""
                    }
                ],
                "year": 1966,
                "venue": "Soviet Physics",
                "volume": "10",
                "issue": "",
                "pages": "707--710",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Vladimir Levenshtein. 1966. Binary codes capable of correcting deletions, insertions and reversals. Soviet Physics, 10:707-710.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Weighted determinization and minimization for large vocabulary speech recognition",
                "authors": [
                    {
                        "first": "Mehryar",
                        "middle": [],
                        "last": "Mohri",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Riley",
                        "suffix": ""
                    }
                ],
                "year": 1997,
                "venue": "Proc",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mehryar Mohri and Michael Riley. 1997. Weighted determinization and minimization for large vocabulary speech recognition. In Proc.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Eurospeech'97",
                "authors": [],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "131--134",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Eurospeech'97, pages 131-134.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "An efficient compiler for weighted rewrite rules",
                "authors": [
                    {
                        "first": "Mehryar",
                        "middle": [],
                        "last": "Mohri",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Sproat",
                        "suffix": ""
                    }
                ],
                "year": 1996,
                "venue": "Proc. ACL'96",
                "volume": "",
                "issue": "",
                "pages": "231--238",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mehryar Mohri and Richard Sproat. 1996. An efficient compiler for weighted rewrite rules. In Proc. ACL'96, pages 231-238.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "The design principles of a weighted finitestate transducer library",
                "authors": [
                    {
                        "first": "Mehryar",
                        "middle": [],
                        "last": "Mohri",
                        "suffix": ""
                    },
                    {
                        "first": "Fernando",
                        "middle": [],
                        "last": "Pereira",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Riley",
                        "suffix": ""
                    }
                ],
                "year": 2000,
                "venue": "Theoretical Computer Science",
                "volume": "231",
                "issue": "1",
                "pages": "17--32",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mehryar Mohri, Fernando Pereira, and Michael Riley. 2000. The design principles of a weighted finite- state transducer library. Theoretical Computer Science, 231(1):17-32.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Generic -removal algorithm for weighted automata",
                "authors": [
                    {
                        "first": "Mehryar",
                        "middle": [],
                        "last": "Mohri",
                        "suffix": ""
                    },
                    {
                        "first": "Fernando",
                        "middle": [],
                        "last": "Pereira",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Riley",
                        "suffix": ""
                    }
                ],
                "year": 2001,
                "venue": "Lecture Notes in Computer Science",
                "volume": "2088",
                "issue": "",
                "pages": "230--242",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mehryar Mohri, Fernando Pereira, and Michael Riley. 2001. Generic -removal algorithm for weighted automata. Lecture Notes in Computer Science, 2088:230-242.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "BLEU: a method for automatic evaluation of machine translation",
                "authors": [
                    {
                        "first": "Kishore",
                        "middle": [],
                        "last": "Papineni",
                        "suffix": ""
                    },
                    {
                        "first": "Salim",
                        "middle": [],
                        "last": "Roukos",
                        "suffix": ""
                    },
                    {
                        "first": "Todd",
                        "middle": [],
                        "last": "Ward",
                        "suffix": ""
                    },
                    {
                        "first": "Wei-Jing",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    }
                ],
                "year": 2001,
                "venue": "Proc. ACL",
                "volume": "",
                "issue": "",
                "pages": "311--318",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2001. BLEU: a method for automatic evaluation of machine translation. In Proc. ACL 2001, pages 311-318.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Finite-state language processing",
                "authors": [],
                "year": 1997,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Emmanuel Roche and Yves Schabes, editors. 1997. Finite-state language processing. MIT Press, Cambridge.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "A mathematical theory of communication",
                "authors": [
                    {
                        "first": "Claude",
                        "middle": [
                            "E"
                        ],
                        "last": "Shannon",
                        "suffix": ""
                    }
                ],
                "year": 1948,
                "venue": "The Bell System Technical Journal",
                "volume": "27",
                "issue": "",
                "pages": "379--423",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Claude E. Shannon. 1948. A mathematical theory of communication. The Bell System Technical Journal, 27:379-423.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Normalization of non-standard words",
                "authors": [
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Sproat",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [
                            "W"
                        ],
                        "last": "Black",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Kumar",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Ostendorf",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Richards",
                        "suffix": ""
                    }
                ],
                "year": 2001,
                "venue": "",
                "volume": "15",
                "issue": "",
                "pages": "287--333",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Richard Sproat, A.W. Black, S. Chen, S. Kumar, M. Ostendorf, and C. Richards. 2001. Normalization of non-standard words. Computer Speech & Language, 15(3):287-333.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Generation txt? The sociolinguistics of young people's textmessaging",
                "authors": [
                    {
                        "first": "Crispin",
                        "middle": [],
                        "last": "Thurlow",
                        "suffix": ""
                    },
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Brown",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Discourse Analysis Online",
                "volume": "",
                "issue": "1",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Crispin Thurlow and Alex Brown. 2003. Generation txt? The sociolinguistics of young people's text- messaging. Discourse Analysis Online, 1(1).",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Pronunciation modeling for improved spelling correction",
                "authors": [
                    {
                        "first": "Kristina",
                        "middle": [],
                        "last": "Toutanova",
                        "suffix": ""
                    },
                    {
                        "first": "Robert",
                        "middle": [
                            "C"
                        ],
                        "last": "Moore",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Proc. ACL'02",
                "volume": "",
                "issue": "",
                "pages": "144--151",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kristina Toutanova and Robert C. Moore. 2002. Pronunciation modeling for improved spelling correction. In Proc. ACL'02, pages 144-151.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Reorthography of SMS messages",
                "authors": [
                    {
                        "first": "Fran\u00e7ois",
                        "middle": [],
                        "last": "Yvon",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Fran\u00e7ois Yvon. 2008. Reorthography of SMS messages. Technical Report 2008, LIMSI/CNRS, Orsay, France.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 1: Architecture of the system",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Figure 3: Application of the split model Sp. The OOV sequence starts and ends with separators.",
                "uris": null,
                "fig_num": "3",
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "Figure 4: Samples from the list of OOV rules. Rules' weights are negative logarithms of probabilities: smaller weights are thus better. Asterisks indicate normalizations solely made of French IV words.",
                "uris": null,
                "fig_num": "45",
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table><tr><td/><td>mean</td><td>dev.</td></tr><tr><td colspan=\"3\">bps 1836.57 159.63</td></tr><tr><td>ms/SMS (140b)</td><td>76.23</td><td>22.34</td></tr><tr><td colspan=\"3\">outperforms theirs. Our results also seem a bit</td></tr><tr><td colspan=\"3\">better than those of Kobus et al. (2008a), although</td></tr><tr><td colspan=\"3\">the comparison with this system, also evaluated in</td></tr><tr><td colspan=\"3\">French, is less easy: they combined the French</td></tr><tr><td colspan=\"3\">corpus we used with another one and performed</td></tr><tr><td colspan=\"3\">a single validation, using a bigger training corpus</td></tr><tr><td colspan=\"3\">(36.704 messages) for a test corpus quite similar</td></tr><tr><td colspan=\"3\">to one of our subsets (2.998 SMS). Other systems</td></tr><tr><td colspan=\"3\">were evaluated in English, and results are more</td></tr><tr><td colspan=\"3\">difficult to compare; at least, our results seem in</td></tr><tr><td>line with them.</td><td/><td/></tr></table>",
                "type_str": "table",
                "text": "Efficiency of the system.",
                "html": null,
                "num": null
            },
            "TABREF1": {
                "content": "<table><tr><td>validation, because their rule-based system did not need any</td></tr><tr><td>training.</td></tr></table>",
                "type_str": "table",
                "text": "Performance of the system. ( * ) Kobus 2008-1 corresponds to the ASR-like system, while Kobus 2008-2 is a combination of this system with a series of open-source machine translation toolkits. ( * * ) Scores obtained on noisy data only, out of the sentence's context. normalization algorithm is original in two ways. First, it is entirely based on models learned from a training corpus. Second, the rewrite model applied to a noisy sequence differs depending on whether this sequence is known or not.",
                "html": null,
                "num": null
            }
        }
    }
}