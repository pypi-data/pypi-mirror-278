{
    "paper_id": "2020",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T11:55:36.133321Z"
    },
    "title": "Aligned Dual Channel Graph Convolutional Network for Visual Question Answering",
    "authors": [
        {
            "first": "Qingbao",
            "middle": [],
            "last": "Huang",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "South China University of Technology",
                "location": {
                    "settlement": "Guangzhou",
                    "country": "China"
                }
            },
            "email": "qbhuang@gxu.edu.cn"
        },
        {
            "first": "Jielong",
            "middle": [],
            "last": "Wei",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Guangxi University",
                "location": {
                    "settlement": "Nanning, Guangxi",
                    "country": "China"
                }
            },
            "email": ""
        },
        {
            "first": "Yi",
            "middle": [],
            "last": "Cai",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "South China University of Technology",
                "location": {
                    "settlement": "Guangzhou",
                    "country": "China"
                }
            },
            "email": "ycai@scut.edu.cn"
        },
        {
            "first": "Changmeng",
            "middle": [],
            "last": "Zheng",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "South China University of Technology",
                "location": {
                    "settlement": "Guangzhou",
                    "country": "China"
                }
            },
            "email": ""
        },
        {
            "first": "Junying",
            "middle": [],
            "last": "Chen",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "South China University of Technology",
                "location": {
                    "settlement": "Guangzhou",
                    "country": "China"
                }
            },
            "email": ""
        },
        {
            "first": "Ho-Fung",
            "middle": [],
            "last": "Leung",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "The Chinese University of Hong Kong",
                "location": {
                    "settlement": "Hong Kong SAR",
                    "country": "China"
                }
            },
            "email": ""
        },
        {
            "first": "Qing",
            "middle": [],
            "last": "Li",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "The Hong Kong Polytechnic University",
                "location": {
                    "addrLine": "Hung Hom",
                    "settlement": "Kowloon, Hong Kong",
                    "country": "China"
                }
            },
            "email": ""
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Visual question answering aims to answer the natural language question about a given image. Existing graph-based methods only focus on the relations between objects in an image and neglect the importance of the syntactic dependency relations between words in a question. To simultaneously capture the relations between objects in an image and the syntactic dependency relations between words in a question, we propose a novel dual channel graph convolutional network (DC-GCN) for better combining visual and textual advantages. The DC-GCN model consists of three parts: an I-GCN module to capture the relations between objects in an image, a Q-GCN module to capture the syntactic dependency relations between words in a question, and an attention alignment module to align image representations and question representations. Experimental results show that our model achieves comparable performance with the state-of-theart approaches.",
    "pdf_parse": {
        "paper_id": "2020",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Visual question answering aims to answer the natural language question about a given image. Existing graph-based methods only focus on the relations between objects in an image and neglect the importance of the syntactic dependency relations between words in a question. To simultaneously capture the relations between objects in an image and the syntactic dependency relations between words in a question, we propose a novel dual channel graph convolutional network (DC-GCN) for better combining visual and textual advantages. The DC-GCN model consists of three parts: an I-GCN module to capture the relations between objects in an image, a Q-GCN module to capture the syntactic dependency relations between words in a question, and an attention alignment module to align image representations and question representations. Experimental results show that our model achieves comparable performance with the state-of-theart approaches.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "As a form of visual Turing test, visual question answering (VQA) has drawn much attention. The goal of VQA (Antol et al., 2015; Goyal et al., 2017 ) is to answer a natural language question related to the contents of a given image. Attention mechanisms are served as the backbone of the previous mainstream approaches (Lu et al., 2016; Yang et al., 2016; Yu et al., 2017) , however, they tend to catch only the most discriminative information, ignoring other rich complementary clues (Liu et al., 2019) .",
                "cite_spans": [
                    {
                        "start": 107,
                        "end": 127,
                        "text": "(Antol et al., 2015;",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 128,
                        "end": 146,
                        "text": "Goyal et al., 2017",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 318,
                        "end": 335,
                        "text": "(Lu et al., 2016;",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 336,
                        "end": 354,
                        "text": "Yang et al., 2016;",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 355,
                        "end": 371,
                        "text": "Yu et al., 2017)",
                        "ref_id": "BIBREF34"
                    },
                    {
                        "start": 484,
                        "end": 502,
                        "text": "(Liu et al., 2019)",
                        "ref_id": "BIBREF18"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Recent VQA studies have been exploring higher level semantic representation of images, notably using graph-based structures for better image understanding, such as scene graph generation (Xu et al., 2017; Yang et al., 2018) , visual relationship detection (Yao et al., 2018) , object counting (Zhang et al. , 2018a), and relation reasoning (Cao et al., 2018; Li et al., 2019; Cadene et al., 2019a) . Representing images as graphs allows one to explicitly model interactions between two objects in an image, so as to seamlessly transfer information between graph nodes (e.g., objects in an image).",
                "cite_spans": [
                    {
                        "start": 187,
                        "end": 204,
                        "text": "(Xu et al., 2017;",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 205,
                        "end": 223,
                        "text": "Yang et al., 2018)",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 256,
                        "end": 274,
                        "text": "(Yao et al., 2018)",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 293,
                        "end": 306,
                        "text": "(Zhang et al.",
                        "ref_id": null
                    },
                    {
                        "start": 340,
                        "end": 358,
                        "text": "(Cao et al., 2018;",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 359,
                        "end": 375,
                        "text": "Li et al., 2019;",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 376,
                        "end": 397,
                        "text": "Cadene et al., 2019a)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Very recent research methods (Li et al., 2019; Cadene et al., 2019a; Yu et al., 2019) have achieved remarkable performances, but there is still a big gap between them and human. As shown in Figure 1 (a), given an image of a group of persons and the corresponding question, a VQA system needs to not only recognize the objects in an image (e.g., batter, umpire and catcher), but also grasp the textual information in the question \"what color is the umpire's shirt\". However, even many competitive VQA models struggle to process them accurately, and as a result predict the incorrect answer (black) rather than the correct answer (blue), including the state-of-the-art methods.",
                "cite_spans": [
                    {
                        "start": 29,
                        "end": 46,
                        "text": "(Li et al., 2019;",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 47,
                        "end": 68,
                        "text": "Cadene et al., 2019a;",
                        "ref_id": null
                    },
                    {
                        "start": 69,
                        "end": 85,
                        "text": "Yu et al., 2019)",
                        "ref_id": "BIBREF33"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 197,
                        "end": 198,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Although the relations between two objects in an image have been considered, the attention-based VQA models lack building blocks to explicitly capture the syntactic dependency relations between words in a question. As shown in Figure 1 (c), these dependency relations can reflect which object is being asked (e.g., the word umpire's modifies the word shirt) and which aspect of the object is being asked (e.g., the word color is the direct object of the word is). If a VQA model only knows the word shirt rather than the relation between words umpire's and shirt in a question, it is difficult to distinguish which object is being asked. In fact, we do need the modified relations to discriminate the correct object from multiple similar objects. Therefore, we consider that it is necessary to explore the relations between words at linguistic level in addition to constructing the relations between objects at visual level.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 234,
                        "end": 235,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Motivated by this, we propose a dual channel graph convolutional network (DC-GCN) to simultaneously capture the relations between objects in an image and the syntactic dependency relations between words in a question. Our proposed DC-GCN model consists of an Image-GCN (I-GCN) module, a Question GCN (Q-GCN) module, and an attention alignment module. The I-GCN module captures the relations between objects in an image, the Q-GCN module captures the syntactic dependency relations between words in a question, and the attention alignment module is used to align two representations of image and question. The contributions of this work are summarized as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "1) We propose a dual channel graph convolutional network (DC-GCN) to simultaneously capture the visual and textual relations, and design the attention alignment module to align the multimodal representations, thus reducing the semantic gaps between vision and language.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "2) We explore how to construct the syntactic dependency relations between words at linguistic level via graph convolutional networks as well as the relations between objects at visual level.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "3) We conduct extensive experiments and ablation studies on VQA-v2 and VQA-CP-v2 datasets to examine the effectiveness of our DC-GCN model. Experimental results show that the DC-GCN model achieves competitive performance with the state-of-the-art approaches.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Visual Question Answering Attention mechanism has been proven effective on many tasks, such as machine translation (Bahdanau et al., 2014) and image captioning (Pedersoli et al., 2017) . A number of methods have been developed so far, in which question-guided attention on image regions is commonly used. These can be categorized into two classes according to the types of employed image features. One class uses visual features from some region proposals, which are generated by Region Proposal Network (Ren et al., 2015) . The other class uses convolutional features (i.e., activations of convolutional layers).",
                "cite_spans": [
                    {
                        "start": 115,
                        "end": 138,
                        "text": "(Bahdanau et al., 2014)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 160,
                        "end": 184,
                        "text": "(Pedersoli et al., 2017)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 504,
                        "end": 522,
                        "text": "(Ren et al., 2015)",
                        "ref_id": "BIBREF24"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Works",
                "sec_num": "2"
            },
            {
                "text": "To learn a better representation of the question, the Stacked Attention Network (Yang et al., 2016) which can search question-related image regions is designed by performing multi-step visual attention operations. A co-attention mechanism that jointly performs question-guided visual attention and image-guided question attention is proposed to solve the problems of which regions to look at and what words to listen to (Shih et al., 2016) . To obtain more fine-grained interaction between image and question, some researchers introduce rather sophisticated fusion strategies. Bilinear pooling method (Kim et al., 2018; Yu et al., 2017 Yu et al., , 2018) ) is one of the pioneering works to efficiently and expressively combine multimodal features by using an outer product of two vectors.",
                "cite_spans": [
                    {
                        "start": 80,
                        "end": 99,
                        "text": "(Yang et al., 2016)",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 420,
                        "end": 439,
                        "text": "(Shih et al., 2016)",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 601,
                        "end": 619,
                        "text": "(Kim et al., 2018;",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 620,
                        "end": 635,
                        "text": "Yu et al., 2017",
                        "ref_id": "BIBREF34"
                    },
                    {
                        "start": 636,
                        "end": 656,
                        "text": "Yu et al., , 2018) )",
                        "ref_id": "BIBREF35"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Works",
                "sec_num": "2"
            },
            {
                "text": "Recently, some researchers devoted to overcome the priors on VQA dataset and proposed the methods like GVQA (Agrawal et al., 2018) , UpDn + Q-Adv + DoE (Ramakrishnan et al., 2018) , and RUBi (Cadene et al., 2019b) to solve the language biases on the VQA-CP-v2 dataset.",
                "cite_spans": [
                    {
                        "start": 108,
                        "end": 130,
                        "text": "(Agrawal et al., 2018)",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 152,
                        "end": 179,
                        "text": "(Ramakrishnan et al., 2018)",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 191,
                        "end": 213,
                        "text": "(Cadene et al., 2019b)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Works",
                "sec_num": "2"
            },
            {
                "text": "Graph Networks Graph networks are powerful models that can perform relational inferences through message passing. The core idea is to enable communication between image regions to build contextualized representations of these regions. Below we review some of the recent works that rely on graph networks and other contextualized representations for VQA.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Works",
                "sec_num": "2"
            },
            {
                "text": "Recent research works (Cadene et al., 2019a; Li et al., 2019) focus on how to deal with complex scene and relation reasoning to obtain better image representations. Based on multimodal attentional networks, (Cadene et al., 2019a) introduces an atomic reasoning primitive to represent interactions between question and image region by a rich vecto- rial representation and model region relations with pairwise combinations. GCNs, which can better explore the visual relations between objects and aggregate its own features and neighbors' features, have been applied to various tasks, such as text classification (Yao et al., 2019) , relation extraction (Guo et al., 2019; Zhang et al., 2018b) , scene graph generation (Yang et al., 2018; Yao et al., 2018 ).",
                "cite_spans": [
                    {
                        "start": 22,
                        "end": 44,
                        "text": "(Cadene et al., 2019a;",
                        "ref_id": null
                    },
                    {
                        "start": 45,
                        "end": 61,
                        "text": "Li et al., 2019)",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 207,
                        "end": 229,
                        "text": "(Cadene et al., 2019a)",
                        "ref_id": null
                    },
                    {
                        "start": 611,
                        "end": 629,
                        "text": "(Yao et al., 2019)",
                        "ref_id": "BIBREF31"
                    },
                    {
                        "start": 652,
                        "end": 670,
                        "text": "(Guo et al., 2019;",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 671,
                        "end": 691,
                        "text": "Zhang et al., 2018b)",
                        "ref_id": null
                    },
                    {
                        "start": 717,
                        "end": 736,
                        "text": "(Yang et al., 2018;",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 737,
                        "end": 753,
                        "text": "Yao et al., 2018",
                        "ref_id": "BIBREF32"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Works",
                "sec_num": "2"
            },
            {
                "text": "To answer complicated questions about an image, a relation-aware graph attention network (Re-GAT) (Li et al., 2019) is proposed to encode each image into a graph and model multi-type interobject relations via a graph attention mechanism, such as spatial relations, semantic relations and implicit relations. One limitation of ReGAT (Li et al., 2019) lies in the fact that it solely consider the relations between objects in an image while neglect the importance of text information. In contrast, our DC-GCN simultaneously capture visual relations in an image and textual relations in a question.",
                "cite_spans": [
                    {
                        "start": 98,
                        "end": 115,
                        "text": "(Li et al., 2019)",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 332,
                        "end": 349,
                        "text": "(Li et al., 2019)",
                        "ref_id": "BIBREF17"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Works",
                "sec_num": "2"
            },
            {
                "text": "Similar to (Anderson et al., 2018) , we extract the image features by using a pretrained Faster RCNN (Ren et al., 2015) . We select \u00b5 object proposals for each image, where each object proposal is represented by a 2048 dimensional feature vector. The obtained visual region features are denoted as",
                "cite_spans": [
                    {
                        "start": 11,
                        "end": 34,
                        "text": "(Anderson et al., 2018)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 101,
                        "end": 119,
                        "text": "(Ren et al., 2015)",
                        "ref_id": "BIBREF24"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Feature Extraction",
                "sec_num": "3.1"
            },
            {
                "text": "h v = {h vi } \u00b5 i=0 \u2208 R \u00b5\u00d72048 .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Feature Extraction",
                "sec_num": "3.1"
            },
            {
                "text": "To extract the question features, each word is embedded into a 300-dimensional Glove vector (Pennington et al., 2014) . The word embeddings are input into a LSTM (Hochreiter and Schmidhuber, 1997) to encode, which produces the initial question representation h q = {h qj } \u03bb j=0 \u2208 R \u03bb\u00d7dq .",
                "cite_spans": [
                    {
                        "start": 92,
                        "end": 117,
                        "text": "(Pennington et al., 2014)",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 162,
                        "end": 196,
                        "text": "(Hochreiter and Schmidhuber, 1997)",
                        "ref_id": "BIBREF13"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Feature Extraction",
                "sec_num": "3.1"
            },
            {
                "text": "Image Fully-connected Relations Graph By treating each object region in an image as a vertex, we can construct a fully-connected undirected graph, as shown in Figure 3 Identifying the correlation between objects is a key step. We calculate the correlation between objects by using spatial relations. The steps are as follows: (1) The features of two nodes are input into multi-layer perceptron respectively, and then the corresponding elements are multiplied to get a relatedness score. (2) The intersection over union of two object regions is calculated. According to the overlapping part of two object regions, different spatial relations are classified into 11 different categories, such as inside, cover, and overlap (Yao et al., 2018) . Following the work (Yao et al., 2018) , we utilize the overlapping region between two object regions to judge whether there is an edge between two regions. If two object regions have large overlapping part, it means that there is a strong correlation between these two objects. If two object regions haven't any overlapping part, we consider two objects have a weak correlation, which means there are no edges to connect these two nodes. According to the spatial relations, we prune some irrelevant relations between objects and obtain a sparse graph, as shown in Figure 3(c) . Image Graph Convolutions Following the previous studies (Li et al., 2019; Zhang et al., 2018b; Yang et al., 2018) , we use GCN to update the representations of objects. Given a graph with \u00b5 nodes, each object region in an image is a node. We represent the graph structure with a \u00b5 \u00d7 \u00b5 adjacency matrix A, where A ij = 1 if there is overlapping region between node i and node j; else A ij = 0.",
                "cite_spans": [
                    {
                        "start": 721,
                        "end": 739,
                        "text": "(Yao et al., 2018)",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 761,
                        "end": 779,
                        "text": "(Yao et al., 2018)",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 1376,
                        "end": 1393,
                        "text": "(Li et al., 2019;",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 1394,
                        "end": 1414,
                        "text": "Zhang et al., 2018b;",
                        "ref_id": null
                    },
                    {
                        "start": 1415,
                        "end": 1433,
                        "text": "Yang et al., 2018)",
                        "ref_id": "BIBREF29"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 166,
                        "end": 167,
                        "text": "3",
                        "ref_id": "FIGREF3"
                    },
                    {
                        "start": 1313,
                        "end": 1317,
                        "text": "3(c)",
                        "ref_id": "FIGREF3"
                    }
                ],
                "eq_spans": [],
                "section": "I-GCN Module",
                "sec_num": "3.2.1"
            },
            {
                "text": "Given a target node i and a neighboring node j \u2208 N (i) in an image, where N (i) is the set of nodes neighboring with node i, and the representations of node i and node j are h vi and h vj , respectively. To obtain the correlation score s ij between node i and j, we learn a fully connected layer over concatenated node features h vi and h vj :",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "I-GCN Module",
                "sec_num": "3.2.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "s ij = w T a \u03c3(W a [h (l) vi , h (l) vj ]),",
                        "eq_num": "(1)"
                    }
                ],
                "section": "I-GCN Module",
                "sec_num": "3.2.1"
            },
            {
                "text": "where w a and W a are learned parameters, \u03c3 is the non-linear activation function, and [h",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "I-GCN Module",
                "sec_num": "3.2.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "(l) vi , h",
                        "eq_num": "(l)"
                    }
                ],
                "section": "I-GCN Module",
                "sec_num": "3.2.1"
            },
            {
                "text": "vj ] denotes the concatenation operation. We apply a softmax function over the correlation score s ij to obtain weight \u03b1 ij , as shown in Figure 3(c) where the numbers in red represent the weight scores:",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 145,
                        "end": 149,
                        "text": "3(c)",
                        "ref_id": "FIGREF3"
                    }
                ],
                "eq_spans": [],
                "section": "I-GCN Module",
                "sec_num": "3.2.1"
            },
            {
                "text": "\u03b1 ij = exp (s ij ) j\u2208N (i) exp (s ij )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "I-GCN Module",
                "sec_num": "3.2.1"
            },
            {
                "text": ".",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "I-GCN Module",
                "sec_num": "3.2.1"
            },
            {
                "text": "(2)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "I-GCN Module",
                "sec_num": "3.2.1"
            },
            {
                "text": "The l-th layer representations of neighboring nodes h (l) vj are first transformed via a learned linear transformation W b . Those transformed representations are then gathered with weight \u03b1 ij , followed by a non-linear function \u03c3. This layer-wise propagation can be denoted as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "I-GCN Module",
                "sec_num": "3.2.1"
            },
            {
                "text": "h (l+1) vi = \u03c3 \uf8eb \uf8ed h (l) vi + j\u2208N (i) A ij \u03b1 ij W b h (l) vj \uf8f6 \uf8f8 . (3)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "I-GCN Module",
                "sec_num": "3.2.1"
            },
            {
                "text": "Following the stacked L layer GCN, the output of I-GCN module H v can be denoted as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "I-GCN Module",
                "sec_num": "3.2.1"
            },
            {
                "text": "H v = h (l+1) vi (l < L).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "I-GCN Module",
                "sec_num": "3.2.1"
            },
            {
                "text": "(4)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "I-GCN Module",
                "sec_num": "3.2.1"
            },
            {
                "text": "In practice, we observe that two words in a sentence usually hold certain relations. Such relations can be identified by the universal Standford Dependencies (De Marneffe et al., 2014) . As shown in Table 1 , we list a part of commonly-used dependency relations. For example, the sentence what color is Pruned Question Graph with Dependency Relations Irrelevant relations between two words may bring noises. Therefore, we need to prune some unrelated relations to reduce the noises. By parsing the dependency relations of a question, we obtain the relations between words (cf. Figure 4 ). According to dependency relations, we prune some edges between two nodes which do not have dependency relations. A sparse graph is obtained, as shown in Figure 5 (b). Question Graph Convolutions Following the previous works (Li et al., 2019; Zhang et al., 2018b; Yang et al., 2018) , we use GCN to update the node representations of words. Given a graph with \u03bb nodes, each word in a question is a node. We represent the graph structure with a \u03bb \u00d7 \u03bb adjacency matrix B where B ij = 1 if there is a dependency relation between node i and node j; else B ij = 0. Given a target node i and a neighboring node j \u2208 \u2126(i) in a question, \u2126(i) is the set of nodes neighboring with node i. The representations of node i and j are h qi and h qj , respectively. To obtain the correlation score t ij between node i and j, we learn a fully connected layer over concatenated node features h qi and h qj :",
                "cite_spans": [
                    {
                        "start": 158,
                        "end": 184,
                        "text": "(De Marneffe et al., 2014)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 813,
                        "end": 830,
                        "text": "(Li et al., 2019;",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 831,
                        "end": 851,
                        "text": "Zhang et al., 2018b;",
                        "ref_id": null
                    },
                    {
                        "start": 852,
                        "end": 870,
                        "text": "Yang et al., 2018)",
                        "ref_id": "BIBREF29"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 205,
                        "end": 206,
                        "text": "1",
                        "ref_id": "TABREF0"
                    },
                    {
                        "start": 584,
                        "end": 585,
                        "text": "4",
                        "ref_id": "FIGREF4"
                    },
                    {
                        "start": 749,
                        "end": 750,
                        "text": "5",
                        "ref_id": "FIGREF5"
                    }
                ],
                "eq_spans": [],
                "section": "Q-GCN Module",
                "sec_num": "3.2.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "t ij = w T c \u03c3(W c [h (l) qi , h (l) qj ]),",
                        "eq_num": "(5)"
                    }
                ],
                "section": "Q-GCN Module",
                "sec_num": "3.2.2"
            },
            {
                "text": "where w c and W c are learned parameters, \u03c3 is the non-linear activation function, and [h",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Q-GCN Module",
                "sec_num": "3.2.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "(l) qi , h",
                        "eq_num": "(l)"
                    }
                ],
                "section": "Q-GCN Module",
                "sec_num": "3.2.2"
            },
            {
                "text": "qj ] de-notes the concatenation operation. We apply a softmax function over the correlation score t ij to obtain weight \u03b2 ij :",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Q-GCN Module",
                "sec_num": "3.2.2"
            },
            {
                "text": "\u03b2 ij = exp (t ij ) j\u2208\u2126(i) exp (t ij ) . (6)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Q-GCN Module",
                "sec_num": "3.2.2"
            },
            {
                "text": "As shown in Figure 5 (c), the numbers in red are the weight scores. The l-th layer representations of neighboring nodes h (l) qj are first transformed via a learned linear transformation W d . Those transformed representations are gathered with weight \u03b2 ij , followed by a non-linear function \u03c3. This layer-wise propagation can be denoted as:",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 19,
                        "end": 20,
                        "text": "5",
                        "ref_id": "FIGREF5"
                    }
                ],
                "eq_spans": [],
                "section": "Q-GCN Module",
                "sec_num": "3.2.2"
            },
            {
                "text": "h (l+1) qi = \u03c3 \uf8eb \uf8ed h (l) qi + j\u2208\u2126(i) B ij \u03b2 ij W d h (l) qj \uf8f6 \uf8f8 . (7)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Q-GCN Module",
                "sec_num": "3.2.2"
            },
            {
                "text": "Following the stacked L layer GCN, the output of Q-GCN module H q is denoted as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Q-GCN Module",
                "sec_num": "3.2.2"
            },
            {
                "text": "H q = h (l+1) qi (l < L).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Q-GCN Module",
                "sec_num": "3.2.2"
            },
            {
                "text": "(8)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Q-GCN Module",
                "sec_num": "3.2.2"
            },
            {
                "text": "Based on the previous works (Gao et al., 2019; Yu et al., 2019) , we use self-attention mechanism (Vaswani et al., 2017) to enhance the correlation between words in a question and the correlation between objects in an image, respectively.",
                "cite_spans": [
                    {
                        "start": 28,
                        "end": 46,
                        "text": "(Gao et al., 2019;",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 47,
                        "end": 63,
                        "text": "Yu et al., 2019)",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 98,
                        "end": 120,
                        "text": "(Vaswani et al., 2017)",
                        "ref_id": "BIBREF27"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Attention Alignment Module",
                "sec_num": "3.3"
            },
            {
                "text": "To enhance the correlation between words and highlight the important words, we utilize the selfattention mechanism to update question representation H q . The updated question representation Hq is obtained as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Attention Alignment Module",
                "sec_num": "3.3"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "Hq = softmax H q H T q d q H q ,",
                        "eq_num": "(9)"
                    }
                ],
                "section": "Attention Alignment Module",
                "sec_num": "3.3"
            },
            {
                "text": "where H T q is the transpose of H q and d q is the dimension of H q . The level of this self-attention is set to 4.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Attention Alignment Module",
                "sec_num": "3.3"
            },
            {
                "text": "To obtain the image representation related to question representation, we align the image representation H v by utilizing the question representation Hq as the guided vector. The similarity score r between H v and Hq is calculated as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Attention Alignment Module",
                "sec_num": "3.3"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "r = Hq H T v \u221a d v ,",
                        "eq_num": "(10)"
                    }
                ],
                "section": "Attention Alignment Module",
                "sec_num": "3.3"
            },
            {
                "text": "where H T v is the transpose of H v and d v is the dimension of H v . A softmax function is used to normalize the score r to obtain the weight score r:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Attention Alignment Module",
                "sec_num": "3.3"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "r = [r 1 , \u2022 \u2022 \u2022 , ri ] = exp (r i ) j\u2208\u00b5 exp (r j ) (",
                        "eq_num": "11"
                    }
                ],
                "section": "Attention Alignment Module",
                "sec_num": "3.3"
            },
            {
                "text": ")",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Attention Alignment Module",
                "sec_num": "3.3"
            },
            {
                "text": "where \u00b5 is the number of image regions. By multiplying the weight r and the image representation H v , the updated image representation Hv is obtained:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Attention Alignment Module",
                "sec_num": "3.3"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "Hv = r \u2022 H v . (",
                        "eq_num": "12"
                    }
                ],
                "section": "Attention Alignment Module",
                "sec_num": "3.3"
            },
            {
                "text": ")",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Attention Alignment Module",
                "sec_num": "3.3"
            },
            {
                "text": "The level of this question guided image attention is set to 4. The final outputs of the attention alignment module are Hq and Hv .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Attention Alignment Module",
                "sec_num": "3.3"
            },
            {
                "text": "We apply the linear multimodal fusion method to fuse two representations Hq and Hv as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Answer Prediction",
                "sec_num": "3.4"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "H r = W T v Hv + W T q Hq , (",
                        "eq_num": "13"
                    }
                ],
                "section": "Answer Prediction",
                "sec_num": "3.4"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": ") pred = softmax (W e H r + b e ) ,",
                        "eq_num": "(14)"
                    }
                ],
                "section": "Answer Prediction",
                "sec_num": "3.4"
            },
            {
                "text": "where ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Answer Prediction",
                "sec_num": "3.4"
            },
            {
                "text": "W v , W q , W e ,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Answer Prediction",
                "sec_num": "3.4"
            },
            {
                "text": "We use the Adam optimizer (Kingma and Ba, 2014) with parameters \u03b1 = 0.0001, \u03b2 1 = 0.9, and \u03b2 2 = 0.99. The size of the answer vocabulary is set to M =3,129 as used in (Anderson et al., 2018) .",
                "cite_spans": [
                    {
                        "start": 167,
                        "end": 190,
                        "text": "(Anderson et al., 2018)",
                        "ref_id": "BIBREF1"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Setup",
                "sec_num": "4.2"
            },
            {
                "text": "The base learning rate is set to 0.0001. After 15 epochs, the learning rate is decayed by 1/5 every 2 epochs. All the models are trained up to 20 epochs with the same batch size 64 and hidden size 512. Each image has \u00b5 \u2208 [10, 100] object regions, all questions are padded and truncated to the same length 14, i.e., \u03bb = 14. The levels of stacked layer L and attention alignment module are both 4.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Setup",
                "sec_num": "4.2"
            },
            {
                "text": "Table 2 shows the performance of our DC-GCN model and baseline models trained with the widelyused VQA-v2 dataset. All results in our paper are based on single-model performance. For a fair comparison, we also train our model with extra visual genome dataset (Krishna et al., 2017) (Anderson et al., 2018) is proposed to use features based on Faster RCNN (Ren et al., 2015) instead of ResNet (He et al., 2016) . Dense Co-Attention Network (DCN) (Nguyen and Okatani, 2018) utilizes dense stack of multiple layers of co-attention mechanism. Counting method (Zhang et al., 2018a ) is good at counting questions by utilizing the information of bounding boxes. DFAF (Gao et al., 2019) dynamically fuses Intra-and Inter-modality information. ReGAT (Li et al., 2019 ) models semantic, spatial, and implicit relations via a graph attention network. MCAN (Yu et al., 2019) utilizes deep modular networks to learn the multimodal feature representations, which is a state-of-the-art approach on VQA-v2 dataset. As shown in Table 2 , our model increases the overall accuracy of DFAF and MCAN by 1.2% and 0.6% on the test-std set, respectively. Although still cannot achieve comparable performance in the category of Num with respect to ReGAT (which is the best one in counting sub-task), our DC-GCN outperforms it in other categories (e.g., Y/N with 1.2%, Other with 1.1% and Overall with 0.9%). It shows that DC-GCN has relation capturing ability in answering all kinds of questions by sufficiently exploring the semantics in both object appearances and object relations. In summary, our DC-GCN achieves outstanding performance on the VQA-v2 dataset.",
                "cite_spans": [
                    {
                        "start": 258,
                        "end": 280,
                        "text": "(Krishna et al., 2017)",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 281,
                        "end": 304,
                        "text": "(Anderson et al., 2018)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 349,
                        "end": 372,
                        "text": "RCNN (Ren et al., 2015)",
                        "ref_id": null
                    },
                    {
                        "start": 391,
                        "end": 408,
                        "text": "(He et al., 2016)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 444,
                        "end": 470,
                        "text": "(Nguyen and Okatani, 2018)",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 554,
                        "end": 574,
                        "text": "(Zhang et al., 2018a",
                        "ref_id": null
                    },
                    {
                        "start": 660,
                        "end": 678,
                        "text": "(Gao et al., 2019)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 741,
                        "end": 757,
                        "text": "(Li et al., 2019",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 845,
                        "end": 862,
                        "text": "(Yu et al., 2019)",
                        "ref_id": "BIBREF33"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "2",
                        "ref_id": "TABREF3"
                    },
                    {
                        "start": 1017,
                        "end": 1018,
                        "text": "2",
                        "ref_id": "TABREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Experimental Results",
                "sec_num": "4.3"
            },
            {
                "text": "To demonstrate the generalizability of our DC-GCN model, we also conduct experiments on the VQA-CP-v2 dataset. To overcome the language biases of the VQA-v2 dataset, the research work (Agrawal et al., 2018) designed the VQA-CP-v2 dataset and specifically proposed the GVQA model for reducing the influence of language biases. Table 3 shows the results on VQA-CP-v2 test split. The Murel (Cadene et al., 2019a) and ReGAT (Li et al., 2019) build the relations between objects to realize the reasoning task and question answering task, which are the state-of-the-art models. Our DC-GCN model surpasses both Murel and ReGAT on VQA- . The performance gain is lifted to +1.05%. Although our proposed method is not designed for VQA-CP-v2 dataset, our model has a slight ad-",
                "cite_spans": [
                    {
                        "start": 184,
                        "end": 206,
                        "text": "(Agrawal et al., 2018)",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 387,
                        "end": 409,
                        "text": "(Cadene et al., 2019a)",
                        "ref_id": null
                    },
                    {
                        "start": 420,
                        "end": 437,
                        "text": "(Li et al., 2019)",
                        "ref_id": "BIBREF17"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 332,
                        "end": 333,
                        "text": "3",
                        "ref_id": "TABREF5"
                    }
                ],
                "eq_spans": [],
                "section": "Experimental Results",
                "sec_num": "4.3"
            },
            {
                "text": "Acc. (%) RAMEN (Robik Shrestha, 2019) 39.21 BAN (Kim et al., 2018) * 39.31 Murel (Cadene et al., 2019a) 39.54 ReGAT-Sem (Li et al., 2019) 39.54 ReGAT-Imp (Li et al., 2019) 39.58 ReGAT-Spa (Li et al., 2019) 40.30 ReGAT (Li et al., 2019) 40.42",
                "cite_spans": [
                    {
                        "start": 48,
                        "end": 66,
                        "text": "(Kim et al., 2018)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 81,
                        "end": 103,
                        "text": "(Cadene et al., 2019a)",
                        "ref_id": null
                    },
                    {
                        "start": 120,
                        "end": 137,
                        "text": "(Li et al., 2019)",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 154,
                        "end": 171,
                        "text": "(Li et al., 2019)",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 188,
                        "end": 205,
                        "text": "(Li et al., 2019)",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 218,
                        "end": 235,
                        "text": "(Li et al., 2019)",
                        "ref_id": "BIBREF17"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model",
                "sec_num": null
            },
            {
                "text": "GVQA (Agrawal et al., 2018) # 31.30 UpDn (Anderson et al., 2018) ",
                "cite_spans": [
                    {
                        "start": 5,
                        "end": 27,
                        "text": "(Agrawal et al., 2018)",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 41,
                        "end": 64,
                        "text": "(Anderson et al., 2018)",
                        "ref_id": "BIBREF1"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model",
                "sec_num": null
            },
            {
                "text": "In Figure 6 , we visualize the learned attentions from the I-GCN module, Q-GCN module and At-tention Alignment module. Due to the space limitation, we only show one example and visualize six attention maps from different attention units and different layers. From the results, we have the following observations. Question GCN Module: The attention maps of Q-GCN(2) focus on the words color and shirt as shown in Figure 6 (a) while the attention maps of Q-GCN(4) correctly focus on the words color, umpire's, and shirt, as shown in Figure 6 (b). Those words have the larger weight than others. That is to say, the keywords color, umpire's and shirt are identified correctly.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 10,
                        "end": 11,
                        "text": "6",
                        "ref_id": "FIGREF6"
                    },
                    {
                        "start": 419,
                        "end": 420,
                        "text": "6",
                        "ref_id": "FIGREF6"
                    },
                    {
                        "start": 538,
                        "end": 539,
                        "text": "6",
                        "ref_id": "FIGREF6"
                    }
                ],
                "eq_spans": [],
                "section": "Qualitative Analysis",
                "sec_num": "4.4"
            },
            {
                "text": "Image GCN Module For the sake of presentation, we only consider 20 object regions in an image.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Qualitative Analysis",
                "sec_num": "4.4"
            },
            {
                "text": "The index within [1, 20] shown on the axes of the attention maps corresponds to each object in the image. Among these indexes, indexes 4, 6, 9, and 12 are the most relevant ones for the question.",
                "cite_spans": [
                    {
                        "start": 17,
                        "end": 20,
                        "text": "[1,",
                        "ref_id": null
                    },
                    {
                        "start": 21,
                        "end": 24,
                        "text": "20]",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Qualitative Analysis",
                "sec_num": "4.4"
            },
            {
                "text": "Compared with I-GCN(2) which focuses on the 4-th, 6-th, 9-th, 12-th, and 14-th objects (cf. Figure 6 (c)), the I-GCN(4) focuses more on the 4-th, 6-th, and 12-th objects where the 4-th object has larger weight than the 6-th and 12-th objects, as shown in Figure 6 (d). The 4-th object region is the region of ground true while the 6-th, 9-th, and 12-th object regions are the most relevant ones.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 99,
                        "end": 100,
                        "text": "6",
                        "ref_id": "FIGREF6"
                    },
                    {
                        "start": 262,
                        "end": 263,
                        "text": "6",
                        "ref_id": "FIGREF6"
                    }
                ],
                "eq_spans": [],
                "section": "Qualitative Analysis",
                "sec_num": "4.4"
            },
            {
                "text": "Attention Alignment Module Given a specific question, a model needs to align image objects guided by the question to update the representations of objects. As shown in Figure 6 (e), the focus regions are more scattered, where the key regions are mainly the 4-th, 9-th and 12-th object regions. Through the guidance of the identified words color, umpire's and shirt, the DC-GCN model gradually pays more attention to the 4-th, 9-th, and 12-th object regions rather than other irrelevant object regions, as shown in Figure 6 (f). This alignment process demonstrates that our model can capture the relations of multiple similar objects.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 175,
                        "end": 176,
                        "text": "6",
                        "ref_id": "FIGREF6"
                    },
                    {
                        "start": 521,
                        "end": 522,
                        "text": "6",
                        "ref_id": "FIGREF6"
                    }
                ],
                "eq_spans": [],
                "section": "Qualitative Analysis",
                "sec_num": "4.4"
            },
            {
                "text": "We also visualize some negative examples predicted by our DC-GCN model. As shown in Figure 7 , which can be classified into three categories: (1) limitation of object detection; (2) text semantic understanding in scenarios; (3) subjective judgment. In Figure 7 (a), although the question how many sheep are pictured is not so difficult, the image content is really confusing. If not observe carefully, it's rather easy to obtain the wrong answer 2 instead of 3. The reasons for this error include object occlusion, near and far degrees, and the limitation (Biten et al., 2019) , which requires to recognize the numbers, symbols and proper nouns in a scene. In Figure 7 (c), subjective judgment is needed to answer the question is this man happy. Making this judgment requires some common sense knowledge and real life experience. Specifically, someone holding a banana against him and just like holding a gun towards him, so he is unhappy. Our model can not make such analysis like a human being done to make a subjective judgment and predict the correct answer yes.",
                "cite_spans": [
                    {
                        "start": 556,
                        "end": 576,
                        "text": "(Biten et al., 2019)",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 91,
                        "end": 92,
                        "text": "7",
                        "ref_id": "FIGREF7"
                    },
                    {
                        "start": 259,
                        "end": 260,
                        "text": "7",
                        "ref_id": "FIGREF7"
                    },
                    {
                        "start": 667,
                        "end": 668,
                        "text": "7",
                        "ref_id": "FIGREF7"
                    }
                ],
                "eq_spans": [],
                "section": "Qualitative Analysis",
                "sec_num": "4.4"
            },
            {
                "text": "Finally, to understand the distribution of three error types, we randomly pick up 100 samples on dev set of VQA-v2. The number of three error types (i.e., overlapping objects, text semantic understanding, and subjective judgment) is 3, 3, and 29, respectively. The predicted answers of the first two questions types are all incorrect. The last one has 12 incorrect answers, which means the error rate of this question type is 41.4%. These observations are helpful to make further improvement in the future.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Qualitative Analysis",
                "sec_num": "4.4"
            },
            {
                "text": "We perform extensive ablation studies on the VQA-v2 validation dataset (cf. Firstly, we investigate the influence of GCN types. There are two GCN types: I-GCN and Q-GCN, as shown in Table 4 . When removing the I-GCN, the performance of our model decreases from 66.57% to 65.52% (p-value = 3.22E-08 < 0.05). When removing the Q-GCN, the performance of our model slightly decreases from 66.57% to 66.15% (p-value = 2.04E-07 < 0.05). We consider that there are two reasons. One is that the image content is more complex than the question's content, hence which has richer semantic information. By building the relations between objects can help clarify what the image represents and help align with the question representations. The other is that the length of question is short, and less information is contained (e.g., what animal is this? and what color is the man's shirt?).",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 188,
                        "end": 189,
                        "text": "4",
                        "ref_id": "TABREF6"
                    }
                ],
                "eq_spans": [],
                "section": "Ablation Study",
                "sec_num": "4.5"
            },
            {
                "text": "Then, we perform ablation study on the influence of dependency relations (cf. Table 1 ). The relations, like nsubj, nmod, dobj and amod, are crucial to semantic representations, therefore, we do not remove them from the sentence. As shown in Table 4 , removing the relations like det, case, aux and advmod individually, has trivial influence to the semantic representations of the question. But the result accuracy decreases significantly when we simultaneously remove the relations det, case and cop. The reason may be that the sentence loses too much information and becomes difficult to fully express the meaning of the original sentence. For example, consider the two phrases on the table and under the table. If we remove the relation case, which means that the words on and under are removed, then it will be hard to distinguish whether it is on the table or under the table.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 84,
                        "end": 85,
                        "text": "1",
                        "ref_id": "TABREF0"
                    },
                    {
                        "start": 248,
                        "end": 249,
                        "text": "4",
                        "ref_id": "TABREF6"
                    }
                ],
                "eq_spans": [],
                "section": "Ablation Study",
                "sec_num": "4.5"
            },
            {
                "text": "In this paper, we propose a dual channel graph convolutional network to explore the relations between objects in an image and the syntactic dependency relations between words in a question. Furthermore, we explicitly construct the relations between words by dependency tree and align the image and question representations by an attention alignment module to reduce the gaps between vision and language. Extensive experiments on the VQA-v2 and VQA-CP-v2 datasets demonstrate that our model achieves comparable performance with the stateof-the-art approaches. We will explore more complicated object relation modeling in future work.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "5"
            }
        ],
        "back_matter": [
            {
                "text": "We thank the anonymous reviewers for valuable comments and thoughtful suggestions. We would also like to thank Professor Yuzhang Lin from University of Massachusetts Lowell for helpful discussions.This work was supported by the Fundamental Research Funds for the Central Universities, SCUT (No. 2017ZD048, D2182480), the Science and Technology Planning Project of Guangdong Province (No.2017B050506004), the Science and Technology Programs of Guangzhou (No.201704030076, 201802010027, 201902010046) and the collaborative research grants from the Guangxi Natural Science Foundation (2017GXNSFAA198225) and the Hong Kong Research Grants Council (project no. PolyU 1121417 and project no. C1031-18G), and an internal research grant from the Hong Kong Polytechnic University (project 1.9B0V).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgements",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Don't just assume; look and answer: Overcoming priors for visual question answering",
                "authors": [
                    {
                        "first": "Aishwarya",
                        "middle": [],
                        "last": "Agrawal",
                        "suffix": ""
                    },
                    {
                        "first": "Dhruv",
                        "middle": [],
                        "last": "Batra",
                        "suffix": ""
                    },
                    {
                        "first": "Devi",
                        "middle": [],
                        "last": "Parikh",
                        "suffix": ""
                    },
                    {
                        "first": "Aniruddha",
                        "middle": [],
                        "last": "Kembhavi",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": "",
                "issue": "",
                "pages": "4971--4980",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Aishwarya Agrawal, Dhruv Batra, Devi Parikh, and Aniruddha Kembhavi. 2018. Don't just assume; look and answer: Overcoming priors for visual ques- tion answering. In Proceedings of the IEEE Confer- ence on Computer Vision and Pattern Recognition, pages 4971-4980.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Bottom-up and top-down attention for image captioning and visual question answering",
                "authors": [
                    {
                        "first": "Peter",
                        "middle": [],
                        "last": "Anderson",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaodong",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Buehler",
                        "suffix": ""
                    },
                    {
                        "first": "Damien",
                        "middle": [],
                        "last": "Teney",
                        "suffix": ""
                    },
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Johnson",
                        "suffix": ""
                    },
                    {
                        "first": "Stephen",
                        "middle": [],
                        "last": "Gould",
                        "suffix": ""
                    },
                    {
                        "first": "Lei",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": "",
                "issue": "",
                "pages": "6077--6086",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. 2018. Bottom-up and top-down attention for image captioning and visual question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6077-6086.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Vqa: Visual question answering",
                "authors": [
                    {
                        "first": "Stanislaw",
                        "middle": [],
                        "last": "Antol",
                        "suffix": ""
                    },
                    {
                        "first": "Aishwarya",
                        "middle": [],
                        "last": "Agrawal",
                        "suffix": ""
                    },
                    {
                        "first": "Jiasen",
                        "middle": [],
                        "last": "Lu",
                        "suffix": ""
                    },
                    {
                        "first": "Margaret",
                        "middle": [],
                        "last": "Mitchell",
                        "suffix": ""
                    },
                    {
                        "first": "Dhruv",
                        "middle": [],
                        "last": "Batra",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Lawrence Zitnick",
                        "suffix": ""
                    },
                    {
                        "first": "Devi",
                        "middle": [],
                        "last": "Parikh",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the IEEE international conference on computer vision",
                "volume": "",
                "issue": "",
                "pages": "2425--2433",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar- garet Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. 2015. Vqa: Visual question an- swering. In Proceedings of the IEEE international conference on computer vision, pages 2425-2433.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Neural machine translation by jointly learning to align and translate",
                "authors": [
                    {
                        "first": "Dzmitry",
                        "middle": [],
                        "last": "Bahdanau",
                        "suffix": ""
                    },
                    {
                        "first": "Kyunghyun",
                        "middle": [],
                        "last": "Cho",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint arX- iv:1409.0473.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Scene text visual question answering",
                "authors": [
                    {
                        "first": "Ruben",
                        "middle": [],
                        "last": "Ali Furkan Biten",
                        "suffix": ""
                    },
                    {
                        "first": "Andres",
                        "middle": [],
                        "last": "Tito",
                        "suffix": ""
                    },
                    {
                        "first": "Lluis",
                        "middle": [],
                        "last": "Mafla",
                        "suffix": ""
                    },
                    {
                        "first": "Maral",
                        "middle": [],
                        "last": "Gomez",
                        "suffix": ""
                    },
                    {
                        "first": "Ernest",
                        "middle": [],
                        "last": "Rusiol",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [
                            "V"
                        ],
                        "last": "Valveny",
                        "suffix": ""
                    },
                    {
                        "first": "Dimosthenis",
                        "middle": [],
                        "last": "Jawahar",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Karatzas",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez, Maral Rusiol, Ernest Valveny, C. V. Jawa- har, and Dimosthenis Karatzas. 2019. Scene text vi- sual question answering. CoRR abs/1905.13648.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Murel: Multimodal relational reasoning for visual question answering",
                "authors": [
                    {
                        "first": "Remi",
                        "middle": [],
                        "last": "Cadene",
                        "suffix": ""
                    },
                    {
                        "first": "Hedi",
                        "middle": [],
                        "last": "Ben-Younes",
                        "suffix": ""
                    },
                    {
                        "first": "Matthieu",
                        "middle": [],
                        "last": "Cord",
                        "suffix": ""
                    },
                    {
                        "first": "Nicolas",
                        "middle": [],
                        "last": "Thome",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": "",
                "issue": "",
                "pages": "1989--1998",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Remi Cadene, Hedi Ben-Younes, Matthieu Cord, and Nicolas Thome. 2019a. Murel: Multimodal rela- tional reasoning for visual question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1989-1998.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Rubi: Reducing unimodal biases for visual question answering",
                "authors": [
                    {
                        "first": "Remi",
                        "middle": [],
                        "last": "Cadene",
                        "suffix": ""
                    },
                    {
                        "first": "Corentin",
                        "middle": [],
                        "last": "Dancette",
                        "suffix": ""
                    },
                    {
                        "first": "Hedi",
                        "middle": [],
                        "last": "Ben Younes",
                        "suffix": ""
                    },
                    {
                        "first": "Matthieu",
                        "middle": [],
                        "last": "Cord",
                        "suffix": ""
                    },
                    {
                        "first": "Devi",
                        "middle": [],
                        "last": "Parikh",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "841--852",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Remi Cadene, Corentin Dancette, Hedi Ben younes, Matthieu Cord, and Devi Parikh. 2019b. Rubi: Re- ducing unimodal biases for visual question answer- ing. In Advances in Neural Information Processing Systems, pages 841-852.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Visual question reasoning on general dependency tree",
                "authors": [
                    {
                        "first": "Qingxing",
                        "middle": [],
                        "last": "Cao",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaodan",
                        "middle": [],
                        "last": "Liang",
                        "suffix": ""
                    },
                    {
                        "first": "Bailing",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Guanbin",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Liang",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": "",
                "issue": "",
                "pages": "7249--7257",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Qingxing Cao, Xiaodan Liang, Bailing Li, Guanbin Li, and Liang Lin. 2018. Visual question reasoning on general dependency tree. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7249-7257.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Universal stanford dependencies: A cross-linguistic typology",
                "authors": [
                    {
                        "first": "Marie-Catherine",
                        "middle": [],
                        "last": "De Marneffe",
                        "suffix": ""
                    },
                    {
                        "first": "Timothy",
                        "middle": [],
                        "last": "Dozat",
                        "suffix": ""
                    },
                    {
                        "first": "Natalia",
                        "middle": [],
                        "last": "Silveira",
                        "suffix": ""
                    },
                    {
                        "first": "Katri",
                        "middle": [],
                        "last": "Haverinen",
                        "suffix": ""
                    },
                    {
                        "first": "Filip",
                        "middle": [],
                        "last": "Ginter",
                        "suffix": ""
                    },
                    {
                        "first": "Joakim",
                        "middle": [],
                        "last": "Nivre",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "LREC",
                "volume": "14",
                "issue": "",
                "pages": "4585--4592",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Marie-Catherine De Marneffe, Timothy Dozat, Natali- a Silveira, Katri Haverinen, Filip Ginter, Joakim Nivre, and Christopher D Manning. 2014. Universal stanford dependencies: A cross-linguistic typology. In LREC, volume 14, pages 4585-4592.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Dynamic fusion with intra-and inter-modality attention flow for visual question answering",
                "authors": [
                    {
                        "first": "Peng",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Zhengkai",
                        "middle": [],
                        "last": "Jiang",
                        "suffix": ""
                    },
                    {
                        "first": "Haoxuan",
                        "middle": [],
                        "last": "You",
                        "suffix": ""
                    },
                    {
                        "first": "Pan",
                        "middle": [],
                        "last": "Lu",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [
                            "H"
                        ],
                        "last": "Steven",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaogang",
                        "middle": [],
                        "last": "Hoi",
                        "suffix": ""
                    },
                    {
                        "first": "Hongsheng",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": "",
                "issue": "",
                "pages": "6639--6648",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Peng Gao, Zhengkai Jiang, Haoxuan You, Pan Lu, Steven CH Hoi, Xiaogang Wang, and Hongsheng Li. 2019. Dynamic fusion with intra-and inter-modality attention flow for visual question answering. In Pro- ceedings of the IEEE Conference on Computer Vi- sion and Pattern Recognition, pages 6639-6648.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
                "authors": [
                    {
                        "first": "Yash",
                        "middle": [],
                        "last": "Goyal",
                        "suffix": ""
                    },
                    {
                        "first": "Tejas",
                        "middle": [],
                        "last": "Khot",
                        "suffix": ""
                    },
                    {
                        "first": "Douglas",
                        "middle": [],
                        "last": "Summers-Stay",
                        "suffix": ""
                    },
                    {
                        "first": "Dhruv",
                        "middle": [],
                        "last": "Batra",
                        "suffix": ""
                    },
                    {
                        "first": "Devi",
                        "middle": [],
                        "last": "Parikh",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": "",
                "issue": "",
                "pages": "6904--6913",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhru- v Batra, and Devi Parikh. 2017. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6904-6913.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Attention guided graph convolutional networks for relation extraction",
                "authors": [
                    {
                        "first": "Zhijiang",
                        "middle": [],
                        "last": "Guo",
                        "suffix": ""
                    },
                    {
                        "first": "Yan",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Lu",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 57th Conference of the Association for Computational Linguistics (ACL)",
                "volume": "",
                "issue": "",
                "pages": "241--251",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zhijiang Guo, Yan Zhang, and Wei Lu. 2019. Atten- tion guided graph convolutional networks for rela- tion extraction. In Proceedings of the 57th Confer- ence of the Association for Computational Linguis- tics (ACL), pages 241-251.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Deep residual learning for image recognition",
                "authors": [
                    {
                        "first": "Kaiming",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Xiangyu",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Shaoqing",
                        "middle": [],
                        "last": "Ren",
                        "suffix": ""
                    },
                    {
                        "first": "Jian",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
                "volume": "",
                "issue": "",
                "pages": "770--778",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recog- nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770- 778.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Long short-term memory",
                "authors": [
                    {
                        "first": "Sepp",
                        "middle": [],
                        "last": "Hochreiter",
                        "suffix": ""
                    },
                    {
                        "first": "J\u00fcrgen",
                        "middle": [],
                        "last": "Schmidhuber",
                        "suffix": ""
                    }
                ],
                "year": 1997,
                "venue": "Neural computation",
                "volume": "9",
                "issue": "8",
                "pages": "1735--1780",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735-1780.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Bilinear attention networks",
                "authors": [
                    {
                        "first": "Jin-Hwa",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "Jaehyun",
                        "middle": [],
                        "last": "Jun",
                        "suffix": ""
                    },
                    {
                        "first": "Byoung-Tak",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "1564--1574",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang. 2018. Bilinear attention networks. In Advances in Neural Information Processing Systems, pages 1564-1574.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Adam: A method for stochastic optimization",
                "authors": [
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Diederik",
                        "suffix": ""
                    },
                    {
                        "first": "Jimmy",
                        "middle": [],
                        "last": "Kingma",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Ba",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1412.6980"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations",
                "authors": [
                    {
                        "first": "Ranjay",
                        "middle": [],
                        "last": "Krishna",
                        "suffix": ""
                    },
                    {
                        "first": "Yuke",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    },
                    {
                        "first": "Oliver",
                        "middle": [],
                        "last": "Groth",
                        "suffix": ""
                    },
                    {
                        "first": "Justin",
                        "middle": [],
                        "last": "Johnson",
                        "suffix": ""
                    },
                    {
                        "first": "Kenji",
                        "middle": [],
                        "last": "Hata",
                        "suffix": ""
                    },
                    {
                        "first": "Joshua",
                        "middle": [],
                        "last": "Kravitz",
                        "suffix": ""
                    },
                    {
                        "first": "Stephanie",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Yannis",
                        "middle": [],
                        "last": "Kalantidis",
                        "suffix": ""
                    },
                    {
                        "first": "Li-Jia",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [
                            "A"
                        ],
                        "last": "Shamma",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "International Journal of Computer Vision",
                "volume": "123",
                "issue": "1",
                "pages": "32--73",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John- son, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. 2017. Visual genome: Connecting language and vision using crowdsourced dense image anno- tations. International Journal of Computer Vision, 123(1):32-73.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Relation-aware graph attention network for visual question answering",
                "authors": [
                    {
                        "first": "Linjie",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Zhe",
                        "middle": [],
                        "last": "Gan",
                        "suffix": ""
                    },
                    {
                        "first": "Yu",
                        "middle": [],
                        "last": "Cheng",
                        "suffix": ""
                    },
                    {
                        "first": "Jingjing",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the IEEE International Conference on Computer Vision",
                "volume": "",
                "issue": "",
                "pages": "10313--10322",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Linjie Li, Zhe Gan, Yu Cheng, and Jingjing Liu. 2019. Relation-aware graph attention network for visual question answering. Proceedings of the IEEE In- ternational Conference on Computer Vision, pages 10313-10322.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Improving referring expression grounding with cross-modal attentionguided erasing",
                "authors": [
                    {
                        "first": "Xihui",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Zihao",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Jing",
                        "middle": [],
                        "last": "Shao",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaogang",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Hongsheng",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": "",
                "issue": "",
                "pages": "1950--1959",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xihui Liu, Zihao Wang, Jing Shao, Xiaogang Wang, and Hongsheng Li. 2019. Improving referring expression grounding with cross-modal attention- guided erasing. In Proceedings of the IEEE Confer- ence on Computer Vision and Pattern Recognition, pages 1950-1959.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Hierarchical question-image co-attention for visual question answering",
                "authors": [
                    {
                        "first": "Jiasen",
                        "middle": [],
                        "last": "Lu",
                        "suffix": ""
                    },
                    {
                        "first": "Jianwei",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Dhruv",
                        "middle": [],
                        "last": "Batra",
                        "suffix": ""
                    },
                    {
                        "first": "Devi",
                        "middle": [],
                        "last": "Parikh",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Advances In Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "289--297",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. 2016. Hierarchical question-image co-attention for visual question answering. In Advances In Neural Information Processing Systems, pages 289-297.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Improved fusion of visual and language representations by dense symmetric co-attention for visual question answering",
                "authors": [
                    {
                        "first": "Duy-Kien",
                        "middle": [],
                        "last": "Nguyen",
                        "suffix": ""
                    },
                    {
                        "first": "Takayuki",
                        "middle": [],
                        "last": "Okatani",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": "",
                "issue": "",
                "pages": "6087--6096",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Duy-Kien Nguyen and Takayuki Okatani. 2018. Im- proved fusion of visual and language representations by dense symmetric co-attention for visual question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6087-6096.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Areas of attention for image captioning",
                "authors": [
                    {
                        "first": "Marco",
                        "middle": [],
                        "last": "Pedersoli",
                        "suffix": ""
                    },
                    {
                        "first": "Thomas",
                        "middle": [],
                        "last": "Lucas",
                        "suffix": ""
                    },
                    {
                        "first": "Cordelia",
                        "middle": [],
                        "last": "Schmid",
                        "suffix": ""
                    },
                    {
                        "first": "Jakob",
                        "middle": [],
                        "last": "Verbeek",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the IEEE International Conference on Computer Vision",
                "volume": "",
                "issue": "",
                "pages": "1242--1250",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Marco Pedersoli, Thomas Lucas, Cordelia Schmid, and Jakob Verbeek. 2017. Areas of attention for image captioning. In Proceedings of the IEEE Interna- tional Conference on Computer Vision, pages 1242- 1250.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Glove: Global vectors for word representation",
                "authors": [
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Pennington",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)",
                "volume": "",
                "issue": "",
                "pages": "1532--1543",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word rep- resentation. In Proceedings of the 2014 conference on empirical methods in natural language process- ing (EMNLP), pages 1532-1543.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Overcoming language priors in visual question answering with adversarial regularization",
                "authors": [
                    {
                        "first": "Aishwarya",
                        "middle": [],
                        "last": "Sainandan Ramakrishnan",
                        "suffix": ""
                    },
                    {
                        "first": "Stefan",
                        "middle": [],
                        "last": "Agrawal",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "1541--1551",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sainandan Ramakrishnan, Aishwarya Agrawal, and Stefan Lee. 2018. Overcoming language priors in visual question answering with adversarial regular- ization. In Advances in Neural Information Process- ing Systems, pages 1541-1551.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Faster r-cnn: Towards real-time object detection with region proposal networks",
                "authors": [
                    {
                        "first": "Kaiming",
                        "middle": [],
                        "last": "Shaoqing Ren",
                        "suffix": ""
                    },
                    {
                        "first": "Ross",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Jian",
                        "middle": [],
                        "last": "Girshick",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Advances in neural information processing systems",
                "volume": "",
                "issue": "",
                "pages": "91--99",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2015. Faster r-cnn: Towards real-time ob- ject detection with region proposal networks. In Advances in neural information processing systems, pages 91-99.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Answer them all! toward universal visual question answering models",
                "authors": [
                    {
                        "first": "Christopher",
                        "middle": [],
                        "last": "Kanan",
                        "suffix": ""
                    },
                    {
                        "first": "Robik",
                        "middle": [],
                        "last": "Shrestha",
                        "suffix": ""
                    },
                    {
                        "first": "Kushal",
                        "middle": [],
                        "last": "Kafle",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": "",
                "issue": "",
                "pages": "10472--10481",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Christopher Kanan Robik Shrestha, Kushal Kafle. 2019. Answer them all! toward universal visual question answering models. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 10472-10481.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Where to look: Focus regions for visual question answering",
                "authors": [
                    {
                        "first": "Kevin",
                        "middle": [
                            "J"
                        ],
                        "last": "Shih",
                        "suffix": ""
                    },
                    {
                        "first": "Saurabh",
                        "middle": [],
                        "last": "Singh",
                        "suffix": ""
                    },
                    {
                        "first": "Derek",
                        "middle": [],
                        "last": "Hoiem",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
                "volume": "",
                "issue": "",
                "pages": "4613--4621",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kevin J Shih, Saurabh Singh, and Derek Hoiem. 2016. Where to look: Focus regions for visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4613-4621.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Attention is all you need",
                "authors": [
                    {
                        "first": "Ashish",
                        "middle": [],
                        "last": "Vaswani",
                        "suffix": ""
                    },
                    {
                        "first": "Noam",
                        "middle": [],
                        "last": "Shazeer",
                        "suffix": ""
                    },
                    {
                        "first": "Niki",
                        "middle": [],
                        "last": "Parmar",
                        "suffix": ""
                    },
                    {
                        "first": "Jakob",
                        "middle": [],
                        "last": "Uszkoreit",
                        "suffix": ""
                    },
                    {
                        "first": "Llion",
                        "middle": [],
                        "last": "Jones",
                        "suffix": ""
                    },
                    {
                        "first": "Aidan",
                        "middle": [
                            "N"
                        ],
                        "last": "Gomez",
                        "suffix": ""
                    },
                    {
                        "first": "\u0141ukasz",
                        "middle": [],
                        "last": "Kaiser",
                        "suffix": ""
                    },
                    {
                        "first": "Illia",
                        "middle": [],
                        "last": "Polosukhin",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Advances in neural information processing systems",
                "volume": "",
                "issue": "",
                "pages": "5998--6008",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is al- l you need. In Advances in neural information pro- cessing systems, pages 5998-6008.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Scene graph generation by iterative message passing",
                "authors": [
                    {
                        "first": "Danfei",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Yuke",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "B"
                        ],
                        "last": "Choy",
                        "suffix": ""
                    },
                    {
                        "first": "Li",
                        "middle": [],
                        "last": "Fei-Fei",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": "",
                "issue": "",
                "pages": "5410--5419",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Danfei Xu, Yuke Zhu, Christopher B Choy, and Li Fei- Fei. 2017. Scene graph generation by iterative mes- sage passing. In Proceedings of the IEEE Confer- ence on Computer Vision and Pattern Recognition, pages 5410-5419.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Graph r-cnn for scene graph generation",
                "authors": [
                    {
                        "first": "Jianwei",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Jiasen",
                        "middle": [],
                        "last": "Lu",
                        "suffix": ""
                    },
                    {
                        "first": "Stefan",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Dhruv",
                        "middle": [],
                        "last": "Batra",
                        "suffix": ""
                    },
                    {
                        "first": "Devi",
                        "middle": [],
                        "last": "Parikh",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the European Conference on Computer Vision (ECCV)",
                "volume": "",
                "issue": "",
                "pages": "670--685",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jianwei Yang, Jiasen Lu, Stefan Lee, Dhruv Batra, and Devi Parikh. 2018. Graph r-cnn for scene graph gen- eration. In Proceedings of the European Conference on Computer Vision (ECCV), pages 670-685.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "Stacked attention networks for image question answering",
                "authors": [
                    {
                        "first": "Zichao",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaodong",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Jianfeng",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Li",
                        "middle": [],
                        "last": "Deng",
                        "suffix": ""
                    },
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Smola",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
                "volume": "",
                "issue": "",
                "pages": "21--29",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola. 2016. Stacked attention network- s for image question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 21-29.",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "Graph convolutional networks for text classification",
                "authors": [
                    {
                        "first": "Liang",
                        "middle": [],
                        "last": "Yao",
                        "suffix": ""
                    },
                    {
                        "first": "Chengsheng",
                        "middle": [],
                        "last": "Mao",
                        "suffix": ""
                    },
                    {
                        "first": "Yuan",
                        "middle": [],
                        "last": "Luo",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "volume": "33",
                "issue": "",
                "pages": "7370--7377",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Liang Yao, Chengsheng Mao, and Yuan Luo. 2019. Graph convolutional networks for text classification. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 7370-7377.",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "Exploring visual relationship for image captioning",
                "authors": [
                    {
                        "first": "Ting",
                        "middle": [],
                        "last": "Yao",
                        "suffix": ""
                    },
                    {
                        "first": "Yingwei",
                        "middle": [],
                        "last": "Pan",
                        "suffix": ""
                    },
                    {
                        "first": "Yehao",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Tao",
                        "middle": [],
                        "last": "Mei",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the European Conference on Computer Vision (ECCV)",
                "volume": "",
                "issue": "",
                "pages": "684--699",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ting Yao, Yingwei Pan, Yehao Li, and Tao Mei. 2018. Exploring visual relationship for image captioning. In Proceedings of the European Conference on Com- puter Vision (ECCV), pages 684-699.",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "Deep modular co-attention networks for visual question answering",
                "authors": [
                    {
                        "first": "Zhou",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Jun",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Yuhao",
                        "middle": [],
                        "last": "Cui",
                        "suffix": ""
                    },
                    {
                        "first": "Dacheng",
                        "middle": [],
                        "last": "Tao",
                        "suffix": ""
                    },
                    {
                        "first": "Qi",
                        "middle": [],
                        "last": "Tian",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": "",
                "issue": "",
                "pages": "6281--6290",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and Qi Tian. 2019. Deep modular co-attention networks for visual question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6281-6290.",
                "links": null
            },
            "BIBREF34": {
                "ref_id": "b34",
                "title": "Multi-modal factorized bilinear pooling with co-attention learning for visual question answering",
                "authors": [
                    {
                        "first": "Zhou",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Jun",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Jianping",
                        "middle": [],
                        "last": "Fan",
                        "suffix": ""
                    },
                    {
                        "first": "Dacheng",
                        "middle": [],
                        "last": "Tao",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the IEEE international conference on computer vision",
                "volume": "",
                "issue": "",
                "pages": "1821--1830",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zhou Yu, Jun Yu, Jianping Fan, and Dacheng Tao. 2017. Multi-modal factorized bilinear pooling with co-attention learning for visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 1821-1830.",
                "links": null
            },
            "BIBREF35": {
                "ref_id": "b35",
                "title": "Beyond bilinear: Generalized multimodal factorized high-order pooling for visual question answering",
                "authors": [
                    {
                        "first": "Zhou",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Jun",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Chenchao",
                        "middle": [],
                        "last": "Xiang",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "IEEE transactions on neural networks and learning systems",
                "volume": "29",
                "issue": "12",
                "pages": "5947--5959",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zhou Yu, Jun Yu, Chenchao Xiang, Jianping Fan, and Dacheng Tao. 2018. Beyond bilinear: Generalized multimodal factorized high-order pooling for visual question answering. IEEE transactions on neural networks and learning systems, 29(12):5947-5959.",
                "links": null
            },
            "BIBREF36": {
                "ref_id": "b36",
                "title": "Learning to count objects in natural images for visual question answering",
                "authors": [
                    {
                        "first": "Yan",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Jonathon",
                        "middle": [],
                        "last": "Hare",
                        "suffix": ""
                    },
                    {
                        "first": "Adam",
                        "middle": [],
                        "last": "Pr\u00fcgel-Bennett",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yan Zhang, Jonathon Hare, and Adam Pr\u00fcgel-Bennett. 2018a. Learning to count objects in natural images for visual question answering. International Confer- ence on Learning Representation (ICLR).",
                "links": null
            },
            "BIBREF37": {
                "ref_id": "b37",
                "title": "Graph convolution over pruned dependency trees improves relation extraction",
                "authors": [
                    {
                        "first": "Yuhao",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Peng",
                        "middle": [],
                        "last": "Qi",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 2018 conference on empirical methods in natural language processing (EMNLP)",
                "volume": "",
                "issue": "",
                "pages": "2205--2215",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yuhao Zhang, Peng Qi, and Christopher D Manning. 2018b. Graph convolution over pruned dependency trees improves relation extraction. In Proceedings of the 2018 conference on empirical methods in natural language processing (EMNLP), pages 2205-2215.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 1: (a) The question and the ground true answer. (b) The wrong answer is predicted by a state-of-the-art model, which focuses on the highlighted region in the image. The depth of the color indicates the weights of the words in the question, where deeper color represents higher weight. The question is performed by syntactic dependency parsing. (c) The dependency parsing of the question is obtained by the universal Standford Dependencies tool (De Marneffe et al., 2014).",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Figure 2: Illustration of our proposed Dual Channel Graph Convolutional Network (DC-GCN) for VQA task. The Dependency Parsing constructs the semantic relations between words in a question, and Q-GCN Module updates every word's features by aggregating the adjacent word features. In addition, the I-GCN Module builds the relations between image objects, and the Attention Alignment Module use question-guided image attention mechanism to learn a new object representation thus align the images and questions. All punctuations and upper cases have been preprocessed. The numbers in red are the weight scores of image objects and words.",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "(b). Each edge represents a relation between two object regions. Pruned Image Graph with Spatial Relations Spatial relations represent an object position in an image, which correspond to a 4-dimensional spatial coordinate [x 1 , y 1 , x 2 , y 2 ]. Note that (x 1 , y 1 ) is the coordinate of the top-left point of the bounding box and (x 2 , y 2 ) is the coordinate of the bottom-right point of the bounding box.",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF3": {
                "num": null,
                "text": "Figure 3: (a) Generate region proposals by pretrained model (Anderson et al., 2018). For display purposes, we only highlight some object regions. (b) Construct the relations between objects. (c) Prune the irrelevant object edges and calculate the weight between objects. The numbers in red are the weights of edges.",
                "uris": null,
                "fig_num": "3",
                "type_str": "figure"
            },
            "FIGREF4": {
                "num": null,
                "text": "Figure 4: The question is performed by syntactic dependency parsing. The word is is the root node of dependency relations while the words in blue (e.g., det, dobj) are dependency relations. The direction of arrow indicates that two words exist a relation. the umpire's shirt is parsed to obtain the relations between words (e.g., cop, det and nmod), as shown in Figure 4. The words in blue are the dependency relations. The ending of arrow indicates that this word is a modifier. The word root in purple is used to indicate which word is the root node of dependency relations.Question Fully-connected Relations Graph By treating each word in a question as a node, we construct a fully-connected undirected graph, as shown in Figure5(a). Each edge represents a relation between two words. Pruned Question Graph with Dependency Relations Irrelevant relations between two words may bring noises. Therefore, we need to prune some unrelated relations to reduce the noises. By parsing the dependency relations of a question, we obtain the relations between words (cf. Figure4). According to dependency relations, we prune some edges between two nodes which do not have dependency relations. A sparse graph is obtained, as shown in Figure5(b).",
                "uris": null,
                "fig_num": "4",
                "type_str": "figure"
            },
            "FIGREF5": {
                "num": null,
                "text": "Figure 5: (a) A fully-connected graph network is built where each word is a node and each word may have relations with other words. (b) the Stanford Syntactic Parsing tool (De Marneffe et al., 2014) is used to obtain the dependency relations between words. According to these relations, we can prune the unrelated edges and obtain a sparse graph. (c) The numbers in red are the weight scores. For the node umpire's, the weight of word the is 0.1 while the weight of word shirt is 0.9. The weight scores reflect the importance of words. The phrase umpires's shirt describes an object, thus the word shirt is more important than word the.",
                "uris": null,
                "fig_num": "5",
                "type_str": "figure"
            },
            "FIGREF6": {
                "num": null,
                "text": "Figure6: Visualizations of the learned attention maps of the Q-GCN module, I-GCN module and Attention Alignment module from some typical layers. We regard the correlation score between nodes as the attention score. Q-GCN(l) and I-GCN(l) denote the question GCN attention maps and image GCN attention maps from the l-th layer, respectively, as shown in (a), (b), (c) and (d). And (e) and (f) mean the question-guided image attention weight of Attention Alignment module in l-th layer. For the sake of presentation, we only consider 20 object regions in an image. The index within[1, 20]  shown on the axes of the attention maps corresponds to each object in the image. For better visualization effect, we highlight in the image three objects which correspond to 4-th, 6-th, 9-th, and 12-th objects, respectively.",
                "uris": null,
                "fig_num": "6",
                "type_str": "figure"
            },
            "FIGREF7": {
                "num": null,
                "text": "Figure 7: We summarize three types of incorrect examples: limitation of object detection, text semantic understanding and subjective judgment which correspond to (a), (b), and (c), respectively. of object detection. The image feature extractor is based on Faster R-CNN model (Ren et al., 2015). The accuracy of object detection can indirectly affect the accuracy of feature extraction. Counting subtask in VQA task has a large room to improve. In Figure 7(b), the question what time should you pay can be answered by recognizing the text semantic understanding in the image. Text semantic understanding belongs to another task, namely text visual question answering(Biten et al., 2019), which requires to recognize the numbers, symbols and proper nouns in a scene. In Figure7(c), subjective judgment is needed to answer the question is this man happy. Making this judgment requires some common sense knowledge and real life experience. Specifically, someone holding a banana against him and just like holding a gun towards him, so he is unhappy. Our model can not make such analysis like a human being done to make a subjective judgment and predict the correct answer yes.",
                "uris": null,
                "fig_num": "7",
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table><tr><td>Relations</td><td>Relation Description</td></tr><tr><td>det</td><td>determiner</td></tr><tr><td>nsubj</td><td>nominal subject</td></tr><tr><td>case</td><td>prepositions, postpositions</td></tr><tr><td>nmod</td><td>nominal modifier</td></tr><tr><td>cop</td><td>copula</td></tr><tr><td>dobj</td><td>direct object</td></tr><tr><td>amod</td><td>adjective modifier</td></tr><tr><td>aux</td><td>auxiliary</td></tr><tr><td>advmod</td><td>adverbial modifier</td></tr><tr><td>compound</td><td>compound</td></tr><tr><td>dep</td><td>dependent</td></tr><tr><td>acl</td><td>claussal modifier of noun</td></tr><tr><td>nsubjpass</td><td>possive nominal subject</td></tr><tr><td>auxpass</td><td>passive auxiliary</td></tr><tr><td>root</td><td>root node</td></tr></table>",
                "type_str": "table",
                "text": "The main categories of relations classified by the dependency parsing tool(De Marneffe et al., 2014).",
                "html": null,
                "num": null
            },
            "TABREF1": {
                "content": "<table><tr><td>4 Experiments</td></tr><tr><td>4.1 Datasets</td></tr><tr><td>VQA-v2 (Goyal et al., 2017) is the most common-</td></tr><tr><td>ly used VQA benchmark dataset which is split in-</td></tr><tr><td>to train, val, and test-standard sets. Among test-</td></tr><tr><td>standard set, 25% are served as test-dev set. Each</td></tr><tr><td>question has 10 answers from different annotators.</td></tr><tr><td>Answers with the highest frequency are treated as</td></tr><tr><td>the ground truth. All answer types can be divided</td></tr><tr><td>into Yes/No, Number, and Other. VQA-CP-v2 (A-</td></tr><tr><td>grawal et al., 2018) is a derivation of the VQA-v2</td></tr><tr><td>dataset, which is introduced to evaluate and reduce</td></tr><tr><td>the question-oriented bias in VQA models. Due to</td></tr><tr><td>significant difference of distribution between train</td></tr><tr><td>set and test set, the VQA-CP-v2 dataset is harder</td></tr><tr><td>than VQA-v2 dataset.</td></tr></table>",
                "type_str": "table",
                "text": "and b e are learned parameters, and pred means the probability of the classified answers from the set of answer vocabulary which contains M candidate answers. Following(Yu et al., 2019), we use binary cross-entropy loss function to train an answer classifier.",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table/>",
                "type_str": "table",
                "text": "",
                "html": null,
                "num": null
            },
            "TABREF5": {
                "content": "<table><tr><td>: Model accuracy on the VQA-CP-v2 bench-</td></tr><tr><td>mark (open-ended setting on the test split). The re-</td></tr><tr><td>sults of models with * and ** are obtained from the</td></tr><tr><td>work (Robik Shrestha, 2019) and (Ramakrishnan et al.,</td></tr><tr><td>2018), respectively. Models with # are designed for</td></tr><tr><td>solving the language biases. The ReGAT model con-</td></tr><tr><td>sists of Semantic (Sem), Implicit (Imp), and Spatial (S-</td></tr><tr><td>pa) relation encoder.</td></tr><tr><td>vantage over UpDn + Q-Adv + DoE model. The</td></tr><tr><td>results on VQA-CP-v2 dataset show that depen-</td></tr><tr><td>dency parsing and DC-GCN can effectively reduce</td></tr><tr><td>question-based overfitting.</td></tr></table>",
                "type_str": "table",
                "text": "",
                "html": null,
                "num": null
            },
            "TABREF6": {
                "content": "<table><tr><td>Component</td><td>Setting</td><td>Acc. (%)</td></tr><tr><td>Bottom-Up (Anderson et al., 2018)</td><td>Bottom-Up</td><td>63.15</td></tr><tr><td>Default</td><td>DC-GCN</td><td>66.57</td></tr><tr><td/><td>DC-GCN</td><td>66.57</td></tr><tr><td>GCN Types</td><td>w/o I-GCN</td><td>65.52</td></tr><tr><td/><td>w/o Q-GCN</td><td>66.15</td></tr><tr><td/><td>-det</td><td>66.50</td></tr><tr><td/><td>-case</td><td>66.42</td></tr><tr><td/><td>-cop</td><td>66.01</td></tr><tr><td>Dependency</td><td>-aux</td><td>66.48</td></tr><tr><td>relations</td><td>-advmod</td><td>66.53</td></tr><tr><td/><td>-compound</td><td>66.35</td></tr><tr><td/><td>-det case</td><td>65.23</td></tr><tr><td/><td>-det case cop</td><td>64.11</td></tr></table>",
                "type_str": "table",
                "text": ". The experimental results are based on one black of our DC-GCN model. All modules inside DC-GCN have the same dimension of 512. The learning rate is 0.0001 and the batch size is 32.",
                "html": null,
                "num": null
            },
            "TABREF7": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Ablation studies of our proposed model on VQA-v2 validation dataset. The experimental results are based on one black of our DC-GCN model. w/o means removing a certain module from DC-GCN model. The detailed descriptions about dependency relations are shown on Table 1.",
                "html": null,
                "num": null
            }
        }
    }
}