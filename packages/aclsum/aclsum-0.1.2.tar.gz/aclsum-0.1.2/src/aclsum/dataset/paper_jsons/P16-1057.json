{
    "paper_id": "P16-1057",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T11:59:02.393550Z"
    },
    "title": "Latent Predictor Networks for Code Generation",
    "authors": [
        {
            "first": "Wang",
            "middle": [],
            "last": "Ling",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Oxford",
                "location": {}
            },
            "email": "lingwang@google.com"
        },
        {
            "first": "\u2666",
            "middle": [
                "Edward"
            ],
            "last": "Grefenstette",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Oxford",
                "location": {}
            },
            "email": ""
        },
        {
            "first": "Karl",
            "middle": [],
            "last": "Moritz",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Oxford",
                "location": {}
            },
            "email": ""
        },
        {
            "first": "Hermann",
            "middle": [
                "\u2666"
            ],
            "last": "Tom\u00e1\u0161 Ko\u010disk\u00fd",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Oxford",
                "location": {}
            },
            "email": ""
        },
        {
            "first": "Andrew",
            "middle": [],
            "last": "Senior",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Oxford",
                "location": {}
            },
            "email": "andrewsenior@google.com"
        },
        {
            "first": "Fumin",
            "middle": [],
            "last": "Wang",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Oxford",
                "location": {}
            },
            "email": ""
        },
        {
            "first": "Phil",
            "middle": [],
            "last": "Blunsom",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Oxford",
                "location": {}
            },
            "email": "pblunsom@google.com"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Many language generation tasks require the production of text conditioned on both structured and unstructured inputs. We present a novel neural network architecture which generates an output sequence conditioned on an arbitrary number of input functions. Crucially, our approach allows both the choice of conditioning context and the granularity of generation, for example characters or tokens, to be marginalised, thus permitting scalable and effective training. Using this framework, we address the problem of generating programming code from a mixed natural language and structured specification. We create two new data sets for this paradigm derived from the collectible trading card games Magic the Gathering and Hearthstone. On these, and a third preexisting corpus, we demonstrate that marginalising multiple predictors allows our model to outperform strong benchmarks.",
    "pdf_parse": {
        "paper_id": "P16-1057",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Many language generation tasks require the production of text conditioned on both structured and unstructured inputs. We present a novel neural network architecture which generates an output sequence conditioned on an arbitrary number of input functions. Crucially, our approach allows both the choice of conditioning context and the granularity of generation, for example characters or tokens, to be marginalised, thus permitting scalable and effective training. Using this framework, we address the problem of generating programming code from a mixed natural language and structured specification. We create two new data sets for this paradigm derived from the collectible trading card games Magic the Gathering and Hearthstone. On these, and a third preexisting corpus, we demonstrate that marginalising multiple predictors allows our model to outperform strong benchmarks.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "The generation of both natural and formal languages often requires models conditioned on diverse predictors (Koehn et al., 2007; Wong and Mooney, 2006) . Most models take the restrictive approach of employing a single predictor, such as a word softmax, to predict all tokens of the output sequence. To illustrate its limitation, suppose we wish to generate the answer to the question \"Who wrote The Foundation?\" as \"The Foundation was written by Isaac Asimov\". The generation of the words \"Issac Asimov\" and \"The Foundation\" from a word softmax trained on annotated data is unlikely to succeed as these words are sparse. A robust model might, for example, employ one pre- dictor to copy \"The Foundation\" from the input, and a another one to find the answer \"Issac Asimov\" by searching through a database. However, training multiple predictors is in itself a challenging task, as no annotation exists regarding the predictor used to generate each output token. Furthermore, predictors generate segments of different granularity, as database queries can generate multiple tokens while a word softmax generates a single token. In this work we introduce Latent Predictor Networks (LPNs), a novel neural architecture that fulfills these desiderata: at the core of the architecture is the exact computation of the marginal likelihood over latent predictors and generated segments allowing for scalable training.",
                "cite_spans": [
                    {
                        "start": 108,
                        "end": 128,
                        "text": "(Koehn et al., 2007;",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 129,
                        "end": 151,
                        "text": "Wong and Mooney, 2006)",
                        "ref_id": "BIBREF32"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "We introduce a new corpus for the automatic generation of code for cards in Trading Card Games (TCGs), on which we validate our model1 . TCGs, such as Magic the Gathering (MTG) and Hearthstone (HS), are games played between two players that build decks from an ever expanding pool of cards. Examples of such cards are shown in Figure 1 . Each card is identified by its attributes (e.g., name and cost) and has an effect that is described in a text box. Digital implementations of these games implement the game logic, which includes the card effects. This is attractive from a data extraction perspective as not only are the data annotations naturally generated, but we can also view the card as a specification communicated from a designer to a software engineer.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 334,
                        "end": 335,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "This dataset presents additional challenges to prior work in code generation (Wong and Mooney, 2006; Jones et al., 2012; Lei et al., 2013; Artzi et al., 2015; Quirk et al., 2015) , including the handling of structured input-i.e. cards are composed by multiple sequences (e.g., name and description)-and attributes (e.g., attack and cost), and the length of the generated sequences. Thus, we propose an extension to attention-based neural models (Bahdanau et al., 2014) to attend over structured inputs. Finally, we propose a code compression method to reduce the size of the code without impacting the quality of the predictions.",
                "cite_spans": [
                    {
                        "start": 77,
                        "end": 100,
                        "text": "(Wong and Mooney, 2006;",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 101,
                        "end": 120,
                        "text": "Jones et al., 2012;",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 121,
                        "end": 138,
                        "text": "Lei et al., 2013;",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 139,
                        "end": 158,
                        "text": "Artzi et al., 2015;",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 159,
                        "end": 178,
                        "text": "Quirk et al., 2015)",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 445,
                        "end": 468,
                        "text": "(Bahdanau et al., 2014)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Experiments performed on our new datasets, and a further pre-existing one, suggest that our extensions outperform strong benchmarks.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The paper is structured as follows: We first describe the data collection process (Section 2) and formally define our problem and our baseline method (Section 3). Then, we propose our extensions, namely, the structured attention mechanism (Section 4) and the LPN architecture (Section 5). We follow with the description of our code compression algorithm (Section 6). Our model is validated by comparing with multiple benchmarks (Section 7). Finally, we contextualize our findings with related work (Section 8) and present the conclusions of this work (Section 9).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "We obtain data from open source implementations of two different TCGs, MTG in Java 2 and HS in Python. 3 The statistics of the corpora are illustrated in Table 1 . In both corpora, each card is implemented in a separate class file, which we strip of imports and comments. We categorize the content of each card into two different groups: singular fields that contain only one value; and text fields, which contain multiple words representing different units of meaning. In MTG, there are six singular fields (attack, defense, rarity, set, id, and Empty fields are replaced with a \"NIL\" token.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 160,
                        "end": 161,
                        "text": "1",
                        "ref_id": "TABREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Dataset Extraction",
                "sec_num": "2"
            },
            {
                "text": "The code for the HS card in Figure 1 is shown in Figure 2 . The effect of \"drawing cards until the player has as many cards as the opponent\" is implemented by computing the difference between the players' hands and invoking the draw method that number of times. This illustrates that the mapping between the description and the code is nonlinear, as no information is given in the text regarding the specifics of the implementation. ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 35,
                        "end": 36,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    },
                    {
                        "start": 56,
                        "end": 57,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Dataset Extraction",
                "sec_num": "2"
            },
            {
                "text": "Given the description of a card x, our decoding problem is to find the code \u0177 so that:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Problem Definition",
                "sec_num": "3"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "\u0177 = argmax y log P (y | x)",
                        "eq_num": "(1)"
                    }
                ],
                "section": "Problem Definition",
                "sec_num": "3"
            },
            {
                "text": "Here log P (y | x) is estimated by a given model. We define y = y 1 ..y |y| as the sequence of characters of the code with length |y|. We index each input field with k = 1..|x|, where |x| quantifies the number of input fields. |x k | denotes the number of tokens in x k and x ki selects the i-th token.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Problem Definition",
                "sec_num": "3"
            },
            {
                "text": "Background When |x| = 1, the attention model of Bahdanau et al. (2014) applies. Following the chain rule, log P (y|x) = t=1..|y| log P (y t |y 1 ..y t-1 , x), each token y t is predicted conditioned on the previously generated sequence y 1 ..y t-1 and input sequence x 1 = x 11 ..x 1|x 1 | . Probability are estimated with a softmax over the vocabulary Y :",
                "cite_spans": [
                    {
                        "start": 48,
                        "end": 70,
                        "text": "Bahdanau et al. (2014)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Structured Attention",
                "sec_num": "4"
            },
            {
                "text": "p(y t |y 1 ..y t-1 , x 1 ) = softmax yt\u2208Y (h t ) (2)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Structured Attention",
                "sec_num": "4"
            },
            {
                "text": "where h t is the Recurrent Neural Network (RNN) state at time stamp t, which is modeled as g(y t-1 , h t-1 , z t ). g(\u2022) is a recurrent update function for generating the new state h t based on the previous token y t-1 , the previous state h t-1 , and the input text representation z t . We implement g using a Long Short-Term Memory (LSTM) RNNs (Hochreiter and Schmidhuber, 1997) .",
                "cite_spans": [
                    {
                        "start": 346,
                        "end": 380,
                        "text": "(Hochreiter and Schmidhuber, 1997)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Structured Attention",
                "sec_num": "4"
            },
            {
                "text": "The attention mechanism generates the representation of the input sequence x = x 11 ..x 1|x 1 | , and z t is computed as the weighted sum z t = i=1..|x 1 | a i h(x 1i ), where a i is the attention coefficient obtained for token x 1i and h is a function that maps each x 1i to a continuous vector. In general, h is a function that projects x 1i by learning a lookup table, and then embedding contextual words by defining an RNN. Coefficients a i are computed with a softmax over input tokens x 11 ..x 1|x 1 | :",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Structured Attention",
                "sec_num": "4"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "a i = softmax x 1i \u2208x (v(h(x 1i ), h t-1 ))",
                        "eq_num": "(3)"
                    }
                ],
                "section": "Structured Attention",
                "sec_num": "4"
            },
            {
                "text": "Function v computes the affinity of each token x 1i and the current output context h t-1 . A common implementation of v is to apply a linear projection from h(x 1i ) : h t-1 (where : is the concatenation operation) into a fixed size vector, followed by a tanh and another linear projection.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Structured Attention",
                "sec_num": "4"
            },
            {
                "text": "Our Approach We extend the computation of z t for cases when x corresponds to multiple fields. Figure 3 illustrates how the MTG card \"Serra Angel\" is encoded, assuming that there are two singular fields and one text field. We first encode each token x ki using the C2W model described in Ling et al. (2015) , which is a replacement for lookup tables where word representations are learned at the character level (cf. C2W row). A context-aware representation is built for words in the text fields using a bidirectional LSTM (cf. Bi-LSTM row).",
                "cite_spans": [
                    {
                        "start": 288,
                        "end": 306,
                        "text": "Ling et al. (2015)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 102,
                        "end": 103,
                        "text": "3",
                        "ref_id": "FIGREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Structured Attention",
                "sec_num": "4"
            },
            {
                "text": "Computing attention over multiple input fields is problematic as each input field's vectors have different sizes and value ranges. Thus, we learn a linear projection mapping each input token x ki to a vector with a common dimensionality and value range (cf. Linear row). Denoting this process as f (x ki ), we extend Equation 3 as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Structured Attention",
                "sec_num": "4"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "a ki = softmax x ki \u2208x (v(f (x ki ), h t-1 ))",
                        "eq_num": "(4)"
                    }
                ],
                "section": "Structured Attention",
                "sec_num": "4"
            },
            {
                "text": "Here a scalar coefficient a ki is computed for each input token x ki (cf. \"Tanh\", \"Linear\", and \"Softmax\" rows). Thus, the overall input representation z t is computed as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Structured Attention",
                "sec_num": "4"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "z t = k=1..|x|,i=1..|x k | a ij f (x ki )",
                        "eq_num": "(5)"
                    }
                ],
                "section": "Structured Attention",
                "sec_num": "4"
            },
            {
                "text": "5 Latent Predictor Networks",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Structured Attention",
                "sec_num": "4"
            },
            {
                "text": "Background In order to decode from x to y, many words must be copied into the code, such as the name of the card, the attack and the cost values. If we observe the HS card in Figure 1 and the respective code in Figure 2 , we observe that the name \"Divine Favor\" must be copied into the class name and in the constructor, along with the cost of the card \"3\". As explained earlier, this problem is not specific to our task: for instance, in the dataset of Oda et al. (2015) , a model must learn to map from timeout = int ( timeout ) to \"convert timeout into an integer.\", where the name of the variable \"timeout\" must be copied into the output sequence. The same issue exists for proper nouns in machine translation which are typically copied from one language to the other. Pointer networks (Vinyals et al., 2015) address this by defining a probability distribution over a set of units that can be copied c = c 1 ..c |c| .",
                "cite_spans": [
                    {
                        "start": 454,
                        "end": 471,
                        "text": "Oda et al. (2015)",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 790,
                        "end": 812,
                        "text": "(Vinyals et al., 2015)",
                        "ref_id": "BIBREF31"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 182,
                        "end": 183,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    },
                    {
                        "start": 218,
                        "end": 219,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Structured Attention",
                "sec_num": "4"
            },
            {
                "text": "The probability of copying a unit c i is modeled as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Structured Attention",
                "sec_num": "4"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "p(c i ) = softmax c i \u2208c (v(h(c i ), q))",
                        "eq_num": "(6)"
                    }
                ],
                "section": "Structured Attention",
                "sec_num": "4"
            },
            {
                "text": "As in the attention model (Equation 3), v is a function that computes the affinity between an embedded copyable unit h(c i ) and an arbitrary vector q.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Structured Attention",
                "sec_num": "4"
            },
            {
                "text": "Our Approach Combining pointer networks with a character-based softmax is in itself difficult as these generate segments of different granularity and there is no ground truth of which predictor to use at each time stamp. We now describe Latent Predictor Networks, which model the conditional probability log P (y|x) over the latent sequence of predictors used to generate y.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Structured Attention",
                "sec_num": "4"
            },
            {
                "text": "We assume that our model uses multiple predictors r \u2208 R, where each r can generate multiple segments s t = y t ..y t+|st|-1 with arbitrary length |s t | at time stamp t. An example is illustrated in Figure 4 , where we observe that to generate the code init('Tirion Fordring ',8,6,6) , a pointer network can be used to generate the sequences y 13 7 =Tirion and y 22 14 =Fordring (cf. \"Copy From Name\" row). These sequences can also be generated using a character softmax (cf. \"Generate Characters\" row). The same applies to the generation of the attack, health and cost values as each of these predictors is an element in R. Thus, we define our objective function as a marginal log likelihood function over a latent variable \u03c9:",
                "cite_spans": [
                    {
                        "start": 275,
                        "end": 283,
                        "text": "',8,6,6)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 206,
                        "end": 207,
                        "text": "4",
                        "ref_id": "FIGREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Structured Attention",
                "sec_num": "4"
            },
            {
                "text": "log P (y | x) = log \u03c9\u2208\u03c9 P (y, \u03c9 | x) (7)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Structured Attention",
                "sec_num": "4"
            },
            {
                "text": "Formally, \u03c9 is a sequence of pairs r t , s t , where r t \u2208 R denotes the predictor that is used at timestamp t and s t the generated string. We decompose P (y, \u03c9 | x) as the product of the probabilities of segments s t and predictors r t :",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Structured Attention",
                "sec_num": "4"
            },
            {
                "text": "P (y, \u03c9 | x) = rt,st\u2208\u03c9 P (s t , r t | y 1 ..y t-1 , x) = rt,st\u2208\u03c9 P (s t | y 1 ..y t-1 , x, r t )P (r t | y 1 ..y t-1 , x)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Structured Attention",
                "sec_num": "4"
            },
            {
                "text": "where the generation of each segment is performed in two steps: select the predictor r t with probability P (r t | y 1 ..y t-1 , x) and then generate s t conditioned on predictor r t with probability log P (s t | y 1 ..y t-1 , x, r t ). The probability of each predictor is computed using a softmax over all predictors in R conditioned on the previous state h t-1 and the input representation z t (cf. \"Select Predictor\" box). Then, the probability of generating the segment s t depends on the predictor type. We define three types of predictors:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Structured Attention",
                "sec_num": "4"
            },
            {
                "text": "Character Generation Generate a single character from observed characters from the training data. Only one character is generated at each time stamp with probability given by Equation 2.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Structured Attention",
                "sec_num": "4"
            },
            {
                "text": "Copy Singular Field For singular fields only the field itself can be copied, for instance, the value of the attack and cost attributes or the type of card. The size of the generated segment is the number of characters in the copied field and the segment is generated with probability 1.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Structured Attention",
                "sec_num": "4"
            },
            {
                "text": "Copy Text Field For text fields, we allow each of the words x ki within the field to be copied. The probability of copying a word is learned with a pointer network (cf. \"Copy From Name\" box), where h(c i ) is set to the representation of the word f (x ki ) and q is the concatenation h t-1 : z t of the state and input vectors. This predictor generates a segment with the size of the copied word.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Structured Attention",
                "sec_num": "4"
            },
            {
                "text": "It is important to note that the state vector h t-1 is generated by building an RNN over the sequence of characters up until the time stamp t -1, i.e. the previous context y t-1 is encoded at the character level. This allows the number of possible states to remain tractable at training time.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Structured Attention",
                "sec_num": "4"
            },
            {
                "text": "At training time we use back-propagation to maximize the probability of observed code, according to Equation 7. Gradient computation must be performed with respect to each computed probability P (r t | y 1 ..y t-1 , x) and P (s t | y 1 ..y t-1 , x, r t ). The derivative \u2202 log P (y|x) \u2202P (rt|y 1 ..y t-1 ,x) yields:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Inference",
                "sec_num": "5.1"
            },
            {
                "text": "\u2202\u03b1 t P (r t | y 1 ..y t-1 , x)\u03b2 t,rt + \u03be rt P (y | x)\u2202P (r t | y 1 ..y t-1 , x) = \u03b1 t \u03b2 t,rt \u03b1 |y|+1",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Inference",
                "sec_num": "5.1"
            },
            {
                "text": "Here \u03b1 t denotes the cumulative probability of all values of \u03c9 up until time stamp t and \u03b1 |y|+1 yields the marginal probability P (y | x). \u03b2 t,rt = P (s t | y 1 ..y t-1 )\u03b2 t+|st|-1 denotes the cumulative probability starting from predictor r t at time stamp t, exclusive. This includes the probability of the generated segment P (s t | y 1 ..y t-1 , x, r t ) and the probability of all values of \u03c9 starting from timestamp t+ |s t |-1, that is, all possible sequences that generate segment y after segment s t is produced. For completeness, \u03be r denotes the cumulative probabilities of all \u03c9 that do not include r t . To illustrate this, we refer to Figure 4 and consider the timestamp t = 14, where the segment s 14 =Fordring is generated. In this case, the cumulative probability \u03b1 14 is the sum of the path that generates the sequence init('Tirion with characters alone, and the path that generates the word Tirion by copying from the input. \u03b2 21 includes the probability of all paths that follow the generation of Fordring, which include 2\u00d73\u00d73 different paths due to the three decision points that follow (e.g. generating 8 using a character softmax vs. copying from the cost). Finally, \u03be r refers to the path that generates Fordring character by character.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 656,
                        "end": 657,
                        "text": "4",
                        "ref_id": "FIGREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Inference",
                "sec_num": "5.1"
            },
            {
                "text": "While the number of possible paths grows exponentially, \u03b1 and \u03b2 can be computed efficiently using the forward-backward algorithm for Semi-Markov models (Sarawagi and Cohen, 2005) , where we associate P (r t | y 1 ..y t-1 , x) to edges and P (s t | y 1 ..y t-1 , x, r t ) to nodes in the Markov chain.",
                "cite_spans": [
                    {
                        "start": 152,
                        "end": 178,
                        "text": "(Sarawagi and Cohen, 2005)",
                        "ref_id": "BIBREF27"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Inference",
                "sec_num": "5.1"
            },
            {
                "text": "The derivative",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Inference",
                "sec_num": "5.1"
            },
            {
                "text": "\u2202 log P (y|x)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Inference",
                "sec_num": "5.1"
            },
            {
                "text": "\u2202P (st|y 1 ..y t-1 ,x,rt) can be computed using the same logic:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Inference",
                "sec_num": "5.1"
            },
            {
                "text": "\u2202\u03b1 t,st P (s t | y 1 ..y t-1 , x, r t )\u03b2 t+|st|-1 + \u03be rt P (y | x)\u2202P (s t | y 1 ..y t-1 , x, r t ) = \u03b1 t,rt \u03b2 t+|st|-1 \u03b1 |y|+1",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Inference",
                "sec_num": "5.1"
            },
            {
                "text": "Once again, we denote \u03b1 t,rt = \u03b1 t P (r t | y 1 ..y t-1 , x) as the cumulative probability of all values of \u03c9 that lead to s t , exclusive.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Inference",
                "sec_num": "5.1"
            },
            {
                "text": "An intuitive interpretation of the derivatives is that gradient updates will be stronger on probability chains that are more likely to generate the output sequence. For instance, if the model learns a good predictor to copy names, such as Fordring, other predictors that can also generate the same sequences, such as the character softmax will allocate less capacity to the generation of names, and focus on elements that they excel at (e.g. generation of keywords).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Inference",
                "sec_num": "5.1"
            },
            {
                "text": "Decoding is performed using a stack-based decoder with beam search. Each state S corresponds to a choice of predictor r t and segment s t at a given time stamp t. This state is scored as V (S) = log P (s t | y 1 ..y t-1 , x, r t ) + log P (r t | y 1 ..y t-1 , x) + V (prev(S)), where prev(S) denotes the predecessor state of S. At each time stamp, the n states with the highest scores V are expanded, where n is the size of the beam. For each predictor r t , each output s t generates a new state. Finally, at each timestamp t, all states which produce the same output up to that point are merged by summing their probabilities.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Decoding",
                "sec_num": "5.2"
            },
            {
                "text": "As the attention-based model traverses all input units at each generation step, generation becomes quite expensive for datasets such as MTG where the average card code contains 1,080 characters. While this is not the essential contribution in our paper, we propose a simple method to compress the code while maintaining the structure of the code, allowing us to train on datasets with longer code (e.g., MTG).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Code Compression",
                "sec_num": "6"
            },
            {
                "text": "The idea behind that method is that many keywords in the programming language (e.g., public and return) as well as frequently used functions and classes (e.g., Card) can be learned without character level information. We exploit this by mapping such strings onto additional symbols X i (e.g., public class copy() \u2192 \"X 1 X 2 X 3 ()\"). Formally, we seek the string v among all strings V (max) up to length max that maximally reduces the size of the corpus:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Code Compression",
                "sec_num": "6"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "v = argmax v\u2208V (max) (len(v) -1)C(v)",
                        "eq_num": "(8)"
                    }
                ],
                "section": "Code Compression",
                "sec_num": "6"
            },
            {
                "text": "where C(v) is the number of occurrences of v in the training corpus and len(v) its length.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Code Compression",
                "sec_num": "6"
            },
            {
                "text": "(len(v) -1)C(v) can be seen as the number of characters reduced by replacing v with a nonterminal symbol. To find q(v) efficiently, we leverage the fact that",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Code Compression",
                "sec_num": "6"
            },
            {
                "text": "C(v) \u2264 C(v ) if v contains v . It follows that (max -1)C(v) \u2264 (max -1)C(v ),",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Code Compression",
                "sec_num": "6"
            },
            {
                "text": "which means that the maximum compression obtainable for v at size max is always lower than that of v . Thus, if we can find a v such that (len(v) -1)C(v) > (max -1)C(v ), that is v at the current size achieves a better compression rate than v at the maximum length, then it follows that all sequences that contain v can be discarded as candidates. Based on this idea, our iterative search starts by obtaining the counts C(v) for all segments of size s = 2, and computing the best scoring segment v. Then, we build a list L(s) of all segments that achieve a better compression rate than v at their maximum size. At size s + 1, only segments that contain a element in L(s -1) need to be considered, making the number of substrings to be tested to be tractable as s increases. The algorithm stops once s reaches max or the newly generated list L(s) contains no elements. Table 2 : First 10 compressed units in MTG. We replaced newlines with \u21d3 and spaces with .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 874,
                        "end": 875,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Code Compression",
                "sec_num": "6"
            },
            {
                "text": "Once v is obtained, we replace all occurrences of v with a new non-terminal symbol. This process is repeated until a desired average size for the code is reached. While training is performed on the compressed code, the decoding will undergo an additional step, where the compressed code is restored by expanding the all X i . Table 2 shows the first 10 replacements from the MTG dataset, reducing its average size from 1080 to 794.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 332,
                        "end": 333,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Code Compression",
                "sec_num": "6"
            },
            {
                "text": "Datasets Tests are performed on the two datasets provided in this paper, described in Table 1. Additionally, to test the model's ability of generalize to other domains, we report results in the Django dataset (Oda et al., 2015) , comprising of 16000 training, 1000 development and 1805 test annotations. Each data point consists of a line of Python code together with a manually created natural language description.",
                "cite_spans": [
                    {
                        "start": 209,
                        "end": 227,
                        "text": "(Oda et al., 2015)",
                        "ref_id": "BIBREF24"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "7"
            },
            {
                "text": "We implement two standard neural networks, namely a sequence-tosequence model (Sutskever et al., 2014) and an attention-based model (Bahdanau et al., 2014) . The former is adapted to work with multiple input fields by concatenating them, while the latter uses our proposed attention model. These models are denoted as \"Sequence\" and \"Attention\".",
                "cite_spans": [
                    {
                        "start": 78,
                        "end": 102,
                        "text": "(Sutskever et al., 2014)",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 132,
                        "end": 155,
                        "text": "(Bahdanau et al., 2014)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Neural Benchmarks",
                "sec_num": null
            },
            {
                "text": "Our problem can also be viewed in the framework of semantic parsing (Wong and Mooney, 2006; Lu et al., 2008; Jones et al., 2012; Artzi et al., 2015) . Unfortunately, these approaches define strong assumptions regarding the grammar and structure of the output, which makes it difficult to generalize for other domains (Kwiatkowski et al., 2010) . However, the work in Andreas et al. (2013) provides evidence that using machine translation systems without committing to such assumptions can lead to results competitive with the systems described above. We follow the same approach and create a phrase-based (Koehn et al., 2007) model and a hierarchical model (or PCFG) (Chiang, 2007) as benchmarks for the work presented here. As these models are optimized to generate words, not characters, we implement a tokenizer that splits on all punctuation characters, except for the \" \" character. We also facilitate the task by splitting Camel-Case words (e.g., class TirionFordring \u2192 class Tirion Fordring). Otherwise all class names would not be generated correctly by these methods. We used the models implemented in Moses to generate these baselines using standard parameters, using IBM Alignment Model 4 for word alignments (Och and Ney, 2003) , MERT for tuning (Sokolov and Yvon, 2011) and a 4-gram Kneser-Ney Smoothed language model (Heafield et al., 2013) . These models will be denoted as \"Phrase\" and \"Hierarchical\", respectively.",
                "cite_spans": [
                    {
                        "start": 68,
                        "end": 91,
                        "text": "(Wong and Mooney, 2006;",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 92,
                        "end": 108,
                        "text": "Lu et al., 2008;",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 109,
                        "end": 128,
                        "text": "Jones et al., 2012;",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 129,
                        "end": 148,
                        "text": "Artzi et al., 2015)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 317,
                        "end": 343,
                        "text": "(Kwiatkowski et al., 2010)",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 367,
                        "end": 388,
                        "text": "Andreas et al. (2013)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 605,
                        "end": 625,
                        "text": "(Koehn et al., 2007)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 667,
                        "end": 681,
                        "text": "(Chiang, 2007)",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 1220,
                        "end": 1239,
                        "text": "(Och and Ney, 2003)",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 1258,
                        "end": 1282,
                        "text": "(Sokolov and Yvon, 2011)",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 1331,
                        "end": 1354,
                        "text": "(Heafield et al., 2013)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Machine Translation Baselines",
                "sec_num": null
            },
            {
                "text": "Retrieval Baseline It was reported in (Quirk et al., 2015) that a simple retrieval method that outputs the most similar input for each sample, measured using Levenshtein Distance, leads to good results. We implement this baseline by computing the average Levenshtein Distance for each input field. This baseline is denoted \"Retrieval\".",
                "cite_spans": [
                    {
                        "start": 38,
                        "end": 58,
                        "text": "(Quirk et al., 2015)",
                        "ref_id": "BIBREF26"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Machine Translation Baselines",
                "sec_num": null
            },
            {
                "text": "Evaluation A typical metric is to compute the accuracy of whether the generated code exactly matches the reference code. This is informative as it gives an intuition of how many samples can be used without further human post-editing. However, it does not provide an illustration on the degree of closeness to achieving the correct code. Thus, we also test using BLEU-4 (Papineni et al., 2002) at the token level. There are clearly problems with these metrics. For instance, source code can be correct without matching the reference. The code in Figure 2 , could have also been implemented by calling the draw function in an cycle that exists once both players have the same number of cards in their hands. Some tasks, such as the generation of queries (Zelle and Mooney, 1996) , have overcome this problem by executing the query and checking if the result is the same as the annotation. However, we shall leave the study of these methologies for future work, as adapting these methods for our tasks is not triv-ial. For instance, the correctness cards with conditional (e.g. if player has no cards, then draw a card) or non-deterministc (e.g. put a random card in your hand) effects cannot be simply validated by running the code.",
                "cite_spans": [
                    {
                        "start": 369,
                        "end": 392,
                        "text": "(Papineni et al., 2002)",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 752,
                        "end": 776,
                        "text": "(Zelle and Mooney, 1996)",
                        "ref_id": "BIBREF34"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 552,
                        "end": 553,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Machine Translation Baselines",
                "sec_num": null
            },
            {
                "text": "Setup The multiple input types (Figure 3 ) are hyper-parametrized as follows: The C2W model (cf. \"C2W\" row) used to obtain continuous vectors for word types uses character embeddings of size 100 and LSTM states of size 300, and generates vectors of size 300. We also report on results using word lookup tables of size 300, where we replace singletons with a special unknown token with probability 0.5 during training, which is then used for out-of-vocabulary words. For text fields, the context (cf. \"Bi-LSTM\" row) is encoded with a Bi-LSTM of size 300 for the forward and backward states. Finally, a linear layer maps the different input tokens into a common space with of size 300 (cf. \"Linear\" row). As for the attention model, we used an hidden layer of size 200 before applying the non-linearity (row \"Tanh\"). As for the decoder (Figure 4 ), we encode output characters with size 100 (cf. \"output (y)\" row), and an LSTM state of size 300 and an input representation of size 300 (cf. \"State(h+z)\" row). For each pointer network (e.g., \"Copy From Name\" box), the intersection between the input units and the state units are performed with a vector of size 200. Training is performed using mini-batches of 20 samples using AdaDelta (Zeiler, 2012) and we report results using the iteration with the highest BLEU score on the validation set (tested at intervals of 5000 mini-batches). Decoding is performed with a beam of 1000. As for compression, we performed a grid search over compressing the code from 0% to 80% of the original average length over intervals of 20% for the HS and Django datasets. On the MTG dataset, we are forced to compress the code up to 80% due to performance issues when training with extremely long sequences.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 39,
                        "end": 40,
                        "text": "3",
                        "ref_id": "FIGREF2"
                    },
                    {
                        "start": 842,
                        "end": 843,
                        "text": "4",
                        "ref_id": "FIGREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Machine Translation Baselines",
                "sec_num": null
            },
            {
                "text": "Baseline Comparison Results are reported in Table 3 . Regarding the retrieval results (cf. \"Retrieval\" row), we observe the best BLEU scores among the baselines in the card datasets (cf. \"MTG\" and \"HS\" columns). A key advantage of this method is that retrieving existing entities guarantees that the output is well formed, with no , where mapping from the input to the output is mostly monotonic, while the hierarchical model (cf. \"Hierarchical\" row) yields better performance on the card datasets as the concatenation of the input fields needs to be reordered extensively into the output sequence. Finally, the sequence-to-sequence model (cf. \"Sequence\" row) yields extremely low results, mainly due to the lack of capacity needed to memorize whole input and output sequences, while the attention based model (cf. \"Attention\" row) produces results on par with phrase-based systems. Finally, we observe that by including all the proposed components (cf. \"Our System\" row), we obtain significant improvements over all baselines in the three datasets and is the only one that obtains non-zero accuracies in the card datasets.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 50,
                        "end": 51,
                        "text": "3",
                        "ref_id": "TABREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "7.1"
            },
            {
                "text": "We present ablation results in order to analyze the contribution of each of our modifications. Removing the C2W model (cf. \"-C2W\" row) yields a small deterioration, as word lookup tables are more susceptible to sparsity. The only exception is in the HS dataset, where lookup tables perform better. We believe that this is because the small size of the training set does not provide enough evidence for the character model to scale to unknown words. Surprisingly, running our model compression code (cf. \"-Compress\" row) actually yields better results. Table 4 provides an illustration of the results for different compression rates. We obtain the best results with an 80% compression rate (cf. \"BLEU Scores\" block), while maximising the time each card is processed (cf. \"Seconds Per Card\" block).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Component Comparison",
                "sec_num": null
            },
            {
                "text": "While the reason for this is uncertain, it is similar to the finding that language models that output characters tend to under-perform those that output words (J\u00f3zefowicz et al., 2016) . This applies when using the regular optimization process with a character softmax (cf. \"Softmax\" rows), but also when using the LPN (cf. \"LPN\" rows). We also note that the training speed of LPNs is not significantly lower as marginalization is performed with a dynamic program. Finally, a significant decrease is observed if we remove the pointer networks (cf. \"-LPN\" row). These improvements also generalize to sequence-to-sequence models (cf. \"-Attention\" row), as the scores are superior to the sequence-tosequence benchmark (cf. \"Sequence\" row).",
                "cite_spans": [
                    {
                        "start": 159,
                        "end": 184,
                        "text": "(J\u00f3zefowicz et al., 2016)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Component Comparison",
                "sec_num": null
            },
            {
                "text": "Result Analysis Examples of the code generated for two cards are illustrated in Figure 5 . We obtain the segments that were copied by the pointer networks by computing the most likely predictor for those segments. We observe from the marked segments that the model effectively copies the attributes that match in the output, including the name of the card that must be collapsed. As expected, the majority of the errors originate from inaccuracies in the generation of the effect of the card. While it is encouraging to observe that a small percentage of the cards are generated correctly, it is worth mentioning that these are the result of many cards possessing similar effects. The \"Madder Bomber\" card is generated correctly as there is a similar card \"Mad Bomber\" in the training set, which implements the same effect, except that it deals 3 damage instead of 6. Yet, it is a promising result that the model was able to capture this difference. However, in many cases, effects that radically differ from seen ones tend to be generated incorrectly. In the card \"Preparation\", we observe that while the properties of the card are generated correctly, the effect implements a unrelated one, with the exception of the value 3, which is correctly copied. Yet, interestingly, it still generates a valid effect, which sets a minion's attack to 3. Investigating better methods to accurately generate these effects will be object of further studies. Copied segments are marked in green and incorrect segments are marked in red.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 87,
                        "end": 88,
                        "text": "5",
                        "ref_id": "FIGREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Component Comparison",
                "sec_num": null
            },
            {
                "text": "While we target widely used programming languages, namely, Java and Python, our work is related to studies on the generation of any executable code. These include generating regular expressions (Kushman and Barzilay, 2013) , and the code for parsing input documents (Lei et al., 2013) . Much research has also been invested in generating formal languages, such as database queries (Zelle and Mooney, 1996; Berant et al., 2013) , agent specific language (Kate et al., 2005) or smart phone instructions (Le et al., 2013) . Finally, mapping natural language into a sequence of actions for the generation of executable code (Branavan et al., 2009) . Finally, a considerable effort in this task has focused on semantic parsing (Wong and Mooney, 2006; Jones et al., 2012; Lei et al., 2013; Artzi et al., 2015; Quirk et al., 2015) . Recently proposed models focus on Combinatory Categorical Grammars (Kushman and Barzilay, 2013; Artzi et al., 2015 ), Bayesian Tree Transducers (Jones et al., 2012; Lei et al., 2013) and Probabilistic Context Free Grammars (Andreas et al., 2013) . The work in natural language programming (Vadas and Curran, 2005; Manshadi et al., 2013) , where users write lines of code from natural language, is also related to our work. Finally, the reverse mapping from code into natural language is explored in (Oda et al., 2015) .",
                "cite_spans": [
                    {
                        "start": 194,
                        "end": 222,
                        "text": "(Kushman and Barzilay, 2013)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 266,
                        "end": 284,
                        "text": "(Lei et al., 2013)",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 381,
                        "end": 405,
                        "text": "(Zelle and Mooney, 1996;",
                        "ref_id": "BIBREF34"
                    },
                    {
                        "start": 406,
                        "end": 426,
                        "text": "Berant et al., 2013)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 453,
                        "end": 472,
                        "text": "(Kate et al., 2005)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 501,
                        "end": 518,
                        "text": "(Le et al., 2013)",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 620,
                        "end": 643,
                        "text": "(Branavan et al., 2009)",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 722,
                        "end": 745,
                        "text": "(Wong and Mooney, 2006;",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 746,
                        "end": 765,
                        "text": "Jones et al., 2012;",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 766,
                        "end": 783,
                        "text": "Lei et al., 2013;",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 784,
                        "end": 803,
                        "text": "Artzi et al., 2015;",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 804,
                        "end": 823,
                        "text": "Quirk et al., 2015)",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 893,
                        "end": 921,
                        "text": "(Kushman and Barzilay, 2013;",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 922,
                        "end": 940,
                        "text": "Artzi et al., 2015",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 941,
                        "end": 990,
                        "text": "), Bayesian Tree Transducers (Jones et al., 2012;",
                        "ref_id": null
                    },
                    {
                        "start": 991,
                        "end": 1008,
                        "text": "Lei et al., 2013)",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 1049,
                        "end": 1071,
                        "text": "(Andreas et al., 2013)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 1115,
                        "end": 1139,
                        "text": "(Vadas and Curran, 2005;",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 1140,
                        "end": 1162,
                        "text": "Manshadi et al., 2013)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 1325,
                        "end": 1343,
                        "text": "(Oda et al., 2015)",
                        "ref_id": "BIBREF24"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "8"
            },
            {
                "text": "Character-based sequence-to-sequence models have previously been used to generate code from natural language in (Mou et al., 2015) . Inspired by these works, LPNs provide a richer framework by employing attention models (Bahdanau et al., 2014) , pointer networks (Vinyals et al., 2015) and character-based embeddings (Ling et al., 2015) . Our formulation can also be seen as a generalization of Allamanis et al. (2016) , who implement a special case where two predictors have the same granularity (a sub-token softmax and a pointer network). Finally, HMMs have been employed in neural models to marginalize over label sequences in (Collobert et al., 2011; Lample et al., 2016) by modeling transitions between labels.",
                "cite_spans": [
                    {
                        "start": 112,
                        "end": 130,
                        "text": "(Mou et al., 2015)",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 220,
                        "end": 243,
                        "text": "(Bahdanau et al., 2014)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 263,
                        "end": 285,
                        "text": "(Vinyals et al., 2015)",
                        "ref_id": "BIBREF31"
                    },
                    {
                        "start": 317,
                        "end": 336,
                        "text": "(Ling et al., 2015)",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 395,
                        "end": 418,
                        "text": "Allamanis et al. (2016)",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 631,
                        "end": 655,
                        "text": "(Collobert et al., 2011;",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 656,
                        "end": 676,
                        "text": "Lample et al., 2016)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "8"
            },
            {
                "text": "We introduced a neural network architecture named Latent Prediction Network, which allows efficient marginalization over multiple predictors. Under this architecture, we propose a generative model for code generation that combines a character level softmax to generate language-specific tokens and multiple pointer networks to copy keywords from the input. Along with other extensions, namely structured attention and code compression, our model is applied on on both existing datasets and also on a newly created one with implementations of TCG game cards. Our experiments show that our model out-performs multiple benchmarks, which demonstrate the importance of combining different types of predictors.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "9"
            },
            {
                "text": "Dataset available at https://deepmind.com/publications.html",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "A Convolutional Attention Network for Extreme Summarization of Source Code",
                "authors": [
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Allamanis",
                        "suffix": ""
                    },
                    {
                        "first": "H",
                        "middle": [],
                        "last": "Peng",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Sutton",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "M. Allamanis, H. Peng, and C. Sutton. 2016. A Con- volutional Attention Network for Extreme Summa- rization of Source Code. ArXiv e-prints, February.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Semantic parsing as machine translation",
                "authors": [
                    {
                        "first": "Jacob",
                        "middle": [],
                        "last": "Andreas",
                        "suffix": ""
                    },
                    {
                        "first": "Andreas",
                        "middle": [],
                        "last": "Vlachos",
                        "suffix": ""
                    },
                    {
                        "first": "Stephen",
                        "middle": [],
                        "last": "Clark",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "47--52",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jacob Andreas, Andreas Vlachos, and Stephen Clark. 2013. Semantic parsing as machine translation. In Proceedings of the 51st Annual Meeting of the Asso- ciation for Computational Linguistics, pages 47-52, August.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Broad-coverage ccg semantic parsing with amr",
                "authors": [
                    {
                        "first": "Yoav",
                        "middle": [],
                        "last": "Artzi",
                        "suffix": ""
                    },
                    {
                        "first": "Kenton",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Luke",
                        "middle": [],
                        "last": "Zettlemoyer",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "1699--1710",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yoav Artzi, Kenton Lee, and Luke Zettlemoyer. 2015. Broad-coverage ccg semantic parsing with amr. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1699-1710, September.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Neural machine translation by jointly learning to align and translate",
                "authors": [
                    {
                        "first": "Dzmitry",
                        "middle": [],
                        "last": "Bahdanau",
                        "suffix": ""
                    },
                    {
                        "first": "Kyunghyun",
                        "middle": [],
                        "last": "Cho",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Semantic parsing on freebase from question-answer pairs",
                "authors": [
                    {
                        "first": "Jonathan",
                        "middle": [],
                        "last": "Berant",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Chou",
                        "suffix": ""
                    },
                    {
                        "first": "Roy",
                        "middle": [],
                        "last": "Frostig",
                        "suffix": ""
                    },
                    {
                        "first": "Percy",
                        "middle": [],
                        "last": "Liang",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "1533--1544",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Lan- guage Processing, pages 1533-1544.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Reinforcement learning for mapping instructions to actions",
                "authors": [
                    {
                        "first": "S",
                        "middle": [
                            "R K"
                        ],
                        "last": "Branavan",
                        "suffix": ""
                    },
                    {
                        "first": "Harr",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Luke",
                        "middle": [
                            "S"
                        ],
                        "last": "Zettlemoyer",
                        "suffix": ""
                    },
                    {
                        "first": "Regina",
                        "middle": [],
                        "last": "Barzilay",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP",
                "volume": "",
                "issue": "",
                "pages": "82--90",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "S. R. K. Branavan, Harr Chen, Luke S. Zettlemoyer, and Regina Barzilay. 2009. Reinforcement learn- ing for mapping instructions to actions. In Pro- ceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 82-90.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Hierarchical phrase-based translation",
                "authors": [
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Chiang",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Comput. Linguist",
                "volume": "33",
                "issue": "2",
                "pages": "201--228",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "David Chiang. 2007. Hierarchical phrase-based trans- lation. Comput. Linguist., 33(2):201-228, June.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Natural language processing (almost) from scratch",
                "authors": [
                    {
                        "first": "Ronan",
                        "middle": [],
                        "last": "Collobert",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Weston",
                        "suffix": ""
                    },
                    {
                        "first": "L\u00e9on",
                        "middle": [],
                        "last": "Bottou",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Karlen",
                        "suffix": ""
                    },
                    {
                        "first": "Koray",
                        "middle": [],
                        "last": "Kavukcuoglu",
                        "suffix": ""
                    },
                    {
                        "first": "Pavel",
                        "middle": [],
                        "last": "Kuksa",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "J. Mach. Learn. Res",
                "volume": "12",
                "issue": "",
                "pages": "2493--2537",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ronan Collobert, Jason Weston, L\u00e9on Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. J. Mach. Learn. Res., 12:2493-2537, November.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Scalable modified Kneser-Ney language model estimation",
                "authors": [
                    {
                        "first": "Kenneth",
                        "middle": [],
                        "last": "Heafield",
                        "suffix": ""
                    },
                    {
                        "first": "Ivan",
                        "middle": [],
                        "last": "Pouzyrevsky",
                        "suffix": ""
                    },
                    {
                        "first": "Jonathan",
                        "middle": [
                            "H"
                        ],
                        "last": "Clark",
                        "suffix": ""
                    },
                    {
                        "first": "Philipp",
                        "middle": [],
                        "last": "Koehn",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of the 51th Annual Meeting on Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "690--696",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H. Clark, and Philipp Koehn. 2013. Scalable modi- fied Kneser-Ney language model estimation. In Pro- ceedings of the 51th Annual Meeting on Association for Computational Linguistics, pages 690-696.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Long short-term memory",
                "authors": [
                    {
                        "first": "Sepp",
                        "middle": [],
                        "last": "Hochreiter",
                        "suffix": ""
                    },
                    {
                        "first": "J\u00fcrgen",
                        "middle": [],
                        "last": "Schmidhuber",
                        "suffix": ""
                    }
                ],
                "year": 1997,
                "venue": "Neural Comput",
                "volume": "9",
                "issue": "8",
                "pages": "1735--1780",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural Comput., 9(8):1735- 1780, November.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Semantic parsing with bayesian tree transducers",
                "authors": [
                    {
                        "first": "Keeley",
                        "middle": [],
                        "last": "Bevan",
                        "suffix": ""
                    },
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Jones",
                        "suffix": ""
                    },
                    {
                        "first": "Sharon",
                        "middle": [],
                        "last": "Johnson",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Goldwater",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "488--496",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Bevan Keeley Jones, Mark Johnson, and Sharon Gold- water. 2012. Semantic parsing with bayesian tree transducers. In Proceedings of the 50th Annual Meeting of the Association for Computational Lin- guistics, pages 488-496.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Exploring the limits of language modeling",
                "authors": [
                    {
                        "first": "Rafal",
                        "middle": [],
                        "last": "J\u00f3zefowicz",
                        "suffix": ""
                    },
                    {
                        "first": "Oriol",
                        "middle": [],
                        "last": "Vinyals",
                        "suffix": ""
                    },
                    {
                        "first": "Mike",
                        "middle": [],
                        "last": "Schuster",
                        "suffix": ""
                    },
                    {
                        "first": "Noam",
                        "middle": [],
                        "last": "Shazeer",
                        "suffix": ""
                    },
                    {
                        "first": "Yonghui",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Rafal J\u00f3zefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. 2016. Exploring the lim- its of language modeling. CoRR, abs/1602.02410.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Learning to transform natural to formal languages",
                "authors": [
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Rohit",
                        "suffix": ""
                    },
                    {
                        "first": "Yuk",
                        "middle": [
                            "Wah"
                        ],
                        "last": "Kate",
                        "suffix": ""
                    },
                    {
                        "first": "Raymond",
                        "middle": [
                            "J"
                        ],
                        "last": "Wong",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Mooney",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of the Twentieth National Conference on Artificial Intelligence (AAAI-05)",
                "volume": "",
                "issue": "",
                "pages": "1062--1068",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Rohit J. Kate, Yuk Wah Wong, and Raymond J. Mooney. 2005. Learning to transform natural to for- mal languages. In Proceedings of the Twentieth Na- tional Conference on Artificial Intelligence (AAAI- 05), pages 1062-1068, Pittsburgh, PA, July.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Moses: Open source toolkit for statistical machine translation",
                "authors": [
                    {
                        "first": "Philipp",
                        "middle": [],
                        "last": "Koehn",
                        "suffix": ""
                    },
                    {
                        "first": "Hieu",
                        "middle": [],
                        "last": "Hoang",
                        "suffix": ""
                    },
                    {
                        "first": "Alexandra",
                        "middle": [],
                        "last": "Birch",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Callison-Burch",
                        "suffix": ""
                    },
                    {
                        "first": "Marcello",
                        "middle": [],
                        "last": "Federico",
                        "suffix": ""
                    },
                    {
                        "first": "Nicola",
                        "middle": [],
                        "last": "Bertoldi",
                        "suffix": ""
                    },
                    {
                        "first": "Brooke",
                        "middle": [],
                        "last": "Cowan",
                        "suffix": ""
                    },
                    {
                        "first": "Wade",
                        "middle": [],
                        "last": "Shen",
                        "suffix": ""
                    },
                    {
                        "first": "Christine",
                        "middle": [],
                        "last": "Moran",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Zens",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Dyer",
                        "suffix": ""
                    },
                    {
                        "first": "Ond\u0159ej",
                        "middle": [],
                        "last": "Bojar",
                        "suffix": ""
                    },
                    {
                        "first": "Alexandra",
                        "middle": [],
                        "last": "Constantin",
                        "suffix": ""
                    },
                    {
                        "first": "Evan",
                        "middle": [],
                        "last": "Herbst",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions",
                "volume": "",
                "issue": "",
                "pages": "177--180",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ond\u0159ej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, pages 177-180.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Using semantic unification to generate regular expressions from natural language",
                "authors": [
                    {
                        "first": "Nate",
                        "middle": [],
                        "last": "Kushman",
                        "suffix": ""
                    },
                    {
                        "first": "Regina",
                        "middle": [],
                        "last": "Barzilay",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "volume": "",
                "issue": "",
                "pages": "826--836",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Nate Kushman and Regina Barzilay. 2013. Using se- mantic unification to generate regular expressions from natural language. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 826-836, Atlanta, Georgia, June.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Inducing probabilistic ccg grammars from logical form with higherorder unification",
                "authors": [
                    {
                        "first": "Tom",
                        "middle": [],
                        "last": "Kwiatkowski",
                        "suffix": ""
                    },
                    {
                        "first": "Luke",
                        "middle": [],
                        "last": "Zettlemoyer",
                        "suffix": ""
                    },
                    {
                        "first": "Sharon",
                        "middle": [],
                        "last": "Goldwater",
                        "suffix": ""
                    },
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Steedman",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "1223--1233",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa- ter, and Mark Steedman. 2010. Inducing proba- bilistic ccg grammars from logical form with higher- order unification. In Proceedings of the 2010 Con- ference on Empirical Methods in Natural Language Processing, pages 1223-1233.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Neural Architectures for Named Entity Recognition",
                "authors": [
                    {
                        "first": "G",
                        "middle": [],
                        "last": "Lample",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Ballesteros",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Subramanian",
                        "suffix": ""
                    },
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Kawakami",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Dyer",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "G. Lample, M. Ballesteros, S. Subramanian, K. Kawakami, and C. Dyer. 2016. Neural Architectures for Named Entity Recognition. ArXiv e-prints, March.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Smartsynth: Synthesizing smartphone automation scripts from natural language",
                "authors": [
                    {
                        "first": "Sumit",
                        "middle": [],
                        "last": "Vu Le",
                        "suffix": ""
                    },
                    {
                        "first": "Zhendong",
                        "middle": [],
                        "last": "Gulwani",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Su",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceeding of the 11th Annual International Conference on Mobile Systems, Applications, and Services",
                "volume": "",
                "issue": "",
                "pages": "193--206",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Vu Le, Sumit Gulwani, and Zhendong Su. 2013. Smartsynth: Synthesizing smartphone automation scripts from natural language. In Proceeding of the 11th Annual International Conference on Mo- bile Systems, Applications, and Services, pages 193- 206.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "From natural language specifications to program input parsers",
                "authors": [
                    {
                        "first": "Tao",
                        "middle": [],
                        "last": "Lei",
                        "suffix": ""
                    },
                    {
                        "first": "Fan",
                        "middle": [],
                        "last": "Long",
                        "suffix": ""
                    },
                    {
                        "first": "Regina",
                        "middle": [],
                        "last": "Barzilay",
                        "suffix": ""
                    },
                    {
                        "first": "Martin",
                        "middle": [],
                        "last": "Rinard",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics",
                "volume": "1",
                "issue": "",
                "pages": "1294--1303",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tao Lei, Fan Long, Regina Barzilay, and Martin Ri- nard. 2013. From natural language specifications to program input parsers. In Proceedings of the 51st Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers), pages 1294-1303, Sofia, Bulgaria, August.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Finding function in form: Compositional character models for open vocabulary word representation",
                "authors": [
                    {
                        "first": "Wang",
                        "middle": [],
                        "last": "Ling",
                        "suffix": ""
                    },
                    {
                        "first": "Tiago",
                        "middle": [],
                        "last": "Lu\u00eds",
                        "suffix": ""
                    },
                    {
                        "first": "Lu\u00eds",
                        "middle": [],
                        "last": "Marujo",
                        "suffix": ""
                    },
                    {
                        "first": "R\u00e1mon",
                        "middle": [],
                        "last": "Fernandez Astudillo",
                        "suffix": ""
                    },
                    {
                        "first": "Silvio",
                        "middle": [],
                        "last": "Amir",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Dyer",
                        "suffix": ""
                    },
                    {
                        "first": "Alan",
                        "middle": [
                            "W"
                        ],
                        "last": "Black",
                        "suffix": ""
                    },
                    {
                        "first": "Isabel",
                        "middle": [],
                        "last": "Trancoso",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Wang Ling, Tiago Lu\u00eds, Lu\u00eds Marujo, R\u00e1mon Fernan- dez Astudillo, Silvio Amir, Chris Dyer, Alan W Black, and Isabel Trancoso. 2015. Finding function in form: Compositional character models for open vocabulary word representation. In Proceedings of the 2015 Conference on Empirical Methods in Nat- ural Language Processing.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "A generative model for parsing natural language to meaning representations",
                "authors": [
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Lu",
                        "suffix": ""
                    },
                    {
                        "first": "Hwee Tou",
                        "middle": [],
                        "last": "Ng",
                        "suffix": ""
                    },
                    {
                        "first": "Wee",
                        "middle": [],
                        "last": "Sun Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Luke",
                        "middle": [
                            "S"
                        ],
                        "last": "Zettlemoyer",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, EMNLP '08",
                "volume": "",
                "issue": "",
                "pages": "783--792",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S. Zettlemoyer. 2008. A generative model for pars- ing natural language to meaning representations. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, EMNLP '08, pages 783-792, Stroudsburg, PA, USA. Associ- ation for Computational Linguistics.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Integrating programming by example and natural language programming",
                "authors": [
                    {
                        "first": "Mehdi",
                        "middle": [],
                        "last": "Hafezi Manshadi",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Gildea",
                        "suffix": ""
                    },
                    {
                        "first": "James",
                        "middle": [
                            "F"
                        ],
                        "last": "Allen",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mehdi Hafezi Manshadi, Daniel Gildea, and James F. Allen. 2013. Integrating programming by exam- ple and natural language programming. In Marie desJardins and Michael L. Littman, editors, AAAI. AAAI Press.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "On end-to-end program generation from user intention by deep neural networks",
                "authors": [
                    {
                        "first": "Lili",
                        "middle": [],
                        "last": "Mou",
                        "suffix": ""
                    },
                    {
                        "first": "Rui",
                        "middle": [],
                        "last": "Men",
                        "suffix": ""
                    },
                    {
                        "first": "Ge",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Lu",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Zhi",
                        "middle": [],
                        "last": "Jin",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Lili Mou, Rui Men, Ge Li, Lu Zhang, and Zhi Jin. 2015. On end-to-end program generation from user intention by deep neural networks. CoRR, abs/1510.07211.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "A systematic comparison of various statistical alignment models",
                "authors": [
                    {
                        "first": "Josef",
                        "middle": [],
                        "last": "Franz",
                        "suffix": ""
                    },
                    {
                        "first": "Hermann",
                        "middle": [],
                        "last": "Och",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Ney",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Comput. Linguist",
                "volume": "29",
                "issue": "1",
                "pages": "19--51",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Franz Josef Och and Hermann Ney. 2003. A sys- tematic comparison of various statistical alignment models. Comput. Linguist., 29(1):19-51, March.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Learning to generate pseudo-code from source code using statistical machine translation",
                "authors": [
                    {
                        "first": "Yusuke",
                        "middle": [],
                        "last": "Oda",
                        "suffix": ""
                    },
                    {
                        "first": "Hiroyuki",
                        "middle": [],
                        "last": "Fudaba",
                        "suffix": ""
                    },
                    {
                        "first": "Graham",
                        "middle": [],
                        "last": "Neubig",
                        "suffix": ""
                    },
                    {
                        "first": "Hideaki",
                        "middle": [],
                        "last": "Hata",
                        "suffix": ""
                    },
                    {
                        "first": "Sakriani",
                        "middle": [],
                        "last": "Sakti",
                        "suffix": ""
                    },
                    {
                        "first": "Tomoki",
                        "middle": [],
                        "last": "Toda",
                        "suffix": ""
                    },
                    {
                        "first": "Satoshi",
                        "middle": [],
                        "last": "Nakamura",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "30th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yusuke Oda, Hiroyuki Fudaba, Graham Neubig, Hideaki Hata, Sakriani Sakti, Tomoki Toda, and Satoshi Nakamura. 2015. Learning to gener- ate pseudo-code from source code using statistical machine translation. In 30th IEEE/ACM Interna- tional Conference on Automated Software Engineer- ing (ASE), Lincoln, Nebraska, USA, November.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Bleu: A method for automatic evaluation of machine translation",
                "authors": [
                    {
                        "first": "Kishore",
                        "middle": [],
                        "last": "Papineni",
                        "suffix": ""
                    },
                    {
                        "first": "Salim",
                        "middle": [],
                        "last": "Roukos",
                        "suffix": ""
                    },
                    {
                        "first": "Todd",
                        "middle": [],
                        "last": "Ward",
                        "suffix": ""
                    },
                    {
                        "first": "Wei-Jing",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "311--318",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: A method for automatic eval- uation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computa- tional Linguistics, pages 311-318.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Language to code: Learning semantic parsers for if-this-then-that recipes",
                "authors": [
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Quirk",
                        "suffix": ""
                    },
                    {
                        "first": "Raymond",
                        "middle": [],
                        "last": "Mooney",
                        "suffix": ""
                    },
                    {
                        "first": "Michel",
                        "middle": [],
                        "last": "Galley",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "878--888",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chris Quirk, Raymond Mooney, and Michel Galley. 2015. Language to code: Learning semantic parsers for if-this-then-that recipes. In Proceedings of the 53rd Annual Meeting of the Association for Compu- tational Linguistics, pages 878-888, Beijing, China, July.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Semimarkov conditional random fields for information extraction",
                "authors": [
                    {
                        "first": "Sunita",
                        "middle": [],
                        "last": "Sarawagi",
                        "suffix": ""
                    },
                    {
                        "first": "William",
                        "middle": [
                            "W"
                        ],
                        "last": "Cohen",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "17",
                "issue": "",
                "pages": "1185--1192",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sunita Sarawagi and William W. Cohen. 2005. Semi- markov conditional random fields for information extraction. In L. K. Saul, Y. Weiss, and L. Bottou, editors, Advances in Neural Information Processing Systems 17, pages 1185-1192. MIT Press.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Minimum Error Rate Semi-Ring",
                "authors": [
                    {
                        "first": "Artem",
                        "middle": [],
                        "last": "Sokolov",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Franc \u00b8ois Yvon",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Proceedings of the European Conference on Machine Translation",
                "volume": "",
                "issue": "",
                "pages": "241--248",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Artem Sokolov and Franc \u00b8ois Yvon. 2011. Mini- mum Error Rate Semi-Ring. In Mikel Forcada and Heidi Depraetere, editors, Proceedings of the Eu- ropean Conference on Machine Translation, pages 241-248, Leuven, Belgium.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Sequence to sequence learning with neural networks",
                "authors": [
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    },
                    {
                        "first": "Oriol",
                        "middle": [],
                        "last": "Vinyals",
                        "suffix": ""
                    },
                    {
                        "first": "V",
                        "middle": [],
                        "last": "Quoc",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Le",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to sequence learning with neural net- works. CoRR, abs/1409.3215.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "Programming with unrestricted natural language",
                "authors": [
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Vadas",
                        "suffix": ""
                    },
                    {
                        "first": "James",
                        "middle": [
                            "R"
                        ],
                        "last": "Curran",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of the Australasian Language Technology Workshop 2005",
                "volume": "",
                "issue": "",
                "pages": "191--199",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "David Vadas and James R. Curran. 2005. Program- ming with unrestricted natural language. In Pro- ceedings of the Australasian Language Technology Workshop 2005, pages 191-199, Sydney, Australia, December.",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "Pointer networks",
                "authors": [
                    {
                        "first": "Oriol",
                        "middle": [],
                        "last": "Vinyals",
                        "suffix": ""
                    },
                    {
                        "first": "Meire",
                        "middle": [],
                        "last": "Fortunato",
                        "suffix": ""
                    },
                    {
                        "first": "Navdeep",
                        "middle": [],
                        "last": "Jaitly",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "28",
                "issue": "",
                "pages": "2674--2682",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. 2015. Pointer networks. In C. Cortes, N.D. Lawrence, D.D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2674-2682. Curran Associates, Inc.",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "Learning for semantic parsing with statistical machine translation",
                "authors": [
                    {
                        "first": "Yuk",
                        "middle": [
                            "Wah"
                        ],
                        "last": "Wong",
                        "suffix": ""
                    },
                    {
                        "first": "Raymond",
                        "middle": [
                            "J"
                        ],
                        "last": "Mooney",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Proceedings of the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "439--446",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yuk Wah Wong and Raymond J. Mooney. 2006. Learning for semantic parsing with statistical ma- chine translation. In Proceedings of the Main Con- ference on Human Language Technology Confer- ence of the North American Chapter of the Associa- tion of Computational Linguistics, pages 439-446.",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "ADADELTA: an adaptive learning rate method",
                "authors": [
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Matthew",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Zeiler",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Matthew D. Zeiler. 2012. ADADELTA: an adaptive learning rate method. CoRR, abs/1212.5701.",
                "links": null
            },
            "BIBREF34": {
                "ref_id": "b34",
                "title": "Learning to parse database queries using inductive logic programming",
                "authors": [
                    {
                        "first": "John",
                        "middle": [
                            "M"
                        ],
                        "last": "Zelle",
                        "suffix": ""
                    },
                    {
                        "first": "Raymond",
                        "middle": [
                            "J"
                        ],
                        "last": "Mooney",
                        "suffix": ""
                    }
                ],
                "year": 1996,
                "venue": "AAAI/IAAI",
                "volume": "",
                "issue": "",
                "pages": "1050--1055",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "John M. Zelle and Raymond J. Mooney. 1996. Learn- ing to parse database queries using inductive logic programming. In AAAI/IAAI, pages 1050-1055, Portland, OR, August. AAAI Press/MIT Press.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 1: Example MTG and HS cards.",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Figure 2: Code for the HS card \"Divine Favor\".",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "Figure 3: Illustration of the structured attention mechanism operating on a single time stamp t.",
                "uris": null,
                "fig_num": "3",
                "type_str": "figure"
            },
            "FIGREF3": {
                "num": null,
                "text": "Figure 4: Generation process for the code init('Tirion Fordring',8,6,6) using LPNs.",
                "uris": null,
                "fig_num": "4",
                "type_str": "figure"
            },
            "FIGREF4": {
                "num": null,
                "text": "Figure 5: Examples of decoded cards from HS. Copied segments are marked in green and incorrect segments are marked in red.",
                "uris": null,
                "fig_num": "5",
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table><tr><td/><td>MTG</td><td>HS</td></tr><tr><td>Programming Language</td><td colspan=\"2\">Java Python</td></tr><tr><td>Cards</td><td>13,297</td><td>665</td></tr><tr><td>Cards (Train)</td><td>11,969</td><td>533</td></tr><tr><td>Cards (Validation)</td><td>664</td><td>66</td></tr><tr><td>Cards (Test)</td><td>664</td><td>66</td></tr><tr><td>Singular Fields</td><td>6</td><td>4</td></tr><tr><td>Text Fields</td><td>8</td><td>2</td></tr><tr><td>Words In Description (Average)</td><td>21</td><td>7</td></tr><tr><td>Characters In Code (Average)</td><td>1,080</td><td>352</td></tr><tr><td colspan=\"3\">health) and four text fields (cost, type, name, and</td></tr><tr><td colspan=\"3\">description), whereas HS cards have eight singu-</td></tr><tr><td colspan=\"3\">lar fields (attack, health, cost and durability, rar-</td></tr><tr><td colspan=\"3\">ity, type, race and class) and two text fields (name</td></tr><tr><td colspan=\"3\">and description). Text fields are tokenized by</td></tr><tr><td colspan=\"3\">splitting on whitespace and punctuation, with ex-</td></tr><tr><td colspan=\"3\">ceptions accounting for domain specific artifacts</td></tr><tr><td>(e.</td><td/><td/></tr></table>",
                "type_str": "table",
                "text": "Statistics of the two TCG datasets.",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table><tr><td/><td>MTG</td><td/><td>HS</td><td>Django</td></tr><tr><td/><td colspan=\"4\">BLEU Acc BLEU Acc BLEU Acc</td></tr><tr><td>Retrieval</td><td colspan=\"2\">54.9 0.0</td><td>62.5 0.0</td><td>18.6 14.7</td></tr><tr><td>Phrase</td><td colspan=\"2\">49.5 0.0</td><td>34.1 0.0</td><td>47.6 31.5</td></tr><tr><td>Hierarchical</td><td colspan=\"2\">50.6 0.0</td><td>43.2 0.0</td><td>35.9 9.5</td></tr><tr><td>Sequence</td><td colspan=\"2\">33.8 0.0</td><td>28.5 0.0</td><td>44.1 33.2</td></tr><tr><td>Attention</td><td colspan=\"2\">50.1 0.0</td><td>43.9 0.0</td><td>58.9 38.8</td></tr><tr><td>Our System</td><td colspan=\"2\">61.4 4.8</td><td>65.6 4.5</td><td>77.6 62.3</td></tr><tr><td>-C2W</td><td colspan=\"2\">60.9 4.4</td><td>67.1 4.5</td><td>75.9 60.9</td></tr><tr><td>-Compress</td><td>-</td><td>-</td><td>59.7 6.1</td><td>76.3 61.3</td></tr><tr><td>-LPN</td><td colspan=\"2\">52.4 0.0</td><td>42.0 0.0</td><td>63.3 40.8</td></tr><tr><td>-Attention</td><td colspan=\"2\">39.1 0.5</td><td>49.9 3.0</td><td>48.8 34.5</td></tr><tr><td>Compression</td><td colspan=\"4\">0% 20% 40% 60% 80%</td></tr><tr><td colspan=\"2\">Seconds Per Card</td><td/><td/><td/></tr><tr><td>Softmax</td><td colspan=\"4\">2.81 2.36 1.88 1.42 0.94</td></tr><tr><td>LPN</td><td colspan=\"4\">3.29 2.65 2.35 1.93 1.41</td></tr><tr><td>BLEU Scores</td><td/><td/><td/><td/></tr><tr><td>Softmax</td><td colspan=\"4\">44.2 46.9 47.2 51.4 52.7</td></tr><tr><td>LPN</td><td colspan=\"4\">59.7 62.8 61.1 66.4 67.1</td></tr><tr><td colspan=\"5\">Table 4: Results with increasing compression rates</td></tr><tr><td colspan=\"5\">with a regular softmax (cf. \"Softmax\") and a LPN</td></tr><tr><td colspan=\"5\">(cf. \"LPN\"). Performance values (cf. \"Seconds Per</td></tr><tr><td colspan=\"5\">Card\" block) are computed using one CPU.</td></tr><tr><td colspan=\"5\">syntactic errors such as producing a non-existent</td></tr><tr><td colspan=\"5\">function call or generating incomplete code. As</td></tr><tr><td colspan=\"5\">BLEU penalizes length mismatches, generating</td></tr><tr><td colspan=\"5\">code that matches the length of the reference pro-</td></tr><tr><td colspan=\"5\">vides a large boost. The phrase-based transla-</td></tr><tr><td colspan=\"5\">tion model (cf. \"Phrase\" row) performs well in</td></tr><tr><td colspan=\"4\">the Django (cf. \"Django\" column)</td><td/></tr></table>",
                "type_str": "table",
                "text": "BLEU and Accuracy scores for the proposed task on two in-domain datasets (HS and MTG) and an out-of-domain dataset (Django).",
                "html": null,
                "num": null
            }
        }
    }
}