{
    "paper_id": "N19-1233",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:09:38.180039Z"
    },
    "title": "Evaluating Text GANs as Language Models",
    "authors": [
        {
            "first": "Guy",
            "middle": [],
            "last": "Tevet",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Tel-Aviv University",
                "location": {}
            },
            "email": "guytevet@mail"
        },
        {
            "first": "Gavriel",
            "middle": [],
            "last": "Habib",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Tel-Aviv University",
                "location": {}
            },
            "email": "gavrielhabib@mail"
        },
        {
            "first": "Vered",
            "middle": [],
            "last": "Shwartz",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Jonathan",
            "middle": [],
            "last": "Berant",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Tel-Aviv University",
                "location": {}
            },
            "email": "joberant@cs.tau.ac"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Generative Adversarial Networks (GANs) are a promising approach for text generation that, unlike traditional language models (LM), does not suffer from the problem of \"exposure bias\". However, A major hurdle for understanding the potential of GANs for text generation is the lack of a clear evaluation metric. In this work, we propose to approximate the distribution of text generated by a GAN, which permits evaluating them with traditional probability-based LM metrics. We apply our approximation procedure on several GAN-based models and show that they currently perform substantially worse than stateof-the-art LMs. Our evaluation procedure promotes better understanding of the relation between GANs and LMs, and can accelerate progress in GAN-based text generation.",
    "pdf_parse": {
        "paper_id": "N19-1233",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Generative Adversarial Networks (GANs) are a promising approach for text generation that, unlike traditional language models (LM), does not suffer from the problem of \"exposure bias\". However, A major hurdle for understanding the potential of GANs for text generation is the lack of a clear evaluation metric. In this work, we propose to approximate the distribution of text generated by a GAN, which permits evaluating them with traditional probability-based LM metrics. We apply our approximation procedure on several GAN-based models and show that they currently perform substantially worse than stateof-the-art LMs. Our evaluation procedure promotes better understanding of the relation between GANs and LMs, and can accelerate progress in GAN-based text generation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Neural networks have revolutionized the field of text generation, in machine translation (Sutskever et al., 2014; Neubig, 2017; Luong et al., 2015; Chen et al., 2018) , summarization (See et al., 2017) , image captioning (You et al., 2016) and many other applications (Goldberg, 2017) .",
                "cite_spans": [
                    {
                        "start": 89,
                        "end": 113,
                        "text": "(Sutskever et al., 2014;",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 114,
                        "end": 127,
                        "text": "Neubig, 2017;",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 128,
                        "end": 147,
                        "text": "Luong et al., 2015;",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 148,
                        "end": 166,
                        "text": "Chen et al., 2018)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 183,
                        "end": 201,
                        "text": "(See et al., 2017)",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 221,
                        "end": 239,
                        "text": "(You et al., 2016)",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 268,
                        "end": 284,
                        "text": "(Goldberg, 2017)",
                        "ref_id": "BIBREF5"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Traditionally, text generation models are trained by going over a gold sequence of symbols (characters or words) from left-to-right, and maximizing the probability of the next symbol given the history, namely, a language modeling (LM) objective. A commonly discussed drawback of such LM-based text generation is exposure bias (Ranzato et al., 2015) : during training, the model predicts the next token conditioned on the ground truth history, while at test time prediction is based on predicted tokens, causing a train-test mismatch. Models trained in this manner often struggle to overcome previous prediction errors.",
                "cite_spans": [
                    {
                        "start": 326,
                        "end": 348,
                        "text": "(Ranzato et al., 2015)",
                        "ref_id": "BIBREF21"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Generative Adversarial Networks (Goodfellow et al., 2014) offer a solution for exposure bias. * The authors contributed equally Originally introduced for images, GANs leverage a discriminator, which is trained to discriminate between real images and generated images via an adversarial loss. In such a framework, the generator is not directly exposed to the ground truth data, but instead learns to imitate it using global feedback from the discriminator. This has led to several attempts to use GANs for text generation, with a generator using either a recurrent neural network (RNN) (Yu et al., 2017; Guo et al., 2017; Press et al., 2017; Rajeswar et al., 2017) , or a Convolutional Neural Network (CNN) (Gulrajani et al., 2017; Rajeswar et al., 2017) .",
                "cite_spans": [
                    {
                        "start": 32,
                        "end": 57,
                        "text": "(Goodfellow et al., 2014)",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 585,
                        "end": 602,
                        "text": "(Yu et al., 2017;",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 603,
                        "end": 620,
                        "text": "Guo et al., 2017;",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 621,
                        "end": 640,
                        "text": "Press et al., 2017;",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 641,
                        "end": 663,
                        "text": "Rajeswar et al., 2017)",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 706,
                        "end": 730,
                        "text": "(Gulrajani et al., 2017;",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 731,
                        "end": 753,
                        "text": "Rajeswar et al., 2017)",
                        "ref_id": "BIBREF20"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "However, evaluating GANs is more difficult than evaluating LMs. While in language modeling, evaluation is based on the log-probability of a model on held-out text, this cannot be straightforwardly extended to GAN-based text generation, because the generator outputs discrete tokens, rather than a probability distribution. Currently, there is no single evaluation metric for GAN-based text generation, and existing metrics that are based on n-gram overlap are known to lack robustness and have low correlation with semantic coherence (Semeniuta et al., 2018) .",
                "cite_spans": [
                    {
                        "start": 534,
                        "end": 558,
                        "text": "(Semeniuta et al., 2018)",
                        "ref_id": "BIBREF23"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this paper, we propose a method for evaluating GANs with standard probability-based evaluation metrics. We show that the expected prediction of a GAN generator can be viewed as a LM, and suggest a simple Monte-Carlo method for approximating it. The approximated probability distribution can then be evaluated with standard LM metrics such as perplexity or Bits Per Character (BPC).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "To empirically establish our claim, we implement our evaluation on several RNN-based GANs: (Press et al., 2017; Yu et al., 2017; Guo et al., 2017) . We find that all models have substantially lower BPC compared to state-of-the-art LMs. By directly comparing to LMs, we put in perspective the current performance of RNN-based GANs for text generation. Our results are also in line with recent concurrent work by Caccia et al. (2018) , who reached a similar conclusion by comparing the performance of textual GANs to that of LMs using metrics suggested for GAN evaluation.",
                "cite_spans": [
                    {
                        "start": 91,
                        "end": 111,
                        "text": "(Press et al., 2017;",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 112,
                        "end": 128,
                        "text": "Yu et al., 2017;",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 129,
                        "end": 146,
                        "text": "Guo et al., 2017)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 411,
                        "end": 431,
                        "text": "Caccia et al. (2018)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Our code is available at: http: //github.com/GuyTevet/SeqGAN-eval and http://github.com/GuyTevet/ rnn-gan-eval.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Following the success of GANs in image generation, several works applied the same idea to texts using convolutional neural networks (Gulrajani et al., 2017; Rajeswar et al., 2017) , and later using RNNs (Press et al., 2017; Yu et al., 2017) . RNNs enable generating variable-length sequences, conditioning each token on the tokens generated in previous time steps. We leverage this characteristic in our approximation model ( \u00a74.1).",
                "cite_spans": [
                    {
                        "start": 132,
                        "end": 156,
                        "text": "(Gulrajani et al., 2017;",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 157,
                        "end": 179,
                        "text": "Rajeswar et al., 2017)",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 203,
                        "end": 223,
                        "text": "(Press et al., 2017;",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 224,
                        "end": 240,
                        "text": "Yu et al., 2017)",
                        "ref_id": "BIBREF28"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Background",
                "sec_num": "2"
            },
            {
                "text": "A main challenge in applying GANs for text is that generating discrete symbols is a nondifferentiable operation. One solution is to perform a continuous relaxation of the GAN output, which leads to generators that emit a nearly discrete continuous distribution (Press et al., 2017) . This keeps the model differentiable and enables end-to-end training through the discriminator. Alternatively, SeqGAN (Yu et al., 2017) and Leak-GAN (Guo et al., 2017) used policy gradient methods to overcome the differentiablity requirement. We apply our approximation to both model types.",
                "cite_spans": [
                    {
                        "start": 261,
                        "end": 281,
                        "text": "(Press et al., 2017)",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 401,
                        "end": 418,
                        "text": "(Yu et al., 2017)",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 432,
                        "end": 450,
                        "text": "(Guo et al., 2017)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Background",
                "sec_num": "2"
            },
            {
                "text": "3 Evaluating GANs and LMs LM Evaluation. Text generation from LMs is commonly evaluated using probabilistic metrics. Specifically, given a test sequence of symbols (t 1 , . . . , t n ), and a LM q, the average crossentropy over the entire test set is computed:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Background",
                "sec_num": "2"
            },
            {
                "text": "ACE = -1 n n i=1 log 2 q(t i | t 1 ...t i-1",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Background",
                "sec_num": "2"
            },
            {
                "text": "). For word-based models, the standard metric is perplexity: P P = 2 ACE , while for character-based models it is BP C = ACE directly.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Background",
                "sec_num": "2"
            },
            {
                "text": "Intrinsic improvement in perplexity does not guarantee an improvement in an extrinsic downstream task that uses a language model. However, perplexity often correlates with extrinsic measures (Jurafsky and Martin, 2018) , and is the de-facto metric for evaluating the quality of language models today.",
                "cite_spans": [
                    {
                        "start": 191,
                        "end": 218,
                        "text": "(Jurafsky and Martin, 2018)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Background",
                "sec_num": "2"
            },
            {
                "text": "GAN-based Text Generation Evaluation. By definition, a text GAN outputs a discrete sequence of symbols rather than a probability distribution. As a result, LM metrics cannot be applied to evaluate the generated text. Consequently, other metrics have been proposed:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Background",
                "sec_num": "2"
            },
            {
                "text": "\u2022 N-gram overlap: (Yu et al., 2017; Press et al., 2017) : Inspired by BLEU (Papineni et al., 2002) , this measures whether n-grams generated by the model appear in a held-out corpus.",
                "cite_spans": [
                    {
                        "start": 18,
                        "end": 35,
                        "text": "(Yu et al., 2017;",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 36,
                        "end": 55,
                        "text": "Press et al., 2017)",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 75,
                        "end": 98,
                        "text": "(Papineni et al., 2002)",
                        "ref_id": "BIBREF18"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Background",
                "sec_num": "2"
            },
            {
                "text": "A major drawback is that this metric favors conservative models that always generate very common text (e.g., \"it is\"). To mitigate this, self-BLEU has been proposed (Lu et al., 2018) as an additional metric, where overlap is measured between two independently sampled texts from the model. ",
                "cite_spans": [
                    {
                        "start": 165,
                        "end": 182,
                        "text": "(Lu et al., 2018)",
                        "ref_id": "BIBREF15"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Background",
                "sec_num": "2"
            },
            {
                "text": "We propose a method for approximating a distribution over tokens from a GAN, and then evaluate the model with standard LM metrics. We will describe our approach given an RNN-based LM, which is the most commonly-used architecture, but the approximation can be applied to other auto-regressive models (Vaswani et al., 2017) .",
                "cite_spans": [
                    {
                        "start": 299,
                        "end": 321,
                        "text": "(Vaswani et al., 2017)",
                        "ref_id": "BIBREF26"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Proposed Method",
                "sec_num": "4"
            },
            {
                "text": "The inputs to an RNN at time step t, are the state vector h t and the current input token x t . The output token (one-hot) is denoted by o t . In RNNbased GANs, the previous output token is used at inference time as the input x t (Yu et al., 2017; Guo et al., 2017; Press et al., 2017; Rajeswar et al., 2017) . In contrast, when evaluating with BPC or perplexity, the gold token x t is given as input. Hence, LM-based evaluation neutralizes the problem of exposure bias addressed by GANs. Nevertheless, this allows us to compare the quality of text produced by GANs and LMs on an equal footing. Figure 1 illustrates the difference between inference time and during LM approximation.",
                "cite_spans": [
                    {
                        "start": 230,
                        "end": 247,
                        "text": "(Yu et al., 2017;",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 248,
                        "end": 265,
                        "text": "Guo et al., 2017;",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 266,
                        "end": 285,
                        "text": "Press et al., 2017;",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 286,
                        "end": 308,
                        "text": "Rajeswar et al., 2017)",
                        "ref_id": "BIBREF20"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 602,
                        "end": 603,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Language Model Approximation",
                "sec_num": "4.1"
            },
            {
                "text": "We can therefore define the generator function at time step t as a function of the initial state h 0 and the past generated tokens (x 0 . . . x t ), which we denote as o t = G t (h 0 , x 0 ...x t ) (x 0 is a start token). Given a past sequence (x 0 . . . be gained either by using a noise vector as the initial state h 0 (Press et al., 2017) , or by sampling from the GAN's internal distribution over possible output tokens (Yu et al., 2017; Guo et al., 2017) . Since h 0 is constant or a noise vector that makes G t stochastic, we can omit it to get G t (x 0 . . . x t ). In such a setup, the expected value E[G t (x 0 . . . x t )] is a distribution q over the next vocabulary token a t :",
                "cite_spans": [
                    {
                        "start": 321,
                        "end": 341,
                        "text": "(Press et al., 2017)",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 424,
                        "end": 441,
                        "text": "(Yu et al., 2017;",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 442,
                        "end": 459,
                        "text": "Guo et al., 2017)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Language Model Approximation",
                "sec_num": "4.1"
            },
            {
                "text": "q(a t | a 0 . . . a t-1 ) = {E[G t (x 0 . . . x t )]} at",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Language Model Approximation",
                "sec_num": "4.1"
            },
            {
                "text": "To empirically approximate q, we can sample from it N i.i.d samples, and compute an approximation Gt,N = 1 N \u03a3 N n=1 g t,n , where g t,n is one sample from G t (x 0 ...x t ). Then, according to the strong law of large numbers:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Language Model Approximation",
                "sec_num": "4.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "E[G t (x 0 . . . x t )] = lim N \u2192\u221e Gt,N",
                        "eq_num": "(1)"
                    }
                ],
                "section": "Language Model Approximation",
                "sec_num": "4.1"
            },
            {
                "text": "Given this approximate LM distribution, we can evaluate a GAN using perplexity or BPC. We summarize the evaluation procedure in Algorithm 1. 1",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Language Model Approximation",
                "sec_num": "4.1"
            },
            {
                "text": "We provide a theoretical bound for choosing a number of samples N that results in a good approximation of Gt,N to E[G t ].",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Approximation Bound",
                "sec_num": "4.2"
            },
            {
                "text": "Perplexity and BPC rely on the log-probability of the ground truth token. Since the ground truth token is unknown, we conservatively define the bad event B in which there exists",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Approximation Bound",
                "sec_num": "4.2"
            },
            {
                "text": "v \u2208 V such that |{E[G t ]} v -{ Gt,N } v | > \u03b3,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Approximation Bound",
                "sec_num": "4.2"
            },
            {
                "text": "where V is the vocabulary. We can then bound the probability of B by some . We define the following notations:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Approximation Bound",
                "sec_num": "4.2"
            },
            {
                "text": "1. The probability of a token a t to be v is p",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Approximation Bound",
                "sec_num": "4.2"
            },
            {
                "text": "v \u2206 = q(a t = v|a 0 . . . a t-1 ) = {E[G t (x 0 . . . x t )]} v . 2. \u03c7 v,n \u2206 = {g t,n } v",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Approximation Bound",
                "sec_num": "4.2"
            },
            {
                "text": "is a random variable representing the binary value of the v'th index of 1 Our evaluation algorithm is linear in the length of the test set and in the number of samples N . g t,n which is a single sample of G t . Note that the average of \u03c7 v,n over N samples is",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Approximation Bound",
                "sec_num": "4.2"
            },
            {
                "text": "X v \u2206 = 1 N N n=1 \u03c7 v,n = 1 N N n=1 g t,n v = { Gt,N } v .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Approximation Bound",
                "sec_num": "4.2"
            },
            {
                "text": "Using the above notation, we can re-define the probability of the bad event B with respect to the individual coordinates in the vectors:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Approximation Bound",
                "sec_num": "4.2"
            },
            {
                "text": "P r(B) = P r E[Gt] -Gt,N \u221e > \u03b3 = P r v\u2208V |pv -Xv| > \u03b3 ! < (2)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Approximation Bound",
                "sec_num": "4.2"
            },
            {
                "text": "We note that \u03c7 v,n \u223c Bernoulli(p v ), and given that {\u03c7 v,n } N n=1 are i.i.d., we can apply the Chernoff-Hoeffding theorem (Chernoff et al., 1952; Hoeffding, 1963) . According to the theorem, for every v \u2208 V , P r(|X v -p v | > \u03b3) < 2e -2N \u03b3 2 . Taking the union bound over V implies:",
                "cite_spans": [
                    {
                        "start": 124,
                        "end": 147,
                        "text": "(Chernoff et al., 1952;",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 148,
                        "end": 164,
                        "text": "Hoeffding, 1963)",
                        "ref_id": "BIBREF10"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Approximation Bound",
                "sec_num": "4.2"
            },
            {
                "text": "P r(B) = P r v\u2208V |X v -p v | > \u03b3 < 2|V |e -2N \u03b3 2 < (3)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Approximation Bound",
                "sec_num": "4.2"
            },
            {
                "text": "Hence, we get a lower bound on N : In practice, probability vectors of LMs tend to be sparse (Kim et al., 2016) . Thus, we argue that we can use a much smaller N for a good approximation Gt,N . Since the sparsity of LMs is difficult to bound, as it differs between models, we suggest an empirical method for choosing N .",
                "cite_spans": [
                    {
                        "start": 93,
                        "end": 111,
                        "text": "(Kim et al., 2016)",
                        "ref_id": "BIBREF12"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Approximation Bound",
                "sec_num": "4.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "N > 1 2\u03b3 2 ln 2|V |",
                        "eq_num": "("
                    }
                ],
                "section": "Approximation Bound",
                "sec_num": "4.2"
            },
            {
                "text": "The approximation Gt,N is a converging sequence, particularly over \u2022 \u221e (see Equation 1). Hence, we can empirically choose an N which satisfies Gt,N-\u03b1 -Gt,N \u221e < \u03b3 , \u03b1 \u2208 N. In Section 5 we empirically measure Gt,N-\u03b1 -Gt,N \u221e as a function of N to choose N . We choose a global N for a model, rather than for every t, by averaging over a subset of the evaluation set.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Approximation Bound",
                "sec_num": "4.2"
            },
            {
                "text": "We focus on character-based GANs as a test-case for our method. We evaluate two RNN-based GANs with different characteristics. As opposed to the original GAN model (Goodfellow et al., 2014) , in which the generator is initialized with random noise, the GANs we evaluated both leverage gold standard text to initialize the generator, as detailed below.",
                "cite_spans": [
                    {
                        "start": 164,
                        "end": 189,
                        "text": "(Goodfellow et al., 2014)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Models",
                "sec_num": "5.1"
            },
            {
                "text": "Recurrent GAN (Press et al., 2017 ) is a continuous RNN-based generator which minimizes the improved WGAN loss (Gulrajani et al., 2017) . To guide the generator, during training it is initialized with the first i -1 characters from the ground truth, starting the prediction in the ith character. Stochasticity is obtained by feeding the generator with a noise vector z as a hidden state. At each time step, the input to the RNN generator is the output distribution of the previous step.",
                "cite_spans": [
                    {
                        "start": 14,
                        "end": 33,
                        "text": "(Press et al., 2017",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 111,
                        "end": 135,
                        "text": "(Gulrajani et al., 2017)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Models",
                "sec_num": "5.1"
            },
            {
                "text": "SeqGAN (Yu et al., 2017 ) is a discrete RNNbased generator. To guide the generator, it is pretrained as a LM on ground truth text. Stochasticity is obtained by sampling tokens from an internal distribution function over the vocabulary. To overcome differentiation problem, it is trained using a policy gradient objective (Sutton et al., 2000) .",
                "cite_spans": [
                    {
                        "start": 7,
                        "end": 23,
                        "text": "(Yu et al., 2017",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 321,
                        "end": 342,
                        "text": "(Sutton et al., 2000)",
                        "ref_id": "BIBREF25"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Models",
                "sec_num": "5.1"
            },
            {
                "text": "We also evaluated LeakGAN (Guo et al., 2017) , another discrete RNN-based generator, but since it is similar to SeqGAN and performed worse, we omit it for brevity.",
                "cite_spans": [
                    {
                        "start": 26,
                        "end": 44,
                        "text": "(Guo et al., 2017)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Models",
                "sec_num": "5.1"
            },
            {
                "text": "To compare to prior work in LM, we follow the common setup and train on the text8 dataset. 2 The dataset is derived from Wikipedia, and includes 26 English characters plus spaces. We use the standard 90/5/5 split to train/validation/test. Finally, we measure performance with BPC.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation Settings",
                "sec_num": "5.2"
            },
            {
                "text": "We tuned hyper-parameters on the validation set, including sequence length to generate at test time (7 for Press et al. (2017) , 1000 for Yu et al. (2017) ). We chose the number of samples N empirically for each model, as described in Section 4.2. We set \u03b1 to 10, and the boundary to \u03b3 = 10 -3 as a good trade-off between accuracy and run-time. Figure 2 plots the approximate error Gt,N-\u03b1 -Gt,N \u221e as a function of N . For both models, N > 1600 satisfies this condition (red line in Figure 2 ). To be safe, we used N = 2000.",
                "cite_spans": [
                    {
                        "start": 107,
                        "end": 126,
                        "text": "Press et al. (2017)",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 138,
                        "end": 154,
                        "text": "Yu et al. (2017)",
                        "ref_id": "BIBREF28"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 352,
                        "end": 353,
                        "text": "2",
                        "ref_id": "FIGREF2"
                    },
                    {
                        "start": 489,
                        "end": 490,
                        "text": "2",
                        "ref_id": "FIGREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Evaluation Settings",
                "sec_num": "5.2"
            },
            {
                "text": "Table 1 shows model performance on the test set.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "1",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "5.3"
            },
            {
                "text": "2 http://mattmahoney.net/dc/textdata Approach Model BPC Approx. BPC Language Models mLSTM + dynamic eval (Krause et al., 2017) 1.19 Large mLSTM +emb +WN +VD (Krause et al., 2016) 1.27 Large RHN (Zilly et al., 2016) 1.27 LayerNorm HM-LSTM (Chung et al., 2016) 1.29 BN LSTM (Cooijmans et al., 2016) 1.36 Unregularised mLSTM (Krause et al., 2016) 1.40",
                "cite_spans": [
                    {
                        "start": 105,
                        "end": 126,
                        "text": "(Krause et al., 2017)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 157,
                        "end": 178,
                        "text": "(Krause et al., 2016)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 190,
                        "end": 214,
                        "text": "RHN (Zilly et al., 2016)",
                        "ref_id": null
                    },
                    {
                        "start": 238,
                        "end": 258,
                        "text": "(Chung et al., 2016)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 272,
                        "end": 296,
                        "text": "(Cooijmans et al., 2016)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 322,
                        "end": 343,
                        "text": "(Krause et al., 2016)",
                        "ref_id": "BIBREF14"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "5.3"
            },
            {
                "text": "SeqGAN -pre-trained LM (Yu et al., 2017) 1.85 1.95",
                "cite_spans": [
                    {
                        "start": 23,
                        "end": 40,
                        "text": "(Yu et al., 2017)",
                        "ref_id": "BIBREF28"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "5.3"
            },
            {
                "text": "SeqGAN -full adversarial training (Yu et al., 2017) 1.99 2.08 Recurrent GAN without pre-training (Press et al., 2017) 3.31",
                "cite_spans": [
                    {
                        "start": 34,
                        "end": 51,
                        "text": "(Yu et al., 2017)",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 97,
                        "end": 117,
                        "text": "(Press et al., 2017)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "GANs (LM Approximation)",
                "sec_num": null
            },
            {
                "text": "Uniform Distribution 4.75 Because SeqGAN models output a distribution over tokens at every time step, we can measure the true BPC and assess the quality of our approximation. Indeed, we observe that approximate BPC is only slightly higher than the true BPC.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "GANs (LM Approximation)",
                "sec_num": null
            },
            {
                "text": "GAN-based models perform worse than stateof-the-art LMs by a large margin. Moreover, in SeqGAN, the pre-trained LM performs better than the fully trained model with approximate BPC scores of 1.95 and 2.06, respectively, and the BPC deteriorates as adversarial training continues.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "GANs (LM Approximation)",
                "sec_num": null
            },
            {
                "text": "Finally, we note that generating sequences larger than 7 characters hurts the BPC of Press et al. (2017) . It is difficult to assess the quality of generation with such short sequences.",
                "cite_spans": [
                    {
                        "start": 85,
                        "end": 104,
                        "text": "Press et al. (2017)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "GANs (LM Approximation)",
                "sec_num": null
            },
            {
                "text": "In Table 2 we present a few randomly gener-ated samples from each model. We indeed observe that adversarial training slightly reduces the quality of generated text for SeqGAN, and find that the quality of 100-character long sequences generated from Press et al. ( 2017) is low.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 9,
                        "end": 10,
                        "text": "2",
                        "ref_id": "TABREF2"
                    }
                ],
                "eq_spans": [],
                "section": "GANs (LM Approximation)",
                "sec_num": null
            },
            {
                "text": "We propose an evaluation procedure for text GANs that is based on approximating the GAN output distribution and using standard LM metrics. We provide a bound for the number of samples required for the approximation and empirically show in practice as few as 2000 samples per time-step suffice. We evaluate character-based GAN models using our procedure, and show their performance is substantially lower than state-of-the-art LM. We hope our simple evaluation method leads to progress in GAN-based text generation by shedding light on the quality of such models.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusions",
                "sec_num": "6"
            }
        ],
        "back_matter": [
            {
                "text": "We would like to thank Shimi Salant for his comments and suggestions. This research was partially supported by The Israel Science Foundation grant 942/16, the Blavatnik Computer Science Research Fund, and The Yandex Initiative for Machine Learning.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgements",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Language gans falling short",
                "authors": [
                    {
                        "first": "Massimo",
                        "middle": [],
                        "last": "Caccia",
                        "suffix": ""
                    },
                    {
                        "first": "Lucas",
                        "middle": [],
                        "last": "Caccia",
                        "suffix": ""
                    },
                    {
                        "first": "William",
                        "middle": [],
                        "last": "Fedus",
                        "suffix": ""
                    },
                    {
                        "first": "Hugo",
                        "middle": [],
                        "last": "Larochelle",
                        "suffix": ""
                    },
                    {
                        "first": "Joelle",
                        "middle": [],
                        "last": "Pineau",
                        "suffix": ""
                    },
                    {
                        "first": "Laurent",
                        "middle": [],
                        "last": "Charlin",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1811.02549"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Massimo Caccia, Lucas Caccia, William Fedus, Hugo Larochelle, Joelle Pineau, and Laurent Charlin. 2018. Language gans falling short. arXiv preprint arXiv:1811.02549.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "The best of both worlds: Combining recent advances in neural machine translation",
                "authors": [
                    {
                        "first": "Mia",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Chen",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Orhan",
                        "middle": [],
                        "last": "Firat",
                        "suffix": ""
                    },
                    {
                        "first": "Ankur",
                        "middle": [],
                        "last": "Bapna",
                        "suffix": ""
                    },
                    {
                        "first": "Melvin",
                        "middle": [],
                        "last": "Johnson",
                        "suffix": ""
                    },
                    {
                        "first": "Wolfgang",
                        "middle": [],
                        "last": "Macherey",
                        "suffix": ""
                    },
                    {
                        "first": "George",
                        "middle": [],
                        "last": "Foster",
                        "suffix": ""
                    },
                    {
                        "first": "Llion",
                        "middle": [],
                        "last": "Jones",
                        "suffix": ""
                    },
                    {
                        "first": "Niki",
                        "middle": [],
                        "last": "Parmar",
                        "suffix": ""
                    },
                    {
                        "first": "Mike",
                        "middle": [],
                        "last": "Schuster",
                        "suffix": ""
                    },
                    {
                        "first": "Zhifeng",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1804.09849"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Mia Xu Chen, Orhan Firat, Ankur Bapna, Melvin Johnson, Wolfgang Macherey, George Foster, Llion Jones, Niki Parmar, Mike Schuster, Zhifeng Chen, et al. 2018. The best of both worlds: Combining re- cent advances in neural machine translation. arXiv preprint arXiv:1804.09849.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "A measure of asymptotic efficiency for tests of a hypothesis based on the sum of observations",
                "authors": [
                    {
                        "first": "Herman",
                        "middle": [],
                        "last": "Chernoff",
                        "suffix": ""
                    }
                ],
                "year": 1952,
                "venue": "The Annals of Mathematical Statistics",
                "volume": "23",
                "issue": "4",
                "pages": "493--507",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Herman Chernoff et al. 1952. A measure of asymp- totic efficiency for tests of a hypothesis based on the sum of observations. The Annals of Mathematical Statistics, 23(4):493-507.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Hierarchical multiscale recurrent neural networks",
                "authors": [
                    {
                        "first": "Junyoung",
                        "middle": [],
                        "last": "Chung",
                        "suffix": ""
                    },
                    {
                        "first": "Sungjin",
                        "middle": [],
                        "last": "Ahn",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1609.01704"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Junyoung Chung, Sungjin Ahn, and Yoshua Bengio. 2016. Hierarchical multiscale recurrent neural net- works. arXiv preprint arXiv:1609.01704.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Recurrent batch normalization",
                "authors": [
                    {
                        "first": "Tim",
                        "middle": [],
                        "last": "Cooijmans",
                        "suffix": ""
                    },
                    {
                        "first": "Nicolas",
                        "middle": [],
                        "last": "Ballas",
                        "suffix": ""
                    },
                    {
                        "first": "C\u00e9sar",
                        "middle": [],
                        "last": "Laurent",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [],
                        "last": "\u00b8aglar G\u00fclc \u00b8ehre",
                        "suffix": ""
                    },
                    {
                        "first": "Aaron",
                        "middle": [],
                        "last": "Courville",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1603.09025"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Tim Cooijmans, Nicolas Ballas, C\u00e9sar Laurent, C \u00b8aglar G\u00fclc \u00b8ehre, and Aaron Courville. 2016. Re- current batch normalization. arXiv preprint arXiv:1603.09025.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Neural network methods for natural language processing",
                "authors": [
                    {
                        "first": "Yoav",
                        "middle": [],
                        "last": "Goldberg",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Synthesis Lectures on Human Language Technologies",
                "volume": "10",
                "issue": "1",
                "pages": "1--309",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yoav Goldberg. 2017. Neural network methods for nat- ural language processing. Synthesis Lectures on Hu- man Language Technologies, 10(1):1-309.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Generative adversarial nets",
                "authors": [
                    {
                        "first": "Ian",
                        "middle": [],
                        "last": "Goodfellow",
                        "suffix": ""
                    },
                    {
                        "first": "Jean",
                        "middle": [],
                        "last": "Pouget-Abadie",
                        "suffix": ""
                    },
                    {
                        "first": "Mehdi",
                        "middle": [],
                        "last": "Mirza",
                        "suffix": ""
                    },
                    {
                        "first": "Bing",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Warde-Farley",
                        "suffix": ""
                    },
                    {
                        "first": "Sherjil",
                        "middle": [],
                        "last": "Ozair",
                        "suffix": ""
                    },
                    {
                        "first": "Aaron",
                        "middle": [],
                        "last": "Courville",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Advances in neural information processing systems",
                "volume": "",
                "issue": "",
                "pages": "2672--2680",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative ad- versarial nets. In Advances in neural information processing systems, pages 2672-2680.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Improved training of wasserstein gans",
                "authors": [
                    {
                        "first": "Ishaan",
                        "middle": [],
                        "last": "Gulrajani",
                        "suffix": ""
                    },
                    {
                        "first": "Faruk",
                        "middle": [],
                        "last": "Ahmed",
                        "suffix": ""
                    },
                    {
                        "first": "Martin",
                        "middle": [],
                        "last": "Arjovsky",
                        "suffix": ""
                    },
                    {
                        "first": "Aaron",
                        "middle": [
                            "C"
                        ],
                        "last": "Vincent Dumoulin",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Courville",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "5767--5777",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vin- cent Dumoulin, and Aaron C Courville. 2017. Im- proved training of wasserstein gans. In Advances in Neural Information Processing Systems, pages 5767-5777.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Long text generation via adversarial training with leaked information",
                "authors": [
                    {
                        "first": "Jiaxian",
                        "middle": [],
                        "last": "Guo",
                        "suffix": ""
                    },
                    {
                        "first": "Sidi",
                        "middle": [],
                        "last": "Lu",
                        "suffix": ""
                    },
                    {
                        "first": "Han",
                        "middle": [],
                        "last": "Cai",
                        "suffix": ""
                    },
                    {
                        "first": "Weinan",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Yong",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Jun",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1709.08624"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jiaxian Guo, Sidi Lu, Han Cai, Weinan Zhang, Yong Yu, and Jun Wang. 2017. Long text generation via adversarial training with leaked information. arXiv preprint arXiv:1709.08624.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Gans trained by a two time-scale update rule converge to a nash equilibrium",
                "authors": [
                    {
                        "first": "Martin",
                        "middle": [],
                        "last": "Heusel",
                        "suffix": ""
                    },
                    {
                        "first": "Hubert",
                        "middle": [],
                        "last": "Ramsauer",
                        "suffix": ""
                    },
                    {
                        "first": "Thomas",
                        "middle": [],
                        "last": "Unterthiner",
                        "suffix": ""
                    },
                    {
                        "first": "Bernhard",
                        "middle": [],
                        "last": "Nessler",
                        "suffix": ""
                    },
                    {
                        "first": "G\u00fcnter",
                        "middle": [],
                        "last": "Klambauer",
                        "suffix": ""
                    },
                    {
                        "first": "Sepp",
                        "middle": [],
                        "last": "Hochreiter",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1706.08500"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, G\u00fcnter Klambauer, and Sepp Hochreiter. 2017. Gans trained by a two time-scale update rule converge to a nash equilibrium. arXiv preprint arXiv:1706.08500.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Probability inequalities for sums of bounded random variables",
                "authors": [
                    {
                        "first": "Wassily",
                        "middle": [],
                        "last": "Hoeffding",
                        "suffix": ""
                    }
                ],
                "year": 1963,
                "venue": "Journal of the American statistical association",
                "volume": "58",
                "issue": "301",
                "pages": "13--30",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Wassily Hoeffding. 1963. Probability inequalities for sums of bounded random variables. Journal of the American statistical association, 58(301):13-30.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Speech and language processing: Third Edition",
                "authors": [
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Jurafsky",
                        "suffix": ""
                    },
                    {
                        "first": "James",
                        "middle": [
                            "H"
                        ],
                        "last": "Martin",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "3",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dan Jurafsky and James H Martin. 2018. Speech and language processing: Third Edition, volume 3. Pearson London.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Character-aware neural language models",
                "authors": [
                    {
                        "first": "Yoon",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "Yacine",
                        "middle": [],
                        "last": "Jernite",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Sontag",
                        "suffix": ""
                    },
                    {
                        "first": "Alexander",
                        "middle": [
                            "M"
                        ],
                        "last": "Rush",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "AAAI",
                "volume": "",
                "issue": "",
                "pages": "2741--2749",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yoon Kim, Yacine Jernite, David Sontag, and Alexan- der M Rush. 2016. Character-aware neural language models. In AAAI, pages 2741-2749.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Dynamic evaluation of neural sequence models",
                "authors": [
                    {
                        "first": "Ben",
                        "middle": [],
                        "last": "Krause",
                        "suffix": ""
                    },
                    {
                        "first": "Emmanuel",
                        "middle": [],
                        "last": "Kahembwe",
                        "suffix": ""
                    },
                    {
                        "first": "Iain",
                        "middle": [],
                        "last": "Murray",
                        "suffix": ""
                    },
                    {
                        "first": "Steve",
                        "middle": [],
                        "last": "Renals",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1709.07432"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ben Krause, Emmanuel Kahembwe, Iain Murray, and Steve Renals. 2017. Dynamic evaluation of neural sequence models. arXiv preprint arXiv:1709.07432.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Multiplicative lstm for sequence modelling",
                "authors": [
                    {
                        "first": "Ben",
                        "middle": [],
                        "last": "Krause",
                        "suffix": ""
                    },
                    {
                        "first": "Liang",
                        "middle": [],
                        "last": "Lu",
                        "suffix": ""
                    },
                    {
                        "first": "Iain",
                        "middle": [],
                        "last": "Murray",
                        "suffix": ""
                    },
                    {
                        "first": "Steve",
                        "middle": [],
                        "last": "Renals",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1609.07959"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ben Krause, Liang Lu, Iain Murray, and Steve Renals. 2016. Multiplicative lstm for sequence modelling. arXiv preprint arXiv:1609.07959.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Neural text generation: Past, present and beyond",
                "authors": [
                    {
                        "first": "Sidi",
                        "middle": [],
                        "last": "Lu",
                        "suffix": ""
                    },
                    {
                        "first": "Yaoming",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    },
                    {
                        "first": "Weinan",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Jun",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Yong",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1803.07133"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Sidi Lu, Yaoming Zhu, Weinan Zhang, Jun Wang, and Yong Yu. 2018. Neural text genera- tion: Past, present and beyond. arXiv preprint arXiv:1803.07133.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Multi-task sequence to sequence learning",
                "authors": [
                    {
                        "first": "Minh-Thang",
                        "middle": [],
                        "last": "Luong",
                        "suffix": ""
                    },
                    {
                        "first": "V",
                        "middle": [],
                        "last": "Quoc",
                        "suffix": ""
                    },
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Le",
                        "suffix": ""
                    },
                    {
                        "first": "Oriol",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    },
                    {
                        "first": "Lukasz",
                        "middle": [],
                        "last": "Vinyals",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Kaiser",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1511.06114"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Minh-Thang Luong, Quoc V Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. 2015. Multi-task sequence to sequence learning. arXiv preprint arXiv:1511.06114.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Neural machine translation and sequence-to-sequence models: A tutorial",
                "authors": [
                    {
                        "first": "Graham",
                        "middle": [],
                        "last": "Neubig",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1703.01619"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Graham Neubig. 2017. Neural machine translation and sequence-to-sequence models: A tutorial. arXiv preprint arXiv:1703.01619.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Bleu: a method for automatic evaluation of machine translation",
                "authors": [
                    {
                        "first": "Kishore",
                        "middle": [],
                        "last": "Papineni",
                        "suffix": ""
                    },
                    {
                        "first": "Salim",
                        "middle": [],
                        "last": "Roukos",
                        "suffix": ""
                    },
                    {
                        "first": "Todd",
                        "middle": [],
                        "last": "Ward",
                        "suffix": ""
                    },
                    {
                        "first": "Wei-Jing",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Proceedings of the 40th annual meeting on association for computational linguistics",
                "volume": "",
                "issue": "",
                "pages": "311--318",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic eval- uation of machine translation. In Proceedings of the 40th annual meeting on association for compu- tational linguistics, pages 311-318. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Language generation with recurrent generative adversarial networks without pretraining",
                "authors": [
                    {
                        "first": "Ofir",
                        "middle": [],
                        "last": "Press",
                        "suffix": ""
                    },
                    {
                        "first": "Amir",
                        "middle": [],
                        "last": "Bar",
                        "suffix": ""
                    },
                    {
                        "first": "Ben",
                        "middle": [],
                        "last": "Bogin",
                        "suffix": ""
                    },
                    {
                        "first": "Jonathan",
                        "middle": [],
                        "last": "Berant",
                        "suffix": ""
                    },
                    {
                        "first": "Lior",
                        "middle": [],
                        "last": "Wolf",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ofir Press, Amir Bar, Ben Bogin, Jonathan Berant, and Lior Wolf. 2017. Language generation with re- current generative adversarial networks without pre- training. In 1st Workshop on Learning to Generate Natural Language at ICML 2017.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Adversarial generation of natural language",
                "authors": [
                    {
                        "first": "Sai",
                        "middle": [],
                        "last": "Rajeswar",
                        "suffix": ""
                    },
                    {
                        "first": "Sandeep",
                        "middle": [],
                        "last": "Subramanian",
                        "suffix": ""
                    },
                    {
                        "first": "Francis",
                        "middle": [],
                        "last": "Dutil",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [],
                        "last": "Pal",
                        "suffix": ""
                    },
                    {
                        "first": "Aaron",
                        "middle": [],
                        "last": "Courville",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1705.10929"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Sai Rajeswar, Sandeep Subramanian, Francis Dutil, Christopher Pal, and Aaron Courville. 2017. Adver- sarial generation of natural language. arXiv preprint arXiv:1705.10929.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Sequence level training with recurrent neural networks",
                "authors": [
                    {
                        "first": "Aurelio",
                        "middle": [],
                        "last": "Marc",
                        "suffix": ""
                    },
                    {
                        "first": "Sumit",
                        "middle": [],
                        "last": "Ranzato",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Chopra",
                        "suffix": ""
                    },
                    {
                        "first": "Wojciech",
                        "middle": [],
                        "last": "Auli",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Zaremba",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1511.06732"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Marc'Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. 2015. Sequence level train- ing with recurrent neural networks. arXiv preprint arXiv:1511.06732.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Get to the point: Summarization with pointergenerator networks",
                "authors": [
                    {
                        "first": "Abigail",
                        "middle": [],
                        "last": "See",
                        "suffix": ""
                    },
                    {
                        "first": "Peter",
                        "middle": [
                            "J"
                        ],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Abigail See, Peter J Liu, and Christopher D Manning. 2017. Get to the point: Summarization with pointer- generator networks.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "On accurate evaluation of gans for language generation",
                "authors": [
                    {
                        "first": "Stanislau",
                        "middle": [],
                        "last": "Semeniuta",
                        "suffix": ""
                    },
                    {
                        "first": "Aliaksei",
                        "middle": [],
                        "last": "Severyn",
                        "suffix": ""
                    },
                    {
                        "first": "Sylvain",
                        "middle": [],
                        "last": "Gelly",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1806.04936"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Stanislau Semeniuta, Aliaksei Severyn, and Syl- vain Gelly. 2018. On accurate evaluation of gans for language generation. arXiv preprint arXiv:1806.04936.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Sequence to sequence learning with neural networks",
                "authors": [
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    },
                    {
                        "first": "Oriol",
                        "middle": [],
                        "last": "Vinyals",
                        "suffix": ""
                    },
                    {
                        "first": "Quoc V",
                        "middle": [],
                        "last": "Le",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Advances in neural information processing systems",
                "volume": "",
                "issue": "",
                "pages": "3104--3112",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural net- works. In Advances in neural information process- ing systems, pages 3104-3112.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Policy gradient methods for reinforcement learning with function approximation",
                "authors": [
                    {
                        "first": "David",
                        "middle": [
                            "A"
                        ],
                        "last": "Richard S Sutton",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Mcallester",
                        "suffix": ""
                    },
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Satinder",
                        "suffix": ""
                    },
                    {
                        "first": "Yishay",
                        "middle": [],
                        "last": "Singh",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Mansour",
                        "suffix": ""
                    }
                ],
                "year": 2000,
                "venue": "Advances in neural information processing systems",
                "volume": "",
                "issue": "",
                "pages": "1057--1063",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. 2000. Policy gradi- ent methods for reinforcement learning with func- tion approximation. In Advances in neural informa- tion processing systems, pages 1057-1063.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Attention is all you need",
                "authors": [
                    {
                        "first": "Ashish",
                        "middle": [],
                        "last": "Vaswani",
                        "suffix": ""
                    },
                    {
                        "first": "Noam",
                        "middle": [],
                        "last": "Shazeer",
                        "suffix": ""
                    },
                    {
                        "first": "Niki",
                        "middle": [],
                        "last": "Parmar",
                        "suffix": ""
                    },
                    {
                        "first": "Jakob",
                        "middle": [],
                        "last": "Uszkoreit",
                        "suffix": ""
                    },
                    {
                        "first": "Llion",
                        "middle": [],
                        "last": "Jones",
                        "suffix": ""
                    },
                    {
                        "first": "Aidan",
                        "middle": [
                            "N"
                        ],
                        "last": "Gomez",
                        "suffix": ""
                    },
                    {
                        "first": "\u0141ukasz",
                        "middle": [],
                        "last": "Kaiser",
                        "suffix": ""
                    },
                    {
                        "first": "Illia",
                        "middle": [],
                        "last": "Polosukhin",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "5998--6008",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Pro- cessing Systems, pages 5998-6008.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Image captioning with semantic attention",
                "authors": [
                    {
                        "first": "Quanzeng",
                        "middle": [],
                        "last": "You",
                        "suffix": ""
                    },
                    {
                        "first": "Hailin",
                        "middle": [],
                        "last": "Jin",
                        "suffix": ""
                    },
                    {
                        "first": "Zhaowen",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Chen",
                        "middle": [],
                        "last": "Fang",
                        "suffix": ""
                    },
                    {
                        "first": "Jiebo",
                        "middle": [],
                        "last": "Luo",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
                "volume": "",
                "issue": "",
                "pages": "4651--4659",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, and Jiebo Luo. 2016. Image captioning with seman- tic attention. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4651-4659.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Seqgan: Sequence generative adversarial nets with policy gradient",
                "authors": [
                    {
                        "first": "Lantao",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Weinan",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Jun",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Yong",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "AAAI",
                "volume": "",
                "issue": "",
                "pages": "2852--2858",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. 2017. Seqgan: Sequence generative adversarial nets with policy gradient. In AAAI, pages 2852-2858.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Adversarially regularized autoencoders for generating discrete structures",
                "authors": [
                    {
                        "first": "Jake",
                        "middle": [],
                        "last": "Junbo",
                        "suffix": ""
                    },
                    {
                        "first": "Yoon",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Kelly",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "Alexander",
                        "middle": [
                            "M"
                        ],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Yann",
                        "middle": [],
                        "last": "Rush",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Lecun",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Junbo Jake Zhao, Yoon Kim, Kelly Zhang, Alexan- der M Rush, and Yann LeCun. 2017. Adversari- ally regularized autoencoders for generating discrete structures. CoRR, abs/1706.04223.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "Recurrent highway networks",
                "authors": [
                    {
                        "first": "Julian",
                        "middle": [],
                        "last": "Georg Zilly",
                        "suffix": ""
                    },
                    {
                        "first": "Rupesh",
                        "middle": [
                            "Kumar"
                        ],
                        "last": "Srivastava",
                        "suffix": ""
                    },
                    {
                        "first": "Jan",
                        "middle": [],
                        "last": "Koutn\u00edk",
                        "suffix": ""
                    },
                    {
                        "first": "J\u00fcrgen",
                        "middle": [],
                        "last": "Schmidhuber",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1607.03474"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Julian Georg Zilly, Rupesh Kumar Srivastava, Jan Koutn\u00edk, and J\u00fcrgen Schmidhuber. 2016. Recurrent highway networks. arXiv preprint arXiv:1607.03474.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "x t ), G t is a stochastic function: the stochasticity of G t can Algorithm 1 LM Evaluation of RNN-based GANs Input: Gt(\u2022): the generator function at time step t (x0, ..., xt): previous gold tokens xt+1: the gold next token (as ground truth) f (\u2022, \u2022): a LM evaluation metric N : number of samples 1: for n \u2190 1 to N do 2: gt,n \u2190sample from Gt(x0...xt) 3: Gt,N = 1 N \u03a3 N n=1 gt,n 4: return f ( Gt,N , xt+1)",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "4) As a numerical example, choosing \u03b3 = 10 -3 and = 10 -2 , for a character-based LM over the text8 dataset, with |V | = 27, we get the bound: N > 4.3 \u2022 10 6 . With the same \u03b3 and , a typical word-based LM with vocabulary size |V | = 50, 000 would require N > 8.1 \u2022 10 6 .",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "Figure 2: Approximate error Gt,N-\u03b1 -Gt,N \u221e as a function of samples N . \u03b1 = 10, \u03b3 = 10 -3 .",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table><tr><td>o t-1</td><td>o t</td><td>o t+1</td><td>o t+2</td></tr><tr><td>h t</td><td/><td/><td/><td>h t+3</td></tr><tr><td/><td>x t</td><td>x t+1</td><td>x t+2</td><td>x t+3</td></tr><tr><td>Figure 1:</td><td/><td/><td/></tr><tr><td/><td/><td/><td/><td>\u2022 Rajeswar et al. (2017) used a context-free gram-</td></tr><tr><td/><td/><td/><td/><td>mar (CFG) to generate a reference corpus, and</td></tr><tr><td/><td/><td/><td/><td>evaluated the model by the likelihood the CFG</td></tr><tr><td/><td/><td/><td/><td>assigns to generated samples. However, sim-</td></tr><tr><td/><td/><td/><td/><td>ple CFGs do not fully capture the complexity</td></tr><tr><td/><td/><td/><td/><td>of natural language.</td></tr><tr><td/><td/><td/><td/><td>\u2022 To overcome the drawbacks of each individual</td></tr><tr><td/><td/><td/><td/><td>method, Semeniuta et al. (2018) proposed a uni-</td></tr><tr><td/><td/><td/><td/><td>fied measure based on multiple evaluation met-</td></tr><tr><td/><td/><td/><td/><td>rics (N-grams, BLEU variations, FID, LM score</td></tr><tr><td/><td/><td/><td/><td>variations and human evaluation). Specifically,</td></tr><tr><td/><td/><td/><td/><td>they argue that the different measures capture</td></tr><tr><td/><td/><td/><td/><td>different desired properties of LMs, e.g., qual-</td></tr><tr><td/><td/><td/><td/><td>ity vs. diversity.</td></tr><tr><td/><td/><td/><td/><td>\u2022 Following Semeniuta et al. (2018), and in paral-</td></tr></table>",
                "type_str": "table",
                "text": "Generator recurrent connections. {h t } is the internal state sequence and {o t } is the generator prediction sequence (one-hot). During inference, the outputs {o t } are fed back as the input for the next time step (dashed lines). During LM approximation, the input {x t } is a sequence of one-hot vectors from the test set.Overall, current evaluation methods cannot fully capture the performance of GAN-based text generation models. While reporting various scores as proposed bySemeniuta et al. (2018) is possible, it is preferable to have a single measure of progress when comparing different text generation models.",
                "html": null,
                "num": null
            },
            "TABREF1": {
                "content": "<table><tr><td>Model</td><td>Samples</td></tr><tr><td colspan=\"2\">SeqGAN Pre-trained LM 1. rics SeqGAN 1. four</td></tr><tr><td>Full adversarial</td><td/></tr><tr><td>training</td><td/></tr></table>",
                "type_str": "table",
                "text": "Test set evaluation of different character-based models on the text8 dataset. State-of-the-art results are taken from https://github.com/sebastianruder/NLP-progress/blob/master/language_ modeling.md. The uniform distribution is equivalent to guessing the next character out of |V | = 27 characters. things where a weeks thered databignand jacob reving the imprisoners could become poveran brown 2. nine other set of of one eight one two by belarigho and singing signal theus to accept natural corp 3. ragems the downran maintain the lagar linear stream hegels p in five six f march one nine nine nine zero five two memaire in afulie war formally dream the living of the centuries to quickly can f 2. part of the pract the name in one nine seven were mustring of the airports tex works to eroses exten 3. eight four th jania lpa ore nine zero zero zero sport for tail concents englished a possible for po Recurrent GAN 1. nteractice computer may became were the generally treat he were computer may became were the general 2. lnannnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnne and and and and and and and and and and and and and and and a 3. perors as as seases as as as as as as as as as selected see see see see see see see see see see see",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table><tr><td>Error (infinity norm)</td><td>10 3 10 2</td><td>Recurrent GAN seqGAN</td></tr><tr><td/><td/><td>0 500 1000 1500 2000 2500 3000 Number of generator runs (N)</td></tr></table>",
                "type_str": "table",
                "text": "Random samples of 100 characters generated by each model.",
                "html": null,
                "num": null
            }
        }
    }
}