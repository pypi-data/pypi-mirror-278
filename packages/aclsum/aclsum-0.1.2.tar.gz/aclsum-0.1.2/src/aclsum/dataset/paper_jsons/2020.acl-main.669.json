{
    "paper_id": "2020",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T11:54:08.628873Z"
    },
    "title": "Revisiting Unsupervised Relation Extraction",
    "authors": [
        {
            "first": "Thy",
            "middle": [
                "Thy"
            ],
            "last": "Tran",
            "suffix": "",
            "affiliation": {
                "laboratory": "National Centre for Text Mining",
                "institution": "University of Manchester",
                "location": {
                    "country": "United Kingdom"
                }
            },
            "email": "thy.tran@manchester.ac.uk"
        },
        {
            "first": "Phong",
            "middle": [],
            "last": "Le",
            "suffix": "",
            "affiliation": {
                "laboratory": "National Centre for Text Mining",
                "institution": "University of Manchester",
                "location": {
                    "country": "United Kingdom"
                }
            },
            "email": "phong.le@manchester.ac.uk"
        },
        {
            "first": "Sophia",
            "middle": [],
            "last": "Ananiadou",
            "suffix": "",
            "affiliation": {
                "laboratory": "National Centre for Text Mining",
                "institution": "University of Manchester",
                "location": {
                    "country": "United Kingdom"
                }
            },
            "email": "sophia.ananiadou@manchester.ac.uk"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Unsupervised relation extraction (URE) extracts relations between named entities from raw text without manually-labelled data and existing knowledge bases (KBs). URE methods can be categorised into generative and discriminative approaches, which rely either on hand-crafted features or surface form. However, we demonstrate that by using only named entities to induce relation types, we can outperform existing methods on two popular datasets. We conduct a comparison and evaluation of our findings with other URE techniques, to ascertain the important features in URE. We conclude that entity types provide a strong inductive bias for URE. 1 ",
    "pdf_parse": {
        "paper_id": "2020",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Unsupervised relation extraction (URE) extracts relations between named entities from raw text without manually-labelled data and existing knowledge bases (KBs). URE methods can be categorised into generative and discriminative approaches, which rely either on hand-crafted features or surface form. However, we demonstrate that by using only named entities to induce relation types, we can outperform existing methods on two popular datasets. We conduct a comparison and evaluation of our findings with other URE techniques, to ascertain the important features in URE. We conclude that entity types provide a strong inductive bias for URE. 1 ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Relation extraction (RE) extracts semantic relations between entities from plain text. For instance, \"Jon Robin Baitz head , born in Los Angeles tail ...\" expresses the relation /people/person/place of birth between the two head-tail entities. Extracted relations are then used for several downstream tasks such as information retrieval (Corcoglioniti et al., 2016) and knowledge base construction (Al-Zaidy and Giles, 2018) . RE has been widely studied using fully supervised learning (Nguyen and Grishman, 2015; Miwa and Bansal, 2016; Zhang et al., 2017 Zhang et al., , 2018) ) and distantly supervised approaches (Mintz et al., 2009; Riedel et al., 2010; Lin et al., 2016) .",
                "cite_spans": [
                    {
                        "start": 337,
                        "end": 365,
                        "text": "(Corcoglioniti et al., 2016)",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 398,
                        "end": 424,
                        "text": "(Al-Zaidy and Giles, 2018)",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 486,
                        "end": 513,
                        "text": "(Nguyen and Grishman, 2015;",
                        "ref_id": null
                    },
                    {
                        "start": 514,
                        "end": 536,
                        "text": "Miwa and Bansal, 2016;",
                        "ref_id": null
                    },
                    {
                        "start": 537,
                        "end": 555,
                        "text": "Zhang et al., 2017",
                        "ref_id": null
                    },
                    {
                        "start": 556,
                        "end": 579,
                        "text": "Zhang et al., , 2018) )",
                        "ref_id": null
                    },
                    {
                        "start": 616,
                        "end": 636,
                        "text": "(Mintz et al., 2009;",
                        "ref_id": null
                    },
                    {
                        "start": 637,
                        "end": 657,
                        "text": "Riedel et al., 2010;",
                        "ref_id": null
                    },
                    {
                        "start": 658,
                        "end": 675,
                        "text": "Lin et al., 2016)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Unsupervised relation extraction (URE) methods have not been explored as much as fully or distantly supervised learning techniques. URE is promising, since it does not require manually annotated data nor human curated knowledge bases (KBs), which are expensive to produce. Therefore, it can be applied to domains and languages where annotated data and KBs are not available. Moreover, URE can discover new relation types, since it is not restricted to specific relation types in the same way as fully and distantly supervised methods. One might argue that Open Information Extraction (OpenIE) can also discover new relations. However, OpenIE identifies relations based on textual surface information. Thus, similar relations with different textual forms may not be recognised. Unlike OpenIE, URE groups similar relations into clusters. Despite these advantages, there are only a few attempts tackling URE using machine learning (ML) (Hasegawa et al., 2004; Banko et al., 2007; Yao et al., 2011; Marcheggiani and Titov, 2016; Simon et al., 2019) .",
                "cite_spans": [
                    {
                        "start": 933,
                        "end": 956,
                        "text": "(Hasegawa et al., 2004;",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 957,
                        "end": 976,
                        "text": "Banko et al., 2007;",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 977,
                        "end": 994,
                        "text": "Yao et al., 2011;",
                        "ref_id": null
                    },
                    {
                        "start": 995,
                        "end": 1024,
                        "text": "Marcheggiani and Titov, 2016;",
                        "ref_id": null
                    },
                    {
                        "start": 1025,
                        "end": 1044,
                        "text": "Simon et al., 2019)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Similarly to other unsupervised learning tasks, a challenge in URE is how to evaluate results. Recent approaches (Yao et al., 2011; Marcheggiani and Titov, 2016; Simon et al., 2019 ) employ a widely used data generation setting in distantly supervised RE, i.e., aligning a large amount of raw text against triplets in a curated KB. A standard metric score is computed by comparing the output relation clusters against the automatically annotated relations. In particular, the NYT-FB dataset (Marcheggiani and Titov, 2016) which is used for evaluation, has been created by mapping relation triplets in Freebase (Bollacker et al., 2008) against plain text articles in the New York Times (NYT) corpus (Sandhaus, 2008) . Standard clustering evaluation metrics for URE include B 3 (Bagga and Baldwin, 1998), V-measure (Rosenberg and Hirschberg, 2007) , and ARI (Hubert and Arabie, 1985) .",
                "cite_spans": [
                    {
                        "start": 113,
                        "end": 131,
                        "text": "(Yao et al., 2011;",
                        "ref_id": null
                    },
                    {
                        "start": 132,
                        "end": 161,
                        "text": "Marcheggiani and Titov, 2016;",
                        "ref_id": null
                    },
                    {
                        "start": 162,
                        "end": 180,
                        "text": "Simon et al., 2019",
                        "ref_id": null
                    },
                    {
                        "start": 491,
                        "end": 521,
                        "text": "(Marcheggiani and Titov, 2016)",
                        "ref_id": null
                    },
                    {
                        "start": 610,
                        "end": 634,
                        "text": "(Bollacker et al., 2008)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 698,
                        "end": 714,
                        "text": "(Sandhaus, 2008)",
                        "ref_id": null
                    },
                    {
                        "start": 776,
                        "end": 786,
                        "text": "(Bagga and",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 787,
                        "end": 827,
                        "text": "Baldwin, 1998), V-measure (Rosenberg and",
                        "ref_id": null
                    },
                    {
                        "start": 828,
                        "end": 845,
                        "text": "Hirschberg, 2007)",
                        "ref_id": null
                    },
                    {
                        "start": 856,
                        "end": 881,
                        "text": "(Hubert and Arabie, 1985)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Although the above mentioned experimental setting can be created automatically, there are three challenges to overcome. Firstly, the development and test sets are silver, i.e., they include noisy labelled instances, since they are not human-curated. Secondly, the development and test sentences are part of the training set, i.e., a transductive setting.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "It is thus unclear how well the existing models perform on unseen sentences. Finally, NYT-FB can be considered highly imbalanced, since only 2.1% of the training sentences can be aligned with Freebase's triplets. Due to the noisy nature of silver data (NYT-FB), evaluation on silver data will not accurately reflect the system performance. We also need unseen data during testing to examine the system generalisation. To overcome these challenges, we will employ the test set of TACRED (Zhang et al., 2017) , a widely used manually annotated corpus. Regarding the imbalanced data, we will demonstrate that in fact around 60% (instead of 2.1%) of instances in the training set express relation types defined in Freebase.",
                "cite_spans": [
                    {
                        "start": 486,
                        "end": 506,
                        "text": "(Zhang et al., 2017)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this work, we present a simple URE approach relying only on entity types that can obtain improved performance compared to current methods. Specifically, given a sentence consisting of two entities and their corresponding entity types, e.g., PERSON and LOCATION, we induce relations as the combination of entity types, e.g., PERSON-LOCATION. It should be noted that we employ only entity types because their combinations form reasonably coarse relation types (e.g., PERSON-LOCATION covers /people/person/place of birth defined in Freebase). We further discuss our improved performance in \u00a73.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Our contributions are as follows: (i) We perform experiments on both automatically/manuallylabelled datasets, namely NYT-FB and TACRED, respectively. We show that two methods using only entity types can outperform the state-of-theart models including both feature-engineering and deep learning approaches. The surprising results raise questions about the current state of unsupervised relation extraction. (ii) For model design, we show that link predictor provides a good signal to train a URE model (Fig 1 ). We also illustrate that entity types are a strong inductive bias for URE (Table 1 ).",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 506,
                        "end": 507,
                        "text": "1",
                        "ref_id": null
                    },
                    {
                        "start": 591,
                        "end": 592,
                        "text": "1",
                        "ref_id": "TABREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The goal of URE is to predict the relation r between two entities e head and e tail in a sentence s. We will describe three recent ML-based methods tackling URE and our own methods. We divide the ML-based methods into two main approaches: generative and discriminative.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Methods for URE",
                "sec_num": "2"
            },
            {
                "text": "Yao et al. ( 2011) extended topic modelling -Latent Dirichlet Allocation (LDA) (Blei et al., 2003) for RE, developing two models, herewith RelLDA and RelLDA1. In both models, a sentence and an entity pair perform as a document in topic modelling, while a relation type corresponds to a topic. RelLDA uses three features, i.e., the shortest dependency path between two entities and the two entity mentions. RelLDA1 extends RelLDA with five more features, i.e., the entity types, words and part-of-speech tags between the two entities.",
                "cite_spans": [
                    {
                        "start": 79,
                        "end": 98,
                        "text": "(Blei et al., 2003)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generative Approach",
                "sec_num": "2.1"
            },
            {
                "text": "Marcheggiani and Titov (2016) proposed a discretestate variational autoencoder (VAE) to tackle URE (herewith March). Their model consists of two components: a relation classifier and a link predictor. The relation classifier, which is discriminative, takes entity types and several linguistic features (e.g., dependencies) as input to predict the relation r. The link predictor then uses the (soft) predicted relation r to predict the missing entity e i in a specific position {head, tail}, given the other entity e -i , where if i = head then -i = tail and vice versa. In other words, entity prediction, in a self-supervised manner, provides training signals to learn the relation classifier. However, by using only entity prediction, only a few relation types are chosen. They thus used entropy over all relations as a regulariser. The maximisation of the entropy regulariser ensures the uniform relation distribution and allows more relations to be predicted.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discriminative Approaches",
                "sec_num": "2.2"
            },
            {
                "text": "Another discriminative method is by Simon et al. (2019) (herewith Simon) which differs from March in the following ways: a) firstly, its relation classifier employs a piece-wise convolutional network (PCNN) using only surface form without requiring hand-crafted features; b) secondly, they replaced entropy with two regularisers: L s (skewness), to encourage the relation classifier to be confident in its prediction, and L d (dispersion), to ensure several relation types are predicted over a minibatch. Note that, L s is equivalent to the negation of the entropy used in March.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discriminative Approaches",
                "sec_num": "2.2"
            },
            {
                "text": "We introduce two entity-based methods, herewith EType and EType+. Our motivation is that entity types are helpful for RE, as mentioned in Zhang et al. ( 2017) for supervised learning and Ren et al.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Our Methods",
                "sec_num": "2.3"
            },
            {
                "text": "(2017) for distant learning. In URE, Yao et al. (2011) ; Marcheggiani and Titov (2016) also used entity types. We therefore propose EType that induces coarse relation clusters from the entity types. In particular, given two entity types t e head , t e tail as input, EType would output their concatenation t e head -t e tail as the relation.",
                "cite_spans": [
                    {
                        "start": 32,
                        "end": 54,
                        "text": "URE, Yao et al. (2011)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Our Methods",
                "sec_num": "2.3"
            },
            {
                "text": "One problem with EType is that the number of relation types is determined by the number of entity types. For instance, 4 entity types lead to 42 = 16 relation types. To extract an arbitrary number of relation types, we build a relation classifier that consists of one-layer feed-forward network taking entity type combinations as input:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Our Methods",
                "sec_num": "2.3"
            },
            {
                "text": "r = FFN(t e head -t e tail ),",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Our Methods",
                "sec_num": "2.3"
            },
            {
                "text": "where t e head -t e tail is the one hot vector of the entity type pair. We then employ the link predictor used in March and the two regularisers used in Simon, to produce a new method, herewith EType+.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Our Methods",
                "sec_num": "2.3"
            },
            {
                "text": "Evaluation metrics We use the following evaluation metrics for our analysis: a) B 3 (Bagga and Baldwin, 1998 ) used in previous work, which is the harmonic mean of precision and recall for clustering task; b) V-measure (Rosenberg and Hirschberg, 2007), and c) ARI (Hubert and Arabie, 1985) used in Simon et al. ( 2019). 2 V-measure is analysed in terms of homogeneity and completeness, while ARI measures the similarity between two clusterings. We note that V-measure is sensitive to the dependency between the number of clusters and instances. A relatively small number of clusters compared to the number of instances should be used to maintain the comparability of using V-measure. More precisely, we evaluated V-measure of the trivial homogeneity, where there are only singular clusters (i.e., each instance is its own cluster). The V-measure of the trivial homogeneity on NYT-FB reached 43.77%, which is higher than all the implemented methods in Table 1 . Meanwhile, neither B 3 nor ARI encounters this problem. Datasets We employed NYT-FB for training and evaluation following previous work (Yao et al., 2011; Marcheggiani and Titov, 2016; Simon et al., 2019) . Because only 2.1% of the sentences in NYT-FB were aligned against Freebase's triplets, we were concerned whether this dataset contains indicates our implementation of the corresponding model. We note that all methods were trained on NYT-FB and evaluated on the test set of both NYT-FB and TACRED.",
                "cite_spans": [
                    {
                        "start": 84,
                        "end": 108,
                        "text": "(Bagga and Baldwin, 1998",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 264,
                        "end": 289,
                        "text": "(Hubert and Arabie, 1985)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 1097,
                        "end": 1115,
                        "text": "(Yao et al., 2011;",
                        "ref_id": null
                    },
                    {
                        "start": 1116,
                        "end": 1145,
                        "text": "Marcheggiani and Titov, 2016;",
                        "ref_id": null
                    },
                    {
                        "start": 1146,
                        "end": 1165,
                        "text": "Simon et al., 2019)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 957,
                        "end": 958,
                        "text": "1",
                        "ref_id": "TABREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Experiments and Results",
                "sec_num": "3"
            },
            {
                "text": "enough sentences for a model to learn relation types from Freebase. We thus examined 100 randomly chosen instances from 1.86m non-aligned sentences. We found that 61% of them (or 60% of the whole dataset) express relation types in Freebase. This suggests that the NYT-FB dataset can be employed to train a relation extractor. However, there are two further issues when evaluating URE methods on NYT-FB. Firstly, the development and test sets are all aligned sentences without human curation, which means that they include wrong/noisy labelled instances. In particular, we found that 35 out of 100 randomly chosen sentences were given incorrect relations. Secondly, the two validation sets are part of the training set. This setting is obviously not inductive, as it does not evaluate how a model performs on unseen sentences. Therefore, we additionally evaluate all methods (except topic modelling) on the test set of TACRED (Zhang et al., 2017) , a widely used manually annotated corpus for supervised RE. The statistics of both NYT-FB and TACRED are provided in Appendix A. Hyper-parameters We examine three models RelLDA1, March, and Simon using the reported hyper-parameters (Yao et al., 2011 ",
                "cite_spans": [
                    {
                        "start": 925,
                        "end": 945,
                        "text": "(Zhang et al., 2017)",
                        "ref_id": null
                    },
                    {
                        "start": 1179,
                        "end": 1196,
                        "text": "(Yao et al., 2011",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments and Results",
                "sec_num": "3"
            },
            {
                "text": "The results of our evaluation demonstrate that our models outperform previous methods, despite being simpler than them. These results lead us to the 3 github.com/diegma/relation-autoencoder following findings.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discussion",
                "sec_num": "4"
            },
            {
                "text": "In common with other unsupervised learning approaches, there is no guarantee that a URE model would learn the relation types in the used KBs and/or annotated data. A common solution is to employ inductive biases (Wagstaff, 2000) to guide the learning process towards desired relation types.",
                "cite_spans": [
                    {
                        "start": 212,
                        "end": 228,
                        "text": "(Wagstaff, 2000)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Do ML models employ proper inductive biases?",
                "sec_num": null
            },
            {
                "text": "Inductive biases can emanate from pre-processed data. Since our models outperform other methods, we conclude that entity type information alone constitutes a better bias than the biases employed by existing ML models. Indeed, entity types constitute a useful bias for this task. 2019), the link predictor itself cannot be trained without a good relation classifier. It suggests that the relation classifiers in both methods need to be improved. Empirical evidence shows that both Simon and March models are outperformed (in B 3 and V) by our Etype+, which uses the same link predictor. We also notice that both One relation and EType at the end sharing similar performances. This might imply that we only need one relation (matrix) to predict head/tail entities, as the link predictor is very expressive. However, the silver relations are clearly helpful as during the first 15 epochs their losses are much lower than others.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Do ML models employ proper inductive biases?",
                "sec_num": null
            },
            {
                "text": "Why was the performance on TACRED lower? Despite the fact that TACRED shares similar relation types with Freebase, we observed that both the March and Simon models consistently fare less well in terms of their performance on the TACRED dataset. More precisely, Simon model results in significantly worse performance on TACRED, with 15.7% in terms of B 3 , which is twice as low as on NYT-FB (39.4%). This performance drop might be attributed to the distributional shift of the two datasets: variation and semantic shift in vocabulary and language structure over time, since NYT was collected long before TACRED.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Do ML models employ proper inductive biases?",
                "sec_num": null
            },
            {
                "text": "How is the performance when combining entity types with other features? Our experiments using only entity types surprisingly perform higher than the previous state-of-the-art methods including feature engineering and deep learning models.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Do ML models employ proper inductive biases?",
                "sec_num": null
            },
            {
                "text": "However, we know that context information is crucial to distinguish the relation between two entities, as many RE studies have been proposed to integrate the context information to improve the RE performance. We conduct experiments when combining entity types with common features for RE in Table 2 . The list of features include: (i) Entity: textual surface form of two entities, (ii) BOW: bag of words between two entities, (iii) DepPath: words on the dependency path between two entities, (iv) POS: part-of-speech tag sequence between two entities, and (v) Trigger: DepPath without stop words.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 297,
                        "end": 298,
                        "text": "2",
                        "ref_id": "TABREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Do ML models employ proper inductive biases?",
                "sec_num": null
            },
            {
                "text": "In general, naively combining entity types with other features could not improve the model performance. Additionally, BOW feature had negative effects on the RE performance. This indicates that bag of words between two entities often include uninformative and redundant words, i.e., noises, that are difficult to eliminate using simple neural architectures. While (i)-(v) are widely used handcrafted features for RE, we also incorporated a neural-based context encoder PCNN which is the combination of Simon's PCNN encoder, the entity masking and position-aware attention proposed in (Zhang et al., 2017) . However, the performance of combining PCNN is also lower than only entity types.",
                "cite_spans": [
                    {
                        "start": 584,
                        "end": 604,
                        "text": "(Zhang et al., 2017)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Do ML models employ proper inductive biases?",
                "sec_num": null
            },
            {
                "text": "We have shown the importance of entity types in URE. Our methods use only entity types, yet they yield higher performance than previous work on both NYT-FB and TACRED. We have investigated the current experimental setting, concluding that a strong inductive bias is required to train a relation extraction model without labelled data. URE remains challenging, which requires improved methods to deal with silver data. We also plan to use different types of labelled data, e.g., domain specific data sets, to ascertain whether entity type information is more discriminative in sub-languages. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "5"
            },
            {
                "text": "Source code is available at https://github.com/ ttthy/ure",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "We used sklearn.metrics package to compute V-measure and ARI.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "We would like to thank the reviewers for their comments, Diego Marcheggiani for sharing his dataset with us, and \u00c9tienne Simon for sharing the hyperparameters. The first author thanks the University of Manchester for the Research Impact Scholarship Award. This work is also funded by Lloyds Register Foundation, Discovering Safety Programme, Thomas Ashton Institute.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgments",
                "sec_num": null
            },
            {
                "text": "Table 3 shows the statistics of the NYT-FB (Marcheggiani and Titov, 2016) and TA-CRED (Zhang et al., 2017) datasets. We followed the same data split and pre-processing described in Marcheggiani and Titov (2016) . For all methods, we trained on NYT-FB and evaluated them on both NYT-FB and TACRED.Figure 2 illustrates the relation distributions of two datasets: NYT-FB and TACRED. We can see that 15/253 most frequent relations account for 82.97% of the total number of instances in NYT-FB. Meanwhile, 15/41 relations sum upto 74.94% of the total number of instances in TACRED.",
                "cite_spans": [
                    {
                        "start": 181,
                        "end": 210,
                        "text": "Marcheggiani and Titov (2016)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "3",
                        "ref_id": null
                    },
                    {
                        "start": 303,
                        "end": 304,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "A Datasets",
                "sec_num": null
            },
            {
                "text": "We used the development set to stop the training process. For every model, we conducted three runs with different initialised parameters and computed the average performance. We list the hyperparameters of different models in Table 4 .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 232,
                        "end": 233,
                        "text": "4",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "B Hyper-parameter Settings",
                "sec_num": null
            },
            {
                "text": "Table 5 presents the average test scores of three runs on the NYT-FB and TACRED datasets. We note that the two models proposed by Marcheggiani and Titov (2016) and Simon et al. (2019) are sensitive to the hyper-parameters and thus difficult to train. We could not replicate the performance of Simon on the NYT-FB dataset. ",
                "cite_spans": [
                    {
                        "start": 130,
                        "end": 146,
                        "text": "Marcheggiani and",
                        "ref_id": null
                    },
                    {
                        "start": 147,
                        "end": 163,
                        "text": "Titov (2016) and",
                        "ref_id": null
                    },
                    {
                        "start": 164,
                        "end": 183,
                        "text": "Simon et al. (2019)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "5",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "C Detailed Results",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Extracting semantic relations for scholarly knowledge base construction",
                "authors": [
                    {
                        "first": "Al-Zaidy",
                        "middle": [],
                        "last": "Rabah",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Giles",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "2018 IEEE 12th international conference on semantic computing (ICSC)",
                "volume": "",
                "issue": "",
                "pages": "56--63",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Rabah A Al-Zaidy and C Lee Giles. 2018. Extracting semantic relations for scholarly knowledge base con- struction. In 2018 IEEE 12th international confer- ence on semantic computing (ICSC), pages 56-63. IEEE.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Algorithms for scoring coreference chains",
                "authors": [
                    {
                        "first": "Amit",
                        "middle": [],
                        "last": "Bagga",
                        "suffix": ""
                    },
                    {
                        "first": "Breck",
                        "middle": [],
                        "last": "Baldwin",
                        "suffix": ""
                    }
                ],
                "year": 1998,
                "venue": "Proceedings of the First Iternational Conference on Language Resources and Evaluation Workshop on Linguistics Coreference",
                "volume": "1",
                "issue": "",
                "pages": "563--566",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Amit Bagga and Breck Baldwin. 1998. Algorithms for scoring coreference chains. In Proceedings of the First Iternational Conference on Language Re- sources and Evaluation Workshop on Linguistics Coreference, volume 1, pages 563-566.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Open information extraction from the web",
                "authors": [
                    {
                        "first": "Michele",
                        "middle": [],
                        "last": "Banko",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Michael",
                        "suffix": ""
                    },
                    {
                        "first": "Stephen",
                        "middle": [],
                        "last": "Cafarella",
                        "suffix": ""
                    },
                    {
                        "first": "Matthew",
                        "middle": [],
                        "last": "Soderland",
                        "suffix": ""
                    },
                    {
                        "first": "Oren",
                        "middle": [],
                        "last": "Broadhead",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Etzioni",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "IJ-CAI",
                "volume": "7",
                "issue": "",
                "pages": "2670--2676",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Michele Banko, Michael J Cafarella, Stephen Soder- land, Matthew Broadhead, and Oren Etzioni. 2007. Open information extraction from the web. In IJ- CAI, volume 7, pages 2670-2676.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Latent dirichlet allocation",
                "authors": [
                    {
                        "first": "Andrew",
                        "middle": [
                            "Y"
                        ],
                        "last": "David M Blei",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [
                            "I"
                        ],
                        "last": "Ng",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Jordan",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Journal of Machine Learning Research",
                "volume": "3",
                "issue": "",
                "pages": "993--1022",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation. Journal of Ma- chine Learning Research, 3(Jan):993-1022.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Freebase: a collaboratively created graph database for structuring human knowledge",
                "authors": [
                    {
                        "first": "Kurt",
                        "middle": [],
                        "last": "Bollacker",
                        "suffix": ""
                    },
                    {
                        "first": "Colin",
                        "middle": [],
                        "last": "Evans",
                        "suffix": ""
                    },
                    {
                        "first": "Praveen",
                        "middle": [],
                        "last": "Paritosh",
                        "suffix": ""
                    },
                    {
                        "first": "Tim",
                        "middle": [],
                        "last": "Sturge",
                        "suffix": ""
                    },
                    {
                        "first": "Jamie",
                        "middle": [],
                        "last": "Taylor",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Proceedings of the 2008 ACM SIGMOD international conference on Management of data",
                "volume": "",
                "issue": "",
                "pages": "1247--1250",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collab- oratively created graph database for structuring hu- man knowledge. In Proceedings of the 2008 ACM SIGMOD international conference on Management of data, pages 1247-1250. AcM.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Knowledge extraction for information retrieval",
                "authors": [
                    {
                        "first": "Francesco",
                        "middle": [],
                        "last": "Corcoglioniti",
                        "suffix": ""
                    },
                    {
                        "first": "Mauro",
                        "middle": [],
                        "last": "Dragoni",
                        "suffix": ""
                    },
                    {
                        "first": "Marco",
                        "middle": [],
                        "last": "Rospocher",
                        "suffix": ""
                    },
                    {
                        "first": "Alessio",
                        "middle": [],
                        "last": "Palmero",
                        "suffix": ""
                    },
                    {
                        "first": "Aprosio",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "European Semantic Web Conference",
                "volume": "",
                "issue": "",
                "pages": "317--333",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Francesco Corcoglioniti, Mauro Dragoni, Marco Rospocher, and Alessio Palmero Aprosio. 2016. Knowledge extraction for information retrieval. In European Semantic Web Conference, pages 317- 333. Springer.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Discovering relations among named entities from large corpora",
                "authors": [
                    {
                        "first": "Takaaki",
                        "middle": [],
                        "last": "Hasegawa",
                        "suffix": ""
                    },
                    {
                        "first": "Satoshi",
                        "middle": [],
                        "last": "Sekine",
                        "suffix": ""
                    },
                    {
                        "first": "Ralph",
                        "middle": [],
                        "last": "Grishman",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04)",
                "volume": "",
                "issue": "",
                "pages": "415--422",
                "other_ids": {
                    "DOI": [
                        "10.3115/1218955.1219008"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Takaaki Hasegawa, Satoshi Sekine, and Ralph Grish- man. 2004. Discovering relations among named entities from large corpora. In Proceedings of the 42nd Annual Meeting of the Association for Com- putational Linguistics (ACL-04), pages 415-422, Barcelona, Spain.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Comparing partitions",
                "authors": [
                    {
                        "first": "Lawrence",
                        "middle": [],
                        "last": "Hubert",
                        "suffix": ""
                    },
                    {
                        "first": "Phipps",
                        "middle": [],
                        "last": "Arabie",
                        "suffix": ""
                    }
                ],
                "year": 1985,
                "venue": "Journal of classification",
                "volume": "2",
                "issue": "1",
                "pages": "193--218",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Lawrence Hubert and Phipps Arabie. 1985. Compar- ing partitions. Journal of classification, 2(1):193- 218.",
                "links": null
            }
        },
        "ref_entries": {
            "TABREF0": {
                "content": "<table><tr><td>Model</td><td/><td>B 3</td><td>V</td><td>ARI</td></tr><tr><td/><td>NYT-FB</td><td/><td/><td/></tr><tr><td>RelLDA</td><td/><td>29.1</td><td>30.0</td><td>13.3</td></tr><tr><td>RelLDA1</td><td/><td>36.9</td><td>34.7</td><td>24.2</td></tr><tr><td>March (Ls+L d )</td><td>c = 10</td><td>37.5</td><td>38.7</td><td>27.6</td></tr><tr><td>Simon</td><td/><td>39.4</td><td>38.3</td><td>33.8</td></tr><tr><td>EType+</td><td/><td>41.9</td><td>40.6</td><td>30.7</td></tr><tr><td>March (Ls+L d )</td><td/><td>36.9</td><td>37.4</td><td>28.1</td></tr><tr><td>EType EType+</td><td>c = 16</td><td>41.7 41.5</td><td>42.1 41.3</td><td>30.7 30.5</td></tr><tr><td>RelLDA1 March</td><td>c = 100</td><td>29.6 35.8</td><td>--</td><td>--</td></tr><tr><td/><td>TACRED</td><td/><td/><td/></tr><tr><td>March (Ls+L d )</td><td/><td>31.0</td><td>43.8</td><td>22.6</td></tr><tr><td>Simon</td><td>c = 10</td><td>15.7</td><td>17.1</td><td>6.1</td></tr><tr><td>EType+</td><td/><td>43.3</td><td>59.7</td><td>25.7</td></tr><tr><td>March (Ls+L d )</td><td/><td>34.6</td><td>47.6</td><td>23.2</td></tr><tr><td>EType</td><td>c = 16</td><td>48.3</td><td>64.4</td><td>29.1</td></tr><tr><td>EType+</td><td/><td>46.1</td><td>62.0</td><td>27.4</td></tr><tr><td>March</td><td colspan=\"4\">c = 100 33.13 43.63 20.21</td></tr></table>",
                "type_str": "table",
                "text": "Average results (%) across three runs of different models (except the EType) on NYT-FB and TACRED. c indicates the number of clusters in each method.",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table><tr><td>Among the topic</td></tr></table>",
                "type_str": "table",
                "text": "Figure 1 illustrates the average loss values of using these settings. If high quality relations were critical for training the link predictor, we would expect lower losses while using annotated relations. Study of EType+ in combination with different features. The results are average across three runs on the development set. Indeed, the loss curve of using 10 correct relation types is consistently below all the others. This implies that the link predictor is able to provide reasonable signals for training a relation classifier. So why are the Simon and March models outperformed by our models? As pointed out by Simon et al. (",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table><tr><td>pages 1003-1011, Suntec, Singapore. Association</td></tr><tr><td>for Computational Linguistics.</td></tr><tr><td>Makoto Miwa and Mohit Bansal. 2016. End-to-end re-</td></tr><tr><td>lation extraction using LSTMs on sequences and tree</td></tr><tr><td>structures. In Proceedings of the 54th Annual Meet-</td></tr><tr><td>ing of the Association for Computational Linguistics</td></tr><tr><td>(Volume 1: Long Papers), pages 1105-1116, Berlin,</td></tr><tr><td>Germany. Association for Computational Linguis-</td></tr><tr><td>tics.</td></tr><tr><td>Thien Huu Nguyen and Ralph Grishman. 2015. Rela-</td></tr><tr><td>tion extraction: Perspective from convolutional neu-</td></tr><tr><td>ral networks. In Proceedings of the 1st Workshop on</td></tr><tr><td>Vector Space Modeling for Natural Language Pro-</td></tr><tr><td>cessing, pages 39-48, Denver, Colorado. Associa-</td></tr><tr><td>tion for Computational Linguistics.</td></tr><tr><td>Xiang Ren,</td></tr><tr><td>Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan,</td></tr><tr><td>and Maosong Sun. 2016. Neural relation extraction</td></tr><tr><td>with selective attention over instances. In Proceed-</td></tr><tr><td>ings of the 54th Annual Meeting of the Association</td></tr><tr><td>for Computational Linguistics (Volume 1: Long Pa-</td></tr><tr><td>pers), pages 2124-2133, Berlin, Germany. Associa-</td></tr><tr><td>tion for Computational Linguistics.</td></tr><tr><td>Diego Marcheggiani and Ivan Titov. 2016. Discrete-</td></tr><tr><td>state variational autoencoders for joint discovery and</td></tr><tr><td>factorization of relations. Transactions of the Asso-</td></tr><tr><td>ciation for Computational Linguistics, 4:231-244.</td></tr><tr><td>Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-</td></tr><tr><td>rafsky. 2009. Distant supervision for relation ex-</td></tr><tr><td>traction without labeled data. In Proceedings of</td></tr><tr><td>the Joint Conference of the 47th Annual Meeting of</td></tr><tr><td>the ACL and the 4th International Joint Conference</td></tr><tr><td>on Natural Language Processing of the AFNLP,</td></tr></table>",
                "type_str": "table",
                "text": "Zeqiu Wu, Wenqi He, Meng Qu, Clare R Voss, Heng Ji, Tarek F Abdelzaher, and Jiawei Han. 2017. Cotype: Joint extraction of typed entities and relations with knowledge bases. In Proceedings of the 26th International Conference on World Wide Web, pages 1015-1024. Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling relations and their mentions without labeled text. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 148-163. Springer. Simone Romano, Nguyen Xuan Vinh, James Bailey, and Karin Verspoor. 2016. Adjusting for chance clustering comparison measures. Journal of Machine Learning Research, 17(1):4635-4666. Andrew Rosenberg and Julia Hirschberg. 2007. Vmeasure: A conditional entropy-based external cluster evaluation measure. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 410-420, Prague, Czech Republic. Association for Computational Linguistics. Evan Sandhaus. 2008. The new york times annotated corpus. Linguistic Data Consortium, Philadelphia, 6(12):e26752. \u00c9tienne Simon, Vincent Guigue, and Benjamin Piwowarski. 2019. Unsupervised information extraction: Regularizing discriminative approaches with relation distribution losses. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1378-1387, Florence, Italy. Association for Computational Linguistics. Kiri Wagstaff. 2000. Refining inductive bias in unsupervised learning via constraints. In AAAI/IAAI, page 1112. Limin Yao, Aria Haghighi, Sebastian Riedel, and Andrew McCallum. 2011. Structured relation discovery using generative models. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1456-1466, Edinburgh,",
                "html": null,
                "num": null
            }
        }
    }
}