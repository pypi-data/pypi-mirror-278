{
    "paper_id": "2020",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:00:07.431034Z"
    },
    "title": "Diverse, Controllable, and Keyphrase-Aware: A Corpus and Method for News Multi-Headline Generation",
    "authors": [
        {
            "first": "Dayiheng",
            "middle": [],
            "last": "Liu",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Sichuan University",
                "location": {}
            },
            "email": ""
        },
        {
            "first": "Yeyun",
            "middle": [],
            "last": "Gong",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Yu",
            "middle": [],
            "last": "Yan",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Jie",
            "middle": [],
            "last": "Fu",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Bo",
            "middle": [],
            "last": "Shao",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Daxin",
            "middle": [],
            "last": "Jiang",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Jiancheng",
            "middle": [],
            "last": "Lv",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Sichuan University",
                "location": {}
            },
            "email": ""
        },
        {
            "first": "Nan",
            "middle": [],
            "last": "Duan",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Microsoft",
            "middle": [],
            "last": "Research",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Asia",
            "middle": [
                "\u2666"
            ],
            "last": "Mila",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "\u2021",
            "middle": [],
            "last": "Microsoft",
            "suffix": "",
            "affiliation": {},
            "email": ""
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "News headline generation aims to produce a short sentence to attract readers to read the news. One news article often contains multiple keyphrases that are of interest to different users, which can naturally have multiple reasonable headlines. However, most existing methods focus on the single headline generation. In this paper, we propose generating multiple headlines with keyphrases of user interests, whose main idea is to generate multiple keyphrases of interest to users for the news first, and then generate multiple keyphrase-relevant headlines. We propose a multi-source Transformer decoder, which takes three sources as inputs: (a) keyphrase, (b) keyphrase-filtered article, and (c) original article to generate keyphrase-relevant, highquality, and diverse headlines. Furthermore, we propose a simple and effective method to mine the keyphrases of interest in the news article and build a first large-scale keyphraseaware news headline corpus, which contains over 180K aligned triples of news article, headline, keyphrase . Extensive experimental comparisons on the real-world dataset show that the proposed method achieves state-of-theart results in terms of quality and diversity 1 .",
    "pdf_parse": {
        "paper_id": "2020",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "News headline generation aims to produce a short sentence to attract readers to read the news. One news article often contains multiple keyphrases that are of interest to different users, which can naturally have multiple reasonable headlines. However, most existing methods focus on the single headline generation. In this paper, we propose generating multiple headlines with keyphrases of user interests, whose main idea is to generate multiple keyphrases of interest to users for the news first, and then generate multiple keyphrase-relevant headlines. We propose a multi-source Transformer decoder, which takes three sources as inputs: (a) keyphrase, (b) keyphrase-filtered article, and (c) original article to generate keyphrase-relevant, highquality, and diverse headlines. Furthermore, we propose a simple and effective method to mine the keyphrases of interest in the news article and build a first large-scale keyphraseaware news headline corpus, which contains over 180K aligned triples of news article, headline, keyphrase . Extensive experimental comparisons on the real-world dataset show that the proposed method achieves state-of-theart results in terms of quality and diversity 1 .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "News Headline Generation is an under-explored subtask of text summarization (See et al., 2017; Gehrmann et al., 2018; Zhong et al., 2019) . Unlike text summaries that contain multiple contextrelated sentences to cover the main ideas of a document, news headlines often contain a single short sentence to encourage users to read the news. Since one news article typically contains multiple keyphrases or topics of interest to different users, it is useful to generate multiple headlines covering different keyphrases for the news article. Multiheadline generation aims to generate multiple independent headlines, which allows us to recommend news with different news headlines based on the interests of users. Besides, multi-headline generation can provide multiple hints for human news editors to assist them in writing news headlines.",
                "cite_spans": [
                    {
                        "start": 76,
                        "end": 94,
                        "text": "(See et al., 2017;",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 95,
                        "end": 117,
                        "text": "Gehrmann et al., 2018;",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 118,
                        "end": 137,
                        "text": "Zhong et al., 2019)",
                        "ref_id": "BIBREF25"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "However, most existing methods (Takase et al., 2016; Ayana et al., 2016; Murao et al., 2019; Colmenares et al., 2019; Zhang et al., 2018) focus on single-headline generation. The headline generation process is treated as an one-to-one mapping (the input is an article and the output is a headline), which trains and tests the models without any additional guiding information or constraints. We argue that this may lead to two problems. Firstly, since it is reasonable to generate multiple headlines for the news, training to generate the single ground-truth might result in a lack of more detailed guidance. Even worse, a single ground-truth without any constraint or guidance is often not enough to measure the quality of the generated headline for model testing. For example, even if a generated headline is considered reasonable by humans, it can get a low score in ROUGE (Lin, 2004) , because it might focus on the keyphrases or aspects that are not consistent with the ground-truth.",
                "cite_spans": [
                    {
                        "start": 31,
                        "end": 52,
                        "text": "(Takase et al., 2016;",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 53,
                        "end": 72,
                        "text": "Ayana et al., 2016;",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 73,
                        "end": 92,
                        "text": "Murao et al., 2019;",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 93,
                        "end": 117,
                        "text": "Colmenares et al., 2019;",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 118,
                        "end": 137,
                        "text": "Zhang et al., 2018)",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 876,
                        "end": 887,
                        "text": "(Lin, 2004)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this paper, we incorporate the keyphrase information into the headline generation as additional guidance. Unlike one-to-one mapping employed in previous works, we treat the headline generation process as a two-to-one mapping, where the inputs are news articles and keyphrases, and the output is a headline. We propose a keyphrase-aware news multi-headline generation method, which contains two modules: (a) Keyphrase Generation Model, which aims to generate multiple keyphrases of interest to users for the news article. (b) Keyphrase-Aware Multi-Headline Generation Model, which takes the news article and a keyphrase as input and generates a keyphrase-relevant news headline.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "For training models, we build a first large-scale news keyphrase-aware headline corpus that contains 180K aligned triples of news article, headline, keyphrase .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "As in years past, a lot of the food trends of the year were based on creating perfectly photogenic dishes. An aesthetically pleasing dish, however, doesn't mean it will stand the test of time. In fact, it's not uncommon for food trends to be all the hype one year and die out the next. From broccoli coffee to \"bowl food,\" here are 10 food trends that you likely won't see in 2019.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "...[15 sentences with 307 words are abbreviated from here.]",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In 2018, restaurants all over the US decided it was a good idea to place gold foil on everything from ice cream to chicken wings to pizza resulting in an expensive food trend.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "For example, the Ainsworth in New York City sells $1,000 worth of gold covered chicken wings. It seems everyone can agree that this is a food trend that might soon disappear.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "place gold foil on foods resulting in an expensive food trend \"bowl food\" became a trend this year bowl food Is the glitter food the next trend? glitter food The proposed approach faces two major challenges. The first one is how to build the keyphraseaware news headline corpus. To our best knowledge, no corpus contains the news article and headline pairs, which are aligned with a keyphrase of interest to users. The second is how to design the keyphrase-aware news headline generation model to ensure that the generated headlines are keyphrase-relevant, high-quality, and diverse. For the first challenge, we propose a simple but efficient method to mine the keyphrases of interest to users in news articles based on the user search queries and news click information that are collected from a real-world search engine. With this method, we build the keyphrase-aware news headline corpus.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "expensive food trend",
                "sec_num": null
            },
            {
                "text": "For the second challenge, we design a multisource Transformer (Vaswani et al., 2017) decoder to improve the generation quality and the keyphrase sensitivity of the model, which takes three source information as inputs: (a) keyphrase, (b) keyphrasefiltered article, and (c) original article. For the proposed multi-source Transformer decoder, we further design and compare several variants of attention-based fusing mechanism. Extensive experiments on real-world dataset have shown that the proposed method can generate high-quality, keyphrase-relevant, and diverse news headlines.",
                "cite_spans": [
                    {
                        "start": 62,
                        "end": 84,
                        "text": "(Vaswani et al., 2017)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "expensive food trend",
                "sec_num": null
            },
            {
                "text": "Our keyphrase-aware news headline corpus called KeyAware News is built by the following steps:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Keyphrase-Aware Headline Corpus",
                "sec_num": "2"
            },
            {
                "text": "(1) Data Collection. We collect 16,000,000 raw samples which contain news articles with user search query information from Microsoft Bing News search engine2 . Each sample can be presented as a tuple Q, X, Y, C where Q is a user search query, X is a news article that the search engine returns to the user based on the search query Q, Y is a human-written headline for X, and C represents the number of times the user clicks on the news under the search query Q. Each news article X has 10 different queries Q on average.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Keyphrase-Aware Headline Corpus",
                "sec_num": "2"
            },
            {
                "text": "(2) Keyphrase Mining. We mine the keyphrase of interest to users with user search queries. We assume that if many users find and click on one news article through different queries containing the same phrase, such a phrase is the keyphrase for the article. For each article, we collect its corresponding user search queries and remove the stop words and special symbols from the queries. Then we find the common phrases (4-gram, 3-gram, or 2-gram) in these queries. These common phrases are scored based on how many times they appear in these queries and normalized by length. The score is also weighted by the user click number C, which means the phrases that appear in the queries have more users click on the article are more important. Finally, we use the n-gram with the highest score as the keyphrase Z of the article X.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Keyphrase-Aware Headline Corpus",
                "sec_num": "2"
            },
            {
                "text": "(3) Article-Headline-Keyphrase Alignment. In order to obtain the aligned article-headlinekeyphrase tuple X, Y, Z . We filter out the sample whose article or headline does not contain the Z. Moreover, we remove such pairs whose article length are greater than 600 or less than 100 tokens, or whose headline length are greater than 20 or less than 3 tokens. After the alignment and data cleaning, we obtain the KeyAware News which contains about 180K aligned article-headline-keyphrase triples. We split it into Train, Test, and Dev sets, each containing 165,913, 10,000, and 5,000 samples. 3 Methodology",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Keyphrase-Aware Headline Corpus",
                "sec_num": "2"
            },
            {
                "text": "The overall keyphrase-aware multi-headline generation procedure is shown in Figure 1 , which involves two modules: (a) keyphrase generation model generates multiple keyphrases of interest to users for the news article. (b) keyphrase-aware headline generation model takes the news article and each generated keyphrase as input, and generates multiple keyphrase-relevant news headlines.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 83,
                        "end": 84,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Overview",
                "sec_num": "3.1"
            },
            {
                "text": "The headline generation can be formalized as a sequence-to-sequence learning (Sutskever et al., 2014) task. Given an input news article X and a specific keyphrase Z, we aim to produce a keyphrase-relevant headline Y .",
                "cite_spans": [
                    {
                        "start": 77,
                        "end": 101,
                        "text": "(Sutskever et al., 2014)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Headline Generation",
                "sec_num": "3.2"
            },
            {
                "text": "We first introduce the basic version of our headline generation model (we call BASE), which is keyphrase-agnostic. BASE is built upon the Transformer Seq2Seq model (Vaswani et al., 2017) , which has made remarkable progress in sequenceto-sequence learning. Transformer contains a multihead self-attention encoder and a multi-head self-attention decoder. As discussed in Vaswani et al. (2017) , an attention function maps a query and a set of key-value pairs to an output as:",
                "cite_spans": [
                    {
                        "start": 164,
                        "end": 186,
                        "text": "(Vaswani et al., 2017)",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 370,
                        "end": 391,
                        "text": "Vaswani et al. (2017)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Headline Generation BASE Model",
                "sec_num": "3.2.1"
            },
            {
                "text": "Attention( Q, K, V ) = Softmax( Q KT \u221a d k ) V ,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Headline Generation BASE Model",
                "sec_num": "3.2.1"
            },
            {
                "text": "where the queries Q, keys K, and values V are all vectors, and d k is the dimension of the key vector. Multi-head attention mechanism projects queries, keys, and values to h different subspaces and calculates corresponding attention as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Headline Generation BASE Model",
                "sec_num": "3.2.1"
            },
            {
                "text": "MultiHead( Q, K, V ) = Concat(h 1 , ..., h h )W O , where h i = Attention( QW Q i , KW K i , VW V i ).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Headline Generation BASE Model",
                "sec_num": "3.2.1"
            },
            {
                "text": "The encoder is composed of a stack of N identical blocks. Each block has two sub-layers: multi-head self-attention mechanism and a position-wise fully connected feed-forward network. All sub-layers are interconnected with residual connections (He et al., 2016) and layer normalization (Ba et al., 2016) .",
                "cite_spans": [
                    {
                        "start": 243,
                        "end": 260,
                        "text": "(He et al., 2016)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 285,
                        "end": 302,
                        "text": "(Ba et al., 2016)",
                        "ref_id": "BIBREF1"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Headline Generation BASE Model",
                "sec_num": "3.2.1"
            },
            {
                "text": "Similarly, the decoder is also composed of a stack of N identical block. In addition to the two sub-layers in each encoder block, the decoder contains a third sub-layer which performs multi-head attention over the output of the encoder. Figure 2 (a) shows the architecture of the block in the decoder.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 244,
                        "end": 245,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Headline Generation BASE Model",
                "sec_num": "3.2.1"
            },
            {
                "text": "BASE uses the pre-trained BERT-base model (Devlin et al., 2018) to initialize the parameters of the encoder. Also, it uses the transformer decoder with a copy mechanism (Gu et al., 2016) , whose hidden size, the number of multi-head h, and the number of blocks N are the same as its encoder.",
                "cite_spans": [
                    {
                        "start": 42,
                        "end": 63,
                        "text": "(Devlin et al., 2018)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 169,
                        "end": 186,
                        "text": "(Gu et al., 2016)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Headline Generation BASE Model",
                "sec_num": "3.2.1"
            },
            {
                "text": "In order to explore more effective ways of incorporating keyphrase information into BASE, we design 5 variants of multi-source Transformer decoders.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Keyphrase-Aware Headline Generation Model",
                "sec_num": "3.2.2"
            },
            {
                "text": "Article + Keyphrase. The basic idea is to add the keyphrase into the decoder directly. The keyphrase X key is represented as a sequence of word embeddings. As shown in Figure 2 (b), we add an extra sub-layer that performs multi-head attention over the X key in each block of the decoder.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 175,
                        "end": 176,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Keyphrase-Aware Headline Generation Model",
                "sec_num": "3.2.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "X (n+1) dec = MultiHead(X (n) dec , X key , X key ),",
                        "eq_num": "(1)"
                    }
                ],
                "section": "Keyphrase-Aware Headline Generation Model",
                "sec_num": "3.2.2"
            },
            {
                "text": "where",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Keyphrase-Aware Headline Generation Model",
                "sec_num": "3.2.2"
            },
            {
                "text": "X (n)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Keyphrase-Aware Headline Generation Model",
                "sec_num": "3.2.2"
            },
            {
                "text": "dec is the output of the n-th block in the decoder. Since the original article has contained sufficient information for the model to learn to generate the headline, the model may tend to mainly use the article information and ignore the keyphrase and become less sensitive to keyphrases. As a byproduct, the generated headlines may lack diversity and keyphrase relevance. Keyphrase-Filtered Article. Intuitively, when people read news articles, they tend to focus on the parts of the article that are matched to the keyphrases of their interests. Inspired by this, before inputting the original article representation into the decoder, we use the attention mechanism to filter the article with the keyphrase (see Figure 2  (c) ).",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 720,
                        "end": 726,
                        "text": "2  (c)",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Keyphrase-Aware Headline Generation Model",
                "sec_num": "3.2.2"
            },
            {
                "text": "Xenc = MultiHead(X key , X enc , X enc ), (2)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Keyphrase-Aware Headline Generation Model",
                "sec_num": "3.2.2"
            },
            {
                "text": "where X enc is the output of the last block in the encoder. The resulting representation Xenc can be seen as the keyphrase-filtered article, which mainly keeps the article information that is related to the keyphrase. Since the decoder cannot directly access the representation of the original article, the model is forced to utilize the information of the keyphrase. Therefore, the sensitivity of the model to keyphrase is improved.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Keyphrase-Aware Headline Generation Model",
                "sec_num": "3.2.2"
            },
            {
                "text": "Fusing Keyphrase-Filtered Article and Original Article. Although feeding the keyphrasefiltered article representation Xenc instead of the original article representation X enc to the decoder can improve the sensitivity of the model to keyphrase, some useful and global information in the original article may also be filtered out. It might reduce the quality of the generated headlines. To further balance the keyphrase sensitivity and headline quality of the model, we use X enc and Xenc as two input sources for the decoder and fuse them. As shown in Figure 2 (d)-(f), we design three decoder variants based on different fusing mechanism to fuse the X enc and the Xenc . (I) Addition-Fusing Mechanism. We directly perform a point-wise addition between the X enc and the Xenc . Then we feed it into the decoder. (II) Stack-Fusing Mechanism. We perform a multi-head attention on Xenc and X enc one by one in each block of the decoder. All of the sub-layers are interconnected with residual connections.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 560,
                        "end": 561,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Keyphrase-Aware Headline Generation Model",
                "sec_num": "3.2.2"
            },
            {
                "text": "X(n) dec = MultiHead(X (n) dec , Xenc , Xenc ) (3) X (n+1) dec = MultiHead( X(n) dec , X enc , X enc ) (4) (III) Parallel-Fusing Mechanism.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Keyphrase-Aware Headline Generation Model",
                "sec_num": "3.2.2"
            },
            {
                "text": "For each block of the decoder, we perform a multi-head attention in parallel on Xenc and X enc . Then, we perform a point-wise addition between them. Similarly, all of the sub-layers are interconnected with residual connections.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Keyphrase-Aware Headline Generation Model",
                "sec_num": "3.2.2"
            },
            {
                "text": "X(n) dec = MultiHead(X (n) dec , Xenc , Xenc ) (5) X(n) dec = MultiHead(X (n) dec , X enc , X enc ) (6) X (n+1) dec = X(n) dec + X(n) dec (7)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Keyphrase-Aware Headline Generation Model",
                "sec_num": "3.2.2"
            },
            {
                "text": "In this subsection, we show how to generate the keyphrases for a given news article X. Here we briefly describe three methods for keyphrase generation. It should be noted that in this paper, we mainly focus on news headline generation rather than keyphrase generation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Keyphrase Generation",
                "sec_num": "3.3"
            },
            {
                "text": "(1) TF-IDF Ranking. We use Term Frequency Inverse Document Frequency (TF-IDF) (Zhang et al., 2007) to weight all n-grams (n = 2, 3, and 4) in the news article X. Then we filter out n-grams with TF-IDF below the threshold or containing any punctuation or special character. For different n of the n-gram, we set different thresholds for filtering.",
                "cite_spans": [
                    {
                        "start": 78,
                        "end": 98,
                        "text": "(Zhang et al., 2007)",
                        "ref_id": "BIBREF24"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Keyphrase Generation",
                "sec_num": "3.3"
            },
            {
                "text": "We take this unsupervised method as a baseline.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Keyphrase Generation",
                "sec_num": "3.3"
            },
            {
                "text": "Method EM@1 EM@3 EM@5 R@1 R@3 R@5 (2) Seq2Seq. Since our KeyAware News corpus contains the article-keyphrase pairs, we treat the keyphrase generation as a sequence-to-sequence learning task. We train the model BASE with article-keyphrase pairs. During inference, we use beam search with length penalty to generate ngrams (n = 2, 3, and 4) as the keyphrases.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Keyphrase Generation",
                "sec_num": "3.3"
            },
            {
                "text": "(3) Slot Tagging. Because the keyphrases also appear in the news articles, we can formulate the keyphrase generation task as a slot tagging task (Zhang et al., 2016; Williams, 2019) . We finetune the BERT-base model to achieve that. Concretely, we use the output sequence of the model to predict the beginning and end position of the keyphrase in the article. During inference, we follow the answer span prediction method used in Seo et al. (2017) to predict n-grams (n = 2, 3, and 4) with the highest probabilities as the keyphrases.",
                "cite_spans": [
                    {
                        "start": 145,
                        "end": 165,
                        "text": "(Zhang et al., 2016;",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 166,
                        "end": 181,
                        "text": "Williams, 2019)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 430,
                        "end": 447,
                        "text": "Seo et al. (2017)",
                        "ref_id": "BIBREF15"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Keyphrase Generation",
                "sec_num": "3.3"
            },
            {
                "text": "In the first experiment, we evaluate the performance of three keyphrase generation methods: Metrics. For evaluation, each method generates top-K keyphrases for every news article in the test set. We use a top-K exact-match rate (EM@K) as an evaluation metric, which tests whether one of the K generated keyphrases matches the golden keyphrase exactly. Some of the generated key phrases may not exactly match the golden keyphrase but have overlapping tokens with it (it may be a sub-sequence of the golden keyphrase or vice versa). We thus report the Recall@K (R@K), 3 https://github.com/google-research/bert.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Keyphrase Generation",
                "sec_num": "4.1"
            },
            {
                "text": "which tests the percentage of the tokens in golden keyphrase covered by the K generated keyphrases.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Keyphrase Generation",
                "sec_num": "4.1"
            },
            {
                "text": "Results. The results are shown in Table 1 . We can see that the EM@1 of TF-IDF is only 18.63%, but SLOT achieves 60.75%. Both of SEQ2SEQ and SLOT significantly outperform the TF-IDF in all metrics. SEQ2SEQ achieves comparable performances in EM@K, but performs worse than SLOT in R@K. SLOT achieves 83.18% EM@5 and 89.08% R@5. In the following experiments, we use SLOT to generate keyphrases for our keyphrase-aware news headline generation models.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 40,
                        "end": 41,
                        "text": "1",
                        "ref_id": "TABREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Keyphrase Generation",
                "sec_num": "4.1"
            },
            {
                "text": "Baselines. In the following experiments, we compare various variants of the proposed keyphraseaware models we introduced in Section 3.2.1 as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "News Headline Generation",
                "sec_num": "4.2"
            },
            {
                "text": "(1) BASE, as shown in Figure 2 ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 29,
                        "end": 30,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "News Headline Generation",
                "sec_num": "4.2"
            },
            {
                "text": "In this experiment, we only give models the news articles without the golden keyphrases. We use SLOT to generate top-K keyphrases for each article. Then each keyphrase-aware generation model using them to generates K different keyphraserelevant headlines. For keyphrase-agnostic baselines, we apply the beam search to generate top k headlines for each article. We also apply the diverse decoding to BASE as a strong baseline (BASE + Diverse) for further comparison. The diversity penalty is set to be 1.0. It should be noted that we can also apply the diverse decoding to our keyphrase-aware models to further improve diversity.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Multi-Headline Generation",
                "sec_num": "4.2.1"
            },
            {
                "text": "Metrics. Following Li et al. (2016a) , we use Distinct-1 and Distinct-2 (the higher the better) to evaluate diversity, which report the degree of the diversity by calculating the number of distinct unigrams and bigrams in generated headlines for each article. Since randomly generated headlines are also highly diverse, we measure the quality as well. As we discussed in Section 1, one news article can have multiple reasonable headlines. However, each article in our test set has only one human written headline, which may only focus on one keyphrase of the news article. We should emphasize that there may be only one generated headline that focuses on the same keyphrase of the human-written headline, while others focus on distinct keyphrases. It is thus not reasonable if we use the same humanwritten headline as the ground-truth to evaluate all generated headlines. We assume that if the headlines generated by the model are high-quality and diverse, there would be a higher probability that one of the headlines is closer to the single groundtruth. Therefore, we report the highest ROUGE score among the multiple generated headlines for each article. This criterion is similar to top-K errors (He et al., 2016) in image classification tasks. We report the results with K=1, 3, 5.",
                "cite_spans": [
                    {
                        "start": 19,
                        "end": 36,
                        "text": "Li et al. (2016a)",
                        "ref_id": null
                    },
                    {
                        "start": 1200,
                        "end": 1217,
                        "text": "(He et al., 2016)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Multi-Headline Generation",
                "sec_num": "4.2.1"
            },
            {
                "text": "Results. Table 2 presents the results. For diversity, we can see that all of our keyphrase-aware generation models performs significantly better than other keyphrase-agnostic baselines in both Distinct-1 and Distinct-2 metrics for all K. After using the diverse decoding, BASE + Diverse achieves higher diversity. Nevertheless, the diversity is still lower than most keyphrase-aware generation models. As expected, BASE + Filter achieves the highest diversity, and BASE + KEY achieves the lowest diversity among the variants of our keyphrase-aware generation models. For quality, except BASE, there is still a big gap between other keyphrase-agnostic baselines and our keyphrase-aware generation models. Except for BASE + Filter, all of our keyphraseaware generation models achieve higher ROUGE scores than BASE and BASE + Diverse (see the last 6 lines in Table 2 ). These results show that our keyphrase-aware generation models can effectively generate high-quality and diverse headlines. ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 15,
                        "end": 16,
                        "text": "2",
                        "ref_id": null
                    },
                    {
                        "start": 862,
                        "end": 863,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Multi-Headline Generation",
                "sec_num": "4.2.1"
            },
            {
                "text": "To further evaluate the quality and diversity of the generation, we design an experiment that uses a search engine to help us verify the diversity and quality of the generated headlines. It should be noted that the main purpose of this experiment is not to improve the performance of the search engine, but to measure the quality and diversity of the generated multiple headlines through a real-world search engine. We first collect the data pairs of the news article and its related user search query X, Q in the following way. If the article X is returned by the search engine based on a user query Q and the user clicks on the article X, then we take the query and the article as a data pair X, Q . After collection, each article X in the test set has 10 different related user queries on average. The article X is used as the ground-truth for Q in the following evaluation. We replace the search key in the original search engine for each article with the K generated multi-headlines. Also, we re-build the indexes of the search engine that contains 10,000 news articles in the test set. Then we re-use the user search queries to retrieve the article. We believe that if the generated multi-headlines have high diversity and quality, then given different user queries, there should be a high probability that the golden article can be retrieved.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "News Article Retrieval",
                "sec_num": "4.2.2"
            },
            {
                "text": "Metrics. We use the mean average precision (mAP), which is widely used for information retrieval as a metric. We report the results of mAP@N (N =1, 3, 5, and 10) which test the average probability that the golden article is ranked by the search engine to the top N . Using the humanwritten headline (HUMAN) as the search key is evaluated as a strong baseline. We also compare the performance of using a different number of headlines (K=1, 3, and 5). It should be noted that increasing the number of headlines as the search key does not ensure the improvement of the mAP, because the number of search keys of all other articles will also be increased. If the generated multiheadlines are not good enough, it will introduce noise and even cause mAP decreasing.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "News Article Retrieval",
                "sec_num": "4.2.2"
            },
            {
                "text": "Results. Table 3 presents the results. Similarly, our models perform much better than other keyphrase-agnostic baselines. In most cases, BASE + Diverse outperforms BASE, but still performs worse than 7 keyphrase-aware generation models (see the last 7 lines in Table 3 ). Generally, with the number of headline K increases, we can see that the performance of our keyphrase-aware generation models improves much higher than other baselines. We find that the mAP@10 of BASE + ParallelFuse (K=5) achieves 76.08, which is even better than HUMAN. These results demonstrate that our keyphrase-aware generation models can generate high-quality and diversity headlines.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 15,
                        "end": 16,
                        "text": "3",
                        "ref_id": "TABREF2"
                    },
                    {
                        "start": 267,
                        "end": 268,
                        "text": "3",
                        "ref_id": "TABREF2"
                    }
                ],
                "eq_spans": [],
                "section": "News Article Retrieval",
                "sec_num": "4.2.2"
            },
            {
                "text": "At a similar level of diversity, we want to investigate the quality of the headlines generated by our keyphrase-aware headline generation model compared to BASE + Diverse. Since the Distinct-1 and Distinct-2 of BASE + Diverse and BASE + Stack-Fuse are close, we compare the quality of them through human evaluation. We randomly sample 100 articles from the test set, let each model generate 3 different headlines. We also mix a random headline and the golden headline for each article, and thus each article has 8 headlines. Three experts are asked to judge whether each headline could be used as the headline of the news. If more than two experts believe that it can be used as the headline of the news, then this headline is considered qualified. 62.6%, 36.2%, and 0.0%, respectively. These results show that the quality of BASE + StackFuse is also higher than BASE + Diverse. We present some examples for comparison as shown in Figure 3 .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 938,
                        "end": 939,
                        "text": "3",
                        "ref_id": "FIGREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Human Evaluation",
                "sec_num": "4.2.3"
            },
            {
                "text": "News headline generation is a subtask of summarization which has been extensively studied recently (Rush et al., 2015; Takase et al., 2016; Ayana et al., 2016; Tan et al., 2017; Zhou et al., 2017b; Higurashi et al., 2018; Zhang et al., 2018; Murao et al., 2019) (2017) propose a coarse-to-fine method, which first extracts the salient sentences and then generates the headline based on these sentences. Zhou et al. (2017b) propose a method which divides the process of headline generation into three phases: a sentence encoder, a selective gate network for sen-tence selection, and a headline decoder. Higurashi et al. (2018) propose an extractive headline generation method, different from previous works that target the headline generation for the articles, this work focus on the task of headline generation for the community question answering forums. Due to lack of supervised training data, they propose a learning-to-rank based method to extract the essential substring from a question and use this substring as the headline of the forums. Zhang et al. (2018) propose a method for question headline generation, which designs a dual-attention seq2seq model.",
                "cite_spans": [
                    {
                        "start": 99,
                        "end": 118,
                        "text": "(Rush et al., 2015;",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 119,
                        "end": 139,
                        "text": "Takase et al., 2016;",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 140,
                        "end": 159,
                        "text": "Ayana et al., 2016;",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 160,
                        "end": 177,
                        "text": "Tan et al., 2017;",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 178,
                        "end": 197,
                        "text": "Zhou et al., 2017b;",
                        "ref_id": null
                    },
                    {
                        "start": 198,
                        "end": 221,
                        "text": "Higurashi et al., 2018;",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 222,
                        "end": 241,
                        "text": "Zhang et al., 2018;",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 242,
                        "end": 261,
                        "text": "Murao et al., 2019)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 262,
                        "end": 268,
                        "text": "(2017)",
                        "ref_id": null
                    },
                    {
                        "start": 403,
                        "end": 422,
                        "text": "Zhou et al. (2017b)",
                        "ref_id": null
                    },
                    {
                        "start": 602,
                        "end": 625,
                        "text": "Higurashi et al. (2018)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 1047,
                        "end": 1066,
                        "text": "Zhang et al. (2018)",
                        "ref_id": "BIBREF23"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Works",
                "sec_num": "5"
            },
            {
                "text": "However, most previous headline generation methods focus on one-to-one mapping, and the headline generation process is not controllable. In this work, we focus on the news multi-headline generation problem and design a keyphrase-aware headline generation method. Different information aware methods have been successfully used in natural language generation tasks (Zhou et al., 2017a (Zhou et al., , 2018;; Wang et al., 2017) , such as responses generation in the dialogue system. Similar to our task, responses generation in a dialogue system is also a one-to-many problem, Zhou et al. (2017a) propose a mechanism-aware seq2seq model for controllable response generation. They model different mechanisms as latent embeddings and learn the latent embeddings in their seq2seq model. Incorporating these mechanisms, their model can generate controllable responses. Zhou et al. (2018) propose a commonsense knowledge aware conversation generation method. More concretely, in the first stage, their model retrieves subgraphs from a knowledge base, and the model encodes the subgraphs using a dynamic graph neural network to facilitate better conversation generation in the second stage. Wang et al. (2017) propose an encoder-decoder based neural network for response generation. To our best knowledge, we are the first to consider keyphraseaware mechanism on news headline generation and build the first keyphrase-aware news headline corpus.",
                "cite_spans": [
                    {
                        "start": 364,
                        "end": 383,
                        "text": "(Zhou et al., 2017a",
                        "ref_id": null
                    },
                    {
                        "start": 384,
                        "end": 406,
                        "text": "(Zhou et al., , 2018;;",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 407,
                        "end": 425,
                        "text": "Wang et al., 2017)",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 575,
                        "end": 594,
                        "text": "Zhou et al. (2017a)",
                        "ref_id": null
                    },
                    {
                        "start": 863,
                        "end": 881,
                        "text": "Zhou et al. (2018)",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 1183,
                        "end": 1201,
                        "text": "Wang et al. (2017)",
                        "ref_id": "BIBREF20"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Works",
                "sec_num": "5"
            },
            {
                "text": "In this paper, we demonstrate how to enable news headline generation systems to be aware of keyphrases such that the model can generate diverse news headlines in a controlled manner. We also build a first large-scale keyphrase-aware news headline corpus, which is based on mining the keyphrases of users' interests in news articles with user queries. Moreover, we propose a keyphraseaware news multi-headline generation model that contains a multi-source Transformer decoder with three variants of attention-based fusing mechanisms. Extensive experiments on the real-world dataset show that our approach can generate highquality, keyphrase-relevant, and diverse news headlines, which outperforms many strong baselines.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "6"
            },
            {
                "text": "All news articles are high-quality, real-world news and all the news headlines are written by humans.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://github.com/tensorflow/tensor2tensor.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://github.com/abisee/pointer-generator.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://github.com/magic282/SEASS.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "This work is supported in part by the National Natural Science Fund for Distinguished Young Scholar under Grant 61625204, and in part by the State Key Program of the National Science Foundation of China under Grant 61836006.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgment",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Neural headline generation with minimum risk training",
                "authors": [
                    {
                        "first": "Shiqi",
                        "middle": [],
                        "last": "Shen",
                        "suffix": ""
                    },
                    {
                        "first": "Ayana",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Zhiyuan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Maosong",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1604.01904"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Shiqi Shen Ayana, Zhiyuan Liu, and Maosong Sun. 2016. Neural headline generation with minimum risk training. arXiv preprint arXiv:1604.01904.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Layer normalization",
                "authors": [
                    {
                        "first": "Jimmy",
                        "middle": [],
                        "last": "Lei Ba",
                        "suffix": ""
                    },
                    {
                        "first": "Jamie",
                        "middle": [],
                        "last": "Ryan Kiros",
                        "suffix": ""
                    },
                    {
                        "first": "Geoffrey",
                        "middle": [
                            "E"
                        ],
                        "last": "Hinton",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1607.06450"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin- ton. 2016. Layer normalization. arXiv preprint arXiv:1607.06450.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
                "authors": [
                    {
                        "first": "Kyunghyun",
                        "middle": [],
                        "last": "Cho",
                        "suffix": ""
                    },
                    {
                        "first": "Bart",
                        "middle": [],
                        "last": "Van Merri\u00ebnboer",
                        "suffix": ""
                    },
                    {
                        "first": "Caglar",
                        "middle": [],
                        "last": "Gulcehre",
                        "suffix": ""
                    },
                    {
                        "first": "Dzmitry",
                        "middle": [],
                        "last": "Bahdanau",
                        "suffix": ""
                    },
                    {
                        "first": "Fethi",
                        "middle": [],
                        "last": "Bougares",
                        "suffix": ""
                    },
                    {
                        "first": "Holger",
                        "middle": [],
                        "last": "Schwenk",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "EMNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kyunghyun Cho, Bart Van Merri\u00ebnboer, Caglar Gul- cehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In EMNLP.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Headline generation as a sequence prediction with conditional random fields",
                "authors": [
                    {
                        "first": "Carlos",
                        "middle": [
                            "A"
                        ],
                        "last": "Colmenares",
                        "suffix": ""
                    },
                    {
                        "first": "Marina",
                        "middle": [],
                        "last": "Litvak",
                        "suffix": ""
                    },
                    {
                        "first": "Amin",
                        "middle": [],
                        "last": "Mantrach",
                        "suffix": ""
                    },
                    {
                        "first": "Fabrizio",
                        "middle": [],
                        "last": "Silvestri",
                        "suffix": ""
                    },
                    {
                        "first": "Horacio",
                        "middle": [],
                        "last": "Rodr\u0131guez",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Multilingual Text Analysis: Challenges, Models, And Approaches",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Carlos A Colmenares, Marina Litvak, Amin Mantrach, Fabrizio Silvestri, and Horacio Rodr\u0131guez. 2019. Headline generation as a sequence prediction with conditional random fields. Multilingual Text Analy- sis: Challenges, Models, And Approaches.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
                "authors": [
                    {
                        "first": "Jacob",
                        "middle": [],
                        "last": "Devlin",
                        "suffix": ""
                    },
                    {
                        "first": "Ming-Wei",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "Kenton",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Kristina",
                        "middle": [],
                        "last": "Toutanova",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "NAACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understand- ing. In NAACL.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Bottom-up abstractive summarization",
                "authors": [
                    {
                        "first": "Sebastian",
                        "middle": [],
                        "last": "Gehrmann",
                        "suffix": ""
                    },
                    {
                        "first": "Yuntian",
                        "middle": [],
                        "last": "Deng",
                        "suffix": ""
                    },
                    {
                        "first": "Alexander",
                        "middle": [
                            "M"
                        ],
                        "last": "Rush",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "EMNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sebastian Gehrmann, Yuntian Deng, and Alexander M Rush. 2018. Bottom-up abstractive summarization. In EMNLP.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Incorporating copying mechanism in sequence-to-sequence learning",
                "authors": [
                    {
                        "first": "Jiatao",
                        "middle": [],
                        "last": "Gu",
                        "suffix": ""
                    },
                    {
                        "first": "Zhengdong",
                        "middle": [],
                        "last": "Lu",
                        "suffix": ""
                    },
                    {
                        "first": "Hang",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "O",
                        "middle": [
                            "K"
                        ],
                        "last": "Victor",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "ACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jiatao Gu, Zhengdong Lu, Hang Li, and Victor OK Li. 2016. Incorporating copying mechanism in sequence-to-sequence learning. In ACL.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Deep residual learning for image recognition",
                "authors": [
                    {
                        "first": "Kaiming",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Xiangyu",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Shaoqing",
                        "middle": [],
                        "last": "Ren",
                        "suffix": ""
                    },
                    {
                        "first": "Jian",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "CVPR",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recog- nition. In CVPR.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Extractive headline generation based on learning to rank for community question answering",
                "authors": [
                    {
                        "first": "Tatsuru",
                        "middle": [],
                        "last": "Higurashi",
                        "suffix": ""
                    },
                    {
                        "first": "Hayato",
                        "middle": [],
                        "last": "Kobayashi",
                        "suffix": ""
                    },
                    {
                        "first": "Takeshi",
                        "middle": [],
                        "last": "Masuyama",
                        "suffix": ""
                    },
                    {
                        "first": "Kazuma",
                        "middle": [],
                        "last": "Murao",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "COLING",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tatsuru Higurashi, Hayato Kobayashi, Takeshi Ma- suyama, and Kazuma Murao. 2018. Extractive head- line generation based on learning to rank for commu- nity question answering. In COLING.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Jianfeng Gao, and Bill Dolan. 2016a. A diversity-promoting objective function for neural conversation models",
                "authors": [
                    {
                        "first": "Jiwei",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Michel",
                        "middle": [],
                        "last": "Galley",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Brockett",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "NAACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. 2016a. A diversity-promoting ob- jective function for neural conversation models. In NAACL.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "A simple, fast diverse decoding algorithm for neural generation",
                "authors": [
                    {
                        "first": "Jiwei",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Will",
                        "middle": [],
                        "last": "Monroe",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Jurafsky",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1611.08562"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jiwei Li, Will Monroe, and Dan Jurafsky. 2016b. A simple, fast diverse decoding algorithm for neural generation. arXiv preprint arXiv:1611.08562.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Rouge: A package for automatic evaluation of summaries",
                "authors": [
                    {
                        "first": "Chin-Yew",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Text summarization branches out",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "A case study on neural headline generation for editing support",
                "authors": [
                    {
                        "first": "Kazuma",
                        "middle": [],
                        "last": "Murao",
                        "suffix": ""
                    },
                    {
                        "first": "Ken",
                        "middle": [],
                        "last": "Kobayashi",
                        "suffix": ""
                    },
                    {
                        "first": "Hayato",
                        "middle": [],
                        "last": "Kobayashi",
                        "suffix": ""
                    },
                    {
                        "first": "Taichi",
                        "middle": [],
                        "last": "Yatsuka",
                        "suffix": ""
                    },
                    {
                        "first": "Takeshi",
                        "middle": [],
                        "last": "Masuyama",
                        "suffix": ""
                    },
                    {
                        "first": "Tatsuru",
                        "middle": [],
                        "last": "Higurashi",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshimune",
                        "middle": [],
                        "last": "Tabuchi",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "NAACL-HLT",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kazuma Murao, Ken Kobayashi, Hayato Kobayashi, Taichi Yatsuka, Takeshi Masuyama, Tatsuru Hig- urashi, and Yoshimune Tabuchi. 2019. A case study on neural headline generation for editing support. In NAACL-HLT.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Sumit Chopra, and Jason Weston",
                "authors": [
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Alexander",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Rush",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "A neural attention model for abstractive sentence summarization",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1509.00685"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Alexander M Rush, Sumit Chopra, and Jason We- ston. 2015. A neural attention model for ab- stractive sentence summarization. arXiv preprint arXiv:1509.00685.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Get to the point: Summarization with pointer-generator networks",
                "authors": [
                    {
                        "first": "Abigail",
                        "middle": [],
                        "last": "See",
                        "suffix": ""
                    },
                    {
                        "first": "Peter",
                        "middle": [
                            "J"
                        ],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1704.04368"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Abigail See, Peter J Liu, and Christopher D Man- ning. 2017. Get to the point: Summarization with pointer-generator networks. arXiv preprint arXiv:1704.04368.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Bidirectional attention flow for machine comprehension",
                "authors": [
                    {
                        "first": "Minjoon",
                        "middle": [],
                        "last": "Seo",
                        "suffix": ""
                    },
                    {
                        "first": "Aniruddha",
                        "middle": [],
                        "last": "Kembhavi",
                        "suffix": ""
                    },
                    {
                        "first": "Ali",
                        "middle": [],
                        "last": "Farhadi",
                        "suffix": ""
                    },
                    {
                        "first": "Hannaneh",
                        "middle": [],
                        "last": "Hajishirzi",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2017. Bidirectional attention flow for machine comprehension. In ICLR.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Sequence to sequence learning with neural networks",
                "authors": [
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    },
                    {
                        "first": "Oriol",
                        "middle": [],
                        "last": "Vinyals",
                        "suffix": ""
                    },
                    {
                        "first": "Quoc V",
                        "middle": [],
                        "last": "Le",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. In NIPS.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Neural headline generation on abstract meaning representation",
                "authors": [
                    {
                        "first": "Sho",
                        "middle": [],
                        "last": "Takase",
                        "suffix": ""
                    },
                    {
                        "first": "Jun",
                        "middle": [],
                        "last": "Suzuki",
                        "suffix": ""
                    },
                    {
                        "first": "Naoaki",
                        "middle": [],
                        "last": "Okazaki",
                        "suffix": ""
                    },
                    {
                        "first": "Tsutomu",
                        "middle": [],
                        "last": "Hirao",
                        "suffix": ""
                    },
                    {
                        "first": "Masaaki",
                        "middle": [],
                        "last": "Nagata",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "EMNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sho Takase, Jun Suzuki, Naoaki Okazaki, Tsutomu Hi- rao, and Masaaki Nagata. 2016. Neural headline generation on abstract meaning representation. In EMNLP.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "From neural sentence summarization to headline generation: A coarse-to-fine approach",
                "authors": [
                    {
                        "first": "Jiwei",
                        "middle": [],
                        "last": "Tan",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaojun",
                        "middle": [],
                        "last": "Wan",
                        "suffix": ""
                    },
                    {
                        "first": "Jianguo",
                        "middle": [],
                        "last": "Xiao",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "IJCAI",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jiwei Tan, Xiaojun Wan, and Jianguo Xiao. 2017. From neural sentence summarization to headline generation: A coarse-to-fine approach. In IJCAI.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Attention is all you need",
                "authors": [
                    {
                        "first": "Ashish",
                        "middle": [],
                        "last": "Vaswani",
                        "suffix": ""
                    },
                    {
                        "first": "Noam",
                        "middle": [],
                        "last": "Shazeer",
                        "suffix": ""
                    },
                    {
                        "first": "Niki",
                        "middle": [],
                        "last": "Parmar",
                        "suffix": ""
                    },
                    {
                        "first": "Jakob",
                        "middle": [],
                        "last": "Uszkoreit",
                        "suffix": ""
                    },
                    {
                        "first": "Llion",
                        "middle": [],
                        "last": "Jones",
                        "suffix": ""
                    },
                    {
                        "first": "Aidan",
                        "middle": [
                            "N"
                        ],
                        "last": "Gomez",
                        "suffix": ""
                    },
                    {
                        "first": "\u0141ukasz",
                        "middle": [],
                        "last": "Kaiser",
                        "suffix": ""
                    },
                    {
                        "first": "Illia",
                        "middle": [],
                        "last": "Polosukhin",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "NIPS",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In NIPS.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Group linguistic bias aware neural response generation",
                "authors": [
                    {
                        "first": "Jianan",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Xin",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Fang",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Zhen",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Zhuoran",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Baoxun",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "SIGHAN Workshop on Chinese Language Processing",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jianan Wang, Xin Wang, Fang Li, Zhen Xu, Zhuoran Wang, and Baoxun Wang. 2017. Group linguistic bias aware neural response generation. In SIGHAN Workshop on Chinese Language Processing.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Neural lexicons for slot tagging in spoken language understanding",
                "authors": [
                    {
                        "first": "Kyle",
                        "middle": [],
                        "last": "Williams",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "NAACL-HLT",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kyle Williams. 2019. Neural lexicons for slot tagging in spoken language understanding. In NAACL-HLT.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Keyphrase extraction using deep recurrent neural networks on twitter",
                "authors": [
                    {
                        "first": "Qi",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Yeyun",
                        "middle": [],
                        "last": "Gong",
                        "suffix": ""
                    },
                    {
                        "first": "Xuanjing",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "EMNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Qi Zhang, Yang Wang, Yeyun Gong, and Xuanjing Huang. 2016. Keyphrase extraction using deep re- current neural networks on twitter. In EMNLP.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Question headline generation for news articles",
                "authors": [
                    {
                        "first": "Ruqing",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Jiafeng",
                        "middle": [],
                        "last": "Guo",
                        "suffix": ""
                    },
                    {
                        "first": "Yixing",
                        "middle": [],
                        "last": "Fan",
                        "suffix": ""
                    },
                    {
                        "first": "Yanyan",
                        "middle": [],
                        "last": "Lan",
                        "suffix": ""
                    },
                    {
                        "first": "Jun",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Huanhuan",
                        "middle": [],
                        "last": "Cao",
                        "suffix": ""
                    },
                    {
                        "first": "Xueqi",
                        "middle": [],
                        "last": "Cheng",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "CIKM",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ruqing Zhang, Jiafeng Guo, Yixing Fan, Yanyan Lan, Jun Xu, Huanhuan Cao, and Xueqi Cheng. 2018. Question headline generation for news articles. In CIKM.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "A comparative study on key phrase extraction methods in automatic web site summarization",
                "authors": [
                    {
                        "first": "Yongzheng",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Evangelos",
                        "middle": [],
                        "last": "Milios",
                        "suffix": ""
                    },
                    {
                        "first": "Nur",
                        "middle": [],
                        "last": "Zincir-Heywood",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "JDIM",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yongzheng Zhang, Evangelos Milios, and Nur Zincir- Heywood. 2007. A comparative study on key phrase extraction methods in automatic web site summa- rization. In JDIM.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Searching for effective neural extractive summarization: What works and what's next",
                "authors": [
                    {
                        "first": "Ming",
                        "middle": [],
                        "last": "Zhong",
                        "suffix": ""
                    },
                    {
                        "first": "Pengfei",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Danqing",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Xipeng",
                        "middle": [],
                        "last": "Qiu",
                        "suffix": ""
                    },
                    {
                        "first": "Xuanjing",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "ACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ming Zhong, Pengfei Liu, Danqing Wang, Xipeng Qiu, and Xuanjing Huang. 2019. Searching for effective neural extractive summarization: What works and what's next. In ACL.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Qing He. 2017a. Mechanism-aware neural machine for dialogue response generation",
                "authors": [
                    {
                        "first": "Ganbin",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Ping",
                        "middle": [],
                        "last": "Luo",
                        "suffix": ""
                    },
                    {
                        "first": "Rongyu",
                        "middle": [],
                        "last": "Cao",
                        "suffix": ""
                    },
                    {
                        "first": "Fen",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Bo",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "AAAI",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ganbin Zhou, Ping Luo, Rongyu Cao, Fen Lin, Bo Chen, and Qing He. 2017a. Mechanism-aware neural machine for dialogue response generation. In AAAI.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Commonsense knowledge aware conversation generation with graph attention",
                "authors": [
                    {
                        "first": "Hao",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Tom",
                        "middle": [],
                        "last": "Young",
                        "suffix": ""
                    },
                    {
                        "first": "Minlie",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Haizhou",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Jingfang",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaoyan",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "IJCAI",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Hao Zhou, Tom Young, Minlie Huang, Haizhou Zhao, Jingfang Xu, and Xiaoyan Zhu. 2018. Com- monsense knowledge aware conversation generation with graph attention. In IJCAI.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Selective encoding for abstractive sentence summarization",
                "authors": [
                    {
                        "first": "Qingyu",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Nan",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Furu",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Ming",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "ACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Qingyu Zhou, Nan Yang, Furu Wei, and Ming Zhou. 2017b. Selective encoding for abstractive sentence summarization. In ACL.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 1: Keyphrase-aware multi-headline generation",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Figure 2: Visualization of the computational steps in each block of our multi-source Transformer decoders.",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "(a) unsupervised TF-IDF Ranking, (b) supervised sequence-to-sequence model (SEQ2SEQ), and (c) supervised slot tagging model (SLOT). Implementation and Hyperparameters. The SEQ2SEQ has the same architecture hyperparameters as BASE model. And the architecture hyperparameters of SLOT are the same as those of the BERT-base 3 . We use article-keyphrase pairs in the train set of KeyAware News to train SEQ2SEQ and SLOT.",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF3": {
                "num": null,
                "text": "(a), which is keyphrase-agnostic and only takes the news article as input. (2) BASE + KEY, as shown in Figure 2 (b), which takes keyphrase and article as input. (3) BASE + Filter, as shown in Figure 2 (c), which takes keyphrase-filtered article as input. (4) BASE + StackFuse, (5) BASE + AddFuse, and (6) BASE + ParallelFuse as shown in Figure 2 (d-f), which take the keyphrase-filtered article and the original article as inputs with stack-fusing, addition-fusing, and parallel-fusing mechanism, respectively. Based on BASE + StackFuse, BASE + AddFuse, and BASE + ParallelFuse, we further use the keyphrase as their additional inputs, like BASE + KEY.",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF4": {
                "num": null,
                "text": "Figure 3: Examples of original articles, golden headlines and multiple generated outputs by BASE, BASE + Diverse and BASE + AddFuse. Each generated keyphrase is shown at the end of each generated headline.",
                "uris": null,
                "fig_num": "3",
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table><tr><td>TF-IDF</td><td>18.63</td><td>42.05</td><td>52.60</td><td>30.13</td><td>53.82</td><td>63.91</td></tr><tr><td>SEQ2SEQ</td><td>57.27</td><td>78.45</td><td>84.26</td><td>59.60</td><td>81.32</td><td>87.04</td></tr><tr><td>SLOT</td><td>60.75</td><td>76.94</td><td>83.18</td><td>65.13</td><td>84.05</td><td>89.08</td></tr></table>",
                "type_str": "table",
                "text": "Keyphrase Generation Results",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table><tr><td>Method</td><td>k=1</td><td>mAP@1 k=3</td><td>k=5</td><td>k=1</td><td>mAP@3 k=3</td><td>k=5</td><td>k=1</td><td>mAP@5 k=3</td><td>k=5</td><td>k=1</td><td>mAP@10 k=3</td><td>k=5</td></tr><tr><td>HUMAN</td><td>49.25</td><td>-</td><td>-</td><td>63.94</td><td>-</td><td>-</td><td>69.40</td><td>-</td><td>-</td><td>75.70</td><td>-</td><td>-</td></tr><tr><td>PT-GEN</td><td>32.08</td><td>35.50</td><td>37.01</td><td>45.23</td><td>49.36</td><td>50.91</td><td>50.51</td><td>54.61</td><td>56.30</td><td>56.77</td><td>60.93</td><td>62.51</td></tr><tr><td>SEASS</td><td>28.82</td><td>31.66</td><td>32.85</td><td>42.09</td><td>45.27</td><td>46.60</td><td>47.66</td><td>50.59</td><td>52.11</td><td>54.51</td><td>57.74</td><td>59.13</td></tr><tr><td>Transformer + Copy</td><td>37.55</td><td>41.88</td><td>43.46</td><td>51.09</td><td>55.99</td><td>57.72</td><td>56.54</td><td>61.45</td><td>63.06</td><td>62.58</td><td>67.21</td><td>69.04</td></tr><tr><td>BASE</td><td>39.43</td><td>43.80</td><td>45.54</td><td>53.77</td><td>58.35</td><td>60.16</td><td>59.29</td><td>63.88</td><td>65.70</td><td>65.51</td><td>69.92</td><td>71.68</td></tr><tr><td>BASE + Diverse</td><td>39.30</td><td>45.02</td><td>47.07</td><td>53.69</td><td>60.05</td><td>62.18</td><td>59.10</td><td>65.49</td><td>67.61</td><td>65.37</td><td>71.65</td><td>73.73</td></tr><tr><td>BASE + Filter</td><td>34.30</td><td>43.02</td><td>45.91</td><td>48.28</td><td>58.07</td><td>61.61</td><td>53.91</td><td>63.78</td><td>67.42</td><td>60.41</td><td>70.51</td><td>74.06</td></tr><tr><td>BASE + KEY</td><td>39.38</td><td>45.04</td><td>46.81</td><td>53.81</td><td>60.05</td><td>61.91</td><td>59.39</td><td>65.75</td><td>67.59</td><td>65.59</td><td>71.99</td><td>73.97</td></tr><tr><td>BASE + AddFuse</td><td>39.95</td><td>46.64</td><td>48.72</td><td>54.59</td><td>61.90</td><td>63.92</td><td>60.20</td><td>67.63</td><td>69.64</td><td>66.56</td><td>73.95</td><td>75.96</td></tr><tr><td>BASE + ParallelFuse</td><td>39.82</td><td>46.57</td><td>49.03</td><td>54.44</td><td>61.80</td><td>64.27</td><td>59.91</td><td>67.50</td><td>69.85</td><td>66.47</td><td>73.82</td><td>76.08</td></tr><tr><td>BASE + StackFuse</td><td>40.11</td><td>45.96</td><td>47.95</td><td>54.52</td><td>61.05</td><td>63.05</td><td>60.11</td><td>66.69</td><td>68.70</td><td>66.57</td><td>72.91</td><td>74.86</td></tr><tr><td>BASE + AddFuse + KEY</td><td>39.19</td><td>46.01</td><td>48.55</td><td>53.73</td><td>61.32</td><td>63.70</td><td>59.34</td><td>66.99</td><td>69.38</td><td>65.70</td><td>73.54</td><td>75.77</td></tr><tr><td>BASE + ParallelFuse + KEY</td><td>40.24</td><td>46.46</td><td>48.66</td><td>54.82</td><td>61.64</td><td>63.68</td><td>60.46</td><td>67.24</td><td>69.30</td><td>67.01</td><td>73.70</td><td>75.53</td></tr><tr><td>BASE + StackFuse + KEY</td><td>39.78</td><td>46.33</td><td>48.55</td><td>54.28</td><td>61.43</td><td>63.62</td><td>59.95</td><td>67.07</td><td>69.21</td><td>66.20</td><td>73.35</td><td>75.37</td></tr></table>",
                "type_str": "table",
                "text": "News Article Retrieval Results",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table><tr><td>Golden</td><td colspan=\"3\">mountain east conference to welcome davis &amp; elkins as full member, unc pembroke as associate member</td></tr><tr><td>BASE</td><td colspan=\"2\">mountain east conference announces new conference members</td></tr><tr><td/><td colspan=\"2\">mountain east conference announces two new conference members</td></tr><tr><td/><td>unc pembroke announces new conference members</td><td/></tr><tr><td>BASE + Diverse</td><td>unc pembroke announces new conference</td><td/></tr><tr><td/><td colspan=\"2\">members mountain east conference announces two new conference members</td></tr><tr><td/><td>unc pembroke announces 2019 -20 conference members</td><td/></tr><tr><td>BASE +AddFuse</td><td colspan=\"2\">mountain east conference announces two new conference members</td><td>[mountain east conference]</td></tr><tr><td/><td>unc pembroke to join frostburg state as new members</td><td/><td>[frostburg state]</td></tr><tr><td/><td colspan=\"3\">unc pembroke will join the conference in football as new football conference members in 2020 [football]</td></tr><tr><td colspan=\"3\">Article#2 one of the Golden suspect on fbi's 10 most wanted list killed in north carolina</td></tr><tr><td>BASE</td><td colspan=\"2\">fbi ' s 10 most wanted shot and killed in incident involving apex police</td></tr><tr><td/><td colspan=\"2\">fbi ' s 10 most wanted shot , killed in incident involving apex police and fbi</td></tr><tr><td/><td>fbi ' s 10 most wanted shot and killed in apex incident</td><td/></tr><tr><td>BASE + Diverse</td><td>fbi ' s 10 most wanted shot , killed in apex incident</td><td/></tr><tr><td/><td>wpd : fbi ' s most wanted shot , killed in apex incident</td><td/></tr><tr><td/><td>fbi ' s most wanted shot and killed in apex , fbi says</td><td/></tr><tr><td>BASE +AddFuse</td><td colspan=\"2\">one of fbi ' s 10 most wanted , killed during an incident [10 wanted]</td></tr><tr><td/><td colspan=\"2\">suspect arrested , killed in incident involving apex police [apex police]</td></tr><tr><td/><td>fbi ' s 10 most wanted shot , killed in south carolina</td><td>[north carolina]</td></tr></table>",
                "type_str": "table",
                "text": "The results of the qualified rate of golden, BASE + Stack, BASE + Diverse, and random are 91.8%, Article#1 the mountain east conference -which says farewell to two members after this season -announced two new members thursday, one full member and one associate member. davis & elkins and the university of north carolina at pembroke will join frostburg state as new conference members. \u2026 [7 sentences with 151 words are abbreviated from here.] two original conference members will depart after the 2018-19 season. shepherd will move to the pennsylvania state athletic conference and uva-wise will move to the south atlantic conference. member school wheeling jesuit is adding football and will play a full schedule next season. when unc pembroke joins, the conference will have 12 football programs. fbi's 10 most wanted was shot and killed during an incident involving apex police and the fbi on wednesday. the fbi and apex police were at woodspring suites, located at 901 lufkin road in apex, after following a tip concerning a fugitive. \u2026 [6 sentences with 129 words are abbreviated from here.] according to officials carlson posted a bond and fled to mount pleasant in south carolina. he was placed on the fbi's list of top ten fugitives in september 2018. the medical examiner will need to positively identify carlson. \"the fbi is grateful to our partners with the apex police department for the assistance,\" the fbi said.",
                "html": null,
                "num": null
            }
        }
    }
}