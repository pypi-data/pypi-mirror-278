{
    "paper_id": "N10-1016",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:24:33.876364Z"
    },
    "title": "Learning Translation Boundaries for Phrase-Based Decoding",
    "authors": [
        {
            "first": "Deyi",
            "middle": [],
            "last": "Xiong",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Min",
            "middle": [],
            "last": "Zhang",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Haizhou",
            "middle": [],
            "last": "Li",
            "suffix": "",
            "affiliation": {},
            "email": ""
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Constrained decoding is of great importance not only for speed but also for translation quality. Previous efforts explore soft syntactic constraints which are based on constituent boundaries deduced from parse trees of the source language. We present a new framework to establish soft constraints based on a more natural alternative: translation boundary rather than constituent boundary. We propose simple classifiers to learn translation boundaries for any source sentences. The classifiers are trained directly on word-aligned corpus without using any additional resources. We report the accuracy of our translation boundary classifiers. We show that using constraints based on translation boundaries predicted by our classifiers achieves significant improvements over the baseline on large-scale Chinese-to-English translation experiments. The new constraints also significantly outperform constituent boundary based syntactic constrains.",
    "pdf_parse": {
        "paper_id": "N10-1016",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Constrained decoding is of great importance not only for speed but also for translation quality. Previous efforts explore soft syntactic constraints which are based on constituent boundaries deduced from parse trees of the source language. We present a new framework to establish soft constraints based on a more natural alternative: translation boundary rather than constituent boundary. We propose simple classifiers to learn translation boundaries for any source sentences. The classifiers are trained directly on word-aligned corpus without using any additional resources. We report the accuracy of our translation boundary classifiers. We show that using constraints based on translation boundaries predicted by our classifiers achieves significant improvements over the baseline on large-scale Chinese-to-English translation experiments. The new constraints also significantly outperform constituent boundary based syntactic constrains.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "It has been known that phrase-based decoding (phrase segmentation/translation/reordering (Chiang, 2005) ) should be constrained to some extent not only for transferring the NP-hard problem (Knight, 1999 ) into a tractable one in practice but also for improving translation quality. For example, Xiong et al. (2008) find that translation quality can be significantly improved by either prohibiting reorderings around punctuation or restricting reorderings within a 15-word window.",
                "cite_spans": [
                    {
                        "start": 89,
                        "end": 103,
                        "text": "(Chiang, 2005)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 189,
                        "end": 202,
                        "text": "(Knight, 1999",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 295,
                        "end": 314,
                        "text": "Xiong et al. (2008)",
                        "ref_id": "BIBREF21"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Recently, more linguistically motivated constraints are introduced to improve phrase-based decoding. (Cherry, 2008) and (Marton and Resnik, 2008) introduce syntactic constraints into the standard phrase-based decoding (Koehn et al., 2003) and hierarchical phrase-based decoding (Chiang, 2005) respectively by using a counting feature which accumulates whenever hypotheses violate syntactic boundaries of source-side parse trees. (Xiong et al., 2009) further presents a bracketing model to include thousands of context-sensitive syntactic constraints. All of these approaches achieve their improvements by guiding the phrase-based decoder to prefer translations which respect source-side parse trees.",
                "cite_spans": [
                    {
                        "start": 101,
                        "end": 115,
                        "text": "(Cherry, 2008)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 120,
                        "end": 145,
                        "text": "(Marton and Resnik, 2008)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 218,
                        "end": 238,
                        "text": "(Koehn et al., 2003)",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 278,
                        "end": 292,
                        "text": "(Chiang, 2005)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 429,
                        "end": 449,
                        "text": "(Xiong et al., 2009)",
                        "ref_id": "BIBREF22"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "One major problem with such constituent boundary based constraints is that syntactic structures of the source language do not necessarily reflect translation structures where the source and target language correspond to each other. In this paper, we investigate building classifiers that directly address the problem of translation boundary, rather than extracting constituent boundary from sourceside parsers built for a different purpose. A translation boundary is a position in the source sequence which begins or ends a translation zone1 spanning multiple source words. In a translation zone, the source phrase is translated as a unit. Reorderings which cross translation zones are not desirable.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Inspired by (Roark and Hollingshead, 2008 ) which introduces classifiers to decide if a word can begin/end a multi-word constituent, we build two discriminative classifiers to tag each word in the source sequence with a binary class label. The first classifier decides if a word can begin a multi-sourceword translation zone; the second classifier decides if a word can end a multi-source-word translation zone. Given a partial translation covering source sequence (i, j) with start word c i and end word c j2 , this translation can be penalized if the first classifier decides that the start word c i can not be a beginning translation boundary or the second classifier decides that the end word c j can not be an ending translation boundary. In such a way, we can guide the decoder to boost hypotheses that respect translation boundaries and therefore the common translation structure shared by the source and target language, rather than the syntactic structure of the source language.",
                "cite_spans": [
                    {
                        "start": 12,
                        "end": 41,
                        "text": "(Roark and Hollingshead, 2008",
                        "ref_id": "BIBREF14"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "We report the accuracy of such classifiers by comparing their outputs with \"gold\" translation boundaries obtained from reference translations on the development set. We integrate translation boundary based constraints into phrase-based decoding and display that they improve translation quality significantly in large-scale experiments. Furthermore, we confirm that they also significantly outperform constituent boundary based syntactic constraints.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "To better understand the particular task that we address in this paper, we study the distribution of classes of translation boundaries in real-world data. First, we introduce some notations. Given a source sentence c 1 ...c n , we will say that a word c i (1 < i < n) is in the class B y if there is a translation zone \u03c4 spanning c i ...c j for some j > i; and c i \u2208 B n otherwise. Similarly, we will say that a word c j is in the class E y if there is a translation zone spanning c i ...c j for some j > i; and c j \u2208 E n otherwise.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Beginning and Ending Translation Zones",
                "sec_num": "2"
            },
            {
                "text": "Here, a translation zone \u03c4 is a pair of aligned source phrase and target phrase",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Beginning and Ending Translation Zones",
                "sec_num": "2"
            },
            {
                "text": "\u03c4 = (c j i , e q p )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Beginning and Ending Translation Zones",
                "sec_num": "2"
            },
            {
                "text": "where \u03c4 must be consistent with the word alignment M",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Beginning and Ending Translation Zones",
                "sec_num": "2"
            },
            {
                "text": "\u2200(u, v) \u2208 M, i \u2264 u \u2264 j \u2194 p \u2264 v \u2264 q",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Beginning and Ending Translation Zones",
                "sec_num": "2"
            },
            {
                "text": "By this, we require that no words inside the source phrase c j i are aligned to words outside the target phrase e q p and that no words outside the source phrase are aligned to words inside the target phrase.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Beginning and Ending Translation Zones",
                "sec_num": "2"
            },
            {
                "text": "Count (M) P (%) This means, in other words, that the source phrase c j i is mapped as a unit onto the target phrase e q p . When defining the B y and E y class, we also require that the source phrase c j i in the translation zone must contain multiple words (j > i). Our interest is the question of whether a sequence of consecutive source words can be translated as a unit (i.e. whether there is a translation zone covering these source words). For a single-word source phrase, if it can be translated separately, it is always translated as a unit in the context of phrase-based decoding. Therefore this question does not exist.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Item",
                "sec_num": null
            },
            {
                "text": "Note that the first word c 1 and the last word c n are unambiguous in terms of whether they begin or end a translation zone. The first word c 1 must begin a translation zone spanning the whole source sentence. The last word c n must end a translation zone spanning the whole source sentence. Therefore, our classifiers only need to predict the other n -2 words for a source sentence of length n.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Item",
                "sec_num": null
            },
            {
                "text": "Table 1 shows statistics of word classes from our training data which contain nearly 100M words in approximately 4M sentences. Among these words, only 22.7M words can begin a translation zone which covers multiple source words. 41M words can end a translation zone spanning multiple source words, which accounts for more than 42% in all words. We still have more than 33M words, accounting for 34.3%, which neither begin nor end a multi-source-word translation zone. Apparently, translations that begin/end on words \u2208 B y /\u2208 E y are preferable to those which begin/end on other words.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "1",
                        "ref_id": "TABREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Item",
                "sec_num": null
            },
            {
                "text": "Yet another interesting study is to compare translation boundaries with constituent boundaries deduced from source-side parse trees. In doing so, we can know further how well constituent boundary based syntactic constraints can improve translation quality. We pair the source sentences of our development set with each of the reference translations and include the created sentence pairs in our bilingual training corpus. Then we obtain word alignments on the new corpus (see Section 5.1 for the details of learning word alignments). From the word alignments we obtain translation boundaries (see details in the next section). We parse the source sentences of our development set and obtain constituent boundaries from parse trees.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Item",
                "sec_num": null
            },
            {
                "text": "To make a clear comparison with our translation boundary classifiers (see Section 3.3), we treat constituent boundaries deduced from source-side parse trees as output from beginning/ending boundary classifiers: the constituent beginning boundary corresponds to B y ; the constituent ending boundary corresponds to E y . We have four reference translations for each source sentence. Therefore we have four translation boundary sets, each of which is produced from word alignments between source sentences and one reference translation set. Each of the four translation boundary sets will be used as a gold standard. We calculate classification accuracy for our constituent boundary deducer on each gold standard and average them finally.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Item",
                "sec_num": null
            },
            {
                "text": "Table 2 shows the accuracy results. The average accuracies on the four gold standard sets are very low, especially for the B y /B n classification task. In section 3.3, we will show that our translation boundary classifiers achieve higher accuracy than that of constituent boundary deducer. This suggests that pure constituent boundary based constraints are not the best choice to constrain phrase-based decoding.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Item",
                "sec_num": null
            },
            {
                "text": "In this section, we investigate building classifiers to predict translation boundaries. First, we elabo-rate the acquisition of training instances from word alignments. Second, we build two classifiers with simple features on the obtained training instances. Finally, we evaluate our classifiers on the development set using the \"gold\" translation boundaries obtained from reference translations.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Translation Boundaries",
                "sec_num": "3"
            },
            {
                "text": "We can easily obtain constituent boundaries from parse trees. Similarly, if we have a tree covering both source and target sentence, we can easily get translation boundaries from this tree. Fortunately, we can build such a tree directly from word alignments. We use (Zhang et al., 2008) 's shift-reduce algorithm (SRA) to decompose word alignments into hierarchical trees.",
                "cite_spans": [
                    {
                        "start": 266,
                        "end": 286,
                        "text": "(Zhang et al., 2008)",
                        "ref_id": "BIBREF24"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Obtaining Translation Boundaries from Word Alignments",
                "sec_num": "3.1"
            },
            {
                "text": "Given an arbitrary word-level alignment as an input, SRA is able to output a tree representation of the word alignment (a.k.a decomposition tree). Each node of the tree is a translation zone as we defined in the Section 2. Therefore the first word on the source side of each multi-source-word node is a beginning translation boundary (\u2208 B y ); the last word on the source side of each multi-source-word node is an ending translation boundary (\u2208 E y ). Figure 1a shows an example of many-to-many alignment, where the source language is Chinese and the target language is English. Each word is indexed with their occurring position from left to right. Figure 1b is the tree representation of the word alignment after hierarchical analysis using SRA. We use ([i, j], [p, q] ) to denote a tree node, where i, j and p, q are the beginning and ending index in the source and target language, respectively. By checking nodes which cover multiple source words, we can easily decide that the source words {\u8fc7\u53bb, \u4e94, \u56e0\u6545} are in the class B y and any other words are in the class B n if we want to train a B y /B n classifier with class labels {B y , B n }. Similarly, the source words {\u6b21, \u98de\u884c, \u90fd, \u5931\u8d25} are in the class E y and any other words are in the class E n when we train a E y /E n classifier with class labels {E y , E n }.",
                "cite_spans": [
                    {
                        "start": 764,
                        "end": 770,
                        "text": "[p, q]",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 459,
                        "end": 461,
                        "text": "1a",
                        "ref_id": null
                    },
                    {
                        "start": 657,
                        "end": 659,
                        "text": "1b",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Obtaining Translation Boundaries from Word Alignments",
                "sec_num": "3.1"
            },
            {
                "text": "By using SRA on each word-aligned bilingual sentence, as described above, we can tag each source word with two sets of class labels: {B y , B n } and {E y , E n }. The tagged source sentences will be used to train our two translation boundary classifiers.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Obtaining Translation Boundaries from Word Alignments",
                "sec_num": "3.1"
            },
            {
                "text": "The last five flights all failed due to accidents",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "\u8fc7\u53bb \u6b21 \u98de\u884c \u90fd \u56e0\u6545 \u5931\u8d25",
                "sec_num": null
            },
            {
                "text": "\u4e94 1 2 3 4 5 6 7 1 2 3 4 5 6 7 8 9 ([1, 7], [1, 9]) ([6, 7], [6, 9]) ([6, 6], [7, 9]) ([7, 7], [6, 6]) ([1, 5], [1, 5]) ([1, 4], [1, 4]) ([5, 5], [5, 5]) ([1, 3], [1, 3]) ([4, 4], [4, 4]) ([1, 1], [1, 2]) ([2, 3], [3, 3]) a) b)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "\u8fc7\u53bb \u6b21 \u98de\u884c \u90fd \u56e0\u6545 \u5931\u8d25",
                "sec_num": null
            },
            {
                "text": "Figure 1 : An example of many-to-many word alignment and its tree representation produced by (Zhang et al., 2008) 's shift-reduce algorithm.",
                "cite_spans": [
                    {
                        "start": 93,
                        "end": 113,
                        "text": "(Zhang et al., 2008)",
                        "ref_id": "BIBREF24"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "\u8fc7\u53bb \u6b21 \u98de\u884c \u90fd \u56e0\u6545 \u5931\u8d25",
                "sec_num": null
            },
            {
                "text": "We We use features from surrounding words, including 2 before and 2 after the current word position (c -2 , c -1 , c +1 , c +2 ). We also use class features to train models with Markov order 1 (including class feature \u03b6 c -1 ), and Markov order 2 (including class",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Building Translation Boundary Classifiers",
                "sec_num": "3.2"
            },
            {
                "text": "features \u03b6 c -1 , \u03b6 c -2 ).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Building Translation Boundary Classifiers",
                "sec_num": "3.2"
            },
            {
                "text": "How well can we perform these binary classification tasks using the classifiers described above? Can we obtain better translation boundary predictions than extracting constituent boundary from sourceside parse trees? To investigate these questions, we evaluate our MEMM based classifiers. We trained them on our 100M-word word-aligned corpus. We ran the two trained classifiers on the development set separately to obtain the B y /B n words and E y /E n words. Then we built our four gold standards using four reference translation sets as described in Sec- generate very different translation boundaries. We can measure these differences in reference translations using the same evaluation metric (classification accuracy). We treat each reference translation set as a translation boundary classifier while the other three reference translation sets as gold standards.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluating Translation Boundary Classifiers",
                "sec_num": "3.3"
            },
            {
                "text": "We calculate the classification accuracy for the current reference translation set and finally average all four accuracies. Table 4 presents the results.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 130,
                        "end": 131,
                        "text": "4",
                        "ref_id": "TABREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Evaluating Translation Boundary Classifiers",
                "sec_num": "3.3"
            },
            {
                "text": "Comparing Table 4 with Table 3 , we can see that the accuracy of our translation boundary classification approach is not that low when considering vast divergences of reference translations. The question now becomes, how can classifier output be used to constrain phrase-based decoding, and what is the impact on the system performance of using such constraints.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 16,
                        "end": 17,
                        "text": "4",
                        "ref_id": "TABREF4"
                    },
                    {
                        "start": 29,
                        "end": 30,
                        "text": "3",
                        "ref_id": "TABREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Evaluating Translation Boundary Classifiers",
                "sec_num": "3.3"
            },
            {
                "text": "By running the two trained classifiers on the source sentence separately, we obtain two classified word sets: B y /B n words, and E y /E n words. We can prohibit any translations or reorderings spanning c i ...c j (j > i) where c i / \u2208 B y according to the first classifier or c j / \u2208 E y according to the second classifier. In such a way, we integrate translation boundaries into phrase-based decoding as hard constraints, which, however, is at the risk of producing no translation covering the whole source sentence.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Integrating Translation Boundaries into Decoding",
                "sec_num": "4"
            },
            {
                "text": "Alternatively, we introduce soft constraints based on translation boundary that our classifiers predict, similar to constituent boundary based soft constraints in (Cherry, 2008) and (Marton and Resnik, 2008) Unlike hard constraints, which simply prevent any hypotheses from violating translation boundaries, soft constraints allow violations of translation boundaries but with a penalty of exp(-\u03bb v C v ) where C v is the violation count. By using soft constraints, we can enable the model to prefer hypotheses which are consistent with translation boundaries.",
                "cite_spans": [
                    {
                        "start": 163,
                        "end": 177,
                        "text": "(Cherry, 2008)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 182,
                        "end": 207,
                        "text": "(Marton and Resnik, 2008)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Integrating Translation Boundaries into Decoding",
                "sec_num": "4"
            },
            {
                "text": "Our baseline system is a phrase-based system using BTGs (Wu, 1997) , which includes a contentdependent reordering model discriminatively trained using reordering examples (Xiong et al., 2006) . We carried out various experiments to evaluate the impact of integrating translation boundary based soft constraints into decoding on the system performance on the Chinese-to-English translation task of the NIST MT-05 using large scale training data.",
                "cite_spans": [
                    {
                        "start": 56,
                        "end": 66,
                        "text": "(Wu, 1997)",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 171,
                        "end": 191,
                        "text": "(Xiong et al., 2006)",
                        "ref_id": "BIBREF20"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiment",
                "sec_num": "5"
            },
            {
                "text": "Our training corpora are listed in Table 5 . The whole corpora consist of 96.9M Chinese words and 109.5M English words in 3.8M sentence pairs. We ran GIZA++ (Och and Ney, 2000) on the parallel corpora in both directions and then applied the \"grow-diag-final\" refinement rule (Koehn et al., 2005) to obtain many-to-many word alignments. From the word-aligned corpora, we extracted bilingual phrases and trained our translation model.",
                "cite_spans": [
                    {
                        "start": 157,
                        "end": 176,
                        "text": "(Och and Ney, 2000)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 275,
                        "end": 295,
                        "text": "(Koehn et al., 2005)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 41,
                        "end": 42,
                        "text": "5",
                        "ref_id": "TABREF5"
                    }
                ],
                "eq_spans": [],
                "section": "Experimental Setup",
                "sec_num": "5.1"
            },
            {
                "text": "We used all corpora in Table 5 except for the United Nations corpus to train our MaxEnt based reordering model (Xiong et al., 2006) , which con-sist of 33.3M Chinese words and 35.8M English words. We built a four-gram language model using the SRILM toolkit (Stolcke, 2002) , which was trained on Xinhua section of the English Gigaword corpus (181.1M words).",
                "cite_spans": [
                    {
                        "start": 111,
                        "end": 131,
                        "text": "(Xiong et al., 2006)",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 257,
                        "end": 272,
                        "text": "(Stolcke, 2002)",
                        "ref_id": "BIBREF15"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 29,
                        "end": 30,
                        "text": "5",
                        "ref_id": "TABREF5"
                    }
                ],
                "eq_spans": [],
                "section": "Experimental Setup",
                "sec_num": "5.1"
            },
            {
                "text": "To train our translation boundary classifiers, we extract training instances from the whole wordaligned corpora, from which we obtain 96.9M training instances for the B y /B n and E y /E n classifier. We ran the off-the-shelf MaxEnt toolkit (Zhang, 2004) to tune classifier feature weights with Gaussian prior set to 1 to avoid overfitting.",
                "cite_spans": [
                    {
                        "start": 241,
                        "end": 254,
                        "text": "(Zhang, 2004)",
                        "ref_id": "BIBREF25"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Setup",
                "sec_num": "5.1"
            },
            {
                "text": "We used the NIST MT-03 evaluation test data as our development set (919 sentences in total, 27.1 words per sentence). The NIST MT-05 test set includes 1082 sentences with an average of 27.4 words per sentence. Both the reference corpus for the NIST MT-03 set and the reference corpus for the NIST MT-05 set contain 4 translations per source sentence. To compare with constituent boundary based constraints, we parsed source sentences of both the development and test sets using a Chinese parser (Xiong et al., 2005) which was trained on the Penn Chinese Treebank with an F 1 -score of 79.4%.",
                "cite_spans": [
                    {
                        "start": 495,
                        "end": 515,
                        "text": "(Xiong et al., 2005)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Setup",
                "sec_num": "5.1"
            },
            {
                "text": "Our evaluation metric is case-insensitive BLEU-4 (Papineni et al., 2002) using the shortest reference sentence length for the brevity penalty. Statistical significance in BLEU score differences was tested by paired bootstrap re-sampling (Koehn, 2004) .",
                "cite_spans": [
                    {
                        "start": 49,
                        "end": 72,
                        "text": "(Papineni et al., 2002)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 237,
                        "end": 250,
                        "text": "(Koehn, 2004)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Setup",
                "sec_num": "5.1"
            },
            {
                "text": "The most direct way to investigate the impact on the system performance of using translation boundaries is to integrate \"right\" translation boundaries into decoding which are directly obtained from reference translations. For both the development set and test set, we have four reference translation sets, which are named ref1, ref2, ref3 and ref4, respectively. For the development set, we used translation boundaries obtained from ref1. Based on these boundaries, we built our translation boundary violation counting feature and tuned its feature weight with other features using MERT. When we obtained the best feature weights \u03bbs, we evaluated on the test set using translation boundaries produced from ref1, ref2, ref3 and ref4 of the test set respectively. ing \"right\" translation boundaries to build soft constraints significantly improve the performance measured by BLEU score. The best result comes from ref4, which achieves an absolute increase of 1.16 BLEU points over the baseline. We believe that the best result here only indicates the lower bound of potential improvement when using right translation boundaries. If we have consistent translation boundaries on the development and test set (for example, we have the same 4 translators build reference translations for both the development and test set), the performance improvement will be higher.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Using Translation Boundaries from Reference Translations",
                "sec_num": "5.2"
            },
            {
                "text": "The success of using translation boundaries from reference translations inspires us to pursue translation boundaries predicted by our MEMM based classifiers. We ran our MEMM1 (Markov order 1) and MEMM2 (Markov order 2) B y /B n and E y /E n classifiers on both the development and test set.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Using Automatically Learned Translation Boundaries",
                "sec_num": "5.3"
            },
            {
                "text": "Based on translation boundaries output by MEMM1 and MEMM2 classifiers, we built our translation boundary violation feature and tuned it on the development set. The evaluation results on the test set are shown in Table 7 .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 218,
                        "end": 219,
                        "text": "7",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Using Automatically Learned Translation Boundaries",
                "sec_num": "5.3"
            },
            {
                "text": "From Table 7 we observe that using soft constraints based on translation boundaries from both our MEMM 1 and MEMM 2 significantly outperform the baseline. Impressively, when using outputs from MEMM 2, we achieve an absolute improvement of almost 1 BLEU point over the baseline. This result is also very close to the best result of using translation boundaries from reference translations.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 11,
                        "end": 12,
                        "text": "7",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Using Automatically Learned Translation Boundaries",
                "sec_num": "5.3"
            },
            {
                "text": "To compare with constituent boundary based syntactic constraints, we also carried out experiments using two kinds of such constraints. Condeducer which uses pure constituent boundary based syntactic constraint: any partial translations which cross any constituent boundaries will be penalized. The other is the XP+ from (Marton and Resnik, 2008) which only penalizes hypotheses which violate the boundaries of a constituent with a label from {NP, VP, CP, IP, PP, ADVP, QP, LCP, DNP}. The XP+ is the best syntactic constraint among all constraints that Marton and Resnik (2008) use for Chinese-to-English translation.",
                "cite_spans": [
                    {
                        "start": 320,
                        "end": 345,
                        "text": "(Marton and Resnik, 2008)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 552,
                        "end": 576,
                        "text": "Marton and Resnik (2008)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Using Automatically Learned Translation Boundaries",
                "sec_num": "5.3"
            },
            {
                "text": "Still in Table 7 , we find that both syntactic constraint Condeducer and XP+ are better than the baseline. But only XP+ is able to obtain significant improvement. Both our MEMM 1 and MEMM 2 outperform Condeducer. MEMM 2 achieves significant improvement over XP+ by approximately 0.5 BLEU points. This comparison suggests that translation boundary is a better option than constituent boundary when we build constraints to restrict phrasebased decoding.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 15,
                        "end": 16,
                        "text": "7",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Using Automatically Learned Translation Boundaries",
                "sec_num": "5.3"
            },
            {
                "text": "Revisiting the classification task in this paper, we can also consider it as a sequence labeling task where the first source word of a translation zone is labeled \"B\", the last source word of the translation zone is labeled \"E\", and other words are labeled \"O\". To complete such a sequence labeling task, we built only one classifier which is still based on MEMM (with Markov order 2) with the same features as described in Section 3.2. We built soft constraints based on the outputs of this classifier and evaluated them on the test set. The case-insensitive BLEU score is 33.62, which is lower than the performance of using two separate classifiers (34.04).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "One Classifier vs. Two Classifiers",
                "sec_num": "5.4"
            },
            {
                "text": "We calculated the accuracy for class \"B\" by mapping \"B\" to B y and \"E\" and \"O\" to B n . The result is 67.9%. Similarly, we obtained the accuracy of class \"E\", which is as low as 48.6%. These two accuracies are much lower than those of using two separate classifiers, especially the accuracy of \"E\". This suggests that the B y and E y are not interrelated tightly. It is better to learn them separately with two classifiers.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "One Classifier vs. Two Classifiers",
                "sec_num": "5.4"
            },
            {
                "text": "Another advantage of using two separate classifiers is that we can explore more constraints. A word c k can be possibly labeled as B y by the first classifier and E y by the second classifier. Therefore we can build soft constraints on span",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "One Classifier vs. Two Classifiers",
                "sec_num": "5.4"
            },
            {
                "text": "(c i , c k ) (c i \u2208 B y , c k \u2208 E y ) and span (c k , c j ) (c k \u2208 B y , c j \u2208 E y )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "One Classifier vs. Two Classifiers",
                "sec_num": "5.4"
            },
            {
                "text": ". This is impossible if we use only one classifier since each word can have only one class label. We can build only one constraint on span (c i , c k ) or span (c k , c j ).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "One Classifier vs. Two Classifiers",
                "sec_num": "5.4"
            },
            {
                "text": "Various approaches incorporate constraints into phrase-based decoding in a soft or hard manner. Our introduction has already briefly mentioned (Cherry, 2008) and (Marton and Resnik, 2008) , which utilize source-side parse tree boundary violation counting feature to build soft constraints for phrase-based decoding, and (Xiong et al., 2009) , which calculates a score to indicate to what extent a source phrase can be translated as a unit using a bracketing model with richer syntactic features. More previously, (Chiang, 2005) rewards hypotheses whenever they exactly match constituent boundaries of parse trees on the source side.",
                "cite_spans": [
                    {
                        "start": 143,
                        "end": 157,
                        "text": "(Cherry, 2008)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 162,
                        "end": 187,
                        "text": "(Marton and Resnik, 2008)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 320,
                        "end": 340,
                        "text": "(Xiong et al., 2009)",
                        "ref_id": "BIBREF22"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "6"
            },
            {
                "text": "In addition, hard linguistic constraints are also explored. (Wu and Ng, 1995) employs syntactic bracketing information to constrain search in order to improve speed and accuracy. (Collins et al., 2005) and (Wang et al., 2007) use hard syntactic constraints to perform reorderings according to source-side parse trees. (Xiong et al., 2008) prohibit any swappings which violate punctuation based constraints.",
                "cite_spans": [
                    {
                        "start": 60,
                        "end": 77,
                        "text": "(Wu and Ng, 1995)",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 179,
                        "end": 201,
                        "text": "(Collins et al., 2005)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 206,
                        "end": 225,
                        "text": "(Wang et al., 2007)",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 318,
                        "end": 338,
                        "text": "(Xiong et al., 2008)",
                        "ref_id": "BIBREF21"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "6"
            },
            {
                "text": "Non-linguistic constraints are also widely used in phrase-based decoding. The IBM and ITG constraints (Zens et al., 2004) are used to restrict reorderings in practical phrase-based systems. (Berger et al., 1996) introduces the concept of rift into a machine translation system, which is similar to our definition of translation boundary. They also use a maximum entropy model to predict whether a source position is a rift based on features only from source sentences. Our work differs from (Berger et al., 1996) 3) The last difference is how segment boundaries are integrated into a machine translation system. Berger et al. use predicted rifts to divide a long source sentence into a series of smaller segments, which are then translated sequentially in order to increase decoding speed (Brown et al., 1992; Berger et al., 1996) . This can be considered as a hard integration, which may undermine translation accuracy given wrongly predicted rifts. We integrate predicted translation boundaries into phrase-based decoding in a soft manner, which improves translation accuracy in terms of BLEU score.",
                "cite_spans": [
                    {
                        "start": 102,
                        "end": 121,
                        "text": "(Zens et al., 2004)",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 190,
                        "end": 211,
                        "text": "(Berger et al., 1996)",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 491,
                        "end": 512,
                        "text": "(Berger et al., 1996)",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 789,
                        "end": 809,
                        "text": "(Brown et al., 1992;",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 810,
                        "end": 830,
                        "text": "Berger et al., 1996)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "6"
            },
            {
                "text": "In this paper, we have presented a simple approach to learn translation boundaries on source sentences. The learned translation boundaries are used to constrain phrase-based decoding in a soft manner. The whole approach has several properties.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion and Future Work",
                "sec_num": "7"
            },
            {
                "text": "\u2022 First, it is based on a simple classification task that can achieve considerably high accuracy when taking translation divergences into account using simple models and features.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion and Future Work",
                "sec_num": "7"
            },
            {
                "text": "\u2022 Second, the classifier output can be straightforwardly used to constrain phrase-based decoder.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion and Future Work",
                "sec_num": "7"
            },
            {
                "text": "\u2022 Finally, we have empirically shown that, to build soft constraints for phrase-based decoding, translation boundary predicted by our classifier is a better choice than constituent boundary deduced from source-side parse tree.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion and Future Work",
                "sec_num": "7"
            },
            {
                "text": "Future work in this direction will involve trying different methods to define more informative translation boundaries, such as a boundary to begin/end a swapping. We would also like to investigate new methods to incorporate automatically learned translation boundaries more efficiently into decoding in an attempt to further improve search in both speed and accuracy.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion and Future Work",
                "sec_num": "7"
            },
            {
                "text": "We will give a formal definition of translation zone in Sec-",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "tion 2.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "In this paper, we use c to denote the source language and e the target language.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "A Maximum Entropy Approach to Natural Language Processing",
                "authors": [
                    {
                        "first": "Adam",
                        "middle": [
                            "L"
                        ],
                        "last": "Berger",
                        "suffix": ""
                    },
                    {
                        "first": "Stephen",
                        "middle": [
                            "A"
                        ],
                        "last": "Della Pietra",
                        "suffix": ""
                    },
                    {
                        "first": "Vincent",
                        "middle": [
                            "J"
                        ],
                        "last": "Della",
                        "suffix": ""
                    },
                    {
                        "first": "Pietra",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 1996,
                "venue": "Computational Linguistics",
                "volume": "22",
                "issue": "1",
                "pages": "39--71",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Adam L. Berger, Stephen A. Della Pietra and Vincent J. Della Pietra. 1996. A Maximum Entropy Approach to Natural Language Processing. Computational Lin- guistics, 22(1):39-71.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Dividing and Conquering Long Sentences in a Translation System",
                "authors": [
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Peter",
                        "suffix": ""
                    },
                    {
                        "first": "Stephen",
                        "middle": [
                            "A"
                        ],
                        "last": "Brown",
                        "suffix": ""
                    },
                    {
                        "first": "Vincent",
                        "middle": [
                            "J"
                        ],
                        "last": "Della Pietra",
                        "suffix": ""
                    },
                    {
                        "first": "Robert",
                        "middle": [
                            "L"
                        ],
                        "last": "Della Pietra",
                        "suffix": ""
                    },
                    {
                        "first": "Surya",
                        "middle": [],
                        "last": "Mercer",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Mohanty",
                        "suffix": ""
                    }
                ],
                "year": 1992,
                "venue": "Proceedings of the workshop on Speech and Natural Language, Human Language Technology",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, Robert L. Mercer, and Surya Mohanty. 1992. Dividing and Conquering Long Sentences in a Translation System. In Proceedings of the workshop on Speech and Natural Language, Human Language Technology.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Cohesive Phrase-based Decoding for Statistical Machine Translation",
                "authors": [
                    {
                        "first": "Colin",
                        "middle": [],
                        "last": "Cherry",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Proceedings of ACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Colin Cherry. 2008. Cohesive Phrase-based Decoding for Statistical Machine Translation. In Proceedings of ACL.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "A Hierarchical Phrase-based Model for Statistical Machine Translation",
                "authors": [
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Chiang",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of ACL",
                "volume": "",
                "issue": "",
                "pages": "263--270",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "David Chiang. 2005. A Hierarchical Phrase-based Model for Statistical Machine Translation. In Pro- ceedings of ACL, pages 263-270.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Clause Restructuring for Statistical Machine Translation",
                "authors": [
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Collins",
                        "suffix": ""
                    },
                    {
                        "first": "Philipp",
                        "middle": [],
                        "last": "Koehn",
                        "suffix": ""
                    },
                    {
                        "first": "Ivona",
                        "middle": [],
                        "last": "Kucerova",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of ACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Michael Collins, Philipp Koehn and Ivona Kucerova. 2005. Clause Restructuring for Statistical Machine Translation. In Proceedings of ACL.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Decoding Complexity in Word Replacement Translation Models",
                "authors": [
                    {
                        "first": "Kevin",
                        "middle": [],
                        "last": "Knight",
                        "suffix": ""
                    }
                ],
                "year": 1999,
                "venue": "Computational Linguistics",
                "volume": "25",
                "issue": "4",
                "pages": "607--615",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kevin Knight. 1999. Decoding Complexity in Word Re- placement Translation Models. In Computational Lin- guistics, 25(4):607 -615.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Statistical Phrase-based Translation",
                "authors": [
                    {
                        "first": "Philipp",
                        "middle": [],
                        "last": "Koehn",
                        "suffix": ""
                    },
                    {
                        "first": "Franz",
                        "middle": [],
                        "last": "Joseph Och",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Marcu",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Proceedings of HLT-NAACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Philipp Koehn, Franz Joseph Och, and Daniel Marcu. 2003. Statistical Phrase-based Translation. In Pro- ceedings of HLT-NAACL.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Statistical Significance Tests for Machine Translation Evaluation",
                "authors": [
                    {
                        "first": "Philipp",
                        "middle": [],
                        "last": "Koehn",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proceedings of EMNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Philipp Koehn. 2004. Statistical Significance Tests for Machine Translation Evaluation. In Proceedings of EMNLP.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Edinburgh System Description for the 2005 IWSLT Speech Translation Evaluation",
                "authors": [
                    {
                        "first": "Philipp",
                        "middle": [],
                        "last": "Koehn",
                        "suffix": ""
                    },
                    {
                        "first": "Amittai",
                        "middle": [],
                        "last": "Axelrod",
                        "suffix": ""
                    },
                    {
                        "first": "Alexandra",
                        "middle": [
                            "Birch"
                        ],
                        "last": "Mayne",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Callison-Burch",
                        "suffix": ""
                    },
                    {
                        "first": "Miles",
                        "middle": [],
                        "last": "Osborne",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Talbot",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of IWSLT",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, Chris Callison-Burch, Miles Osborne and David Talbot. 2005. Edinburgh System Description for the 2005 IWSLT Speech Translation Evaluation. In Proceedings of IWSLT.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Soft Syntactic Constraints for Hierarchical Phrase-Based Translation",
                "authors": [
                    {
                        "first": "Yuval",
                        "middle": [],
                        "last": "Marton",
                        "suffix": ""
                    },
                    {
                        "first": "Philip",
                        "middle": [],
                        "last": "Resnik",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Proceedings of ACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yuval Marton and Philip Resnik. 2008. Soft Syntactic Constraints for Hierarchical Phrase-Based Translation. In Proceedings of ACL.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Maximum Entropy Markov Models for Information Extraction and Segmentation",
                "authors": [
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Mccallum",
                        "suffix": ""
                    },
                    {
                        "first": "Dayne",
                        "middle": [],
                        "last": "Freitag",
                        "suffix": ""
                    },
                    {
                        "first": "Fernando",
                        "middle": [],
                        "last": "Pereira",
                        "suffix": ""
                    }
                ],
                "year": 2000,
                "venue": "Proceedings of the Seventeenth International Conference on Machine Learning",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Andrew McCallum, Dayne Freitag and Fernando Pereira 2000. Maximum Entropy Markov Models for Infor- mation Extraction and Segmentation. In Proceedings of the Seventeenth International Conference on Ma- chine Learning 2000.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Improved Statistical Alignment Models",
                "authors": [
                    {
                        "first": "Josef",
                        "middle": [],
                        "last": "Franz",
                        "suffix": ""
                    },
                    {
                        "first": "Hermann",
                        "middle": [],
                        "last": "Och",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Ney",
                        "suffix": ""
                    }
                ],
                "year": 2000,
                "venue": "Proceedings of ACL 2000",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Franz Josef Och and Hermann Ney. 2000. Improved Statistical Alignment Models. In Proceedings of ACL 2000.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Minimum Error Rate Training in Statistical Machine Translation",
                "authors": [
                    {
                        "first": "Josef",
                        "middle": [],
                        "last": "Franz",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Och",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Proceedings of ACL 2003",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Franz Josef Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Proceedings of ACL 2003.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "BLEU: a Method for Automatically Evaluation of Machine Translation",
                "authors": [
                    {
                        "first": "Kishore",
                        "middle": [],
                        "last": "Papineni",
                        "suffix": ""
                    },
                    {
                        "first": "Salim",
                        "middle": [],
                        "last": "Roukos",
                        "suffix": ""
                    },
                    {
                        "first": "Todd",
                        "middle": [],
                        "last": "Ward",
                        "suffix": ""
                    },
                    {
                        "first": "Wei-Jing",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Proceedings of ACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kishore Papineni, Salim Roukos, Todd Ward and Wei- Jing Zhu. 2002. BLEU: a Method for Automatically Evaluation of Machine Translation. In Proceedings of ACL 2002.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Classifying Chart Cells for Quadratic Complexity Context-Free Inference",
                "authors": [
                    {
                        "first": "Brian",
                        "middle": [],
                        "last": "Roark",
                        "suffix": ""
                    },
                    {
                        "first": "Kristy",
                        "middle": [],
                        "last": "Hollingshead",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Proceedings of COLING",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Brian Roark and Kristy Hollingshead. 2008. Classifying Chart Cells for Quadratic Complexity Context-Free In- ference. In Proceedings of COLING 2008.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "SRILM -an Extensible Language Modeling Toolkit",
                "authors": [
                    {
                        "first": "Andreas",
                        "middle": [],
                        "last": "Stolcke",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Proceedings of International Conference on Spoken Language Processing",
                "volume": "2",
                "issue": "",
                "pages": "901--904",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Andreas Stolcke. 2002. SRILM -an Extensible Lan- guage Modeling Toolkit. In Proceedings of Interna- tional Conference on Spoken Language Processing, volume 2, pages 901-904.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Chinese Syntactic Reordering for Statistical Machine Translation",
                "authors": [
                    {
                        "first": "Chao",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Collins",
                        "suffix": ""
                    },
                    {
                        "first": "Philipp",
                        "middle": [],
                        "last": "Koehn",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Proceedings of EMNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chao Wang, Michael Collins and Philipp Koehn 2007. Chinese Syntactic Reordering for Statistical Machine Translation. In Proceedings of EMNLP.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Using Brackets to Improve Search for Statistical Machine Translation",
                "authors": [
                    {
                        "first": "Dekai",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Cindy",
                        "middle": [],
                        "last": "Ng",
                        "suffix": ""
                    }
                ],
                "year": 1995,
                "venue": "Proceedings of PACLIC-IO, Pacific Asia Conference on Language, Information and Computation",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dekai Wu and Cindy Ng. 1995. Using Brackets to Im- prove Search for Statistical Machine Translation In Proceedings of PACLIC-IO, Pacific Asia Conference on Language, Information and Computation.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora",
                "authors": [
                    {
                        "first": "Dekai",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    }
                ],
                "year": 1997,
                "venue": "Computational Linguistics",
                "volume": "23",
                "issue": "3",
                "pages": "377--403",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dekai Wu. 1997. Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora. Computational Linguistics, 23(3):377-403.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Parsing the Penn Chinese Treebank with Semantic Knowledge",
                "authors": [
                    {
                        "first": "Deyi",
                        "middle": [],
                        "last": "Xiong",
                        "suffix": ""
                    },
                    {
                        "first": "Shuanglong",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Qun",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Shouxun",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Yueliang",
                        "middle": [],
                        "last": "Qian",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of IJCNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin, Yueliang Qian. 2005. Parsing the Penn Chinese Tree- bank with Semantic Knowledge. In Proceedings of IJCNLP, Jeju Island, Korea.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Maximum Entropy Based Phrase Reordering Model for Statistical Machine Translation",
                "authors": [
                    {
                        "first": "Deyi",
                        "middle": [],
                        "last": "Xiong",
                        "suffix": ""
                    },
                    {
                        "first": "Qun",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Shouxun",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Proceedings of ACL-COLING",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Deyi Xiong, Qun Liu and Shouxun Lin. 2006. Maxi- mum Entropy Based Phrase Reordering Model for Sta- tistical Machine Translation. In Proceedings of ACL- COLING 2006.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Refinements in BTG-based Statistical Machine Translation",
                "authors": [
                    {
                        "first": "Deyi",
                        "middle": [],
                        "last": "Xiong",
                        "suffix": ""
                    },
                    {
                        "first": "Min",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Ai",
                        "middle": [],
                        "last": "Ti Aw",
                        "suffix": ""
                    },
                    {
                        "first": "Haitao",
                        "middle": [],
                        "last": "Mi",
                        "suffix": ""
                    },
                    {
                        "first": "Qun",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Shouxun",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Proceedings of IJCNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Deyi Xiong, Min Zhang, Ai Ti Aw, Haitao Mi, Qun Liu and Shouxun Lin. 2008. Refinements in BTG-based Statistical Machine Translation. In Proceedings of IJCNLP 2008.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "A Syntax-Driven Bracketing Model for Phrase-Based Translation",
                "authors": [
                    {
                        "first": "Deyi",
                        "middle": [],
                        "last": "Xiong",
                        "suffix": ""
                    },
                    {
                        "first": "Min",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Ai",
                        "middle": [],
                        "last": "Ti Aw",
                        "suffix": ""
                    },
                    {
                        "first": "Haizhou",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of ACL-IJCNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Deyi Xiong, Min Zhang, Ai Ti Aw, and Haizhou Li. 2009. A Syntax-Driven Bracketing Model for Phrase- Based Translation. In Proceedings of ACL-IJCNLP 2009.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Reordering Constraints for Phrase-Based Statistical Machine Translation",
                "authors": [
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Zens",
                        "suffix": ""
                    },
                    {
                        "first": "Hermann",
                        "middle": [],
                        "last": "Ney",
                        "suffix": ""
                    },
                    {
                        "first": "Taro",
                        "middle": [],
                        "last": "Watanabe",
                        "suffix": ""
                    },
                    {
                        "first": "Eiichiro",
                        "middle": [],
                        "last": "Sumita",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proceedings of COLING",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Richard Zens, Hermann Ney, Taro Watanabe and Eiichiro Sumita 2004. Reordering Constraints for Phrase- Based Statistical Machine Translation. In Proceedings of COLING.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Extracting Synchronous Grammars Rules from Word-Level Alignments in Linear Time",
                "authors": [
                    {
                        "first": "Hao",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Gildea",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Chiang",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Proceeding of COLING",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Hao Zhang, Daniel Gildea, and David Chiang. 2008. Extracting Synchronous Grammars Rules from Word- Level Alignments in Linear Time. In Proceeding of COLING 2008.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Maximum Entropy Modeling Tooklkit for Python and C++",
                "authors": [
                    {
                        "first": "Le",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Le Zhang. 2004. Maximum Entropy Model- ing Tooklkit for Python and C++. Available at http://homepages.inf.ed.ac.uk/s0450736 /maxent toolkit.html.",
                "links": null
            }
        },
        "ref_entries": {
            "TABREF0": {
                "content": "<table><tr><td>Sentences</td><td>3.8</td><td>-</td></tr><tr><td>Words</td><td>96.9</td><td>-</td></tr><tr><td>Words \u2208 B y</td><td>22.7</td><td>23.4</td></tr><tr><td>Words \u2208 E y</td><td>41.0</td><td>42.3</td></tr><tr><td>Words / \u2208 B y and / \u2208 E y</td><td>33.2</td><td>34.3</td></tr></table>",
                "type_str": "table",
                "text": "Statistics on word classes from our bilingual training data. All numbers are calculated on the source side. P means the percentage.",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table><tr><td/><td colspan=\"2\">Avg. Accuracy (%)</td></tr><tr><td colspan=\"3\">Classification Task MEMM 1 MEMM 2</td></tr><tr><td>B y /B n</td><td>71.7</td><td>70.2</td></tr><tr><td>E y /E n</td><td>59.2</td><td>58.8</td></tr></table>",
                "type_str": "table",
                "text": "Average classification accuracy on the development set for our MEMM based translation boundary classifiers with various Markov orders. tion 2. The average classification accuracy results are shown in Table3.Comparing Table3with Table2, we find that our MEMM based classifiers significantly outperform constituent boundary deducer in predicting translation boundaries, especially in the B y /B n classification task, where our MEMM based B y /B n classifier (Markov order 1) achieves a relative increase of 52.9% in accuracy over the constituent boundary deducer. In the E y /E n classification task, our classifiers also perform much better than constituent boundary deducer.Then are our MEMM based translation boundary classifiers good enough? The accuracies are still low although they are higher than those of constituent boundary deducer. One reason why we have low accuracies is that our gold standard based evaluation is not established on real gold standards. In other words, we don't have gold standards in terms of translation boundary since different translations",
                "html": null,
                "num": null
            },
            "TABREF4": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Average classification accuracy on the development set when treating each reference translation set as a boundary classifier.",
                "html": null,
                "num": null
            },
            "TABREF5": {
                "content": "<table><tr><td>. We add a new feature to the decoder's log-</td></tr><tr><td>linear model: translation boundary violation count-</td></tr><tr><td>ing feature. This counting feature accumulates</td></tr><tr><td>whenever hypotheses have a partial translation span-</td></tr><tr><td>ning c</td></tr></table>",
                "type_str": "table",
                "text": "Training corpora.",
                "html": null,
                "num": null
            },
            "TABREF6": {
                "content": "<table><tr><td colspan=\"2\">System BLEU-4 (%)</td></tr><tr><td>Base</td><td>33.05</td></tr><tr><td>Ref1</td><td>33.99*</td></tr><tr><td>Ref2</td><td>34.17*</td></tr><tr><td>Ref3</td><td>33.93*</td></tr><tr><td>Ref4</td><td>34.21*</td></tr><tr><td colspan=\"2\">Table 6: Results of using translation boundaries obtained</td></tr><tr><td colspan=\"2\">from reference translations. *: significantly better than</td></tr><tr><td>baseline (p &lt; 0.01).</td><td/></tr></table>",
                "type_str": "table",
                "text": "Table 6 shows the results. We clearly see that us-",
                "html": null,
                "num": null
            },
            "TABREF7": {
                "content": "<table><tr><td>System</td><td>BLEU-4 (%)</td></tr><tr><td>Base</td><td>33.05</td></tr><tr><td colspan=\"2\">Condeducer 33.18</td></tr><tr><td>XP+</td><td>33.58*</td></tr><tr><td>BestRef</td><td>34.21*+</td></tr><tr><td>MEMM 1</td><td>33.70*</td></tr><tr><td>MEMM 2</td><td>34.04*+</td></tr><tr><td colspan=\"2\">Table 7: Results of using automatically learned trans-</td></tr><tr><td colspan=\"2\">lation boundaries. Condeducer means using pure con-</td></tr><tr><td colspan=\"2\">stituent boundary based soft constraint. XP+ is another</td></tr><tr><td colspan=\"2\">constituent boundary based soft constraint but with dis-</td></tr><tr><td colspan=\"2\">tinction among special constituent types (Marton and</td></tr><tr><td colspan=\"2\">Resnik, 2008). BestRef is the best result using reference</td></tr><tr><td colspan=\"2\">translation boundaries in Table 6. MEMM 1 and MEMM</td></tr><tr><td colspan=\"2\">2 are our MEMM based translation boundary classifiers</td></tr><tr><td colspan=\"2\">with Markov order 1 and 2. *: significantly better than</td></tr><tr><td colspan=\"2\">baseline (p &lt; 0.01). +: significantly better than XP+</td></tr><tr><td>(p &lt; 0.01).</td><td/></tr></table>",
                "type_str": "table",
                "text": "One is the",
                "html": null,
                "num": null
            },
            "TABREF8": {
                "content": "<table/>",
                "type_str": "table",
                "text": "in three major respects. 1) We distinguish a segment boundary into two categories: beginning and ending boundary due to their different distributions (see Table 1). However, Berger et al. ignore this difference. 2) We train two classifiers to predict beginning and ending boundary respectively while Berger et al. build only one classifier. Our experiments show that two separate classifiers outperform one classifier.",
                "html": null,
                "num": null
            }
        }
    }
}