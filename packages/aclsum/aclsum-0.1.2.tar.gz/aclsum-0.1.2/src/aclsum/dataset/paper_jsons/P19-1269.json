{
    "paper_id": "P19-1269",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:36:15.613654Z"
    },
    "title": "Putting Evaluation in Context: Contextual Embeddings improve Machine Translation Evaluation",
    "authors": [
        {
            "first": "Nitika",
            "middle": [],
            "last": "Mathur",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "The University of Melbourne",
                "location": {
                    "postCode": "3010",
                    "settlement": "Victoria",
                    "country": "Australia"
                }
            },
            "email": "nmathur@student.unimelb.edu.au"
        },
        {
            "first": "Timothy",
            "middle": [],
            "last": "Baldwin",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "The University of Melbourne",
                "location": {
                    "postCode": "3010",
                    "settlement": "Victoria",
                    "country": "Australia"
                }
            },
            "email": "tbaldwin@unimelb.edu.au"
        },
        {
            "first": "Trevor",
            "middle": [],
            "last": "Cohn",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "The University of Melbourne",
                "location": {
                    "postCode": "3010",
                    "settlement": "Victoria",
                    "country": "Australia"
                }
            },
            "email": "tcohn@unimelb.edu.au"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Accurate, automatic evaluation of machine translation is critical for system tuning, and evaluating progress in the field. We proposed a simple unsupervised metric, and additional supervised metrics which rely on contextual word embeddings to encode the translation and reference sentences. We find that these models rival or surpass all existing metrics in the WMT 2017 sentence-level and systemlevel tracks, and our trained model has a substantially higher correlation with human judgements than all existing metrics on the WMT 2017 to-English sentence level dataset.",
    "pdf_parse": {
        "paper_id": "P19-1269",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Accurate, automatic evaluation of machine translation is critical for system tuning, and evaluating progress in the field. We proposed a simple unsupervised metric, and additional supervised metrics which rely on contextual word embeddings to encode the translation and reference sentences. We find that these models rival or surpass all existing metrics in the WMT 2017 sentence-level and systemlevel tracks, and our trained model has a substantially higher correlation with human judgements than all existing metrics on the WMT 2017 to-English sentence level dataset.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Evaluation metrics are a fundamental component of machine translation (MT) and other language generation tasks. The problem of assessing whether a translation is both adequate and coherent is a challenging text analysis problem, which is still unsolved, despite many years of effort by the research community. Shallow surfacelevel metrics, such as BLEU and TER (Papineni et al., 2002; Snover et al., 2006) , still predominate in practice, due in part to their reasonable correlation to human judgements, and their being parameter free, making them easily portable to new languages. In contrast, trained metrics (Song and Cohn, 2011; Stanojevic and Sima'an, 2014; Ma et al., 2017; Shimanaka et al., 2018) , which are learned to match human evaluation data, have been shown to result in a large boost in performance.",
                "cite_spans": [
                    {
                        "start": 361,
                        "end": 384,
                        "text": "(Papineni et al., 2002;",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 385,
                        "end": 405,
                        "text": "Snover et al., 2006)",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 611,
                        "end": 632,
                        "text": "(Song and Cohn, 2011;",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 633,
                        "end": 662,
                        "text": "Stanojevic and Sima'an, 2014;",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 663,
                        "end": 679,
                        "text": "Ma et al., 2017;",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 680,
                        "end": 703,
                        "text": "Shimanaka et al., 2018)",
                        "ref_id": "BIBREF25"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "This paper aims to improve over existing MT evaluation methods, through developing a series of new metrics based on contextual word embeddings (Peters et al., 2018; Devlin et al., 2019) , a technique which captures rich and portable representations of words in context, which have been shown to provide important signal to many other NLP tasks (Rajpurkar et al., 2018) . We propose a simple untrained model that uses off-the-shelf contextual embeddings to compute approximate recall, when comparing a reference to an automatic translation, as well as trained models, including: a recurrent model over reference and translation sequences, incorporating attention; and the adaptation of an NLI method (Chen et al., 2017) to MT evaluation. These approaches, though simple in formulation, are highly effective, and rival or surpass the best approaches from WMT 2017. Moreover, we show further improvements in performance when our trained models are learned using noisy crowd-sourced data, i.e., having single annotations for more instances is better than collecting and aggregating multiple annotations for single instances. The net result is an approach that is more data efficient than existing methods, while producing substantially better human correlations.1 ",
                "cite_spans": [
                    {
                        "start": 143,
                        "end": 164,
                        "text": "(Peters et al., 2018;",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 165,
                        "end": 185,
                        "text": "Devlin et al., 2019)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 344,
                        "end": 368,
                        "text": "(Rajpurkar et al., 2018)",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 699,
                        "end": 718,
                        "text": "(Chen et al., 2017)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "MT metrics attempt to automatically predict the quality of a translation by comparing it to a reference translation of the same source sentence.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related work",
                "sec_num": "2"
            },
            {
                "text": "Metrics such as BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) use n-gram matching or more explicit word alignment to match the system output with the reference translation. Character-level variants such as BEER, CHRF and CHARACTER overcome the problem of harshly penalising morphological variants, and perform surprisingly well despite their simplicity (Stanojevic and Sima'an, 2014; Popovi\u0107, 2015; Wang et al., 2016) .",
                "cite_spans": [
                    {
                        "start": 21,
                        "end": 44,
                        "text": "(Papineni et al., 2002)",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 53,
                        "end": 74,
                        "text": "(Snover et al., 2006)",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 366,
                        "end": 396,
                        "text": "(Stanojevic and Sima'an, 2014;",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 397,
                        "end": 411,
                        "text": "Popovi\u0107, 2015;",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 412,
                        "end": 430,
                        "text": "Wang et al., 2016)",
                        "ref_id": "BIBREF29"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related work",
                "sec_num": "2"
            },
            {
                "text": "In order to allow for variation in word choice and sentence structure, other metrics use information from shallow linguistic tools such as POStaggers, lemmatizers and synonym dictionaries (Banerjee and Lavie, 2005; Snover et al., 2006; Liu et al., 2010) , or deeper linguistic informa-tion such as semantic roles, dependency relationships, syntactic constituents, and discourse roles (Gim\u00e9nez and M\u00e0rquez, 2007; Castillo and Estrella, 2012; Guzm\u00e1n et al., 2014) . On the flip side, it is likely that these are too permissive of mistakes.",
                "cite_spans": [
                    {
                        "start": 188,
                        "end": 214,
                        "text": "(Banerjee and Lavie, 2005;",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 215,
                        "end": 235,
                        "text": "Snover et al., 2006;",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 236,
                        "end": 253,
                        "text": "Liu et al., 2010)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 384,
                        "end": 411,
                        "text": "(Gim\u00e9nez and M\u00e0rquez, 2007;",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 412,
                        "end": 440,
                        "text": "Castillo and Estrella, 2012;",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 441,
                        "end": 461,
                        "text": "Guzm\u00e1n et al., 2014)",
                        "ref_id": "BIBREF13"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related work",
                "sec_num": "2"
            },
            {
                "text": "More recently, metrics such as MEANT 2.0 (Lo, 2017 ) have adopted word embeddings (Mikolov et al., 2013) to capture the semantics of individual words. However, classic word embeddings are independent of word context, and context is captured instead using hand-crafted features or heuristics.",
                "cite_spans": [
                    {
                        "start": 41,
                        "end": 50,
                        "text": "(Lo, 2017",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 82,
                        "end": 104,
                        "text": "(Mikolov et al., 2013)",
                        "ref_id": "BIBREF17"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related work",
                "sec_num": "2"
            },
            {
                "text": "Neural metrics such as ReVal and RUSE solve this problem by directly learning embeddings of the entire translation and reference sentences. ReVal (Gupta et al., 2015) learns sentence representations of the MT output and reference translation as a Tree-LSTM, and then models their interactions using the element-wise difference and angle between the two. RUSE (Shimanaka et al., 2018) has a similar architecture, but it uses pretrained sentence representations instead of learning the sentence representations from the data.",
                "cite_spans": [
                    {
                        "start": 146,
                        "end": 166,
                        "text": "(Gupta et al., 2015)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 359,
                        "end": 383,
                        "text": "(Shimanaka et al., 2018)",
                        "ref_id": "BIBREF25"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related work",
                "sec_num": "2"
            },
            {
                "text": "The Natural Language Inference (NLI) task is similar to MT evaluation (Pad\u00f3 et al., 2009) : a good translation entails the reference and viceversa. An irrelevant/wrong translation would be neutral/contradictory compared to the reference. An additional complexity is that MT outputs are not always fluent. On the NLI datasets, systems that include pairwise word interactions when learning sentence representations have a higher accuracy than systems that process the two sentences independently (Rockt\u00e4schel et al., 2016; Chen et al., 2017; Wang et al., 2017) . In this paper, we attempt to introduce this idea to neural MT metrics.",
                "cite_spans": [
                    {
                        "start": 70,
                        "end": 89,
                        "text": "(Pad\u00f3 et al., 2009)",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 494,
                        "end": 520,
                        "text": "(Rockt\u00e4schel et al., 2016;",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 521,
                        "end": 539,
                        "text": "Chen et al., 2017;",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 540,
                        "end": 558,
                        "text": "Wang et al., 2017)",
                        "ref_id": "BIBREF30"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related work",
                "sec_num": "2"
            },
            {
                "text": "We wish to predict the score of a translation t of length l t against a human reference r of length l r . For all models, we use fixed pre-trained contextualised word embeddings e k to represent each word in the MT output and reference translation, in the form of matrices W t and W r .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model",
                "sec_num": "3"
            },
            {
                "text": "We use cosine similarity to measure the pairwise similarity between t and r based on the maximum similarity score for each word embedding e i \u2208 t with respect to each word embedding e j \u2208 r. We approximate recall of a word in r with its maximum similarity with any word in t. The final predicted score, y, for a translation is the average recall of its reference:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Unsupervised Model",
                "sec_num": "3.1"
            },
            {
                "text": "recall j = lt max i=1 cosine(e i , e j )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Unsupervised Model",
                "sec_num": "3.1"
            },
            {
                "text": "(1)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Unsupervised Model",
                "sec_num": "3.1"
            },
            {
                "text": "y = lr j=1 recall j l r (2)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Unsupervised Model",
                "sec_num": "3.1"
            },
            {
                "text": "Trained BiLSTM We first encode the embeddings of the translation and reference with a bidirectional LSTM, and concatenate the max-pooled and average-pooled hidden states of the BiLSTM to generate v t and v r , respectively:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Supervised Models",
                "sec_num": "3.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "v s,max = ls max k=1 h s,k , v s,avg = ls k=1 h s,k l s (3) v s = [v s,max ; v s,avg ]",
                        "eq_num": "(4)"
                    }
                ],
                "section": "Supervised Models",
                "sec_num": "3.2"
            },
            {
                "text": "To get the predicted score, we run a feedforward network over the concatenation of the sentence representations of t and r, and their element-wise product and difference (a useful heuristic first proposed by Mou et al. (2016) ). We train the model by minimizing mean squared error with respect to human scores.",
                "cite_spans": [
                    {
                        "start": 208,
                        "end": 225,
                        "text": "Mou et al. (2016)",
                        "ref_id": "BIBREF18"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Supervised Models",
                "sec_num": "3.2"
            },
            {
                "text": "m = [v t ; v r ; v t v r ; v t -v r ] (5) y = w ReLU(W m + b) + b (6)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Supervised Models",
                "sec_num": "3.2"
            },
            {
                "text": "This is similar to RUSE, except that we learn the sentence representation instead of using pretrained sentence embeddings.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Supervised Models",
                "sec_num": "3.2"
            },
            {
                "text": "Trained BiLSTM + attention To obtain a sentence representation of the translation which is conditioned on the reference, we compute the attention-weighted representation of each word in the translation. The attention weights are obtained by running a softmax over the dot product similarity between the hidden state of the translation and reference BiLSTM. Similarly, we compute the relevant representation of the reference:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Supervised Models",
                "sec_num": "3.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "a i,j = h r i h tj (7) hr = lt j=1 exp(a i,j ) \u03a3 i exp(a i,j ) \u2022 h t (8) ht = lr i=1 exp(a i,j ) \u03a3 j exp(a i,j ) \u2022 h r",
                        "eq_num": "(9)"
                    }
                ],
                "section": "Supervised Models",
                "sec_num": "3.2"
            },
            {
                "text": "We then use ht and hr as our sentence representations in Eq. ( 3)-( 6) to compute the final scores.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Supervised Models",
                "sec_num": "3.2"
            },
            {
                "text": "We also directly adapt ESIM (Chen et al., 2017) , a high-performing model on the Natural Language Inference task, to the MT evaluation setting. We treat the human reference translation and the MT output as the premise and hypothesis, respectively.",
                "cite_spans": [
                    {
                        "start": 28,
                        "end": 47,
                        "text": "(Chen et al., 2017)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Enhanced Sequential Inference Model (ESIM):",
                "sec_num": null
            },
            {
                "text": "The ESIM model first encodes r and t with a BiLSTM, then computes the attention-weighted representations of each with respect to the other (Eq. ( 7)-( 9)). This model next \"enhances\" the representations of the translation (and reference) by capturing the interactions between h t and ht (and h r and hr ):",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Enhanced Sequential Inference Model (ESIM):",
                "sec_num": null
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "m r = [h r ; hr ; h r hr ; h r -hr ] (",
                        "eq_num": "10"
                    }
                ],
                "section": "Enhanced Sequential Inference Model (ESIM):",
                "sec_num": null
            },
            {
                "text": ")",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Enhanced Sequential Inference Model (ESIM):",
                "sec_num": null
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "m t = [h t ; ht ; h t ht ; h t -ht ]",
                        "eq_num": "(11)"
                    }
                ],
                "section": "Enhanced Sequential Inference Model (ESIM):",
                "sec_num": null
            },
            {
                "text": "We use a feedforward projection layer to project these representations back to the model dimension, and then run a BiLSTM over each representation to compose local sequential information. The final representation of each pair of reference and translation sentences is the concatenation of the average-pooled and max-pooled hidden states of this BiLSTM. To compute the final predicted score, we apply a feedforward regressor over the concatenation of the two sentence representations.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Enhanced Sequential Inference Model (ESIM):",
                "sec_num": null
            },
            {
                "text": "p = [v r,avg ; v r,max ; v t,avg ; v t,max ] (12) y = w ReLU(W p + b) + b (13)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Enhanced Sequential Inference Model (ESIM):",
                "sec_num": null
            },
            {
                "text": "For all models, the predicted score of an MT system is the average predicted score of all its translations in the testset.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Enhanced Sequential Inference Model (ESIM):",
                "sec_num": null
            },
            {
                "text": "We use human evaluation data from the Conference on Machine Translation (WMT) to train and evaluate our models (Bojar et al., 2016 (Bojar et al., , 2017a)) , which is based on the Direct Assessment (\"DA\") method (Graham et al., 2015 (Graham et al., , 2017 ). Here, system translations are evaluated by humans in comparison to a human reference translation, using a continuous scale (Graham et al., 2015 (Graham et al., , 2017)) . Each annotator assesses a set of 100 items, of which 30 items are for quality control, which is used to filter out annotators who are unskilled or careless. Individual worker scores are first standardised, and then the final score of an MT system is computed as the average score across all translations in the test set. Manual MT evaluation is subjective and difficult, and it is not possible even for a diligent human to be entirely consistent on a continuous scale. Thus, any human annotations are noisy by nature. To obtain an accurate score for individual translations, the average score is calculated from scores of at least 15 \"good\" annotators. This data is then used to evaluate automatic metrics at the sentence level (Graham et al., 2015) .",
                "cite_spans": [
                    {
                        "start": 111,
                        "end": 130,
                        "text": "(Bojar et al., 2016",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 131,
                        "end": 155,
                        "text": "(Bojar et al., , 2017a))",
                        "ref_id": null
                    },
                    {
                        "start": 212,
                        "end": 232,
                        "text": "(Graham et al., 2015",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 233,
                        "end": 255,
                        "text": "(Graham et al., , 2017",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 382,
                        "end": 402,
                        "text": "(Graham et al., 2015",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 403,
                        "end": 427,
                        "text": "(Graham et al., , 2017))",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 1158,
                        "end": 1179,
                        "text": "(Graham et al., 2015)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Setup",
                "sec_num": "4"
            },
            {
                "text": "We train on the human evaluation data of news domain of WMT 2016, which is entirely crowdsourced. The sentence-level-metric evaluation data consists of accurate scores for 560 translations each for 6 to-English language pairs and English-to-Russian (we call this the \"TrainS\" dataset). The dataset also includes mostly singly-annotated2 DA scores for around 125 thousand translations from six source languages into English, and 12.5 thousand translations from English-to-Russian (\"TrainL\" dataset), that were collected to obtain human scores for MT systems.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Setup",
                "sec_num": "4"
            },
            {
                "text": "For the validation set, we use the sentencelevel DA judgements collected for the WMT 2015 data (Bojar et al., 2015) : 500 translation-reference pairs each of four to-English language pairs, and English-to-Russian.",
                "cite_spans": [
                    {
                        "start": 95,
                        "end": 115,
                        "text": "(Bojar et al., 2015)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Setup",
                "sec_num": "4"
            },
            {
                "text": "For more details on implementation and training of our models, see Appendix A.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Setup",
                "sec_num": "4"
            },
            {
                "text": "We test our metrics on all language pairs from the WMT 2017 (Bojar et al., 2017b) news task in both the sentence and system level setting, and evaluate using Pearson's correlation between our metrics' predictions and the Human DA scores.",
                "cite_spans": [
                    {
                        "start": 60,
                        "end": 81,
                        "text": "(Bojar et al., 2017b)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Setup",
                "sec_num": "4"
            },
            {
                "text": "For the sentence level evaluation, insufficient DA annotations were collected for five from-English language pairs, and these were converted to preference judgements. If two MT system translations of a source sentence were evaluated by at least two reliable annotators, and the average score for System A is reasonably greater than the average score of System B, then this is interpreted as a Relative Ranking (DARR) judgement where Sys A is better than Sys B. The metrics are then evaluated using (a modified version of) Kendall's Tau correlation over these preference judgements.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Setup",
                "sec_num": "4"
            },
            {
                "text": "We also evaluate on out-of-domain, system level data for five from-English language pairs from the WMT 2016 IT task.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Setup",
                "sec_num": "4"
            },
            {
                "text": "Tab. 1 compares the performance of our proposed metrics against existing metrics on the WMT 17 to-English news dataset. MEANT 2.0 (Lo, 2017) is the best untrained metric -it uses pre-trained word2vec embeddings (Mikolov et al., 2013 )-, and RUSE (Shimanaka et al., 2018) is the best trained metric. We also include SENT-BLEU and CHRF baselines.",
                "cite_spans": [
                    {
                        "start": 130,
                        "end": 140,
                        "text": "(Lo, 2017)",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 211,
                        "end": 232,
                        "text": "(Mikolov et al., 2013",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 246,
                        "end": 270,
                        "text": "(Shimanaka et al., 2018)",
                        "ref_id": "BIBREF25"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "5"
            },
            {
                "text": "Our simple average recall metric (\"BERTR\") has a higher correlation than all existing metrics, and is highly competitive with RUSE. When trained on the sentence-level data (as with RUSE), the BiLSTM baseline does not perform well, however adding attention makes it competitive with RUSE. The ESIM model -which has many more parameters -underperforms compared to the BiLSTM model with attention.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "5"
            },
            {
                "text": "However, the performance of all models improves substantially when these metrics are trained on the larger, singly-annotated training data (denoted \"TrainL\"), i.e., using data from only those annotators who passed quality control. Clearly the additional input instances make up for the increased noise level in the prediction variable. The simple BiLSTM model performs as well as RUSE, and both the models with attention substantially outperform this benchmark.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "5"
            },
            {
                "text": "In this setting, we look at how the performance of ESIM improves as we increase the number of training instances (Fig. 1 ). We find that on the same number of training instances (3360), the model performs better on cleaner data compared to singly-annotated data (r = 0.57 vs 0.64). However, when we have a choice between collecting multiple annotations for the same instances vs collecting annotations for additional instances, the second strategy leads to more gains.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 119,
                        "end": 120,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "5"
            },
            {
                "text": "We now evaluate the unsupervised BERTR model and the ESIM model (trained on the large dataset) in the other settings. In the sentence level tasks out-of-English (Tab. 4), the BERTR model (based on BERT-Chinese) significantly outperforms all metrics in the English-to-Chinese testset. For other language pairs, BERTR (based on multilingual BERT) is highly competitive with other metrics. ESIM performs well in the language pairs that are evaluated using Pearson's cor- relation. But the results are mixed when evaluated based on preference judgements. This could be an effect of our training method -using squared error as part of regression loss -being better suited to Pearson's r -and might be resolved through a different loss, such as hinge loss over pairwise preferences which would better reflect Kendall's Tau (Stanojevic and Sima'an, 2014) . Furthermore, ESIM is trained only on to-English and to-Russian data. It is likely that including more language pairs in the training data will increase correlation.",
                "cite_spans": [
                    {
                        "start": 817,
                        "end": 847,
                        "text": "(Stanojevic and Sima'an, 2014)",
                        "ref_id": "BIBREF28"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "5"
            },
            {
                "text": "On the system level evaluation of the news domain, both metrics are competitive with all other metrics in all language pairs both to-and out-of-English (see Tab. 3 and Tab. 4 in Appendix B).",
                "cite_spans": [
                    {
                        "start": 157,
                        "end": 174,
                        "text": "Tab. 3 and Tab. 4",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "5"
            },
            {
                "text": "In the IT domain, we have mixed results (Tab. 5 in the Appendix). ESIM significantly outperforms all other metrics in English-Spanish, is competitive in two other language pairs, and is outperformed by other metrics in the remaining two language pairs.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "5"
            },
            {
                "text": "We manually inspect translations in the validation set. Tab. 6 in Appendix C shows examples of good translations, where our proposed metrics correctly recognise synonyms and valid word re-orderings, unlike SENT-BLEU. However, none of the metrics recognise a different way of expressing the same meaning. From Tab. 7, we see that SENT-BLEU gives high scores to translations with high partial overlap with the reference, but ESIM cor-cs-en de-en fi-en lv-en ru-en tr-en zh-en AVE. Pearson's r and Kendall's \u03c4 on the WMT 2017 from-English system-level evaluation data. The first section represents existing metrics, both trained and untrained. We then present results of our unsupervised metric, followed by our supervised metric trained in the TrainL setting: noisy 125k instances. Correlations of metrics not significantly outperformed by any other for that language pair are highlighted in bold (William's test (Graham and Baldwin, 2014) for Pearson's r and Bootstrap (Efron and Tibshirani, 1993) for Kendall's \u03c4 .)",
                "cite_spans": [
                    {
                        "start": 911,
                        "end": 937,
                        "text": "(Graham and Baldwin, 2014)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Qualitative Analysis",
                "sec_num": "5.1"
            },
            {
                "text": "rectly recognises then as low quality translations. However, in some cases, ESIM can be too permissive of bad translations which contain closely related words. There are also examples where a small difference in words completely changes the meaning of the sentence, but all the metrics score these translations highly.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baselines",
                "sec_num": null
            },
            {
                "text": "We show that contextual embeddings are very useful for evaluation, even in simple untrained models, as well as in deeper attention based methods. When trained on a larger, much noisier range of instances, we demonstrate a substantial improvement over the state of the art.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "6"
            },
            {
                "text": "In future work, we plan to extend these models by using cross-lingual embeddings, and combine information from translation-source interactions as well as translation-reference interactions. There are also direct applications to Quality Estimation, by using the source instead of the reference.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "6"
            },
            {
                "text": "B System-level results for WMT 17 news and WMT 2016 IT domain ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "6"
            },
            {
                "text": "code is available at https://github.com/nitikam/mtevalin-context",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "about 15% of the translations have a repeat annotation collected as part of quality-control",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "We thank the anonymous reviewers for their feedback and suggestions to improve the paper. This work was supported in part by the Australian Research Council. This research was undertaken using the LIEF HPC-GPGPU Facility hosted at the University of Melbourne.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgements",
                "sec_num": null
            },
            {
                "text": "We implement our models using AllenNLP in Py-Torch. We experimented with both ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) embeddings, and found that BERT consistently performs as well as, or better than ELMo, thus we report results using only BERT embeddings in this paper.For BERTR, we use the top layer embeddings of the wordpieces of the MT and Reference translations. We use bert base uncased for all to-English language pairs, bert base chinese models for English-to-Chinese and bert base multilingual cased for the remaining to-English language pairs. For the trained metrics, we learn a weighted average of all layers of BERT embeddings.On the to-English testsets, we use bert base uncased embeddings and train on the WMT16 to-English data.On all other testsets, we use the bert base multilingual cased embeddings and train on the WMT 2016 English-to-Russian, as well as all to-English data.Following the recommendations of the original ESIM paper, we fix the dimension of the BiLSTM hidden state to 300 and set the Dropout rate to 0.5. We use the Adam optimizer with an initial learning rate of 0.0004 and batch size of 32, and use early stopping on the validation dataset.Training the ESIM model on the full dataset takes around two hours on a single V100 GPU, and all models take less than two minutes to evaluate a standard WMT dataset of 3000 translations. ",
                "cite_spans": [
                    {
                        "start": 83,
                        "end": 104,
                        "text": "(Peters et al., 2018)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 114,
                        "end": 135,
                        "text": "(Devlin et al., 2019)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Implementation details",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments",
                "authors": [
                    {
                        "first": "Satanjeev",
                        "middle": [],
                        "last": "Banerjee",
                        "suffix": ""
                    },
                    {
                        "first": "Alon",
                        "middle": [],
                        "last": "Lavie",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization",
                "volume": "",
                "issue": "",
                "pages": "65--72",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with im- proved correlation with human judgments. In Pro- ceedings of the ACL Workshop on Intrinsic and Ex- trinsic Evaluation Measures for Machine Transla- tion and/or Summarization, pages 65-72.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Findings of the 2017 Conference on Machine Translation (WMT17)",
                "authors": [
                    {
                        "first": "Ond\u0159ej",
                        "middle": [],
                        "last": "Bojar",
                        "suffix": ""
                    },
                    {
                        "first": "Rajen",
                        "middle": [],
                        "last": "Chatterjee",
                        "suffix": ""
                    },
                    {
                        "first": "Christian",
                        "middle": [],
                        "last": "Federmann",
                        "suffix": ""
                    },
                    {
                        "first": "Yvette",
                        "middle": [],
                        "last": "Graham",
                        "suffix": ""
                    },
                    {
                        "first": "Barry",
                        "middle": [],
                        "last": "Haddow",
                        "suffix": ""
                    },
                    {
                        "first": "Shujian",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Matthias",
                        "middle": [],
                        "last": "Huck",
                        "suffix": ""
                    },
                    {
                        "first": "Philipp",
                        "middle": [],
                        "last": "Koehn",
                        "suffix": ""
                    },
                    {
                        "first": "Qun",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Varvara",
                        "middle": [],
                        "last": "Logacheva",
                        "suffix": ""
                    },
                    {
                        "first": "Christof",
                        "middle": [],
                        "last": "Monz",
                        "suffix": ""
                    },
                    {
                        "first": "Matteo",
                        "middle": [],
                        "last": "Negri",
                        "suffix": ""
                    },
                    {
                        "first": "Matt",
                        "middle": [],
                        "last": "Post",
                        "suffix": ""
                    },
                    {
                        "first": "Raphael",
                        "middle": [],
                        "last": "Rubino",
                        "suffix": ""
                    },
                    {
                        "first": "Lucia",
                        "middle": [],
                        "last": "Specia",
                        "suffix": ""
                    },
                    {
                        "first": "Marco",
                        "middle": [],
                        "last": "Turchi",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the Second Conference on Machine Translation",
                "volume": "2",
                "issue": "",
                "pages": "169--214",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ond\u0159ej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Shujian Huang, Matthias Huck, Philipp Koehn, Qun Liu, Varvara Logacheva, Christof Monz, Matteo Negri, Matt Post, Raphael Rubino, Lucia Specia, and Marco Turchi. 2017a. Findings of the 2017 Conference on Machine Translation (WMT17). In Proceedings of the Second Conference on Machine Translation, Vol- ume 2: Shared Task Papers, pages 169-214, Copen- hagen, Denmark.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Findings of the 2016 Conference on Machine Translation",
                "authors": [
                    {
                        "first": "Ond\u0159ej",
                        "middle": [],
                        "last": "Bojar",
                        "suffix": ""
                    },
                    {
                        "first": "Rajen",
                        "middle": [],
                        "last": "Chatterjee",
                        "suffix": ""
                    },
                    {
                        "first": "Christian",
                        "middle": [],
                        "last": "Federmann",
                        "suffix": ""
                    },
                    {
                        "first": "Yvette",
                        "middle": [],
                        "last": "Graham",
                        "suffix": ""
                    },
                    {
                        "first": "Barry",
                        "middle": [],
                        "last": "Haddow",
                        "suffix": ""
                    },
                    {
                        "first": "Matthias",
                        "middle": [],
                        "last": "Huck",
                        "suffix": ""
                    },
                    {
                        "first": "Antonio",
                        "middle": [],
                        "last": "Jimeno Yepes",
                        "suffix": ""
                    },
                    {
                        "first": "Philipp",
                        "middle": [],
                        "last": "Koehn",
                        "suffix": ""
                    },
                    {
                        "first": "Varvara",
                        "middle": [],
                        "last": "Logacheva",
                        "suffix": ""
                    },
                    {
                        "first": "Christof",
                        "middle": [],
                        "last": "Monz",
                        "suffix": ""
                    },
                    {
                        "first": "Matteo",
                        "middle": [],
                        "last": "Negri",
                        "suffix": ""
                    },
                    {
                        "first": "Aurelie",
                        "middle": [],
                        "last": "Neveol",
                        "suffix": ""
                    },
                    {
                        "first": "Mariana",
                        "middle": [],
                        "last": "Neves",
                        "suffix": ""
                    },
                    {
                        "first": "Martin",
                        "middle": [],
                        "last": "Popel",
                        "suffix": ""
                    },
                    {
                        "first": "Matt",
                        "middle": [],
                        "last": "Post",
                        "suffix": ""
                    },
                    {
                        "first": "Raphael",
                        "middle": [],
                        "last": "Rubino",
                        "suffix": ""
                    },
                    {
                        "first": "Carolina",
                        "middle": [],
                        "last": "Scarton",
                        "suffix": ""
                    },
                    {
                        "first": "Lucia",
                        "middle": [],
                        "last": "Specia",
                        "suffix": ""
                    },
                    {
                        "first": "Marco",
                        "middle": [],
                        "last": "Turchi",
                        "suffix": ""
                    },
                    {
                        "first": "Karin",
                        "middle": [],
                        "last": "Verspoor",
                        "suffix": ""
                    },
                    {
                        "first": "Marcos",
                        "middle": [],
                        "last": "Zampieri",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the First Conference on Machine Translation",
                "volume": "",
                "issue": "",
                "pages": "131--198",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ond\u0159ej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Varvara Logacheva, Christof Monz, Matteo Negri, Aure- lie Neveol, Mariana Neves, Martin Popel, Matt Post, Raphael Rubino, Carolina Scarton, Lucia Spe- cia, Marco Turchi, Karin Verspoor, and Marcos Zampieri. 2016. Findings of the 2016 Conference on Machine Translation. In Proceedings of the First Conference on Machine Translation, pages 131- 198, Berlin, Germany.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Findings of the 2015 workshop on statistical machine translation",
                "authors": [
                    {
                        "first": "Ond\u0159ej",
                        "middle": [],
                        "last": "Bojar",
                        "suffix": ""
                    },
                    {
                        "first": "Rajen",
                        "middle": [],
                        "last": "Chatterjee",
                        "suffix": ""
                    },
                    {
                        "first": "Christian",
                        "middle": [],
                        "last": "Federmann",
                        "suffix": ""
                    },
                    {
                        "first": "Barry",
                        "middle": [],
                        "last": "Haddow",
                        "suffix": ""
                    },
                    {
                        "first": "Matthias",
                        "middle": [],
                        "last": "Huck",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Hokamp",
                        "suffix": ""
                    },
                    {
                        "first": "Philipp",
                        "middle": [],
                        "last": "Koehn",
                        "suffix": ""
                    },
                    {
                        "first": "Varvara",
                        "middle": [],
                        "last": "Logacheva",
                        "suffix": ""
                    },
                    {
                        "first": "Christof",
                        "middle": [],
                        "last": "Monz",
                        "suffix": ""
                    },
                    {
                        "first": "Matteo",
                        "middle": [],
                        "last": "Negri",
                        "suffix": ""
                    },
                    {
                        "first": "Matt",
                        "middle": [],
                        "last": "Post",
                        "suffix": ""
                    },
                    {
                        "first": "Carolina",
                        "middle": [],
                        "last": "Scarton",
                        "suffix": ""
                    },
                    {
                        "first": "Lucia",
                        "middle": [],
                        "last": "Specia",
                        "suffix": ""
                    },
                    {
                        "first": "Marco",
                        "middle": [],
                        "last": "Turchi",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the Tenth Workshop on Statistical Machine Translation",
                "volume": "",
                "issue": "",
                "pages": "1--46",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ond\u0159ej Bojar, Rajen Chatterjee, Christian Federmann, Barry Haddow, Matthias Huck, Chris Hokamp, Philipp Koehn, Varvara Logacheva, Christof Monz, Matteo Negri, Matt Post, Carolina Scarton, Lucia Specia, and Marco Turchi. 2015. Findings of the 2015 workshop on statistical machine translation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 1-46, Lisbon, Portugal.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Results of the wmt17 metrics shared task",
                "authors": [
                    {
                        "first": "Ond\u0159ej",
                        "middle": [],
                        "last": "Bojar",
                        "suffix": ""
                    },
                    {
                        "first": "Yvette",
                        "middle": [],
                        "last": "Graham",
                        "suffix": ""
                    },
                    {
                        "first": "Amir",
                        "middle": [],
                        "last": "Kamran",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the Second Conference on Machine Translation",
                "volume": "2",
                "issue": "",
                "pages": "489--513",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ond\u0159ej Bojar, Yvette Graham, and Amir Kamran. 2017b. Results of the wmt17 metrics shared task. In Proceedings of the Second Conference on Machine Translation, Volume 2: Shared Task Papers, pages 489-513, Copenhagen, Denmark.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Semantic textual similarity for MT evaluation",
                "authors": [
                    {
                        "first": "Julio",
                        "middle": [],
                        "last": "Castillo",
                        "suffix": ""
                    },
                    {
                        "first": "Paula",
                        "middle": [],
                        "last": "Estrella",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Proceedings of the Seventh Workshop on Statistical Machine Translation",
                "volume": "",
                "issue": "",
                "pages": "52--58",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Julio Castillo and Paula Estrella. 2012. Semantic tex- tual similarity for MT evaluation. In Proceedings of the Seventh Workshop on Statistical Machine Trans- lation, pages 52-58.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Enhanced LSTM for natural language inference",
                "authors": [
                    {
                        "first": "Qian",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaodan",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    },
                    {
                        "first": "Zhenhua",
                        "middle": [],
                        "last": "Ling",
                        "suffix": ""
                    },
                    {
                        "first": "Si",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Hui",
                        "middle": [],
                        "last": "Jiang",
                        "suffix": ""
                    },
                    {
                        "first": "Diana",
                        "middle": [],
                        "last": "Inkpen",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Qian Chen, Xiaodan Zhu, Zhenhua Ling, Si Wei, Hui Jiang, and Diana Inkpen. 2017. Enhanced LSTM for natural language inference. In Proceedings of the 55th Annual Meeting of the Association for Compu- tational Linguistics, Vancouver, Canada.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
                "authors": [
                    {
                        "first": "Jacob",
                        "middle": [],
                        "last": "Devlin",
                        "suffix": ""
                    },
                    {
                        "first": "Ming-Wei",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "Kenton",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Kristina",
                        "middle": [],
                        "last": "Toutanova",
                        "suffix": ""
                    }
                ],
                "year": 1993,
                "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT 2019)",
                "volume": "57",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies (NAACL HLT 2019), Minneapolis, USA. Bradley Efron and Robert Tibshirani. 1993. An intro- duction to the bootstrap, volume 57. CRC press.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Linguistic features for automatic evaluation of heterogenous MT systems",
                "authors": [
                    {
                        "first": "Jes\u00fas",
                        "middle": [],
                        "last": "Gim\u00e9nez",
                        "suffix": ""
                    },
                    {
                        "first": "Llu\u00eds",
                        "middle": [],
                        "last": "M\u00e0rquez",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Proceedings of the Second Workshop on Statistical Machine Translation",
                "volume": "",
                "issue": "",
                "pages": "256--264",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jes\u00fas Gim\u00e9nez and Llu\u00eds M\u00e0rquez. 2007. Linguistic features for automatic evaluation of heterogenous MT systems. In Proceedings of the Second Work- shop on Statistical Machine Translation, pages 256- 264.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Testing for significance of increased correlation with human judgment",
                "authors": [
                    {
                        "first": "Yvette",
                        "middle": [],
                        "last": "Graham",
                        "suffix": ""
                    },
                    {
                        "first": "Timothy",
                        "middle": [],
                        "last": "Baldwin",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "EMNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yvette Graham and Timothy Baldwin. 2014. Testing for significance of increased correlation with human judgment. In EMNLP.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Can machine translation systems be evaluated by the crowd alone",
                "authors": [
                    {
                        "first": "Yvette",
                        "middle": [],
                        "last": "Graham",
                        "suffix": ""
                    },
                    {
                        "first": "Timothy",
                        "middle": [],
                        "last": "Baldwin",
                        "suffix": ""
                    },
                    {
                        "first": "Alistair",
                        "middle": [],
                        "last": "Moffat",
                        "suffix": ""
                    },
                    {
                        "first": "Justin",
                        "middle": [],
                        "last": "Zobel",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Natural Language Engineering",
                "volume": "23",
                "issue": "1",
                "pages": "3--30",
                "other_ids": {
                    "DOI": [
                        "10.1017/S1351324915000339"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yvette Graham, Timothy Baldwin, Alistair Moffat, and Justin Zobel. 2017. Can machine translation sys- tems be evaluated by the crowd alone. Natural Lan- guage Engineering, 23(1):3-30.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Accurate evaluation of segment-level machine translation metrics",
                "authors": [
                    {
                        "first": "Yvette",
                        "middle": [],
                        "last": "Graham",
                        "suffix": ""
                    },
                    {
                        "first": "Nitika",
                        "middle": [],
                        "last": "Mathur",
                        "suffix": ""
                    },
                    {
                        "first": "Timothy",
                        "middle": [],
                        "last": "Baldwin",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the 2015 Conference of the North American Chapter",
                "volume": "",
                "issue": "",
                "pages": "1183--1191",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yvette Graham, Nitika Mathur, and Timothy Baldwin. 2015. Accurate evaluation of segment-level ma- chine translation metrics. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies, pages 1183-1191, Denver, USA.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "ReVal: A simple and effective machine translation evaluation metric based on recurrent neural networks",
                "authors": [
                    {
                        "first": "Rohit",
                        "middle": [],
                        "last": "Gupta",
                        "suffix": ""
                    },
                    {
                        "first": "Constantin",
                        "middle": [],
                        "last": "Orasan",
                        "suffix": ""
                    },
                    {
                        "first": "Josef",
                        "middle": [],
                        "last": "Van Genabith",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "1066--1072",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Rohit Gupta, Constantin Orasan, and Josef van Gen- abith. 2015. ReVal: A simple and effective ma- chine translation evaluation metric based on recur- rent neural networks. In Proceedings of the 2015 Conference on Empirical Methods in Natural Lan- guage Processing, pages 1066-1072, Lisbon, Portu- gal.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Using discourse structure improves machine translation evaluation",
                "authors": [
                    {
                        "first": "Francisco",
                        "middle": [],
                        "last": "Guzm\u00e1n",
                        "suffix": ""
                    },
                    {
                        "first": "Llu\u00eds",
                        "middle": [],
                        "last": "Shafiq R Joty",
                        "suffix": ""
                    },
                    {
                        "first": "Preslav",
                        "middle": [],
                        "last": "M\u00e0rquez",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Nakov",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014)",
                "volume": "",
                "issue": "",
                "pages": "687--698",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Francisco Guzm\u00e1n, Shafiq R Joty, Llu\u00eds M\u00e0rquez, and Preslav Nakov. 2014. Using discourse structure im- proves machine translation evaluation. In Proceed- ings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014), pages 687-698.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "TESLA: Translation evaluation of sentences with linear-programming-based analysis",
                "authors": [
                    {
                        "first": "Chang",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Dahlmeier",
                        "suffix": ""
                    },
                    {
                        "first": "Hwee Tou",
                        "middle": [],
                        "last": "Ng",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and Metrics (MATR)",
                "volume": "",
                "issue": "",
                "pages": "354--359",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng. 2010. TESLA: Translation evaluation of sentences with linear-programming-based analysis. In Pro- ceedings of the Joint Fifth Workshop on Statistical Machine Translation and Metrics (MATR), pages 354-359.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "MEANT 2.0: Accurate semantic MT evaluation for any output language",
                "authors": [
                    {
                        "first": "Chi-Kiu",
                        "middle": [],
                        "last": "Lo",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the Second Conference on Machine Translation",
                "volume": "2",
                "issue": "",
                "pages": "589--597",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chi-kiu Lo. 2017. MEANT 2.0: Accurate semantic MT evaluation for any output language. In Proceed- ings of the Second Conference on Machine Transla- tion, Volume 2: Shared Task Papers, pages 589-597, Copenhagen, Denmark.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Blend: a novel combined MT metric based on direct assessment -CASICT-DCU submission to WMT17 metrics task",
                "authors": [
                    {
                        "first": "Qingsong",
                        "middle": [],
                        "last": "Ma",
                        "suffix": ""
                    },
                    {
                        "first": "Yvette",
                        "middle": [],
                        "last": "Graham",
                        "suffix": ""
                    },
                    {
                        "first": "Shugen",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Qun",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the Second Conference on Machine Translation",
                "volume": "2",
                "issue": "",
                "pages": "598--603",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Qingsong Ma, Yvette Graham, Shugen Wang, and Qun Liu. 2017. Blend: a novel combined MT metric based on direct assessment -CASICT-DCU sub- mission to WMT17 metrics task. In Proceedings of the Second Conference on Machine Translation, Vol- ume 2: Shared Task Papers, pages 598-603, Copen- hagen, Denmark.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Distributed representations of words and phrases and their compositionality",
                "authors": [
                    {
                        "first": "Tomas",
                        "middle": [],
                        "last": "Mikolov",
                        "suffix": ""
                    },
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    },
                    {
                        "first": "Kai",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Greg",
                        "middle": [
                            "S"
                        ],
                        "last": "Corrado",
                        "suffix": ""
                    },
                    {
                        "first": "Jeff",
                        "middle": [],
                        "last": "Dean",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "26",
                "issue": "",
                "pages": "3111--3119",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor- rado, and Jeff Dean. 2013. Distributed representa- tions of words and phrases and their compositional- ity. In Advances in Neural Information Processing Systems 26, pages 3111-3119.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Natural language inference by tree-based convolution and heuristic matching",
                "authors": [
                    {
                        "first": "Lili",
                        "middle": [],
                        "last": "Mou",
                        "suffix": ""
                    },
                    {
                        "first": "Rui",
                        "middle": [],
                        "last": "Men",
                        "suffix": ""
                    },
                    {
                        "first": "Ge",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Yan",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Lu",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Rui",
                        "middle": [],
                        "last": "Yan",
                        "suffix": ""
                    },
                    {
                        "first": "Zhi",
                        "middle": [],
                        "last": "Jin",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics",
                "volume": "2",
                "issue": "",
                "pages": "130--136",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/P16-2022"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Lili Mou, Rui Men, Ge Li, Yan Xu, Lu Zhang, Rui Yan, and Zhi Jin. 2016. Natural language inference by tree-based convolution and heuristic matching. In Proceedings of the 54th Annual Meeting of the As- sociation for Computational Linguistics (Volume 2: Short Papers), pages 130-136.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Robust machine translation evaluation with entailment features",
                "authors": [
                    {
                        "first": "Sebastian",
                        "middle": [],
                        "last": "Pad\u00f3",
                        "suffix": ""
                    },
                    {
                        "first": "Michel",
                        "middle": [],
                        "last": "Galley",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Jurafsky",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP",
                "volume": "1",
                "issue": "",
                "pages": "297--305",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sebastian Pad\u00f3, Michel Galley, Dan Jurafsky, and Chris Manning. 2009. Robust machine translation evaluation with entailment features. In Proceed- ings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1-Volume 1, pages 297-305.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "BLEU: A method for automatic evaluation of machine translation",
                "authors": [
                    {
                        "first": "Kishore",
                        "middle": [],
                        "last": "Papineni",
                        "suffix": ""
                    },
                    {
                        "first": "Salim",
                        "middle": [],
                        "last": "Roukos",
                        "suffix": ""
                    },
                    {
                        "first": "Todd",
                        "middle": [],
                        "last": "Ward",
                        "suffix": ""
                    },
                    {
                        "first": "Wei-Jing",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL 2002)",
                "volume": "",
                "issue": "",
                "pages": "311--318",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL 2002), pages 311- 318, Philadelphia, USA.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Deep contextualized word representations",
                "authors": [
                    {
                        "first": "Matthew",
                        "middle": [],
                        "last": "Peters",
                        "suffix": ""
                    },
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Neumann",
                        "suffix": ""
                    },
                    {
                        "first": "Mohit",
                        "middle": [],
                        "last": "Iyyer",
                        "suffix": ""
                    },
                    {
                        "first": "Matt",
                        "middle": [],
                        "last": "Gardner",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [],
                        "last": "Clark",
                        "suffix": ""
                    },
                    {
                        "first": "Kenton",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Luke",
                        "middle": [],
                        "last": "Zettlemoyer",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 2018 Conference of the North American Chapter",
                "volume": "1",
                "issue": "",
                "pages": "2227--2237",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word repre- sentations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), volume 1, pages 2227-2237.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "chrF: character n-gram F-score for automatic MT evaluation",
                "authors": [
                    {
                        "first": "Maja",
                        "middle": [],
                        "last": "Popovi\u0107",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the Tenth Workshop on Statistical Machine Translation",
                "volume": "",
                "issue": "",
                "pages": "392--395",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Maja Popovi\u0107. 2015. chrF: character n-gram F-score for automatic MT evaluation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 392-395, Lisbon, Portugal.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Know what you don't know: Unanswerable questions for squad",
                "authors": [
                    {
                        "first": "Pranav",
                        "middle": [],
                        "last": "Rajpurkar",
                        "suffix": ""
                    },
                    {
                        "first": "Robin",
                        "middle": [],
                        "last": "Jia",
                        "suffix": ""
                    },
                    {
                        "first": "Percy",
                        "middle": [],
                        "last": "Liang",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
                "volume": "2",
                "issue": "",
                "pages": "784--789",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don't know: Unanswerable ques- tions for squad. In Proceedings of the 56th Annual Meeting of the Association for Computational Lin- guistics (Volume 2: Short Papers), pages 784-789.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Reasoning about entailment with neural attention",
                "authors": [
                    {
                        "first": "Tim",
                        "middle": [],
                        "last": "Rockt\u00e4schel",
                        "suffix": ""
                    },
                    {
                        "first": "Edward",
                        "middle": [],
                        "last": "Grefenstette",
                        "suffix": ""
                    },
                    {
                        "first": "Karl",
                        "middle": [
                            "Moritz"
                        ],
                        "last": "Hermann",
                        "suffix": ""
                    },
                    {
                        "first": "Tomas",
                        "middle": [],
                        "last": "Kocisky",
                        "suffix": ""
                    },
                    {
                        "first": "Phil",
                        "middle": [],
                        "last": "Blunsom",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "International Conference on Learning Representations (ICLR)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tim Rockt\u00e4schel, Edward Grefenstette, Karl Moritz Hermann, Tomas Kocisky, and Phil Blunsom. 2016. Reasoning about entailment with neural attention. In International Conference on Learning Represen- tations (ICLR).",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "RUSE: Regressor using sentence embeddings for automatic machine translation evaluation",
                "authors": [
                    {
                        "first": "Hiroki",
                        "middle": [],
                        "last": "Shimanaka",
                        "suffix": ""
                    },
                    {
                        "first": "Tomoyuki",
                        "middle": [],
                        "last": "Kajiwara",
                        "suffix": ""
                    },
                    {
                        "first": "Mamoru",
                        "middle": [],
                        "last": "Komachi",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the Third Conference on Machine Translation",
                "volume": "2",
                "issue": "",
                "pages": "764--771",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Hiroki Shimanaka, Tomoyuki Kajiwara, and Mamoru Komachi. 2018. RUSE: Regressor using sentence embeddings for automatic machine translation eval- uation. In Proceedings of the Third Conference on Machine Translation, Volume 2: Shared Task Pa- pers, pages 764-771, Belgium, Brussels.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "A study of translation edit rate with targeted human annotation",
                "authors": [
                    {
                        "first": "Matthew",
                        "middle": [],
                        "last": "Snover",
                        "suffix": ""
                    },
                    {
                        "first": "Bonnie",
                        "middle": [],
                        "last": "Dorr",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Schwartz",
                        "suffix": ""
                    },
                    {
                        "first": "Linnea",
                        "middle": [],
                        "last": "Micciulla",
                        "suffix": ""
                    },
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Makhoul",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Proceedings of the Association for Machine Transaltion in the Americas",
                "volume": "",
                "issue": "",
                "pages": "223--231",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin- nea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annota- tion. In Proceedings of the Association for Machine Transaltion in the Americas, pages 223-231.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Regression and ranking based optimisation for sentence level machine translation evaluation",
                "authors": [
                    {
                        "first": "Xingyi",
                        "middle": [],
                        "last": "Song",
                        "suffix": ""
                    },
                    {
                        "first": "Trevor",
                        "middle": [],
                        "last": "Cohn",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Proceedings of the Sixth Workshop on Statistical Machine Translation",
                "volume": "",
                "issue": "",
                "pages": "123--129",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xingyi Song and Trevor Cohn. 2011. Regression and ranking based optimisation for sentence level ma- chine translation evaluation. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 123-129.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "BEER: BEtter evaluation as ranking",
                "authors": [
                    {
                        "first": "Milos",
                        "middle": [],
                        "last": "Stanojevic",
                        "suffix": ""
                    },
                    {
                        "first": "Khalil",
                        "middle": [],
                        "last": "Sima'an",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of the Ninth Workshop on Statistical Machine Translation",
                "volume": "",
                "issue": "",
                "pages": "414--419",
                "other_ids": {
                    "DOI": [
                        "10.3115/v1/W14-3354"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Milos Stanojevic and Khalil Sima'an. 2014. BEER: BEtter evaluation as ranking. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 414-419, Baltimore, USA.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "CharacTer: Translation edit rate on character level",
                "authors": [
                    {
                        "first": "Weiyue",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Jan-Thorsten",
                        "middle": [],
                        "last": "Peter",
                        "suffix": ""
                    },
                    {
                        "first": "Hendrik",
                        "middle": [],
                        "last": "Rosendahl",
                        "suffix": ""
                    },
                    {
                        "first": "Hermann",
                        "middle": [],
                        "last": "Ney",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the First Conference on Machine Translation",
                "volume": "",
                "issue": "",
                "pages": "505--510",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Weiyue Wang, Jan-Thorsten Peter, Hendrik Rosendahl, and Hermann Ney. 2016. CharacTer: Translation edit rate on character level. In Proceedings of the First Conference on Machine Translation, pages 505-510, Berlin, Germany.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "Bilateral multi-perspective matching for natural language sentences",
                "authors": [
                    {
                        "first": "Zhiguo",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Wael",
                        "middle": [],
                        "last": "Hamza",
                        "suffix": ""
                    },
                    {
                        "first": "Radu",
                        "middle": [],
                        "last": "Florian",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the 26th International Joint Conference on Artificial Intelligence",
                "volume": "",
                "issue": "",
                "pages": "4144--4150",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zhiguo Wang, Wael Hamza, and Radu Florian. 2017. Bilateral multi-perspective matching for natural lan- guage sentences. In Proceedings of the 26th Inter- national Joint Conference on Artificial Intelligence, pages 4144-4150.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure1: Average Pearson's r for ESIM over the WMT 2017 to-English sentence-level dataset vs. the total number of annotations in the training set. We contrast two styles of collecting data: (1) the circles are trained on a single annotation per instance; and (2) the crosses are trained on the mean of N annotations per instance, as N goes from 1 to 14. The first strategy is more data-efficient.",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table><tr><td/><td colspan=\"2\">BLEU</td><td colspan=\"6\">0.435 0.432 0.571 0.393 0.484 0.538 0.512 0.481</td></tr><tr><td/><td colspan=\"2\">CHRF</td><td colspan=\"6\">0.514 0.531 0.671 0.525 0.599 0.607 0.591 0.577</td></tr><tr><td/><td colspan=\"2\">MEANT 2.0</td><td colspan=\"6\">0.578 0.565 0.687 0.586 0.607 0.596 0.639 0.608</td></tr><tr><td/><td colspan=\"2\">RUSE</td><td colspan=\"6\">0.614 0.637 0.756 0.705 0.680 0.704 0.677 0.682</td></tr><tr><td>P</td><td colspan=\"2\">BERTR</td><td colspan=\"6\">0.655 0.650 0.777 0.671 0.680 0.702 0.687 0.689</td></tr><tr><td>TrainS</td><td colspan=\"8\">BiLSTM BiLSTM + attention 0.611 0.603 0.763 0.740 0.655 0.695 0.694 0.680 0.517 0.556 0.735 0.672 0.606 0.619 0.565 0.610 ESIM 0.534 0.546 0.757 0.704 0.621 0.632 0.629 0.632</td></tr><tr><td>TrainL</td><td colspan=\"8\">BiLSTM BiLSTM + attention 0.704 0.710 0.818 0.777 0.744 0.753 0.737 0.749 0.628 0.621 0.774 0.732 0.689 0.682 0.655 0.682 ESIM 0.692 0.706 0.829 0.764 0.726 0.776 0.732 0.746</td></tr><tr><td/><td/><td/><td colspan=\"2\">en-cs en-de</td><td>en-fi</td><td colspan=\"2\">en-lv en-ru</td><td>en-tr en-zh</td></tr><tr><td/><td/><td/><td>\u03c4</td><td>\u03c4</td><td>\u03c4</td><td>\u03c4</td><td>\u03c1</td><td>\u03c4</td><td>\u03c1</td></tr><tr><td/><td>Baselines</td><td colspan=\"7\">SENT-BLEU CHRF BEER MEANT 2.0-NOSRL 0.395 0.324 0.565 0.425 0.636 0.482 0.705 0.274 0.269 0.446 0.259 0.468 0.377 0.642 0.376 0.336 0.503 0.420 0.605 0.466 0.608 0.398 0.336 0.557 0.420 0.569 0.490 0.622</td></tr><tr><td/><td/><td>MEANT 2.0</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>0.727</td></tr><tr><td/><td>P</td><td>BERTR</td><td colspan=\"6\">0.390 0.365 0.564 0.417 0.630 0.457 0.803</td></tr><tr><td/><td>T</td><td>ESIM</td><td colspan=\"6\">0.338 0.362 0.523 0.350 0.700 0.506 0.699</td></tr></table>",
                "type_str": "table",
                "text": "Pearson's r on the WMT 2017 sentence-level evaluation data. P: Unsupervised metric that relies on pretrained embeddings; TrainS: trained on accurate 3360 instances; TrainL: trained on noisy 125k instances. Correlations of metrics not significantly outperformed by any other for that language pair are highlighted in bold (William's test;Graham and Baldwin, 2014)",
                "html": null,
                "num": null
            },
            "TABREF1": {
                "content": "<table/>",
                "type_str": "table",
                "text": "",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table><tr><td/><td/><td colspan=\"7\">en-cs en-de en-fi en-lv en-ru en-tr en-zh</td></tr><tr><td/><td>num systems</td><td>14</td><td>16</td><td>12</td><td>17</td><td>9</td><td>8</td><td>11</td></tr><tr><td>Baselines</td><td colspan=\"8\">BLEU BEER CHARACTER 0.981 0.938 0.972 0.897 0.939 0.975 0.933 0.956 0.804 0.920 0.866 0.898 0.924 0.981 0.970 0.842 0.976 0.930 0.944 0.980 0.914 CHRF 0.976 0.863 0.981 0.955 0.950 0.991 0.976</td></tr><tr><td>P</td><td>BERTR</td><td colspan=\"7\">0.982 0.877 0.979 0.949 0.971 0.996 0.992</td></tr><tr><td>T</td><td>ESIM</td><td colspan=\"7\">0.974 0.861 0.971 0.954 0.968 0.978 0.970</td></tr></table>",
                "type_str": "table",
                "text": "Pearson's r on the WMT 2017 to-English system-level evaluation data. The first section represents existing metrics, both trained and untrained. We then present results of our unsupervised metric, followed by our supervised metric trained in the TrainL setting: noisy 130k instances. Correlations of metrics not significantly outperformed by any other for that language pair are highlighted in bold.",
                "html": null,
                "num": null
            },
            "TABREF4": {
                "content": "<table><tr><td/><td/><td colspan=\"5\">en-cs en-de en-es en-nl en-pt</td></tr><tr><td/><td>num systems</td><td>5</td><td>10</td><td>4</td><td>4</td><td>4</td></tr><tr><td>Baselines</td><td colspan=\"6\">BLEU CHRF BEER CHARACTER 0.901 0.930 0.963 0.927 0.976 0.750 0.621 0.976 0.596 0.997 0.845 0.588 0.915 0.951 0.967 0.744 0.621 0.931 0.983 0.989</td></tr><tr><td>P</td><td>BERTR</td><td colspan=\"5\">0.974 0.780 0.925 0.896 0.980</td></tr><tr><td>T</td><td>ESIM</td><td colspan=\"5\">0.964 0.780 0.991 0.798 0.996</td></tr></table>",
                "type_str": "table",
                "text": "Pearson's r on the WMT 2017 from-English system-level evaluation data. The first section represents existing metrics, both trained and untrained. We then present results of our unsupervised metric, followed by our supervised metric trained in the TrainL setting: noisy 130k instances. Correlations of metrics not significantly outperformed by any other for that language pair are highlighted in bold.",
                "html": null,
                "num": null
            },
            "TABREF5": {
                "content": "<table><tr><td>Translations with LOW Human scores</td><td colspan=\"3\">ESIM BERTR SENT-</td></tr><tr><td/><td/><td/><td>BLEU</td></tr><tr><td colspan=\"2\">ref: LOW</td><td>LOW</td><td>HIGH</td></tr><tr><td colspan=\"2\">sys: LOW</td><td>HIGH</td><td>HIGH</td></tr><tr><td>ref: Foreign goods trade had slowed, too.</td><td/><td/><td/></tr><tr><td>sys: Foreign trade also slowed the economy.</td><td>HIGH</td><td>LOW</td><td>LOW</td></tr><tr><td colspan=\"2\">ref: HIGH</td><td>HIGH</td><td>HIGH</td></tr><tr><td>sys:</td><td/><td/><td/></tr></table>",
                "type_str": "table",
                "text": "Pearson's r on the WMT 2016 IT domain system-level evaluation data. The first section represents existing metrics, both trained and untrained. We then present results of our pretrained metric, followed by our supervised metric trained in the TrainL setting: noisy 130k instances. Correlations of metrics not significantly outperformed by any other for that language pair are highlighted in bold. For the benefit of the school, Richter nurtured a good relationship with the then Mayor, Ludwig Gtz (CSU). sys: For the good of the school of judges as rector of a good relationship with the former mayor Ludwig Gtz (CSU) ref: The military plays an important role in Pakistan and has taken power by force several times in the past. The military plays an important role in Pakistan and has already more frequently geputscht. ref: Behind much of the pro-democracy campaign in Hong Kong is the Occupy Central With Love and Peace movement, whose organizers have threatened to shut down the financial district if Beijing does not grant authentic universal suffrage. sys: Behind the pro-democracy campaign in Hong Kong is the movement Occupy Central With Love and Peace, whose organizers have threatened the acupuncture, off, if Beijing allows no real universal suffrage. Some shrapnel pieces are still in my knee. sys: Some garnet fragments are still in my knee. ref: Stewart hit the wall for the second time after his right front tire blew out on lap 172, ending his night. Stewart raced for the second time against the wall after his right front tire on lap 172 and ended his evening.",
                "html": null,
                "num": null
            },
            "TABREF6": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Examples of bad quality translations in the WMT 2015 sentence level DA dataset and whether ESIM, BERTR and SENT-BLEU correctly give them low scores",
                "html": null,
                "num": null
            }
        }
    }
}