{
    "paper_id": "2021",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:12:31.272665Z"
    },
    "title": "On Attention Redundancy: A Comprehensive Study",
    "authors": [
        {
            "first": "Yuchen",
            "middle": [],
            "last": "Bian",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Baidu Research",
                "location": {
                    "settlement": "Sunnyvale",
                    "region": "CA",
                    "country": "USA"
                }
            },
            "email": "yuchenbian@baidu.com"
        },
        {
            "first": "Jiaji",
            "middle": [],
            "last": "Huang",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Baidu Research",
                "location": {
                    "settlement": "Sunnyvale",
                    "region": "CA",
                    "country": "USA"
                }
            },
            "email": "huangjiaji@baidu.com"
        },
        {
            "first": "Xingyu",
            "middle": [],
            "last": "Cai",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Baidu Research",
                "location": {
                    "settlement": "Sunnyvale",
                    "region": "CA",
                    "country": "USA"
                }
            },
            "email": "xingyucai@baidu.com"
        },
        {
            "first": "Jiahong",
            "middle": [],
            "last": "Yuan",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Baidu Research",
                "location": {
                    "settlement": "Sunnyvale",
                    "region": "CA",
                    "country": "USA"
                }
            },
            "email": "jiahongyuan@baidu.com"
        },
        {
            "first": "Kenneth",
            "middle": [],
            "last": "Church",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Baidu Research",
                "location": {
                    "settlement": "Sunnyvale",
                    "region": "CA",
                    "country": "USA"
                }
            },
            "email": "kennethchurch@baidu.com"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Multi-layer multi-head self-attention mechanism is widely applied in modern neural language models. Attention redundancy has been observed among attention heads but has not been deeply studied in the literature. Using BERT-base model as an example, this paper provides a comprehensive study on attention redundancy which is helpful for model interpretation and model compression. We analyze the attention redundancy with Five-Ws and How. (What) We define and focus the study on redundancy matrices generated from pre-trained and fine-tuned BERT-base model for GLUE datasets. (How) We use both token-based and sentence-based distance functions to measure the redundancy. (Where) Clear and similar redundancy patterns (cluster structure) are observed among attention heads. (When) Redundancy patterns are similar in both pre-training and fine-tuning phases.\n(Who) We discover that redundancy patterns are task-agnostic. Similar redundancy patterns even exist for randomly generated token sequences. (\"Why\") We also evaluate influences of the pre-training dropout ratios on attention redundancy. Based on the phaseindependent and task-agnostic attention redundancy patterns, we propose a simple zero-shot pruning method as a case study. Experiments on fine-tuning GLUE tasks verify its effectiveness. The comprehensive analyses on attention redundancy make model understanding and zero-shot model pruning promising.",
    "pdf_parse": {
        "paper_id": "2021",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Multi-layer multi-head self-attention mechanism is widely applied in modern neural language models. Attention redundancy has been observed among attention heads but has not been deeply studied in the literature. Using BERT-base model as an example, this paper provides a comprehensive study on attention redundancy which is helpful for model interpretation and model compression. We analyze the attention redundancy with Five-Ws and How. (What) We define and focus the study on redundancy matrices generated from pre-trained and fine-tuned BERT-base model for GLUE datasets. (How) We use both token-based and sentence-based distance functions to measure the redundancy. (Where) Clear and similar redundancy patterns (cluster structure) are observed among attention heads. (When) Redundancy patterns are similar in both pre-training and fine-tuning phases.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            },
            {
                "text": "(Who) We discover that redundancy patterns are task-agnostic. Similar redundancy patterns even exist for randomly generated token sequences. (\"Why\") We also evaluate influences of the pre-training dropout ratios on attention redundancy. Based on the phaseindependent and task-agnostic attention redundancy patterns, we propose a simple zero-shot pruning method as a case study. Experiments on fine-tuning GLUE tasks verify its effectiveness. The comprehensive analyses on attention redundancy make model understanding and zero-shot model pruning promising.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Multi-layer multi-head self-attention architectures (Transformer (Vaswani et al., 2017) ) are widely applied in modern language models, such as BERT (Devlin et al., 2019) , RoBERTa (Liu et al., 2019) , OpenAI GPT (Radford et al., 2018) , GPT-2 (Radford et al., 2019) and ERNIE2.0 (Sun et al., 2019) , to name a few.",
                "cite_spans": [
                    {
                        "start": 65,
                        "end": 87,
                        "text": "(Vaswani et al., 2017)",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 149,
                        "end": 170,
                        "text": "(Devlin et al., 2019)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 181,
                        "end": 199,
                        "text": "(Liu et al., 2019)",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 213,
                        "end": 235,
                        "text": "(Radford et al., 2018)",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 238,
                        "end": 266,
                        "text": "GPT-2 (Radford et al., 2019)",
                        "ref_id": null
                    },
                    {
                        "start": 280,
                        "end": 298,
                        "text": "(Sun et al., 2019)",
                        "ref_id": "BIBREF22"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Redundancy phenomenon is discovered among attention heads. It demonstrates that many attention heads generate very similar attention matrices (Clark et al., 2019; Kovaleva et al., 2019) . We take the pre-trained BERT-base model as an example. It learns 12-layer-12-head self-attention matrices describing dependencies between each pair of tokens in a sentence. Then for each token, there are 144 attention vectors. We use Jensen-Shannon distance to measure the relationship between each pair of vectors. Then for one sentence (consisting of a sequence of tokens), the token-averaged distance is utilized to imply the redundancy between each pair of attention matrices. Smaller distance values reflect more redundancy. Figure 1 shows the redundancy (distance) among 144 \u00d7 144 pairs of attention matrices averaged over 1000 randomly sampled sentences. We can see clear redundancy patterns (clusters with smaller distance areas) in consecutive attention layers.",
                "cite_spans": [
                    {
                        "start": 142,
                        "end": 162,
                        "text": "(Clark et al., 2019;",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 163,
                        "end": 185,
                        "text": "Kovaleva et al., 2019)",
                        "ref_id": "BIBREF13"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 725,
                        "end": 726,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Analyzing the attention redundancy helps to interpret the multi-layer multi-head self-attention architecture. Various studies have attempted to re-veal the relationship among attention heads. Examples are attention visualization (Vig and Belinkov, 2019) , common attention patterns (Kovaleva et al., 2019) , attention head pruning (Voita et al., 2019) , and probing test (Clark et al., 2019) . Existing works either focus on the 12 \u00d7 12 attention matrices and their effects on (pre-training or/and finetuning) performances or focus on linguistic features extracted by latent token vectors and attention matrices.",
                "cite_spans": [
                    {
                        "start": 229,
                        "end": 253,
                        "text": "(Vig and Belinkov, 2019)",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 282,
                        "end": 305,
                        "text": "(Kovaleva et al., 2019)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 331,
                        "end": 351,
                        "text": "(Voita et al., 2019)",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 371,
                        "end": 391,
                        "text": "(Clark et al., 2019)",
                        "ref_id": "BIBREF2"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Though the redundancy phenomenon was discovered, no existing work studies the attention redundancy pattern itself (i.e., the 144 \u00d7 144 distance matrix in Figure 1 ) deeply. This motivates us to conduct a comprehensive and complementary study on the attention redundancy phenomenon.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 161,
                        "end": 162,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this paper, we take the BERT-base model as a representative model to analyze the attention redundancy with Five Ws and How. As far as we know, many of the following discoveries are new to the research community.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "What is attention redundancy? Given a distance function, we define the pairwise distance matrix (\u2208 R 144\u00d7144 ) of the 12 \u00d7 12 attention matrices of BERT-base model as attention redundancy matrix. In this paper, we obtain redundancy matrices from both pre-trained and finetuned BERT-base model for GLUE tasks as the research objects.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "How to measure attention redundancy? Except for the two token-based measures, Jensen-Shannon distance (Clark et al., 2019) and cosine similarity (Kovaleva et al., 2019) used in literature, we employ two more token-based distance function and three sentence-based ones to measure attention redundancy and analyze their similar redundancy patterns (please refer to Section 4.1 for more details). The purpose is to alleviate the measuring bias of just using one distance function. Sentence-based distances directly measure the relationship between two attention matrices without averaging over tokens. We visualize the redundancy patterns using various distance functions.",
                "cite_spans": [
                    {
                        "start": 102,
                        "end": 122,
                        "text": "(Clark et al., 2019)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 145,
                        "end": 168,
                        "text": "(Kovaleva et al., 2019)",
                        "ref_id": "BIBREF13"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Where does attention redundancy exist? We find common hierarchical cluster structures in the set of token-based redundancy matrices and the set of sentence-based redundancy matrices, respectively. Attention heads of earlier, middle, and deeper attention layers are clearly clustered in the redundancy matrices. We also demonstrate that highly correlated similar redundancy patterns exist in redundancy matrices generated based on different type of distances.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "When does attention redundancy occur?",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The redundancy is phase-independent. Common redundancy patterns are discovered in both the pre-trained phase and fine-tuned phases. For any downstream task with any distance function, we notice highly correlated attention redundancy patterns between two phases.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Who (which task) has attention redundancy?",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "We surprisingly realize that the redundancy is task-agnostic. The redundancy patterns are highly correlated across different tasks. We even randomly generate token sequences as input in the pre-trained BERT-base model. Very similar attention redundancy patterns occur as well.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Based on this astonishing discovery, as a case study application, we propose a simple zero-shot head-pruning strategy based on clustering results using redundancy matrices. Compared to other complex pruning strategies, e.g., (Tang et al., 2019; Jiao et al., 2019; Fan et al., 2019; Wang et al., 2019; McCarley, 2019) , the most important is that without knowing any data of fine-tuning tasks, this pruning can be effectively and efficiently conducted just based on some randomly generated token sequences with the pre-trained BERT-base model. The only effort is to compute one or several redundancy matrices. Results reflect that for most GLUE tasks, the proposed pruning strategy based on redundancy matrices can prune up to 75% to 85% of attention heads while keeping comparable fine-tuning performances.",
                "cite_spans": [
                    {
                        "start": 225,
                        "end": 244,
                        "text": "(Tang et al., 2019;",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 245,
                        "end": 263,
                        "text": "Jiao et al., 2019;",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 264,
                        "end": 281,
                        "text": "Fan et al., 2019;",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 282,
                        "end": 300,
                        "text": "Wang et al., 2019;",
                        "ref_id": "BIBREF31"
                    },
                    {
                        "start": 301,
                        "end": 316,
                        "text": "McCarley, 2019)",
                        "ref_id": "BIBREF17"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\"Why\" does the phase-independent and taskagnostic attention redundancy happen?",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "It's hard to tell the reason of the redundancy patterns (that's why we use the quoted \"Why\"). However, we conduct experiments to evaluate the effects on attention redundancy of dropout ratios in the pre-training phase which are suspected as one of reasons (Clark et al., 2019) .",
                "cite_spans": [
                    {
                        "start": 256,
                        "end": 276,
                        "text": "(Clark et al., 2019)",
                        "ref_id": "BIBREF2"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "When we use sentence-based distance, a monotonic trend is found. Attention heads tend to be more redundant when increasing dropout ratios. When we use token-based distances, a complex \"N\"-shape effect exists. We also notice that the redundancy is more sensitive to dropouts in hidden linear transformations than to dropouts in the self-attention mechanism.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "We believe that above-mentioned new findings in this paper make the redundancy analyses a promis-ing research direction in model interpretation and model compression, and so on.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Existing literature analyzes multi-layer multi-head self-attention architectures based language models (e.g., BERT) from different aspects including word/token embedding latent space, linguistic knowledge interpretability, attention mechanism, and so on (Rogers et al., 2020) .",
                "cite_spans": [
                    {
                        "start": 254,
                        "end": 275,
                        "text": "(Rogers et al., 2020)",
                        "ref_id": "BIBREF21"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "The output of BERT attention layers are token embedding vectors. One output vector for one token aggregates contextual information from the whole sentence. In the vector space, attention can produce strong representations for syntactic phenomena and phrasal information (Jawahar et al., 2019) , but small improvements on semantic tasks (Tenney et al., 2019) . Ethayarajh (2019) showed that contextualized representations of all words are not isotropic in any layer. Cai et al. (2021) revealed isotropy in the clustered contextual embedding space, and found low-dimensional manifolds. In the fine-tuning process, Kovaleva et al. (2019) showed that the last two attention layers encode task-specific features while earlier layers capture more fundamental information.",
                "cite_spans": [
                    {
                        "start": 270,
                        "end": 292,
                        "text": "(Jawahar et al., 2019)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 336,
                        "end": 357,
                        "text": "(Tenney et al., 2019)",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 360,
                        "end": 377,
                        "text": "Ethayarajh (2019)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 466,
                        "end": 483,
                        "text": "Cai et al. (2021)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 612,
                        "end": 634,
                        "text": "Kovaleva et al. (2019)",
                        "ref_id": "BIBREF13"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "Another set of works focus on the ability of extracting linguistic knowledge. BERT can obtain syntactic dependencies, parts of speech tags, word disambiguation, and so forth (Ethayarajh, 2019; Vig and Belinkov, 2019; Clark et al., 2019; Goldberg, 2019) . It showed that the same layer learns similar knowledge (Clark et al., 2019) . Positional information is encoded in BERT lower layers (Lin et al., 2019) . Vig and Belinkov (2019) argued that middle and last attention layers can extract dependency relations and distant information, respectively. Even with BERT's success, it still struggles handling some linguistic information and tasks. BERT does not excel at numbers, negation, inferences and role-based event prediction (Wallace et al., 2019; Ettinger, 2020) . It's questionable to provide transparency or meaningful explanations for model predictions on downtream tasks (Jain and Wallace, 2019) .",
                "cite_spans": [
                    {
                        "start": 174,
                        "end": 192,
                        "text": "(Ethayarajh, 2019;",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 193,
                        "end": 216,
                        "text": "Vig and Belinkov, 2019;",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 217,
                        "end": 236,
                        "text": "Clark et al., 2019;",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 237,
                        "end": 252,
                        "text": "Goldberg, 2019)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 310,
                        "end": 330,
                        "text": "(Clark et al., 2019)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 388,
                        "end": 406,
                        "text": "(Lin et al., 2019)",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 409,
                        "end": 432,
                        "text": "Vig and Belinkov (2019)",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 728,
                        "end": 750,
                        "text": "(Wallace et al., 2019;",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 751,
                        "end": 766,
                        "text": "Ettinger, 2020)",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 879,
                        "end": 903,
                        "text": "(Jain and Wallace, 2019)",
                        "ref_id": "BIBREF10"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "Since self-attention is the fundamental mechanism in BERT, existing works also investigate extracted attention vectors and/or matrices. These are most relevant to our study. Clark et al. (2019) and Kovaleva et al. (2019) found some common attention patterns, such as patterns on delimiter tokens, block, and heterogeneous patterns. Also, redundancy and overparameterization is discovered in attention heads. By using attention-based probing classifiers, Clark et al. (2019) showed that heads in the same layer often exhibits similar behaviors. Attention heads can be pruned with different strategies but keep comparable performance in downstream tasks (Voita et al., 2019; Clark et al., 2019) . Some layers can even be reduced to a single head (Michel et al., 2019) .",
                "cite_spans": [
                    {
                        "start": 174,
                        "end": 193,
                        "text": "Clark et al. (2019)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 198,
                        "end": 220,
                        "text": "Kovaleva et al. (2019)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 454,
                        "end": 473,
                        "text": "Clark et al. (2019)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 652,
                        "end": 672,
                        "text": "(Voita et al., 2019;",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 673,
                        "end": 692,
                        "text": "Clark et al., 2019)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 744,
                        "end": 765,
                        "text": "(Michel et al., 2019)",
                        "ref_id": "BIBREF18"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "Understanding attention redundancy can help interpret pre-trained/fine-tuned language models and guide model compression. But no systematic study on attention redundancy exists. This paper provides a deep study (Five-Ws and How) on the attention redundancy.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "In this paper, we use BERT-base model as a representative to study the attention redundancy existing in multi-layer multi-head self-attention mechanisms. As a pre-trained language model, BERT has been verified with outstanding fine-tuning performances on many downstream language understanding tasks.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "What: Redundancy Matrices",
                "sec_num": "3"
            },
            {
                "text": "In BERT-base model there are 12 self-attention layers each of which consists of 12 self-attention heads. For one input (sentence or sentence pair with special tokens [CLS] and [SEP]), we extract 12 \u00d7 12 attention matrices. The size of each matrix is n \u00d7 n where n is the number of tokens after BERT tokenization. Given a metric (e.g., distance function) and extracted attention matrices, we can measure the relationship between each pair of the 144 attention matrices, for instance the 144 \u00d7 144 matrix in Figure 1 . We define the pair-wise relationship among the 144 heads (i.e., attention matrices) as redundancy matrix. We consider that the smaller the distances among some attention heads are, the more attention redundancy exists in them.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 513,
                        "end": 514,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "What: Redundancy Matrices",
                "sec_num": "3"
            },
            {
                "text": "As for the studying objects, we use the wellknown natural language understanding benchmark GLUE task1 (Wang et al., 2018) as evaluation objects for the attention redundancy analysis. Among GLUE, CoLA is for English sentence acceptability judgments. SST-2 and STS-B are sentiment analyses tasks. MRPC and QQP are for sentencepair similarity classification. MNLI, QNLI, and RTE are natural language inference tasks. CoLA and SST-2 are single sentence tasks, while others are predicting relationship between a pair of sentences. Except that STS-B is a regression task, in the BERT framework, others are formed as classification tasks.",
                "cite_spans": [
                    {
                        "start": 102,
                        "end": 121,
                        "text": "(Wang et al., 2018)",
                        "ref_id": "BIBREF30"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "What: Redundancy Matrices",
                "sec_num": "3"
            },
            {
                "text": "In experiments, we randomly select 1000 data samples from each development set2 as data instances and report the averaged results. We feed them to (pre-trained and fine-tuned) BERT-base model to generate attention matrices.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "What: Redundancy Matrices",
                "sec_num": "3"
            },
            {
                "text": "We use PyTorch BERT-base model as the pretrained model3 (Wolf et al., 2019) . For finetuning, we train each task with suggested hyperparameters max-length=128, num-epoch=3, batchsize-per-GPU=32 on 8 GPUs. All results in this paper are averaged over 5 trials.",
                "cite_spans": [
                    {
                        "start": 56,
                        "end": 75,
                        "text": "(Wolf et al., 2019)",
                        "ref_id": "BIBREF32"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "What: Redundancy Matrices",
                "sec_num": "3"
            },
            {
                "text": "A fundamental question about attention redundancy is how to measure the similarity or distance between two attention matrices. The smaller the distance is, the more redundancy should exist. In this section, we introduce two levels of distance functions, token-based and sentence-based distances. The aim is to inspect their interpretabilities and consistency on quantifying the attention redundancy and to alleviate measuring bias of just using a certain distance function.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "How: Distance Functions",
                "sec_num": "4"
            },
            {
                "text": "Let dist(A i , A j ) be a distance function to measure the relationship between the ith and jth attention matrices A i and A j . Note that we reshape the 12 layers\u00d712 heads into 144 attention matrices because the inputs of dist are two matrices. Intuitively, there should be three properties in a defined distance function.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "How: Distance Functions",
                "sec_num": "4"
            },
            {
                "text": "(i) dist(A i , A i ) = 0; (ii) the distance function should be symmetric, i.e., dist(A i , A j ) = dist(A j , A i ); (iii) If A i is more similar to A j than A k , then dist(A i , A j ) < dist(A i , A k ).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "How: Distance Functions",
                "sec_num": "4"
            },
            {
                "text": "So for the following distances, we modify them according to these requirements and normalize them into range [0,1] for easy comparisons.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "How: Distance Functions",
                "sec_num": "4"
            },
            {
                "text": "Token-Based Distance For one input sentence, we can extract 144 attention weight matrices. Each matrix is \u2208 [0, 1] n\u00d7n where n is the number of tokens in the sentence. Then for each token in a sentence, we get 144 attention vectors (\u2208 [0, 1] n ). We can compute the pairwise distance of them and average on the n tokens to obtain a final scalar value for a pair of attention matrices.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Two Levels of Distances",
                "sec_num": "4.1"
            },
            {
                "text": "Four token-based distance functions are generated from the following: cosine similarity (cos), Pearson correlation coefficient (corr), Jensen-Shannon distance (JS), and Bhattacharyya coefficient (BC). Note that for readability, please refer to Appendix A for detailed descriptions, implementations, and our modifications.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Two Levels of Distances",
                "sec_num": "4.1"
            },
            {
                "text": "Sentence-Based Distance Unlike the tokenbased distances, here we directly measure the distance between two n \u00d7 n attention matrices corresponding to the whole input sentence. We modified three measures: distance correlation (dCor) (Sz\u00e9kely et al., 2007) , Procrustes coefficient (PC) (Gower, 1971) , and Canonical correlation coefficient (CC) (Hotelling, 1992) . Please refer to Appendix A for more details.",
                "cite_spans": [
                    {
                        "start": 231,
                        "end": 253,
                        "text": "(Sz\u00e9kely et al., 2007)",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 284,
                        "end": 297,
                        "text": "(Gower, 1971)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 343,
                        "end": 360,
                        "text": "(Hotelling, 1992)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Two Levels of Distances",
                "sec_num": "4.1"
            },
            {
                "text": "In general, sentence-based measures care about covariance and/or linear/nonlinear dependency for the given two attention matrices while token-based ones only focus on two single attention vectors.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Two Levels of Distances",
                "sec_num": "4.1"
            },
            {
                "text": "We take CoLA with pre-trained BERT-base as an example to visualize the redundancy matrices based on the two sets of distance functions. We also conduct visualization for other GLUE tasks in Appendix B. Shown in Figure 2 , each heatmap shows the normalized distances (averaged over the 1000 selected CoLA development data) using one distance function. Each entry reflects the distance between a pair of attention heads. We can see clear common redundancy patterns existing in the redundancy matrices based on token-based distances. Similar observations are shown for sentence-based distances.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 218,
                        "end": 219,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Visualization of Redundancy Matrices",
                "sec_num": "4.2"
            },
            {
                "text": "In this section, we discuss the redundancy patterns in details and show that similar redundancy patterns exist in redundancy matrices when using the four token-based distance functions. So do for the three sentence-based distance functions.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Where: Redundancy in Layers",
                "sec_num": "5"
            },
            {
                "text": "There are two observations in Figure 2 . First, attention heads in the same layer and consecutive layers ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 37,
                        "end": 38,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Redundancy Patterns",
                "sec_num": "5.1"
            },
            {
                "text": "Next, we show that similar redundancy patterns occur when the four token-based distances are employed. So do the three sentence-based distances. In Figure 3 , we compute the correlation 4 between each pair of the redundancy matrices (with CoLA and pre-trained BERT-base) in Figure 2 . High correlations are observed in redundancy matrices of each type of distances. Note that we also provide experimental results for other tasks with both pretrained and fine-tuned BERT-base model in Appendix C.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 155,
                        "end": 156,
                        "text": "3",
                        "ref_id": "FIGREF2"
                    },
                    {
                        "start": 281,
                        "end": 282,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Similar Patterns",
                "sec_num": "5.2"
            },
            {
                "text": "As for the different cluster structures for tokenbased and sentence-based distance functions, the reason is related to the information captured by different distance functions. Sentence-based measures consider the covariance or nonlinear relation-4 Similar normalization and modification in Section 4.1 are conducted without being subtracted by 1. 6 When: Redundancy Is Phase-Independent",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Similar Patterns",
                "sec_num": "5.2"
            },
            {
                "text": "As a pre-trained language model, BERT is finetuned for downstream tasks. In this section, we check the redundancy patterns in both pre-trained and fine-tuned BERT-base model.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Similar Patterns",
                "sec_num": "5.2"
            },
            {
                "text": "Figure 4 shows the attention redundancy matrices of fine-tuned BERT-base for CoLA using the two levels of distances (results for other tasks are shown in Appendix B). Very similar redundancy patterns to those in Figure 2 can be observed. To quantify the similarity, we simply compute the absolute values of differences between two corresponding redundancy matrices in Figure 2 and Figure 4 . The mean difference value is just 0.035. In addition, we compute correlations between pretrained and fine-tuned BERT-base for each GLUE task under each distance function. As shown in Figure 5 , high correlations are shown in pre-trained and fine-tuned phases for all GLUE tasks and all distance functions.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "4",
                        "ref_id": null
                    },
                    {
                        "start": 219,
                        "end": 220,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    },
                    {
                        "start": 375,
                        "end": 376,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    },
                    {
                        "start": 388,
                        "end": 389,
                        "text": "4",
                        "ref_id": null
                    },
                    {
                        "start": 582,
                        "end": 583,
                        "text": "5",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Similar Patterns",
                "sec_num": "5.2"
            },
            {
                "text": "To summarize, attention redundancy patterns in pre-trained and fine-tuned phases are very similar and highly correlated.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Similar Patterns",
                "sec_num": "5.2"
            },
            {
                "text": "In this section, we analyze the attention redundancy from the task-oriented perspective. Specifically, for each distance function and each phase, we compute the correlation between redundancy matrices of each pair of tasks. The correlation results for pre-trained BERT-base model under different distances are shown in Figure 6 (results for fine-tuned BERT-base are presented in Appendix D). We find that the redundancy patterns across tasks are very similar with each other. Based on the task categories provided in Wang et al. (2018) , we can also notice that relatively higher correlations occur in the two single-sentence tasks (CoLA and SST-2), the three similarity and paraphrase tasks (MRPC, QQP, and STS-B), and the three inference tasks (MNLI, QNLI, and RTE), respectively.",
                "cite_spans": [
                    {
                        "start": 517,
                        "end": 535,
                        "text": "Wang et al. (2018)",
                        "ref_id": "BIBREF30"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 326,
                        "end": 327,
                        "text": "6",
                        "ref_id": "FIGREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Who: Redundancy Is Task-Agnostic",
                "sec_num": "7"
            },
            {
                "text": "To further check the task-agnostic observation, we even randomly generate 1000 token sequences with various lengths (uniformly distributed) as input for the pre-trained BERT-base (\"RANDOM\" in Figure 6 ). We can observe high correlations among redundancy patterns of GLUE datasets and random inputs.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 199,
                        "end": 200,
                        "text": "6",
                        "ref_id": "FIGREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Who: Redundancy Is Task-Agnostic",
                "sec_num": "7"
            },
            {
                "text": "We conclude that attention redundancy patterns are task-agnostic. We think that it may be caused by the input formatting in BERT. In fine-tuning, the special token [CLS] and [SEP] are inserted in the beginning and at the end of one input sentence, respectively. If there are two sentences as input, they are also delimited by the special token [SEP]. It is found that [CLS] and [SEP] may dominate the attention distribution in some attention heads (Clark et al., 2019; Kovaleva et al., 2019) such that high similarities are observed through different tasks.",
                "cite_spans": [
                    {
                        "start": 448,
                        "end": 468,
                        "text": "(Clark et al., 2019;",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 469,
                        "end": 491,
                        "text": "Kovaleva et al., 2019)",
                        "ref_id": "BIBREF13"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Who: Redundancy Is Task-Agnostic",
                "sec_num": "7"
            },
            {
                "text": "Based on this surprising discovery, we can apply this task-agnostic observation for pruning redundant attention heads. Without knowing any data of downstream task, we conduct a simple but effective clustering-based zero-shot pruning strategy based on attention redundancy in the following.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Who: Redundancy Is Task-Agnostic",
                "sec_num": "7"
            },
            {
                "text": "In this section, we introduce a simple zero-shot pruning strategy based on the task-agnostic cluster structure of attention redundancy matrices. Note that there are some complicated pruning strategies in the literature, such as (Tang et al., 2019; Jiao et al., 2019; Fan et al., 2019; Wang et al., 2019; McCarley, 2019) . Most of them compress pretrained models during or after fine-tuning to re- x-axis is the pruning ratio. Performances can be preserved after pruning even 75 -85% attention heads for most tasks.",
                "cite_spans": [
                    {
                        "start": 228,
                        "end": 247,
                        "text": "(Tang et al., 2019;",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 248,
                        "end": 266,
                        "text": "Jiao et al., 2019;",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 267,
                        "end": 284,
                        "text": "Fan et al., 2019;",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 285,
                        "end": 303,
                        "text": "Wang et al., 2019;",
                        "ref_id": "BIBREF31"
                    },
                    {
                        "start": 304,
                        "end": 319,
                        "text": "McCarley, 2019)",
                        "ref_id": "BIBREF17"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Case Study Application: Zero-Shot Attention Head Pruning",
                "sec_num": "7.1"
            },
            {
                "text": "duce the computational load in the inference phase. However, our proposed pruning strategy is before fine-tuning and zero-shot (i.e., without knowing any data in fine-tuning tasks).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Case Study Application: Zero-Shot Attention Head Pruning",
                "sec_num": "7.1"
            },
            {
                "text": "Comparing and evaluating existing pruning works are beyond the scope of this paper. We leave the pruning topic as a future work. However, we believe that attention redundancy analyses in this paper benefit developing efficient pruning methods.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Case Study Application: Zero-Shot Attention Head Pruning",
                "sec_num": "7.1"
            },
            {
                "text": "The whole procedure is as the following. First, we feed randomly generated token sequences as input data in BERT-base model and obtain the redundancy matrices like Figure 2 . Second, a clustering algorithm is applied on one redundancy matrix or the averaged redundancy matrices. If a pruning ratio p is given and the clustering method needs a given number of cluster, then we set the cluster number to be 144 \u00d7 (1 -p) . Otherwise, a cluster-number-free clustering method is preferred. Third, a clustering goodness metric is used for each object (i.e., attention head) to obtain the cluster representative head which is kept during pruning. We can simply prune trainable parameters corresponding to other heads and relevant parameters in subsequent feed-forward layers, then fine-tune as usual for downstream tasks.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 171,
                        "end": 172,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Pruning Method",
                "sec_num": "7.1.1"
            },
            {
                "text": "In this section, we applied the well-known spectral clustering 6 and Silhouettee Score 7 as the clustering goodness metric to obtain cluster representative heads. We prune attention heads using the same obtained pruning strategy (since the pruning is zero-shot and task-agnostic) and fine-tune the pruned BERT-base model for each GLUE task with suggested hyper-parameters in Section 3. The finetuned performances on development sets are shown in Figure 7 . Each sub-figure corresponds to one task. x-axis is the pruning ratio between 5% and 95% with an interval of 5%. We conduct 10 trials for each fine-tuning task. Red dashed line reflects the average performance without pruning 8 . Blue line plots averaged performances under different pruning ratios. Boundaries of shadow areas cover the best and worst results of the 10 trials.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 453,
                        "end": 454,
                        "text": "7",
                        "ref_id": "FIGREF5"
                    }
                ],
                "eq_spans": [],
                "section": "Pruning Performance",
                "sec_num": "7.1.2"
            },
            {
                "text": "In Figure 7 , not surprisingly, performances drop as the pruning ratio increases. However, in 4 (SST-2, MRPC, QQP and RTE) out of these 8 tasks , we could use a pruning ratio as big as 85%, without significant performance loss (< 5%) against an unpruned model. In MNLI and QNLI, the performance loss is bigger. But still, the pruning ratio In each heatmap, the title shows the pruning ratio (larger pruning ratios reflect that more heads are pruned). The lighter the entry color is, the more often the head gets pruned. We observe that heads in earlier and deeper layers are pruned with high chances under larger pruning ratios. Please refer to Section 7.1.3 for more details. can be as big as 75% to guarantee a small performance loss (< 5%). The only two outliers are CoLA and STS-B. We think their smaller validation sets (\u2264 1000) may result in fluctuations and prevent us from making a strong conclusion, and a further study is deferred to future work.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 10,
                        "end": 11,
                        "text": "7",
                        "ref_id": "FIGREF5"
                    }
                ],
                "eq_spans": [],
                "section": "Pruning Performance",
                "sec_num": "7.1.2"
            },
            {
                "text": "In Figure 8 , we visualize some pruning results averaged over 10 clustering trials. Lighter colors represent that one head is pruned more often. When pruning ratios are small (e.g., <15%), heads in the first four layers are pruned with higher chances than those in further layers. When pruning ratio increases (25%\u223c75%), heads in earlier and deeper layers are pruned more often. However, some heads are always kept, for example, Head-2,6,10,12 in Layer-11 and Head-1,3,12 in Layer-12. In extreme cases (85% and 95% pruning ratios), pruning these heads hurts fine-tuning performances (Figure 7 ). Interestingly, along all pruning ratios (including cases of 85% and 95%), some heads in the middle layers are kept with high chances.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 10,
                        "end": 11,
                        "text": "8",
                        "ref_id": "FIGREF6"
                    },
                    {
                        "start": 591,
                        "end": 592,
                        "text": "7",
                        "ref_id": "FIGREF5"
                    }
                ],
                "eq_spans": [],
                "section": "Pruning Heads Visualization",
                "sec_num": "7.1.3"
            },
            {
                "text": "We observe that heads in the first four layers are always very likely to be pruned. This verifies the cluster structure in redundancy matrices (e.g., Figure 2 ). This may due to the fact that heads in earlier layers capture more superficial linguistic features (Kovaleva et al., 2019) which might be less informative in fine-tuning than other heads. As pruning ratio increases, heads in deeper layers are also more likely to be pruned, than those in the middle layers. We conjecture that after the middle layers, the contextualized embeddings are already very \"strong\" for down-stream tasks. Therefore the deeper layers do not require too many heads (though a few are left) to handle the task.",
                "cite_spans": [
                    {
                        "start": 261,
                        "end": 284,
                        "text": "(Kovaleva et al., 2019)",
                        "ref_id": "BIBREF13"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 157,
                        "end": 158,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Pruning Heads Visualization",
                "sec_num": "7.1.3"
            },
            {
                "text": "To summarize the case study of pruning, we emphasize that the proposed simple but robust redundancy clustering based pruning method is task- The effects of dropout ratio on the attention redundancy. \"N\"-shape is shown on the left (tokenbased JS distance). Almost monotonic effects exist on the right (sentence-based dCor distance): higher dropout results in more redundancy. Redundancy is more sensitive to hidden-dropout-ratio than attentionhead-dropout-ratio.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Pruning Heads Visualization",
                "sec_num": "7.1.3"
            },
            {
                "text": "agnostic and zero-shot. Compared to other pruning methods, (i) it requires no data from downstream tasks; (ii) one pruning strategy obtained from the pre-trained model can robustly prune less informative heads and preserve comparable finetuning performances for all GLUE downstream tasks. The powerful strength of the simple pruning strategy results from the phase-independent and task-independent attention redundancy patterns existing in BERT-base model. As a future work, we would check if similar attention redundancy patterns exist in other multi-layer multi-head selfattention based models and develop corresponding pruning mechanisms.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Pruning Heads Visualization",
                "sec_num": "7.1.3"
            },
            {
                "text": "It is hard to answer the fundamental question why attention redundancy exists. However, in this section, we examine one factor that may be the cause. Dropout in the pre-training phase is suspected to be one reason resulting in redundant attentions (Clark et al., 2019) . But Clark et al. (2019) didn't evaluate that. In this section, we investigate effects of various dropout ratios in the pre-training process on the attention redundancy. In BERT pretraining, there are two dropout ratios, attentionhead-dropout-ratio and hidden-dropout-ratio. The former randomly deactivates a ratio of trainable weights in the attention head (key, query, and value transformation matrices). The latter deactivates some weights in the linear transformation matrices which integrate all attention heads after each attention layer. Their default values are 0.1.",
                "cite_spans": [
                    {
                        "start": 248,
                        "end": 268,
                        "text": "(Clark et al., 2019)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 275,
                        "end": 294,
                        "text": "Clark et al. (2019)",
                        "ref_id": "BIBREF2"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "\"Why\": Effects of Pre-Training Dropouts",
                "sec_num": "8"
            },
            {
                "text": "We manually set various values (from 0.0 to 0.9) for those two dropout ratios and train a BERT masked language model on Wiki103 training dataset9 from scratch. We train 100 epochs or until training loss converges. Then we randomly select 1000 sentences from the test dataset as objects.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "\"Why\": Effects of Pre-Training Dropouts",
                "sec_num": "8"
            },
            {
                "text": "For each sentence, we calculate the mean value of attention redundancy matrix using JS and dCor distance. The final reported values are averaged over the 1000 sentences.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "\"Why\": Effects of Pre-Training Dropouts",
                "sec_num": "8"
            },
            {
                "text": "Results of three settings are shown in Figure 9 . \"Only attn\" means that we only change the attention-head-dropout-ratio and keep the hiddendropout-ratio as the default value 0.1. \"Only hidden\" is changing hidden-dropout-ratio and keeping attention-head-dropout-ratio as default. \"Attn & hidden\" modifies both dropout ratios. In both figures, smaller distance values reflect heavier attention redundancy.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 46,
                        "end": 47,
                        "text": "9",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "\"Why\": Effects of Pre-Training Dropouts",
                "sec_num": "8"
            },
            {
                "text": "We can see that the token-based measure JS and sentence-based measure dCor show reversed trends in the middle range ([0,2 0.7]). For JS on all three settings, when dropout ratio increases (i.e., more inactive weights updating in pre-training) the distances first drop down to the lowest values (when dropout ratio is 0.2) and increase and then drop again heavily. On the other hand, for dCor, the distance decreases (heavier attention redundancy) along the increased dropout ratio.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "\"Why\": Effects of Pre-Training Dropouts",
                "sec_num": "8"
            },
            {
                "text": "We conclude that dropout ratio does not play a simple effect on the attention redundancy. The \"N\"-shape effects shown in the left figure (tokenbased distance JS) is demonstrated from the view of attention vectors' similarity. On the other hand, the sentence-based distance (dCor) figure shows that higher dropout ratios result in more redundancy 10 .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "\"Why\": Effects of Pre-Training Dropouts",
                "sec_num": "8"
            },
            {
                "text": "In both figures, we observe that slopes of \"Attn & hidden\" and \"Only hidden\" are steeper than that of \"Only attn\". This means that dropout in the hidden transformations affects the attention redundancy more than the attention dropout.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "\"Why\": Effects of Pre-Training Dropouts",
                "sec_num": "8"
            },
            {
                "text": "There is no doubt that more factors must affect the attention redundancy. We leave the \"why\" in future study.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "\"Why\": Effects of Pre-Training Dropouts",
                "sec_num": "8"
            },
            {
                "text": "Using BERT-base model as an example, we comprehensively investigated the attention redundancy in multi-layer multi-head self-attention based language models. The redundancy was measured by distance functions at token level and sentence level. At both levels, we found that many heads are not distinct from each other, and clear clustering effects were observed. We discovered that the attention redundancy is phase-independent and taskagnostic. Specifically, compared to a pre-trained model, the redundancy patterns do not change much after fine-tuning on multiple downstream tasks. We also shown complex influences on redundancy of dropout ratios in hidden transformations and self-attention. Based on these discoveries, we design a zero-shot strategy to prune attention heads. Compared to existing methods, the zero-shot pruning is simple and robust (task-agnostic). In the near future, we are interested in experimenting this method over more self-attention based pre-trained language models and more downstream tasks. This is the appendix for NAACL-HLT 2021 paper: Yuchen Bian, Jiaji Huang, Xingyu Cai, Jiahong Yuan, and Kenneth Church. On Attention Redundancy: A Comprehensive Study.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "9"
            },
            {
                "text": "We provide detailed descriptions of distance functions and our modifications in this section. Most of them exist in python scipy and sklearn packages. For others, we also provide implementation references or implement by ourselves. But more mathematical details are beyond this paper's scope. Please refer to original papers or (Josse and Holmes, 2016) for discussions.",
                "cite_spans": [
                    {
                        "start": 328,
                        "end": 352,
                        "text": "(Josse and Holmes, 2016)",
                        "ref_id": "BIBREF12"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Distance Functions",
                "sec_num": null
            },
            {
                "text": "For an input sentence, each token corresponds to 144 attention vectors.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A.1 Token-Based Distances",
                "sec_num": null
            },
            {
                "text": "Let p, q be two attention vectors or attention distributions (since the sum of each attention vector is 1). Four modified distance functions used in this paper are:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A.1 Token-Based Distances",
                "sec_num": null
            },
            {
                "text": "\u2022 cosine similarity (cos): Since cos(p, q) for a pair of distribution vectors is bounded in [0, 1], we modify it with 1 -cos(p, q) to keep the distance properties in Section 4.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A.1 Token-Based Distances",
                "sec_num": null
            },
            {
                "text": "\u2022 Pearson correlation coefficient (corr): We normalize as (1 -corr(p, q))/2 due to its range [-1, 1].",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A.1 Token-Based Distances",
                "sec_num": null
            },
            {
                "text": "\u2022 Jensen-Shannon distance (JS): Its range is [0,1] and it's consistent with the distance properties. It is also used in the literature (Clark et al., 2019; Jain and Wallace, 2019) to measure the distance of two attention distributions.",
                "cite_spans": [
                    {
                        "start": 135,
                        "end": 155,
                        "text": "(Clark et al., 2019;",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 156,
                        "end": 179,
                        "text": "Jain and Wallace, 2019)",
                        "ref_id": "BIBREF10"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A.1 Token-Based Distances",
                "sec_num": null
            },
            {
                "text": "\u2022 Bhattacharyya coefficient (BC) (Bhattacharyya, 1946) : It measures the amount of overlap between two statistical populations.",
                "cite_spans": [
                    {
                        "start": 33,
                        "end": 54,
                        "text": "(Bhattacharyya, 1946)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A.1 Token-Based Distances",
                "sec_num": null
            },
            {
                "text": "It can be used to determine the relative closeness of the two attention vectors. BC(p, q) = x p(x)q(x). Its range is [0, 1], and we modify it by 1 -BC(p, q).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A.1 Token-Based Distances",
                "sec_num": null
            },
            {
                "text": "For an input sentence, let A i and A j be arbitrary two attention matrices among the 144 attention matrices extracted from the BERT-base model.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A.2 Sentence-Based Distances",
                "sec_num": null
            },
            {
                "text": "\u2022 Distance correlation11 (dCor) (Sz\u00e9kely et al., 2007) : It is introduced to address the deficiency of Pearson's correlation which is sensitive to a linear relationship between two variables. It's widely used in statistical community. It's based on nonlinear operation and the range is [0, 1] where 0 is got when two sets of random variables are independent. We modify it as 1-dCor(A i , A j ).",
                "cite_spans": [
                    {
                        "start": 32,
                        "end": 54,
                        "text": "(Sz\u00e9kely et al., 2007)",
                        "ref_id": "BIBREF23"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A.2 Sentence-Based Distances",
                "sec_num": null
            },
            {
                "text": "\u2022 Procrustes coefficient (PC) (Gower, 1971) : It can measure the closeness of two data matrices. It's also known as Lingoes and Sch\u00f6nemann (RLS) coefficient (Legendre and FORTIN, 2010) . It varies from 0 to 1 and can be used as a distance measure.",
                "cite_spans": [
                    {
                        "start": 30,
                        "end": 43,
                        "text": "(Gower, 1971)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 157,
                        "end": 184,
                        "text": "(Legendre and FORTIN, 2010)",
                        "ref_id": "BIBREF14"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A.2 Sentence-Based Distances",
                "sec_num": null
            },
            {
                "text": "\u2022 Canonical correlation coefficient (CC) (Hotelling, 1992 ): It's a famous method to study the relationship between two sets of variables. It's defined as the trace of a matrix combining the covariance of two input data matrices. We modify it as 1-CC(A i , A j ).",
                "cite_spans": [
                    {
                        "start": 41,
                        "end": 57,
                        "text": "(Hotelling, 1992",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A.2 Sentence-Based Distances",
                "sec_num": null
            },
            {
                "text": "In this section, we provide attention redundancy matrices visualization results for GLUE datasets and randomly generated token sequences (Figure 10 to Figure 18 ).",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 145,
                        "end": 147,
                        "text": "10",
                        "ref_id": "FIGREF0"
                    },
                    {
                        "start": 158,
                        "end": 160,
                        "text": "18",
                        "ref_id": "FIGREF8"
                    }
                ],
                "eq_spans": [],
                "section": "B Attention Redundancy Matrices",
                "sec_num": null
            },
            {
                "text": "In this section, we provide more consistency results of attention redundancy patterns measured based on token-based and sentence-based distances for GLUE tasks in Figure 19 including both pre-trained and fine-tuned BERT-base model.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 170,
                        "end": 172,
                        "text": "19",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "C Consistency of Redundancy Patterns in GLUE Tasks",
                "sec_num": null
            },
            {
                "text": "In this section, we show the cross-task correlation results of attention redundancy patterns measured based on different distances for GLUE tasks and random inputs in fine-tuned BERT-base model in Figure 20 . ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 204,
                        "end": 206,
                        "text": "20",
                        "ref_id": "FIGREF9"
                    }
                ],
                "eq_spans": [],
                "section": "D Cross-Task Correlations of Redundancy Patterns in GLUE Tasks",
                "sec_num": null
            },
            {
                "text": "We eliminate WNLI due to its very small size. The same elimination was conducted in the literature(Kovaleva et al.,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "2019).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "If the total number of development data is less than 1000, we select all data.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://github.com/huggingface/transformers",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "dCor distance = 1 is equivalent to that the original dCor correlation = 0 since we modify it with being subtracted by 1.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://blog.einstein.ai/the-wikitext-long-termdependency-language-modeling-dataset/",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "10 Note that we use 1 -original distance correlation for consistency among distance functions. Original distance cor-relation=0 means that two sets of random variables are independent.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://github.com/vnmabus/dcor",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": " 1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9 101112   Layer   cos   1 2 3 4 5 6 7 8 9 101112   Layer   corr   1 2 3 4 5 6 7 8 9 101112   Layer   BC   1 2 3 4 5 6 7 8 9 101112   Layer   dCor   1 2 3 4 5 6 7 8 9 101112   Layer   PC   1 2 3 4 5 6 7 8 9 ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 1,
                        "end": 18,
                        "text": "1 2 3 4 5 6 7 8 9",
                        "ref_id": null
                    },
                    {
                        "start": 19,
                        "end": 36,
                        "text": "1 2 3 4 5 6 7 8 9",
                        "ref_id": null
                    },
                    {
                        "start": 37,
                        "end": 259,
                        "text": "1 2 3 4 5 6 7 8 9 101112   Layer   cos   1 2 3 4 5 6 7 8 9 101112   Layer   corr   1 2 3 4 5 6 7 8 9 101112   Layer   BC   1 2 3 4 5 6 7 8 9 101112   Layer   dCor   1 2 3 4 5 6 7 8 9 101112   Layer   PC   1 2 3 4 5 6 7 8 9",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "annex",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "On a measure of divergence between two multinomial populations. Sankhy\u0101: the indian journal of statistics",
                "authors": [
                    {
                        "first": "Anil",
                        "middle": [],
                        "last": "Bhattacharyya",
                        "suffix": ""
                    }
                ],
                "year": 1946,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "401--406",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Anil Bhattacharyya. 1946. On a measure of divergence be- tween two multinomial populations. Sankhy\u0101: the indian journal of statistics, pages 401-406.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Isotropy in the contextual embedding space: Clusters and manifolds",
                "authors": [
                    {
                        "first": "Xingyu",
                        "middle": [],
                        "last": "Cai",
                        "suffix": ""
                    },
                    {
                        "first": "Jiaji",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Yuchen",
                        "middle": [],
                        "last": "Bian",
                        "suffix": ""
                    },
                    {
                        "first": "Kenneth",
                        "middle": [],
                        "last": "Church",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "ICLR",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xingyu Cai, Jiaji Huang, Yuchen Bian, and Kenneth Church. 2021. Isotropy in the contextual embedding space: Clusters and manifolds. In ICLR.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "What does bert look at? an analysis of bert's attention",
                "authors": [
                    {
                        "first": "Kevin",
                        "middle": [],
                        "last": "Clark",
                        "suffix": ""
                    },
                    {
                        "first": "Urvashi",
                        "middle": [],
                        "last": "Khandelwal",
                        "suffix": ""
                    },
                    {
                        "first": "Omer",
                        "middle": [],
                        "last": "Levy",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1906.04341"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christo- pher D Manning. 2019. What does bert look at? an analysis of bert's attention. arXiv preprint arXiv:1906.04341.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
                "authors": [
                    {
                        "first": "Jacob",
                        "middle": [],
                        "last": "Devlin",
                        "suffix": ""
                    },
                    {
                        "first": "Ming-Wei",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "Kenton",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Kristina",
                        "middle": [],
                        "last": "Toutanova",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "NAACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In NAACL.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "How contextual are contextualized word representations? comparing the geometry of bert, elmo, and gpt-2 embeddings",
                "authors": [
                    {
                        "first": "Kawin",
                        "middle": [],
                        "last": "Ethayarajh",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1909.00512"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Kawin Ethayarajh. 2019. How contextual are contextu- alized word representations? comparing the geometry of bert, elmo, and gpt-2 embeddings. arXiv preprint arXiv:1909.00512.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "What bert is not: Lessons from a new suite of psycholinguistic diagnostics for language models",
                "authors": [
                    {
                        "first": "Allyson",
                        "middle": [],
                        "last": "Ettinger",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Transactions of the Association for Computational Linguistics",
                "volume": "8",
                "issue": "",
                "pages": "34--48",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Allyson Ettinger. 2020. What bert is not: Lessons from a new suite of psycholinguistic diagnostics for language mod- els. Transactions of the Association for Computational Linguistics, 8:34-48.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Reducing transformer depth on demand with structured dropout",
                "authors": [
                    {
                        "first": "Angela",
                        "middle": [],
                        "last": "Fan",
                        "suffix": ""
                    },
                    {
                        "first": "Edouard",
                        "middle": [],
                        "last": "Grave",
                        "suffix": ""
                    },
                    {
                        "first": "Armand",
                        "middle": [],
                        "last": "Joulin",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Angela Fan, Edouard Grave, and Armand Joulin. 2019. Reduc- ing transformer depth on demand with structured dropout. In ICLR.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Assessing bert's syntactic abilities",
                "authors": [
                    {
                        "first": "Yoav",
                        "middle": [],
                        "last": "Goldberg",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1901.05287"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yoav Goldberg. 2019. Assessing bert's syntactic abilities. arXiv preprint arXiv:1901.05287.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Statistical methods of comparing different multivariate analyses of the same data",
                "authors": [
                    {
                        "first": "J",
                        "middle": [
                            "C"
                        ],
                        "last": "Gower",
                        "suffix": ""
                    }
                ],
                "year": 1971,
                "venue": "Mathematics in the archaeological and historical sciences",
                "volume": "",
                "issue": "",
                "pages": "138--149",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "JC Gower. 1971. Statistical methods of comparing different multivariate analyses of the same data. Mathematics in the archaeological and historical sciences, pages 138-149.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Relations between two sets of variates",
                "authors": [
                    {
                        "first": "Harold",
                        "middle": [],
                        "last": "Hotelling",
                        "suffix": ""
                    }
                ],
                "year": 1992,
                "venue": "Breakthroughs in statistics",
                "volume": "",
                "issue": "",
                "pages": "162--190",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Harold Hotelling. 1992. Relations between two sets of variates. In Breakthroughs in statistics, pages 162-190. Springer.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Attention is not explanation",
                "authors": [
                    {
                        "first": "Sarthak",
                        "middle": [],
                        "last": "Jain",
                        "suffix": ""
                    },
                    {
                        "first": "Byron",
                        "middle": [
                            "C"
                        ],
                        "last": "Wallace",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "NAACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sarthak Jain and Byron C Wallace. 2019. Attention is not explanation. In NAACL.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "What does bert learn about the structure of language?",
                "authors": [
                    {
                        "first": "Ganesh",
                        "middle": [],
                        "last": "Jawahar",
                        "suffix": ""
                    },
                    {
                        "first": "Beno\u00eet",
                        "middle": [],
                        "last": "Sagot",
                        "suffix": ""
                    },
                    {
                        "first": "Djam\u00e9",
                        "middle": [],
                        "last": "Seddah ; Xiaoqi",
                        "suffix": ""
                    },
                    {
                        "first": "Yichun",
                        "middle": [],
                        "last": "Jiao",
                        "suffix": ""
                    },
                    {
                        "first": "Lifeng",
                        "middle": [],
                        "last": "Yin",
                        "suffix": ""
                    },
                    {
                        "first": "Xin",
                        "middle": [],
                        "last": "Shang",
                        "suffix": ""
                    },
                    {
                        "first": "Xiao",
                        "middle": [],
                        "last": "Jiang",
                        "suffix": ""
                    },
                    {
                        "first": "Linlin",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Fang",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Qun",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Tinybert: Distilling bert for natural language understanding",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1909.10351"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ganesh Jawahar, Beno\u00eet Sagot, and Djam\u00e9 Seddah. 2019. What does bert learn about the structure of language? In ACL. Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. 2019. Tinybert: Distill- ing bert for natural language understanding. arXiv preprint arXiv:1909.10351.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Measuring multivariate association and beyond",
                "authors": [
                    {
                        "first": "Julie",
                        "middle": [],
                        "last": "Josse",
                        "suffix": ""
                    },
                    {
                        "first": "Susan",
                        "middle": [],
                        "last": "Holmes",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Statistics surveys",
                "volume": "10",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Julie Josse and Susan Holmes. 2016. Measuring multivariate association and beyond. Statistics surveys, 10:132.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Revealing the dark secrets of bert",
                "authors": [
                    {
                        "first": "Olga",
                        "middle": [],
                        "last": "Kovaleva",
                        "suffix": ""
                    },
                    {
                        "first": "Alexey",
                        "middle": [],
                        "last": "Romanov",
                        "suffix": ""
                    },
                    {
                        "first": "Anna",
                        "middle": [],
                        "last": "Rogers",
                        "suffix": ""
                    },
                    {
                        "first": "Anna",
                        "middle": [],
                        "last": "Rumshisky",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1908.08593"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Olga Kovaleva, Alexey Romanov, Anna Rogers, and Anna Rumshisky. 2019. Revealing the dark secrets of bert. arXiv preprint arXiv:1908.08593.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Comparison of the mantel test and alternative approaches for detecting complex multivariate relationships in the spatial analysis of genetic data",
                "authors": [
                    {
                        "first": "Pierre",
                        "middle": [],
                        "last": "Legendre",
                        "suffix": ""
                    },
                    {
                        "first": "Marie-Jos\u00e9e",
                        "middle": [],
                        "last": "Fortin",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Molecular ecology resources",
                "volume": "10",
                "issue": "5",
                "pages": "831--844",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Pierre Legendre and MARIE-JOS\u00c9E FORTIN. 2010. Com- parison of the mantel test and alternative approaches for detecting complex multivariate relationships in the spatial analysis of genetic data. Molecular ecology resources, 10(5):831-844.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Open sesame: Getting inside bert's linguistic knowledge",
                "authors": [
                    {
                        "first": "Yongjie",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Yi",
                        "middle": [],
                        "last": "Chern Tan",
                        "suffix": ""
                    },
                    {
                        "first": "Robert",
                        "middle": [],
                        "last": "Frank",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1906.01698"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yongjie Lin, Yi Chern Tan, and Robert Frank. 2019. Open sesame: Getting inside bert's linguistic knowledge. arXiv preprint arXiv:1906.01698.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Roberta: A robustly optimized bert pretraining approach",
                "authors": [
                    {
                        "first": "Yinhan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Myle",
                        "middle": [],
                        "last": "Ott",
                        "suffix": ""
                    },
                    {
                        "first": "Naman",
                        "middle": [],
                        "last": "Goyal",
                        "suffix": ""
                    },
                    {
                        "first": "Jingfei",
                        "middle": [],
                        "last": "Du",
                        "suffix": ""
                    },
                    {
                        "first": "Mandar",
                        "middle": [],
                        "last": "Joshi",
                        "suffix": ""
                    },
                    {
                        "first": "Danqi",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Omer",
                        "middle": [],
                        "last": "Levy",
                        "suffix": ""
                    },
                    {
                        "first": "Mike",
                        "middle": [],
                        "last": "Lewis",
                        "suffix": ""
                    },
                    {
                        "first": "Luke",
                        "middle": [],
                        "last": "Zettlemoyer",
                        "suffix": ""
                    },
                    {
                        "first": "Veselin",
                        "middle": [],
                        "last": "Stoyanov",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1907.11692"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man- dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A ro- bustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Pruning a bert-based question answering model",
                "authors": [
                    {
                        "first": "Mccarley",
                        "middle": [],
                        "last": "Scott",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1910.06360"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "J Scott McCarley. 2019. Pruning a bert-based question an- swering model. arXiv preprint arXiv:1910.06360.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Are sixteen heads really better than one",
                "authors": [
                    {
                        "first": "Paul",
                        "middle": [],
                        "last": "Michel",
                        "suffix": ""
                    },
                    {
                        "first": "Omer",
                        "middle": [],
                        "last": "Levy",
                        "suffix": ""
                    },
                    {
                        "first": "Graham",
                        "middle": [],
                        "last": "Neubig",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Paul Michel, Omer Levy, and Graham Neubig. 2019. Are sixteen heads really better than one? In NeurIPS.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Improving language understanding by generative pre-training",
                "authors": [
                    {
                        "first": "Alec",
                        "middle": [],
                        "last": "Radford",
                        "suffix": ""
                    },
                    {
                        "first": "Karthik",
                        "middle": [],
                        "last": "Narasimhan",
                        "suffix": ""
                    },
                    {
                        "first": "Tim",
                        "middle": [],
                        "last": "Salimans",
                        "suffix": ""
                    },
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training. URL https://s3-us-west-2. amazonaws. com/openai- assets/researchcovers/languageunsupervised/language understanding paper. pdf.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Language models are unsupervised multitask learners",
                "authors": [
                    {
                        "first": "Alec",
                        "middle": [],
                        "last": "Radford",
                        "suffix": ""
                    },
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Rewon",
                        "middle": [],
                        "last": "Child",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Luan",
                        "suffix": ""
                    },
                    {
                        "first": "Dario",
                        "middle": [],
                        "last": "Amodei",
                        "suffix": ""
                    },
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "OpenAI Blog",
                "volume": "1",
                "issue": "8",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI Blog, 1(8):9.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "A primer in bertology: What we know about how bert works",
                "authors": [
                    {
                        "first": "Anna",
                        "middle": [],
                        "last": "Rogers",
                        "suffix": ""
                    },
                    {
                        "first": "Olga",
                        "middle": [],
                        "last": "Kovaleva",
                        "suffix": ""
                    },
                    {
                        "first": "Anna",
                        "middle": [],
                        "last": "Rumshisky",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2002.12327"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Anna Rogers, Olga Kovaleva, and Anna Rumshisky. 2020. A primer in bertology: What we know about how bert works. arXiv preprint arXiv:2002.12327.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Ernie 2.0: A continual pre-training framework for language understanding",
                "authors": [
                    {
                        "first": "Yu",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Shuohuan",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Yukun",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Shikun",
                        "middle": [],
                        "last": "Feng",
                        "suffix": ""
                    },
                    {
                        "first": "Hua",
                        "middle": [],
                        "last": "Hao Tian",
                        "suffix": ""
                    },
                    {
                        "first": "Haifeng",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1907.12412"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Hao Tian, Hua Wu, and Haifeng Wang. 2019. Ernie 2.0: A continual pre-training framework for language understanding. arXiv preprint arXiv:1907.12412.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Measuring and testing dependence by correlation of distances. The annals of statistics",
                "authors": [
                    {
                        "first": "J",
                        "middle": [],
                        "last": "G\u00e1bor",
                        "suffix": ""
                    },
                    {
                        "first": "Maria",
                        "middle": [
                            "L"
                        ],
                        "last": "Sz\u00e9kely",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Rizzo",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Nail K Bakirov",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "",
                "volume": "35",
                "issue": "",
                "pages": "2769--2794",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "G\u00e1bor J Sz\u00e9kely, Maria L Rizzo, Nail K Bakirov, et al. 2007. Measuring and testing dependence by correlation of dis- tances. The annals of statistics, 35(6):2769-2794.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Distilling task-specific knowledge from bert into simple neural networks",
                "authors": [
                    {
                        "first": "Raphael",
                        "middle": [],
                        "last": "Tang",
                        "suffix": ""
                    },
                    {
                        "first": "Yao",
                        "middle": [],
                        "last": "Lu",
                        "suffix": ""
                    },
                    {
                        "first": "Linqing",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Lili",
                        "middle": [],
                        "last": "Mou",
                        "suffix": ""
                    },
                    {
                        "first": "Olga",
                        "middle": [],
                        "last": "Vechtomova",
                        "suffix": ""
                    },
                    {
                        "first": "Jimmy",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1903.12136"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vech- tomova, and Jimmy Lin. 2019. Distilling task-specific knowledge from bert into simple neural networks. arXiv preprint arXiv:1903.12136.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "What do you learn from context? probing for sentence structure in contextualized word representations",
                "authors": [
                    {
                        "first": "Ian",
                        "middle": [],
                        "last": "Tenney",
                        "suffix": ""
                    },
                    {
                        "first": "Patrick",
                        "middle": [],
                        "last": "Xia",
                        "suffix": ""
                    },
                    {
                        "first": "Berlin",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Adam",
                        "middle": [],
                        "last": "Poliak",
                        "suffix": ""
                    },
                    {
                        "first": "Thomas",
                        "middle": [],
                        "last": "Mccoy",
                        "suffix": ""
                    },
                    {
                        "first": "Najoung",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "Benjamin",
                        "middle": [],
                        "last": "Van Durme",
                        "suffix": ""
                    },
                    {
                        "first": "Samuel",
                        "middle": [],
                        "last": "Bowman",
                        "suffix": ""
                    },
                    {
                        "first": "Dipanjan",
                        "middle": [],
                        "last": "Das",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "ICLR",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy, Najoung Kim, Benjamin Van Durme, Samuel Bowman, Dipanjan Das, et al. 2019. What do you learn from context? probing for sentence structure in contextualized word representations. In ICLR.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Attention is all you need",
                "authors": [
                    {
                        "first": "Ashish",
                        "middle": [],
                        "last": "Vaswani",
                        "suffix": ""
                    },
                    {
                        "first": "Noam",
                        "middle": [],
                        "last": "Shazeer",
                        "suffix": ""
                    },
                    {
                        "first": "Niki",
                        "middle": [],
                        "last": "Parmar",
                        "suffix": ""
                    },
                    {
                        "first": "Jakob",
                        "middle": [],
                        "last": "Uszkoreit",
                        "suffix": ""
                    },
                    {
                        "first": "Llion",
                        "middle": [],
                        "last": "Jones",
                        "suffix": ""
                    },
                    {
                        "first": "Aidan",
                        "middle": [
                            "N"
                        ],
                        "last": "Gomez",
                        "suffix": ""
                    },
                    {
                        "first": "\u0141ukasz",
                        "middle": [],
                        "last": "Kaiser",
                        "suffix": ""
                    },
                    {
                        "first": "Illia",
                        "middle": [],
                        "last": "Polosukhin",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "NeurIPS",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In NeurIPS.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Analyzing the structure of attention in a transformer language model",
                "authors": [
                    {
                        "first": "Jesse",
                        "middle": [],
                        "last": "Vig",
                        "suffix": ""
                    },
                    {
                        "first": "Yonatan",
                        "middle": [],
                        "last": "Belinkov",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1906.04284"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jesse Vig and Yonatan Belinkov. 2019. Analyzing the struc- ture of attention in a transformer language model. arXiv preprint arXiv:1906.04284.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned",
                "authors": [
                    {
                        "first": "Elena",
                        "middle": [],
                        "last": "Voita",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Talbot",
                        "suffix": ""
                    },
                    {
                        "first": "Fedor",
                        "middle": [],
                        "last": "Moiseev",
                        "suffix": ""
                    },
                    {
                        "first": "Rico",
                        "middle": [],
                        "last": "Sennrich",
                        "suffix": ""
                    },
                    {
                        "first": "Ivan",
                        "middle": [],
                        "last": "Titov",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "ACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. 2019. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. In ACL.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Do nlp models know numbers? probing numeracy in embeddings",
                "authors": [
                    {
                        "first": "Eric",
                        "middle": [],
                        "last": "Wallace",
                        "suffix": ""
                    },
                    {
                        "first": "Yizhong",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Sujian",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Sameer",
                        "middle": [],
                        "last": "Singh",
                        "suffix": ""
                    },
                    {
                        "first": "Matt",
                        "middle": [],
                        "last": "Gardner",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "EMNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Eric Wallace, Yizhong Wang, Sujian Li, Sameer Singh, and Matt Gardner. 2019. Do nlp models know numbers? prob- ing numeracy in embeddings. In EMNLP.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "Glue: A multitask benchmark and analysis platform for natural language understanding",
                "authors": [
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Amanpreet",
                        "middle": [],
                        "last": "Singh",
                        "suffix": ""
                    },
                    {
                        "first": "Julian",
                        "middle": [],
                        "last": "Michael",
                        "suffix": ""
                    },
                    {
                        "first": "Felix",
                        "middle": [],
                        "last": "Hill",
                        "suffix": ""
                    },
                    {
                        "first": "Omer",
                        "middle": [],
                        "last": "Levy",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Samuel R Bowman",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1804.07461"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 2018. Glue: A multi- task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461.",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "Structured pruning of large language models",
                "authors": [
                    {
                        "first": "Ziheng",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Jeremy",
                        "middle": [],
                        "last": "Wohlwend",
                        "suffix": ""
                    },
                    {
                        "first": "Tao",
                        "middle": [],
                        "last": "Lei",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1910.04732"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ziheng Wang, Jeremy Wohlwend, and Tao Lei. 2019. Struc- tured pruning of large language models. arXiv preprint arXiv:1910.04732.",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "Huggingface's transformers: State-of-the-art natural language processing",
                "authors": [
                    {
                        "first": "Thomas",
                        "middle": [],
                        "last": "Wolf",
                        "suffix": ""
                    },
                    {
                        "first": "Lysandre",
                        "middle": [],
                        "last": "Debut",
                        "suffix": ""
                    },
                    {
                        "first": "Victor",
                        "middle": [],
                        "last": "Sanh",
                        "suffix": ""
                    },
                    {
                        "first": "Julien",
                        "middle": [],
                        "last": "Chaumond",
                        "suffix": ""
                    },
                    {
                        "first": "Clement",
                        "middle": [],
                        "last": "Delangue",
                        "suffix": ""
                    },
                    {
                        "first": "Anthony",
                        "middle": [],
                        "last": "Moi",
                        "suffix": ""
                    },
                    {
                        "first": "Pierric",
                        "middle": [],
                        "last": "Cistac",
                        "suffix": ""
                    },
                    {
                        "first": "Tim",
                        "middle": [],
                        "last": "Rault",
                        "suffix": ""
                    },
                    {
                        "first": "R'emi",
                        "middle": [],
                        "last": "Louf",
                        "suffix": ""
                    },
                    {
                        "first": "Morgan",
                        "middle": [],
                        "last": "Funtowicz",
                        "suffix": ""
                    },
                    {
                        "first": "Jamie",
                        "middle": [],
                        "last": "Brew",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1910.03771"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R'emi Louf, Morgan Funtowicz, and Jamie Brew. 2019. Huggingface's transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 1: Pair-wise Jensen-Shannon distance of attention heads for the pre-trained BERT-base model (12layer-12-head self-attention). Attention redundancy (cluster with small distances) exists in adjacent attention heads and layers.",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Figure 2: Redundancy matrices in the pre-trained BERT-base model for CoLA development data using various distance functions. The left four and right three are for token-based and sentence-based distances, respectively. Similar attention redundancy patterns (cluster with small distances) exist in each set of distance functions.",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "Figure 3: Correlation in two types of redundancy matrices (with pre-trained BERT-base for CoLA). There are high correlations among token-based and sentencebased distances, respectively.",
                "uris": null,
                "fig_num": "3",
                "type_str": "figure"
            },
            "FIGREF3": {
                "num": null,
                "text": "Figure 4: Redundancy matrices in fine-tuned BERT-base for CoLA. They are very similar to redundancy patterns in pre-trained BERT-base shown in Figure 2.",
                "uris": null,
                "fig_num": "45",
                "type_str": "figure"
            },
            "FIGREF4": {
                "num": null,
                "text": "Figure 6: Correlations of redundancy matrices of task pairs (using pre-trained BERT-base). Redundancy patterns are task-agnostic. They are very similar across different tasks, even with random inputs.",
                "uris": null,
                "fig_num": "6",
                "type_str": "figure"
            },
            "FIGREF5": {
                "num": null,
                "text": "Figure 7: Performances of pruned BERT-base model for GLUE tasks.x-axis is the pruning ratio. Performances can be preserved after pruning even 75 -85% attention heads for most tasks.",
                "uris": null,
                "fig_num": "7",
                "type_str": "figure"
            },
            "FIGREF6": {
                "num": null,
                "text": "Figure 8: Chances of a head being pruned at various pruning ratios when using our redundancy clustering based zero-shot pruning strategy (BERT-base model with random inputs). Results are averaged over 10 clustering trials.In each heatmap, the title shows the pruning ratio (larger pruning ratios reflect that more heads are pruned). The lighter the entry color is, the more often the head gets pruned. We observe that heads in earlier and deeper layers are pruned with high chances under larger pruning ratios. Please refer to Section 7.1.3 for more details.",
                "uris": null,
                "fig_num": "8",
                "type_str": "figure"
            },
            "FIGREF7": {
                "num": null,
                "text": "Figure9: The effects of dropout ratio on the attention redundancy. \"N\"-shape is shown on the left (tokenbased JS distance). Almost monotonic effects exist on the right (sentence-based dCor distance): higher dropout results in more redundancy. Redundancy is more sensitive to hidden-dropout-ratio than attentionhead-dropout-ratio.",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF8": {
                "num": null,
                "text": "Figure 18: Redundancy matrices in pre-trained BERT-base for randomly generated token sequences",
                "uris": null,
                "fig_num": "18",
                "type_str": "figure"
            },
            "FIGREF9": {
                "num": null,
                "text": "Figure 19: Correlation of redundancy matrices in BERT-base for GLUE tasks and random inputs",
                "uris": null,
                "fig_num": "20",
                "type_str": "figure"
            }
        }
    }
}