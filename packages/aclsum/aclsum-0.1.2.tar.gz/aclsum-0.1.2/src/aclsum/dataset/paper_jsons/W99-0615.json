{
    "paper_id": "W99-0615",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:00:53.299757Z"
    },
    "title": "HMM Specialization with Selective Lexicalization*",
    "authors": [
        {
            "first": "Jin-Dong",
            "middle": [],
            "last": "Kim",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Korea University",
                "location": {
                    "addrLine": "Anam-dong, Seongbuk-ku",
                    "postCode": "136-701",
                    "settlement": "Seoul",
                    "country": "Korea"
                }
            },
            "email": ""
        },
        {
            "first": "Sang-Zoo",
            "middle": [],
            "last": "Lee",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Korea University",
                "location": {
                    "addrLine": "Anam-dong, Seongbuk-ku",
                    "postCode": "136-701",
                    "settlement": "Seoul",
                    "country": "Korea"
                }
            },
            "email": ""
        },
        {
            "first": "Hae-Chang",
            "middle": [],
            "last": "Rim",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Korea University",
                "location": {
                    "addrLine": "Anam-dong, Seongbuk-ku",
                    "postCode": "136-701",
                    "settlement": "Seoul",
                    "country": "Korea"
                }
            },
            "email": ""
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "We present a technique which complements Hidden Markov Models by incorporating some lexicalized states representing syntactically uncommon words. 'Our approach examines the distribution of transitions, selects the uncommon words, and makes lexicalized states for the words. We perfor'med a part-of-speech tagging experiment on the Brown corpus to evaluate the resultant language model and discovered that this technique improved the tagging accuracy by 0.21% at the 95% level of confidence.",
    "pdf_parse": {
        "paper_id": "W99-0615",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "We present a technique which complements Hidden Markov Models by incorporating some lexicalized states representing syntactically uncommon words. 'Our approach examines the distribution of transitions, selects the uncommon words, and makes lexicalized states for the words. We perfor'med a part-of-speech tagging experiment on the Brown corpus to evaluate the resultant language model and discovered that this technique improved the tagging accuracy by 0.21% at the 95% level of confidence.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Hidden Markov 'Models are widely used for statistical language modelling in various fields, e.g., part-of-speech tagging or speech recognition (Rabiner and Juang, 1986) . The models are based on Markov assumptions, which make it possible to view the language prediction as a Markov process. 'In general, we make the firstorder Markov ass'umptions that the current tag is only dependant on the previous tag and that the current word is only dependant on the current tag. These are very 'strong' assumptions, so that the first-order Hidden Markov Models have the advantage of drastically reducing the number of its parameters. On the other hand, the assumptions restrict the model from utilizing enough constraints provided by the local context and the resultant model consults only a single category 'as the contex.",
                "cite_spans": [
                    {
                        "start": 143,
                        "end": 168,
                        "text": "(Rabiner and Juang, 1986)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "A lot of effort has been devoted in the past to make up for the insufficient contextual information of the first-order probabilistic model. The second order Hidden Markov Models with \" The research underlying this paper was supported t) 3\" research grants fl'om Korea Science and Engineering Foundation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "appropriate smoothing techniques show better performance than the first order models and is considered a state-of-the-art technique (Merialdo, 1994; Brants, 1996) . The complexity of the model is however relatively very high considering the small improvement of the performance.",
                "cite_spans": [
                    {
                        "start": 132,
                        "end": 148,
                        "text": "(Merialdo, 1994;",
                        "ref_id": null
                    },
                    {
                        "start": 149,
                        "end": 162,
                        "text": "Brants, 1996)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Garside describes IDIOMTAG (Garside et al., 1987) which is a component of a part-ofspeech tagging system named CLAWS. ID-IOMTAG serves as a front-end to the tagger and modifies some initially assigned tags in order to reduce the amount of ambiguity to be dealt with by the tagger. IDIOMTAG can look at any combination of words and tags, with or without intervening words. By using the IDIOMTAG, CLAWS system improved tagging accuracy from 94% to 96-97%. However, the manual-intensive process of producing idiom tags is very expensive although IDIOMTAG proved fruitful.",
                "cite_spans": [
                    {
                        "start": 27,
                        "end": 49,
                        "text": "(Garside et al., 1987)",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Kupiec (Kupiec, 1992) describes a technique of augmenting the Hidden Markov Models for part-of-speech tagging by the use of networks. Besides the original states representing each part-of-speech, the network contains additional states to reduce the noun/adjective confusion, and to extend the context for predicting past participles from preceding auxiliary verbs when they are separated by adverbs. By using these additional states, the tagging system improved the accuracy from 95.7% to 96.0%. However, the additional context is chosen by analyzing the tagging errors manually.",
                "cite_spans": [
                    {
                        "start": 7,
                        "end": 21,
                        "text": "(Kupiec, 1992)",
                        "ref_id": "BIBREF5"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "An automatic refining technique for Hidden Markov Models has been proposed by Brants (Brants, 1996) . It starts with some initial first order Markov Model. Some states of the model are selected to be split or merged to take into account their predecessors. As a result, each of new states represents a extended context. With this technique, Brants reported a performance cquivalent to the second order Hidden Markov Models.",
                "cite_spans": [
                    {
                        "start": 85,
                        "end": 99,
                        "text": "(Brants, 1996)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this paper, we present an automatic refining technique for statistical language models. First, we examine the distribution of transitions of lexicalized categories. Next, we break out the uncommon ones from their categories and make new states for them. All processes are automated and the user has only to determine the extent of the breaking-out.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\"Standard\" Part-of-Speech Tagging Model based on HMM From the statistical point of view, the tagging problem can be defined as the problem of finding the proper sequence of categories c:,r~ = Cl, c2, ..., cn (n _> 1) given the sequence of words w:,n = wl, w2, ...,wn (We denote the i'th word by wi, and the category assigned to the wi by ci), which is formally defined by the following equation:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "2",
                "sec_num": null
            },
            {
                "text": "\"]-(Wl,n) -= argmaxP(Cl,nlW:,~) (1)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "2",
                "sec_num": null
            },
            {
                "text": "Charniak (Charniak et al., 1993) P(cilcl,i-l,Wl,i_l) ~ P(cilci-1)",
                "cite_spans": [
                    {
                        "start": 9,
                        "end": 32,
                        "text": "(Charniak et al., 1993)",
                        "ref_id": "BIBREF1"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "2",
                "sec_num": null
            },
            {
                "text": "(3) P(wi[cd (4) With Equation 3, we assume that the current category is independent of the previous words and only dependent on the previous category.",
                "cite_spans": [
                    {
                        "start": 4,
                        "end": 15,
                        "text": "P(wi[cd (4)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "2",
                "sec_num": null
            },
            {
                "text": "With Equation 4, we also assume that the correct word is independent of everything except the knowledge of its category. Through these assmnptions, the Hidden Markov Models have the advantage of drastically reducing the number of parameters, thereby alleviating the sparse data problem. However, as mentioned above, this model consults only a single category as context and does not utilize enough constraints provided by the local context.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "2",
                "sec_num": null
            },
            {
                "text": "3 Some Refining Techniques for HMM Tile first-order Hidden Markov Models described in the previous section provides only a single category as context. Sometimes, this first-order context is sufficient to predict the following parts-of-speech, but at other times (probably much more often) it is insufficient.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "2",
                "sec_num": null
            },
            {
                "text": "The goal of the work reported here is to develop a method that can automatically refine the Hidden Markov Models to produce a more accurate language model. We start with the careful observation on the assumptions which are made for the \"standard\" Hidden Markov Models. With the Equation 3, we assume that the current category is only dependent on the preceding category. As we know, it is not always true and this first-order Markov assumption restricts the disambiguation information witlfin the first-order context. The immediate ways of enriching the context are as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "2",
                "sec_num": null
            },
            {
                "text": "\u2022 to lexicalize the context.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "2",
                "sec_num": null
            },
            {
                "text": "\u2022 to extend the context to higher-order.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "2",
                "sec_num": null
            },
            {
                "text": "To lexiealize the context, we include the preceding word into the context. Contextual probabilities are then defined by P(eilci_l,Wi-1). To extend the context to higher-order, we extend the contextual probability to the second- The simple way of enriching the context is to extend or lexica!ize it uniformly. The uniform extension of context to the second order is feasible with an appropriate smoothing technique and is considered a state-of-the-art technique, though its complexity is very high: In the case of the Brown cerpus, we need trigrams up to the number of 0.6 million. An alternative to the uniform extension of context is the selective extension of context. Brants (Brants, 1996) takes this approach and reports a performance equivalent to the uniform extension with relatively much low complexity of the model.",
                "cite_spans": [
                    {
                        "start": 678,
                        "end": 692,
                        "text": "(Brants, 1996)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "2",
                "sec_num": null
            },
            {
                "text": "The uniform lexicalization of context is computationally prohibitively expensive: In the case of the Brown corpus, we need lexicalized bigrams up to the number of almost 3 billion. Moreover, manylof these bigrams neither contribute to the per~formance of the model, nor occur frequently enough to be estimated properly. An alternative to the uniform lexicalization is the selective lexicalization of context, which is the main topic of this paper.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "2",
                "sec_num": null
            },
            {
                "text": "Selective Lexicalization of HMM This section describes a new technique for refining the Hidden Markov Model, which we call selective lexicalization. Our approach automatically finds out s'yntactically uncommon words and makes a new state (we call it a lexiealized state)for each of the words.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "4",
                "sec_num": null
            },
            {
                "text": "Given a fixed set of categories, {C 1 , C 2, ..., cC}, e.g., {adjective,..., verb}, we assume the discrete random variable XcJ with domain the set of categories and range a set of conditional probabilities. The random variable XcJ then represents a process of assigning a conditional probability p(cilc j) to every category c i (e i ranges over cl ...c C)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "4",
                "sec_num": null
            },
            {
                "text": "xc (c = P(d = P(c21cJ) Xc) (c C) = p(cClc j)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "4",
                "sec_num": null
            },
            {
                "text": "We convert the process of Xcj into the state transition vector, VcJ , which consists of the corresponding conditional probabilities, e.g.,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "4",
                "sec_num": null
            },
            {
                "text": "Vprep ----( P(adjectiveiprep), ..., P(verbiprep) ) T.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "4",
                "sec_num": null
            },
            {
                "text": "The (squared) distance between two arbitrary vectors is then computed as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "4",
                "sec_num": null
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "l~(Vl, V2) = (Vl --v2)T(v1 -V2)",
                        "eq_num": "(5)"
                    }
                ],
                "section": "4",
                "sec_num": null
            },
            {
                "text": "Similarly, we define the lexicalized state transition vector 1, VO,wk , e.g.,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "4",
                "sec_num": null
            },
            {
                "text": "In this situation, it is possible to regard each lexicMized state transition vector, VcJ,wk, of the same category cJ as members of a cluster whose centroid is the state transition vector, Vc). We can then compute the deviation of each lexicalized state transition vector, Vc~,wk , from its corresponding centroid. From the viewpoint of a network, the state representing preposition is split into two states; the one is the state representing ordinary prepositions except out, and the other is the state representing the special preposition out, which we call a lexicalized state. This process of splitting is illustrated in Figure 4 .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 631,
                        "end": 632,
                        "text": "4",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Vprep,i n -~ ( P (adjectivelprep, in),..., P (verblprep, in)) Y",
                "sec_num": null
            },
            {
                "text": "Splitting a state results in some changes of the parameters. The changes of the parameters resulting from lexicalizing a word, w k, in a category, C j, are indicated in Table 1 (c i ranges over cl...cC). This full splitting will increase the complexity of the model rapidly, so that estimating the parameters may suffer from the sparseness of the data.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 175,
                        "end": 176,
                        "text": "1",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Vprep,i n -~ ( P (adjectivelprep, in),..., P (verblprep, in)) Y",
                "sec_num": null
            },
            {
                "text": "To alleviate it, we use the pseudo splitting which leads to relatively small increment of the ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Vprep,i n -~ ( P (adjectivelprep, in),..., P (verblprep, in)) Y",
                "sec_num": null
            },
            {
                "text": "sentences were chosen at random, from which we collected all of the statistical data. We reserved the other 10% for testing. Table 3 lists the basic statistics of our corpus. We used a tag set containing 85 categories. The amount of ambiguity of the test set is summarized in Table 4 . The second column shows that words to the ratio of 52% (the number of 57,808) are not ambiguous. The tagger attempts to resolve the ambiguity of the remaining words. 6 show the results of our part-of-speech tagging experiments with the \"standard\" Hidden Markov Model and variously lexicalized Hidden Markov Models using full splitting method and pseudo splitting method respectively.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 131,
                        "end": 132,
                        "text": "3",
                        "ref_id": "TABREF2"
                    },
                    {
                        "start": 282,
                        "end": 283,
                        "text": "4",
                        "ref_id": "TABREF3"
                    },
                    {
                        "start": 452,
                        "end": 453,
                        "text": "6",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "P(elc P(c lc')",
                "sec_num": null
            },
            {
                "text": "We got 95.7858% of the tags correct when we applied the standard Hidden Markov Model without any lexicalized states. As the number of lexicalized states increases, the tagging accuracy increases until the number of lexicalized states becomes 160 (using full splitting) and 210 (using pseudo splitting). As you can see in these figures, the full splitting improves the performance of the model more rapidly but suffer more sevelery from the sparseness of the training data. In this experiment, we employed Mackay and Peto's smoothing techniques for estimating the parameters required for the models. The best precision has been found to be 95:9966% through the model with the 210 lexcalized states using the pseudo splitting method.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "P(elc P(c lc')",
                "sec_num": null
            },
            {
                "text": "In this paper, we present a method for complementing the Hidden Markov Models. With this method, we lexicalize the Hidden Markov Model seletively and automatically by examining the transition distribution of each state relating to certain words.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "6"
            },
            {
                "text": "Experimental results showed that the selective lexicalization improved the tagging accurary from about 95.79% to about 96.00%. Using normal tests for statistical significance we found that the improvement is significant at the 95% level of confidence. Tile cost for this imt~rovenmnt is minimal. The resulting network contains 210 additional lexicalized states which are found automatically. Moreover, the lexicalization will not decrease the tagging speed 2, because the lexicalized states and their corresponding original states are exclusive in our lexicalized network, and thus the rate of ambiguity is not increased even if the lexicalized states are included.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "6"
            },
            {
                "text": "Our approach leaves much room for improvement. We have so far considered only the outgoing transitions from the target states. As a result, we have discriminated only the words with right-associativity. We could also discriminate the words with left-associativity by examining the incoming transitions to the state. Furthermore, we could extend the context by using the second-order context as represented in Figure l(c ). We believe that the same technique presented in this paper could be applied to the proposed extensions. ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 416,
                        "end": 419,
                        "text": "l(c",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "6"
            }
        ],
        "back_matter": [],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Estimating markov model structures",
                "authors": [
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Brants",
                        "suffix": ""
                    }
                ],
                "year": 1996,
                "venue": "Proceedings of the Fourth International Conference on Spoken Language Processing",
                "volume": "",
                "issue": "",
                "pages": "893--896",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "T. Brants. 1996. Estimating markov model structures. In Proceedings of the Fourth In- ternational Conference on Spoken Language Processing, pages 893-896.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Equations for part-of-speech tagging",
                "authors": [
                    {
                        "first": "E",
                        "middle": [],
                        "last": "Charniak",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Hendrickson",
                        "suffix": ""
                    },
                    {
                        "first": "N",
                        "middle": [],
                        "last": "Jacobson",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Perkowitz",
                        "suffix": ""
                    }
                ],
                "year": 1993,
                "venue": "Proceedings of the Eleventh National Conference on Artificial Intelligence",
                "volume": "",
                "issue": "",
                "pages": "784--789",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "E. Charniak, C. Hendrickson, N. Jacobson, and M. Perkowitz. 1993. Equations for part-of-speech tagging. In Proceedings of the Eleventh National Conference on Artificial Intelligence, pages 784-789.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "A stochastic parts program and noun phrase parser for unrestricted text",
                "authors": [
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Church",
                        "suffix": ""
                    }
                ],
                "year": 1988,
                "venue": "Proceedings of the Second Conference on Applied Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "136--143",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "K. Church. 1988. A stochastic parts program and noun phrase parser for unrestricted text. In Proceedings of the Second Conference on Applied Natural Language Processing, pages 136-143.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Grammatical category disambiguation by statistical optimization",
                "authors": [
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Derose",
                        "suffix": ""
                    }
                ],
                "year": 1988,
                "venue": "Computational Linguistics",
                "volume": "14",
                "issue": "1",
                "pages": "31--39",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "S. Derose. 1988. Grammatical category disam- biguation by statistical optimization. Com- putational Linguistics, 14(1):31-39.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "The Computational Analysis of English",
                "authors": [
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Garside",
                        "suffix": ""
                    },
                    {
                        "first": "G",
                        "middle": [],
                        "last": "Leech",
                        "suffix": ""
                    },
                    {
                        "first": "G",
                        "middle": [],
                        "last": "Sampson",
                        "suffix": ""
                    }
                ],
                "year": 1987,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "R. Garside, G. Leech, and G. Sampson. 1987. The Computational Analysis of En- glish. Longman Group.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Robust part-of-speech tagging using a hidden markoV model",
                "authors": [
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Kupiec",
                        "suffix": ""
                    }
                ],
                "year": 1992,
                "venue": "Computer Speech and Language",
                "volume": "6",
                "issue": "",
                "pages": "225--242",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "J. Kupiec. 1992. Robust part-of-speech tag- ging using a hidden markoV model. Computer Speech and Language, 6:225-242.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "A hierarchical dirichlet language model",
                "authors": [
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Mackay",
                        "suffix": ""
                    },
                    {
                        "first": "L",
                        "middle": [],
                        "last": "Peto",
                        "suffix": ""
                    }
                ],
                "year": 1995,
                "venue": "Natural Language Engineering",
                "volume": "1",
                "issue": "3",
                "pages": "289--307",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "D. MacKay and L. Peto. 1995. A hierarchical dirichlet language model. Natural Language Engineering, 1(3):289-307.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "The Viterbi algorithm for finding the best tags runs in O(n 2) where n is the number of states",
                "authors": [],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "2The Viterbi algorithm for finding the best tags runs in O(n 2) where n is the number of states.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 1 illustrates the change of dependency when each method is applied respectively. Figure l(a) represents that each first-order contextual probability and lexical probability are independent of each other in the \"standard\" Hidden Markov Models, where Figure l(b) represents that the lexical probability of the preceding word and the contextual probability of the current category are tied into a lexicalized contextual probability.To extend the context to higher-order, we extend the contextual probability to the second-",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Figure 1: Two Types of Weakening the Markov Assumption",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "Figure l(c) represents that the two adjacent contextual probabilities are tied into the sec0nd-order contextual probability.",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF3": {
                "num": null,
                "text": "Figure 2 represents the distribution of lexicalized state transition vectors according to their deviations. As you can see in the figure, the majority of the vectors are near their centroids and only a small number of vectors are very far from their centroids. In the first-order context model (without considering lexicalized context). 1To alleviate the sparse data problenl, we smoothed the lexicalized state transition probabilities by MacKay and Peto(MacKay and Peto, 1995)'s smoothing technique.",
                "uris": null,
                "fig_num": "24",
                "type_str": "figure"
            },
            "FIGREF4": {
                "num": null,
                "text": "Figure 3: Transition Vectors in preposition Cluster",
                "uris": null,
                "fig_num": "3",
                "type_str": "figure"
            },
            "FIGREF6": {
                "num": null,
                "text": "Figure 5: POS tagging results with lexicalized HMM using full splitting method 0.96",
                "uris": null,
                "fig_num": "56",
                "type_str": "figure"
            },
            "TABREF1": {
                "content": "<table><tr><td/><td/><td colspan=\"2\">Table 2: Changes of Parameters in Pseudo</td></tr><tr><td/><td/><td>Splitting</td><td/></tr><tr><td colspan=\"2\">before splitting after splitting</td><td colspan=\"2\">before splitting after splitting</td></tr><tr><td>P(w~lc y)</td><td>p(wilcJ, w k) p(wilc j , ~W k)</td><td>P(w'ld) P(cilc j)</td><td>P(w~l d) p(cilcJ, w k)</td></tr><tr><td>P(cil~)</td><td>P(dlcJ , w k)</td><td/><td/></tr><tr><td/><td>P(cilcJ, ~w k)</td><td/><td/></tr><tr><td>P(dlci)</td><td>P(cJ, w k Ic i)</td><td/><td/></tr><tr><td/><td>P(cJ, -~w~l ci)</td><td/><td/></tr><tr><td colspan=\"2\">parameters. The changes of the parameters in</td><td/><td/></tr><tr><td colspan=\"2\">pseudo splitting ate indicated in Table 2.</td><td/><td/></tr><tr><td colspan=\"2\">5 Experimental Result</td><td/><td/></tr><tr><td colspan=\"2\">We have tested our technique through part-of-</td><td/><td/></tr><tr><td colspan=\"2\">speech tagging eXperiments with the Hidden</td><td/><td/></tr><tr><td colspan=\"2\">Markov Models which are variously lexicalized.</td><td/><td/></tr><tr><td colspan=\"2\">In ordcr to conduct the tagging experiments, we</td><td/><td/></tr><tr><td colspan=\"2\">divided the whole Brown (tagged) corpus con-</td><td/><td/></tr><tr><td colspan=\"2\">taining 53,887 sentences (1,113,191 words) into</td><td/><td/></tr><tr><td colspan=\"2\">two parts. For tlle training set. 90% of the</td><td/><td/></tr></table>",
                "type_str": "table",
                "text": "Changes of Parameters in Full Splitting",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table><tr><td colspan=\"3\">: Overview of Our Corpora</td></tr><tr><td/><td colspan=\"2\">I # of sentences # of words</td></tr><tr><td>training set</td><td>48,499</td><td>1,001,712</td></tr><tr><td>test set</td><td>5,388</td><td>111.479</td></tr></table>",
                "type_str": "table",
                "text": "",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table><tr><td>I ambiguity(#) ratio(%)</td><td>3 4 5121:018 71 5</td><td>1 I total 100 I</td></tr><tr><td colspan=\"2\">Figure 5 and Figure</td><td/></tr></table>",
                "type_str": "table",
                "text": "Amount of Ambiguity of Test Set",
                "html": null,
                "num": null
            }
        }
    }
}