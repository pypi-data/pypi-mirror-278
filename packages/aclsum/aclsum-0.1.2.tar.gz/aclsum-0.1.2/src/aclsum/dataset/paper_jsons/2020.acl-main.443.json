{
    "paper_id": "2020",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T11:54:20.881861Z"
    },
    "title": "Code and Named Entity Recognition in StackOverflow",
    "authors": [
        {
            "first": "Jeniya",
            "middle": [],
            "last": "Tabassum",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "The Ohio State University",
                "location": {}
            },
            "email": ""
        },
        {
            "first": "Mounica",
            "middle": [],
            "last": "Maddela",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "The Ohio State University",
                "location": {}
            },
            "email": "maddela.4@osu.edu"
        },
        {
            "first": "Wei",
            "middle": [],
            "last": "Xu",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "The Ohio State University",
                "location": {}
            },
            "email": ""
        },
        {
            "first": "Alan",
            "middle": [],
            "last": "Ritter",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "The Ohio State University",
                "location": {}
            },
            "email": ""
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "There is an increasing interest in studying natural language and computer code together, as large corpora of programming texts become readily available on the Internet. For example, StackOverflow currently has over 15 million programming related questions written by 8.5 million users. Meanwhile, there is still a lack of fundamental NLP techniques for identifying code tokens or software-related named entities that appear within natural language sentences. In this paper, we introduce a new named entity recognition (NER) corpus for the computer programming domain, consisting of 15,372 sentences annotated with 20 fine-grained entity types. We trained indomain BERT representations (BERTOverflow) on 152 million sentences from Stack-Overflow, which lead to an absolute increase of +10 F 1 score over off-the-shelf BERT. We also present the SoftNER model which achieves an overall 79.10 F 1 score for code and named entity recognition on StackOverflow data. Our SoftNER model incorporates a context-independent code token classifier with corpus-level features to improve the BERTbased tagging model. 1 ",
    "pdf_parse": {
        "paper_id": "2020",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "There is an increasing interest in studying natural language and computer code together, as large corpora of programming texts become readily available on the Internet. For example, StackOverflow currently has over 15 million programming related questions written by 8.5 million users. Meanwhile, there is still a lack of fundamental NLP techniques for identifying code tokens or software-related named entities that appear within natural language sentences. In this paper, we introduce a new named entity recognition (NER) corpus for the computer programming domain, consisting of 15,372 sentences annotated with 20 fine-grained entity types. We trained indomain BERT representations (BERTOverflow) on 152 million sentences from Stack-Overflow, which lead to an absolute increase of +10 F 1 score over off-the-shelf BERT. We also present the SoftNER model which achieves an overall 79.10 F 1 score for code and named entity recognition on StackOverflow data. Our SoftNER model incorporates a context-independent code token classifier with corpus-level features to improve the BERTbased tagging model. 1 ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Recently there has been significant interest in modeling human language together with computer code (Quirk et al., 2015; Iyer et al., 2016; Yin and Neubig, 2018) , as more data becomes available on websites such as StackOverflow and GitHub. This is an ambitious yet promising direction for scaling up language understanding to richer domains. Access to domain-specific NLP tools could help a wide range of downstream applications. For example, extracting software knowledge bases from 1 Our code and data are available at: https:// github.com/jeniyat/StackOverflowNER/ text (Movshovitz-Attias and Cohen, 2015) , developing better quality measurements of StackOverflow posts (Ravi et al., 2014) , finding similar questions (Amirreza Shirani, 2019) and more. However, there is a lack of NLP resources and techniques for identifying software-related named entities (e.g., variable names or application names) within natural language texts.",
                "cite_spans": [
                    {
                        "start": 100,
                        "end": 120,
                        "text": "(Quirk et al., 2015;",
                        "ref_id": "BIBREF36"
                    },
                    {
                        "start": 121,
                        "end": 139,
                        "text": "Iyer et al., 2016;",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 140,
                        "end": 161,
                        "text": "Yin and Neubig, 2018)",
                        "ref_id": "BIBREF49"
                    },
                    {
                        "start": 574,
                        "end": 609,
                        "text": "(Movshovitz-Attias and Cohen, 2015)",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 674,
                        "end": 693,
                        "text": "(Ravi et al., 2014)",
                        "ref_id": "BIBREF37"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this paper, we present a comprehensive study that investigates the unique challenges of named entity recognition in the social computer programming domain. These named entities are often ambiguous and have implicit reliance on the accompanied code snippets. For example, the word 'list' commonly refers to a data structure, but can also be used as a variable name (Figure 1 ). In order to recognize these entities, we propose a software-related named entity recognizer (Soft-NER) that utilizes an attention network to combine the local sentence-level context with corpuslevel information extracted from the code snippets. Using our newly annotated corpus of 15,372 sentences in StackOverflow, we rigorously test our proposed SoftNER model, which outperforms BiLSTM-CRF model and fine-tuned BERT model for identifying 20 types of software-related named entities. Our key contributions are the following:",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 375,
                        "end": 376,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\u2022 A new StackOverflow NER corpus manually annotated with 20 types of named en-tities, including all in-line code within natural language sentences ( \u00a72). We demonstrate that NER in the software domain is an ideal benchmark task for testing effectiveness of contextual word representations, such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) , due to its inherent polysemy and salient reliance on context. \u2022 An in-domain trained neural SoftNER tagger for StackOveflow ( \u00a73) that can recognize 20 fine-grained named entity types related to software developing. We also tested its performance on GitHub data of readme files and issue reports. \u2022 A code token recognizer ( \u00a73.1) that utilizes StackOveflow code snippets to capture the spelling patterns of code-related tokens, and consistently improves the NER tagger. \u2022 In-domain pretrained ELMo and BERT representations ( \u00a73.3) on 152 million sentences from StackOverflow that significantly outperforms off-the-shelf ELMo and leads to more than 21 points increase in F 1 score over offthe-shelf BERT.",
                "cite_spans": [
                    {
                        "start": 303,
                        "end": 324,
                        "text": "(Peters et al., 2018)",
                        "ref_id": "BIBREF34"
                    },
                    {
                        "start": 329,
                        "end": 355,
                        "text": "BERT (Devlin et al., 2019)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Overall, our named entity tagger (SoftNER) achieves a 79.10% F 1 score on StackOverflow and 61.08% F 1 score on GitHub data for extracting the 20 software related named entity types. We believe this performance is sufficiently strong to be practically useful. We have released our data and code, including the named entity tagger, our annotated corpus, annotation guideline, a specially designed tokenizer, and pre-trained StackOverflow BERT and ELMo embeddings.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this section, we describe the construction of our StackOverflow NER corpus. We randomly selected 1,237 question-answer threads from Stack-Overflow 10-year archive (from September 2008 to March 2018) and manually annotated them with 20 types of entities. For each question, four answers were annotated, including the accepted answer, the most upvoted answer, as well as two randomly selected answers (if they exist). Table 1 shows the statistics of our corpus. 40% of the question-answer threads were double-annotated, which are used as the development and test sets in our experiments ( \u00a74). We also annotated 6,501 sentences from GitHub readme files and issue reports as additional evaluation data. ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 425,
                        "end": 426,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Annotated StackOverflow Corpus",
                "sec_num": "2"
            },
            {
                "text": "We defined and annotated 20 types of fine-grained entities, including 8 code-related entities and 12 natural language entities. Our annotation guideline was developed through several pilots and further updated with notes to resolve difficult cases as the annotation progressed.2 Each entity type was defined to encourage maximum span length (e.g., 'SGML parser' instead of 'SGML'). We annotated noun phrases without including modifiers (e.g., 'C' instead of 'Plain C'), except a few special cases (e.g., 'rich text' as a common FILE TYPE). On average, an entity contains about 1.5 tokens. While VARIABLE, FUNCTION and CLASS names mostly consist of only a single token, our annotators found that some are written as multiple tokens when mentioned in natural language text (e.g., 'array list' for 'ArrayList' in Figure 1 ). The annotators were asked to read relevant code blocks or software repositories to make a decision, if needed. Annotators also searched Google or Wikipedia to categorize unfamiliar cases.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 817,
                        "end": 818,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Annotation Schema",
                "sec_num": "2.1"
            },
            {
                "text": "The annotators were asked to update, correct, or add annotations from the user provided code markdown tags. StackOverflow users can utilize code markdowns to highlight the code entities within the natural language sentences. However, in reality, many users do not enclose the code snippets within the code tags; and sometimes use them to highlight non-code elements, such as email addresses, user names, or natural language words. While creating the StackOverflow NER corpurs, we found that 59.73% of code-related entities are not marked by the StackOverflow users. Moreover, only 75.54% of the code enclosed texts are actually code-related, while 10.12% used to are highlighting natural language texts. The rest of cases are referring to non-code entities, such as SOFTWARE NAMES and VERSIONS. While markdown tag could be a useful feature for entity segmentation ( \u00a73.1.3), we emphasize the importance of having a human annotated corpus for training and evaluating NLP tools in the software domain.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Annotation Schema",
                "sec_num": "2.1"
            },
            {
                "text": "Our corpus was annotated by four annotators who are college students majored in computer science. We used a web-based annotation tool, BRAT (Stenetorp et al., 2012) , and provided annotators with links to the original post on StackOverflow. For every iteration, each annotator was given 50 question-answer threads to annotate, 20 of which were double-annotated. An adjudicator then discussed disagreements with annotators, who also cross-checked the 30 single-annotated questions in each batch. The inter-annotator agreement is 0.62 before adjudication, measured by span-level Cohen's Kappa (Cohen, 1960) .",
                "cite_spans": [
                    {
                        "start": 140,
                        "end": 164,
                        "text": "(Stenetorp et al., 2012)",
                        "ref_id": "BIBREF42"
                    },
                    {
                        "start": 591,
                        "end": 604,
                        "text": "(Cohen, 1960)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Annotation Agreement",
                "sec_num": "2.2"
            },
            {
                "text": "To better understand the domain adaptability of our work, we further annotated the readme files and issue reports from 143 randomly sampled repositories in the GitHub dump (Gousios and Spinellis, 2012 ) (from October 29, 2007 to December 31, 2017) . We removed all the code blocks from the issue reports and readme files collected from these 143 repositories. The resulting GitHub NER dataset consists of 6,510 sentences and 10,963 entities of 20 types labeled by two inhouse annotators. The inter-annotator agreement of this dataset is 0.68, measured by span-level Cohen's Kappa.",
                "cite_spans": [
                    {
                        "start": 172,
                        "end": 200,
                        "text": "(Gousios and Spinellis, 2012",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 201,
                        "end": 225,
                        "text": ") (from October 29, 2007",
                        "ref_id": null
                    },
                    {
                        "start": 226,
                        "end": 247,
                        "text": "to December 31, 2017)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Additional GitHub Data",
                "sec_num": "2.3"
            },
            {
                "text": "We designed a new tokenizer, SOTOKENIZER, specifically for the social computer programming domain. StackOverflow and GitHub posts exhibit common features of web texts, including abbreviations, emoticons, URLs, ungrammatical sentences and spelling errors. We found that tokenization is non-trivial as many code-related tokens are mistakenly split by the existing web-text tokenizers, including the CMU Twokenizer (Gimpel et al., 2011 ), Stanford TweetTokenizer (Manning et al., 2014) , and NLTK Twitter Tokenizer (Bird et al., 2009) Therefore, we implemented a new tokenizer, using Twokenizer3 as the starting point and added additional regular expression rules to avoid splitting code-related tokens.",
                "cite_spans": [
                    {
                        "start": 412,
                        "end": 432,
                        "text": "(Gimpel et al., 2011",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 433,
                        "end": 482,
                        "text": "), Stanford TweetTokenizer (Manning et al., 2014)",
                        "ref_id": null
                    },
                    {
                        "start": 512,
                        "end": 531,
                        "text": "(Bird et al., 2009)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "StackOverflow/GitHub Tokenization",
                "sec_num": "2.4"
            },
            {
                "text": "The extraction of software-related named entities imposes significant challenges as it requires resolving a significant amount of unseen tokens, inherent polysemy, and salient reliance on context. Unlike news or biomedical data, spelling patterns and long-distance dependencies are more crucial in the software domain to resolve ambiguities and categorize unseen words. Taken in isolation, many tokens are highly ambiguous and can refer to either programming concepts or common English words, such as: 'go', 'react', 'spring', 'while', 'if ', 'select'. To address these challenges, we design the SoftNER model that leverages sentential context to disambiguate and domain-specific character representations to handle rare words. Figure 2 shows the architecture of our model, which consists of primarily three components:",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 735,
                        "end": 736,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Named Entity Recognition Models",
                "sec_num": "3"
            },
            {
                "text": "\u2022 An input embedding layer ( \u00a73.1) that extracts contextualized embeddings from the BERT base model and two new domainspecific embeddings for each word in the input sentence. \u2022 A embedding attention layer ( \u00a73.2) that combines the three word embeddings using an attention network. \u2022 A linear-CRF layer that predicts the entity type of each word using the attentive word representations from the previous layer. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Named Entity Recognition Models",
                "sec_num": "3"
            },
            {
                "text": "For each word in the input sentence, we extract in-domain BERT (Devlin et al., 2019) representations and two new domain-specific embeddings produced by (i) a Code Recognizer, which represents if a word can be part of a code entity regardless of context; and (ii) an Entity Segmenter, that predicts whether a word is part of any named entity in the given sentence. Each domain-specific embedding is created by passing a binary value, predicted by a network independent from the Soft-NER model. We describe the two standalone auxiliary models that generate these domain-based vectors below.",
                "cite_spans": [
                    {
                        "start": 58,
                        "end": 84,
                        "text": "BERT (Devlin et al., 2019)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Input Embeddings",
                "sec_num": "3.1"
            },
            {
                "text": "Texts in the software engineering domain contain programming language tokens, such as variable names or code segments, interspersed with natural language words. This makes input representations pre-trained on general book or Wikipedia texts unsuitable for software domain. We pre-trained different in-domain word embeddings, including BERT (BERTOverflow), ELMo (ELMoVerflow), and GloVe (GloVerflow) vectors on the Stack-Overflow 10-year archive4 of 152 million sentences and 2.3 billion tokens ( \u00a73.3).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "In-domain Word Embeddings",
                "sec_num": "3.1.1"
            },
            {
                "text": "Humans with prior programming knowledge can easily recognize that 'list()' is code, 'list' can be either code or a common English word, whereas 'listing' is more likely a non-code natural language token. We thus introduce a code recognition module to capture such prior probability of how likely a word can be a code token without considering any contextual information. It is worth noting that this standalone code recognition model is also useful for language-and-code research, such as retrieving code snippets based on natural language queries (Iyer et al., 2016; Giorgi and Bader, 2018; Yao et al., 2019) Our code recognition model (Code Recognizer) is a binary classifier. It utilizes language model features and spelling patterns to predict whether a word is a code entity. The input features include unigram word and 6-gram character probabilities from two language models (LMs) that are trained on the Gigaword corpus (Napoles et al., 2012 ) and all the code-snippets in the Stack-Overflow 10-year archive respectively. We also pre-trained FastText (Joulin et al., 2016) word embeddings using these code-snippets, where a word vector is represented as a sum of its character ngrams. We first transform each ngram probability into a k-dimensional vector using Gaussian binning (Maddela and Xu, 2018) , which has shown to improve the performance of neural models using numeric features (Sil et al., 2017; Liu et al., 2016; Maddela and Xu, 2018) . We then feed the vectorized features into a linear layer, concatenate the output with FastText character-level embeddings, and pass them through another hidden layer with sigmoid activation. We predict the token as a codeentity if the output probability is greater than 0.5. This binary prediction is then converted into a vector and used as an input to the SoftNER model.",
                "cite_spans": [
                    {
                        "start": 548,
                        "end": 567,
                        "text": "(Iyer et al., 2016;",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 568,
                        "end": 591,
                        "text": "Giorgi and Bader, 2018;",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 592,
                        "end": 609,
                        "text": "Yao et al., 2019)",
                        "ref_id": "BIBREF47"
                    },
                    {
                        "start": 927,
                        "end": 948,
                        "text": "(Napoles et al., 2012",
                        "ref_id": "BIBREF31"
                    },
                    {
                        "start": 1058,
                        "end": 1079,
                        "text": "(Joulin et al., 2016)",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 1285,
                        "end": 1307,
                        "text": "(Maddela and Xu, 2018)",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 1393,
                        "end": 1411,
                        "text": "(Sil et al., 2017;",
                        "ref_id": "BIBREF41"
                    },
                    {
                        "start": 1412,
                        "end": 1429,
                        "text": "Liu et al., 2016;",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 1430,
                        "end": 1451,
                        "text": "Maddela and Xu, 2018)",
                        "ref_id": "BIBREF27"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Context-independent Code Recognition",
                "sec_num": "3.1.2"
            },
            {
                "text": "The segmentation task refers to identifying entity spans without assigning entity category. Entity segmentation is simpler and less error-prone than entity recognition as it does not require a fine-grained classification of the entity types. In fact, a segmentation model (Entity Segmenter) trained on our annotated StackOverflow corpus can achieve 90.41% precision on the dev set (details in \u00a74.5), predicting whether each token is a part of entity in the given sentence. Our segmentation model fine-tunes the in-domain BERT after concatenating it with two hand-crafted features:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Entity Segmentation",
                "sec_num": "3.1.3"
            },
            {
                "text": "\u2022 Word Frequency represents the word occurrence count in the training set. As many code tokens are defined by individual users, they occur much less frequently than normal English words. In fact, code and non-code tokens have an average frequency of 1.47 and 7.41 respectively in our corpus. Moreover, ambiguous token that can be either code or non-code entities, such as 'windows', have a much higher average frequency of 92.57. To leverage this observation, we include word frequency as a feature, converting the scalar value into a k-dimensional vector by Gaussian binning (Maddela and Xu, 2018 ). \u2022 Code Markdown indicates whether the given token appears inside a code markdown tag in the StackOverflow post. It is worth noting that code tags are noisy as users do not always enclose inline code in a code tag or sometimes use the tag to highlight non-code texts (details in \u00a72.1). Nevertheless, we find it helpful to include the markdown information as a feature as it improves the performance of our segmentation model.",
                "cite_spans": [
                    {
                        "start": 576,
                        "end": 597,
                        "text": "(Maddela and Xu, 2018",
                        "ref_id": "BIBREF27"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Entity Segmentation",
                "sec_num": "3.1.3"
            },
            {
                "text": "The inclusion of hand-crafted features is influenced by Wu et al. (2018) , where word-shapes and POS tags were shown to improve the performance of sequence tagging models.",
                "cite_spans": [
                    {
                        "start": 56,
                        "end": 72,
                        "text": "Wu et al. (2018)",
                        "ref_id": "BIBREF43"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Entity Segmentation",
                "sec_num": "3.1.3"
            },
            {
                "text": "For each input word w i in the input sentence, we have three embeddings: BERT (w i1 ), Code Recognizer (w i2 ), and Entity Segmenter (w i3 ). We introduce the embedding-level attention \u03b1 it (t \u2208 {1, 2, 3}), which captures each embedding's contribution towards the meaning of the word, to combine them together. To compute \u03b1 it , we pass the input embeddings through a bidirectional GRU and generate their corresponding hidden representations h it = \u2190 --\u2192 GRU (w it ). These vectors are then passed through a non-linear layer, which outputs u it = tanh(W e h it + b e ). We introduce an embedding-level context vector u e , which is randomly initialized and updated during the training process. This context vector is combined with the hidden embedding representation using a softmax function to extract weight of the embeddings:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Embedding-Level Attention",
                "sec_num": "3.2"
            },
            {
                "text": "\u03b1 it = exp(u it T ue)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Embedding-Level Attention",
                "sec_num": "3.2"
            },
            {
                "text": "t exp(u it T ue) . Finally, we create the word vector by a weighted sum of all the information from different embeddings as word i = t \u03b1 it h it . The aggregated word vector word i is then fed into a linear-CRF layer, which predicts the entity category for each word based the BIO tagging schema.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Embedding-Level Attention",
                "sec_num": "3.2"
            },
            {
                "text": "We use PyTorch framework to implement our proposed SoftNER model and its two auxiliary components, namely code recognition and entity segmentation. The input to the SoftNER model include 850-dimensional vectors extracted from both the code recognizer and the entity segmenter.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementation Details",
                "sec_num": "3.3"
            },
            {
                "text": "We pre-trained BERT base , ELMo and GloVe vectors on 152 million sentences from the Stack-Overflow, excluding sentences from the 1,237 posts in our annotated corpus.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementation Details",
                "sec_num": "3.3"
            },
            {
                "text": "The pretraining of the 768-dimensional BERT base model with 64,000 WordPiece vocabulary took 7 days on a Google TPU. The pre-training of 1024dimensional ELMo vectors took 46 days on 3 NVIDIA Titan X Pascal GPUs. The pre-training of 300-dimensional GloVe embeddings (Pennington et al., 2014) with a frequency cut-off of 5 took 8 hours on a server with 32 CPU cores and 386 GB memory.",
                "cite_spans": [
                    {
                        "start": 265,
                        "end": 290,
                        "text": "(Pennington et al., 2014)",
                        "ref_id": "BIBREF33"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementation Details",
                "sec_num": "3.3"
            },
            {
                "text": "We train the SoftNER model and the two auxiliary models separately. Our segmentation model follows the simple BERT fine-tuning architecture except for the input, where BERT embeddings are concatenated with 100-dimensional code markdown and 10-dimensional word frequency features. We set the number of bins k to 10 for Gaussian vectorization. Our code recognition model is a feedforward network with two hidden layers and a single output node with sigmoid activation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementation Details",
                "sec_num": "3.3"
            },
            {
                "text": "In this section, we show that our SoftNER model outperforms all the previous NER approaches on the StackOverflow and GitHub data. We also discuss the factors pivotal to the performance of our model, namely pre-trained in-domain BERT embeddings and two domain-specific auxiliary tasks. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation",
                "sec_num": "4"
            },
            {
                "text": "We compare our model with the following baseline and state-of-the-art approaches:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baselines",
                "sec_num": "4.2"
            },
            {
                "text": "\u2022 A Feature-based Linear CRF model which uses the standard orthographic, context and gazetteer features, along with the code markdown tags and handcrafted regular expressions to recognize code entities (details in Appendix A).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baselines",
                "sec_num": "4.2"
            },
            {
                "text": "\u2022 A BiLSTM-CRF model with in-domain ELMo embeddings (ELMoVerflow; details in \u00a73.3). This architecture is used as the stateof-the-art baseline named-entity recognition models in various domains (Lample et al., 2016; Kulkarni et al., 2018; Dai et al., 2019) .",
                "cite_spans": [
                    {
                        "start": 193,
                        "end": 214,
                        "text": "(Lample et al., 2016;",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 215,
                        "end": 237,
                        "text": "Kulkarni et al., 2018;",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 238,
                        "end": 255,
                        "text": "Dai et al., 2019)",
                        "ref_id": "BIBREF12"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baselines",
                "sec_num": "4.2"
            },
            {
                "text": "\u2022 An Attentive BiLSTM-CRF model with in-domain ELMo embeddings as well as domain-specific embeddings from the code recognizer and the entity segmenter. This model combines these three word embeddings using an attention network and then utilizes a BiLSTM-CRF layer to predict the entity type of each input word (details in Appendix B).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baselines",
                "sec_num": "4.2"
            },
            {
                "text": "\u2022 A Fine-tuned out-of-domain BERT model where we fine-tune the original BERT base cased checkpoint5 on our annotated corpus.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baselines",
                "sec_num": "4.2"
            },
            {
                "text": "\u2022 A Fine-tuned in-domain BERT model where we fine-tune the in-domain pre-trained BERT base (BERTOverflow; details in \u00a73.3) cased checkpoint6 on our annotated corpus.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baselines",
                "sec_num": "4.2"
            },
            {
                "text": "Table 2 shows the precision (P), recall (R) and F 1 score comparison of different models evaluated on the StackOverflow NER corpus. Our Soft-NER model outperforms the existing NER approaches in all the three metrics. Fine-tuning over in-domain trained BERT (BERTOverflow), in particular, improves F 1 score by more than 10 points in comparison to using the original BERT.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "2",
                        "ref_id": "TABREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "4.3"
            },
            {
                "text": "Table 3 shows the performance comparison between in-domain and out-of-domain word embeddings. We consider off-the-shelf BERT (Devlin et al., 2019) , ELMo (Peters et al., 2018) and GloVe (Pennington et al., 2014) It is worth noting that, the performance improvements from contextual word embeddings are more pronounced on our software domain than on newswire and biomedical domains. Original ELMo and BERT outperform GloVe by 2.06 and 2.12 points in F 1 respectively on CoNLL 2003 NER task of newswire data (Peters et al., 2018; Devlin et al., 2019) . For biomedical domain, indomain ELMo outperforms out-of-domain ELMo by only 1.33 points in F 1 on the BC2GM dataset (Sheikhshabbafghi et al., 2018) .",
                "cite_spans": [
                    {
                        "start": 125,
                        "end": 146,
                        "text": "(Devlin et al., 2019)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 154,
                        "end": 175,
                        "text": "(Peters et al., 2018)",
                        "ref_id": "BIBREF34"
                    },
                    {
                        "start": 186,
                        "end": 211,
                        "text": "(Pennington et al., 2014)",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 506,
                        "end": 527,
                        "text": "(Peters et al., 2018;",
                        "ref_id": "BIBREF34"
                    },
                    {
                        "start": 528,
                        "end": 548,
                        "text": "Devlin et al., 2019)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 667,
                        "end": 698,
                        "text": "(Sheikhshabbafghi et al., 2018)",
                        "ref_id": "BIBREF40"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "3",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "In-domain vs. out-of-domain Word Embeddings",
                "sec_num": "4.4"
            },
            {
                "text": "We hypothesized that the performance gains from the in-domain contextual embeddings are largely aided by the model's ability to handle ambiguous and unseen tokens. The increase in performance is especially notable (41% -\u2192 70% accuracy) for unseen tokens, which constitute 38% of the tokens inside gold entity spans in our dataset. This experiment also demonstrates that our annotated NER corpus provides an attractive test-bed for measuring the adaptability of different contextual word representations.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "In-domain vs. out-of-domain Word Embeddings",
                "sec_num": "4.4"
            },
            {
                "text": "The domain-specific vectors produced by the Code Recognizer and the Entity Segmenter are also crucial for the overall performance of our SoftNER model. beddings, the performance also drop by 2.81 F 1 .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation of Auxiliary Systems",
                "sec_num": "4.5"
            },
            {
                "text": "In addition, we evaluate the effectiveness of our two domain-specific auxiliary systems on their respective tasks.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation of Auxiliary Systems",
                "sec_num": "4.5"
            },
            {
                "text": "Code Recognition: Table 5 compares the performance of our code recognition model with other baselines on the SLEXICON test set ( \u00a74.1), which consists of 1,000 random words from the train set of StackOverflow NER corpus classified as either a code or a non-code token. The baselines include: (i) a Most Frequent Label baseline, which assigns the most frequent label according to the human annotation in SOLEXICON train set; and (ii) a frequency baseline, which learns a threshold over token frequency in the train set of StackOverflow NER corpus using a decision tree classifier.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 24,
                        "end": 25,
                        "text": "5",
                        "ref_id": "TABREF6"
                    }
                ],
                "eq_spans": [],
                "section": "Evaluation of Auxiliary Systems",
                "sec_num": "4.5"
            },
            {
                "text": "Our model outperforms both baselines in terms of F 1 score. Although the most frequent label baseline achieves better precision than our model, it performs poorly on unseen tokens resulting in a large drop in recall and F 1 score. The ablation experiments show that the FastText word embeddings along with the character and word-level features are crucial for the code recognition model. tagger (Manning et al., 2014) , which is trained on newswire text, demonstrates the importance of domain-specific tools for the software engineering domain.",
                "cite_spans": [
                    {
                        "start": 395,
                        "end": 417,
                        "text": "(Manning et al., 2014)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation of Auxiliary Systems",
                "sec_num": "4.5"
            },
            {
                "text": "Based on our manual inspection, the incorrect predictions made by NER systems on StackOverflow data can be largely classified into the following two categories (see examples in Table 7 ):",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 183,
                        "end": 184,
                        "text": "7",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Error Analysis",
                "sec_num": "4.6"
            },
            {
                "text": "\u2022 Segmentation Mismatch refers to the cases where model predicts the boundary of entities incorrectly. Our SoftNER model reduces such segmentation errors by 89.36% compared to the fine-tuned BERTOverflow baseline. \u2022 Entity-Type Mismatch refers to the errors where a code entity (e.g., names of variables) is predicted as a non-code entity (e.g., names of devices), and vice-versa. Our SoftNER model reduces such entity type errors by 13.54% compared to the fine-tuned BERTOverflow baseline.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Error Analysis",
                "sec_num": "4.6"
            },
            {
                "text": "As illustrated in Figure 3 , our SoftNER model reduced the errors in both categories by incorporating the auxiliary outputs from segmenter and code recognizer model.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 25,
                        "end": 26,
                        "text": "3",
                        "ref_id": "FIGREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Error Analysis",
                "sec_num": "4.6"
            },
            {
                "text": "To understand the domain adaptability of our StackOverflow based SoftNER, we evaluate its performance on readme files and issue reports from 143 randomly sampled repositories in the GitHub dump (Gousios and Spinellis, 2012) . We also trained ELMo embeddings (ELMoGithub) on 4 million sentences from randomly sampled 5,000 GitHub repositories.",
                "cite_spans": [
                    {
                        "start": 194,
                        "end": 223,
                        "text": "(Gousios and Spinellis, 2012)",
                        "ref_id": "BIBREF17"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Domain Adaptation to GitHub data",
                "sec_num": "5"
            },
            {
                "text": "Table 8 shows that the performance of our Soft-NER model using StackOverflow ELMo embeddings is similar to the top performing BiLSTM-CRF model using GitHub ELMo embeddings with a difference of only 1.61 points in F 1 . We also did not observe any significant gain after adding Segmentation Mismatch Entity-Type Mismatch Table 7 : Representative examples of system errors. the code recognizer and segmenter vectors to the Github ELMo embeddings. We think one likely explanation is that GitHub data contains less coderelated tokens when compared to StackOverflow. The percentage of code-related entity tokens is 63.20% in GitHub and 77.21% in StackOverflow.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "8",
                        "ref_id": "TABREF8"
                    },
                    {
                        "start": 326,
                        "end": 327,
                        "text": "7",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Domain Adaptation to GitHub data",
                "sec_num": "5"
            },
            {
                "text": "Overall, we observe a drop of our SoftNER tagger from 79.10 F 1 on StackOverflow (Table 2 ) to 61.08 F 1 on GitHub data (Table 8 ) in F 1 due to domain mismatch. However, we believe that our NER tagger still achieves sufficient performance to be useful for applications on GitHub. 7We leave investigation of semi-supervised learning and other domain adaptation approaches for future work.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 88,
                        "end": 89,
                        "text": "2",
                        "ref_id": "TABREF3"
                    },
                    {
                        "start": 127,
                        "end": 128,
                        "text": "8",
                        "ref_id": "TABREF8"
                    }
                ],
                "eq_spans": [],
                "section": "Domain Adaptation to GitHub data",
                "sec_num": "5"
            },
            {
                "text": "The CoNLL 2003 dataset (Sang and De Meulder, 2003 ) is a widely used benchmark for named entity recognition, which contains annotated newswire text from the Reuters RCV1 corpus. State-of-the-art approaches on this dataset (Baevski et al., 2019) use a bidirectional LSTM (Lample et al., 2016; Ma and Hovy, 2016) with conditional random field (Collobert et al., 2011) and contextualized word representations (McCann et al., 2017; Peters et al., 2018; Devlin et al., 2019) . Named entity recognition has been explored for new domains and languages, such as social media (Finin et al., 2010; Ritter et al., 2011; Plank et al., 2014; Derczynski et al., 2015; Limsopatham and Collier, 2016; Aguilar et al., 2017) , biomedical texts (Collier and Kim, 2004; Greenberg et al., 2018; Kulkarni et al., 2018) , multilingual texts (Benajiba et al., 2008; Xie et al., 2018) and codeswitched corpora (Aguilar et al., 2018; Ball and Garrette, 2018) . Various methods have been investigated for handling rare entities, for example incorporating external context (Long et al., 2017) or approaches that make use of distant supervision (Choi et al., 2018; Yang et al., 2018; Onoe and Durrett, 2019) .",
                "cite_spans": [
                    {
                        "start": 4,
                        "end": 32,
                        "text": "CoNLL 2003 dataset (Sang and",
                        "ref_id": null
                    },
                    {
                        "start": 33,
                        "end": 49,
                        "text": "De Meulder, 2003",
                        "ref_id": "BIBREF39"
                    },
                    {
                        "start": 222,
                        "end": 244,
                        "text": "(Baevski et al., 2019)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 270,
                        "end": 291,
                        "text": "(Lample et al., 2016;",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 292,
                        "end": 310,
                        "text": "Ma and Hovy, 2016)",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 341,
                        "end": 365,
                        "text": "(Collobert et al., 2011)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 406,
                        "end": 427,
                        "text": "(McCann et al., 2017;",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 428,
                        "end": 448,
                        "text": "Peters et al., 2018;",
                        "ref_id": "BIBREF34"
                    },
                    {
                        "start": 449,
                        "end": 469,
                        "text": "Devlin et al., 2019)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 567,
                        "end": 587,
                        "text": "(Finin et al., 2010;",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 588,
                        "end": 608,
                        "text": "Ritter et al., 2011;",
                        "ref_id": "BIBREF38"
                    },
                    {
                        "start": 609,
                        "end": 628,
                        "text": "Plank et al., 2014;",
                        "ref_id": "BIBREF35"
                    },
                    {
                        "start": 629,
                        "end": 653,
                        "text": "Derczynski et al., 2015;",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 654,
                        "end": 684,
                        "text": "Limsopatham and Collier, 2016;",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 685,
                        "end": 706,
                        "text": "Aguilar et al., 2017)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 726,
                        "end": 749,
                        "text": "(Collier and Kim, 2004;",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 750,
                        "end": 773,
                        "text": "Greenberg et al., 2018;",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 774,
                        "end": 796,
                        "text": "Kulkarni et al., 2018)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 818,
                        "end": 841,
                        "text": "(Benajiba et al., 2008;",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 842,
                        "end": 859,
                        "text": "Xie et al., 2018)",
                        "ref_id": "BIBREF44"
                    },
                    {
                        "start": 885,
                        "end": 907,
                        "text": "(Aguilar et al., 2018;",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 908,
                        "end": 932,
                        "text": "Ball and Garrette, 2018)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 1045,
                        "end": 1064,
                        "text": "(Long et al., 2017)",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 1116,
                        "end": 1135,
                        "text": "(Choi et al., 2018;",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 1136,
                        "end": 1154,
                        "text": "Yang et al., 2018;",
                        "ref_id": "BIBREF45"
                    },
                    {
                        "start": 1155,
                        "end": 1178,
                        "text": "Onoe and Durrett, 2019)",
                        "ref_id": "BIBREF32"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "6"
            },
            {
                "text": "There has been relatively little prior work on named entity recognition in the software engineering domain. Ye et al. (2016) annotated 4,646 sentences from StackOverflow with five named entity types (Programming Language, Platform, API, Tool-Library-Framework and Software Standard). The authors used a traditional feature-based CRF to recognize these entities. In contrast, we present a much larger annotated corpus consisting of 15,372 sentences labeled with 20 fine-grained entity types. We also develop a novel attention based neural NER model to extract those finegrained entities.",
                "cite_spans": [
                    {
                        "start": 108,
                        "end": 124,
                        "text": "Ye et al. (2016)",
                        "ref_id": "BIBREF48"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "6"
            },
            {
                "text": "In this work, we investigated the task of named entity recognition in the social computer programming domain. We developed a new NER corpus of 15,372 sentences from StackOverflow and 6,510 sentences from GitHub annotated with 20 fine-grained named entities. We demonstrate that this new corpus is an ideal benchmark dataset for contextual word representations, as there are many challenging ambiguities that often require long-distance context to resolve. We also proposed a novel attention based model, named Soft-NER, that outperforms the state-of-the-art NER models on this dataset. Furthermore, we investigated the important sub-task of code recognition. Our code recognition model captures additional spelling information beyond then contextual word representations and consistently helps to improve the NER performance. We believe our corpus, StackOverflow-specific BERT embeddings and named entity tagger will be useful for various language-and-code tasks, such as code retrieval, software knowledge base extraction and automated question-answering.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "7"
            },
            {
                "text": "A Feature-Based CRF Baseline",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "7"
            },
            {
                "text": "We implemented a CRF baseline model using CRFsuite 8 to extract the software entities. This model uses standard orthographic, contextual and gazetteer features. It also includes the code markdown tags ( \u00a73.1.3) and a set of regular expression features. The regular expressions are developed to recognize specific categories of coderelated entities. Feature ablation experiments on this CRF model are presented in Table 9 ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 419,
                        "end": 420,
                        "text": "9",
                        "ref_id": "TABREF9"
                    }
                ],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "7"
            },
            {
                "text": "We propose a baseline Attentive NER model that utilizes a BiLSTM-CRF network to predict the entity type of each word from its weighted representations. The weighted word representations are extracted by a multi-level attention network, similar to Yang et al.(2016) , that combines the contextualized ELMo embeddings with the code recognizer ( \u00a73.1.2) and segmenter vector ( \u00a7C). These three input embeddings are merged together in the first attention layer and then their corresponding weights are calculated using the second layer. Although such multi-level attention is not commonly used in NER, we found it empirically helpful for the software domain (see the input sentence. The embedding-level attention \u03b1 it (t \u2208 {1, 2, 3}) to captures each embedding's contribution towards the meaning of the word. To compute \u03b1 it , it pass the input embeddings through a bidirectional GRU and generate their corresponding hidden representations h it = \u2190 --\u2192 GRU (w it ). These vectors are then passed through a non-linear layer, which outputs u it = tanh(W e h it + b e ). It uses an embedding-level context vector, u e , which is learned during the training process. This context vector is combined with the hidden embedding representation using a softmax function to extract weight of the embeddings, \u03b1 it = exp(u it T ue)",
                "cite_spans": [
                    {
                        "start": 247,
                        "end": 264,
                        "text": "Yang et al.(2016)",
                        "ref_id": "BIBREF46"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B Attentive BiLSTM CRF with ELMoVerflow",
                "sec_num": null
            },
            {
                "text": "t exp(u it T ue) . Finally, the word vector is created by a weighted sum of all the information from different embeddings as word i = t \u03b1 it h it .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B Attentive BiLSTM CRF with ELMoVerflow",
                "sec_num": null
            },
            {
                "text": "Weighted Word Representation uses a wordlevel weighting factor \u03b1 i to emphasize the importance of each word w i for the NER task. Similar to the embedding-level attention, it calculates \u03b1 i from the weighted word vectors word i . A bidirectional GRU is used to encode the summarized information from neighbouring words and thus it get h i = \u2190 --\u2192 GRU (word i ). This is then passed through a hidden layer which outputs u i = tanh(W w h i + b w ). Then the normalized weight for each word vector is extracted by \u03b1 i = exp(u i T uw) t exp(u i T uw) , where u w is another word-level context vector that is learned during training. The final weighted word representation is computed by word i = \u03b1 i h i . Subsequently, the aggregated word vector word i is fed into a BiLSTM-CRF network, which predicts the entity category for each word. The complete architecture of the Attentive BiLSTM CRF model is illustrated in Figure 4 . Compared to BiLSTM-CRF, our proposed Attentive BiLSTM-CRF demonstrates a 9.7 increase in F 1 on the test set (Table 2 ) and reduces the segmentation errors and entity type errors by 80.33% 23.34% respectively. ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 919,
                        "end": 920,
                        "text": "4",
                        "ref_id": "FIGREF3"
                    },
                    {
                        "start": 1039,
                        "end": 1040,
                        "text": "2",
                        "ref_id": "TABREF3"
                    }
                ],
                "eq_spans": [],
                "section": "B Attentive BiLSTM CRF with ELMoVerflow",
                "sec_num": null
            },
            {
                "text": "The Attentive-NER tagger utilizes the outputs from an auxiliary segmentation module which consists of a BiLSTM encoder and a CRF decoder. This model concatenates ELMo embeddings with two hand-crafted features-word frequency and code markdown ( \u00a73.1.3).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "C Entity Segmentation with ELMoVerflow",
                "sec_num": null
            },
            {
                "text": "The segmentation model follows the same architecture and training setup as the Attentive-NER model except for the input, where ELMo embeddings are concatenated with 100-dimensional code markdown and 10-dimensional word frequency features. The binary output from this entity segmenter model is later passed as through an embedding layer and used as one of the auxiliary inputs of the Attentive NER model.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "C Entity Segmentation with ELMoVerflow",
                "sec_num": null
            },
            {
                "text": "Table 11 shows the performance of this segmentation model with ELMoVerflow on the dev set. This model achieves an F 1 score of 84.3 and an accuracy of 97.4%. The ablation study in Table 11 depicts the importance of the hand-crafted frequency and markdown features for this segmenter model by providing an increment of 1.2 and 2.1 points in the F 1 score respectively. ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 8,
                        "text": "11",
                        "ref_id": "TABREF12"
                    },
                    {
                        "start": 186,
                        "end": 188,
                        "text": "11",
                        "ref_id": "TABREF12"
                    }
                ],
                "eq_spans": [],
                "section": "C Entity Segmentation with ELMoVerflow",
                "sec_num": null
            },
            {
                "text": "Our annotation guideline is available at: https:// github.com/jeniyat/StackOverflowNER/.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://github.com/myleott/ ark-twokenize-py",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://archive.org/details/ stackexchange",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://github.com/google-research/ BERT",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://github.com/lanwuwei/ BERTOverflow/",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "As a reference, the state-of-the-art performance for 10class Twitter NER is 70.69 F1(Zhang et al., 2018).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "We thank anonymous reviewers for their thoughtful comments. We also thank NVIDIA, Google, and Ohio Supercomputer Center (Center, 2012) for providing GPU/TPU computing resources; Wuwei Lan for kindly helping to train in-domain BERT on StackOverflow data; Sydney Lee, Rita Tong, Lillian Chow, and Raleigh Potluri for help with data annotation. This research is supported in part by the NSF awards IIS-1822754 and IIS-1845670, ODNI and IARPA via the BETTER program contract 19051600004, ARO and DARPA via the SocialSim program contract W911NF-17-C-0095, Criteo Faculty Research Award to Wei Xu, and Amazon Faculty Research Award to Alan Ritter. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of NSF, ODNI, IARPA, ARO, DARPA or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgement",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Named Entity Recognition on Code-Switched Data: Overview of the CALCS 2018 Shared Task",
                "authors": [
                    {
                        "first": "Gustavo",
                        "middle": [],
                        "last": "Aguilar",
                        "suffix": ""
                    },
                    {
                        "first": "Fahad",
                        "middle": [],
                        "last": "Alghamdi",
                        "suffix": ""
                    },
                    {
                        "first": "Victor",
                        "middle": [],
                        "last": "Soto",
                        "suffix": ""
                    },
                    {
                        "first": "Mona",
                        "middle": [],
                        "last": "Diab",
                        "suffix": ""
                    },
                    {
                        "first": "Julia",
                        "middle": [],
                        "last": "Hirschberg",
                        "suffix": ""
                    },
                    {
                        "first": "Thamar",
                        "middle": [],
                        "last": "Solorio",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Gustavo Aguilar, Fahad AlGhamdi, Victor Soto, Mona Diab, Julia Hirschberg, and Thamar Solorio. 2018. Named Entity Recognition on Code-Switched Data: Overview of the CALCS 2018 Shared Task. In Pro- ceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "A Multi-task Approach for Named Entity Recognition in Social Media Data",
                "authors": [
                    {
                        "first": "Gustavo",
                        "middle": [],
                        "last": "Aguilar",
                        "suffix": ""
                    },
                    {
                        "first": "Suraj",
                        "middle": [],
                        "last": "Maharjan",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the 3rd Workshop on Noisy User-generated Text (WNUT)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Gustavo Aguilar, Suraj Maharjan, Adrian Pastor L\u00f3pez-Monroy, and Thamar Solorio. 2017. A Multi-task Approach for Named Entity Recognition in Social Media Data. In Proceedings of the 3rd Workshop on Noisy User-generated Text (WNUT).",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Question Relatedness on Stack Overflow: The Task, Dataset, and Corpusinspired Models",
                "authors": [
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Lo",
                        "suffix": ""
                    },
                    {
                        "first": "Thamar",
                        "middle": [],
                        "last": "Solorio",
                        "suffix": ""
                    },
                    {
                        "first": "Amin",
                        "middle": [],
                        "last": "Alipour",
                        "suffix": ""
                    },
                    {
                        "first": "Amirreza",
                        "middle": [],
                        "last": "Shirani",
                        "suffix": ""
                    },
                    {
                        "first": "Bowen",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the AAAI Reasoning for Complex Question Answering Workshop",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "David Lo Thamar Solorio Amin Alipour Amirreza Shi- rani, Bowen Xu. 2019. Question Relatedness on Stack Overflow: The Task, Dataset, and Corpus- inspired Models. In Proceedings of the AAAI Rea- soning for Complex Question Answering Workshop.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Cloze-driven pretraining of self-attention networks",
                "authors": [
                    {
                        "first": "Alexei",
                        "middle": [],
                        "last": "Baevski",
                        "suffix": ""
                    },
                    {
                        "first": "Sergey",
                        "middle": [],
                        "last": "Edunov",
                        "suffix": ""
                    },
                    {
                        "first": "Yinhan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Luke",
                        "middle": [],
                        "last": "Zettlemoyer",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Auli",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alexei Baevski, Sergey Edunov, Yinhan Liu, Luke Zettlemoyer, and Michael Auli. 2019. Cloze-driven pretraining of self-attention networks. In Proceed- ings of the 2019 Conference on Empirical Meth- ods in Natural Language Processing and the 9th International Joint Conference on Natural Lan- guage Processing (EMNLP-IJCNLP). Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Part-of-Speech Tagging for Code-Switched, Transliterated Texts without Explicit Language Identification",
                "authors": [
                    {
                        "first": "Kelsey",
                        "middle": [],
                        "last": "Ball",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Garrette",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kelsey Ball and Dan Garrette. 2018. Part-of-Speech Tagging for Code-Switched, Transliterated Texts without Explicit Language Identification. In Pro- ceedings of the 2018 Conference on Empirical Meth- ods in Natural Language Processing (EMNLP).",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Arabic Named Entity Recognition using Optimized Feature Sets",
                "authors": [
                    {
                        "first": "Yassine",
                        "middle": [],
                        "last": "Benajiba",
                        "suffix": ""
                    },
                    {
                        "first": "Mona",
                        "middle": [],
                        "last": "Diab",
                        "suffix": ""
                    },
                    {
                        "first": "Paolo",
                        "middle": [],
                        "last": "Rosso",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yassine Benajiba, Mona Diab, and Paolo Rosso. 2008. Arabic Named Entity Recognition using Optimized Feature Sets. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Natural Language Processing with Python",
                "authors": [
                    {
                        "first": "Steven",
                        "middle": [],
                        "last": "Bird",
                        "suffix": ""
                    },
                    {
                        "first": "Ewan",
                        "middle": [],
                        "last": "Klein",
                        "suffix": ""
                    },
                    {
                        "first": "Edward",
                        "middle": [],
                        "last": "Loper",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Steven Bird, Ewan Klein, and Edward Loper. 2009. Natural Language Processing with Python. O'Reilly Media Inc.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Oakley supercomputer",
                "authors": [
                    {
                        "first": "Supercomputer",
                        "middle": [],
                        "last": "Ohio",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Center",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ohio Supercomputer Center. 2012. Oakley supercom- puter.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Ultra-Fine Entity Typing",
                "authors": [
                    {
                        "first": "Eunsol",
                        "middle": [],
                        "last": "Choi",
                        "suffix": ""
                    },
                    {
                        "first": "Omer",
                        "middle": [],
                        "last": "Levy",
                        "suffix": ""
                    },
                    {
                        "first": "Yejin",
                        "middle": [],
                        "last": "Choi",
                        "suffix": ""
                    },
                    {
                        "first": "Luke",
                        "middle": [],
                        "last": "Zettlemoyer",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Eunsol Choi, Omer Levy, Yejin Choi, and Luke Zettle- moyer. 2018. Ultra-Fine Entity Typing. In Proceed- ings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL).",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "A Coefficient of Agreement for Nominal Scales",
                "authors": [
                    {
                        "first": "Jacob",
                        "middle": [],
                        "last": "Cohen",
                        "suffix": ""
                    }
                ],
                "year": 1960,
                "venue": "Educational and Psychological Measurement",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jacob Cohen. 1960. A Coefficient of Agreement for Nominal Scales. Educational and Psychological Measurement.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Introduction to the Bio-entity Recognition Task at JNLPBA",
                "authors": [
                    {
                        "first": "Nigel",
                        "middle": [],
                        "last": "Collier",
                        "suffix": ""
                    },
                    {
                        "first": "Jin-Dong",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Nigel Collier and Jin-Dong Kim. 2004. Introduction to the Bio-entity Recognition Task at JNLPBA. In Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications (NLPBA/BioNLP).",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Natural Language Processing (almost) from Scratch",
                "authors": [
                    {
                        "first": "Ronan",
                        "middle": [],
                        "last": "Collobert",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Weston",
                        "suffix": ""
                    },
                    {
                        "first": "L\u00e9on",
                        "middle": [],
                        "last": "Bottou",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Karlen",
                        "suffix": ""
                    },
                    {
                        "first": "Koray",
                        "middle": [],
                        "last": "Kavukcuoglu",
                        "suffix": ""
                    },
                    {
                        "first": "Pavel",
                        "middle": [],
                        "last": "Kuksa",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Journal of Machine Learning Research",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ronan Collobert, Jason Weston, L\u00e9on Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural Language Processing (almost) from Scratch. Journal of Machine Learning Research (JMLR).",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Using Similarity Measures to Select Pretraining Data for NER",
                "authors": [
                    {
                        "first": "Xiang",
                        "middle": [],
                        "last": "Dai",
                        "suffix": ""
                    },
                    {
                        "first": "Sarvnaz",
                        "middle": [],
                        "last": "Karimi",
                        "suffix": ""
                    },
                    {
                        "first": "Ben",
                        "middle": [],
                        "last": "Hachey",
                        "suffix": ""
                    },
                    {
                        "first": "Cecile",
                        "middle": [],
                        "last": "Paris",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1904.00585"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Xiang Dai, Sarvnaz Karimi, Ben Hachey, and Ce- cile Paris. 2019. Using Similarity Measures to Select Pretraining Data for NER. arXiv preprint arXiv:1904.00585.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                "authors": [
                    {
                        "first": "Leon",
                        "middle": [],
                        "last": "Derczynski",
                        "suffix": ""
                    },
                    {
                        "first": "Diana",
                        "middle": [],
                        "last": "Maynard",
                        "suffix": ""
                    },
                    {
                        "first": "Giuseppe",
                        "middle": [],
                        "last": "Rizzo",
                        "suffix": ""
                    },
                    {
                        "first": "Marieke",
                        "middle": [],
                        "last": "Van Erp",
                        "suffix": ""
                    },
                    {
                        "first": "Genevieve",
                        "middle": [],
                        "last": "Gorrell",
                        "suffix": ""
                    },
                    {
                        "first": "Rapha\u00ebl",
                        "middle": [],
                        "last": "Troncy",
                        "suffix": ""
                    },
                    {
                        "first": "Johann",
                        "middle": [],
                        "last": "Petrak",
                        "suffix": ""
                    },
                    {
                        "first": "Kalina",
                        "middle": [],
                        "last": "Bontcheva",
                        "suffix": ""
                    },
                    {
                        "first": ";",
                        "middle": [],
                        "last": "Jacob Devlin",
                        "suffix": ""
                    },
                    {
                        "first": "Ming-Wei",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "Kenton",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Kristina",
                        "middle": [],
                        "last": "Toutanova",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Analysis of Named Entity Recognition and Linking for Tweets. Information Processing & Management",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Leon Derczynski, Diana Maynard, Giuseppe Rizzo, Marieke Van Erp, Genevieve Gorrell, Rapha\u00ebl Troncy, Johann Petrak, and Kalina Bontcheva. 2015. Analysis of Named Entity Recognition and Linking for Tweets. Information Processing & Management. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Un- derstanding. In Proceedings of the 2019 An- nual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Annotating Named Entities in Twitter Data with Crowdsourcing",
                "authors": [
                    {
                        "first": "Tim",
                        "middle": [],
                        "last": "Finin",
                        "suffix": ""
                    },
                    {
                        "first": "Will",
                        "middle": [],
                        "last": "Murnane",
                        "suffix": ""
                    },
                    {
                        "first": "Anand",
                        "middle": [],
                        "last": "Karandikar",
                        "suffix": ""
                    },
                    {
                        "first": "Nicholas",
                        "middle": [],
                        "last": "Keller",
                        "suffix": ""
                    },
                    {
                        "first": "Justin",
                        "middle": [],
                        "last": "Martineau",
                        "suffix": ""
                    },
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Dredze",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the Workshop on Creating Speech and Language Data with Amazon's Mechanical Turk",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tim Finin, Will Murnane, Anand Karandikar, Nicholas Keller, Justin Martineau, and Mark Dredze. 2010. Annotating Named Entities in Twitter Data with Crowdsourcing. In Proceedings of the Workshop on Creating Speech and Language Data with Amazon's Mechanical Turk.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Part-of-speech Tagging for Twitter: Annotation, Features, and Experiments",
                "authors": [
                    {
                        "first": "Kevin",
                        "middle": [],
                        "last": "Gimpel",
                        "suffix": ""
                    },
                    {
                        "first": "Nathan",
                        "middle": [],
                        "last": "Schneider",
                        "suffix": ""
                    },
                    {
                        "first": "Brendan O'",
                        "middle": [],
                        "last": "Connor",
                        "suffix": ""
                    },
                    {
                        "first": "Dipanjan",
                        "middle": [],
                        "last": "Das",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Mills",
                        "suffix": ""
                    },
                    {
                        "first": "Jacob",
                        "middle": [],
                        "last": "Eisenstein",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Heilman",
                        "suffix": ""
                    },
                    {
                        "first": "Dani",
                        "middle": [],
                        "last": "Yogatama",
                        "suffix": ""
                    },
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Flanigan",
                        "suffix": ""
                    },
                    {
                        "first": "Noah",
                        "middle": [
                            "A"
                        ],
                        "last": "Smith",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kevin Gimpel, Nathan Schneider, Brendan O'Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith. 2011. Part-of-speech Tagging for Twitter: Annotation, Features, and Experiments. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT).",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Transfer Learning for Biomedical Named Entity Recognition with Neural Networks",
                "authors": [
                    {
                        "first": "M",
                        "middle": [],
                        "last": "John",
                        "suffix": ""
                    },
                    {
                        "first": "Giorgi",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Gary",
                        "middle": [],
                        "last": "Bader",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "bioRxiv",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "John M Giorgi and Gary Bader. 2018. Transfer Learn- ing for Biomedical Named Entity Recognition with Neural Networks. bioRxiv.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "GHTorrent: Github's Data from a Firehose",
                "authors": [
                    {
                        "first": "G",
                        "middle": [],
                        "last": "Gousios",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Spinellis",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Proceedings of the 9th IEEEConference on Mining Software Repositories (MSR)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "G. Gousios and D. Spinellis. 2012. GHTorrent: Github's Data from a Firehose. In Proceedings of the 9th IEEEConference on Mining Software Repos- itories (MSR).",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Marginal Likelihood Training of BiLSTM-CRF for Biomedical Named Entity Recognition from Disjoint Label Sets",
                "authors": [
                    {
                        "first": "Nathan",
                        "middle": [],
                        "last": "Greenberg",
                        "suffix": ""
                    },
                    {
                        "first": "Trapit",
                        "middle": [],
                        "last": "Bansal",
                        "suffix": ""
                    },
                    {
                        "first": "Patrick",
                        "middle": [],
                        "last": "Verga",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Mccallum",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Nathan Greenberg, Trapit Bansal, Patrick Verga, and Andrew McCallum. 2018. Marginal Like- lihood Training of BiLSTM-CRF for Biomedical Named Entity Recognition from Disjoint Label Sets. In Proceedings of the 2018 Conference on Em- pirical Methods in Natural Language Processing (EMNLP).",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Summarizing Source Code using a Neural Attention Model",
                "authors": [
                    {
                        "first": "Srinivasan",
                        "middle": [],
                        "last": "Iyer",
                        "suffix": ""
                    },
                    {
                        "first": "Ioannis",
                        "middle": [],
                        "last": "Konstas",
                        "suffix": ""
                    },
                    {
                        "first": "Alvin",
                        "middle": [],
                        "last": "Cheung",
                        "suffix": ""
                    },
                    {
                        "first": "Luke",
                        "middle": [],
                        "last": "Zettlemoyer",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016. Summarizing Source Code using a Neural Attention Model. In Proceedings of the 54th Annual Meeting of the Association for Com- putational Linguistics (ACL).",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "FastText.zip: Compressing Text Classification Models",
                "authors": [
                    {
                        "first": "Armand",
                        "middle": [],
                        "last": "Joulin",
                        "suffix": ""
                    },
                    {
                        "first": "Edouard",
                        "middle": [],
                        "last": "Grave",
                        "suffix": ""
                    },
                    {
                        "first": "Piotr",
                        "middle": [],
                        "last": "Bojanowski",
                        "suffix": ""
                    },
                    {
                        "first": "Matthijs",
                        "middle": [],
                        "last": "Douze",
                        "suffix": ""
                    },
                    {
                        "first": "H\u00e9rve",
                        "middle": [],
                        "last": "J\u00e9gou",
                        "suffix": ""
                    },
                    {
                        "first": "Tomas",
                        "middle": [],
                        "last": "Mikolov",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1612.03651"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, H\u00e9rve J\u00e9gou, and Tomas Mikolov. 2016. FastText.zip: Compressing Text Classifica- tion Models. arXiv preprint arXiv:1612.03651.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "An Annotated Corpus for Machine Reading of Instructions in Wet Lab Protocols",
                "authors": [
                    {
                        "first": "Chaitanya",
                        "middle": [],
                        "last": "Kulkarni",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Alan",
                        "middle": [],
                        "last": "Ritter",
                        "suffix": ""
                    },
                    {
                        "first": "Raghu",
                        "middle": [],
                        "last": "Machiraju",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chaitanya Kulkarni, Wei Xu, Alan Ritter, and Raghu Machiraju. 2018. An Annotated Corpus for Ma- chine Reading of Instructions in Wet Lab Protocols. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies (NAACL-HLT).",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Neural Architectures for Named Entity Recognition",
                "authors": [
                    {
                        "first": "Guillaume",
                        "middle": [],
                        "last": "Lample",
                        "suffix": ""
                    },
                    {
                        "first": "Miguel",
                        "middle": [],
                        "last": "Ballesteros",
                        "suffix": ""
                    },
                    {
                        "first": "Sandeep",
                        "middle": [],
                        "last": "Subramanian",
                        "suffix": ""
                    },
                    {
                        "first": "Kazuya",
                        "middle": [],
                        "last": "Kawakami",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Dyer",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Guillaume Lample, Miguel Ballesteros, Sandeep Sub- ramanian, Kazuya Kawakami, and Chris Dyer. 2016. Neural Architectures for Named Entity Recognition. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies (NAACL-HLT).",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Bidirectional LSTM for Named Entity Recognition in Twitter Messages",
                "authors": [
                    {
                        "first": "Nut",
                        "middle": [],
                        "last": "Limsopatham",
                        "suffix": ""
                    },
                    {
                        "first": "Nigel",
                        "middle": [],
                        "last": "Collier",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of 2016 the Workshop on Noisy User-generated Text (WNUT)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Nut Limsopatham and Nigel Collier. 2016. Bidirec- tional LSTM for Named Entity Recognition in Twit- ter Messages. In Proceedings of 2016 the Workshop on Noisy User-generated Text (WNUT).",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Neural Networks Models for Entity Discovery and Linking",
                "authors": [
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Shiliang",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Si",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Hui",
                        "middle": [],
                        "last": "Jiang",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1611.03558"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Dan Liu, Wei Lin, Shiliang Zhang, Si Wei, and Hui Jiang. 2016. Neural Networks Models for Entity Discovery and Linking. arXiv preprint arXiv:1611.03558.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "World Knowledge for Reading Comprehension: Rare Entity Prediction with Hierarchical LSTMs Using External Descriptions",
                "authors": [
                    {
                        "first": "Teng",
                        "middle": [],
                        "last": "Long",
                        "suffix": ""
                    },
                    {
                        "first": "Emmanuel",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    },
                    {
                        "first": "Ryan",
                        "middle": [],
                        "last": "Lowe",
                        "suffix": ""
                    },
                    {
                        "first": "Jackie",
                        "middle": [],
                        "last": "Chi",
                        "suffix": ""
                    },
                    {
                        "first": "Kit",
                        "middle": [],
                        "last": "Cheung",
                        "suffix": ""
                    },
                    {
                        "first": "Doina",
                        "middle": [],
                        "last": "Precup",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Teng Long, Emmanuel Bengio, Ryan Lowe, Jackie Chi Kit Cheung, and Doina Precup. 2017. World Knowledge for Reading Comprehension: Rare En- tity Prediction with Hierarchical LSTMs Using Ex- ternal Descriptions. In Proceedings of the 2017 Conference on Empirical Methods in Natural Lan- guage Processing (EMNLP).",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF",
                "authors": [
                    {
                        "first": "Xuezhe",
                        "middle": [],
                        "last": "Ma",
                        "suffix": ""
                    },
                    {
                        "first": "Eduard",
                        "middle": [],
                        "last": "Hovy",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xuezhe Ma and Eduard Hovy. 2016. End-to-end Se- quence Labeling via Bi-directional LSTM-CNNs- CRF. In Proceedings of the 54th Annual Meet- ing of the Association for Computational Linguistics (ACL).",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "A Word-Complexity Lexicon and A Neural Readability Ranking Model for Lexical Simplification",
                "authors": [
                    {
                        "first": "Mounica",
                        "middle": [],
                        "last": "Maddela",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mounica Maddela and Wei Xu. 2018. A Word- Complexity Lexicon and A Neural Readability Ranking Model for Lexical Simplification. In Pro- ceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "The Stanford CoreNLP Natural Language Processing Toolkit",
                "authors": [
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Christopher",
                        "suffix": ""
                    },
                    {
                        "first": "Mihai",
                        "middle": [],
                        "last": "Manning",
                        "suffix": ""
                    },
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Surdeanu",
                        "suffix": ""
                    },
                    {
                        "first": "Jenny",
                        "middle": [],
                        "last": "Bauer",
                        "suffix": ""
                    },
                    {
                        "first": "Steven",
                        "middle": [
                            "J"
                        ],
                        "last": "Finkel",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Bethard",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Mc-Closky",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of the 2014 Association for Computational Linguistics System Demonstrations (ACL)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David Mc- Closky. 2014. The Stanford CoreNLP Natural Language Processing Toolkit. In Proceedings of the 2014 Association for Computational Linguistics System Demonstrations (ACL).",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Learned in Translation: Contextualized Word Vectors",
                "authors": [
                    {
                        "first": "Bryan",
                        "middle": [],
                        "last": "Mccann",
                        "suffix": ""
                    },
                    {
                        "first": "James",
                        "middle": [],
                        "last": "Bradbury",
                        "suffix": ""
                    },
                    {
                        "first": "Caiming",
                        "middle": [],
                        "last": "Xiong",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the 2017 Conference on Neural Information Processing Systems (NeurIPS)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. 2017. Learned in Translation: Con- textualized Word Vectors. In Proceedings of the 2017 Conference on Neural Information Processing Systems (NeurIPS).",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "KB-LDA: Jointly Learning a Knowledge Base of Hierarchy, Relations, and Facts",
                "authors": [
                    {
                        "first": "Dana",
                        "middle": [],
                        "last": "Movshovitz",
                        "suffix": ""
                    },
                    {
                        "first": "-Attias",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "William",
                        "middle": [
                            "W"
                        ],
                        "last": "Cohen",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (ACL-IJCNLP)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dana Movshovitz-Attias and William W Cohen. 2015. KB-LDA: Jointly Learning a Knowledge Base of Hierarchy, Relations, and Facts. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (ACL-IJCNLP).",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "Annotated Gigaword",
                "authors": [
                    {
                        "first": "Courtney",
                        "middle": [],
                        "last": "Napoles",
                        "suffix": ""
                    },
                    {
                        "first": "Matthew",
                        "middle": [],
                        "last": "Gormley",
                        "suffix": ""
                    },
                    {
                        "first": "Benjamin",
                        "middle": [],
                        "last": "Van Durme",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-Scale Knowledge Extraction",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Courtney Napoles, Matthew Gormley, and Benjamin Van Durme. 2012. Annotated Gigaword. In Pro- ceedings of the Joint Workshop on Automatic Knowl- edge Base Construction and Web-Scale Knowledge Extraction.",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "Learning to Denoise Distantly-Labeled Data for Entity Typing",
                "authors": [
                    {
                        "first": "Yasumasa",
                        "middle": [],
                        "last": "Onoe",
                        "suffix": ""
                    },
                    {
                        "first": "Greg",
                        "middle": [],
                        "last": "Durrett",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (NACL)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/n19-1250"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yasumasa Onoe and Greg Durrett. 2019. Learning to Denoise Distantly-Labeled Data for Entity Typing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computa- tional Linguistics (NACL).",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "GloVe: Global Vectors for Word Representation",
                "authors": [
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Pennington",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of the Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jeffrey Pennington, Richard Socher, and Christo- pher D. Manning. 2014. GloVe: Global Vectors for Word Representation. In Proceedings of the Empirical Methods in Natural Language Processing (EMNLP).",
                "links": null
            },
            "BIBREF34": {
                "ref_id": "b34",
                "title": "Deep Contextualized Word Representations",
                "authors": [
                    {
                        "first": "Matthew",
                        "middle": [
                            "E"
                        ],
                        "last": "Peters",
                        "suffix": ""
                    },
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Neumann",
                        "suffix": ""
                    },
                    {
                        "first": "Mohit",
                        "middle": [],
                        "last": "Iyyer",
                        "suffix": ""
                    },
                    {
                        "first": "Matt",
                        "middle": [],
                        "last": "Gardner",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [],
                        "last": "Clark",
                        "suffix": ""
                    },
                    {
                        "first": "Kenton",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Luke",
                        "middle": [],
                        "last": "Zettlemoyer",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the of Conference of the North American Chapter",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep Contextualized Word Rep- resentations. In Proceedings of the of Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).",
                "links": null
            },
            "BIBREF35": {
                "ref_id": "b35",
                "title": "Learning part-of-speech taggers with inter-annotator agreement loss",
                "authors": [
                    {
                        "first": "Barbara",
                        "middle": [],
                        "last": "Plank",
                        "suffix": ""
                    },
                    {
                        "first": "Dirk",
                        "middle": [],
                        "last": "Hovy",
                        "suffix": ""
                    },
                    {
                        "first": "Anders",
                        "middle": [],
                        "last": "S\u00f8gaard",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of the 14th Conference of the European Chapter",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Barbara Plank, Dirk Hovy, and Anders S\u00f8gaard. 2014. Learning part-of-speech taggers with inter-annotator agreement loss. In Proceedings of the 14th Confer- ence of the European Chapter of the Association for Computational Linguistics (EACL).",
                "links": null
            },
            "BIBREF36": {
                "ref_id": "b36",
                "title": "Language to Code: Learning Semantic Parsers for If-This-Then-That Recipes",
                "authors": [
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Quirk",
                        "suffix": ""
                    },
                    {
                        "first": "Raymond",
                        "middle": [],
                        "last": "Mooney",
                        "suffix": ""
                    },
                    {
                        "first": "Michel",
                        "middle": [],
                        "last": "Galley",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (ACL-IJCNLP)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chris Quirk, Raymond Mooney, and Michel Galley. 2015. Language to Code: Learning Semantic Parsers for If-This-Then-That Recipes. In Proceed- ings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th Interna- tional Joint Conference on Natural Language Pro- cessing (ACL-IJCNLP).",
                "links": null
            },
            "BIBREF37": {
                "ref_id": "b37",
                "title": "Great Question! Question Quality in Community Q&A",
                "authors": [
                    {
                        "first": "Sujith",
                        "middle": [],
                        "last": "Ravi",
                        "suffix": ""
                    },
                    {
                        "first": "Bo",
                        "middle": [],
                        "last": "Pang",
                        "suffix": ""
                    },
                    {
                        "first": "Vibhor",
                        "middle": [],
                        "last": "Rastogi",
                        "suffix": ""
                    },
                    {
                        "first": "Ravi",
                        "middle": [],
                        "last": "Kumar",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Eighth International AAAI Conference on Weblogs and Social Media",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sujith Ravi, Bo Pang, Vibhor Rastogi, and Ravi Kumar. 2014. Great Question! Question Quality in Com- munity Q&A. In Eighth International AAAI Confer- ence on Weblogs and Social Media.",
                "links": null
            },
            "BIBREF38": {
                "ref_id": "b38",
                "title": "Named Entity Recognition in Tweets: An Experimental Study",
                "authors": [
                    {
                        "first": "Alan",
                        "middle": [],
                        "last": "Ritter",
                        "suffix": ""
                    },
                    {
                        "first": "Sam",
                        "middle": [],
                        "last": "Clark",
                        "suffix": ""
                    },
                    {
                        "first": "Mausam",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Oren",
                        "middle": [],
                        "last": "Etzioni",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alan Ritter, Sam Clark, Mausam, and Oren Etzioni. 2011. Named Entity Recognition in Tweets: An Ex- perimental Study. In Proceedings of the 2011 Con- ference on Empirical Methods in Natural Language Processing (EMNLP).",
                "links": null
            },
            "BIBREF39": {
                "ref_id": "b39",
                "title": "Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition",
                "authors": [
                    {
                        "first": "Erik",
                        "middle": [
                            "F"
                        ],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Tjong",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "Sang",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Fien",
                        "middle": [],
                        "last": "De",
                        "suffix": ""
                    },
                    {
                        "first": "Meulder",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Erik F Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recog- nition. In Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL.",
                "links": null
            },
            "BIBREF40": {
                "ref_id": "b40",
                "title": "In-domain Context-aware Token Embeddings Improve Biomedical Named Entity Recognition",
                "authors": [
                    {
                        "first": "Golnar",
                        "middle": [],
                        "last": "Sheikhshabbafghi",
                        "suffix": ""
                    },
                    {
                        "first": "Inanc",
                        "middle": [],
                        "last": "Birol",
                        "suffix": ""
                    },
                    {
                        "first": "Anoop",
                        "middle": [],
                        "last": "Sarkar",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Golnar Sheikhshabbafghi, Inanc Birol, and Anoop Sarkar. 2018. In-domain Context-aware Token Em- beddings Improve Biomedical Named Entity Recog- nition. In Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis.",
                "links": null
            },
            "BIBREF41": {
                "ref_id": "b41",
                "title": "Neural Cross-Lingual Entity Linking",
                "authors": [
                    {
                        "first": "Avirup",
                        "middle": [],
                        "last": "Sil",
                        "suffix": ""
                    },
                    {
                        "first": "Gourab",
                        "middle": [],
                        "last": "Kundu",
                        "suffix": ""
                    },
                    {
                        "first": "Radu",
                        "middle": [],
                        "last": "Florian",
                        "suffix": ""
                    },
                    {
                        "first": "Wael",
                        "middle": [],
                        "last": "Hamza",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the 30th AAAI Conference on Artificial Intelligence",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Avirup Sil, Gourab Kundu, Radu Florian, and Wael Hamza. 2017. Neural Cross-Lingual Entity Link- ing. In Proceedings of the 30th AAAI Conference on Artificial Intelligence.",
                "links": null
            },
            "BIBREF42": {
                "ref_id": "b42",
                "title": "brat: a Web-based Tool for NLP-Assisted Text Annotation",
                "authors": [
                    {
                        "first": "Pontus",
                        "middle": [],
                        "last": "Stenetorp",
                        "suffix": ""
                    },
                    {
                        "first": "Sampo",
                        "middle": [],
                        "last": "Pyysalo",
                        "suffix": ""
                    },
                    {
                        "first": "Goran",
                        "middle": [],
                        "last": "Topi\u0107",
                        "suffix": ""
                    },
                    {
                        "first": "Tomoko",
                        "middle": [],
                        "last": "Ohta",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Proceedings of the Demonstrations at the 13th Conference of the European Chapter of the",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Pontus Stenetorp, Sampo Pyysalo, Goran Topi\u0107, Tomoko Ohta, Sophia Ananiadou, and Jun'ichi Tsu- jii. 2012. brat: a Web-based Tool for NLP-Assisted Text Annotation. In Proceedings of the Demonstra- tions at the 13th Conference of the European Chap- ter of the Association for Computational Linguistics (EACL).",
                "links": null
            },
            "BIBREF43": {
                "ref_id": "b43",
                "title": "Evaluating the Utility of Hand-crafted Features in Sequence Labelling",
                "authors": [
                    {
                        "first": "Minghao",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Fei",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Trevor",
                        "middle": [],
                        "last": "Cohn",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Minghao Wu, Fei Liu, and Trevor Cohn. 2018. Eval- uating the Utility of Hand-crafted Features in Se- quence Labelling. In Proceedings of the 2018 Con- ference on Empirical Methods in Natural Language Processing (EMNLP).",
                "links": null
            },
            "BIBREF44": {
                "ref_id": "b44",
                "title": "Neural Cross-Lingual Named Entity Recognition with Minimal Resources",
                "authors": [
                    {
                        "first": "Jiateng",
                        "middle": [],
                        "last": "Xie",
                        "suffix": ""
                    },
                    {
                        "first": "Zhilin",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Graham",
                        "middle": [],
                        "last": "Neubig",
                        "suffix": ""
                    },
                    {
                        "first": "Noah",
                        "middle": [
                            "A"
                        ],
                        "last": "Smith",
                        "suffix": ""
                    },
                    {
                        "first": "Jaime",
                        "middle": [],
                        "last": "Carbonell",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jiateng Xie, Zhilin Yang, Graham Neubig, Noah A Smith, and Jaime Carbonell. 2018. Neural Cross- Lingual Named Entity Recognition with Minimal Resources. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Pro- cessing (EMNLP).",
                "links": null
            },
            "BIBREF45": {
                "ref_id": "b45",
                "title": "Distantly Supervised NER with Partial Annotation Learning and Reinforcement Learning",
                "authors": [
                    {
                        "first": "Yaosheng",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Wenliang",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Zhenghua",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Zhengqiu",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Min",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 27th International Conference on Computational Linguistics (COLING)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yaosheng Yang, Wenliang Chen, Zhenghua Li, Zhengqiu He, and Min Zhang. 2018. Distantly Supervised NER with Partial Annotation Learning and Reinforcement Learning. In Proceedings of the 27th International Conference on Computational Linguistics (COLING).",
                "links": null
            },
            "BIBREF46": {
                "ref_id": "b46",
                "title": "Hierarchical Attention Networks for Document Classification",
                "authors": [
                    {
                        "first": "Zichao",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Diyi",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Dyer",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaodong",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Smola",
                        "suffix": ""
                    },
                    {
                        "first": "Eduard",
                        "middle": [],
                        "last": "Hovy",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, and Eduard Hovy. 2016. Hierarchi- cal Attention Networks for Document Classification. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies (NAACL).",
                "links": null
            },
            "BIBREF47": {
                "ref_id": "b47",
                "title": "CoaCor: Code Annotation for Code Retrieval with Reinforcement Learning",
                "authors": [
                    {
                        "first": "Ziyu",
                        "middle": [],
                        "last": "Yao",
                        "suffix": ""
                    },
                    {
                        "first": "Jayavardhan",
                        "middle": [],
                        "last": "Reddy Peddamail",
                        "suffix": ""
                    },
                    {
                        "first": "Huan",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the World Wide Web Conference",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.1145/3308558.3313632"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ziyu Yao, Jayavardhan Reddy Peddamail, and Huan Sun. 2019. CoaCor: Code Annotation for Code Re- trieval with Reinforcement Learning. In Proceed- ings of the World Wide Web Conference (WWW).",
                "links": null
            },
            "BIBREF48": {
                "ref_id": "b48",
                "title": "Softwarespecific Named Entity Recognition in Software Engineering Social Content",
                "authors": [
                    {
                        "first": "Deheng",
                        "middle": [],
                        "last": "Ye",
                        "suffix": ""
                    },
                    {
                        "first": "Zhenchang",
                        "middle": [],
                        "last": "Xing",
                        "suffix": ""
                    },
                    {
                        "first": "Chee",
                        "middle": [],
                        "last": "Yong Foo",
                        "suffix": ""
                    },
                    {
                        "first": "Zi",
                        "middle": [],
                        "last": "Qun Ang",
                        "suffix": ""
                    },
                    {
                        "first": "Jing",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Nachiket",
                        "middle": [],
                        "last": "Kapre",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the 2016 IEEE International Conference on Software Analysis, Evolution, and Reengineering (SANER)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Deheng Ye, Zhenchang Xing, Chee Yong Foo, Zi Qun Ang, Jing Li, and Nachiket Kapre. 2016. Software- specific Named Entity Recognition in Software En- gineering Social Content. In Proceedings of the 2016 IEEE International Conference on Software Analysis, Evolution, and Reengineering (SANER).",
                "links": null
            },
            "BIBREF49": {
                "ref_id": "b49",
                "title": "TRANX: A Transition-based Neural Abstract Syntax Parser for Semantic Parsing and Code Generation",
                "authors": [
                    {
                        "first": "Pengcheng",
                        "middle": [],
                        "last": "Yin",
                        "suffix": ""
                    },
                    {
                        "first": "Graham",
                        "middle": [],
                        "last": "Neubig",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations (EMNLP)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Pengcheng Yin and Graham Neubig. 2018. TRANX: A Transition-based Neural Abstract Syntax Parser for Semantic Parsing and Code Generation. In Proceed- ings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demon- strations (EMNLP).",
                "links": null
            },
            "BIBREF50": {
                "ref_id": "b50",
                "title": "Adaptive Co-attention Network for Named Entity Recognition in Tweets",
                "authors": [
                    {
                        "first": "Qi",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Jinlan",
                        "middle": [],
                        "last": "Fu",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaoyu",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Xuanjing",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Qi Zhang, Jinlan Fu, Xiaoyu Liu, and Xuanjing Huang. 2018. Adaptive Co-attention Network for Named Entity Recognition in Tweets. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelli- gence.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 1: Examples of software-related named entities in a StackOverflow post.",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Figure 2: Our SoftNER model. It utilizes an attention network to combine the contextual word embeddings (BERT base ) with the domain-specific embeddings (Code Recognizer and Entity Segmenter). The detailed structure of the attention network is depicted on the right.",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "Figure 3: Comparison of errors made by the fine-tuned BERTOverflow baseline and our SoftNER model on the dev set of the StackOverflow NER corpus. In the heatmap, darker cell color corresponds to higher error counts. Our SoftNER model reduces errors in all the categories. P R F1 Feature-Based CRF 43.16 35.71 39.09 BiLSTM-CRF (ELMoGitHub) 64.53 60.96 62.69 Attentive BiLSTM-CRF (ELMoVerflow) 62.05 59.20 60.59 Attentive BiLSTM-CRF (ELMoGitHub) 63.29 60.89 62.07 Fine-tuned out-of-domain BERT 56.59 48.13 52.02 Fine-tuned BERTOverflow 61.71 58.75 60.19 SoftNER (BERTOverflow) 61.92 60.26 61.08",
                "uris": null,
                "fig_num": "3",
                "type_str": "figure"
            },
            "FIGREF3": {
                "num": null,
                "text": "Figure 4: Our SoftNER model. It utilizes an attention network to combine the contextual word embeddings (ELMo) with the domain-specific embeddings (Code Recognizer and Entity Segmenter). The detailed structure of the attention network is depicted on the right.",
                "uris": null,
                "fig_num": "4",
                "type_str": "figure"
            },
            "TABREF2": {
                "content": "<table><tr><td>txScope.Complete()</td><td>[ 'txScope' '.' 'Complete' '(' ')' ]</td></tr><tr><td colspan=\"2\">std::condition variable [ 'std' ':' ':' 'condition variable']</td></tr><tr><td>math.h</td><td>[ 'math' '.' 'h']</td></tr><tr><td>span</td><td>[' ' 'span' ' ']</td></tr><tr><td>a==b</td><td>['a' '=' '=' 'b']</td></tr></table>",
                "type_str": "table",
                "text": ":",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table><tr><td/><td>P</td><td>R</td><td>F1</td></tr><tr><td>Test set</td><td/><td/></tr><tr><td>Feature-based CRF</td><td colspan=\"3\">71.77 39.70 51.12</td></tr><tr><td>BiLSTM-CRF (ELMoVerflow)</td><td colspan=\"3\">73.03 64.82 68.68</td></tr><tr><td colspan=\"4\">Attentive BiLSTM-CRF (ELMoVerflow) 78.22 78.59 78.41</td></tr><tr><td>Fine-tuned BERT</td><td colspan=\"3\">77.02 45.92 57.54</td></tr><tr><td>Fine-tuned BERTOverflow</td><td colspan=\"3\">68.77 67.47 68.12</td></tr><tr><td>SoftNER (BERTOverflow)</td><td colspan=\"3\">78.42 79.79 79.10</td></tr><tr><td>Dev set</td><td/><td/></tr><tr><td>Feature-based CRF</td><td colspan=\"3\">66.85 46.19 54.64</td></tr><tr><td>BiLSTM-CRF (ELMoVerflow)</td><td colspan=\"3\">74.44 68.71 71.46</td></tr><tr><td colspan=\"4\">Attentive BiLSTM-CRF (ELMoVerflow) 79.43 80.00 79.72</td></tr><tr><td>Fine-tuned BERT</td><td colspan=\"3\">79.57 46.42 58.64</td></tr><tr><td>Fine-tuned BERTOverflow</td><td colspan=\"3\">72.11 70.51 71.30</td></tr><tr><td>SoftNER (BERTOverflow)</td><td colspan=\"3\">78.81 81.72 80.24</td></tr><tr><td>4.1 Data</td><td/><td/></tr><tr><td colspan=\"4\">We train and evaluate our SoftNER model on the</td></tr><tr><td colspan=\"4\">StackOverflow NER corpus of 9,352 train, 2,942</td></tr><tr><td colspan=\"4\">development and 3,115 test sentences we con-</td></tr><tr><td colspan=\"4\">structed in  \u00a72. We use the same data for our seg-</td></tr><tr><td colspan=\"4\">mentation model but replace all the entity tags</td></tr><tr><td colspan=\"4\">with an I-ENTITY tag. For the code recogni-</td></tr><tr><td colspan=\"4\">tion model, we created a new lexicon of 6,000</td></tr><tr><td colspan=\"4\">unique tokens randomly sampled from the train-</td></tr><tr><td colspan=\"4\">ing set of the StackOverflow NER corpus. Each</td></tr><tr><td colspan=\"4\">token was labelled independently without context</td></tr><tr><td colspan=\"4\">as CODE, AMBIGUOUS or NON-CODE by two an-</td></tr><tr><td colspan=\"4\">notators with computer science background. The</td></tr><tr><td colspan=\"4\">inter-annotator agreement was 0.89, measured by</td></tr><tr><td colspan=\"4\">Cohen's Kappa. After discarding disagreements,</td></tr><tr><td colspan=\"4\">we divided the remaining 5,312 tokens into 4,312</td></tr><tr><td colspan=\"4\">train and 1,000 test instances. Then, we merged</td></tr><tr><td colspan=\"4\">AMBIGUOUS and NON-CODE categories to facili-</td></tr><tr><td colspan=\"4\">tate binary classification. We name this dataset of</td></tr><tr><td colspan=\"3\">5312 individual tokens as SOLEXICON.</td></tr></table>",
                "type_str": "table",
                "text": "Evaluation on the dev and test sets of the StackOverflow NER corpus. Our SoftNER model outperforms the existing approaches.",
                "html": null,
                "num": null
            },
            "TABREF5": {
                "content": "<table><tr><td/><td>P</td><td>R</td><td>F1</td></tr><tr><td>SoftNER</td><td colspan=\"3\">78.81 81.72 80.24</td></tr><tr><td colspan=\"4\">-Embedding Attention 75.83 79.09 77.43</td></tr><tr><td>-Code Recognizer</td><td colspan=\"3\">78.76 77.35 78.05</td></tr><tr><td>-Entity Segmenter</td><td colspan=\"3\">77.82 75.32 76.55</td></tr><tr><td/><td>P</td><td>R</td><td>F1</td></tr><tr><td>Token Frequency</td><td>33.33</td><td>2.25</td><td>4.22</td></tr><tr><td>Most Frequent Label</td><td colspan=\"3\">82.21 58.59 68.42</td></tr><tr><td colspan=\"4\">Our Code Recognition Model 78.43 83.33 80.80</td></tr><tr><td>-Character ngram LMs</td><td colspan=\"3\">64.13 84.51 72.90</td></tr><tr><td>-Word ngram LMs</td><td colspan=\"3\">67.98 72.96 70.38</td></tr><tr><td>-FastText Embeddings</td><td colspan=\"3\">76.12 81.69 78.81</td></tr></table>",
                "type_str": "table",
                "text": "Table 4 shows an ablation study. Removing code recognizer vectors and entity segmenter vectors results in a drop of 2.19 and 3.69 in F 1 scores respectively. If we replace embeddinglevel attention with a simple concatenation of em-Ablation study of SoftNER on the dev set of StackOverflow NER corpus.",
                "html": null,
                "num": null
            },
            "TABREF6": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Evaluation results and feature ablation of our code recognition model on SOLEXICON test set of 1000 manually labeled unique tokens, which are sampled from the train set of StackOverflow NER corpus.",
                "html": null,
                "num": null
            },
            "TABREF7": {
                "content": "<table><tr><td/><td>P</td><td>R</td><td>F1</td></tr><tr><td>Stanford NER Tagger</td><td colspan=\"2\">63.02 5.74</td><td>10.52</td></tr><tr><td colspan=\"4\">Our Entity Segmentation Model 90.41 85.89 88.09</td></tr><tr><td>-Word Frequency</td><td colspan=\"3\">88.32 84.79 86.52</td></tr><tr><td>-Code Markdown</td><td colspan=\"3\">86.23 84.64 85.43</td></tr></table>",
                "type_str": "table",
                "text": "Table 6 shows the performance of our segmentation model on the dev set of our StackOverflow corpus, where the entity tags are replaced by an I-ENTITY tag. Our model achieves an F 1 score of 88.09 and with 90.41% precision and 85.89% recall. Incorporating word frequency and code markdown feature increases the F 1 score by 1.57 and 2.66 points respectively. The low 10.5 F 1 score of Stanford NER Evaluation of our segmentation model on the dev set of the StackOverflow NER corpus.",
                "html": null,
                "num": null
            },
            "TABREF8": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Evaluation on the GitHub NER dataset of readme files and issue posts. All the models are trained on our StackOverflow NER corpus. Our SoftNER model performs close to BiLSTM-CRF model trained on the GitHub ELMo embeddings.",
                "html": null,
                "num": null
            },
            "TABREF9": {
                "content": "<table><tr><td/><td>P</td><td>R</td><td>F1</td></tr><tr><td>Feature-based CRF</td><td colspan=\"3\">66.85 46.19 54.64</td></tr><tr><td>-Context Features</td><td colspan=\"3\">68.91 43.58 53.39</td></tr><tr><td>-Markdown Feature</td><td colspan=\"3\">70.64 40.15 51.20</td></tr><tr><td colspan=\"4\">-Rule and Gazetteer Features 69.71 40.66 51.36</td></tr></table>",
                "type_str": "table",
                "text": ". One noticeable distinction from the named entity recognizer in many other domains is that the contextual features are not as helpful in feature-based CRFs for classifying software entities. This is because, in the StackOverflow NER corpus a significant number of neighbouring words are shared among different software entities. As an example, the bigram 'in the' frequently appears as the left context of the following types: APPLICATION, CLASS, FUNCTION, FILE TYPE, UI ELEMENT, LIBRARY, DATA STRUCTURE and LANGUAGE. Feature based CRF performance with varying input features on dev data.",
                "html": null,
                "num": null
            },
            "TABREF10": {
                "content": "<table><tr><td/><td>P</td><td>R</td><td>F1</td></tr><tr><td>Attentive BiLSTM-CRF</td><td colspan=\"3\">79.43 80.00 79.72</td></tr><tr><td colspan=\"4\">-Multi-level Attention 77.68 78.08 77.88</td></tr><tr><td>-Code Recognizer</td><td colspan=\"3\">77.18 77.76 77.47</td></tr><tr><td>-Entity Segmenter</td><td colspan=\"3\">74.82 75.32 75.07</td></tr></table>",
                "type_str": "table",
                "text": "",
                "html": null,
                "num": null
            },
            "TABREF11": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Ablation study of Attentive-NER on the dev set of StackOverflow NER corpus.",
                "html": null,
                "num": null
            },
            "TABREF12": {
                "content": "<table><tr><td/><td>P</td><td>R</td><td>F1</td></tr><tr><td colspan=\"4\">Entity Segmentation (ELMoVerflow) 86.80 81.86 84.26</td></tr><tr><td>-Word Frequency</td><td colspan=\"3\">84.61 81.53 83.04</td></tr><tr><td>-Code Markdown</td><td colspan=\"3\">82.49 81.83 82.16</td></tr></table>",
                "type_str": "table",
                "text": "Ablation study of our segmentation model with ELMoVerflow on the dev set of the StackOverflow NER corpus.",
                "html": null,
                "num": null
            }
        }
    }
}