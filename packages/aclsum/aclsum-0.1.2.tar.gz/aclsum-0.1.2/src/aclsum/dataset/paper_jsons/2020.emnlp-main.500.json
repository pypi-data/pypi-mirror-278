{
    "paper_id": "2020",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:06:36.506207Z"
    },
    "title": "BERT-ATTACK: Adversarial Attack Against BERT Using BERT",
    "authors": [
        {
            "first": "Linyang",
            "middle": [],
            "last": "Li",
            "suffix": "",
            "affiliation": {
                "laboratory": "Shanghai Key Laboratory of Intelligent Information Processing",
                "institution": "Fudan University",
                "location": {
                    "addrLine": "825 Zhangheng Road",
                    "settlement": "Shanghai",
                    "country": "China"
                }
            },
            "email": "linyangli19@fudan.edu.cn"
        },
        {
            "first": "Ruotian",
            "middle": [],
            "last": "Ma",
            "suffix": "",
            "affiliation": {
                "laboratory": "Shanghai Key Laboratory of Intelligent Information Processing",
                "institution": "Fudan University",
                "location": {
                    "addrLine": "825 Zhangheng Road",
                    "settlement": "Shanghai",
                    "country": "China"
                }
            },
            "email": ""
        },
        {
            "first": "Qipeng",
            "middle": [],
            "last": "Guo",
            "suffix": "",
            "affiliation": {
                "laboratory": "Shanghai Key Laboratory of Intelligent Information Processing",
                "institution": "Fudan University",
                "location": {
                    "addrLine": "825 Zhangheng Road",
                    "settlement": "Shanghai",
                    "country": "China"
                }
            },
            "email": ""
        },
        {
            "first": "Xiangyang",
            "middle": [],
            "last": "Xue",
            "suffix": "",
            "affiliation": {
                "laboratory": "Shanghai Key Laboratory of Intelligent Information Processing",
                "institution": "Fudan University",
                "location": {
                    "addrLine": "825 Zhangheng Road",
                    "settlement": "Shanghai",
                    "country": "China"
                }
            },
            "email": "xyxue@fudan.edu.cn"
        },
        {
            "first": "Xipeng",
            "middle": [],
            "last": "Qiu",
            "suffix": "",
            "affiliation": {
                "laboratory": "Shanghai Key Laboratory of Intelligent Information Processing",
                "institution": "Fudan University",
                "location": {
                    "addrLine": "825 Zhangheng Road",
                    "settlement": "Shanghai",
                    "country": "China"
                }
            },
            "email": "xpqiu@fudan.edu.cn"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Adversarial attacks for discrete data (such as texts) have been proved significantly more challenging than continuous data (such as images) since it is difficult to generate adversarial samples with gradient-based methods. Current successful attack methods for texts usually adopt heuristic replacement strategies on the character or word level, which remains challenging to find the optimal solution in the massive space of possible combinations of replacements while preserving semantic consistency and language fluency. In this paper, we propose BERT-Attack, a high-quality and effective method to generate adversarial samples using pre-trained masked language models exemplified by BERT. We turn BERT against its fine-tuned models and other deep neural models in downstream tasks so that we can successfully mislead the target models to predict incorrectly. Our method outperforms state-of-theart attack strategies in both success rate and perturb percentage, while the generated adversarial samples are fluent and semantically preserved. Also, the cost of calculation is low, thus possible for large-scale generations. The code is available at https://github.com/ LinyangLee/BERT-Attack.",
    "pdf_parse": {
        "paper_id": "2020",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Adversarial attacks for discrete data (such as texts) have been proved significantly more challenging than continuous data (such as images) since it is difficult to generate adversarial samples with gradient-based methods. Current successful attack methods for texts usually adopt heuristic replacement strategies on the character or word level, which remains challenging to find the optimal solution in the massive space of possible combinations of replacements while preserving semantic consistency and language fluency. In this paper, we propose BERT-Attack, a high-quality and effective method to generate adversarial samples using pre-trained masked language models exemplified by BERT. We turn BERT against its fine-tuned models and other deep neural models in downstream tasks so that we can successfully mislead the target models to predict incorrectly. Our method outperforms state-of-theart attack strategies in both success rate and perturb percentage, while the generated adversarial samples are fluent and semantically preserved. Also, the cost of calculation is low, thus possible for large-scale generations. The code is available at https://github.com/ LinyangLee/BERT-Attack.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Despite the success of deep learning, recent works have found that these neural networks are vulnerable to adversarial samples, which are crafted with small perturbations to the original inputs (Goodfellow et al., 2014; Kurakin et al., 2016; Chakraborty et al., 2018) . That is, these adversarial samples are imperceptible to human judges while they can mislead the neural networks to incorrect predictions. Therefore, it is essential to explore these adversarial attack methods since the ultimate goal is to make sure the neural networks are highly reliable and robust. While in computer vision fields, both attack strategies and their defense countermeasures are well-explored (Chakraborty et al., 2018) , the adversarial attack for text is still challenging due to the discrete nature of languages. Generating of adversarial samples for texts needs to possess such qualities: (1) imperceptible to human judges yet misleading to neural models; (2) fluent in grammar and semantically consistent with original inputs.",
                "cite_spans": [
                    {
                        "start": 194,
                        "end": 219,
                        "text": "(Goodfellow et al., 2014;",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 220,
                        "end": 241,
                        "text": "Kurakin et al., 2016;",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 242,
                        "end": 267,
                        "text": "Chakraborty et al., 2018)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 679,
                        "end": 705,
                        "text": "(Chakraborty et al., 2018)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Previous methods craft adversarial samples mainly based on specific rules (Li et al., 2018; Gao et al., 2018; Yang et al., 2018; Alzantot et al., 2018; Ren et al., 2019; Jin et al., 2019; Zang et al., 2020) . Therefore, these methods are difficult to guarantee the fluency and semantically preservation in the generated adversarial samples at the same time. Plus, these manual craft methods are rather complicated. They use multiple linguistic constraints like NER tagging or POS tagging. Introducing contextualized language models to serve as an automatic perturbation generator could make these rules designing much easier.",
                "cite_spans": [
                    {
                        "start": 74,
                        "end": 91,
                        "text": "(Li et al., 2018;",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 92,
                        "end": 109,
                        "text": "Gao et al., 2018;",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 110,
                        "end": 128,
                        "text": "Yang et al., 2018;",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 129,
                        "end": 151,
                        "text": "Alzantot et al., 2018;",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 152,
                        "end": 169,
                        "text": "Ren et al., 2019;",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 170,
                        "end": 187,
                        "text": "Jin et al., 2019;",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 188,
                        "end": 206,
                        "text": "Zang et al., 2020)",
                        "ref_id": "BIBREF23"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The recent rise of pre-trained language models, such as BERT (Devlin et al., 2018) , push the performances of NLP tasks to a new level. On the one hand, the powerful ability of a fine-tuned BERT on downstream tasks makes it more challenging to be adversarial attacked (Jin et al., 2019) . On the other hand, BERT is a pre-trained masked language model on extremely large-scale unsupervised data and has learned general-purpose language knowledge. Therefore, BERT has the potential to generate more fluent and semantic-consistent substitutions for an input text. Naturally, both the properties of BERT motivate us to explore the possibility of attacking a fine-tuned BERT with another BERT as the attacker.",
                "cite_spans": [
                    {
                        "start": 61,
                        "end": 82,
                        "text": "(Devlin et al., 2018)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 268,
                        "end": 286,
                        "text": "(Jin et al., 2019)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this paper, we propose an effective and high-quality adversarial sample generation method: BERT-Attack, using BERT as a language model to generate adversarial samples. The core algorithm of BERT-Attack is straightforward and consists of two stages: finding the vulnerable words in one given input sequence for the target model; then applying BERT in a semantic-preserving way to generate substitutes for the vulnerable words. With the ability of BERT, the perturbations are generated considering the context around. Therefore, the perturbations are fluent and reasonable. We use the masked language model as a perturbation generator and find perturbations that maximize the risk of making wrong predictions (Goodfellow et al., 2014) . Differently from previous attacking strategies that require traditional single-direction language models as a constraint, we only need to inference the language model once as a perturbation generator rather than repeatedly using language models to score the generated adversarial samples in a trial and error process.",
                "cite_spans": [
                    {
                        "start": 710,
                        "end": 735,
                        "text": "(Goodfellow et al., 2014)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Experimental results show that the proposed BERT-Attack method successfully fooled its finetuned downstream model with the highest attack success rate compared with previous methods. Meanwhile, the perturb percentage and the query number are considerably lower, while the semantic preservation is high.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "To summarize our main contributions:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\u2022 We propose a simple and effective method, named BERT-Attack, to effectively generate fluent and semantically-preserved adversarial samples that can successfully mislead stateof-the-art models in NLP, such as fine-tuned BERT for various downstream tasks.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\u2022 BERT-Attack has a higher attacking success rate and a lower perturb percentage with fewer access numbers to the target model compared with previous attacking algorithms, while does not require extra scoring models therefore extremely effective.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "To explore the robustness of neural networks, adversarial attacks have been extensively studied for continuous data (such as images) (Goodfellow et al., 2014; Nguyen et al., 2015; Chakraborty et al., 2018) . The key idea is to find a minimal perturbation that maximizes the risk of making wrong predictions. This minimax problem can be easily achieved by applying gradient descent over the continuous space of images (Miyato et al., 2017) .",
                "cite_spans": [
                    {
                        "start": 133,
                        "end": 158,
                        "text": "(Goodfellow et al., 2014;",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 159,
                        "end": 179,
                        "text": "Nguyen et al., 2015;",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 180,
                        "end": 205,
                        "text": "Chakraborty et al., 2018)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 417,
                        "end": 438,
                        "text": "(Miyato et al., 2017)",
                        "ref_id": "BIBREF14"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "However, adversarial attack for discrete data such as text remains challenging.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "Current successful attacks for text usually adopt heuristic rules to modify the characters of a word (Jin et al., 2019) , and substituting words with synonyms (Ren et al., 2019) . Li et al. (2018) ; Gao et al. (2018) apply perturbations based on word embeddings such as Glove (Pennington et al., 2014) , which is not strictly semantically and grammatically coordinated. Alzantot et al. (2018) adopts language models to score the perturbations generated by searching for close meaning words in the word embedding space (Mrk\u0161i\u0107 et al., 2016) , using a trial and error process to find possible perturbations, yet the perturbations generated are still not contextaware and heavily rely on cosine similarity measurement of word embeddings. Glove embeddings do not guarantee similar vector space with cosine similarity distance, therefore the perturbations are less semantically consistent. Jin et al. (2019) apply a semantically enhanced embedding (Mrk\u0161i\u0107 et al., 2016) , which is context unaware, thus less consistent with the unperturbed inputs. Liang et al. (2017) use phrase-level insertion and deletion, which produces unnatural sentences inconsistent with the original inputs, lacking fluency control. To preserve semantic information, Glockner et al. (2018) replace words manually to break the language inference system (Bowman et al., 2015) . Jia and Liang (2017) propose manual craft methods to attack machine reading comprehension systems. Lei et al. (2019) introduce replacement strategies using embedding transition.",
                "cite_spans": [
                    {
                        "start": 101,
                        "end": 119,
                        "text": "(Jin et al., 2019)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 159,
                        "end": 177,
                        "text": "(Ren et al., 2019)",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 180,
                        "end": 196,
                        "text": "Li et al. (2018)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 199,
                        "end": 216,
                        "text": "Gao et al. (2018)",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 276,
                        "end": 301,
                        "text": "(Pennington et al., 2014)",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 370,
                        "end": 392,
                        "text": "Alzantot et al. (2018)",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 518,
                        "end": 539,
                        "text": "(Mrk\u0161i\u0107 et al., 2016)",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 885,
                        "end": 902,
                        "text": "Jin et al. (2019)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 943,
                        "end": 964,
                        "text": "(Mrk\u0161i\u0107 et al., 2016)",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 1043,
                        "end": 1062,
                        "text": "Liang et al. (2017)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 1237,
                        "end": 1259,
                        "text": "Glockner et al. (2018)",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 1322,
                        "end": 1343,
                        "text": "(Bowman et al., 2015)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 1346,
                        "end": 1366,
                        "text": "Jia and Liang (2017)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 1445,
                        "end": 1462,
                        "text": "Lei et al. (2019)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Adversarial Attack for Text",
                "sec_num": null
            },
            {
                "text": "Although the above approaches have achieved good results, there is still much room for improvement regarding the perturbed percentage, attacking success rate, grammatical correctness and semantic consistency, etc. Moreover, the substitution strategies of these approaches are usually non-trivial, resulting in that they are limited to specific tasks.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Adversarial Attack for Text",
                "sec_num": null
            },
            {
                "text": "Pre-trained language models have become mainstream for many NLP tasks. Works such as (Wallace et al., 2019; Jin et al., 2019; Pruthi et al., 2019) have explored these pre-trained language models from many different angles. Wallace et al. (2019) explored the possible ethical problems of learned knowledge in pre-trained models.",
                "cite_spans": [
                    {
                        "start": 85,
                        "end": 107,
                        "text": "(Wallace et al., 2019;",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 108,
                        "end": 125,
                        "text": "Jin et al., 2019;",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 126,
                        "end": 146,
                        "text": "Pruthi et al., 2019)",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 223,
                        "end": 244,
                        "text": "Wallace et al. (2019)",
                        "ref_id": "BIBREF20"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Adversarial Attack against BERT",
                "sec_num": null
            },
            {
                "text": "Motivated by the interesting idea of turning BERT against BERT, we propose BERT-Attack, using the original BERT model to craft adversarial samples to fool the fine-tuned BERT model.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "BERT-Attack",
                "sec_num": "3"
            },
            {
                "text": "Our method consists of two steps: (1) finding the vulnerable words for the target model and then (2) replacing them with the semantically similar and grammatically correct words until a successful attack.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "BERT-Attack",
                "sec_num": "3"
            },
            {
                "text": "The most-vulnerable words are the keywords that help the target model make judgments. Perturbations over these words can be most beneficial in crafting adversarial samples. After finding which words that we are aimed to replace, we use masked language models to generate perturbations based on the top-K predictions from the masked language model.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "BERT-Attack",
                "sec_num": "3"
            },
            {
                "text": "Under the black-box scenario, the logit output by the target model (fine-tuned BERT or other neural models) is the only supervision we can get. We first select the words in the sequence which have a high significance influence on the final output logit.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Finding Vulnerable Words",
                "sec_num": "3.1"
            },
            {
                "text": "Let",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Finding Vulnerable Words",
                "sec_num": "3.1"
            },
            {
                "text": "S = [w 0 , \u2022 \u2022 \u2022 , w i \u2022 \u2022 \u2022 ]",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Finding Vulnerable Words",
                "sec_num": "3.1"
            },
            {
                "text": "denote the input sentence, and o y (S) denote the logit output by the target model for correct label y, the importance score I w i is defined as",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Finding Vulnerable Words",
                "sec_num": "3.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "I w i = o y (S) -o y (S \\w i ),",
                        "eq_num": "(1)"
                    }
                ],
                "section": "Finding Vulnerable Words",
                "sec_num": "3.1"
            },
            {
                "text": "where",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Finding Vulnerable Words",
                "sec_num": "3.1"
            },
            {
                "text": "S \\w i = [w 0 , \u2022 \u2022 \u2022 , w i-1 , [MASK], w i+1 , \u2022 \u2022 \u2022 ] is the sentence after replacing w i with [MASK].",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Finding Vulnerable Words",
                "sec_num": "3.1"
            },
            {
                "text": "Then we rank all the words according to the ranking score I w i in descending order to create word list L. We only take percent of the most important words since we tend to keep perturbations minimum.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Finding Vulnerable Words",
                "sec_num": "3.1"
            },
            {
                "text": "This process maximizes the risk of making wrong predictions, which is previously done by calculating gradients in image domains. The problem is then formulated as replacing these most vulnerable words with semantically consistent perturbations.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Finding Vulnerable Words",
                "sec_num": "3.1"
            },
            {
                "text": "After finding the vulnerable words, we iteratively replace the words in list L one by one to find perturbations that can mislead the target model. Previous approaches usually use multiple human-crafted rules to ensure the generated example is semantically consistent with the original one and grammatically correct, such as a synonym dictionary (Ren et al., 2019), POS checker (Jin et al., 2019) , semantic similarity checker (Jin et al., 2019 ), etc. Alzantot et al. (2018) applies a traditional language model to score the perturbed sentence at every attempt of replacing a word. These strategies of generating substitutes are unaware of the context between the substitution positions (usually using language models to test the substitutions), thus are insufficient in fluency control and semantic consistency. More importantly, using language models or POS checkers in scoring the perturbed samples is costly since this trial and error process requires massive inference time.",
                "cite_spans": [
                    {
                        "start": 377,
                        "end": 395,
                        "text": "(Jin et al., 2019)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 426,
                        "end": 443,
                        "text": "(Jin et al., 2019",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 444,
                        "end": 474,
                        "text": "), etc. Alzantot et al. (2018)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word Replacement via BERT",
                "sec_num": "3.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "{ subword of w i Full-Permutation of top-K predictions BERT \u2026 \u2026 p i k \u2026 p i 2 p i+1 1 p i+1 k \u2026 p i+1 2 p i+2 1 p i+2 k \u2026 p i+2 2 p i 1 c 1 c 2 c k \u2026 { Rank Target model \u2026 w0 c 1 c 2 c k Iterate \u2026 w1 ow i oc 1 oc 2 oc k \u2026 \u2026",
                        "eq_num": "w0"
                    }
                ],
                "section": "Word Replacement via BERT",
                "sec_num": "3.2"
            },
            {
                "text": "To overcome the lack of fluency control and semantic preservation by using synonyms or similar words in the embedding space, we leverage BERT for word replacement. The genuine nature of the masked language model makes sure that the generated sentences are relatively fluent and grammar-correct, also preserve most semantic information, which is later confirmed by human evaluators. Further, compared with previous approaches using rule-based perturbation strategies, the masked language model prediction is contextaware, thus dynamically searches for perturbations rather than simple synonyms replacing.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word Replacement via BERT",
                "sec_num": "3.2"
            },
            {
                "text": "Different from previous methods using complicated strategies to score and constrain the perturbations, the contextualized perturbation generator generates minimal perturbations with only one forward pass. Without running additional neural models to score the sentence, the time-consuming part is accessing the target model only. Therefore the process is extremely efficient.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word Replacement via BERT",
                "sec_num": "3.2"
            },
            {
                "text": "Algorithm 1 BERT-Attack 1: procedure WORD IMPORTANCE RANKING 2: S = [w 0 , w 1 , \u2022 \u2022 \u2022 ] // input: tokenized sentence 3: Y \u2190 gold-label 4:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word Replacement via BERT",
                "sec_num": "3.2"
            },
            {
                "text": "for w i in S do 5: calculate importance score I w i using Eq. 1",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word Replacement via BERT",
                "sec_num": "3.2"
            },
            {
                "text": "6: select word list L = [w top-1 , w top-2 , \u2022 \u2022 \u2022 ] 7:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word Replacement via BERT",
                "sec_num": "3.2"
            },
            {
                "text": "// sort S using I w i in descending order and collect top -K words 8: procedure REPLACEMENT USING BERT 9: if o y (S ) < o y (S adv ) then 25:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word Replacement via BERT",
                "sec_num": "3.2"
            },
            {
                "text": "H = [h 0 , \u2022 \u2022 \u2022 , h n ] // sub-",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word Replacement via BERT",
                "sec_num": "3.2"
            },
            {
                "text": "for c k in C do 20: S = [w 0 , \u2022 \u2022 \u2022 , w j-1 , c k , \u2022 \u2022 \u2022 ] // attempt 21: if argmax(o y (S ))! = Y then",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word Replacement via BERT",
                "sec_num": "3.2"
            },
            {
                "text": "S adv = [w 0 , \u2022 \u2022 \u2022 , w j-1 , c, \u2022 \u2022 \u2022 ] // do one perturbation 26:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word Replacement via BERT",
                "sec_num": "3.2"
            },
            {
                "text": "return None Thus, using the masked language model as a contextualized perturbation generator can be one possible solution to craft high-quality adversarial samples efficiently.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word Replacement via BERT",
                "sec_num": "3.2"
            },
            {
                "text": "As seen in Figure 1 , given a chosen word w to be replaced, we apply BERT to predict the possible words that are similar to w yet can mislead the target model. Instead of following the masked language model settings, we do not mask the chosen word w and use the original sequence as input, which can generate more semantic-consistent substitutes (Zhou et al., 2019) . For instance, given a sequence \"I like the cat.\", if we mask the word cat, it would be very hard for a masked language model to predict the original word cat since it could be just as fluent if the sequence is \"I like the dog.\". Further, if we mask out the given word w, for each iteration we would have to rerun the masked language model prediction process which is costly.",
                "cite_spans": [
                    {
                        "start": 346,
                        "end": 365,
                        "text": "(Zhou et al., 2019)",
                        "ref_id": "BIBREF25"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 18,
                        "end": 19,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Word Replacement Strategy",
                "sec_num": "3.2.1"
            },
            {
                "text": "Since BERT uses Bytes-Pair-Encoding (BPE)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word Replacement Strategy",
                "sec_num": "3.2.1"
            },
            {
                "text": "to tokenize the sequence S = [w 0 , \u2022 \u2022 \u2022 , w i , \u2022 \u2022 \u2022 ] into sub-word tokens: H = [h 0 , h 1 , h 2 , \u2022 \u2022 \u2022 ],",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word Replacement Strategy",
                "sec_num": "3.2.1"
            },
            {
                "text": "we need to align the chosen word to its corresponding sub-words in BERT.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word Replacement Strategy",
                "sec_num": "3.2.1"
            },
            {
                "text": "Let M denote the BERT model, we feed the tokenized sequence H into the BERT M to get output prediction P = M(H). Instead of using the argmax prediction, we take the most possible K predictions at each position, where K is a hyperparameter.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word Replacement Strategy",
                "sec_num": "3.2.1"
            },
            {
                "text": "We iterate words that are sorted by word importance ranking process to find perturbations. The BERT model uses BPE encoding to construct vocabularies. While most words are still single words, rare words are tokenized into sub-words. Therefore, we treat single words and sub-words separately to generate the substitutes.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word Replacement Strategy",
                "sec_num": "3.2.1"
            },
            {
                "text": "Single words For a single word w j , we make attempts using the corresponding top-K prediction candidates P j . We first filter out stop words collected from NLTK; for sentiment classifica-tion tasks we filter out antonyms using synonym dictionaries (Mrk\u0161i\u0107 et al., 2016) since BERT masked language model does not distinguish synonyms and antonyms. Then for given candidate c k we construct a perturbed sequence H",
                "cite_spans": [
                    {
                        "start": 250,
                        "end": 271,
                        "text": "(Mrk\u0161i\u0107 et al., 2016)",
                        "ref_id": "BIBREF15"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word Replacement Strategy",
                "sec_num": "3.2.1"
            },
            {
                "text": "= [h 0 , \u2022 \u2022 \u2022 , h j-1 , c k , h j+1 \u2022 \u2022 \u2022 ].",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word Replacement Strategy",
                "sec_num": "3.2.1"
            },
            {
                "text": "If the target model is already fooled to predict incorrectly, we break the loop to obtain the final adversarial sample H adv ; otherwise, we select from the filtered candidates to pick one best perturbation and turn to the next word in word list L.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word Replacement Strategy",
                "sec_num": "3.2.1"
            },
            {
                "text": "Sub-words For a word that is tokenized into subwords in BERT, we cannot obtain its substitutes directly. Thus we use the perplexity of sub-word combinations to find suitable word substitutes from predictions in the sub-word level. Given sub-words",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word Replacement Strategy",
                "sec_num": "3.2.1"
            },
            {
                "text": "[h 0 , h 1 , \u2022 \u2022 \u2022 , h t ]",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word Replacement Strategy",
                "sec_num": "3.2.1"
            },
            {
                "text": "of word w, we list all possible combinations from the prediction P \u2208t\u00d7K from M, which is K t sub-word combinations, we can convert them back to normal words by reversing the BERT tokenization process. We feed these combinations into the BERT-MLM to get the perplexity of these combinations. Then we rank the perplexity of all combinations to get the top-K combinations to find the suitable sub-word combinations.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word Replacement Strategy",
                "sec_num": "3.2.1"
            },
            {
                "text": "Given the suitable perturbations, we replace the original word with the most likely perturbation and repeat this process by iterating the importance word ranking list to find the final adversarial sample. In this way, we acquire the adversarial samples S adv effectively since we only iterate the masked language model once and do perturbations using the masked language model without other checking strategies.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word Replacement Strategy",
                "sec_num": "3.2.1"
            },
            {
                "text": "We summarize the two-step BERT-Attack process in Algorithm 1.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word Replacement Strategy",
                "sec_num": "3.2.1"
            },
            {
                "text": "We apply our method to attack different types of NLP tasks in the form of text classification and natural language inference. Following Jin et al. (2019) , we evaluate our method on 1k test samples randomly selected from the test set of the given task which are the same splits used by Alzantot et al. (2018) ; Jin et al. (2019) . The GA method only uses a subset of 50 samples in the FAKE, IMDB dataset.",
                "cite_spans": [
                    {
                        "start": 136,
                        "end": 153,
                        "text": "Jin et al. (2019)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 286,
                        "end": 308,
                        "text": "Alzantot et al. (2018)",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 311,
                        "end": 328,
                        "text": "Jin et al. (2019)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Datasets",
                "sec_num": "4.1"
            },
            {
                "text": "We use different types of text classification tasks to study the effectiveness of our method.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Text Classification",
                "sec_num": null
            },
            {
                "text": "\u2022 Yelp Review classification dataset, containing.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Text Classification",
                "sec_num": null
            },
            {
                "text": "Following Zhang et al. (2015) , we process the dataset to construct a polarity classification task.",
                "cite_spans": [
                    {
                        "start": 10,
                        "end": 29,
                        "text": "Zhang et al. (2015)",
                        "ref_id": "BIBREF24"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Text Classification",
                "sec_num": null
            },
            {
                "text": "\u2022 IMDB Document-level movie review dataset, where the average sequence length is longer than the Yelp dataset. We process the dataset into a polarity classification task1 . \u2022 AG's News Sentence level news-type classification dataset, containing 4 types of news: World, Sports, Business, and Science. \u2022 FAKE Fake News Classification dataset, detecting whether a news document is fake from Kaggle Fake News Challenge2 .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Text Classification",
                "sec_num": null
            },
            {
                "text": "\u2022 SNLI Stanford language inference task (Bowman et al., 2015) . Given one premise and one hypothesis, and the goal is to predict if the hypothesis is entailment, neural, or contradiction of the premise. \u2022 MNLI Language inference dataset on multigenre texts, covering transcribed speech, popular fiction, and government reports (Williams et al., 2018) , which is more complicated with diversified written and spoken style texts, compared with the SNLI dataset, including eval data matched with training domains and eval data mismatched with training domains.",
                "cite_spans": [
                    {
                        "start": 40,
                        "end": 61,
                        "text": "(Bowman et al., 2015)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 327,
                        "end": 350,
                        "text": "(Williams et al., 2018)",
                        "ref_id": "BIBREF21"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Natural Language Inference",
                "sec_num": null
            },
            {
                "text": "To measure the quality of the generated samples, we set up various automatic evaluation metrics. The success rate, which is the counter-part of afterattack accuracy, is the core metric measuring the success of the attacking method. Meanwhile, the perturbed percentage is also crucial since, generally, less perturbation results in more semantic consistency. Further, under the black-box setting, queries of the target model are the only accessible information. Constant queries for one sample is less applicable. Thus query number per sample is also a key metric. As used in TextFooler (Jin et al., 2019) , we also use Universal Sentence Encoder (Cer et al., 2018) to measure the semantic consistency between the adversarial sample and the original sequence. To balance between semantic preservation and attack success rate, we set up a threshold of semantic similarity score to filter the less similar examples. ",
                "cite_spans": [
                    {
                        "start": 586,
                        "end": 604,
                        "text": "(Jin et al., 2019)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 646,
                        "end": 664,
                        "text": "(Cer et al., 2018)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Automatic Evaluation Metrics",
                "sec_num": "4.2"
            },
            {
                "text": "As shown in Table 1 , the BERT-Attack method successfully fool its downstream fine-tuned model. In both text classification and natural language inference tasks, the fine-tuned BERTs fail to classify the generated adversarial samples correctly. The average after-attack accuracy is lower than 10%, indicating that most samples are successfully perturbed to fool the state-of-the-art classification models. Meanwhile, the perturb percentage is less than 10 %, which is significantly less than previous works.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 18,
                        "end": 19,
                        "text": "1",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Attacking Results",
                "sec_num": "4.3"
            },
            {
                "text": "Further, BERT-Attack successfully attacked all tasks listed, which are in diversified domains such as News classification, review classification, language inference in different domains. The results indicate that the attacking method is robust in different tasks. Compared with the strong baseline introduced by Jin et al. (2019) 3 and Alzantot et al. (2018) 4 , the BERT-Attack method is more efficient 3 https://github.com/jind11/TextFooler 4 https://github.com/QData/TextAttack and more imperceptible. The query number and the perturbation percentage of our method are much less.",
                "cite_spans": [
                    {
                        "start": 312,
                        "end": 329,
                        "text": "Jin et al. (2019)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Attacking Results",
                "sec_num": "4.3"
            },
            {
                "text": "We can observe that it is generally easier to attack the review classification task since the perturb percentage is incredibly low. BERT-Attack can mislead the target model by replacing a handful of words only. Since the average sequence length is relatively long, the target model tends to make judgments by only a few words in a sequence, which is not the natural way of human prediction. Thus, the perturbation of these keywords would result in incorrect prediction from the target model, revealing the vulnerability of it.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Attacking Results",
                "sec_num": "4.3"
            },
            {
                "text": "For further evaluation of the generated adversarial samples, we set up human evaluations to measure the quality of the generated samples in fluency and grammar as well as semantic preservation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Human Evaluations",
                "sec_num": "4.4"
            },
            {
                "text": "We ask human judges to score the grammar correctness of the mixed sentences of generated ad-versarial samples and original sequences, scoring from 1-5 following Jin et al. (2019) . Then we ask human judges to make predictions in a shuffled mix of original and adversarial texts. We use the IMDB dataset and the MNLI dataset, and for each task, we select 100 samples of both original and adversarial samples for human judges. We ask three human annotators to evaluate the examples. For label prediction, we take the majority class as the predicted label, and for semantic and grammar check we use an average score among the annotators.",
                "cite_spans": [
                    {
                        "start": 161,
                        "end": 178,
                        "text": "Jin et al. (2019)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Human Evaluations",
                "sec_num": "4.4"
            },
            {
                "text": "Seen in Table 2 , the semantic score and the grammar score of the adversarial samples are close to the original ones. MNLI task is a sentence pair prediction task constructed by human crafted hypotheses based on the premises, therefore original pairs share a considerable amount of same words. Perturbations on these words would make it difficult for human judges to predict correctly therefore the accuracy is lower than simple sentence classification tasks. ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 14,
                        "end": 15,
                        "text": "2",
                        "ref_id": "TABREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Human Evaluations",
                "sec_num": "4.4"
            },
            {
                "text": "The BERT-Attack method is also applicable in attacking other target models, not limited to its fine-tuned model only. As seen in Table 3 , the attack is successful against LSTM-based models, indicating that BERT-Attack is feasible for a wide range of models. Under BERT-Attack, the ESIM model is more robust in the MNLI dataset. We assume that encoding two sentences separately gets higher robustness. In attacking BERT-large models, the performance is also excellent, indicating that BERT-Attack is successful in attacking different pre-trained models not only against its own finetuned downstream models.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 135,
                        "end": 136,
                        "text": "3",
                        "ref_id": "TABREF3"
                    }
                ],
                "eq_spans": [],
                "section": "BERT-Attack against Other Models",
                "sec_num": "4.5"
            },
            {
                "text": "5 Ablations and Discussions",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "BERT-Attack against Other Models",
                "sec_num": "4.5"
            },
            {
                "text": "The candidate pool range is the major hyperparameter used in the BERT-Attack algorithm. As seen in Figure 2 , the attack rate is rising along with the candidate size increasing. Intuitively, a larger K would result in less semantic similarity. However, the semantic measure via Universal Sentence Encoder is maintained in a stable range, (experiments show that semantic similarities drop less than 2%), indicating that the candidates are all reasonable and semantically consistent with the original sentence.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 106,
                        "end": 107,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Importance of Candidate Numbers",
                "sec_num": "5.1"
            },
            {
                "text": "Further, a fixed candidate number could be rigid in practical usage, so we run a test using a threshold to cut off candidates that are less possible as a plausible perturbation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Importance of Candidate Numbers",
                "sec_num": "5.1"
            },
            {
                "text": "As seen in Table 4 , when using a flexible threshold to cut off unsuitable candidates, the attacking process has a lower query number. This indicates that some candidates predicted by the masked language model with a lower prediction score may not be meaningful so skipping these candidates can save the unnecessary queries. ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 17,
                        "end": 18,
                        "text": "4",
                        "ref_id": "TABREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Importance of Candidate Numbers",
                "sec_num": "5.1"
            },
            {
                "text": "The BERT-Attack method is based on the contextualized masked language model. Thus the sequence length plays an important role in the high-quality perturbation process. As seen, instead of the previous methods focusing on attacking the hypothesis of the NLI task, we aim at premises whose average length is longer. This is because we believe that contextual replacement would be less reasonable when dealing with extremely short sequences.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Importance of Sequence Length",
                "sec_num": "5.2"
            },
            {
                "text": "To avoid such a problem, we believe that many word-level synonym replacement strategies can be combined with BERT-Attack, allowing the BERT-Attack method to be more applicable. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Importance of Sequence Length",
                "sec_num": "5.2"
            },
            {
                "text": "To test the transferability of the generated adversarial samples, we take samples aimed at different target models to attack other target models. Here, we use BERT-base as the masked language model for all different target models. As seen in Table 6 , samples are transferable in NLI task while less transferable in text classification. Meanwhile, we further fine-tune the target model using the generated adversarial samples from the train set and then test it on the test set used before. As seen in Table 5 , generated samples used in finetuning help the target model become more robust while accuracy is close to the model trained with clean datasets. The attack becomes more difficult, indicating that the model is harder to be attacked. Therefore, the generated dataset can be used as additional data for further exploration of making neural models more robust. ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 248,
                        "end": 249,
                        "text": "6",
                        "ref_id": "TABREF6"
                    },
                    {
                        "start": 508,
                        "end": 509,
                        "text": "5",
                        "ref_id": "TABREF5"
                    }
                ],
                "eq_spans": [],
                "section": "Transferability and Adversarial Training",
                "sec_num": "5.3"
            },
            {
                "text": "BPE method is currently the most efficient way to deal with a large number of words, as used in BERT.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Effects on Sub-Word Level Attack",
                "sec_num": "5.4"
            },
            {
                "text": "We establish a comparative experiment where we do not use the sub-word level attack. That is we skip those words that are tokenized with multiple sub-words.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Effects on Sub-Word Level Attack",
                "sec_num": "5.4"
            },
            {
                "text": "As seen in Table 7 , using the sub-word level attack can achieve higher performances, not only in higher attacking success rate but also in less perturbation percentage. ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 17,
                        "end": 18,
                        "text": "7",
                        "ref_id": "TABREF7"
                    }
                ],
                "eq_spans": [],
                "section": "Effects on Sub-Word Level Attack",
                "sec_num": "5.4"
            },
            {
                "text": "it is hard for a lover of the novel northanger abbey to sit through this bbc adaptation and to Negative keep from throwing objects at the tv screen... why are so many facts concerning the tilney family and mrs . tilney ' s death altered unnecessarily ? to make the story more ' horrible ? ' Adv it is hard for a lover of the novel northanger abbey to sit through this bbc adaptation and to Positive keep from throwing objects at the tv screen... why are so many facts concerning the tilney family and mrs . tilney ' s death altered unnecessarily ? to make the plot more ' horrible ? '",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Ori",
                "sec_num": null
            },
            {
                "text": "Ori i first seen this movie in the early 80s .. it really had nice picture quality too . anyways , i 'm Positive glad i found this movie again ... the part i loved best was when he hijacked the car from this poor guy... this is a movie i could watch over and over again . i highly recommend it .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "IMDB",
                "sec_num": null
            },
            {
                "text": "Adv i first seen this movie in the early 80s .. it really had nice picture quality too . anyways , i 'm Negative glad i found this movie again ... the part i loved best was when he hijacked the car from this poor guy... this is a movie i could watch over and over again . i inordinately recommend it . ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "IMDB",
                "sec_num": null
            },
            {
                "text": "Since BERT-Attack does not use language models or sentence encoders to measure the output sequence during the generation process, also, the query number is lower, therefore the runtime is faster than previous methods. As seen in Table 9 , BERT-Attack is much faster than generic algorithm (Alzantot et al., 2018) and 3 times faster then Textfooler.",
                "cite_spans": [
                    {
                        "start": 289,
                        "end": 312,
                        "text": "(Alzantot et al., 2018)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 235,
                        "end": 236,
                        "text": "9",
                        "ref_id": "TABREF10"
                    }
                ],
                "eq_spans": [],
                "section": "Runtime Comparison",
                "sec_num": "5.6"
            },
            {
                "text": "As seen in Table 10 , the generated adversarial samples are semantically consistent with its original input, while the target model makes incorrect predictions. In both review classification samples and language inference samples, the perturbations do not mislead human judges.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 17,
                        "end": 19,
                        "text": "10",
                        "ref_id": "TABREF11"
                    }
                ],
                "eq_spans": [],
                "section": "Examples of Generated Adversarial Sentences",
                "sec_num": "5.7"
            },
            {
                "text": "In this work, we propose a high-quality and effective method BERT-Attack to generate adversarial samples using BERT masked language model. Experiment results show that the proposed method achieves a high success rate while maintaining a minimum perturbation. Nevertheless, candidates generated from the masked language model can sometimes be antonyms or irrelevant to the original words, causing a semantic loss. Thus, enhancing language models to generate more semantically related perturbations can be one possible solution to perfect BERT-Attack in the future.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "6"
            },
            {
                "text": "https://datasets.imdbws.com/",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://www.kaggle.com/c/fake-news/data",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "We would like to thank the anonymous reviewers for their valuable comments. We are thankful for the help of Demin Song, Hang Yan and Pengfei Liu. This work was supported by the National Natural Science Foundation of China (No. 61751201, 62022027 and 61976056), Shanghai Municipal Science and Technology Major Project (No. 2018SHZDZX01) and ZJLab.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgments",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Generating natural language adversarial examples",
                "authors": [
                    {
                        "first": "Moustafa",
                        "middle": [],
                        "last": "Alzantot",
                        "suffix": ""
                    },
                    {
                        "first": "Yash",
                        "middle": [],
                        "last": "Sharma",
                        "suffix": ""
                    },
                    {
                        "first": "Ahmed",
                        "middle": [],
                        "last": "Elgohary",
                        "suffix": ""
                    },
                    {
                        "first": "Bo-Jhang",
                        "middle": [],
                        "last": "Ho",
                        "suffix": ""
                    },
                    {
                        "first": "Mani",
                        "middle": [
                            "B"
                        ],
                        "last": "Srivastava",
                        "suffix": ""
                    },
                    {
                        "first": "Kai-Wei",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani B. Srivastava, and Kai-Wei Chang. 2018. Generating natural language adversar- ial examples. CoRR, abs/1804.07998.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "A large annotated corpus for learning natural language inference",
                "authors": [
                    {
                        "first": "Gabor",
                        "middle": [],
                        "last": "Samuel R Bowman",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [],
                        "last": "Angeli",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Potts",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1508.05326"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Samuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. 2015. A large anno- tated corpus for learning natural language inference. arXiv preprint arXiv:1508.05326.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Chris Tar, et al. 2018. Universal sentence encoder",
                "authors": [
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Cer",
                        "suffix": ""
                    },
                    {
                        "first": "Yinfei",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Sheng-Yi",
                        "middle": [],
                        "last": "Kong",
                        "suffix": ""
                    },
                    {
                        "first": "Nan",
                        "middle": [],
                        "last": "Hua",
                        "suffix": ""
                    },
                    {
                        "first": "Nicole",
                        "middle": [],
                        "last": "Limtiaco",
                        "suffix": ""
                    },
                    {
                        "first": "Rhomni",
                        "middle": [],
                        "last": "St John",
                        "suffix": ""
                    },
                    {
                        "first": "Noah",
                        "middle": [],
                        "last": "Constant",
                        "suffix": ""
                    },
                    {
                        "first": "Mario",
                        "middle": [],
                        "last": "Guajardo-Cespedes",
                        "suffix": ""
                    },
                    {
                        "first": "Steve",
                        "middle": [],
                        "last": "Yuan",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1803.11175"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, et al. 2018. Universal sentence encoder. arXiv preprint arXiv:1803.11175.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Adversarial attacks and defences: A survey",
                "authors": [
                    {
                        "first": "Anirban",
                        "middle": [],
                        "last": "Chakraborty",
                        "suffix": ""
                    },
                    {
                        "first": "Manaar",
                        "middle": [],
                        "last": "Alam",
                        "suffix": ""
                    },
                    {
                        "first": "Vishal",
                        "middle": [],
                        "last": "Dey",
                        "suffix": ""
                    },
                    {
                        "first": "Anupam",
                        "middle": [],
                        "last": "Chattopadhyay",
                        "suffix": ""
                    },
                    {
                        "first": "Debdeep",
                        "middle": [],
                        "last": "Mukhopadhyay",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1810.00069"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Anirban Chakraborty, Manaar Alam, Vishal Dey, Anu- pam Chattopadhyay, and Debdeep Mukhopadhyay. 2018. Adversarial attacks and defences: A survey. arXiv preprint arXiv:1810.00069.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
                "authors": [
                    {
                        "first": "Jacob",
                        "middle": [],
                        "last": "Devlin",
                        "suffix": ""
                    },
                    {
                        "first": "Ming-Wei",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "Kenton",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Kristina",
                        "middle": [],
                        "last": "Toutanova",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: pre-training of deep bidirectional transformers for language under- standing. CoRR, abs/1810.04805.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Black-box generation of adversarial text sequences to evade deep learning classifiers",
                "authors": [
                    {
                        "first": "Ji",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Jack",
                        "middle": [],
                        "last": "Lanchantin",
                        "suffix": ""
                    },
                    {
                        "first": "Mary",
                        "middle": [
                            "Lou"
                        ],
                        "last": "Soffa",
                        "suffix": ""
                    },
                    {
                        "first": "Yanjun",
                        "middle": [],
                        "last": "Qi",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "2018 IEEE Security and Privacy Workshops (SPW)",
                "volume": "",
                "issue": "",
                "pages": "50--56",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ji Gao, Jack Lanchantin, Mary Lou Soffa, and Yan- jun Qi. 2018. Black-box generation of adversarial text sequences to evade deep learning classifiers. In 2018 IEEE Security and Privacy Workshops (SPW), pages 50-56.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Breaking nli systems with sentences that require simple lexical inferences",
                "authors": [
                    {
                        "first": "Max",
                        "middle": [],
                        "last": "Glockner",
                        "suffix": ""
                    },
                    {
                        "first": "Vered",
                        "middle": [],
                        "last": "Shwartz",
                        "suffix": ""
                    },
                    {
                        "first": "Yoav",
                        "middle": [],
                        "last": "Goldberg",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1805.02266"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Max Glockner, Vered Shwartz, and Yoav Goldberg. 2018. Breaking nli systems with sentences that require simple lexical inferences. arXiv preprint arXiv:1805.02266.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Explaining and harnessing adversarial examples",
                "authors": [
                    {
                        "first": "Ian",
                        "middle": [
                            "J"
                        ],
                        "last": "Goodfellow",
                        "suffix": ""
                    },
                    {
                        "first": "Jonathon",
                        "middle": [],
                        "last": "Shlens",
                        "suffix": ""
                    },
                    {
                        "first": "Christian",
                        "middle": [],
                        "last": "Szegedy",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1412.6572"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. 2014. Explaining and harnessing adversar- ial examples. arXiv preprint arXiv:1412.6572.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Adversarial examples for evaluating reading comprehension systems",
                "authors": [
                    {
                        "first": "Robin",
                        "middle": [],
                        "last": "Jia",
                        "suffix": ""
                    },
                    {
                        "first": "Percy",
                        "middle": [],
                        "last": "Liang",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1707.07328"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Robin Jia and Percy Liang. 2017. Adversarial exam- ples for evaluating reading comprehension systems. arXiv preprint arXiv:1707.07328.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Is BERT really robust? natural language attack on text classification and entailment",
                "authors": [
                    {
                        "first": "Di",
                        "middle": [],
                        "last": "Jin",
                        "suffix": ""
                    },
                    {
                        "first": "Zhijing",
                        "middle": [],
                        "last": "Jin",
                        "suffix": ""
                    },
                    {
                        "first": "Joey",
                        "middle": [
                            "Tianyi"
                        ],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Peter",
                        "middle": [],
                        "last": "Szolovits",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. 2019. Is BERT really robust? natural language attack on text classification and entailment. CoRR, abs/1907.11932.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Adversarial examples in the physical world",
                "authors": [
                    {
                        "first": "Alexey",
                        "middle": [],
                        "last": "Kurakin",
                        "suffix": ""
                    },
                    {
                        "first": "Ian",
                        "middle": [],
                        "last": "Goodfellow",
                        "suffix": ""
                    },
                    {
                        "first": "Samy",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1607.02533"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Alexey Kurakin, Ian Goodfellow, and Samy Bengio. 2016. Adversarial examples in the physical world. arXiv preprint arXiv:1607.02533.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Discrete adversarial attacks and submodular optimization with applications to text classification",
                "authors": [
                    {
                        "first": "Qi",
                        "middle": [],
                        "last": "Lei",
                        "suffix": ""
                    },
                    {
                        "first": "Lingfei",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Pin-Yu",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Alexandros",
                        "middle": [
                            "G"
                        ],
                        "last": "Dimakis",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Inderjit S Dhillon",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Witbrock",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Systems and Machine Learning (SysML)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Qi Lei, Lingfei Wu, Pin-Yu Chen, Alexandros G Di- makis, Inderjit S Dhillon, and Michael Witbrock. 2019. Discrete adversarial attacks and submodular optimization with applications to text classification. Systems and Machine Learning (SysML).",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Textbugger: Generating adversarial text against real-world applications",
                "authors": [
                    {
                        "first": "Jinfeng",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Shouling",
                        "middle": [],
                        "last": "Ji",
                        "suffix": ""
                    },
                    {
                        "first": "Tianyu",
                        "middle": [],
                        "last": "Du",
                        "suffix": ""
                    },
                    {
                        "first": "Bo",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Ting",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1812.05271"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, and Ting Wang. 2018. Textbugger: Generating adversarial text against real-world applications. arXiv preprint arXiv:1812.05271.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Deep text classification can be fooled",
                "authors": [
                    {
                        "first": "Bin",
                        "middle": [],
                        "last": "Liang",
                        "suffix": ""
                    },
                    {
                        "first": "Hongcheng",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Miaoqiang",
                        "middle": [],
                        "last": "Su",
                        "suffix": ""
                    },
                    {
                        "first": "Pan",
                        "middle": [],
                        "last": "Bian",
                        "suffix": ""
                    },
                    {
                        "first": "Xirong",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Wenchang",
                        "middle": [],
                        "last": "Shi",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1704.08006"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Bin Liang, Hongcheng Li, Miaoqiang Su, Pan Bian, Xirong Li, and Wenchang Shi. 2017. Deep text classification can be fooled. arXiv preprint arXiv:1704.08006.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Virtual adversarial training: A regularization method for supervised and semisupervised learning",
                "authors": [
                    {
                        "first": "Takeru",
                        "middle": [],
                        "last": "Miyato",
                        "suffix": ""
                    },
                    {
                        "first": "Masanori",
                        "middle": [],
                        "last": "Shin Ichi Maeda",
                        "suffix": ""
                    },
                    {
                        "first": "Shin",
                        "middle": [],
                        "last": "Koyama",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Ishii",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "41",
                "issue": "",
                "pages": "1979--1993",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Takeru Miyato, Shin ichi Maeda, Masanori Koyama, and Shin Ishii. 2017. Virtual adversarial training: A regularization method for supervised and semi- supervised learning. volume 41, pages 1979-1993.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Counter-fitting word vectors to linguistic constraints",
                "authors": [
                    {
                        "first": "Nikola",
                        "middle": [],
                        "last": "Mrk\u0161i\u0107",
                        "suffix": ""
                    },
                    {
                        "first": "O",
                        "middle": [],
                        "last": "Diarmuid",
                        "suffix": ""
                    },
                    {
                        "first": "Blaise",
                        "middle": [],
                        "last": "S\u00e9aghdha",
                        "suffix": ""
                    },
                    {
                        "first": "Milica",
                        "middle": [],
                        "last": "Thomson",
                        "suffix": ""
                    },
                    {
                        "first": "Lina",
                        "middle": [],
                        "last": "Ga\u0161i\u0107",
                        "suffix": ""
                    },
                    {
                        "first": "Pei-Hao",
                        "middle": [],
                        "last": "Rojas-Barahona",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Su",
                        "suffix": ""
                    },
                    {
                        "first": "Tsung-Hsien",
                        "middle": [],
                        "last": "Vandyke",
                        "suffix": ""
                    },
                    {
                        "first": "Steve",
                        "middle": [],
                        "last": "Wen",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Young",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1603.00892"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Nikola Mrk\u0161i\u0107, Diarmuid O S\u00e9aghdha, Blaise Thom- son, Milica Ga\u0161i\u0107, Lina Rojas-Barahona, Pei- Hao Su, David Vandyke, Tsung-Hsien Wen, and Steve Young. 2016. Counter-fitting word vec- tors to linguistic constraints. arXiv preprint arXiv:1603.00892.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images",
                "authors": [
                    {
                        "first": "Anh",
                        "middle": [],
                        "last": "Nguyen",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Yosinski",
                        "suffix": ""
                    },
                    {
                        "first": "Jeff",
                        "middle": [],
                        "last": "Clune",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
                "volume": "",
                "issue": "",
                "pages": "427--436",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Anh Nguyen, Jason Yosinski, and Jeff Clune. 2015. Deep neural networks are easily fooled: High con- fidence predictions for unrecognizable images. In Proceedings of the IEEE conference on computer vi- sion and pattern recognition, pages 427-436.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Glove: Global vectors for word representation",
                "authors": [
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Pennington",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of the conference on empirical methods in natural language processing",
                "volume": "",
                "issue": "",
                "pages": "1532--1543",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the conference on empirical methods in natural language processing, pages 1532-1543.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Combating adversarial misspellings with robust word recognition",
                "authors": [
                    {
                        "first": "Danish",
                        "middle": [],
                        "last": "Pruthi",
                        "suffix": ""
                    },
                    {
                        "first": "Bhuwan",
                        "middle": [],
                        "last": "Dhingra",
                        "suffix": ""
                    },
                    {
                        "first": "Zachary",
                        "middle": [
                            "C"
                        ],
                        "last": "Lipton",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1905.11268"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Danish Pruthi, Bhuwan Dhingra, and Zachary C Lip- ton. 2019. Combating adversarial misspellings with robust word recognition. arXiv preprint arXiv:1905.11268.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Generating natural language adversarial examples through probability weighted word saliency",
                "authors": [
                    {
                        "first": "Yihe",
                        "middle": [],
                        "last": "Shuhuai Ren",
                        "suffix": ""
                    },
                    {
                        "first": "Kun",
                        "middle": [],
                        "last": "Deng",
                        "suffix": ""
                    },
                    {
                        "first": "Wanxiang",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Che",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "1085--1097",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Shuhuai Ren, Yihe Deng, Kun He, and Wanxiang Che. 2019. Generating natural language adversarial ex- amples through probability weighted word saliency. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1085-1097.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Universal adversarial triggers for attacking and analyzing NLP",
                "authors": [
                    {
                        "first": "Eric",
                        "middle": [],
                        "last": "Wallace",
                        "suffix": ""
                    },
                    {
                        "first": "Shi",
                        "middle": [],
                        "last": "Feng",
                        "suffix": ""
                    },
                    {
                        "first": "Nikhil",
                        "middle": [],
                        "last": "Kandpal",
                        "suffix": ""
                    },
                    {
                        "first": "Matt",
                        "middle": [],
                        "last": "Gardner",
                        "suffix": ""
                    },
                    {
                        "first": "Sameer",
                        "middle": [],
                        "last": "Singh",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. 2019. Universal adversarial trig- gers for attacking and analyzing NLP. Empirical Methods in Natural Language Processing.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "A broad-coverage challenge corpus for sentence understanding through inference",
                "authors": [
                    {
                        "first": "Adina",
                        "middle": [],
                        "last": "Williams",
                        "suffix": ""
                    },
                    {
                        "first": "Nikita",
                        "middle": [],
                        "last": "Nangia",
                        "suffix": ""
                    },
                    {
                        "first": "Samuel",
                        "middle": [],
                        "last": "Bowman",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "volume": "",
                "issue": "",
                "pages": "1112--1122",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sen- tence understanding through inference. In Proceed- ings of the Conference of the North American Chap- ter of the Association for Computational Linguistics: Human Language Technologies, pages 1112-1122.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Greedy attack and gumbel attack: Generating adversarial examples for discrete data",
                "authors": [
                    {
                        "first": "Puyudi",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Jianbo",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Cho-Jui",
                        "middle": [],
                        "last": "Hsieh",
                        "suffix": ""
                    },
                    {
                        "first": "Jane-Ling",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [
                            "I"
                        ],
                        "last": "Jordan",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1805.12316"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Puyudi Yang, Jianbo Chen, Cho-Jui Hsieh, Jane-Ling Wang, and Michael I Jordan. 2018. Greedy attack and gumbel attack: Generating adversarial examples for discrete data. arXiv preprint arXiv:1805.12316.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Word-level textual adversarial attacking as combinatorial optimization",
                "authors": [
                    {
                        "first": "Yuan",
                        "middle": [],
                        "last": "Zang",
                        "suffix": ""
                    },
                    {
                        "first": "Fanchao",
                        "middle": [],
                        "last": "Qi",
                        "suffix": ""
                    },
                    {
                        "first": "Chenghao",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Zhiyuan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Meng",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Qun",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Maosong",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "6066--6080",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yuan Zang, Fanchao Qi, Chenghao Yang, Zhiyuan Liu, Meng Zhang, Qun Liu, and Maosong Sun. 2020. Word-level textual adversarial attacking as combina- torial optimization. In Proceedings of the 58th An- nual Meeting of the Association for Computational Linguistics, pages 6066-6080.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Character-level convolutional networks for text classification",
                "authors": [
                    {
                        "first": "Xiang",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Junbo",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Yann",
                        "middle": [],
                        "last": "Lecun",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Advances in neural information processing systems",
                "volume": "",
                "issue": "",
                "pages": "649--657",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text clas- sification. In Advances in neural information pro- cessing systems, pages 649-657.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "BERT-based lexical substitution",
                "authors": [
                    {
                        "first": "Wangchunshu",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Tao",
                        "middle": [],
                        "last": "Ge",
                        "suffix": ""
                    },
                    {
                        "first": "Ke",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Furu",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Ming",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "3368--3373",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/P19-1328"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Wangchunshu Zhou, Tao Ge, Ke Xu, Furu Wei, and Ming Zhou. 2019. BERT-based lexical substitution. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3368-3373, Florence, Italy. Association for Compu- tational Linguistics.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 1: One step of our replacement strategy.",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table><tr><td>10:</td><td>generate top-K candidates for all sub-words using BERT and get P \u2208n\u00d7K</td></tr><tr><td>11:</td><td>for w j in L do</td></tr><tr><td>12:</td><td>if w j is a whole word then</td></tr><tr><td>13:</td><td>get candidate C = F ilter(P j )</td></tr><tr><td>14:</td><td>replace word w j</td></tr><tr><td>15:</td><td>else</td></tr><tr><td>16:</td><td>get candidate C using PPL ranking and Filter</td></tr><tr><td>17:</td><td>replace sub-words [h j , \u2022 \u2022 \u2022 , h j+t ]</td></tr><tr><td>18:</td><td>Find Possible Adversarial Sample</td></tr><tr><td>19:</td><td/></tr></table>",
                "type_str": "table",
                "text": "word tokenized sequence of S",
                "html": null,
                "num": null
            },
            "TABREF1": {
                "content": "<table><tr><td>Dataset</td><td>Method</td><td colspan=\"6\">Original Acc Attacked Acc Perturb % Query Number Avg Len Semantic Sim</td></tr><tr><td/><td>BERT-Attack(ours)</td><td/><td>15.5</td><td>1.1</td><td>1558</td><td/><td>0.81</td></tr><tr><td>Fake</td><td>TextFooler(Jin et al., 2019)</td><td>97.8</td><td>19.3</td><td>11.7</td><td>4403</td><td>885</td><td>0.76</td></tr><tr><td/><td>GA(Alzantot et al., 2018)</td><td/><td>58.3</td><td>1.1</td><td>28508</td><td/><td>-</td></tr><tr><td/><td>BERT-Attack(ours)</td><td/><td>5.1</td><td>4.1</td><td>273</td><td/><td>0.77</td></tr><tr><td>Yelp</td><td>TextFooler</td><td>95.6</td><td>6.6</td><td>12.8</td><td>743</td><td>157</td><td>0.74</td></tr><tr><td/><td>GA</td><td/><td>31.0</td><td>10.1</td><td>6137</td><td/><td>-</td></tr><tr><td/><td>BERT-Attack(ours)</td><td/><td>11.4</td><td>4.4</td><td>454</td><td/><td>0.86</td></tr><tr><td>IMDB</td><td>TextFooler</td><td>90.9</td><td>13.6</td><td>6.1</td><td>1134</td><td>215</td><td>0.86</td></tr><tr><td/><td>GA</td><td/><td>45.7</td><td>4.9</td><td>6493</td><td/><td>-</td></tr><tr><td/><td>BERT-Attack(ours)</td><td/><td>10.6</td><td>15.4</td><td>213</td><td/><td>0.63</td></tr><tr><td>AG</td><td>TextFooler</td><td>94.2</td><td>12.5</td><td>22.0</td><td>357</td><td>43</td><td>0.57</td></tr><tr><td/><td>GA</td><td/><td>51</td><td>16.9</td><td>3495</td><td/><td>-</td></tr><tr><td/><td>BERT-Attack(ours)</td><td/><td>7.4/16.1</td><td>12.4/9.3</td><td>16/30</td><td/><td>0.40/0.55</td></tr><tr><td>SNLI</td><td>TextFooler</td><td>89.4(H/P)</td><td>4.0/20.8</td><td>18.5/33.4</td><td>60/142</td><td>8/18</td><td>0.45/0.54</td></tr><tr><td/><td>GA</td><td/><td>14.7/-</td><td>20.8/-</td><td>613/-</td><td/><td>-</td></tr><tr><td/><td>BERT-Attack(ours)</td><td/><td>7.9/11.9</td><td>8.8/7.9</td><td>19/44</td><td/><td>0.55/0.68</td></tr><tr><td>MNLI matched</td><td>TextFooler</td><td>85.1(H/P)</td><td>9.6/25.3</td><td>15.2/26.5</td><td>78/152</td><td>11/21</td><td>0.57/0.65</td></tr><tr><td/><td>GA</td><td/><td>21.8/-</td><td>18.2/-</td><td>692/-</td><td/><td>-</td></tr><tr><td/><td>BERT-Attack(ours)</td><td/><td>7/13.7</td><td>8.0/7.1</td><td>24/43</td><td/><td>0.53/0.69</td></tr><tr><td>MNLI mismatched</td><td>TextFooler</td><td>82.1(H/P)</td><td>8.3/22.9</td><td>14.6/24.7</td><td>86/162</td><td>12/22</td><td>0.58/0.65</td></tr><tr><td/><td>GA</td><td/><td>20.9/-</td><td>19.0/-</td><td>737/-</td><td/><td>-</td></tr></table>",
                "type_str": "table",
                "text": "Results of attacking against various fine-tuned BERT models. TextFooler is the state-of-the-art baseline.",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table><tr><td colspan=\"2\">Dataset</td><td colspan=\"3\">Accuracy Semantic Grammar</td></tr><tr><td>MNLI</td><td>Original Adversarial</td><td>0.90 0.70</td><td>3.9 3.7</td><td>4.0 3.6</td></tr><tr><td>IMDB</td><td>Original Adversarial</td><td>0.91 0.85</td><td>4.1 3.9</td><td>3.9 3.7</td></tr></table>",
                "type_str": "table",
                "text": "Human-Evaluation Results.",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table><tr><td colspan=\"2\">Dataset</td><td>Model</td><td colspan=\"3\">Ori Acc Atk Acc Perturb %</td></tr><tr><td/><td>IMDB</td><td colspan=\"2\">Word-LSTM 89.8</td><td>10.2</td><td>2.7</td></tr><tr><td/><td/><td colspan=\"2\">BERT-Large 98.2</td><td>12.4</td><td>2.9</td></tr><tr><td/><td>Yelp</td><td colspan=\"2\">Word-LSTM 96.0</td><td>1.1</td><td>4.7</td></tr><tr><td/><td/><td colspan=\"2\">BERT-Large 97.9</td><td>8.2</td><td>4.1</td></tr><tr><td/><td>MNLI</td><td>ESIM</td><td>76.2</td><td>9.6</td><td>21.7</td></tr><tr><td colspan=\"4\">matched BERT-Large 86.4</td><td>13.2</td><td>7.4</td></tr><tr><td/><td>100</td><td/><td/><td/></tr><tr><td/><td>90</td><td/><td/><td/></tr><tr><td>Attack success rate</td><td>60 70 80</td><td/><td/><td/><td>IMDB Yelp SNLI FAKE MNLI AG</td></tr><tr><td/><td>6</td><td>12</td><td>K value</td><td>24</td><td>36</td></tr><tr><td colspan=\"6\">Figure 2: Using different candidate number K in the</td></tr><tr><td colspan=\"3\">attacking process.</td><td/><td/></tr></table>",
                "type_str": "table",
                "text": "BERT-Attack against other models.",
                "html": null,
                "num": null
            },
            "TABREF4": {
                "content": "<table><tr><td>Dataset</td><td>Method</td><td colspan=\"3\">Ori Acc Atk Acc Queries %</td></tr><tr><td>IMDB</td><td>Fixed-K</td><td>90.9</td><td>11.4</td><td>454</td></tr><tr><td/><td colspan=\"2\">With Threshold 90.9</td><td>12.4</td><td>440</td></tr></table>",
                "type_str": "table",
                "text": "Flexible Candidates Using a threshold to cut off unsuitable candidates.",
                "html": null,
                "num": null
            },
            "TABREF5": {
                "content": "<table><tr><td colspan=\"5\">Dataset Method Ori Acc Atk Acc Perturb %</td></tr><tr><td>MNLI</td><td colspan=\"2\">BERT-Atk 85.1</td><td>7.9</td><td>8.8</td></tr><tr><td colspan=\"3\">matched +Adv Train 84.6</td><td>23.1</td><td>10.5</td></tr><tr><td colspan=\"5\">Dataset Model LSTM BERT-base BERT-large</td></tr><tr><td/><td colspan=\"2\">Word-LSTM -</td><td>0.78</td><td>0.75</td></tr><tr><td>IMDB</td><td colspan=\"2\">BERT-base 0.83</td><td>-</td><td>0.71</td></tr><tr><td/><td colspan=\"2\">BERT-large 0.87</td><td>0.86</td><td>-</td></tr><tr><td colspan=\"2\">Dataset Model</td><td colspan=\"3\">ESIM BERT-base BERT-large</td></tr><tr><td/><td>ESIM</td><td>-</td><td>0.59</td><td>0.60</td></tr><tr><td>MNLI</td><td colspan=\"2\">BERT-base 0.60</td><td>-</td><td>0.45</td></tr><tr><td/><td colspan=\"2\">BERT-large 0.59</td><td>0.43</td><td>-</td></tr></table>",
                "type_str": "table",
                "text": "Adversarial training results.",
                "html": null,
                "num": null
            },
            "TABREF6": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Transferability analysis using attacked accuracy as the evaluation metric. The column is the target model used in attack, and the row is the tested model.",
                "html": null,
                "num": null
            },
            "TABREF7": {
                "content": "<table><tr><td>Dataset</td><td>Model</td><td colspan=\"3\">Atk Acc Perturb % Semantic</td></tr><tr><td>Yelp</td><td>BERT-Atk</td><td>5.1</td><td>4.1</td><td>0.77</td></tr><tr><td/><td>w/o sub-word</td><td>7.1</td><td>4.3</td><td>0.74</td></tr><tr><td>MNLI</td><td>BERT-Atk</td><td>11.9</td><td>7.9</td><td>0.68</td></tr><tr><td/><td colspan=\"2\">w/o sub-word 14.7</td><td>9.3</td><td>0.63</td></tr></table>",
                "type_str": "table",
                "text": "Effects on sub-word level attack.",
                "html": null,
                "num": null
            },
            "TABREF9": {
                "content": "<table><tr><td colspan=\"3\">: Most Importance Ranking (MIR) vs Least Im-</td></tr><tr><td colspan=\"2\">portance Ranking (LIR)</td><td/></tr><tr><td colspan=\"3\">5.5 Effects on Word Importance Ranking</td></tr><tr><td colspan=\"3\">Word importance ranking strategy is supposed to</td></tr><tr><td colspan=\"3\">find keys that are essential to NN models, which</td></tr><tr><td colspan=\"3\">is very much like calculating the maximum risk of</td></tr><tr><td colspan=\"3\">wrong predictions in the FGSM algorithm (Good-</td></tr><tr><td colspan=\"3\">fellow et al., 2014). When not using word im-</td></tr><tr><td colspan=\"3\">portance ranking, the attacking algorithm is less</td></tr><tr><td colspan=\"2\">successful.</td><td/></tr><tr><td>Dataset</td><td>Method</td><td>Runtime(s/sample)</td></tr><tr><td/><td>BERT-Attack(w/o BPE)</td><td>14.2</td></tr><tr><td>IMDB</td><td>BERT-Attack(w/ BPE)</td><td>16.0</td></tr><tr><td/><td>Textfooler(Jin et al., 2019)</td><td>42.4</td></tr><tr><td/><td>GA(Alzantot et al., 2018)</td><td>2582.0</td></tr></table>",
                "type_str": "table",
                "text": "",
                "html": null,
                "num": null
            },
            "TABREF10": {
                "content": "<table><tr><td>Dataset</td></tr></table>",
                "type_str": "table",
                "text": "Runtime comparison.Ori Some rooms have balconies . Hypothesis All of the rooms have balconies off of them . Contradiction Adv Many rooms have balconies . Hypothesis All of the rooms have balconies off of them . Neutral",
                "html": null,
                "num": null
            },
            "TABREF11": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Some generated adversarial samples. Origin label is the correct prediction while label is adverse prediction. Only red color parts are perturbed. We only attack premises in MNLI task. Text in FAKE dataset and IMDB dataset is cut to fit in the table. Original text contains more than 200 words.",
                "html": null,
                "num": null
            }
        }
    }
}