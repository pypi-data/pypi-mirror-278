{
    "paper_id": "P16-1218",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:02:22.429750Z"
    },
    "title": "Graph-based Dependency Parsing with Bidirectional LSTM",
    "authors": [
        {
            "first": "Wenhui",
            "middle": [],
            "last": "Wang",
            "suffix": "",
            "affiliation": {},
            "email": "wangwenhui@pku.edu.cn"
        },
        {
            "first": "Baobao",
            "middle": [],
            "last": "Chang",
            "suffix": "",
            "affiliation": {},
            "email": ""
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "In this paper, we propose a neural network model for graph-based dependency parsing which utilizes Bidirectional LSTM (BLSTM) to capture richer contextual information instead of using high-order factorization, and enable our model to use much fewer features than previous work. In addition, we propose an effective way to learn sentence segment embedding on sentence-level based on an extra forward LSTM network. Although our model uses only first-order factorization, experiments on English Peen Treebank and Chinese Penn Treebank show that our model could be competitive with previous higher-order graph-based dependency parsing models and state-of-the-art models.",
    "pdf_parse": {
        "paper_id": "P16-1218",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "In this paper, we propose a neural network model for graph-based dependency parsing which utilizes Bidirectional LSTM (BLSTM) to capture richer contextual information instead of using high-order factorization, and enable our model to use much fewer features than previous work. In addition, we propose an effective way to learn sentence segment embedding on sentence-level based on an extra forward LSTM network. Although our model uses only first-order factorization, experiments on English Peen Treebank and Chinese Penn Treebank show that our model could be competitive with previous higher-order graph-based dependency parsing models and state-of-the-art models.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Dependency parsing is a fundamental task for language processing which has been investigated for decades. It has been applied in a wide range of applications such as information extraction and machine translation. Among a variety of dependency parsing models, graph-based models are attractive for their ability of scoring the parsing decisions on a whole-tree basis. Typical graph-based models factor the dependency tree into subgraphs, including single arcs (McDonald et al., 2005) , sibling or grandparent arcs (McDonald and Pereira, 2006; Carreras, 2007) or higher-order substructures (Koo and Collins, 2010; Ma and Zhao, 2012) and then score the whole tree by summing scores of the subgraphs. In these models, subgraphs are usually represented as high-dimensional feature vectors which are then fed into a linear model to learn the feature weights.",
                "cite_spans": [
                    {
                        "start": 460,
                        "end": 483,
                        "text": "(McDonald et al., 2005)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 514,
                        "end": 542,
                        "text": "(McDonald and Pereira, 2006;",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 543,
                        "end": 558,
                        "text": "Carreras, 2007)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 589,
                        "end": 612,
                        "text": "(Koo and Collins, 2010;",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 613,
                        "end": 631,
                        "text": "Ma and Zhao, 2012)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "However, conventional graph-based models heavily rely on feature engineering and their performance is restricted by the design of features.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In addition, standard decoding algorithm (Eisner, 2000) only works for the first-order model which limits the scope of feature selection. To incorporate high-order features, Eisner algorithm must be somehow extended or modified, which is usually done at high cost in terms of efficiency. The fourth-order graph-based model (Ma and Zhao, 2012) , which seems the highest-order model so far to our knowledge, requires O(n 5 ) time and O(n 4 ) space. Due to the high computational cost, highorder models are normally restricted to producing only unlabeled parses to avoid extra cost introduced by inclusion of arc-labels into the parse trees.",
                "cite_spans": [
                    {
                        "start": 41,
                        "end": 55,
                        "text": "(Eisner, 2000)",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 323,
                        "end": 342,
                        "text": "(Ma and Zhao, 2012)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "To alleviate the burden of feature engineering, Pei et al. (2015) presented an effective neural network model for graph-based dependency parsing. They only use atomic features such as word unigrams and POS tag unigrams and leave the model to automatically learn the feature combinations. However, their model requires many atomic features and still relies on high-order factorization strategy to further improve the accuracy.",
                "cite_spans": [
                    {
                        "start": 48,
                        "end": 65,
                        "text": "Pei et al. (2015)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Different from previous work, we propose an LSTM-based dependency parsing model in this paper and aim to use LSTM network to capture richer contextual information to support parsing decisions, instead of adopting a high-order factorization. The main advantages of our model are as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\u2022 By introducing Bidirectional LSTM, our model shows strong ability to capture potential long range contextual information and exhibits improved accuracy in recovering long distance dependencies. It is different to previous work in which a similar effect is usually achieved by high-order factorization. More-over, our model also eliminates the need for setting feature selection windows and reduces the number of features to a minimum level.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\u2022 We propose an LSTM-based sentence segment embedding method named LSTM-Minus, in which distributed representation of sentence segment is learned by using subtraction between LSTM hidden vectors. Experiment shows this further enhances our model's ability to access to sentence-level information.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\u2022 Last but important, our model is a first-order model using standard Eisner algorithm for decoding, the computational cost remains at the lowest level among graph-based models.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Our model does not trade-off efficiency for accuracy.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "We evaluate our model on the English Penn Treebank and Chinese Penn Treebank, experiments show that our model achieves competitive parsing accuracy compared with conventional high-order models, however, with a much lower computational cost.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In dependency parsing, syntactic relationships are represented as directed arcs between head words and their modifier words. Each word in a sentence modifies exactly one head, but can have any number of modifiers itself. The whole sentence is rooted at a designated special symbol ROOT, thus the dependency graph for a sentence is constrained to be a rooted, directed tree.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Graph-based dependency parsing",
                "sec_num": "2"
            },
            {
                "text": "For a sentence x, graph-based dependency parsing model searches for the highest-scoring tree of x:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Graph-based dependency parsing",
                "sec_num": "2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "y * (x) = arg max \u0177\u2208Y (x) Score(x, \u0177; \u03b8)",
                        "eq_num": "(1)"
                    }
                ],
                "section": "Graph-based dependency parsing",
                "sec_num": "2"
            },
            {
                "text": "Here y * (x) is the tree with the highest score, Y (x) is the set of all valid dependency trees for x and Score(x, \u0177; \u03b8) measures how likely the tree \u0177 is the correct analysis of the sentence x, \u03b8 are the model parameters. However, the size of Y (x) grows exponentially with respect to the length of the sentence, directly solving equation ( 1) is impractical.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Graph-based dependency parsing",
                "sec_num": "2"
            },
            {
                "text": "The common strategy adopted in the graphbased model is to factor the dependency tree \u0177 into ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Graph-based dependency parsing",
                "sec_num": "2"
            },
            {
                "text": "Score(x, \u0177; \u03b8) = c\u2208\u0177 ScoreC(x, c; \u03b8) (2)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Graph-based dependency parsing",
                "sec_num": "2"
            },
            {
                "text": "Figure 1 shows several factorization strategies. The order of the factorization is defined according to the number of dependencies that subgraph contains. The simplest first-order factorization (McDonald et al., 2005) decomposes a dependency tree into single dependency arcs. Based on the first-order factorization, second-order factorization (McDonald and Pereira, 2006; Carreras, 2007) brings sibling and grandparent information into their model. Third-order factorization (Koo and Collins, 2010) further incorporates richer contextual information by utilizing grand-sibling and tri-sibling parts.",
                "cite_spans": [
                    {
                        "start": 194,
                        "end": 217,
                        "text": "(McDonald et al., 2005)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 343,
                        "end": 371,
                        "text": "(McDonald and Pereira, 2006;",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 372,
                        "end": 387,
                        "text": "Carreras, 2007)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 475,
                        "end": 498,
                        "text": "(Koo and Collins, 2010)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Graph-based dependency parsing",
                "sec_num": "2"
            },
            {
                "text": "Conventional graph-based models (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Ma and Zhao, 2012) score subgraph by a linear model, which heavily depends on feature engineering. The neural network model proposed by Pei et al. (2015) alleviates the dependence on feature engineering to a large extent, but not completely. We follow Pei et al. (2015) to score dependency arcs using neural network model. However, different from their work, we introduce a Bidirectional LSTM to capture long range contextual information and an extra forward LSTM to better represent segments of the sentence separated by the head and modifier. These make our model more accurate in recovering long-distance dependencies and further decrease the number of atomic features. ",
                "cite_spans": [
                    {
                        "start": 32,
                        "end": 55,
                        "text": "(McDonald et al., 2005;",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 56,
                        "end": 83,
                        "text": "McDonald and Pereira, 2006;",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 84,
                        "end": 99,
                        "text": "Carreras, 2007;",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 100,
                        "end": 122,
                        "text": "Koo and Collins, 2010;",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 123,
                        "end": 141,
                        "text": "Ma and Zhao, 2012)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 259,
                        "end": 276,
                        "text": "Pei et al. (2015)",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 375,
                        "end": 392,
                        "text": "Pei et al. (2015)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Graph-based dependency parsing",
                "sec_num": "2"
            },
            {
                "text": "In this section, we describe the architecture of our neural network model in detail, which is summarized in Figure 2 .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 115,
                        "end": 116,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Neural Network Model",
                "sec_num": "3"
            },
            {
                "text": "In our neural network model, the words, POS tags are mapped into distributed embeddings. We represent each input token x i which is the input of Bidirectional LSTM by concatenating POS tag embedding e p i \u2208 R de and word embedding e w i \u2208 R de , d e is the the dimensionality of embedding, then a linear transformation w e is performed and passed though an element-wise activation function g:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Input layer",
                "sec_num": "3.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "x i = g(w e [e w i ; e p i ] + b e )",
                        "eq_num": "(3)"
                    }
                ],
                "section": "Input layer",
                "sec_num": "3.1"
            },
            {
                "text": "where x i \u2208 R de , w e \u2208 R de\u00d72de is weight matrix, b e \u2208 R de is bias term. the dimensionality of input token x i is equal to the dimensionality of word and POS tag embeddings in our experiment, ReLU is used as our activation function g.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Input layer",
                "sec_num": "3.1"
            },
            {
                "text": "Given an input sequence x = (x 1 , . . . , x n ), where n stands for the number of words in a sentence, a standard LSTM recurrent network computes the hidden vector sequence h = (h 1 , . . . , h n ) in one direction.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Bidirectional LSTM",
                "sec_num": "3.2"
            },
            {
                "text": "Bidirectional LSTM processes the data in both directions with two separate hidden layers, which are then fed to the same output layer. It computes the forward hidden sequence -\u2192 h , the backward hidden sequence \u2190h and the output sequence v by iterating the forward layer from t = 1 to n, the backward layer from t = n to 1 and then updating the output layer:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Bidirectional LSTM",
                "sec_num": "3.2"
            },
            {
                "text": "v t = - \u2192 h t + \u2190 - h t (4)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Bidirectional LSTM",
                "sec_num": "3.2"
            },
            {
                "text": "where",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Bidirectional LSTM",
                "sec_num": "3.2"
            },
            {
                "text": "v t \u2208 R d l is the output vector of Bidirec- tional LSTM for input x t , - \u2192 h t \u2208 R d l , \u2190 - h t \u2208 R d l , d l",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Bidirectional LSTM",
                "sec_num": "3.2"
            },
            {
                "text": "is the dimensionality of LSTM hidden vector. We simply add the forward hidden vector -\u2192 h t and the backward hidden vector \u2190h t together, which gets similar experiment result as concatenating them together with a faster speed.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Bidirectional LSTM",
                "sec_num": "3.2"
            },
            {
                "text": "The output vectors of Bidirectional LSTM are used as word feature embeddings. In addition, they are also fed into a forward LSTM network to learn segment embedding.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Bidirectional LSTM",
                "sec_num": "3.2"
            },
            {
                "text": "Contextual information of word pairs1 has been widely utilized in previous work (McDonald et al., 2005; McDonald and Pereira, 2006; Pei et al., 2015) . For a dependency pair (h, m), previous work divides a sentence into three parts (prefix, infix and suffix) by head word h and modifier word m. These parts which we call segments in our work make up the context of the dependency pair (h, m).",
                "cite_spans": [
                    {
                        "start": 80,
                        "end": 103,
                        "text": "(McDonald et al., 2005;",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 104,
                        "end": 131,
                        "text": "McDonald and Pereira, 2006;",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 132,
                        "end": 149,
                        "text": "Pei et al., 2015)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Segment Embedding",
                "sec_num": "3.3"
            },
            {
                "text": "Due to the problem of data sparseness, conventional graph-based models can only capture contextual information of word pairs by using bigrams or tri-grams features. Unlike conventional models, Pei et al. (2015) use distributed representations obtained by averaging word embeddings in segments to represent contextual information of the word pair, which could capture richer syntactic and semantic information. However, their method is restricted to segment-level since their segment embedding only consider the word information within the segment. Besides, averaging operation simply treats all the words in segment equally. However, some words might carry more salient syntactic or semantic information and they are expected to be given more attention.",
                "cite_spans": [
                    {
                        "start": 193,
                        "end": 210,
                        "text": "Pei et al. (2015)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Segment Embedding",
                "sec_num": "3.3"
            },
            {
                "text": "A useful property of forward LSTM is that it could keep previous useful information in their memory cell by exploiting input, output and forget gates to decide how to utilize and update the memory of previous information. Given an input sequence v = (v 1 , . . . , v n ), previous work (Sutskever et al., 2014; Vinyals et al., 2014) often uses the last hidden vector h n of the forward LSTM to represent the whole sequence. Each hidden vector h t (1 \u2264 t \u2264 n) can capture useful information before and including v t .",
                "cite_spans": [
                    {
                        "start": 286,
                        "end": 310,
                        "text": "(Sutskever et al., 2014;",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 311,
                        "end": 332,
                        "text": "Vinyals et al., 2014)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Segment Embedding",
                "sec_num": "3.3"
            },
            {
                "text": "Inspired by this, we propose a method named LSTM-Minus to learn segment embedding. We utilize subtraction between LSTM hidden vectors to represent segment's information. As illustrated in Figure 3 , the segment infix can be described as h m -h 2 , h m and h 2 are hidden vector of the forward LSTM network. The segment embedding of suffix can also be obtained by subtraction between the last LSTM hidden vector of the sequence (h 7 ) and the last LSTM hidden vector in infix (h m ). For prefix, we directly use the last LSTM hidden vector in prefix to represent it, which equals to subtract a zero embedding. When no prefix or suffix exists, the corresponding embedding is set to zero.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 195,
                        "end": 196,
                        "text": "3",
                        "ref_id": "FIGREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Segment Embedding",
                "sec_num": "3.3"
            },
            {
                "text": "Specifically, we place an extra forward LSTM layer on top of the Bidirectional LSTM layer and learn segment embeddings using LSTM-Minus based on this forward LSTM. LSTM-minus enables our model to learn segment embeddings from information both outside and inside the segments and thus enhances our model's ability to access to sentence-level information.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Segment Embedding",
                "sec_num": "3.3"
            },
            {
                "text": "As illustrated in Figure 2 , we map all the feature embeddings to a hidden layer. Following Pei et al. (2015) , we use direction-specific transformation to model edge direction and tanh-cube as our activation function:",
                "cite_spans": [
                    {
                        "start": 92,
                        "end": 109,
                        "text": "Pei et al. (2015)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 25,
                        "end": 26,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Hidden layer and output layer",
                "sec_num": "3.4"
            },
            {
                "text": "h = g i W d h i a i + b d h (5)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Hidden layer and output layer",
                "sec_num": "3.4"
            },
            {
                "text": "where a i \u2208 R da i is the feature embedding, d a i indicates the dimensionality of feature embedding",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Hidden layer and output layer",
                "sec_num": "3.4"
            },
            {
                "text": "a i , W d h i \u2208 R d h \u00d7da i is weight matrices which cor- responding to a i , d h indicates the dimensionality of hidden layer vector, b d h \u2208 R d h is bias term. W d h i",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Hidden layer and output layer",
                "sec_num": "3.4"
            },
            {
                "text": "and b d h are bound with index d \u2208 {0, 1} which indicates the direction between head and modifier.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Hidden layer and output layer",
                "sec_num": "3.4"
            },
            {
                "text": "A output layer is finally added on the top of the hidden layer for scoring dependency arcs:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Hidden layer and output layer",
                "sec_num": "3.4"
            },
            {
                "text": "ScoreC(x, c) = W d o h + b d o (6) Where W d o \u2208 R L\u00d7d h is weight matrices, b d o \u2208 R L is bias term, ScoreC(x, c) \u2208 R L",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Hidden layer and output layer",
                "sec_num": "3.4"
            },
            {
                "text": "is the output vector, L is the number of dependency types. Each dimension of the output vector is the score for each kind of dependency type of head-modifier pair.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Hidden layer and output layer",
                "sec_num": "3.4"
            },
            {
                "text": "Previous neural network models (Pei et al., 2015; Pei et al., 2014; Zheng et al., 2013) normally set context window around a word and extract atomic features within the window to represent the contextual information. However, context window limits their ability in detecting long-distance information. Simply increasing the context window size to get more contextual information puts their model in the risk of overfitting and heavily slows down the speed.",
                "cite_spans": [
                    {
                        "start": 31,
                        "end": 49,
                        "text": "(Pei et al., 2015;",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 50,
                        "end": 67,
                        "text": "Pei et al., 2014;",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 68,
                        "end": 87,
                        "text": "Zheng et al., 2013)",
                        "ref_id": "BIBREF27"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Features in our model",
                "sec_num": "3.5"
            },
            {
                "text": "Unlike previous work, we apply Bidirectional LSTM to capture long range contextual information and eliminate the need for context windows, avoiding the limit of the window-based feature selection approach. Compared with Pei et al. (2015) , the cancellation of the context window allows our model to use much fewer features. Moreover, by combining a word's atomic features (word form and POS tag) together, our model further decreases the number of features. Pei et al. (2015) h-2.w, h-1.w, h.w, h1.w, h2.w h-2.p, h-1.p, h.p, h1.p, h2.p m-2.w, m-1.w, m.w, m1.w, m2.w m-2.p, m-1.p, m.p, m1.p, m2.p dis(h, m) Pei et al. (2015) and atomic features used in our basic model. Our basic model only uses the outputs of Bidirectional LSTM for head word and modifier word, and the distance between them as features. Distance features are encoded as randomly initialized embeddings. As we can see, our basic model reduces the number of atomic features to a minimum level, making our model run with a faster speed. Based on our basic model, we incorporate additional segment information (prefix, infix and suffix), which further improves the effect of our model.",
                "cite_spans": [
                    {
                        "start": 220,
                        "end": 237,
                        "text": "Pei et al. (2015)",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 458,
                        "end": 475,
                        "text": "Pei et al. (2015)",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 606,
                        "end": 623,
                        "text": "Pei et al. (2015)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Features in our model",
                "sec_num": "3.5"
            },
            {
                "text": "Our basic model v h , vm dis(h, m)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Features in our model",
                "sec_num": "3.5"
            },
            {
                "text": "In this section, we provide details about training the neural network.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Neural Training",
                "sec_num": "4"
            },
            {
                "text": "We use the Max-Margin criterion to train our model. Given a training instance (x (i) , y (i) ), we use Y (x (i) ) to denote the set of all possible dependency trees and y (i) is the correct dependency tree for sentence x (i) . The goal of Max Margin training is to find parameters \u03b8 such that the difference in score of the correct tree y (i) from an incorrect tree \u0177 \u2208 Y (x (i) ) is at least (y (i) , \u0177). Score(x (i) ,y (i) ; \u03b8) \u2265 Score(x (i) ,\u0177; \u03b8)+ (y (i) ,\u0177)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Max-Margin Training",
                "sec_num": "4.1"
            },
            {
                "text": "The structured margin loss (y (i) , \u0177) is defined as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Max-Margin Training",
                "sec_num": "4.1"
            },
            {
                "text": "(y (i) , \u0177) = n j \u03ba1{h(y (i) , x (i) j ) = h(\u0177, x (i) j )}",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Max-Margin Training",
                "sec_num": "4.1"
            },
            {
                "text": "where n is the length of sentence x, h(y (i) , x (i) j ) is the head (with type) for the j-th word of x (i) in tree y (i) and \u03ba is a discount parameter. The loss is proportional to the number of word with an incorrect head and edge type in the proposed tree.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Max-Margin Training",
                "sec_num": "4.1"
            },
            {
                "text": "Given a training set with size m, The regularized objective function is the loss function J(\u03b8) including a l 2 -norm term:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Max-Margin Training",
                "sec_num": "4.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "J(\u03b8) = 1 m m i=1 l i (\u03b8) + \u03bb 2 ||\u03b8|| 2 l i (\u03b8) = max \u0177\u2208Y (x (i) ) (Score(x (i) ,\u0177; \u03b8)+ (y (i) ,\u0177)) -Score(x (i) ,y (i) ; \u03b8)",
                        "eq_num": "(7)"
                    }
                ],
                "section": "Max-Margin Training",
                "sec_num": "4.1"
            },
            {
                "text": "By minimizing this objective, the score of the correct tree is increased and score of the highest scoring incorrect tree is decreased.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Max-Margin Training",
                "sec_num": "4.1"
            },
            {
                "text": "Parameter optimization is performed with the diagonal variant of AdaGrad (Duchi et al., 2011) with minibatchs (batch size = 20) . The parameter update for the i-th parameter \u03b8 t,i at time step t is as follows:",
                "cite_spans": [
                    {
                        "start": 73,
                        "end": 93,
                        "text": "(Duchi et al., 2011)",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Optimization Algorithm",
                "sec_num": "4.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "\u03b8 t,i = \u03b8 t-1,i - \u03b1 t \u03c4 =1 g 2 \u03c4,i g t,i",
                        "eq_num": "(8)"
                    }
                ],
                "section": "Optimization Algorithm",
                "sec_num": "4.2"
            },
            {
                "text": "where \u03b1 is the initial learning rate (\u03b1 = 0.2 in our experiment) and g \u03c4 \u2208 R |\u03b8 i | is the subgradient at time step \u03c4 for parameter \u03b8 i .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Optimization Algorithm",
                "sec_num": "4.2"
            },
            {
                "text": "To mitigate overfitting, dropout (Hinton et al., 2012) is used to regularize our model. we apply dropout on the hidden layer with 0.2 rate.",
                "cite_spans": [
                    {
                        "start": 33,
                        "end": 54,
                        "text": "(Hinton et al., 2012)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Optimization Algorithm",
                "sec_num": "4.2"
            },
            {
                "text": "The following hyper-parameters are used in all experiments: word embedding size = 100, POS tag embedding size = 100, hidden layer size = 200, LSTM hidden vector size = 100, Bidirectional LSTM layers = 2, regularization parameter \u03bb = 10 -4 .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model Initialization&Hyperparameters",
                "sec_num": "4.3"
            },
            {
                "text": "We initialized the parameters using pretrained word embeddings. Following Dyer et al. (2015) , we use a variant of the skip n-gram model introduced by Ling et al. (2015) on Gigaword corpus (Graff et al., 2003) . We also experimented with randomly initialized embeddings, where embeddings are uniformly sampled from range [-0.3, 0.3] . All other parameters are uniformly sampled from range [-0.05, 0.05].",
                "cite_spans": [
                    {
                        "start": 74,
                        "end": 92,
                        "text": "Dyer et al. (2015)",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 151,
                        "end": 169,
                        "text": "Ling et al. (2015)",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 189,
                        "end": 209,
                        "text": "(Graff et al., 2003)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 321,
                        "end": 332,
                        "text": "[-0.3, 0.3]",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model Initialization&Hyperparameters",
                "sec_num": "4.3"
            },
            {
                "text": "UAS LAS Speed(sent/s) First-order MSTParser 91.60 90.39 20 1st-order atomic (Pei et al., 2015) 92.14 90.92 55 1st-order phrase (Pei et al., 2015) ",
                "cite_spans": [
                    {
                        "start": 76,
                        "end": 94,
                        "text": "(Pei et al., 2015)",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 127,
                        "end": 145,
                        "text": "(Pei et al., 2015)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Models",
                "sec_num": null
            },
            {
                "text": "In this section, we present our experimental setup and the main result of our work.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "5"
            },
            {
                "text": "We conduct our experiments on the English Penn Treebank (PTB) and the Chinese Penn Treebank (CTB) datasets.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments Setup",
                "sec_num": "5.1"
            },
            {
                "text": "For English, we follow the standard splits of PTB3. Using section 2-21 for training, section 22 as development set and 23 as test set. We conduct experiments on two different constituency-todependency-converted Penn Treebank data sets. The first one, Penn-YM, was created by the Penn2Malt tool 2 based on Yamada and Matsumoto (2003) head rules. The second one, Penn-SD, use Stanford Basic Dependencies (Marneffe et al., 2006) and was converted by version 3.3.0 3 of the Stanford parser. The Stanford POS Tagger (Toutanova et al., 2003) with ten-way jackknifing of the training data is used for assigning POS tags (accuracy \u2248 97.2%).",
                "cite_spans": [
                    {
                        "start": 305,
                        "end": 332,
                        "text": "Yamada and Matsumoto (2003)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 402,
                        "end": 425,
                        "text": "(Marneffe et al., 2006)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 511,
                        "end": 535,
                        "text": "(Toutanova et al., 2003)",
                        "ref_id": "BIBREF18"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments Setup",
                "sec_num": "5.1"
            },
            {
                "text": "For Chinese, we adopt the same split of CTB5 as described in (Zhang and Clark, 2008) . Following (Zhang and Clark, 2008; Dyer et al., 2015; Chen and Manning, 2014) , we use gold segmentation and POS tags for the input.",
                "cite_spans": [
                    {
                        "start": 61,
                        "end": 84,
                        "text": "(Zhang and Clark, 2008)",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 97,
                        "end": 120,
                        "text": "(Zhang and Clark, 2008;",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 121,
                        "end": 139,
                        "text": "Dyer et al., 2015;",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 140,
                        "end": 163,
                        "text": "Chen and Manning, 2014)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments Setup",
                "sec_num": "5.1"
            },
            {
                "text": "We first make comparisons with previous graphbased models of different orders as shown in Ta-2 http://stp.lingfil.uu.se/nivre/ research/Penn2Malt.html 3 http://nlp.stanford.edu/software/ lex-parser.shtml ble 2. We use MSTParser4 for conventional firstorder model (McDonald et al., 2005) and secondorder model (McDonald and Pereira, 2006) . We also include the results of conventional high-order models (Koo and Collins, 2010; Ma and Zhao, 2012; Zhang and McDonald, 2012; Zhang et al., 2013; Zhang and McDonald, 2014) and the neural network model of Pei et al. (2015) . Different from typical high-order models (Koo and Collins, 2010; Ma and Zhao, 2012) , which need to extend their decoding algorithm to score new types of higher-order dependencies. Zhang and McDonald (2012) generalized the Eisner algorithm to handle arbitrary features over higher-order dependencies and controlled complexity via approximate decoding with cube pruning. They further improve their work by using perceptron update strategies for inexact hypergraph search (Zhang et al., 2013) and forcing inference to maintain both label and structural ambiguity through a secondary beam (Zhang and McDonald, 2014) .",
                "cite_spans": [
                    {
                        "start": 263,
                        "end": 286,
                        "text": "(McDonald et al., 2005)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 309,
                        "end": 337,
                        "text": "(McDonald and Pereira, 2006)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 402,
                        "end": 425,
                        "text": "(Koo and Collins, 2010;",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 426,
                        "end": 444,
                        "text": "Ma and Zhao, 2012;",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 445,
                        "end": 470,
                        "text": "Zhang and McDonald, 2012;",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 471,
                        "end": 490,
                        "text": "Zhang et al., 2013;",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 491,
                        "end": 516,
                        "text": "Zhang and McDonald, 2014)",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 549,
                        "end": 566,
                        "text": "Pei et al. (2015)",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 610,
                        "end": 633,
                        "text": "(Koo and Collins, 2010;",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 634,
                        "end": 652,
                        "text": "Ma and Zhao, 2012)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 750,
                        "end": 775,
                        "text": "Zhang and McDonald (2012)",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 1039,
                        "end": 1059,
                        "text": "(Zhang et al., 2013)",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 1155,
                        "end": 1181,
                        "text": "(Zhang and McDonald, 2014)",
                        "ref_id": "BIBREF24"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments Results",
                "sec_num": "5.2"
            },
            {
                "text": "Following previous work, UAS (unlabeled attachment scores) and LAS (labeled attachment scores) are calculated by excluding punctuation5 . The parsing speeds are measured on a workstation with Intel Xeon 3.4GHz CPU and 32GB RAM which is same to Pei et al. (2015) . We measure the parsing speeds of Pei et al. (2015) according to their codes6 and parameters.",
                "cite_spans": [
                    {
                        "start": 244,
                        "end": 261,
                        "text": "Pei et al. (2015)",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 297,
                        "end": 314,
                        "text": "Pei et al. (2015)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments Results",
                "sec_num": "5.2"
            },
            {
                "text": "On accuracy, as shown in basic model outperforms previous first-order graph-based models by a substantial margin, even outperforms Zhang and McDonald (2012) 's unlimited-order model. Moreover, incorporating segment information further improves our model's accuracy, which shows that segment embeddings do capture richer contextual information. By using segment embeddings, our improved model could be comparable to high-order graph-based models 7 .",
                "cite_spans": [
                    {
                        "start": 131,
                        "end": 156,
                        "text": "Zhang and McDonald (2012)",
                        "ref_id": "BIBREF23"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments Results",
                "sec_num": "5.2"
            },
            {
                "text": "With regard to parsing speed, our model also shows advantage of efficiency. Our model uses only first-order factorization and requires O(n 3 ) time to decode. Third-order model requires O(n 4 ) time and fourth-order model requires O(n 5 ) time. By using approximate decoding, the unlimitedorder model of Zhang and McDonald (2012) ",
                "cite_spans": [
                    {
                        "start": 304,
                        "end": 329,
                        "text": "Zhang and McDonald (2012)",
                        "ref_id": "BIBREF23"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments Results",
                "sec_num": "5.2"
            },
            {
                "text": "re- quires O(k \u2022log(k)\u2022n 3 ) time,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments Results",
                "sec_num": "5.2"
            },
            {
                "text": "where k is the beam size. The computational cost of our model is the lowest among graph-based models. Moreover, although using LSTM requires much computational cost. However, compared with Pei's 1st-order model, our model decreases the number of atomic features from 21 to 3, this allows our model to require a much smaller matrix computation in the scoring model, which cancels out the extra computation cost introduced by the LSTM computation. Our basic model is the fastest among first-order and second-order models. Incorporating segment information slows down the parsing speed while it is still slightly faster than conventional first-order model. To compare with conventional high-order models on practical parsing speed, we can make an indirect comparison according to Zhang and McDonald (2012) We further compare our model with previous state-of-the-art systems for English and Chinese. Table 3 lists the performances of our model as well as previous state-of-the-art systems on on Penn-YM, Penn-SD and CTB5. We compare to conventional state-of-the-art graph-based model (Zhang and McDonald, 2014) , conventional state-of-theart transition-based model using beam search (Zhang and Nivre, 2011), transition-based model combining graph-based approach (Bernd Bohnet, 2012) , transition-based neural network model using stack LSTM (Dyer et al., 2015) and transitionbased neural network model using beam search (Weiss et al., 2015) . Overall, our model achieves competitive accuracy on all three datasets. Although our model is slightly lower in accuarcy than unlimited-order double beam model (Zhang and McDonald, 2014) on Penn-YM and CTB5, our model outperforms their model on Penn-SD. It seems that our model performs better on data sets with larger label sets, given the number of labels used in Penn-SD data set is almost four times more than Penn-YM and CTB5 data sets.",
                "cite_spans": [
                    {
                        "start": 777,
                        "end": 802,
                        "text": "Zhang and McDonald (2012)",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 1080,
                        "end": 1106,
                        "text": "(Zhang and McDonald, 2014)",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 1179,
                        "end": 1189,
                        "text": "(Zhang and",
                        "ref_id": null
                    },
                    {
                        "start": 1190,
                        "end": 1278,
                        "text": "Nivre, 2011), transition-based model combining graph-based approach (Bernd Bohnet, 2012)",
                        "ref_id": null
                    },
                    {
                        "start": 1336,
                        "end": 1355,
                        "text": "(Dyer et al., 2015)",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 1415,
                        "end": 1435,
                        "text": "(Weiss et al., 2015)",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 1598,
                        "end": 1624,
                        "text": "(Zhang and McDonald, 2014)",
                        "ref_id": "BIBREF24"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 902,
                        "end": 903,
                        "text": "3",
                        "ref_id": "TABREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Experiments Results",
                "sec_num": "5.2"
            },
            {
                "text": "To show the effectiveness of our segment embedding method LSTM-Minus, we compare with averaging method proposed by Pei et al. (2015) . We get segment embeddings by averaging the output vectors of Bidirectional LSTM in segments. To make comparison as fair as possible, we let two models have almost the same number parameters. Table 4 lists the UAS of two methods on test set. As we can see, LSTM-Minus shows better performance because our method further incorporates more sentence-level information into our model.",
                "cite_spans": [
                    {
                        "start": 115,
                        "end": 132,
                        "text": "Pei et al. (2015)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 332,
                        "end": 333,
                        "text": "4",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Experiments Results",
                "sec_num": "5.2"
            },
            {
                "text": "In this part, we investigate the impact of the components of our approach.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Impact of Network Structure",
                "sec_num": "5.3"
            },
            {
                "text": "To evaluate the impact of LSTM, we make error analysis on Penn-YM. We compare our model with Pei et al. (2015) on error rates of different distance between head and modifier.",
                "cite_spans": [
                    {
                        "start": 93,
                        "end": 110,
                        "text": "Pei et al. (2015)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "LSTM Recurrent Network",
                "sec_num": null
            },
            {
                "text": "As we can see, the five models do not show much difference for short dependencies whose distance less than three. For long dependencies, both our two models show better performance compared with the 1st-order model of Pei et al. (2015) , which proves that LSTM can effectively capture long-distance dependencies. Moreover, our models and Pei's 2nd-order phrase model both improve accuracy on long dependencies compared with Pei's 1st-order model, which is in line with our expectations. Using LSTM shows the same effect as high-order factorization strategy. Compared with 2nd-order phrase model of Pei et al. (2015) , our basic model occasionally performs worse in recovering long distant dependencies. However, this should not be a surprise since higher order models are also motivated to recover longdistance dependencies. Nevertheless, with the introduction of LSTM-minus segment embeddings, our model consistently outperforms the 2nd-order phrase model of Pei et al. (2015) in accuracies of all long dependencies. We carried out significance test on the difference between our and Pei's models. Our basic model performs significantly better than all 1st-order models of Pei et al. (2015) (ttest with p<0.001) and our basic+segment model (still a 1st-order model) performs significantly better than their 2nd-order phrase model (t-test with p<0.001) in recovering long-distance dependencies.",
                "cite_spans": [
                    {
                        "start": 218,
                        "end": 235,
                        "text": "Pei et al. (2015)",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 598,
                        "end": 615,
                        "text": "Pei et al. (2015)",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 960,
                        "end": 977,
                        "text": "Pei et al. (2015)",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 1174,
                        "end": 1191,
                        "text": "Pei et al. (2015)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "LSTM Recurrent Network",
                "sec_num": null
            },
            {
                "text": "We further analyze the influence of using pretrained word embeddings for initialization. without using pretrained word embeddings, our improved model achieves 92.94% UAS / 91.83% LAS on Penn-YM, 93.46% UAS / 91.19% LAS on Penn-SD and 86.5% UAS / 85.0% LAS on CTB5. Using pre-trained word embeddings can obtain around 0.5%\u223c1.0% improvement.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Initialization of pre-trained word embeddings",
                "sec_num": null
            },
            {
                "text": "Dependency parsing has gained widespread interest in the computational linguistics community. There are a lot of approaches to solve it. Among them, we will mainly focus on graph-based dependency parsing model here. Dependency tree factorization and decoding algorithm are necessary for graph-based models. McDonald et al. (2005) proposed the first-order model which decomposes a dependency tree into its individual edges and use a effective dynamic programming algorithm (Eisner, 2000) to decode. Based on firstorder model, higher-order models (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Ma and Zhao, 2012) factor a dependency tree into a set of high-order dependencies which bring interactions between head, modifier, siblings and (or) grandparent into their model. However, for above models, scoring new types of higherorder dependencies requires extensions of the underlying decoding algorithm, which also requires higher computational cost. Unlike above models, unlimited-order models (Zhang and McDonald, 2012; Zhang et al., 2013; Zhang and McDonald, 2014 ) could handle arbitrary features over higherorder dependencies by generalizing the Eisner algorithm.",
                "cite_spans": [
                    {
                        "start": 307,
                        "end": 329,
                        "text": "McDonald et al. (2005)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 472,
                        "end": 486,
                        "text": "(Eisner, 2000)",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 545,
                        "end": 573,
                        "text": "(McDonald and Pereira, 2006;",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 574,
                        "end": 589,
                        "text": "Carreras, 2007;",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 590,
                        "end": 612,
                        "text": "Koo and Collins, 2010;",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 613,
                        "end": 631,
                        "text": "Ma and Zhao, 2012)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 1014,
                        "end": 1040,
                        "text": "(Zhang and McDonald, 2012;",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 1041,
                        "end": 1060,
                        "text": "Zhang et al., 2013;",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 1061,
                        "end": 1085,
                        "text": "Zhang and McDonald, 2014",
                        "ref_id": "BIBREF24"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related work",
                "sec_num": "6"
            },
            {
                "text": "In contrast to conventional methods, neural network model shows their ability to reduce the effort in feature engineering. Pei et al. (2015) proposed a model to automatically learn high-order feature combinations via a novel activation function, allowing their model to use a set of atomic features instead of millions of hand-crafted features.",
                "cite_spans": [
                    {
                        "start": 123,
                        "end": 140,
                        "text": "Pei et al. (2015)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related work",
                "sec_num": "6"
            },
            {
                "text": "Different from previous work, which is sensitive to local state and accesses to larger context by higher-order factorization. Our model makes parsing decisions on a global perspective with firstorder factorization, avoiding the expensive computational cost introduced by high-order factorization.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related work",
                "sec_num": "6"
            },
            {
                "text": "LSTM network is heavily utilized in our model. LSTM network has already been explored in transition-based dependency parsing. Dyer et al. (2015) presented stack LSTMs with push and pop operations and used them to implement a state-of-the-art transition-based dependency parser. Ballesteros et al. (2015) replaced lookup-based word representations with characterbased representations obtained by Bidirectional LSTM in the continuous-state parser of Dyer et al. (2015) , which was proved experimentally to be useful for morphologically rich languages.",
                "cite_spans": [
                    {
                        "start": 126,
                        "end": 144,
                        "text": "Dyer et al. (2015)",
                        "ref_id": null
                    },
                    {
                        "start": 278,
                        "end": 303,
                        "text": "Ballesteros et al. (2015)",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 448,
                        "end": 466,
                        "text": "Dyer et al. (2015)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related work",
                "sec_num": "6"
            },
            {
                "text": "In this paper, we propose an LSTM-based neural network model for graph-based dependency parsing. Utilizing Bidirectional LSTM and segment embeddings learned by LSTM-Minus allows our model access to sentence-level information, making our model more accurate in recovering longdistance dependencies with only first-order factorization. Experiments on PTB and CTB show that our model could be competitive with conventional high-order models with a faster speed.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "7"
            },
            {
                "text": "A word pair is limited to the dependency pair (h, m) in our work since we use only first-order factorization. In previous work, word pair could be any pair with particular relation (e.g., sibling pair (s, m) in Figure1).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "http://sourceforge.net/projects/ mstparser",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Following previous work, a token is a punctuation if its POS tag is {\" \" : , .}",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://github.com/Williammed/ DeepParser",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Note that our model can't be strictly comparable with third-order model(Koo and Collins, 2010) and fourthorder model(Ma and Zhao, 2012) since they are unlabeled model. However, our model is comparable with all the three unlimited-order models presented in(Zhang and McDonald, 2012),(Zhang et al., 2013) and(Zhang and McDonald, 2014), since they all are labeled models as ours.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "This work is supported by National Key Basic Research Program of China under Grant No.2014CB340504 and National Natural Science Foundation of China under Grant No.61273318. The Corresponding author of this paper is Baobao Chang.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgments",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Improved transition-based parsing by modeling characters instead of words with lstms",
                "authors": [
                    {
                        "first": "Miguel",
                        "middle": [],
                        "last": "Ballesteros",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Dyer",
                        "suffix": ""
                    },
                    {
                        "first": "Noah",
                        "middle": [
                            "A"
                        ],
                        "last": "Smith",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "349--359",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Miguel Ballesteros, Chris Dyer, and Noah A. Smith. 2015. Improved transition-based parsing by model- ing characters instead of words with lstms. In Pro- ceedings of the 2015 Conference on Empirical Meth- ods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015, pages 349-359.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "The best of both worlds: a graph-based completion model for transition-based parsers",
                "authors": [
                    {
                        "first": "Jonas",
                        "middle": [],
                        "last": "Kuhn",
                        "suffix": ""
                    },
                    {
                        "first": "Bernd",
                        "middle": [],
                        "last": "Bohnet",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Conference of the European Chapter",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jonas Kuhn Bernd Bohnet. 2012. The best of both worlds: a graph-based completion model for transition-based parsers. Conference of the Euro- pean Chapter of the Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Experiments with a higherorder projective dependency parser",
                "authors": [
                    {
                        "first": "Xavier",
                        "middle": [],
                        "last": "Carreras",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "EMNLP-CoNLL",
                "volume": "",
                "issue": "",
                "pages": "957--961",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xavier Carreras. 2007. Experiments with a higher- order projective dependency parser. In EMNLP- CoNLL, pages 957-961.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "A fast and accurate dependency parser using neural networks",
                "authors": [
                    {
                        "first": "Danqi",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "740--750",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Danqi Chen and Christopher D. Manning. 2014. A fast and accurate dependency parser using neural net- works. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Process- ing, pages 740-750.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Adaptive subgradient methods for online learning and stochastic optimization",
                "authors": [
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Duchi",
                        "suffix": ""
                    },
                    {
                        "first": "Elad",
                        "middle": [],
                        "last": "Hazan",
                        "suffix": ""
                    },
                    {
                        "first": "Yoram",
                        "middle": [],
                        "last": "Singer",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "The Journal of Machine Learning Research",
                "volume": "",
                "issue": "",
                "pages": "2121--2159",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. The Journal of Ma- chine Learning Research, pages 2121-2159.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Transitionbased dependency parsing with stack long shortterm memory",
                "authors": [
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Dyer",
                        "suffix": ""
                    },
                    {
                        "first": "Miguel",
                        "middle": [],
                        "last": "Ballesteros",
                        "suffix": ""
                    },
                    {
                        "first": "Wang",
                        "middle": [],
                        "last": "Ling",
                        "suffix": ""
                    },
                    {
                        "first": "Austin",
                        "middle": [],
                        "last": "Matthews",
                        "suffix": ""
                    },
                    {
                        "first": "Noah",
                        "middle": [
                            "A"
                        ],
                        "last": "Smith",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "334--343",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chris Dyer, Miguel Ballesteros, Wang Ling, Austin Matthews, and Noah A. Smith. 2015. Transition- based dependency parsing with stack long short- term memory. In Proceedings of the 53rd Annual Meeting of the Association for Computational Lin- guistics, pages 334-343.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Bilexical Grammars and their Cubic-Time Parsing Algorithms",
                "authors": [
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Eisner",
                        "suffix": ""
                    }
                ],
                "year": 2000,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jason Eisner. 2000. Bilexical Grammars and their Cubic-Time Parsing Algorithms. Springer Nether- lands.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "English gigaword. Linguistic Data Consortium",
                "authors": [
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Graff",
                        "suffix": ""
                    },
                    {
                        "first": "Junbo",
                        "middle": [],
                        "last": "Kong",
                        "suffix": ""
                    },
                    {
                        "first": "Ke",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Kazuaki",
                        "middle": [],
                        "last": "Maeda",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. 2003. English gigaword. Linguistic Data Consortium, Philadelphia.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Improving neural networks by preventing coadaptation of feature detectors",
                "authors": [
                    {
                        "first": "Geoffrey",
                        "middle": [
                            "E"
                        ],
                        "last": "Hinton",
                        "suffix": ""
                    },
                    {
                        "first": "Nitish",
                        "middle": [],
                        "last": "Srivastava",
                        "suffix": ""
                    },
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Krizhevsky",
                        "suffix": ""
                    },
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    },
                    {
                        "first": "Ruslan",
                        "middle": [
                            "R"
                        ],
                        "last": "Salakhutdinov",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1207.0580"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdinov. 2012. Improving neural networks by preventing co- adaptation of feature detectors. arXiv preprint arXiv:1207.0580.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Efficient thirdorder dependency parsers",
                "authors": [
                    {
                        "first": "Terry",
                        "middle": [],
                        "last": "Koo",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Collins",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "1--11",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Terry Koo and Michael Collins. 2010. Efficient third- order dependency parsers. In Proceedings of the 48th Annual Meeting of the Association for Com- putational Linguistics, pages 1-11. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Two/too simple adaptations of word2vec for syntax problems",
                "authors": [
                    {
                        "first": "Wang",
                        "middle": [],
                        "last": "Ling",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Dyer",
                        "suffix": ""
                    },
                    {
                        "first": "Alan",
                        "middle": [],
                        "last": "Black",
                        "suffix": ""
                    },
                    {
                        "first": "Isabel",
                        "middle": [],
                        "last": "Trancoso",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the 2015 Conference of the North American Chapter of the",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Wang Ling, Chris Dyer, Alan Black, and Isabel Trancoso. 2015. Two/too simple adaptations of word2vec for syntax problems. In Proceedings of the 2015 Conference of the North American Chap- ter of the Association for Computational Linguistics: Human Language Technologies.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Fourth-order dependency parsing",
                "authors": [
                    {
                        "first": "Xuezhe",
                        "middle": [],
                        "last": "Ma",
                        "suffix": ""
                    },
                    {
                        "first": "Hai",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "COLING 2012, 24th International Conference on Computational Linguistics, Proceedings of the Conference: Posters, 8-15 December",
                "volume": "",
                "issue": "",
                "pages": "785--796",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xuezhe Ma and Hai Zhao. 2012. Fourth-order de- pendency parsing. In COLING 2012, 24th Inter- national Conference on Computational Linguistics, Proceedings of the Conference: Posters, 8-15 De- cember 2012, Mumbai, India, pages 785-796.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Generating typed dependency parses from phrase structure parses",
                "authors": [
                    {
                        "first": "Marie",
                        "middle": [],
                        "last": "Catherine",
                        "suffix": ""
                    },
                    {
                        "first": "De",
                        "middle": [],
                        "last": "Marneffe",
                        "suffix": ""
                    },
                    {
                        "first": "Bill",
                        "middle": [],
                        "last": "Maccartney",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Lrec",
                "volume": "",
                "issue": "",
                "pages": "449--454",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Marie Catherine De Marneffe, Bill Maccartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. Lrec, pages 449-454.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Online learning of approximate dependency parsing algorithms",
                "authors": [
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Ryan",
                        "suffix": ""
                    },
                    {
                        "first": "Fernando Cn",
                        "middle": [],
                        "last": "Mcdonald",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Pereira",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "EACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ryan T McDonald and Fernando CN Pereira. 2006. Online learning of approximate dependency parsing algorithms. In EACL. Citeseer.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Online large-margin training of dependency parsers",
                "authors": [
                    {
                        "first": "Ryan",
                        "middle": [],
                        "last": "Mcdonald",
                        "suffix": ""
                    },
                    {
                        "first": "Koby",
                        "middle": [],
                        "last": "Crammer",
                        "suffix": ""
                    },
                    {
                        "first": "Fernando",
                        "middle": [],
                        "last": "Pereira",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of the 43rd annual meeting on association for computational linguistics",
                "volume": "",
                "issue": "",
                "pages": "91--98",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of de- pendency parsers. In Proceedings of the 43rd an- nual meeting on association for computational lin- guistics, pages 91-98. Association for Computa- tional Linguistics.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Maxmargin tensor neural network for chinese word segmentation",
                "authors": [
                    {
                        "first": "Wenzhe",
                        "middle": [],
                        "last": "Pei",
                        "suffix": ""
                    },
                    {
                        "first": "Tao",
                        "middle": [],
                        "last": "Ge",
                        "suffix": ""
                    },
                    {
                        "first": "Baobao",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics",
                "volume": "1",
                "issue": "",
                "pages": "293--303",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Wenzhe Pei, Tao Ge, and Baobao Chang. 2014. Max- margin tensor neural network for chinese word seg- mentation. In Proceedings of the 52nd Annual Meet- ing of the Association for Computational Linguistics (Volume 1: Long Papers), pages 293-303.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "An effective neural network model for graph-based dependency parsing",
                "authors": [
                    {
                        "first": "Wenzhe",
                        "middle": [],
                        "last": "Pei",
                        "suffix": ""
                    },
                    {
                        "first": "Tao",
                        "middle": [],
                        "last": "Ge",
                        "suffix": ""
                    },
                    {
                        "first": "Baobao",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "313--322",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Wenzhe Pei, Tao Ge, and Baobao Chang. 2015. An effective neural network model for graph-based de- pendency parsing. In Proceedings of the 53rd An- nual Meeting of the Association for Computational Linguistics, pages 313-322.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Sequence to sequence learning with neural networks",
                "authors": [
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    },
                    {
                        "first": "Oriol",
                        "middle": [],
                        "last": "Vinyals",
                        "suffix": ""
                    },
                    {
                        "first": "V",
                        "middle": [],
                        "last": "Quoc",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Le",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing System",
                "volume": "",
                "issue": "",
                "pages": "3104--3112",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to sequence learning with neural net- works. In Advances in Neural Information Process- ing Systems 27: Annual Conference on Neural In- formation Processing System, pages 3104-3112.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Feature-rich part-ofspeech tagging with a cyclic dependency network",
                "authors": [
                    {
                        "first": "Kristina",
                        "middle": [],
                        "last": "Toutanova",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Klein",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    },
                    {
                        "first": "Yoram",
                        "middle": [],
                        "last": "Singer",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology",
                "volume": "1",
                "issue": "",
                "pages": "173--180",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kristina Toutanova, Dan Klein, Christopher D Man- ning, and Yoram Singer. 2003. Feature-rich part-of- speech tagging with a cyclic dependency network. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computa- tional Linguistics on Human Language Technology- Volume 1, pages 173-180. Association for Compu- tational Linguistics.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Grammar as a foreign language",
                "authors": [
                    {
                        "first": "Oriol",
                        "middle": [],
                        "last": "Vinyals",
                        "suffix": ""
                    },
                    {
                        "first": "Lukasz",
                        "middle": [],
                        "last": "Kaiser",
                        "suffix": ""
                    },
                    {
                        "first": "Terry",
                        "middle": [],
                        "last": "Koo",
                        "suffix": ""
                    },
                    {
                        "first": "Slav",
                        "middle": [],
                        "last": "Petrov",
                        "suffix": ""
                    },
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    },
                    {
                        "first": "Geoffrey",
                        "middle": [
                            "E"
                        ],
                        "last": "Hinton",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey E. Hinton. 2014. Grammar as a foreign language. CoRR, abs/1412.7449.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Structured training for neural network transition-based parsing",
                "authors": [
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Weiss",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Alberti",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Collins",
                        "suffix": ""
                    },
                    {
                        "first": "Slav",
                        "middle": [],
                        "last": "Petrov",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "David Weiss, Chris Alberti, Michael Collins, and Slav Petrov. 2015. Structured training for neural net- work transition-based parsing. In Proceedings of the 53rd Annual Meeting of the Association for Compu- tational Linguistics.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Statistical dependency analysis with support vector machines",
                "authors": [
                    {
                        "first": "Hiroyasu",
                        "middle": [],
                        "last": "Yamada",
                        "suffix": ""
                    },
                    {
                        "first": "Yuji",
                        "middle": [],
                        "last": "Matsumoto",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Proceedings of IWPT",
                "volume": "3",
                "issue": "",
                "pages": "195--206",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis- tical dependency analysis with support vector ma- chines. In Proceedings of IWPT, volume 3, pages 195-206.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "A tale of two parsers: Investigating and combining graph-based and transition-based dependency parsing",
                "authors": [
                    {
                        "first": "Yue",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Stephen",
                        "middle": [],
                        "last": "Clark",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "2008 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "562--571",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yue Zhang and Stephen Clark. 2008. A tale of two parsers: Investigating and combining graph-based and transition-based dependency parsing. In 2008 Conference on Empirical Methods in Natural Lan- guage Processing, pages 562-571.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Generalized higher-order dependency parsing with cube pruning",
                "authors": [
                    {
                        "first": "Hao",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Ryan",
                        "middle": [
                            "T"
                        ],
                        "last": "Mcdonald",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning",
                "volume": "",
                "issue": "",
                "pages": "320--331",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Hao Zhang and Ryan T. McDonald. 2012. Generalized higher-order dependency parsing with cube prun- ing. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Process- ing and Computational Natural Language Learning, pages 320-331.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Enforcing structural diversity in cube-pruned dependency parsing",
                "authors": [
                    {
                        "first": "Hao",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Ryan",
                        "middle": [
                            "T"
                        ],
                        "last": "Mcdonald",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "656--661",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Hao Zhang and Ryan T. McDonald. 2014. Enforc- ing structural diversity in cube-pruned dependency parsing. In Proceedings of the 52nd Annual Meet- ing of the Association for Computational Linguis- tics, pages 656-661.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Transition-based dependency parsing with rich non-local features",
                "authors": [
                    {
                        "first": "Yue",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Joakim",
                        "middle": [],
                        "last": "Nivre",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies",
                "volume": "",
                "issue": "",
                "pages": "188--193",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yue Zhang and Joakim Nivre. 2011. Transition-based dependency parsing with rich non-local features. In The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Tech- nologies, pages 188-193.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Online learning for inexact hypergraph search",
                "authors": [
                    {
                        "first": "Hao",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Liang",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Kai",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Ryan",
                        "middle": [],
                        "last": "Mcdonald",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of Emnlp",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Hao Zhang, Liang Huang Kai Zhao, and Ryan Mcdon- ald. 2013. Online learning for inexact hypergraph search. Proceedings of Emnlp.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Deep learning for Chinese word segmentation and POS tagging",
                "authors": [
                    {
                        "first": "Xiaoqing",
                        "middle": [],
                        "last": "Zheng",
                        "suffix": ""
                    },
                    {
                        "first": "Hanyang",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Tianyu",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "647--657",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu. 2013. Deep learning for Chinese word segmentation and POS tagging. In Proceedings of the 2013 Con- ference on Empirical Methods in Natural Language Processing, pages 647-657, October.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 1: First-order, Second-order and Thirdorder factorization strategy. Here h stands for head word, m stands for modifier word, s and t stand for the sibling of m. g stands for the grandparent of m.",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Figure 2: Architecture of the Neural Network. x 1 to x 5 stand for the input token of Bidirectional LSTM. a 1 to a 5 stand for the feature embeddings used in our model.",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "Figure 3: Illustration for learning segment embeddings based on an extra forward LSTM network, v h , v m and v 1 to v 7 indicate the output vectors of Bidirectional LSTM for head word h, modifier word m and other words in sentence, h h , h m and h 1 to h 7 indicate the hidden vectors of the forward LSTM corresponding to v h , v m and v 1 to v 7 .",
                "uris": null,
                "fig_num": "3",
                "type_str": "figure"
            },
            "FIGREF3": {
                "num": null,
                "text": "Figure 4: Error rates of different distance between head and modifier on Peen-YM.",
                "uris": null,
                "fig_num": "4",
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table><tr><td>Table 1 lists the atomic features used in 1st-</td></tr><tr><td>order atomic model of</td></tr></table>",
                "type_str": "table",
                "text": "Atomic features in our basic model and Pei's 1st-order atomic model. w is short for word and p for POS tag. h indicates head and m indicates modifier. The subscript represents the relative position to the center word. dis(h, m) is the distance between head and modifier. v h and v m indicate the outputs of Bidirectional LSTM for head word and modifier word.",
                "html": null,
                "num": null
            },
            "TABREF1": {
                "content": "<table><tr><td>92.59 91.37</td><td>26</td></tr></table>",
                "type_str": "table",
                "text": "Comparison with previous graph-based models on Penn-YM.",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table><tr><td>, our</td></tr></table>",
                "type_str": "table",
                "text": "",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Comparison with previous state-of-the-art models on Penn-YM, Penn-SD and CTB5.",
                "html": null,
                "num": null
            }
        }
    }
}