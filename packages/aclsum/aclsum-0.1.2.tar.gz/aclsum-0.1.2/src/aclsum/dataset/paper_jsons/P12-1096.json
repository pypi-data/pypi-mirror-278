{
    "paper_id": "P12-1096",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:34:18.341622Z"
    },
    "title": "A Ranking-based Approach to Word Reordering for Statistical Machine Translation *",
    "authors": [
        {
            "first": "Nan",
            "middle": [],
            "last": "Yang",
            "suffix": "",
            "affiliation": {
                "laboratory": "MOE-MS Key Lab of MCC",
                "institution": "University of Science and Technology of China",
                "location": {}
            },
            "email": "v-nayang@microsoft.com"
        },
        {
            "first": "Mu",
            "middle": [],
            "last": "Li",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Microsoft Research Asia",
                "location": {}
            },
            "email": "muli@microsoft.com"
        },
        {
            "first": "Dongdong",
            "middle": [],
            "last": "Zhang",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Microsoft Research Asia",
                "location": {}
            },
            "email": "dozhang@microsoft.com"
        },
        {
            "first": "Nenghai",
            "middle": [],
            "last": "Yu",
            "suffix": "",
            "affiliation": {
                "laboratory": "MOE-MS Key Lab of MCC",
                "institution": "University of Science and Technology of China",
                "location": {}
            },
            "email": ""
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Long distance word reordering is a major challenge in statistical machine translation research. Previous work has shown using source syntactic trees is an effective way to tackle this problem between two languages with substantial word order difference. In this work, we further extend this line of exploration and propose a novel but simple approach, which utilizes a ranking model based on word order precedence in the target language to reposition nodes in the syntactic parse tree of a source sentence. The ranking model is automatically derived from word aligned parallel data with a syntactic parser for source language based on both lexical and syntactical features. We evaluated our approach on largescale Japanese-English and English-Japanese machine translation tasks, and show that it can significantly outperform the baseline phrasebased SMT system. *\nThis work has been done while the first author was visiting",
    "pdf_parse": {
        "paper_id": "P12-1096",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Long distance word reordering is a major challenge in statistical machine translation research. Previous work has shown using source syntactic trees is an effective way to tackle this problem between two languages with substantial word order difference. In this work, we further extend this line of exploration and propose a novel but simple approach, which utilizes a ranking model based on word order precedence in the target language to reposition nodes in the syntactic parse tree of a source sentence. The ranking model is automatically derived from word aligned parallel data with a syntactic parser for source language based on both lexical and syntactical features. We evaluated our approach on largescale Japanese-English and English-Japanese machine translation tasks, and show that it can significantly outperform the baseline phrasebased SMT system. *",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            },
            {
                "text": "This work has been done while the first author was visiting",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Modeling word reordering between source and target sentences has been a research focus since the emerging of statistical machine translation. In phrase-based models (Och, 2002; Koehn et al., 2003) , phrase is introduced to serve as the fundamental translation element and deal with local reordering, while a distance based distortion model is used to coarsely depict the exponentially decayed word movement probabilities in language translation. Further work in this direction employed lexi-calized distortion models, including both generative (Koehn et al., 2005) and discriminative (Zens and Ney, 2006; Xiong et al., 2006) variants, to achieve finer-grained estimations, while other work took into account the hierarchical language structures in translation (Chiang, 2005; Galley and Manning, 2008) .",
                "cite_spans": [
                    {
                        "start": 165,
                        "end": 176,
                        "text": "(Och, 2002;",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 177,
                        "end": 196,
                        "text": "Koehn et al., 2003)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 544,
                        "end": 564,
                        "text": "(Koehn et al., 2005)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 584,
                        "end": 604,
                        "text": "(Zens and Ney, 2006;",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 605,
                        "end": 624,
                        "text": "Xiong et al., 2006)",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 760,
                        "end": 774,
                        "text": "(Chiang, 2005;",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 775,
                        "end": 800,
                        "text": "Galley and Manning, 2008)",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Long-distance word reordering between language pairs with substantial word order difference, such as Japanese with Subject-Object-Verb (SOV) structure and English with Subject-Verb-Object (SVO) structure, is generally viewed beyond the scope of the phrase-based systems discussed above, because of either distortion limits or lack of discriminative features for modeling. The most notable solution to this problem is adopting syntax-based SMT models, especially methods making use of source side syntactic parse trees. There are two major categories in this line of research. One is tree-to-string model (Quirk et al., 2005; Liu et al., 2006) which directly uses source parse trees to derive a large set of translation rules and associated model parameters. The other is called syntax pre-reordering -an approach that re-positions source words to approximate target language word order as much as possible based on the features from source syntactic parse trees. This is usually done in a preprocessing step, and then followed by a standard phrase-based SMT system that takes the re-ordered source sentence as input to finish the translation.",
                "cite_spans": [
                    {
                        "start": 604,
                        "end": 624,
                        "text": "(Quirk et al., 2005;",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 625,
                        "end": 642,
                        "text": "Liu et al., 2006)",
                        "ref_id": "BIBREF12"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this paper, we continue this line of work and address the problem of word reordering based on source syntactic parse trees for SMT. Similar to most previous work, our approach tries to rearrange the source tree nodes sharing a common parent to mimic the word order in target language. To this end, we propose a simple but effective ranking-based approach to word reordering. The ranking model is automatically derived from the word aligned parallel data, viewing the source tree nodes to be reordered as list items to be ranked. The ranks of tree nodes are determined by their relative positions in the target language -the node in the most front gets the highest rank, while the ending word in the target sentence gets the lowest rank. The ranking model is trained to directly minimize the mis-ordering of tree nodes, which differs from the prior work based on maximum likelihood estimations of reordering patterns (Li et al., 2007; Genzel, 2010) , and does not require any special tweaking in model training. The ranking model can not only be used in a pre-reordering based SMT system, but also be integrated into a phrasebased decoder serving as additional distortion features.",
                "cite_spans": [
                    {
                        "start": 919,
                        "end": 936,
                        "text": "(Li et al., 2007;",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 937,
                        "end": 950,
                        "text": "Genzel, 2010)",
                        "ref_id": "BIBREF5"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "We evaluated our approach on large-scale Japanese-English and English-Japanese machine translation tasks, and experimental results show that our approach can bring significant improvements to the baseline phrase-based SMT system in both preordering and integrated decoding settings.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In the rest of the paper, we will first formally present our ranking-based word reordering model, then followed by detailed steps of modeling training and integration into a phrase-based SMT system. Experimental results are shown in Section 5. Section 6 consists of more discussions on related work, and Section 7 concludes the paper.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Given a source side parse tree T e , the task of word reordering is to transform T e to T e , so that e can match the word order in target language as much as possible. In this work, we only focus on reordering that can be obtained by permuting children of every tree nodes in T e . We use children to denote direct descendants of tree nodes for constituent trees; while for dependency trees, children of a node include not only all direct dependents, but also the head word itself. Figure 1 gives a simple example showing the word reordering between English and Japanese. By rearranging the position of tree nodes in the English e 0 e 1 e 2 e 3 e 4 e 5 j 0 j 1 j 2 j 3 j 4",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 490,
                        "end": 491,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Word Reordering as Syntax Tree Node Ranking",
                "sec_num": "2"
            },
            {
                "text": "e 0 e 1 e 2 e 3 e 4 e 5",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word Reordering as Syntax Tree Node Ranking",
                "sec_num": "2"
            },
            {
                "text": "Figure 1 : An English-to-Japanese sentence pair. By permuting tree nodes in the parse tree, the source sentence is reordered into the target language order. Constituent tree is shown above the source sentence; arrows below the source sentences show head-dependent arcs for dependency tree; word alignment links are lines without arrow between the source and target sentences.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Word Reordering as Syntax Tree Node Ranking",
                "sec_num": "2"
            },
            {
                "text": "parse tree, we can obtain the same word order of Japanese translation. It is true that tree-based reordering cannot cover all word movement operations in language translation, previous work showed that this method is still very effective in practice (Xu et al., 2009 , Visweswariah et al., 2010) . Following this principle, the word reordering task can be broken into sub-tasks, in which we only need to determine the order of children nodes for all non-leaf nodes in the source parse tree. For a tree node t with children {c 1 , c 2 , . . . , c n }, we rearrange the children to target-language-like order {c \u03c0(i 1 ) , c \u03c0(i 2 ) , . . . , c \u03c0(in) }. If we treat the reordered position \u03c0(i) of child c i as its \"rank\", the reorder-ing problem is naturally translated into a ranking problem: to reorder, we determine a \"rank\" for each child, then the children are sorted according to their \"ranks\". As it is often impractical to directly assign a score for each permutation due to huge number of possible permutations, a widely used method is to use a real valued function f to assign a value to each node, which is called a ranking function (Herbrich et al., 2000) . If we can guarantee (f (i) -f (j)) and (\u03c0(i) -\u03c0(j)) always has the same sign, we can get the same permutation as \u03c0 because values of f are only used to sort the children. For example, consider the node rooted at trying in the dependency tree in Figure 1 . Four children form a list {I, am, trying, play} to be ranked. Assuming ranking function f can assign values {0.94, -1.83, -1.50, -1.20} for {I, am, trying, play} respectively, we can get a sorted list {I, play, trying, am}, which is the desired permutation according to the target.",
                "cite_spans": [
                    {
                        "start": 250,
                        "end": 266,
                        "text": "(Xu et al., 2009",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 267,
                        "end": 295,
                        "text": ", Visweswariah et al., 2010)",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 1141,
                        "end": 1164,
                        "text": "(Herbrich et al., 2000)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 1419,
                        "end": 1420,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Word Reordering as Syntax Tree Node Ranking",
                "sec_num": "2"
            },
            {
                "text": "More formally, for a tree node t with children {c 1 , c 2 , . . . , c n }, our ranking model assigns a rank f (c i , t) for each child c i , then the children are sorted according to the rank in a descending order. The ranking function f has the following form:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word Reordering as Syntax Tree Node Ranking",
                "sec_num": "2"
            },
            {
                "text": "f (c i , t) = j \u03b8 j (c i , t) \u2022 w j (1)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word Reordering as Syntax Tree Node Ranking",
                "sec_num": "2"
            },
            {
                "text": "where the \u03b8 j is a feature representing the tree node t and its child c i , and w j is the corresponding feature weight.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Word Reordering as Syntax Tree Node Ranking",
                "sec_num": "2"
            },
            {
                "text": "To learn ranking function in Equation ( 1), we need to determine the feature set \u03b8 and learn weight vector w from reorder examples. In this section, we first describe how to extract reordering examples from parallel corpus; then we show our features for ranking function; finally, we discuss how to train the model from the extracted examples.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Ranking Model Training",
                "sec_num": "3"
            },
            {
                "text": "For a sentence pair (e, f, a) with syntax tree T e on the source side, we need to determine which reordered tree T e best represents the word order in target sentence f . For a tree node t in T e , if its children align to disjoint target spans, we can simply arrange them in the order of their corresponding target spans. Figure 2 shows a fragment of one sentence pair in our training data. Consider the subtree rooted at word \"Problem\". With the gold alignment, \"Problem\" is aligned to the 5th target word, and \"with latter procedure\" are aligned to target span [1, 3], thus we can simply put \"Problem\" after \"with latter procedure\". Recursively applying this process down the subtree, we get \"latter procedure with Problem\" which perfectly matches the target language.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 330,
                        "end": 331,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Reorder Example Acquisition",
                "sec_num": "3.1"
            },
            {
                "text": "As pointed out by (Li et al., 2007) , in practice, nodes often have overlapping target spans due to erroneous word alignment or different syntactic structures between source and target sentences. (b) in Figure 2 shows the automatically generated alignment for the sentence pair fragment. The word \"with\" is incorrectly aligned to the 6th Japanese word \"ha\"; as a result, \"with latter procedure\" now has target span [1, 6], while \"Problem\" aligns to [5, 5] . Due to this overlapping, it becomes unclear which permutation of \"Problem\" and \"with latter procedure\" is a better match of the target phrase; we need a better metric to measure word order similarity between reordered source and target sentences. We choose to find the tree T e with minimal alignment crossing-link number (CLN) (Genzel, 2010) to f as our golden reordered tree. 1 Each crossing-link (i 1 j 1 , i 2 j 2 ) is a pair of alignment links crossing each other. CLN reaches zero if f is monotonically aligned to e , and increases as there are more word reordering between e and f . For example, in Figure 1 , there are 6 crossing-links in the original tree: (e 1 j 4 , e 2 j 3 ), (e 1 j 4 , e 4 j 2 ), (e 1 j 4 , e 5 j 1 ), (e 2 j 3 , e 4 j 2 ), (e 2 j 3 , e 5 j 1 ) and (e 4 j 2 , e 5 j 1 ); thus CLN for the original tree is 6. CLN for the reordered tree is 0 as there are no crossing-links. This metric is easy to compute, and is not affected by unaligned words (Genzel, 2010) .",
                "cite_spans": [
                    {
                        "start": 18,
                        "end": 35,
                        "text": "(Li et al., 2007)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 449,
                        "end": 452,
                        "text": "[5,",
                        "ref_id": null
                    },
                    {
                        "start": 453,
                        "end": 455,
                        "text": "5]",
                        "ref_id": null
                    },
                    {
                        "start": 786,
                        "end": 800,
                        "text": "(Genzel, 2010)",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 1431,
                        "end": 1445,
                        "text": "(Genzel, 2010)",
                        "ref_id": "BIBREF5"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 210,
                        "end": 211,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    },
                    {
                        "start": 1071,
                        "end": 1072,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Reorder Example Acquisition",
                "sec_num": "3.1"
            },
            {
                "text": "We need to find the reordered tree with minimal CLN among all reorder candidates. As the number of candidates is in the magnitude exponential with respect to the degree of tree T e 2 , it is not always computationally feasible to enumerate through all candidates. Our solution is as follows.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Reorder Example Acquisition",
                "sec_num": "3.1"
            },
            {
                "text": "First, we give two definitions.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Reorder Example Acquisition",
                "sec_num": "3.1"
            },
            {
                "text": "\u2022 CLN (t): the number of crossing-links (i 1 j 1 , i 2 j 2 ) whose source words e i 1 and e i 2 both fall under sub span of the tree node t.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Reorder Example Acquisition",
                "sec_num": "3.1"
            },
            {
                "text": "\u2022 CCLN (t): the number of crossing-links (i 1 j 1 , i 2 j 2 ) whose source words e i 1 and e i 2 fall under sub span of t's two different children nodes c 1 and c 2 respectively.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Reorder Example Acquisition",
                "sec_num": "3.1"
            },
            {
                "text": "Apparently CLN of a tree T equals to CLN (root of T ), and CLN (t) can be recursively expressed as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Reorder Example Acquisition",
                "sec_num": "3.1"
            },
            {
                "text": "CLN (t) = CCLN (t) + child c of t CLN (c)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Reorder Example Acquisition",
                "sec_num": "3.1"
            },
            {
                "text": "Take the original tree in Figure 1 for example. At the root node trying, CLN(trying) is 6 because there are six crossing-links under its sub-span: (e 1 j 4 , e 2 j 3 ), (e 1 j 4 , e 4 j 2 ), (e 1 j 4 , e 5 j 1 ), (e 2 j 3 , e 4 j 2 ), (e 2 j 3 , e 5 j 1 ) and (e 4 j 2 , e 5 j 1 ). On the other hand, CCLN(trying) is 5 because (e 4 j 2 , e 5 j 1 ) falls under its child node play, thus does not count towards CCLN of trying.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 33,
                        "end": 34,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Reorder Example Acquisition",
                "sec_num": "3.1"
            },
            {
                "text": "From the definition, we can easily see that CCLN(t) can be determined solely by the order of t's direct children, and CLN (t) is only affected by discarded too many training instances and led to degraded reordering performance.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Reorder Example Acquisition",
                "sec_num": "3.1"
            },
            {
                "text": "2 In our experiments, there are nodes with more than 10 children for English dependency trees.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Reorder Example Acquisition",
                "sec_num": "3.1"
            },
            {
                "text": "the reorder in the subtree of t. This observation enables us to divide the task of finding the reordered tree T e with minimal CLN into independently finding the children permutation of each node with minimal CCLN. Unfortunately, the time cost for the subtask is still O(n!) for a node with n children. Instead of enumerating through all permutations, we only search the Inversion Transduction Grammar neighborhood of the initial sequence (Tromble, 2009) . As pointed out by (Tromble, 2009) , the ITG neighborhood is large enough for reordering task, and can be searched through efficiently using a CKY decoder.",
                "cite_spans": [
                    {
                        "start": 439,
                        "end": 454,
                        "text": "(Tromble, 2009)",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 475,
                        "end": 490,
                        "text": "(Tromble, 2009)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Reorder Example Acquisition",
                "sec_num": "3.1"
            },
            {
                "text": "After finding the best reordered tree T e , we can extract one reorder example from every node with more than one child.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Reorder Example Acquisition",
                "sec_num": "3.1"
            },
            {
                "text": "Features for the ranking model are extracted from source syntax trees. For English-to-Japanese task, we extract features from Stanford English Dependency Tree (Marneffe et al., 2006) , including lexicons, Part-of-Speech tags, dependency labels, punctuations and tree distance between head and dependent. For Japanese-to-English task, we use a chunkbased Japanese dependency tree (Kudo and Matsumoto, 2002) . Different from features for English, we do not use dependency labels because they are not available from the Japanese parser. Additionally, Japanese function words are also included as features because they are important grammatical clues. The detailed feature templates are shown in Table 1 .",
                "cite_spans": [
                    {
                        "start": 159,
                        "end": 182,
                        "text": "(Marneffe et al., 2006)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 379,
                        "end": 405,
                        "text": "(Kudo and Matsumoto, 2002)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 698,
                        "end": 699,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Features",
                "sec_num": "3.2"
            },
            {
                "text": "There are many well studied methods available to learn the ranking function from extracted examples., ListNet (?) etc. We choose to use RankingSVM (Herbrich et al., 2000) , a pair-wised ranking method, for its simplicity and good performance.",
                "cite_spans": [
                    {
                        "start": 147,
                        "end": 170,
                        "text": "(Herbrich et al., 2000)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Method",
                "sec_num": "3.3"
            },
            {
                "text": "For every reorder example t with children {c 1 , c 2 , . . . , c n } and their desired permutation {c \u03c0(i 1 ) , c \u03c0(i 2 ) , . . . , c \u03c0(in) }, we decompose it into a set of pair-wised training instances. For any two children nodes c i and c j with i < j , we extract a positive instance if \u03c0(i) < \u03c0(j), otherwise we extract a negative instance. The feature vector for both positive instance and negative instance is (\u03b8 c i -\u03b8 c j ), where \u03b8 c i and \u03b8 c j are feature vectors for c i and c j Table 1: Feature templates for ranking function. All templates are implicitly conjuncted with the pos tag of head node. c: child to be ranked; h: head node lc: left sibling of c; rc: right sibling of c l: dependency label; t: pos tag lex: top frequency lexicons f : Japanese function word dst: tree distance between c and h pct: punctuation node between c and h respectively. In this way, ranking function learning is turned into a simple binary classification problem, which can be easily solved by a two-class linear support vector machine.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Method",
                "sec_num": "3.3"
            },
            {
                "text": "There are two ways to integrate the ranking reordering model into a phrase-based SMT system: the prereorder method, and the decoding time constraint method.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Integration into SMT system",
                "sec_num": "4"
            },
            {
                "text": "For pre-reorder method, ranking reorder model is applied to reorder source sentences during both training and decoding. Reordered sentences can go through the normal pipeline of a phrase-based decoder.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Integration into SMT system",
                "sec_num": "4"
            },
            {
                "text": "The ranking reorder model can also be integrated into a phrase based decoder. Integrated method takes the original source sentence e as input, and ranking model generates a reordered e as a word order ref-erence for the decoder. A simple penalty scheme is utilized to penalize decoder reordering violating ranking reorder model's prediction e . In this paper, our underlying decoder is a CKY decoder following Bracketing Transduction Grammar (Wu, 1997; Xiong et al., 2006) , thus we show how the penalty is implemented in the BTG decoder as an example. Similar penalty can be designed for other decoders without much effort.",
                "cite_spans": [
                    {
                        "start": 442,
                        "end": 452,
                        "text": "(Wu, 1997;",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 453,
                        "end": 472,
                        "text": "Xiong et al., 2006)",
                        "ref_id": "BIBREF23"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Integration into SMT system",
                "sec_num": "4"
            },
            {
                "text": "Under BTG, three rules are used to derive translations: one unary terminal rule, one straight rule and one inverse rule:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Integration into SMT system",
                "sec_num": "4"
            },
            {
                "text": "A \u2192 e/f A \u2192 [A 1 , A 2 ] A \u2192 A 1 , A 2",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Integration into SMT system",
                "sec_num": "4"
            },
            {
                "text": "We have three penalty triggers when any rules are applied during decoding:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Integration into SMT system",
                "sec_num": "4"
            },
            {
                "text": "\u2022 Discontinuous penalty f dc : it fires for all rules when source span of either A, A 1 or A 2 is mapped to discontinuous span in e .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Integration into SMT system",
                "sec_num": "4"
            },
            {
                "text": "\u2022 Wrong straight rule penalty f st : it fires for straight rule when source spans of A 1 and A 2 are not mapped to two adjacent spans in e in straight order.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Integration into SMT system",
                "sec_num": "4"
            },
            {
                "text": "\u2022 Wrong inverse rule penalty f iv : it fires for inverse rule when source spans of A 1 and A 2 are not mapped to two adjacent spans in e in inverse order.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Integration into SMT system",
                "sec_num": "4"
            },
            {
                "text": "The above three penalties are added as additional features into the log-linear model of the phrasebased system. Essentially they are soft constraints to encourage the decoder to choose translations with word order similar to the prediction of ranking reorder model.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Integration into SMT system",
                "sec_num": "4"
            },
            {
                "text": "To test our ranking reorder model, we carry out experiments on large scale English-To-Japanese, and Japanese-To-English translation tasks.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "5"
            },
            {
                "text": "We collect 3,500 Japanese sentences and 3,500 English sentences from the web. They come from a wide range of domains, such as technical documents, web forum data, travel logs etc. They are manually translated into the other language to produce 7,000 sentence pairs, which are split into two parts: 2,000 pairs as development set (dev) and the other 5,000 pairs as test set (web test).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation Data",
                "sec_num": "5.1.1"
            },
            {
                "text": "Beside that, we collect another 999 English sentences from newswire domain which are translated into Japanese to form an out-of-domain test data set (news test).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation Data",
                "sec_num": "5.1.1"
            },
            {
                "text": "Our parallel corpus is crawled from the web, containing news articles, technical documents, blog entries etc. After removing duplicates, we have about 18 million sentence pairs, which contain about 270 millions of English tokens and 320 millions of Japanese tokens. We use Giza++ (Och and Ney, 2003) to generate the word alignment for the parallel corpus.",
                "cite_spans": [
                    {
                        "start": 280,
                        "end": 299,
                        "text": "(Och and Ney, 2003)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Parallel Corpus",
                "sec_num": "5.1.2"
            },
            {
                "text": "Our monolingual Corpus is also crawled from the web. After removing duplicate sentences, we have a corpus of over 10 billion tokens for both English and Japanese. This monolingual corpus is used to train a 4-gram language model for English and Japanese respectively.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Monolingual Corpus",
                "sec_num": "5.1.3"
            },
            {
                "text": "For English, we train a dependency parser as (Nivre and Scholz, 2004) on WSJ portion of Penn Treebank, which are converted to dependency trees using Stanford Parser (Marneffe et al., 2006) . We convert the tokens in training data to lower case, and re-tokenize the sentences using the same tokenizer from our MT system.",
                "cite_spans": [
                    {
                        "start": 45,
                        "end": 69,
                        "text": "(Nivre and Scholz, 2004)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 165,
                        "end": 188,
                        "text": "(Marneffe et al., 2006)",
                        "ref_id": "BIBREF13"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Parsers",
                "sec_num": "5.2"
            },
            {
                "text": "For Japanese parser, we use CABOCHA, a chunk-based dependency parser (Kudo and Matsumoto, 2002) . Some heuristics are used to adapt CABOCHA generated trees to our word segmentation.",
                "cite_spans": [
                    {
                        "start": 69,
                        "end": 95,
                        "text": "(Kudo and Matsumoto, 2002)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Parsers",
                "sec_num": "5.2"
            },
            {
                "text": "We use a BTG phrase-based system with a Max-Ent based lexicalized reordering model (Wu, 1997; Xiong et al., 2006 ) as our baseline system for both English-to-Japanese and Japanese-to-English Experiment. The distortion model is trained on the same parallel corpus as the phrase table using a home implemented maximum entropy trainer.",
                "cite_spans": [
                    {
                        "start": 83,
                        "end": 93,
                        "text": "(Wu, 1997;",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 94,
                        "end": 112,
                        "text": "Xiong et al., 2006",
                        "ref_id": "BIBREF23"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baseline System",
                "sec_num": "5.3.1"
            },
            {
                "text": "In addition, a pre-reorder system using manual rules as (Xu et al., 2009) is included for the Englishto-Japanese experiment (ManR-PR). Manual rules are tuned by a bilingual speaker on the development set.",
                "cite_spans": [
                    {
                        "start": 56,
                        "end": 73,
                        "text": "(Xu et al., 2009)",
                        "ref_id": "BIBREF24"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baseline System",
                "sec_num": "5.3.1"
            },
            {
                "text": "Ranking reordering model is learned from the same parallel corpus as phrase table. For efficiency reason, we only use 25% of the corpus to train our reordering model. LIBLINEAR (Fan et al., 2008) is used to do the SVM optimization for RankingSVM.",
                "cite_spans": [
                    {
                        "start": 177,
                        "end": 195,
                        "text": "(Fan et al., 2008)",
                        "ref_id": "BIBREF2"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Ranking Reordering System",
                "sec_num": "5.3.2"
            },
            {
                "text": "We test it on both pre-reorder setting (Rank-PR) and integrated setting (Rank-IT). 2 : BLEU(%) score on dev and test data for both E-J and J-E experiment. All settings significantly improve over the baseline at 95% confidence level. Baseline is the BTG phrase system system; ManR-PR is pre-reorder with manual rule; Rank-PR is pre-reorder with ranking reorder model; Rank-IT is system with integrated ranking reorder model.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 83,
                        "end": 84,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Ranking Reordering System",
                "sec_num": "5.3.2"
            },
            {
                "text": "From Table 2 , we can see our ranking reordering model significantly improves the performance for both English-to-Japanese and Japanese-to-English experiments over the BTG baseline system. It also out-performs the manual rule set on English-to-Japanese result, but the difference is not significant.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 11,
                        "end": 12,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "End",
                "sec_num": "5.4"
            },
            {
                "text": "In order to show whether the improved performance is really due to improved reordering, we would like to measure the reorder performance directly.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Reordering Performance",
                "sec_num": "5.5"
            },
            {
                "text": "As we do not have access to a golden reordered sentence set, we decide to use the alignment crossing-link numbers between aligned sentence pairs as the measure for reorder performance.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Reordering Performance",
                "sec_num": "5.5"
            },
            {
                "text": "We train the ranking model on 25% of our parallel corpus, and use the rest 75% as test data (auto). We sample a small corpus (575 sentence pairs) and do manual alignment (man-small). We denote the automatic alignment for these 575 sentences as (auto-small). From Table 3 our ranking reordering model indeed significantly reduces the crossing-link numbers over the original sentence pairs. On the other hand, the performance of the ranking reorder model still fall far short of oracle, which is the lowest crossing-link number of all possible permutations allowed by the parse tree. By manual analysis, we find that the gap is due to both errors of the ranking reorder model and errors from word alignment and parser.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 269,
                        "end": 270,
                        "text": "3",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Reordering Performance",
                "sec_num": "5.5"
            },
            {
                "text": "Another thing to note is that the crossing-link number of manual alignment is higher than automatic alignment. The reason is that our annotators tend to align function words which might be left unaligned by automatic word aligner.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Reordering Performance",
                "sec_num": "5.5"
            },
            {
                "text": "Here we examine the effect of features for ranking reorder model. We compare their influence on Rank-ingSVM accuracy, alignment crossing-link number, end-to-end BLEU score, and the model size. As dependency labels (for English), function words (for Japanese), and the distance and punctuations between child and head. These features also correspond to BLEU score improvement for End-to-End evaluations. Lexicon features generally continue to improve the RankingSVM accuracy and reduce CLN on training data, but they do not bring further improvement for SMT systems beyond the top 100 most frequent words. Our explanation is that less frequent lexicons tend to help local reordering only, which is already handled by the underlying phrasebased system.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Effect of Ranking Features",
                "sec_num": "5.6"
            },
            {
                "text": "From Table 2 we can see that pre-reorder method has higher BLEU score on news test, while integrated model performs better on web test set which contains informal texts. By error analysis, we find that the parser commits more errors on informal texts, and informal texts usually have more flexible translations. Pre-reorder method makes \"hard\" decision before decoding, thus is more sensitive to parser errors; on the other hand, integrated model is forced to use a longer distortion limit which leads to more search errors during decoding time. It is possible to use system combination method to get the best of both systems, but we leave this to future work.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 11,
                        "end": 12,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Performance on different domains",
                "sec_num": "5.7"
            },
            {
                "text": "There have been several studies focusing on compiling hand-crafted syntactic reorder rules. Collins et al. (2005) , Wang et al. (2007) , Ramanathan et al. (2008) , Lee et al. (2010) have developed rules for German-English, Chinese-English, English-Hindi and English-Japanese respectively. Xu et al. (2009) designed a clever precedence reordering rule set for translation from English to several SOV languages.",
                "cite_spans": [
                    {
                        "start": 92,
                        "end": 113,
                        "text": "Collins et al. (2005)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 116,
                        "end": 134,
                        "text": "Wang et al. (2007)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 137,
                        "end": 161,
                        "text": "Ramanathan et al. (2008)",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 164,
                        "end": 181,
                        "text": "Lee et al. (2010)",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 289,
                        "end": 305,
                        "text": "Xu et al. (2009)",
                        "ref_id": "BIBREF24"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discussion on Related Work",
                "sec_num": "6"
            },
            {
                "text": "The drawback for hand-crafted rules is that they depend upon expert knowledge to produce and are limited to their targeted language pairs. Automatically learning syntactic reordering rules have also been explored in several work. Li et al. (2007) and Visweswariah et al. (2010) learned probability of reordering patterns from constituent trees using either Maximum Entropy or maximum likelihood estimation. Since reordering patterns are matched against a tree node together with all its direct children, data sparseness problem will arise when tree nodes have many children (Li et al., 2007) ; Visweswariah et al. (2010) also mentioned their method yielded no improvement when applied to dependency trees in their initial experiments. Genzel (2010) dealt with the data sparseness problem by using window heuristic, and learned reordering pattern sequence from dependency trees. Even with the window heuristic, they were unable to evaluate all candidates due to the huge number of possible patterns. Different from the previous approaches, we treat syntax-based reordering as a ranking problem between different source tree nodes. Our method does not require the source nodes to match some specific patterns, but encodes reordering knowledge in the form of a ranking function, which naturally handles reordering between any number of tree nodes; the ranking function is trained by well-established rank learning method to minimize the number of mis-ordered tree nodes in the training data.",
                "cite_spans": [
                    {
                        "start": 230,
                        "end": 246,
                        "text": "Li et al. (2007)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 251,
                        "end": 277,
                        "text": "Visweswariah et al. (2010)",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 574,
                        "end": 591,
                        "text": "(Li et al., 2007)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 594,
                        "end": 620,
                        "text": "Visweswariah et al. (2010)",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 735,
                        "end": 748,
                        "text": "Genzel (2010)",
                        "ref_id": "BIBREF5"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discussion on Related Work",
                "sec_num": "6"
            },
            {
                "text": "Tree-to-string systems (Quirk et al., 2005; Liu et al., 2006) model syntactic reordering using minimal or composed translation rules, which may contain reordering involving tree nodes from multiple tree levels. Our method can be naturally extended to deal with such multiple level reordering. For a tree-tostring rule with multiple tree levels, instead of ranking the direct children of the root node, we rank all leaf nodes (Most are frontier nodes (Galley et al., 2006) ) in the translation rule. We need to redesign our ranking feature templates to encode the reordering information in the source part of the translation rules. We need to remember the source side context of the rules, the model size would still be much smaller than a full-fledged tree-to-string system because we do not need to explicitly store the target variants for each rule.",
                "cite_spans": [
                    {
                        "start": 23,
                        "end": 43,
                        "text": "(Quirk et al., 2005;",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 44,
                        "end": 61,
                        "text": "Liu et al., 2006)",
                        "ref_id": null
                    },
                    {
                        "start": 450,
                        "end": 471,
                        "text": "(Galley et al., 2006)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discussion on Related Work",
                "sec_num": "6"
            },
            {
                "text": "In this paper we present a ranking based reordering method to reorder source language to match the word order of target language given the source side parse tree. Reordering is formulated as a task to rank different nodes in the source side syntax tree according to their relative position in the target language. The ranking model is automatically trained to minimize the mis-ordering of tree nodes in the training data. Large scale experiment shows improvement on both reordering metric and SMT performance, with up to 1.73 point BLEU gain in our evaluation test.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion and Future Work",
                "sec_num": "7"
            },
            {
                "text": "In future work, we plan to extend the ranking model to handle reordering between multiple levels of source trees. We also expect to explore better way to integrate ranking reorder model into SMT system instead of a simple penalty scheme. Along the research direction of preprocessing the source language to facilitate translation, we consider to not only change the order of the source language, but also inject syntactic structure of the target language into source language by adding pseudo words into source sentences.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion and Future Work",
                "sec_num": "7"
            },
            {
                "text": "A simple solution is to exclude all trees with overlapping target spans from training. But in our experiment, this method",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "Nan Yang and Nenghai Yu were partially supported by Fundamental Research Funds for the Central Universities (No. WK2100230002), National Natural Science Foundation of China (No. 60933013), and National Science and Technology Major Project (No. 2010ZX03004-003).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgements",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "A Hierarchical Phrase-Based Model for Statistical Machine Translation",
                "authors": [
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Chiang",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proc. ACL",
                "volume": "",
                "issue": "",
                "pages": "263--270",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "David Chiang. 2005. A Hierarchical Phrase-Based Model for Statistical Machine Translation. In Proc. ACL, pages 263-270.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Clause restructuring for statistical machine translation",
                "authors": [
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Collins",
                        "suffix": ""
                    },
                    {
                        "first": "Philipp",
                        "middle": [],
                        "last": "Koehn",
                        "suffix": ""
                    },
                    {
                        "first": "Ivona",
                        "middle": [],
                        "last": "Kucerova",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proc. ACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Michael Collins, Philipp Koehn and Ivona Kucerova. 2005. Clause restructuring for statistical machine translation. In Proc. ACL.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "LIBLINEAR: A library for large linear classification",
                "authors": [
                    {
                        "first": "R.-E",
                        "middle": [],
                        "last": "Fan",
                        "suffix": ""
                    },
                    {
                        "first": "K.-W",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "C.-J",
                        "middle": [],
                        "last": "Hsieh",
                        "suffix": ""
                    },
                    {
                        "first": "X.-R",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "C.-J",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Journal of Machine Learning Research",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin. 2008. LIBLINEAR: A library for large lin- ear classification. In Journal of Machine Learning Re- search.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Scalable Inference and Training of Context-Rich Syntactic Translation Models",
                "authors": [
                    {
                        "first": "Michel",
                        "middle": [],
                        "last": "Galley",
                        "suffix": ""
                    },
                    {
                        "first": "Jonathan",
                        "middle": [],
                        "last": "Graehl",
                        "suffix": ""
                    },
                    {
                        "first": "Kevin",
                        "middle": [],
                        "last": "Knight",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Marcu",
                        "suffix": ""
                    },
                    {
                        "first": "Steve",
                        "middle": [],
                        "last": "Deneefe",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Ignacio",
                        "middle": [],
                        "last": "Thayer",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Proc. ACL-Coling",
                "volume": "",
                "issue": "",
                "pages": "961--968",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable Inference and Training of Context-Rich Syntactic Translation Models. In Proc. ACL-Coling, pages 961-968.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "A Simple and Effective Hierarchical Phrase Reordering Model",
                "authors": [
                    {
                        "first": "Michel",
                        "middle": [],
                        "last": "Galley",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Proc. EMNLP",
                "volume": "",
                "issue": "",
                "pages": "263--270",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Michel Galley and Christopher D. Manning. 2008. A Simple and Effective Hierarchical Phrase Reordering Model. In Proc. EMNLP, pages 263-270.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Automatically Learning Sourceside Reordering Rules for Large Scale Machine Translation",
                "authors": [
                    {
                        "first": "Dmitriy",
                        "middle": [],
                        "last": "Genzel",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proc. Coling",
                "volume": "",
                "issue": "",
                "pages": "376--384",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dmitriy Genzel. 2010. Automatically Learning Source- side Reordering Rules for Large Scale Machine Trans- lation. In Proc. Coling, pages 376-384.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Large Margin Rank Boundaries for Ordinal Regression",
                "authors": [
                    {
                        "first": "Ralf",
                        "middle": [],
                        "last": "Herbrich",
                        "suffix": ""
                    },
                    {
                        "first": "Thore",
                        "middle": [],
                        "last": "Graepel",
                        "suffix": ""
                    },
                    {
                        "first": "Klaus",
                        "middle": [],
                        "last": "Obermayer",
                        "suffix": ""
                    }
                ],
                "year": 2000,
                "venue": "Advances in Large Margin Classifiers",
                "volume": "",
                "issue": "",
                "pages": "115--132",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ralf Herbrich, Thore Graepel, and Klaus Obermayer 2000. Large Margin Rank Boundaries for Ordinal Re- gression. In Advances in Large Margin Classifiers, pages 115-132.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Edinborgh System Description for the 2005 IWSLT Speech Translation Evaluation",
                "authors": [
                    {
                        "first": "Philipp",
                        "middle": [],
                        "last": "Koehn",
                        "suffix": ""
                    },
                    {
                        "first": "Amittai",
                        "middle": [],
                        "last": "Axelrod",
                        "suffix": ""
                    },
                    {
                        "first": "Alexandra",
                        "middle": [
                            "Birch"
                        ],
                        "last": "Mayne",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Callison-Burch",
                        "suffix": ""
                    },
                    {
                        "first": "Miles",
                        "middle": [],
                        "last": "Osborne",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Talbot",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "International Workshop on Spoken Language Translation",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, Chris Callison-Burch, Miles Osborne and David Talbot. 2005. Edinborgh System Description for the 2005 IWSLT Speech Translation Evaluation. In International Workshop on Spoken Language Transla- tion.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Statistical Phrase-Based Translation",
                "authors": [
                    {
                        "first": "Philipp",
                        "middle": [],
                        "last": "Koehn",
                        "suffix": ""
                    },
                    {
                        "first": "Franz",
                        "middle": [
                            "J"
                        ],
                        "last": "Och",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Marcu",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Proc. HLT-NAACL",
                "volume": "",
                "issue": "",
                "pages": "127--133",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical Phrase-Based Translation. In Proc. HLT- NAACL, pages 127-133.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Japanese Dependency Analysis using Cascaded Chunking",
                "authors": [
                    {
                        "first": "Taku",
                        "middle": [],
                        "last": "Kudo",
                        "suffix": ""
                    },
                    {
                        "first": "Yuji",
                        "middle": [],
                        "last": "Matsumoto",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Proc. CoNLL",
                "volume": "",
                "issue": "",
                "pages": "63--69",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Taku Kudo, Yuji Matsumoto. 2002. Japanese Depen- dency Analysis using Cascaded Chunking. In Proc. CoNLL, pages 63-69.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Constituent reordering and syntax models for Englishto-Japanese statistical machine translation",
                "authors": [
                    {
                        "first": "Young-Suk",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Bing",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaoqiang",
                        "middle": [],
                        "last": "Luo",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proc. Coling",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Young-Suk Lee, Bing Zhao and Xiaoqiang Luo. 2010. Constituent reordering and syntax models for English- to-Japanese statistical machine translation. In Proc. Coling.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "A Probabilistic Approach to Syntax-based Reordering for Statistical Machine Translation",
                "authors": [
                    {
                        "first": "Chi-Ho",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Minghui",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Dongdong",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Mu",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Ming",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Yi",
                        "middle": [],
                        "last": "Guan",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Proc. ACL",
                "volume": "",
                "issue": "",
                "pages": "720--727",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li and Ming Zhou and Yi Guan 2007. A Probabilistic Ap- proach to Syntax-based Reordering for Statistical Ma- chine Translation. In Proc. ACL, pages 720-727.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Treeto-String Alignment Template for Statistical Machine Translation",
                "authors": [
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Qun",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Shouxun",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Proc. ACL-Coling",
                "volume": "",
                "issue": "",
                "pages": "609--616",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree- to-String Alignment Template for Statistical Machine Translation. In Proc. ACL-Coling, pages 609-616.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Generating Typed Dependency Parses from Phrase Structure Parses",
                "authors": [
                    {
                        "first": "Marie-Catherine",
                        "middle": [],
                        "last": "De Marneffe",
                        "suffix": ""
                    },
                    {
                        "first": "Bill",
                        "middle": [],
                        "last": "Maccartney",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "LREC",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Marie-Catherine de Marneffe, Bill MacCartney and Christopher D. Manning. 2006. Generating Typed Dependency Parses from Phrase Structure Parses. In LREC 2006",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Deterministic Dependency Parsing for English Text",
                "authors": [
                    {
                        "first": "Joakim",
                        "middle": [],
                        "last": "Nivre",
                        "suffix": ""
                    },
                    {
                        "first": "Mario",
                        "middle": [],
                        "last": "Scholz",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proc. Coling",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Joakim Nivre and Mario Scholz 2004. Deterministic De- pendency Parsing for English Text. In Proc. Coling.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Statistical Machine Translation: From Single Word Models to Alignment Template",
                "authors": [
                    {
                        "first": "Franz",
                        "middle": [
                            "J"
                        ],
                        "last": "Och",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Franz J. Och. 2002. Statistical Machine Translation: From Single Word Models to Alignment Template. Ph.D.Thesis, RWTH Aachen, Germany",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "A Systematic Comparison of Various Statistical Alignment Models",
                "authors": [
                    {
                        "first": "Franz",
                        "middle": [
                            "J"
                        ],
                        "last": "Och",
                        "suffix": ""
                    },
                    {
                        "first": "Hermann",
                        "middle": [],
                        "last": "Ney",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Computational Linguistics",
                "volume": "29",
                "issue": "1",
                "pages": "19--51",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Franz J. Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1): pages 19-51.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Dependency Treelet Translation: Syntactically Informed Phrasal SMT",
                "authors": [
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Quirk",
                        "suffix": ""
                    },
                    {
                        "first": "Arul",
                        "middle": [],
                        "last": "Menezes",
                        "suffix": ""
                    },
                    {
                        "first": "Colin",
                        "middle": [],
                        "last": "Cherry",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proc. ACL",
                "volume": "",
                "issue": "",
                "pages": "271--279",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De- pendency Treelet Translation: Syntactically Informed Phrasal SMT. In Proc. ACL, pages 271-279.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Simple syntactic and morphological processing can help English-Hindi Statistical Machine Translation",
                "authors": [
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Ramanathan",
                        "suffix": ""
                    },
                    {
                        "first": "Pushpak",
                        "middle": [],
                        "last": "Bhattacharyya",
                        "suffix": ""
                    },
                    {
                        "first": "Jayprasad",
                        "middle": [],
                        "last": "Hegde",
                        "suffix": ""
                    },
                    {
                        "first": "Ritesh",
                        "middle": [
                            "M"
                        ],
                        "last": "Shah",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Sasikumar",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Proc. IJCNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "A. Ramanathan, Pushpak Bhattacharyya, Jayprasad Hegde, Ritesh M. Shah and Sasikumar M. 2008. Simple syntactic and morphological processing can help English-Hindi Statistical Machine Translation. In Proc. IJCNLP.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Search and Learning for the Linear Ordering Problem with an Application to Machine Translation",
                "authors": [
                    {
                        "first": "Roy",
                        "middle": [],
                        "last": "Tromble",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Roy Tromble. 2009. Search and Learning for the Lin- ear Ordering Problem with an Application to Machine Translation. Ph.D. Thesis.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Syntax Based Reordering with Automatically Derived Rules for Improved Statistical Machine Translation",
                "authors": [
                    {
                        "first": "Karthik",
                        "middle": [],
                        "last": "Visweswariah",
                        "suffix": ""
                    },
                    {
                        "first": "Jiri",
                        "middle": [],
                        "last": "Navratil",
                        "suffix": ""
                    },
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Sorensen",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proc. Coling",
                "volume": "",
                "issue": "",
                "pages": "1119--1127",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Karthik Visweswariah, Jiri Navratil, Jeffrey Sorensen, Vijil Chenthamarakshan and Nandakishore Kamb- hatla. 2010. Syntax Based Reordering with Automat- ically Derived Rules for Improved Statistical Machine Translation. In Proc. Coling, pages 1119-1127.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Chinese syntactic reordering for statistical machine translation",
                "authors": [
                    {
                        "first": "Chao",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Collins",
                        "suffix": ""
                    },
                    {
                        "first": "Philipp",
                        "middle": [],
                        "last": "Koehn",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Proc. EMNLP-CoNLL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chao Wang, Michael Collins, Philipp Koehn. 2007. Chi- nese syntactic reordering for statistical machine trans- lation. In Proc. EMNLP-CoNLL.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora",
                "authors": [
                    {
                        "first": "Dekai",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    }
                ],
                "year": 1997,
                "venue": "Computational Linguistics",
                "volume": "23",
                "issue": "3",
                "pages": "377--403",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dekai Wu. 1997. Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora. Computational Linguistics, 23(3): pages 377-403.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Maximum Entropy Based Phrase Reordering Model for Statistical Machine Translation",
                "authors": [
                    {
                        "first": "Deyi",
                        "middle": [],
                        "last": "Xiong",
                        "suffix": ""
                    },
                    {
                        "first": "Qun",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Shouxun",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Proc. ACL-Coling",
                "volume": "",
                "issue": "",
                "pages": "521--528",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi- mum Entropy Based Phrase Reordering Model for Sta- tistical Machine Translation. In Proc. ACL-Coling, pages 521-528.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Using a Dependency Parser to Improve SMT for Subject-Object-Verb Languages",
                "authors": [
                    {
                        "first": "Peng",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Jaeho",
                        "middle": [],
                        "last": "Kang",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Ringgaard",
                        "suffix": ""
                    },
                    {
                        "first": "Franz",
                        "middle": [],
                        "last": "Och",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proc. HLT-NAACL",
                "volume": "",
                "issue": "",
                "pages": "376--384",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Peng Xu, Jaeho Kang, Michael Ringgaard, Franz Och. 2009. Using a Dependency Parser to Improve SMT for Subject-Object-Verb Languages. In Proc. HLT- NAACL, pages 376-384.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Discriminative Reordering Models for Statistical Machine Translation",
                "authors": [
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Zens",
                        "suffix": ""
                    },
                    {
                        "first": "Hermann",
                        "middle": [],
                        "last": "Ney",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Proc. Workshop on Statistical Machine Translation, HLT-NAACL",
                "volume": "",
                "issue": "",
                "pages": "127--133",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Richard Zens and Hermann Ney. 2006. Discriminative Reordering Models for Statistical Machine Transla- tion. In Proc. Workshop on Statistical Machine Trans- lation, HLT-NAACL, pages 127-133.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF1": {
                "num": null,
                "text": "Figure 2: Fragment of a sentence pair. (a) shows gold alignment; (b) shows automatically generated alignment which contains errors.",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "TABREF1": {
                "content": "<table><tr><td/><td/><td/><td>, we can see</td></tr><tr><td/><td colspan=\"3\">setting auto auto-small man-small</td></tr><tr><td/><td>None 36.3</td><td>35.9</td><td>40.1</td></tr><tr><td/><td>Oracle 4.3</td><td>4.1</td><td>7.4</td></tr><tr><td>E-J</td><td>ManR 13.4</td><td>13.6</td><td>16.7</td></tr><tr><td/><td>Rank 12.1</td><td>12.8</td><td>17.2</td></tr><tr><td>J-E</td><td>Oracle 6.9 Rank 15.7</td><td>7.0 15.3</td><td>9.4 20.5</td></tr></table>",
                "type_str": "table",
                "text": "Reorder performance measured by crossing-link number per sentence. None means the original sentences without reordering; Oracle means the best permutation allowed by the source parse tree; ManR refers to manual reorder rules; Rank means ranking reordering model.",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table><tr><td/><td>Features</td><td colspan=\"3\">Acc. CLN BLEU Feat.#</td></tr><tr><td/><td colspan=\"2\">tag+label 88.6 16.4</td><td>22.24</td><td>26k</td></tr><tr><td/><td>+dst</td><td>91.5 13.5</td><td>22.66</td><td>55k</td></tr><tr><td>E-J</td><td>+pct +lex 100</td><td>92.2 13.1 92.9 12.1</td><td>22.73 22.85</td><td>79k 347k</td></tr><tr><td/><td>+lex 1000</td><td>94.0 11.5</td><td colspan=\"2\">22.79 2,410k</td></tr><tr><td/><td>+lex 2000</td><td>95.2 10.7</td><td colspan=\"2\">22.81 3,794k</td></tr><tr><td/><td>tag+f w</td><td>85.0 18.6</td><td>25.43</td><td>31k</td></tr><tr><td/><td>+dst</td><td>90.3 16.9</td><td>25.62</td><td>65k</td></tr><tr><td>J-E</td><td>+lex 100 +lex 1000</td><td>91.6 15.7 92.4 14.8</td><td colspan=\"2\">25.87 25.91 2,156k 293k</td></tr><tr><td/><td>+lex 2000</td><td>93.0 14.3</td><td colspan=\"2\">25.84 3,297k</td></tr></table>",
                "type_str": "table",
                "text": "Table 4 shows, a major part of reduction of CLN comes from features such as Part-of-Speech tags,",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Effect of ranking features. Acc. is Rank-ingSVM accuracy in percentage on the training data; CLN is the crossing-link number per sentence on parallel corpus with automatically generated word alignment; BLEU is the BLEU score in percentage on web test set on Rank-IT setting (system with integrated rank reordering model); lex n means n most frequent lexicons in the training corpus.",
                "html": null,
                "num": null
            }
        }
    }
}