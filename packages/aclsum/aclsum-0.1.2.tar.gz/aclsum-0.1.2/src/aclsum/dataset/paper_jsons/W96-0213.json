{
    "paper_id": "W96-0213",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:13:44.396209Z"
    },
    "title": "A Maximum Entropy Model for Part-Of-Speech Tagging",
    "authors": [
        {
            "first": "Adwait",
            "middle": [],
            "last": "Ratnaparkhi",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Pennsylvania",
                "location": {}
            },
            "email": ""
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "This paper presents a statistical model which trains from a corpus annotated with Part-Of-Speech tags and assigns them to previously unseen text with state-of-the-art accuracy(96.6%). The model can be classified as a Maximum Entropy model and simultaneously uses many contextual \"features\" to predict the POS tag. Furthermore, this paper demonstrates the use of specialized features to model difficult tagging decisions, discusses the corpus consistency problems discovered during the implementation of these features, and proposes a training strategy that mitigates these problems.",
    "pdf_parse": {
        "paper_id": "W96-0213",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "This paper presents a statistical model which trains from a corpus annotated with Part-Of-Speech tags and assigns them to previously unseen text with state-of-the-art accuracy(96.6%). The model can be classified as a Maximum Entropy model and simultaneously uses many contextual \"features\" to predict the POS tag. Furthermore, this paper demonstrates the use of specialized features to model difficult tagging decisions, discusses the corpus consistency problems discovered during the implementation of these features, and proposes a training strategy that mitigates these problems.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Many natural language tasks require the accurate assignment of Part-Of-Speech (POS) tags to previously unseen text. Due to the availability of large corpora which have been manually annotated with POS information, many taggers use annotated text to \"learn\" either probability distributions or rules and use them to automatically assign POS tags to unseen text.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": null
            },
            {
                "text": "The experiments in this paper were conducted on the Wall Street Journal corpus from the Penn Treebank project (Marcus et al., 1994) , although the model can trai~n from any large corpus annotated with POS tags. Since most realistic natural language applications must process words that were never seen before in training data, all experiments in this paper are conducted on test data that include unknown words.",
                "cite_spans": [
                    {
                        "start": 110,
                        "end": 131,
                        "text": "(Marcus et al., 1994)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": null
            },
            {
                "text": "Several recent papers (Brill, 1994 , Magerman, 1995) have reported 96.5% tagging accuracy on the Wall St. Journal corpus. The experiments in this paper test the hypothesis that better use of context will improve the accuracy. A Maximum Entropy model is well-suited for such experiments since it corn-bines diverse forms of contextual information in a principled manner, and does not impose any distributional assumptions on the training data. Previous uses of this model include language modeling (Lau et al., 1993) , machine translation (Berger et al., 1996) , prepositional phrase attachment (Ratnaparkhi et al., 1994) , and word morphology (Della Pietra et al., 1995) . This paper briefly describes the maximum entropy and maximum likelihood properties of the model, features used for POS tagging, and the experiments on the Penn Treebank Wall St. Journal corpus. It then discusses the consistency problems discovered during an attempt to use specialized features on the word context. Lastly, the results in this paper are compared to those from previous work on POS tagging.",
                "cite_spans": [
                    {
                        "start": 22,
                        "end": 34,
                        "text": "(Brill, 1994",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 35,
                        "end": 52,
                        "text": ", Magerman, 1995)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 497,
                        "end": 515,
                        "text": "(Lau et al., 1993)",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 538,
                        "end": 559,
                        "text": "(Berger et al., 1996)",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 594,
                        "end": 620,
                        "text": "(Ratnaparkhi et al., 1994)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 643,
                        "end": 670,
                        "text": "(Della Pietra et al., 1995)",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": null
            },
            {
                "text": "The probability model is defined over 7-/x 7-, where 7t is the set of possible word and tag contexts, or \"histories\", and T is the set of allowable tags. The model's probability of a history h together with a tag t is defined as: ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Probability Model",
                "sec_num": null
            },
            {
                "text": "r(p) = 1-[ p(h.t,) = 1-[ i=1 i=1 j=l",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Probability Model",
                "sec_num": null
            },
            {
                "text": "This model also can be interpreted under the Maximum Entropy formalism, in which the goal is to maximize the entropy of a distribution subject to certain constraints. Here, the entropy of the distribution p is defined as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Probability Model",
                "sec_num": null
            },
            {
                "text": "H(p) = -E p(h,t) logp(h,t) hE74,tET",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Probability Model",
                "sec_num": null
            },
            {
                "text": "and the constraints are given by:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Probability Model",
                "sec_num": null
            },
            {
                "text": "Efj = Efj, l < j _< k (2)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Probability Model",
                "sec_num": null
            },
            {
                "text": "where the model's feature expectation is",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Probability Model",
                "sec_num": null
            },
            {
                "text": "Efj= E p(h,t)fj(h,t) hET.l,tET",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Probability Model",
                "sec_num": null
            },
            {
                "text": "and the observed feature expectation is i=1 and where iS(hi, ti) denotes the observed probability of (hi,ti) in the training data. Thus the constraints force the model to match its feature expectations with those observed in the training data.",
                "cite_spans": [
                    {
                        "start": 101,
                        "end": 108,
                        "text": "(hi,ti)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Probability Model",
                "sec_num": null
            },
            {
                "text": "In practice, 7-/ is very large and the model's expectation Efj cannot be computed directly, so the following approximation (Lau et al., 1993) is used:",
                "cite_spans": [
                    {
                        "start": 123,
                        "end": 141,
                        "text": "(Lau et al., 1993)",
                        "ref_id": "BIBREF5"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Probability Model",
                "sec_num": null
            },
            {
                "text": "n E fj ,~ E15(hi)p(tilhi)fj(hi,ti) i=1",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Probability Model",
                "sec_num": null
            },
            {
                "text": "where fi(hi) is the observed probability of the history hi in the training set.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Probability Model",
                "sec_num": null
            },
            {
                "text": "It can be shown (Darroch and Ratcliff, 1972 ) that if p has the form (1) and satisfies the k constraints (2), it uniquely maximizes the entropy H(p) over distributions that satisfy (2), and uniquely maximizes the likelihood L(p) over distributions of the form (1). The model parameters for the distribution p are obtained via Generalized Iterative Scaling (Darroch and Ratcliff, 1972) .",
                "cite_spans": [
                    {
                        "start": 16,
                        "end": 43,
                        "text": "(Darroch and Ratcliff, 1972",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 356,
                        "end": 384,
                        "text": "(Darroch and Ratcliff, 1972)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Probability Model",
                "sec_num": null
            },
            {
                "text": "The joint probability of a history h and tag t is determined by those parameters whose corresponding features are active, i.e., those o~j such 134 that fj(h,t) = 1. A feature, given (h,t), may activate on any word or tag in the history h, and must encode any information that might help predict t, such as the spelling of the current word, or the identity of the previous two tags. The specific word and tag context available to a feature is given in the following definition of a history hi:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Features for POS Tagging",
                "sec_num": null
            },
            {
                "text": "hi \u2022 {wi, wi+I, wi-}-2, wi-1, wi-2, ti-1, ti-2}",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Features for POS Tagging",
                "sec_num": null
            },
            {
                "text": "For example,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Features for POS Tagging",
                "sec_num": null
            },
            {
                "text": "1 if suffix(w/) = \"ing\" & ti = VBG fj(hl,ti)= 0 otherwise",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Features for POS Tagging",
                "sec_num": null
            },
            {
                "text": "If the above feature exists in the feature set of the model, its corresponding model parameter will contribute towards the joint probability p(hi,ti) when wi ends with \"\u00b1ng\" and when ti =VBG 1. Thus a model parameter aj effectively serves as a \"weight\" for a certain contextual predictor, in this case the suffix \"ing\", towards the probability of observing a certain tag, in this case a VBG.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Features for POS Tagging",
                "sec_num": null
            },
            {
                "text": "The model generates the space of features by scanning each pair (hi, ti) in the training data with the feature \"templates\" given in Table 1 . Given hi as the current history, a feature always asks some yes/no question about hi, and furthermore constrains ti to be a certain tag. The instantiations for the variables X, Y, and T in Table 1 are obtained automatically from the training data.",
                "cite_spans": [
                    {
                        "start": 64,
                        "end": 72,
                        "text": "(hi, ti)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 138,
                        "end": 139,
                        "text": "1",
                        "ref_id": null
                    },
                    {
                        "start": 337,
                        "end": 338,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Features for POS Tagging",
                "sec_num": null
            },
            {
                "text": "The generation of features for tagging unknown words relies on the hypothesized distinction that \"rare\" words 2 in the training set are similar to unknown words in test data, with respect to how their spellings help predict their tags. The rare word features in Table 1 , which look at the word spellings, will apply to both rare words and unknown words in test data.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 268,
                        "end": 269,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Features for POS Tagging",
                "sec_num": null
            },
            {
                "text": "For example, Table 2 contains an excerpt from training data while Table 3 contains the features generated while scanning (ha, t3), in which the current word is about, and Table 4 contains features generated while scanning (h4, t4), in which the current word, well-heeled, occurs 3 times in training data and is therefore classified as \"rare\".",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 19,
                        "end": 20,
                        "text": "2",
                        "ref_id": null
                    },
                    {
                        "start": 72,
                        "end": 73,
                        "text": "3",
                        "ref_id": "TABREF1"
                    },
                    {
                        "start": 177,
                        "end": 178,
                        "text": "4",
                        "ref_id": "TABREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Features for POS Tagging",
                "sec_num": null
            },
            {
                "text": "The behavior of a feature that occurs very sparsely in the training set is often difficult to predict, since its statistics may not be reliable. Therefore, the model uses the heuristic that any feature 1VBG is the Treebank POS tag for Verb Gerund. 2A \"rare\" word here denotes a word which occurs less than 5 times in the training set. The count of 5 was chosen by subjective inspection of words in the training data. Table ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Features for POS Tagging",
                "sec_num": null
            },
            {
                "text": "w i is not rare wi = X & ti = T wi is rare \u00a5 wi X is prefix of wi, IXI ~ 4 &ti=T X is suffix of wi, IXI < 4 & li \u2022 T wi contains number ~z ti = T wi contains uppercase character ~ ti = T wi contains hyphen ~ti=T ti-1 = X & ti = T ti_2ti_t = XY ~ ti = T wi-1 = X & li = T wi-2 = X ~ ti = T wi+ 1 = X ~s t i = T wi+2 = X & ti = T",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Condition Features",
                "sec_num": null
            },
            {
                "text": "2: Sample Data wi ----about wi-i -~ stories wi-2 ----the Wi+l = well-heeled wi+2 ----communities ti-I = NNS ti-2ti-1 = DT NNS",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Condition Features",
                "sec_num": null
            },
            {
                "text": "& ti = IN g5 ti = IN & ti = IN ti = IN ~2 ti = IN ~: ti = IN ti = IN tagging about) from Table 2 Wi-1 -~ about \u2022 ti = JJ wi-2 = stories ~z ti = JJ Wi+l = communities & ti = JJ wi+2 = and ~ ti = JJ ti-1 = IN ~ ti = JJ ti-2ti-I ----I~IS IN ~z ti = JJ prefix(wi)=w ~ ii = JJ prefix(wi)----we & ti = JJ prefix(wi)=wel ~z li = JJ prefix(wi)=well & ti = JJ sulffix(wi)=d ~ ti = JJ suffix(wi)=ed & ti = JJ sufx(wi)=led ~ ti = JJ suffix(wi)=eled & ti = JJ wi contains hyphen & ti = JJ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Condition Features",
                "sec_num": null
            },
            {
                "text": "i=l",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "P(tl . .tnlwl ..wn) = II p(tilh,)",
                "sec_num": null
            },
            {
                "text": "In addition the search procedure optionally consults a Tag Dictionary, which, for each known word, lists the tags that it has appeared with in the training set. If the Tag Dictionary is in effect, the search procedure, for known words, generates only tags given by the dictionary entry, while for unknown words, generates all tags in the tag set. Without the Tag Dictionary, the search procedure generates all tags in the tag set for every word.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "P(tl . .tnlwl ..wn) = II p(tilh,)",
                "sec_num": null
            },
            {
                "text": "Let W = {wl...w,~} be a test sentence, and let sij be the jth highest probability tag sequence up to and including word wi. The search is described below:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "P(tl . .tnlwl ..wn) = II p(tilh,)",
                "sec_num": null
            },
            {
                "text": "1. Generate tags for wl, find top N, set 81j , 1 _< j < N, accordingly. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "P(tl . .tnlwl ..wn) = II p(tilh,)",
                "sec_num": null
            },
            {
                "text": "In order to conduct tagging experiments, the Wall St. Journal data has been split into three contiguous sections, as shown in Table 5 . The feature set and search algorithm were tested and debugged only on the Training and Development sets, and the official test result on the unseen Test set is presented in the conclusion of the paper. The performances of the \"baseline\" model on the Development Set, both with and without the Tag Dictionary, are shown in Table 6 .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 132,
                        "end": 133,
                        "text": "5",
                        "ref_id": null
                    },
                    {
                        "start": 464,
                        "end": 465,
                        "text": "6",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": null
            },
            {
                "text": "All experiments use a beam size of N = 5; further increasing the beam size does not significantly increase performance on the Development Set but adversely affects the speed of the tagger. Even though use of the Tag Dictionary gave an apparently insignificant (. 12%) improvement in accuracy, it is used in further experiments since it significantly reduces the number of hypotheses and thus speeds up the tagger.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": null
            },
            {
                "text": "The running time of the parameter estimation algorithm is O(NTA), where N is the training set size, T is the number of allowable tags, and A is the average number of features that are active for a given event (h, t). The running time of the search procedure on a sentence of length N is O(NTAB), where T, A are defined above, and B is the beam size. In practice, the model for the experiment shown in Table 6 requires approximately 24 hours to train, and 1 hour to test 4 on an IBM RS/6000 Model 380 with 256MB of RAM.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 407,
                        "end": 408,
                        "text": "6",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": null
            },
            {
                "text": "The Maximum Entropy model allows arbitrary binary-valued features on the context, so it can use additional specialized, i.e., word-specific, features 4The search procedure has not been optimized and the author expects it to run 3 to 5 times faster after optimizations. 7 ; clearly, the model has trouble with the words that and about, among others. As hypothesized in the introduction, better features on the context surrounding that and about should correct the tagging mistakes for these two words, assuming that the tagging errors are due to an impoverished feature set, and not inconsistent data.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 269,
                        "end": 270,
                        "text": "7",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Consistency",
                "sec_num": null
            },
            {
                "text": "Specialized features for a given word are constructed by conjoining certain features in the baseline model with a question about the word itself. The features which ask about previous tags and surrounding words now additionally ask about the identity of the current word, e.g., a specialized feature for the word about in Table 3 could be:",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 328,
                        "end": 329,
                        "text": "3",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "DataSet",
                "sec_num": null
            },
            {
                "text": "1 if wi : about ~ ti-2ti-1 = DT NNS fj (hi, ti) = & ti = IN 0 otherwise",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "DataSet",
                "sec_num": null
            },
            {
                "text": "Table 8 shows the results of an experiment in which specialized features are constructed for \"difficult\" words, and are added to the baseline feature set. Here, \"difficult\" words are those that are mistagged a certain way at least 50 times when the training set is tagged with the baseline model. Using the set of 29 difficult words, the model performs at 96.49% accuracy on the Development Set, an insignificant improvement from the baseline accuracy of 96.43%. Table 9 shows the change in error rates on the Development Set for the frequently occurring \"difficult\" words. For most words, the specialized model yields little or no improvement, and for some, i.e., more and about, the specialized model performs worse.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "8",
                        "ref_id": "TABREF5"
                    },
                    {
                        "start": 469,
                        "end": 470,
                        "text": "9",
                        "ref_id": "TABREF6"
                    }
                ],
                "eq_spans": [],
                "section": "DataSet",
                "sec_num": null
            },
            {
                "text": "The lack of improvement implies that either the feature set is still impoverished, or that the training data is inconsistent. A simple consistency test is to graph the POS tag assignments for a given word as a function of the article in which it occurs. Consistently tagged words should have roughly the same tag distribution as the article numbers vary. Figure 1 represents each POS tag with a unique integer and graphs the POS annotation of about in the training set as a function of the 138 articles (the points are \"scattered\" to show density). As seen in figure 1 , about is usually annotated with tag#l, which denotes IN (preposition), or tag#9, which denotes RB (adverb), and the observed probability of either choice depends heavily on the current article-~. Upon further examination 5, the tagging distribution for about changes precisely when the annotator changes. Figure 2 , which again uses integers to denote POS tags, shows the tag distribution of about as a function of annotator, and implies that the tagging errors for this word are due mostly to inconsistent data. The words ago, chief, down, executive, off, out, up and yen also exhibit similar bias.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 362,
                        "end": 363,
                        "text": "1",
                        "ref_id": "FIGREF4"
                    },
                    {
                        "start": 567,
                        "end": 568,
                        "text": "1",
                        "ref_id": "FIGREF4"
                    },
                    {
                        "start": 883,
                        "end": 884,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "DataSet",
                "sec_num": null
            },
            {
                "text": "Thus specialized features may be less effective for those words affected by inter-annotator bias. A simple solution to eliminate inter-annotator inconsistency is to train and test the model on data that has been created by the same annotator. The results of such an experiment 6 are shown in Table 10. The total accuracy is higher, implying that the singly-annotated training and test sets are more consistent, and the improvement due to the specialized features is higher than before (.1%) but still modest, implying that either the features need further improvement or that intra-annotator inconsistencies exist in the corpus.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "DataSet",
                "sec_num": null
            },
            {
                "text": "Most of the recent corpus-based POS taggers in the literature are either statistically based, and use Markov Model (Weischedel et al., 1993 , Merialdo, 1994) or Statistical Decision Tree (Jelinek et al., 1994 , Magerman, 1995) Figure 2 : Distribution of Tags for the word \"about\" vs. Annotator (Weischedel et al., 1993) provide the results from a battery of \"tri-tag\" Markov Model experiments, in which the probability P(W,T) of observing a word sequence W = {wl,w2,...,wn} together with a tag sequence T = {tl,t2,...,tn} is given by: P(TIW)P( W This approximation works as well as the MaxEnt model, giving 85% unknown word accuracy (Weischedel et al., 1993) on the Wall St.",
                "cite_spans": [
                    {
                        "start": 115,
                        "end": 139,
                        "text": "(Weischedel et al., 1993",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 140,
                        "end": 157,
                        "text": ", Merialdo, 1994)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 187,
                        "end": 208,
                        "text": "(Jelinek et al., 1994",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 209,
                        "end": 226,
                        "text": ", Magerman, 1995)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 294,
                        "end": 319,
                        "text": "(Weischedel et al., 1993)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 633,
                        "end": 658,
                        "text": "(Weischedel et al., 1993)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 234,
                        "end": 235,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Comparison With Previous Work",
                "sec_num": null
            },
            {
                "text": "Journal, but cannot be generalized to handle more diverse information sources. Multiplying together all the probabilities becomes less convincing of an approximation as the information sources become less independent. In contrast, the Max-Ent model combines diverse and non-local information sources without making any independence assumptions.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Comparison With Previous Work",
                "sec_num": null
            },
            {
                "text": "A POS tagger is one component in the SDT based statisticM parsing system described in (Jelinek et al., 1994 , Magerman, 1995) . The total word accuracy on Wall St.",
                "cite_spans": [
                    {
                        "start": 86,
                        "end": 107,
                        "text": "(Jelinek et al., 1994",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 108,
                        "end": 125,
                        "text": ", Magerman, 1995)",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "140",
                "sec_num": null
            },
            {
                "text": "Journal data, 96.5% (Magerman, 1995) , is similar to that presented in this paper.",
                "cite_spans": [
                    {
                        "start": 20,
                        "end": 36,
                        "text": "(Magerman, 1995)",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "140",
                "sec_num": null
            },
            {
                "text": "However, the aforementioned SDT techniques require word classes (Brown et al., 1992) to help prevent data fragmentation, and a sophisticated smoothing algorithm to mitigate the effects of any fragmentation that occurs. Unlike SDT, the MaxEnt training procedure does not recursively split the data, and hence does not suffer from unreliable counts due to data fragmentation. As a result, no word classes are required and a trivial count cutoff sufrices as a smoothing procedure in order to achieve roughly the same level of accuracy.",
                "cite_spans": [
                    {
                        "start": 64,
                        "end": 84,
                        "text": "(Brown et al., 1992)",
                        "ref_id": "BIBREF2"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "140",
                "sec_num": null
            },
            {
                "text": "TBL is a non-statistical approach to POS tagging which also uses a rich feature representation, and performs at a total word accuracy of 96.5% and an unknown word accuracy of 85%. (Bri11, 1994) . The TBL representation of the surrounding word context is almost the same 7 and the TBL representation of unknown words is a superset s of the unknown word representation in this paper. However, since TBL is non-statistical, it does not provide probability distributions and 7 (Brill, 1994) looks at words \u00b13 away from the current, whereas the feature set in this paper uses a window of \u00b12.",
                "cite_spans": [
                    {
                        "start": 180,
                        "end": 193,
                        "text": "(Bri11, 1994)",
                        "ref_id": null
                    },
                    {
                        "start": 473,
                        "end": 486,
                        "text": "(Brill, 1994)",
                        "ref_id": "BIBREF1"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "140",
                "sec_num": null
            },
            {
                "text": "8 (Brill, 1994) uses prefix/suffix additions and deletions, which are not used in this paper.",
                "cite_spans": [
                    {
                        "start": 2,
                        "end": 15,
                        "text": "(Brill, 1994)",
                        "ref_id": "BIBREF1"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "140",
                "sec_num": null
            },
            {
                "text": "unlike MaxEnt, cannot be used as a probabilistic component in a larger model. MaxEnt can provide a probability for each tagging decision, which can be used in the probability calculation of any structure that is predicted over the POS tags, such as noun phrases, or entire parse trees, as in (Jelinek et al., 1994 , Magerman, 1995) .",
                "cite_spans": [
                    {
                        "start": 292,
                        "end": 313,
                        "text": "(Jelinek et al., 1994",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 314,
                        "end": 331,
                        "text": ", Magerman, 1995)",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "140",
                "sec_num": null
            },
            {
                "text": "Thus MaxEnt has at least one advantage over each of the reviewed POS tagging techniques. It is better able to use diverse information than Markov Models, requires less supporting techniques than SDT, and unlike TBL, can be used in a probabilistic framework. However, the POS tagging accuracy on the Penn Wall St. Journal corpus is roughly the same for all these modelling techniques. The convergence of the accuracy rate implies that either all these techniques are missing the right predictors in their representation to get the \"residue\", or more likely, that any corpus based algorithm on the Penn Treebank Wall St. Journal corpus will not perform much higher than 96.5% due to consistency problems.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "140",
                "sec_num": null
            },
            {
                "text": "The Maximum Entropy model is an extremely flexible technique for linguistic modelling, since it can use a virtually unrestricted and rich feature set in the framework of a probability model. The implementation in this paper is a state-of-the-art POS tagger, as evidenced by the 96.6% accuracy on the unseen Test set, shown in Table 11 .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 332,
                        "end": 334,
                        "text": "11",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": null
            },
            {
                "text": "The model with specialized features does not perform much better than the baseline model, and further discovery or refinement of word-based features is difficult given the inconsistencies in the training data. A model trained and tested on data from a single annotator performs at .5% higher accuracy than the baseline model and should produce more consistent input for applications that require tagged text.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": null
            }
        ],
        "back_matter": [],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "ARPA",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Arp ; Berger",
                        "suffix": ""
                    }
                ],
                "year": 1994,
                "venue": "Proceedings of the Human Language Technology Workshop",
                "volume": "22",
                "issue": "",
                "pages": "39--71",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "ARP, 1994] ARPA. 1994. Proceedings of the Hu- man Language Technology Workshop. [Berger et al., 1996] Adam Berger, Stephen A. Della Pietra, and Vincent J. Della Pietra. 1996. A Maximum Entropy Approach to Natural Language Processing. Computational Linguistics, 22(1):39-71.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Some Advances in Transformation-Based Part of Speech Tagging",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Brill",
                        "suffix": ""
                    }
                ],
                "year": 1994,
                "venue": "Proceedings off the Twelfth National Conference on Artificial Intelligence",
                "volume": "1",
                "issue": "",
                "pages": "722--727",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Brill, 1994] Eric Brill. 1994. Some Advances in Transformation-Based Part of Speech Tagging. In Proceedings off the Twelfth National Confer- ence on Artificial Intelligence, volume 1, pages 722-727.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Class-Based n-gram Models of Natural Language",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Brown",
                        "suffix": ""
                    }
                ],
                "year": 1992,
                "venue": "Computational Linguistics",
                "volume": "18",
                "issue": "4",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Brown et al., 1992] Peter F Brown, Vincent Del- laPietra, Peter V deSouza, Jennifer C Lai, and Robert L Mercer. 1992. Class-Based n-gram Models of Natural Language. Computational Linguistics, 18(4).",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Generalized Iterative Scaling for Log-Linear Models",
                "authors": [
                    {
                        "first": "Ratcliff",
                        "middle": [],
                        "last": "Darroch",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [
                            "N"
                        ],
                        "last": "Darroch",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Ratcliff",
                        "suffix": ""
                    }
                ],
                "year": 1972,
                "venue": "The Annals of Mathematical Statistics",
                "volume": "43",
                "issue": "5",
                "pages": "1470--1480",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Darroch and Ratcliff, 1972] J. N. Darroch and D. Ratcliff. 1972. Generalized Iterative Scaling for Log-Linear Models. The Annals of Mathe- matical Statistics, 43(5) :1470-1480.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Decision Tree Parsing using a Hidden Derivational Model",
                "authors": [
                    {
                        "first": "Della",
                        "middle": [],
                        "last": "Pietra",
                        "suffix": ""
                    }
                ],
                "year": 1994,
                "venue": "Proceedings of the Human Language Technology Workshop",
                "volume": "",
                "issue": "",
                "pages": "272--277",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Della Pietra et al., 1995] Steven Della Pietra, Vincent Della Pietra, and John Lafferty. 1995. Inducing Features of Random Fields. Techni- cal Report CMU-CS95-144, School of Computer Science, Carnegie-Mellon University. [Jelinek et al., 1994] F Jelinek, J Lafferty, D Magerman, R Mercer, A Ratnaparkhi, and S Roukos. 1994. Decision Tree Parsing using a Hidden Derivational Model. In Proceedings of the Human Language Technology Workshop (ARP, 1994), pages 272-277.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Adaptive Language Modeling Using The Maximum Entropy Principle",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Lau",
                        "suffix": ""
                    }
                ],
                "year": 1993,
                "venue": "Proceedings of the Human Language Technology Workshop",
                "volume": "",
                "issue": "",
                "pages": "108--113",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Lau et al., 1993] Ray Lau, Ronald Rosenfeld, and Salim Roukos. 1993. Adaptive Language Modeling Using The Maximum Entropy Prin- ciple. In Proceedings of the Human Language Technology Workshop, pages 108-113. ARPA.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Building a large annotated corpus of English: the Penn Treebank",
                "authors": [
                    {
                        "first": "David",
                        "middle": [
                            "M"
                        ],
                        "last": "Magerman",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Magerman",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Marcus",
                        "suffix": ""
                    }
                ],
                "year": 1994,
                "venue": "Proceedings of the 33rd Annual Meeting of the ACL",
                "volume": "19",
                "issue": "",
                "pages": "313--330",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Magerman, 1995] David M. Magerman. 1995. Statistical Decision-Tree Models for Parsing. In Proceedings of the 33rd Annual Meeting of the ACL. [Marcus et al., 1994] Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Mareinkiewicz. 1994. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19(2):313-330.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Tagging English Text with a Probabilistic Model",
                "authors": [
                    {
                        "first": "Bernard",
                        "middle": [],
                        "last": "Merialdo",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Merialdo",
                        "suffix": ""
                    }
                ],
                "year": 1994,
                "venue": "Computational Linguistics",
                "volume": "20",
                "issue": "2",
                "pages": "155--172",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Merialdo, 1994] Bernard Merialdo. 1994. Tag- ging English Text with a Probabilistic Model. Computational Linguistics, 20(2):155-172.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "A Maximum Entropy Model for Prepositional Phrase Attachment",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Ratnaparkhi",
                        "suffix": ""
                    }
                ],
                "year": 1994,
                "venue": "Proceedings of the Human Language Technology Workshop (ARP, 1994)",
                "volume": "",
                "issue": "",
                "pages": "250--255",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ratnaparkhi et al., 1994] Adwait Ratnaparkhi, Jeff Reynar, and Salim Roukos. 1994. A Maxi- mum Entropy Model for Prepositional Phrase Attachment. In Proceedings of the Human Language Technology Workshop (ARP, 1994), pages 250-255.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "% Sentence AccuracY47. 51% I Table 11: Performance of Specialized Model on Unseen Test Data",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Weischedel",
                        "suffix": ""
                    }
                ],
                "year": 1993,
                "venue": "Coping With Ambiguity and Unknown Words through Probabilistic Models",
                "volume": "19",
                "issue": "",
                "pages": "359--382",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "T\u00b0tal W\u00b0rd Accuracy I Unkn\u00b0wn W\u00b0rd Accuracy 196.63% 85.56% Sentence AccuracY47. 51% I Table 11: Performance of Specialized Model on Unseen Test Data [Weischedel et al., 1993] Ralph Weischedel, Marie Meteer, Richard Schwartz, Lance Ramshaw, and Jeff Palmucci. 1993. Coping With Ambigu- ity and Unknown Words through Probabilistic Models. Computational Linguistics, 19(2):359- 382.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": ",..., ak} are the positive model parameters and {fl,..-,fk} are known as \"features\", where fj(h,t) E {O, 1}. Note that each parameter aj corresponds to a feature fj. Given a sequence of words {wl,..., Wn} and tags {tl,...t,~} as training data, define hi as the history available when predicting ti. The parameters {p, al ..... ak} are then chosen to maximize the likelihood of the training data using p:",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "2. Initialize i = 2(a) Initialize j = 1 3Except for features that look only at the current word, i.e., features of the form wl ----<word> and tl :<TAG>. The count of 10 was chosen by inspection of Training and Development data.",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "Generate tags for wi, given s(i-1)j as previous tag context, and append each tag to s(i-1)j to make a new sequence (c) j = j + 1, Repeat from (b) ifj _< g 3. Find N highest probability sequences generated by above loop, and set sij, 1 < j _< N, accordingly.4. i = i + 1, Repeat from (a) if i _< n 5. Return highest probability sequence, s~l",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF3": {
                "num": null,
                "text": "5The mapping from article to annotator is in the file doc/wsj .wht on the Treebank CDROM.6The single-annotator training data was obtained by extracting those articles tagged by \"maryann\" in the Treebank v.5 CDROM. This training data does not overlap with the Development and Test set used in the paper. The single-annotator Development Set is the portion of the Development Set which has also been annotated by \"maryann\". The word vocabulary and tag dictionary are the same as in the baseline experiment.",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF4": {
                "num": null,
                "text": "Figure 1: Distribution of Tags for the word \"about\" vs. Article#",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table><tr><td>Word:</td><td>the</td><td>stories</td><td>about</td><td>well-heeled</td><td>communities</td><td>and</td><td>developers</td></tr><tr><td>Tag:</td><td>DT</td><td>NNS</td><td>IN</td><td>JJ</td><td>NNS</td><td>CC</td><td>NNS</td></tr><tr><td>Position:</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td></tr></table>",
                "type_str": "table",
                "text": "Features on the current history hi",
                "html": null,
                "num": null
            },
            "TABREF1": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Features Generated From h3 (for",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table><tr><td colspan=\"3\">Testing the Model</td></tr><tr><td colspan=\"3\">The test corpus is tagged one sentence at a time.</td></tr><tr><td colspan=\"3\">The testing procedure requires a search to enumer-</td></tr><tr><td colspan=\"3\">ate the candidate tag sequences for the sentence,</td></tr><tr><td colspan=\"3\">and the tag sequence with the highest probability</td></tr><tr><td colspan=\"2\">is chosen as the answer.</td><td/></tr><tr><td colspan=\"2\">Search Algorithm</td><td/></tr><tr><td colspan=\"3\">The search algorithm, essentially a \"beam search\",</td></tr><tr><td colspan=\"3\">uses the conditional tag probability</td></tr><tr><td/><td/><td>p(h,t)</td></tr><tr><td>p(tlh)</td><td>-</td><td>p(h,t')</td></tr></table>",
                "type_str": "table",
                "text": "Features Generated From h4 (for tagging well-heeled) from Table2which occurs less than 10 times in the data is unreliable, and ignores features whose counts are less than 10. 3 While there are many smoothing algorithms which use techniques more rigorous than a simple count cutoff, they have not yet been investigated in conjunction with this tagger.",
                "html": null,
                "num": null
            },
            "TABREF5": {
                "content": "<table><tr><td>Word</td><td colspan=\"2\">~ Baseline Model Errors # Specialized Model Errors</td></tr><tr><td>that</td><td>246</td><td>207</td></tr><tr><td>up</td><td>186</td><td>169</td></tr><tr><td>about</td><td>110</td><td>120</td></tr><tr><td>out</td><td>104</td><td>97</td></tr><tr><td>more</td><td>88</td><td>89</td></tr><tr><td>down</td><td>81</td><td>84</td></tr><tr><td>off</td><td>73</td><td>78</td></tr><tr><td>as</td><td>50</td><td>38</td></tr><tr><td>much</td><td>47</td><td>40</td></tr><tr><td>chief</td><td>46</td><td>47</td></tr><tr><td>in</td><td>39</td><td>39</td></tr><tr><td>executive</td><td>37</td><td>33</td></tr><tr><td>most</td><td>23</td><td>34</td></tr><tr><td>ago</td><td>22</td><td>18</td></tr><tr><td>yen</td><td>18</td><td>17</td></tr></table>",
                "type_str": "table",
                "text": "Performance of Baseline Model with Specialized Features",
                "html": null,
                "num": null
            },
            "TABREF6": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Errors on Development Set with Baseline and Specialized Models",
                "html": null,
                "num": null
            }
        }
    }
}