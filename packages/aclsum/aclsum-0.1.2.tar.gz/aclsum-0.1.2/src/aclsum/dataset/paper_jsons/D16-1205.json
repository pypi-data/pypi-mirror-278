{
    "paper_id": "D16-1205",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:35:48.661739Z"
    },
    "title": "Representing Verbs with Rich Contexts: an Evaluation on Verb Similarity",
    "authors": [
        {
            "first": "Emmanuele",
            "middle": [],
            "last": "Chersoni",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Aix-Marseille University",
                "location": {}
            },
            "email": "emmanuelechersoni@gmail.com"
        },
        {
            "first": "Enrico",
            "middle": [],
            "last": "Santus",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "The Hong Kong Polytechnic University",
                "location": {}
            },
            "email": "esantus@gmail.com"
        },
        {
            "first": "Alessandro",
            "middle": [],
            "last": "Lenci",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Pisa",
                "location": {}
            },
            "email": "alessandro.lenci@unipi.it"
        },
        {
            "first": "Philippe",
            "middle": [],
            "last": "Blache",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Aix-Marseille University",
                "location": {}
            },
            "email": "philippe.blache@univ-amu.fr"
        },
        {
            "first": "Chu-Ren",
            "middle": [],
            "last": "Huang",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "The Hong Kong Polytechnic University",
                "location": {}
            },
            "email": "churen.huang@polyu.edu.hk"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Several studies on sentence processing suggest that the mental lexicon keeps track of the mutual expectations between words. Current DSMs, however, represent context words as separate features, thereby loosing important information for word expectations, such as word interrelations. In this paper, we present a DSM that addresses this issue by defining verb contexts as joint syntactic dependencies. We test our representation in a verb similarity task on two datasets, showing that joint contexts achieve performances comparable to single dependencies or even better. Moreover, they are able to overcome the data sparsity problem of joint feature spaces, in spite of the limited size of our training corpus.",
    "pdf_parse": {
        "paper_id": "D16-1205",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Several studies on sentence processing suggest that the mental lexicon keeps track of the mutual expectations between words. Current DSMs, however, represent context words as separate features, thereby loosing important information for word expectations, such as word interrelations. In this paper, we present a DSM that addresses this issue by defining verb contexts as joint syntactic dependencies. We test our representation in a verb similarity task on two datasets, showing that joint contexts achieve performances comparable to single dependencies or even better. Moreover, they are able to overcome the data sparsity problem of joint feature spaces, in spite of the limited size of our training corpus.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Distributional Semantic Models (DSMs) rely on the Distributional Hypothesis (Harris, 1954; Sahlgren, 2008) , stating that words occurring in similar contexts have similar meanings. On such theoretical grounds, word co-occurrences extracted from corpora are used to build semantic representations in the form of vectors, which have become very popular in the NLP community. Proximity between word vectors is taken as an index of meaning similarity, and vector cosine is generally adopted to measure such proximity, even though other measures have been proposed (Weeds et al., 2004; Santus et al., 2016) .",
                "cite_spans": [
                    {
                        "start": 76,
                        "end": 90,
                        "text": "(Harris, 1954;",
                        "ref_id": null
                    },
                    {
                        "start": 91,
                        "end": 106,
                        "text": "Sahlgren, 2008)",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 560,
                        "end": 580,
                        "text": "(Weeds et al., 2004;",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 581,
                        "end": 601,
                        "text": "Santus et al., 2016)",
                        "ref_id": "BIBREF23"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Most of DSMs adopt a bag-of-words approach, that is they turn a text span (i.e., a word window or a parsed sentence) into a set of words and they register separately the co-occurrence of each word with a given target. The problem with this approach is that valuable information concerning word interrelations in a context gets lost, because words co-occurring with a target are treated as independent features. This is why works like Ruiz-Casado et al. (2005) , Agirre et al. (2009) and Melamud et al. (2014) proposed to introduce richer contexts in distributional spaces, by using entire word windows as features. These richer contexts proved to be helpful to semantically represent verbs, which are characterized by highly context-sensitive meanings, and complex argument structures. In fact, two verbs may share independent words as features despite being very dissimilar from the semantic point of view. For instance kill and heal share the same object nouns in The doctor healed the patient and the The poison killed the patient, but are highly different if we consider their joint dependencies as a single context. Nonetheless, richer contexts like these suffer from data sparsity, therefore requiring either larger corpora or complex smoothing processes.",
                "cite_spans": [
                    {
                        "start": 434,
                        "end": 459,
                        "text": "Ruiz-Casado et al. (2005)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 462,
                        "end": 482,
                        "text": "Agirre et al. (2009)",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 487,
                        "end": 508,
                        "text": "Melamud et al. (2014)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this paper, we propose a syntactically savvy notion of joint contexts. To test our representation, we implement several DSMs and we evaluate them in a verb similarity task on two datasets. The results show that, even using a relatively small corpus, our syntactic joint contexts are robust with respect to data sparseness and perform similarly or better than single dependencies in a wider range of parameter settings.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The paper is organized as follows. In Section 2, we provide psycholinguistic and computational background for this research, describing recent models based on word windows. In Section 3, we describe our reinterpretation of joint contexts with syntactic dependencies. Evaluation settings and results are presented in Section 4.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "A number of studies in sentence processing suggests that verbs activate expectations on their typical argument nouns and vice versa (McRae et al., 1998; McRae et al., 2005) and nouns do the same with other nouns occurring as co-arguments in the same events (Hare et al., 2009; Bicknell et al., 2010) . Experimental subjects seem to exploit a rich event knowledge to activate or inhibit dynamically the representations of the potential arguments. This phenomenon, generally referred to as thematic fit (McRae et al., 1998) , supports the idea of a mental lexicon arranged as a web of mutual expectations. Some past works in computational linguistics (Baroni and Lenci, 2010; Lenci, 2011; Sayeed and Demberg, 2014; Greenberg et al., 2015) modeled thematic fit estimations by means of dependencybased or of thematic roles-based DSMs. However, these semantic spaces are built similarly to traditional DSMs as they split verb arguments into separate vector dimensions. By using syntactic-semantic links, they encode the relation between an event and each of its participants, but they do not encode directly the relation between participants co-occurring in the same event.",
                "cite_spans": [
                    {
                        "start": 132,
                        "end": 152,
                        "text": "(McRae et al., 1998;",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 153,
                        "end": 172,
                        "text": "McRae et al., 2005)",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 257,
                        "end": 276,
                        "text": "(Hare et al., 2009;",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 277,
                        "end": 299,
                        "text": "Bicknell et al., 2010)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 501,
                        "end": 521,
                        "text": "(McRae et al., 1998)",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 649,
                        "end": 673,
                        "text": "(Baroni and Lenci, 2010;",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 674,
                        "end": 686,
                        "text": "Lenci, 2011;",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 687,
                        "end": 712,
                        "text": "Sayeed and Demberg, 2014;",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 713,
                        "end": 736,
                        "text": "Greenberg et al., 2015)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "Another trend of studies in the NLP community aimed at the introduction of richer contextual features in DSMs, mostly based on word windows. The first example was the composite-feature model by Ruiz-Casado et al. (2005) , who extracted word windows through a Web Search engine. A composite feature for the target word watches is Alicia always ____ romantic movies, extracted from the sentence I heard that Alicia always watches romantic movies with Antony (the placeholder represents the target position). Thanks to this approach, Ruiz-Casado and colleagues achieved 82.50 in the TOEFL synonym detection test, outperforming Latent Semantic Analysis (LSA; see Landauer et al. (1998) ) and several other methods. Agirre et al. (2009) adopted an analogous approach, relying on a huge learning corpus (1.6 Teraword) to build composite-feature vectors. Their model outperformed a traditional DSM on the similarity subset of the WordSim-353 test set (Finkelstein et al., 2001) . Melamud et al. (2014) introduced a probabilistic similarity scheme for modeling the so-called joint context. By making use of the Kneser-Ney language model (Kneser and Ney, 1995) and of a probabilistic distributional measure, they were able to overcome data sparsity, outperforming a wide variety of DSMs on two similarity tasks, evaluated on Verb-Sim (Yang and Powers, 2006) and on a set of 1,000 verbs extracted from WordNet (Fellbaum, 1998) . On the basis of their results, the authors claimed that composite-feature models are particularly advantageous for measuring verb similarity.",
                "cite_spans": [
                    {
                        "start": 194,
                        "end": 219,
                        "text": "Ruiz-Casado et al. (2005)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 659,
                        "end": 681,
                        "text": "Landauer et al. (1998)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 711,
                        "end": 731,
                        "text": "Agirre et al. (2009)",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 944,
                        "end": 970,
                        "text": "(Finkelstein et al., 2001)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 973,
                        "end": 994,
                        "text": "Melamud et al. (2014)",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 1129,
                        "end": 1151,
                        "text": "(Kneser and Ney, 1995)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 1325,
                        "end": 1348,
                        "text": "(Yang and Powers, 2006)",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 1400,
                        "end": 1416,
                        "text": "(Fellbaum, 1998)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "A joint context, as defined in Melamud et al. (2014) , is a word window of order n around a target word. The target is replaced by a placeholder, and the value of the feature for a word w is the probability of w to fill the placeholder position. Assuming n=3, a word like love would be represented by a collection of contexts such as the new students ____ the school campus. Such representation introduces data sparseness, which has been addressed by previous studies either by adopting huge corpora or by relying on ngram language models to approximate the probabilities of long sequences of words.",
                "cite_spans": [
                    {
                        "start": 31,
                        "end": 52,
                        "text": "Melamud et al. (2014)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Syntactic joint contexts",
                "sec_num": "3"
            },
            {
                "text": "However, features based on word windows do not guarantee to include all the most salient event participants. Moreover, they could include unrelated words, also differentiating contexts describing the same event (e.g. consider Luis ____ the red ball and Luis ____ the blue ball).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Syntactic joint contexts",
                "sec_num": "3"
            },
            {
                "text": "For these reasons, we introduce the notion of syntactic joint contexts, further abstracting from linear word windows by using dependencies. Each feature of the word vector, in our view, should correspond to a typical verb-argument combination, as an approx-imation to our knowledge about typical event participants. In the present study, we are focusing on verbs because verb meaning is highly context sensitive and include information about complex argument configurations. Therefore, verb representation should benefit more from the introduction of joint features (Melamud et al., 2014) .",
                "cite_spans": [
                    {
                        "start": 566,
                        "end": 588,
                        "text": "(Melamud et al., 2014)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Syntactic joint contexts",
                "sec_num": "3"
            },
            {
                "text": "The procedure for defining of our representations is the following:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Syntactic joint contexts",
                "sec_num": "3"
            },
            {
                "text": "\u2022 we extract a list of verb-argument dependencies from a parsed corpus, and for each target verb we extract all the direct dependencies from the sentence of occurrence. For instance, in Finally, the dictator acknowledged his failure, we will have: target = 'acknowledge-v'; subject = 'dictator-n'; and object = 'failure-n'.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Syntactic joint contexts",
                "sec_num": "3"
            },
            {
                "text": "\u2022 for each sentence, we generate a joint context feature by joining all the dependencies for the grammatical relations of interest. From the example above, we would generate the feature dictator-n.subj+____+failure-n.obj.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Syntactic joint contexts",
                "sec_num": "3"
            },
            {
                "text": "For our experiments, the grammatical relations that we used are subject, object and complement, where complement is a generic relation grouping together all dependencies introduced by a preposition. Our distributional representation for a target word is a vector of syntatic joint contexts. For instance, the word vector for the verb to begin would include features like {jury-n.subj+____+deliberation-n.obj, operation-n.subj+____+on-i_thursday-n.comp, recruit-n.subj+____+training-n.obj+on-i_streetn.comp ...}. The value of each joint feature will be the frequency of occurrence of the target verb with the corresponding argument combination, possibly weighted by some statistical association measure.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Syntactic joint contexts",
                "sec_num": "3"
            },
            {
                "text": "We trained our DSMs on the RCV1 corpus, which contains approximately 150 million words (Lewis et al., 2004) . The corpus was tagged with the tagger described in Dell'Orletta (2009) and dependencyparsed with DeSR (Attardi et al., 2009) . RCV1 was chosen for two reasons: i) to show that our joint context-based representation can deal with data sparseness even with a training corpus of limited size; ii) to allow a comparison with the results reported by Melamud et al. (2014) .",
                "cite_spans": [
                    {
                        "start": 87,
                        "end": 107,
                        "text": "(Lewis et al., 2004)",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 161,
                        "end": 180,
                        "text": "Dell'Orletta (2009)",
                        "ref_id": null
                    },
                    {
                        "start": 212,
                        "end": 234,
                        "text": "(Attardi et al., 2009)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 455,
                        "end": 476,
                        "text": "Melamud et al. (2014)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Corpus and DSMs",
                "sec_num": "4.1"
            },
            {
                "text": "All DSMs adopt Positive Pointwise Mutual Information (PPMI; Church and Hanks (1990) ) as a context weighting scheme and vary according to three main parameters: i) type of contexts; ii) number of dimensions; iii) application of Singular Value Decomposition (SVD; see Landauer et al. (1998)) .",
                "cite_spans": [
                    {
                        "start": 60,
                        "end": 83,
                        "text": "Church and Hanks (1990)",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 267,
                        "end": 290,
                        "text": "Landauer et al. (1998))",
                        "ref_id": "BIBREF14"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Corpus and DSMs",
                "sec_num": "4.1"
            },
            {
                "text": "For what concerns the first parameter, we developed three types of DSMs: a) traditional bag-ofwords DSMs, where the features are content words co-occurring with the target in a window of width 2; b) dependency-based DSMs, where the features are words in a direct dependency relation with the target; c) joint context-based DSMs, using the joint features described in the previous section. The second parameter refers instead to the number of contexts that have been used as vector dimensions. Several values were explored (i.e. 10K, 50K and 100K), selecting the contexts according to their frequency. Finally, the third parameter concerns the application of SVD to reduce the matrix. We report only the results for a number k of latent dimensions ranging from 200 to 400, since the performance drops significantly out of this interval.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Corpus and DSMs",
                "sec_num": "4.1"
            },
            {
                "text": "As a similarity measure, we used vector cosine, which is by far the most popular in the existing literature (Turney et al., 2010) . Melamud et al. (2014) have proposed the Probabilistic Distributional Similarity (PDS), based on the intuition that two words, w 1 and w 2 , are similar if they are likely to occur in each other's contexts. PDS assigns a high similarity score when both p(w 1 | contexts of w 2 ) and p(w 2 | contexts of w 1 ) are high. We tried to test variations of this measure with our representation, but we were not able to achieve satisfying results. Therefore, we report here only the scores with the cosine.",
                "cite_spans": [
                    {
                        "start": 108,
                        "end": 129,
                        "text": "(Turney et al., 2010)",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 132,
                        "end": 153,
                        "text": "Melamud et al. (2014)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Similarity Measures",
                "sec_num": "4.2"
            },
            {
                "text": "The DSMs are evaluated on two test sets: Verb-Sim (Yang and Powers, 2006) and the verb subset of SimLex-999 (Hill et al., 2015) . The former includes 130 verb pairs, while the latter includes 222 verb pairs. Both datasets are annotated with similarity judgements, so we measured the Spearman correlation between them and the scores assigned by the model. The VerbSim dataset allows for comparison with Melamud et al. (2014) , since they also evaluated their model on this test set, achieving a Spearman correlation score of 0.616 and outperforming all the baseline methods.",
                "cite_spans": [
                    {
                        "start": 50,
                        "end": 73,
                        "text": "(Yang and Powers, 2006)",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 108,
                        "end": 127,
                        "text": "(Hill et al., 2015)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 402,
                        "end": 423,
                        "text": "Melamud et al. (2014)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Datasets",
                "sec_num": "4.3"
            },
            {
                "text": "The verb subset of SimLex-999, at the best of our knowledge, has never been used as a benchmark dataset for verb similarity. The SimLex dataset is known for being quite challenging: as reported by Hill et al. (2015) , the average performances of similarity models on this dataset are much lower than on alternative benchmarks like WordSim (Finkelstein et al., 2001) and MEN (Bruni et al., 2014) .",
                "cite_spans": [
                    {
                        "start": 197,
                        "end": 215,
                        "text": "Hill et al. (2015)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 339,
                        "end": 365,
                        "text": "(Finkelstein et al., 2001)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 374,
                        "end": 394,
                        "text": "(Bruni et al., 2014)",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Datasets",
                "sec_num": "4.3"
            },
            {
                "text": "We exclude from the evaluation datasets all the target words occurring less than 100 times in our corpus. Consequently, we cover 107 pairs in the VerbSim dataset (82.3, the same of Melamud et al. (2014) ) and 214 pairs in the SimLex verbs dataset (96.3).",
                "cite_spans": [
                    {
                        "start": 181,
                        "end": 202,
                        "text": "Melamud et al. (2014)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Datasets",
                "sec_num": "4.3"
            },
            {
                "text": "Table 1 reports the Spearman correlation scores for the vector cosine on our DSMs. At a glance, we can notice the discrepancy between the results obtained in the two datasets, as SimLex verbs confirms to be very difficult to model. We can also recognize a trend related to the number of contexts, as the performance tends to improve when more contexts are taken into account (with some exceptions). Single dependencies and joint contexts perform very similarly, and no one has a clear edge on the other. Both of them outperform the bag-of-words model on the VerbSim dataset by a nice margin, whereas the scores of all the model types are pretty much the same on SimLex verbs. Finally, it is noteworthy that the score obtained on VerbSim by the joint context model with 100K dimensions goes very close to the result reported by Melamud et al. (2014) (0.616) .",
                "cite_spans": [
                    {
                        "start": 827,
                        "end": 856,
                        "text": "Melamud et al. (2014) (0.616)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "1",
                        "ref_id": "TABREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "4.4"
            },
            {
                "text": "Table 2 and Table 3 report the results of the models with SVD reduction. Independently of the number of dimensions k, the joint contexts almost always outperform the other model types. Overall, the performance of the joint contexts seems to be more stable across several parameter configurations, whereas bag-of-words and single dependencies are subject to bigger drops. Exceptions can be noticed only for the VerbSim dataset, and only with a low number of dimensions. Finally, the correlation coefficients for the two datasets seem to follow different trends, as the models with a higher number of contexts perform better on SimLex verbs, while the opposite is true for the VerbSim dataset.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "2",
                        "ref_id": "TABREF1"
                    },
                    {
                        "start": 18,
                        "end": 19,
                        "text": "3",
                        "ref_id": "TABREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "4.4"
            },
            {
                "text": "On the VerbSim dataset, both single dependencies and joint contexts have again a clear advantage over bag-of-words representations Although they achieve a similar performance with 10K contexts, the correlation scores of the former decrease more quickly as the number of contexts increases, while the latter are more stable. Moreover, joint contexts are able to outperform single dependencies. On SimLex verbs, all the models are very close and -differently from the previous dataset -the higherdimensional DSMs are the better performing ones. Though differences are not statistically significant, joint context are able to achieve top scores over the other models. 1More in general, the best results are obtained with SVD reduction and k=200. The joint context-based DSM with 10K dimensions and k = 200 achieves 0.65, which is above the result of Melamud et al. (2014) , although the difference between the two correlation scores is not significant. As for SimLex verbs, the best result (0.283) is obtained by the joint context DSM with 100K dimensions and k = 200. ",
                "cite_spans": [
                    {
                        "start": 847,
                        "end": 868,
                        "text": "Melamud et al. (2014)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "4.4"
            },
            {
                "text": "In this paper, we have presented our proposal for a new type of vector representation based on joint features, which should emulate more closely the general knowledge about event participants that seems to be the organizing principle of our mental lexicon. A core issue of previous studies was the data sparseness challenge, and we coped with it by means of a more abstract, syntactic notion of joint context. The models using joint dependencies were able at least to perform comparably to traditional, dependency-based DSMs. In our experiments, they even achieved the best correlation scores across several parameter settings, especially after the application of SVD. We want to emphasize that previous works such as Agirre et al. (2009) already showed that large word windows can have a higher discriminative power than indipendent features, but they did it by using a huge training corpus. In our study, joint context-based representations derived from a small corpus such as RCV1 are already showing competitive performances. This result strengthens our belief that dependencies are a possible solution for the data sparsity problem of joint feature spaces.",
                "cite_spans": [
                    {
                        "start": 718,
                        "end": 738,
                        "text": "Agirre et al. (2009)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusions",
                "sec_num": "4.5"
            },
            {
                "text": "We also believe that verb similarity might not be the best task to show the usefulness of joint contexts for semantic representation. The main goal of the present paper was to show that joint contexts are a viable option to exploit the full potential of distributional information. Our successful tests on verb similarity prove that syntactic joint contexts do not suffer of data sparsity and are also able to beat other types of representations based on independent word features. Moreover, syntactic joint contexts are much simpler and more competitive with respect to window-based ones. The good performance in the verb similarity task motivates us to further test syntactic joint contexts on a larger range of tasks, such as word sense disambiguation, textual entailment and classification of semantic relations, so that they can unleash their full potential. Moreover, our proposal opens interesting perspectives for computational psycholinguistics, especially for modeling those semantic phenomena that are inherently related to the activation of event knowledge (e.g. thematic fit).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusions",
                "sec_num": "4.5"
            },
            {
                "text": "p-values computed with Fisher's r-to-z transformation comparing correlation coefficients between the joint context-DSMs and the other models on the same parameter settings.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "This paper is partially supported by HK PhD Fellowship Scheme, under PF12-13656. Emmanuele Chersoni's research is funded by a grant of the University Foundation A*MIDEX.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgments",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "A study on similarity and relatedness using distributional and wordnet-based approaches",
                "authors": [
                    {
                        "first": "Eneko",
                        "middle": [],
                        "last": "Agirre",
                        "suffix": ""
                    },
                    {
                        "first": "Enrique",
                        "middle": [],
                        "last": "Alfonseca",
                        "suffix": ""
                    },
                    {
                        "first": "Keith",
                        "middle": [],
                        "last": "Hall",
                        "suffix": ""
                    },
                    {
                        "first": "Jana",
                        "middle": [],
                        "last": "Kravalova",
                        "suffix": ""
                    },
                    {
                        "first": "Marius",
                        "middle": [],
                        "last": "Pa\u015fca",
                        "suffix": ""
                    },
                    {
                        "first": "Aitor",
                        "middle": [],
                        "last": "Soroa",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of the 2009 conference of the NAACL-HLT",
                "volume": "",
                "issue": "",
                "pages": "19--27",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Pa\u015fca, and Aitor Soroa. 2009. A study on similarity and relatedness using distributional and wordnet-based approaches. In Proceedings of the 2009 conference of the NAACL-HLT, pages 19-27. As- sociation for Computational Linguistics.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Accurate dependency parsing with a stacked multilayer perceptron",
                "authors": [
                    {
                        "first": "Giuseppe",
                        "middle": [],
                        "last": "Attardi",
                        "suffix": ""
                    },
                    {
                        "first": "Felice",
                        "middle": [],
                        "last": "Dell'orletta",
                        "suffix": ""
                    },
                    {
                        "first": "Maria",
                        "middle": [],
                        "last": "Simi",
                        "suffix": ""
                    },
                    {
                        "first": "Joseph",
                        "middle": [],
                        "last": "Turian",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of EVALITA",
                "volume": "9",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Giuseppe Attardi, Felice Dell'Orletta, Maria Simi, and Joseph Turian. 2009. Accurate dependency parsing with a stacked multilayer perceptron. In Proceedings of EVALITA, 9.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Distributional memory: A general framework for corpus-based semantics",
                "authors": [
                    {
                        "first": "Marco",
                        "middle": [],
                        "last": "Baroni",
                        "suffix": ""
                    },
                    {
                        "first": "Alessandro",
                        "middle": [],
                        "last": "Lenci",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Computational Linguistics",
                "volume": "36",
                "issue": "4",
                "pages": "673--721",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Marco Baroni and Alessandro Lenci. 2010. Distribu- tional memory: A general framework for corpus-based semantics. Computational Linguistics, 36(4):673- 721.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Effects of event knowledge in processing verbal arguments",
                "authors": [
                    {
                        "first": "Klinton",
                        "middle": [],
                        "last": "Bicknell",
                        "suffix": ""
                    },
                    {
                        "first": "Mary",
                        "middle": [],
                        "last": "Jeffrey L Elman",
                        "suffix": ""
                    },
                    {
                        "first": "Ken",
                        "middle": [],
                        "last": "Hare",
                        "suffix": ""
                    },
                    {
                        "first": "Marta",
                        "middle": [],
                        "last": "Mcrae",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Kutas",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Journal of Memory and Language",
                "volume": "63",
                "issue": "4",
                "pages": "489--505",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Klinton Bicknell, Jeffrey L Elman, Mary Hare, Ken McRae, and Marta Kutas. 2010. Effects of event knowledge in processing verbal arguments. Journal of Memory and Language, 63(4):489-505.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Multimodal distributional semantics",
                "authors": [
                    {
                        "first": "Elia",
                        "middle": [],
                        "last": "Bruni",
                        "suffix": ""
                    },
                    {
                        "first": "Nam-Khanh",
                        "middle": [],
                        "last": "Tran",
                        "suffix": ""
                    },
                    {
                        "first": "Marco",
                        "middle": [],
                        "last": "Baroni",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "J. Artif. Intell. Res.(JAIR)",
                "volume": "49",
                "issue": "",
                "pages": "1--47",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Elia Bruni, Nam-Khanh Tran, and Marco Baroni. 2014. Multimodal distributional semantics. J. Artif. Intell. Res.(JAIR), 49(1-47).",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Word association norms, mutual information, and lexicography",
                "authors": [
                    {
                        "first": "Kenneth",
                        "middle": [
                            "Ward"
                        ],
                        "last": "Church",
                        "suffix": ""
                    },
                    {
                        "first": "Patrick",
                        "middle": [],
                        "last": "Hanks",
                        "suffix": ""
                    }
                ],
                "year": 1990,
                "venue": "Computational linguistics",
                "volume": "16",
                "issue": "1",
                "pages": "22--29",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kenneth Ward Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicogra- phy. Computational linguistics, 16(1):22-29.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Ensemble system for part-ofspeech tagging",
                "authors": [
                    {
                        "first": "Felice",
                        "middle": [],
                        "last": "Dell",
                        "suffix": ""
                    },
                    {
                        "first": "'",
                        "middle": [],
                        "last": "Orletta",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of EVALITA",
                "volume": "9",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Felice Dell'Orletta. 2009. Ensemble system for part-of- speech tagging. In Proceedings of EVALITA, 9.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Placing search in context: The concept revisited",
                "authors": [
                    {
                        "first": "Lev",
                        "middle": [],
                        "last": "Finkelstein",
                        "suffix": ""
                    },
                    {
                        "first": "Evgeniy",
                        "middle": [],
                        "last": "Gabrilovich",
                        "suffix": ""
                    },
                    {
                        "first": "Yossi",
                        "middle": [],
                        "last": "Matias",
                        "suffix": ""
                    },
                    {
                        "first": "Ehud",
                        "middle": [],
                        "last": "Rivlin",
                        "suffix": ""
                    },
                    {
                        "first": "Zach",
                        "middle": [],
                        "last": "Solan",
                        "suffix": ""
                    },
                    {
                        "first": "Gadi",
                        "middle": [],
                        "last": "Wolfman",
                        "suffix": ""
                    },
                    {
                        "first": "Eytan",
                        "middle": [],
                        "last": "Ruppin",
                        "suffix": ""
                    }
                ],
                "year": 2001,
                "venue": "Proceedings of the 10th international conference on World Wide Web",
                "volume": "",
                "issue": "",
                "pages": "406--414",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2001. Placing search in context: The con- cept revisited. In Proceedings of the 10th international conference on World Wide Web, pages 406-414. ACM.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Improving unsupervised vector-space thematic fit evaluation via role-filler prototype clustering",
                "authors": [
                    {
                        "first": "Clayton",
                        "middle": [],
                        "last": "Greenberg",
                        "suffix": ""
                    },
                    {
                        "first": "Asad",
                        "middle": [],
                        "last": "Sayeed",
                        "suffix": ""
                    },
                    {
                        "first": "Vera",
                        "middle": [],
                        "last": "Demberg",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the 2015 conference of the NAACL-HLT",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Clayton Greenberg, Asad Sayeed, and Vera Demberg. 2015. Improving unsupervised vector-space thematic fit evaluation via role-filler prototype clustering. In Proceedings of the 2015 conference of the NAACL- HLT, Denver, USA.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Activating event knowledge",
                "authors": [
                    {
                        "first": "Mary",
                        "middle": [],
                        "last": "Hare",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Jones",
                        "suffix": ""
                    },
                    {
                        "first": "Caroline",
                        "middle": [],
                        "last": "Thomson",
                        "suffix": ""
                    },
                    {
                        "first": "Sarah",
                        "middle": [],
                        "last": "Kelly",
                        "suffix": ""
                    },
                    {
                        "first": "Ken",
                        "middle": [],
                        "last": "Mcrae",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Cognition",
                "volume": "111",
                "issue": "2",
                "pages": "151--167",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mary Hare, Michael Jones, Caroline Thomson, Sarah Kelly, and Ken McRae. 2009. Activating event knowl- edge. Cognition, 111(2):151-167.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation",
                "authors": [
                    {
                        "first": "Felix",
                        "middle": [],
                        "last": "Hill",
                        "suffix": ""
                    },
                    {
                        "first": "Roi",
                        "middle": [],
                        "last": "Reichart",
                        "suffix": ""
                    },
                    {
                        "first": "Anna",
                        "middle": [],
                        "last": "Korhonen",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Felix Hill, Roi Reichart, and Anna Korhonen. 2015. Simlex-999: Evaluating semantic models with (gen- uine) similarity estimation. Computational Linguis- tics.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Improved backing-off for m-gram language modeling",
                "authors": [
                    {
                        "first": "Reinhard",
                        "middle": [],
                        "last": "Kneser",
                        "suffix": ""
                    },
                    {
                        "first": "Hermann",
                        "middle": [],
                        "last": "Ney",
                        "suffix": ""
                    }
                ],
                "year": 1995,
                "venue": "Acoustics, Speech, and Signal Processing",
                "volume": "1",
                "issue": "",
                "pages": "181--184",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. In Acous- tics, Speech, and Signal Processing, 1995. ICASSP- 95., 1995 International Conference on, volume 1, pages 181-184. IEEE.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "An introduction to latent semantic analysis",
                "authors": [
                    {
                        "first": "Peter",
                        "middle": [
                            "W"
                        ],
                        "last": "Thomas K Landauer",
                        "suffix": ""
                    },
                    {
                        "first": "Darrell",
                        "middle": [],
                        "last": "Foltz",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Laham",
                        "suffix": ""
                    }
                ],
                "year": 1998,
                "venue": "Discourse processes",
                "volume": "25",
                "issue": "2-3",
                "pages": "259--284",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Thomas K Landauer, Peter W Foltz, and Darrell Laham. 1998. An introduction to latent semantic analysis. Discourse processes, 25(2-3):259-284.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Composing and updating verb argument expectations: A distributional semantic model",
                "authors": [
                    {
                        "first": "Alessandro",
                        "middle": [],
                        "last": "Lenci",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "58--66",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alessandro Lenci. 2011. Composing and updating verb argument expectations: A distributional semantic model. In Proceedings of the 2nd Workshop on Cog- nitive Modeling and Computational Linguistics, pages 58-66. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Rcv1: A new benchmark collection for text categorization research",
                "authors": [
                    {
                        "first": "Yiming",
                        "middle": [],
                        "last": "David D Lewis",
                        "suffix": ""
                    },
                    {
                        "first": "Tony",
                        "middle": [
                            "G"
                        ],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Fan",
                        "middle": [],
                        "last": "Rose",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "The Journal of Machine Learning Research",
                "volume": "5",
                "issue": "",
                "pages": "361--397",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "David D Lewis, Yiming Yang, Tony G Rose, and Fan Li. 2004. Rcv1: A new benchmark collection for text cat- egorization research. The Journal of Machine Learn- ing Research, 5:361-397.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Modeling the influence of thematic fit (and other constraints) in on-line sentence comprehension",
                "authors": [
                    {
                        "first": "Ken",
                        "middle": [],
                        "last": "Mcrae",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [
                            "J"
                        ],
                        "last": "Spivey-Knowlton",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [
                            "K"
                        ],
                        "last": "Tanenhaus",
                        "suffix": ""
                    }
                ],
                "year": 1998,
                "venue": "Journal of Memory and Language",
                "volume": "38",
                "issue": "3",
                "pages": "283--312",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ken McRae, Michael J Spivey-Knowlton, and Michael K Tanenhaus. 1998. Modeling the influence of the- matic fit (and other constraints) in on-line sentence comprehension. Journal of Memory and Language, 38(3):283-312.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "A basis for generating expectancies for verbs from nouns",
                "authors": [
                    {
                        "first": "Ken",
                        "middle": [],
                        "last": "Mcrae",
                        "suffix": ""
                    },
                    {
                        "first": "Mary",
                        "middle": [],
                        "last": "Hare",
                        "suffix": ""
                    },
                    {
                        "first": "Todd",
                        "middle": [],
                        "last": "Jeffrey L Elman",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Ferretti",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Memory & Cognition",
                "volume": "33",
                "issue": "7",
                "pages": "1174--1184",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ken McRae, Mary Hare, Jeffrey L Elman, and Todd Fer- retti. 2005. A basis for generating expectancies for verbs from nouns. Memory & Cognition, 33(7):1174- 1184.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Probabilistic modeling of joint-context in distributional similarity",
                "authors": [
                    {
                        "first": "Oren",
                        "middle": [],
                        "last": "Melamud",
                        "suffix": ""
                    },
                    {
                        "first": "Ido",
                        "middle": [],
                        "last": "Dagan",
                        "suffix": ""
                    },
                    {
                        "first": "Jacob",
                        "middle": [],
                        "last": "Goldberger",
                        "suffix": ""
                    },
                    {
                        "first": "Idan",
                        "middle": [],
                        "last": "Szpektor",
                        "suffix": ""
                    },
                    {
                        "first": "Deniz",
                        "middle": [],
                        "last": "Yuret",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "CoNLL",
                "volume": "",
                "issue": "",
                "pages": "181--190",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Oren Melamud, Ido Dagan, Jacob Goldberger, Idan Szpektor, and Deniz Yuret. 2014. Probabilistic mod- eling of joint-context in distributional similarity. In CoNLL, pages 181-190.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Efficient estimation of word representations in vector space",
                "authors": [
                    {
                        "first": "Tomas",
                        "middle": [],
                        "last": "Mikolov",
                        "suffix": ""
                    },
                    {
                        "first": "Kai",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Greg",
                        "middle": [],
                        "last": "Corrado",
                        "suffix": ""
                    },
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Dean",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1301.3781"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representa- tions in vector space. arXiv preprint arXiv:1301.3781.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Using context-window overlapping in synonym discovery and ontology extension",
                "authors": [
                    {
                        "first": "Maria",
                        "middle": [],
                        "last": "Ruiz-Casado",
                        "suffix": ""
                    },
                    {
                        "first": "Enrique",
                        "middle": [],
                        "last": "Alfonseca",
                        "suffix": ""
                    },
                    {
                        "first": "Pablo",
                        "middle": [],
                        "last": "Castells",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of RANLP",
                "volume": "",
                "issue": "",
                "pages": "1--7",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Maria Ruiz-Casado, Enrique Alfonseca, and Pablo Castells. 2005. Using context-window overlapping in synonym discovery and ontology extension. In Pro- ceedings of RANLP, pages 1-7.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "The distributional hypothesis",
                "authors": [
                    {
                        "first": "Magnus",
                        "middle": [],
                        "last": "Sahlgren",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Italian Journal of Linguistics",
                "volume": "20",
                "issue": "1",
                "pages": "33--54",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Magnus Sahlgren. 2008. The distributional hypothesis. Italian Journal of Linguistics, 20(1):33-54.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Testing APSyn against Vector Cosine on Similarity Estimation",
                "authors": [
                    {
                        "first": "Enrico",
                        "middle": [],
                        "last": "Santus",
                        "suffix": ""
                    },
                    {
                        "first": "Emmanuele",
                        "middle": [],
                        "last": "Chersoni",
                        "suffix": ""
                    },
                    {
                        "first": "Alessandro",
                        "middle": [],
                        "last": "Lenci",
                        "suffix": ""
                    },
                    {
                        "first": "Chu-Ren",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Philippe",
                        "middle": [],
                        "last": "Blache",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the Pacific Asia Conference on Language, Information and Computing",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Enrico Santus, Emmanuele Chersoni, Alessandro Lenci, Chu-Ren Huang, and Philippe Blache. 2016. Testing APSyn against Vector Cosine on Similarity Estima- tion. In Proceedings of the Pacific Asia Conference on Language, Information and Computing (PACLIC 30).",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Combining unsupervised syntactic and semantic models of thematic fit",
                "authors": [
                    {
                        "first": "Asad",
                        "middle": [],
                        "last": "Sayeed",
                        "suffix": ""
                    },
                    {
                        "first": "Vera",
                        "middle": [],
                        "last": "Demberg",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of the first Italian Conference on Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Asad Sayeed and Vera Demberg. 2014. Combining un- supervised syntactic and semantic models of thematic fit. In Proceedings of the first Italian Conference on Computational Linguistics (CLiC-it 2014).",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "From frequency to meaning: Vector space models of semantics",
                "authors": [
                    {
                        "first": "Patrick",
                        "middle": [],
                        "last": "Peter D Turney",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Pantel",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Journal of artificial intelligence research",
                "volume": "37",
                "issue": "1",
                "pages": "141--188",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Peter D Turney, Patrick Pantel, et al. 2010. From fre- quency to meaning: Vector space models of semantics. Journal of artificial intelligence research, 37(1):141- 188.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Characterising measures of lexical distributional similarity",
                "authors": [
                    {
                        "first": "Julie",
                        "middle": [],
                        "last": "Weeds",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Weir",
                        "suffix": ""
                    },
                    {
                        "first": "Diana",
                        "middle": [],
                        "last": "Mccarthy",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proceedings of the 20th international conference on Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Julie Weeds, David Weir, and Diana McCarthy. 2004. Characterising measures of lexical distributional simi- larity. In Proceedings of the 20th international confer- ence on Computational Linguistics, page 1015. Asso- ciation for Computational Linguistics.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Verb similarity on the taxonomy of WordNet",
                "authors": [
                    {
                        "first": "Dongqiang",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "David Mw",
                        "middle": [],
                        "last": "Powers",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dongqiang Yang and David MW Powers. 2006. Verb similarity on the taxonomy of WordNet. Masaryk Uni- versity.",
                "links": null
            }
        },
        "ref_entries": {
            "TABREF0": {
                "content": "<table><tr><td>Model Bag-of-Words-10K Single -10k Joint -10k Bag-of-Words-50K Single -50k Joint -50k Bag-of-Words-100K Single -100k Joint -100k</td><td>VerbSim SimLex verbs 0.385 0.085 0.561 0.090 0.568 0.105 0.478 0.095 0.592 0.115 0.592 0.105 0.488 0.114 0.587 0.132 0.607 0.114</td></tr></table>",
                "type_str": "table",
                "text": "Spearman correlation scores for VerbSim and for the verb subset of SimLex-999. Each model is identified by the type and by the number of features of the semantic space.",
                "html": null,
                "num": null
            },
            "TABREF1": {
                "content": "<table><tr><td>Model Bag-of-Words-10K Single -10k Joint -10k Bag-of-Words-50K Single -50k Joint -50k Bag-of-Words-100K Single -100k Joint -100k</td><td>k = 200 k = 300 k = 400 0.127 0.113 0.111 0.168 0.172 0.165 0.190 0.177 0.181 0.196 0.191 0.21 0.218 0.228 0.222 0.256 0.250 0.227 0.222 0.18 0.16 0.225 0.218 0.199 0.283 0.256 0.222</td></tr></table>",
                "type_str": "table",
                "text": "Spearman correlation scores for VerbSim, after the application of SVD with different values of k.",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Spearman correlation scores for the verb subset of SimLex-999, after the application of SVD with different values of k.",
                "html": null,
                "num": null
            }
        }
    }
}