{
    "paper_id": "N19-1257",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:32:35.920610Z"
    },
    "title": "Zero-Shot Cross-Lingual Opinion Target Extraction",
    "authors": [
        {
            "first": "Soufian",
            "middle": [],
            "last": "Jebbara",
            "suffix": "",
            "affiliation": {},
            "email": "soufian.jebbara@semalytix.com"
        },
        {
            "first": "Philipp",
            "middle": [],
            "last": "Cimiano",
            "suffix": "",
            "affiliation": {},
            "email": "cimiano@cit-ec.uni-bielefeld.de"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Aspect-based sentiment analysis involves the recognition of so called opinion target expressions (OTEs). To automatically extract OTEs, supervised learning algorithms are usually employed which are trained on manually annotated corpora. The creation of these corpora is labor-intensive and sufficiently large datasets are therefore usually only available for a very narrow selection of languages and domains. In this work, we address the lack of available annotated data for specific languages by proposing a zero-shot cross-lingual approach for the extraction of opinion target expressions. We leverage multilingual word embeddings that share a common vector space across various languages and incorporate these into a convolutional neural network architecture for OTE extraction. Our experiments with 5 languages give promising results: We can successfully train a model on annotated data of a source language and perform accurate prediction on a target language without ever using any annotated samples in that target language. Depending on the source and target language pairs, we reach performances in a zero-shot regime of up to 77% of a model trained on target language data. Furthermore, we can increase this performance up to 87% of a baseline model trained on target language data by performing cross-lingual learning from multiple source languages.",
    "pdf_parse": {
        "paper_id": "N19-1257",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Aspect-based sentiment analysis involves the recognition of so called opinion target expressions (OTEs). To automatically extract OTEs, supervised learning algorithms are usually employed which are trained on manually annotated corpora. The creation of these corpora is labor-intensive and sufficiently large datasets are therefore usually only available for a very narrow selection of languages and domains. In this work, we address the lack of available annotated data for specific languages by proposing a zero-shot cross-lingual approach for the extraction of opinion target expressions. We leverage multilingual word embeddings that share a common vector space across various languages and incorporate these into a convolutional neural network architecture for OTE extraction. Our experiments with 5 languages give promising results: We can successfully train a model on annotated data of a source language and perform accurate prediction on a target language without ever using any annotated samples in that target language. Depending on the source and target language pairs, we reach performances in a zero-shot regime of up to 77% of a model trained on target language data. Furthermore, we can increase this performance up to 87% of a baseline model trained on target language data by performing cross-lingual learning from multiple source languages.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "In recent years, there has been an increasing interest in developing sentiment analysis models that predict sentiment at a more fine-grained level than at the level of a complete document. A paradigm coined as Aspect-based Sentiment Analysis (ABSA) addresses this need by defining the sentiment expressed in a text relative to an opinion target (also called aspect). Consider the following example from a restaurant review:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\" Moules were excellent , lobster ravioli was VERY salty ! \"",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this example, there are two sentiment statements, one positive and one negative. The positive one is indicated by the word \"excellent\" and is expressed towards the opinion target \"Moules\". The second, negative sentiment, is indicated by the word \"salty\" and is expressed towards the \"lobster ravioli\".",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "A key task within this fine-grained sentiment analysis consists of identifying so called opinion target expressions (OTE). To automatically extract OTEs, supervised learning algorithms are usually employed which are trained on manually annotated corpora. In this paper, we are concerned with how to transfer classifiers trained on one domain to another domain. In particular, we focus on the transfer of models across languages to alleviate the need for multilingual training data. We propose a model that is capable of accurate zero-shot cross-lingual OTE extraction, thus reducing the reliance on annotated data for every language. Similar to Upadhyay et al. (2018) , our model leverages multilingual word embeddings (Smith et al., 2017; Lample et al., 2018) that share a common vector space across various languages. The shared space allows us to transfer a model trained on source language data to predict OTEs in a target language for which no (i.e. zero-shot setting) or only small amounts of data are available, thus allowing to apply our model to under-resourced languages.",
                "cite_spans": [
                    {
                        "start": 645,
                        "end": 667,
                        "text": "Upadhyay et al. (2018)",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 719,
                        "end": 739,
                        "text": "(Smith et al., 2017;",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 740,
                        "end": 760,
                        "text": "Lample et al., 2018)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Our main contributions can be summarized as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\u2022 We present the first approach for zero-shot cross-lingual opinion target extraction and achieve up to 87% of the performance of a monolingual baseline.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\u2022 We investigate the benefit of using multi-ple source languages for cross-lingual learning and show that we can improve by 6 to 8 points in F 1 -Score compared to a model trained on a single source language.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\u2022 We investigate the benefit of augmenting the zero-shot approach with additional data points from the target language. We observe that we can save hundreds of annotated data points by employing a cross-lingual approach.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\u2022 We compare two methods for obtaining cross-lingual word embeddings on the task.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "A common approach for extracting opinion target expressions is to phrase the task as a sequence tagging problem using the well-known IOB scheme (Tjong Kim Sang and Veenstra, 1999) to represent OTEs as a sequence of tags. According to this scheme, each word in our text is marked with one of three tags, namely I, O or B that indicate if the word is at the Beginning1 , Inside or Outside of a target expression. An example of such an encoding can be seen below:",
                "cite_spans": [
                    {
                        "start": 144,
                        "end": 179,
                        "text": "(Tjong Kim Sang and Veenstra, 1999)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Approach",
                "sec_num": "2"
            },
            {
                "text": "The wine list is also really nice .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Approach",
                "sec_num": "2"
            },
            {
                "text": "O I I O O O O O",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Approach",
                "sec_num": "2"
            },
            {
                "text": "By rephrasing the task in this way, we can address it using established sequence tagging models. In this work, we use a multi-layer convolutional neural network (CNN) as our sequence tagging model. The model receives a sequence of words as input features and predicts an output sequence of IOB tags. In order to keep our model simple and our results clear, we restrict our input representation to a sequence of word embeddings. While additional features such as Part-of-Speech (POS) tags are known to perform well in the domain of OTE extraction (Toh and Su, 2016; Kumar et al., 2016; Jebbara and Cimiano, 2016) , they would require a separately trained model for POS-tag prediction which can not be assumed to be available for every language. We refrain from using more complex architectures such as memory networks as our goal is mainly to investigate the possibility of performing zero-shot cross-lingual transfer learning for OTE prediction. Being the first approach proposing this, we leave the question of how to increase performance of the approach by using more complex architectures to future work.",
                "cite_spans": [
                    {
                        "start": 546,
                        "end": 564,
                        "text": "(Toh and Su, 2016;",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 565,
                        "end": 584,
                        "text": "Kumar et al., 2016;",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 585,
                        "end": 611,
                        "text": "Jebbara and Cimiano, 2016)",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Approach",
                "sec_num": "2"
            },
            {
                "text": "In the following, we describe our monolingual CNN model for OTE extraction which we use as our baseline model. Afterwards, we show how we adapt this model for a cross-lingual and even zeroshot regime.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Approach",
                "sec_num": "2"
            },
            {
                "text": "Our monolingual baseline model consists of a word embedding layer, a stack of convolution layers, a standard feed-forward layer followed by a final output layer. Formally, the word sequence w = (w 1 , . . . , w n ) is passed to the word embedding layer that maps each word w i to its embedding vector x i using an embedding matrix W. The sequence of word embedding vectors x = (x 1 , . . . , x n ) is processed by a stack of L convolutional layers2 , each with a kernel width of l conv , d conv filter maps and RELU activation function f (Nair and Hinton, 2010) . The final output of these convolution layers is a sequence of abstract representations h L = (h L 1 , . . . , h L n ) that incorporate the immediate context of each word by means of the learned convolution operations. The hidden states h L i of the last convolution layer are processed by a regular feed-forward layer to further increase the model's capacity and the resulting sequence is passed to the output layer.",
                "cite_spans": [
                    {
                        "start": 538,
                        "end": 561,
                        "text": "(Nair and Hinton, 2010)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Monolingual Model",
                "sec_num": "2.1"
            },
            {
                "text": "In a last step, each hidden state is projected to a probability distribution over all possible output tags q i = (q B i , q I i , q O i ) using a standard feedforward layer with weights W tag , bias b tag and a softmax activation function.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Monolingual Model",
                "sec_num": "2.1"
            },
            {
                "text": "Since the prediction of each tag can be interpreted as a classification, the network is trained to minimize the categorical cross-entropy between expected tag distribution p i and predicted tag distribution q i of each word i:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Monolingual Model",
                "sec_num": "2.1"
            },
            {
                "text": "H(p i , q i ) = - t\u2208T p t i log(q t i ),",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Monolingual Model",
                "sec_num": "2.1"
            },
            {
                "text": "where T = {I, O, B} is the set of IOB tags, p t i \u2208 {0, 1} is the expected probability of tag t and q t i \u2208 [0, 1] the predicted probability. Figure 1 depicts the sequence labeling architecture. ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 149,
                        "end": 150,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Monolingual Model",
                "sec_num": "2.1"
            },
            {
                "text": "Our cross-lingual model works purely with crosslingual embeddings that have been trained on monolingual datasets and in a second step have been aligned across languages. In fact, the embeddings are pre-computed in an offline fashion and are not adapted while training the convolutional network on data from a specific language. As the inputs to the convolutional network are only the cross-lingual embeddings, the network can be applied to any language for which the embeddings have been aligned.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Cross-Lingual Model",
                "sec_num": "2.2"
            },
            {
                "text": "Since the word embeddings for source and target language share a common vector space, the shared parts of the target language model are able to process data samples from the completely unseen target language and perform accurate prediction i.e. enabling zero-shot cross-lingual extraction of opinion target expressions.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Cross-Lingual Model",
                "sec_num": "2.2"
            },
            {
                "text": "We rely on two approaches to compute embeddings that are aligned across languages. Both methods rely on fastText (Bojanowski et al., 2017) to compute monolingual embeddings trained on Wikipedia articles. The first method is the one proposed by Smith et al. (2017) , which computes a singular value decomposition (SVD) on a dictionary of translated word pairs to obtain an optimal, orthogonal projection matrix from one space into the other. We refer to this method as SVD-aligned. We use these embeddings3 in our experiments in Sections 3.3, 3.4 and 3.6.",
                "cite_spans": [
                    {
                        "start": 113,
                        "end": 138,
                        "text": "(Bojanowski et al., 2017)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 244,
                        "end": 263,
                        "text": "Smith et al. (2017)",
                        "ref_id": "BIBREF14"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Cross-Lingual Model",
                "sec_num": "2.2"
            },
            {
                "text": "The second method proposed by Lample et al. (2018) performs the alignment of embeddings across languages in an unsupervised fashion, without requiring translation pairs.",
                "cite_spans": [
                    {
                        "start": 30,
                        "end": 50,
                        "text": "Lample et al. (2018)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Cross-Lingual Model",
                "sec_num": "2.2"
            },
            {
                "text": "The approach uses adversarial training to initialize the cross-lingual mapping and a synthetically generated bilingual dictionary to fine-tune it with the Procrustes algorithm (Sch\u00f6nemann, 1966) . We refer to the multilingual embeddings4 from Lample et al. (2018) as ADV-aligned. These are used in Section 3.5.",
                "cite_spans": [
                    {
                        "start": 176,
                        "end": 194,
                        "text": "(Sch\u00f6nemann, 1966)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 243,
                        "end": 263,
                        "text": "Lample et al. (2018)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Cross-Lingual Model",
                "sec_num": "2.2"
            },
            {
                "text": "In this section, we investigate the proposed zeroshot cross-lingual approach and evaluate it on the widely used dataset of Task 5 of the SemEval 2016 workshop. With our evaluation, we answer the following research questions:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation",
                "sec_num": "3"
            },
            {
                "text": "RQ1: To what degree is the model capable of performing OTE extraction for unseen languages?",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation",
                "sec_num": "3"
            },
            {
                "text": "RQ2: Is there a benefit in training on more than one source language?",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation",
                "sec_num": "3"
            },
            {
                "text": "RQ3: What improvements can be expected when a small amount of samples for the target language are available?",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation",
                "sec_num": "3"
            },
            {
                "text": "RQ4: How big is the impact of the used alignment method on the OTE extraction performance?",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation",
                "sec_num": "3"
            },
            {
                "text": "Before we answer these questions, we give a brief overview over the used datasets and resources.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation",
                "sec_num": "3"
            },
            {
                "text": "As part of Task 5 of the SemEval 2016 workshop (Pontiki et al., 2016) , a collection of datasets for aspect-based sentiment analysis on various languages and domains was published. Due to its relatively large number of samples and high coverage of languages and domains, the datasets are commonly used to evaluate ABSA approaches. To answer our research questions, we make use of a selection of the available datasets. We evaluate our cross-lingual approach on the available datasets for the restaurant domain for the 5 languages Dutch (nl), English (en), Russian (ru), Spanish (es) and Turkish (tr)5 . ",
                "cite_spans": [
                    {
                        "start": 47,
                        "end": 69,
                        "text": "(Pontiki et al., 2016)",
                        "ref_id": "BIBREF10"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Datasets",
                "sec_num": "3.1"
            },
            {
                "text": "In all our experiments, we report F 1 -scores for the extracted opinion target expressions computed on exact matches of the character spans as in the original SemEval task (Pontiki et al., 2016) .",
                "cite_spans": [
                    {
                        "start": 172,
                        "end": 194,
                        "text": "(Pontiki et al., 2016)",
                        "ref_id": "BIBREF10"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Settings",
                "sec_num": "3.2"
            },
            {
                "text": "As described in Section 2.2, our model relies on pretrained multilingual embeddings. For both SVD-aligned and ADV-aligned, we use the embeddings as provided by the original authors. However, we restrict our vocabulary to the most frequent 50,000 words per language 6 to reduce memory consumption.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Settings",
                "sec_num": "3.2"
            },
            {
                "text": "For all experiments, we fix our model architecture to 5 convolution layers with each having a kernel size of 3, a dimensionality of 300 units and a ReLU activation function (Nair and Hinton, 2010) . The penultimate feed-forward layer has 300 dimensions and a ReLU activation, as well. We apply dropout (Srivastava et al., 2014) on the word embedding layer with a rate of 0.3 and between all other layers with 0.5. The word embeddings and the penultimate layer are L1-regularized (Ng, 2004) .",
                "cite_spans": [
                    {
                        "start": 173,
                        "end": 196,
                        "text": "(Nair and Hinton, 2010)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 302,
                        "end": 327,
                        "text": "(Srivastava et al., 2014)",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 479,
                        "end": 489,
                        "text": "(Ng, 2004)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Settings",
                "sec_num": "3.2"
            },
            {
                "text": "The network's parameters are optimized using the stochastic optimization technique Adam (Kingma and Ba, 2015) . We optimize the number of training epochs for each model using early stopping (Caruana et al., 2000) but do not tune other hyperparameters of our models. We always pick 20% of our available training data for the validation process. For the zero-shot scenario, this entails that we optimize the number of epochs on the source language and not on the target language to simulate true zero-shot learning. 6 As appearing in the respective embedding files.",
                "cite_spans": [
                    {
                        "start": 88,
                        "end": 109,
                        "text": "(Kingma and Ba, 2015)",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 190,
                        "end": 212,
                        "text": "(Caruana et al., 2000)",
                        "ref_id": "BIBREF2"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Settings",
                "sec_num": "3.2"
            },
            {
                "text": "In this section, we present our evaluation for zeroshot learning. We first examine a setting with a single source language. Then, we evaluate the effect of cross-lingual learning from multiple source languages. To answer this question, we perform a set of experiments in the zero-shot setting. We train a model on the training portion of a source language and evaluate the model performance on all possible target languages. Figure 2 shows the obtained scores. The reported results are averaged over 10 runs with different random seeds. The main diagonal represents results of models both trained and tested on target language data. We considered these our monolingual baselines.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 432,
                        "end": 433,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Zero-Shot Transfer Learning",
                "sec_num": "3.3"
            },
            {
                "text": "In general, the proposed approach achieves relatively high scores for some language pairs, although with large performance differences depending on the exact source and target language pairs. Looking at the absolute scores, the best performing cross-lingual language pair is en\u2192es with an F 1 -score of 0.5. This is followed by en\u2192nl at 0.46. The lowest is es\u2192tr with an F 1 -score of 0.14. When considering the results relative to their respective monolingual baselines, the highest relative performance is achieved by en\u2192nl at 77% of a nl\u2192nl model, followed by en\u2192es and ru\u2192nl, which both reach an F-Measure of about 74%. The weakest performing language pair is still es\u2192tr at 29% relative performance. In general, the Turkish language seems to benefit the least from the cross-lingual transfer learning, while Russian is on average the best source language in terms of relative performance achievement for the target languages.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Single Source Language",
                "sec_num": null
            },
            {
                "text": "Overall, the presented results show that it is in fact possible for most considered languages to train a model for OTE extraction without ever using any annotated data in that target language. The row best\u2192target represents the best performing cross-lingual model from Figure 2 for each target language. all others\u2192target are the results for training on all languages except for the target language. target\u2192target shows the monolingual scores that act as a baseline.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 276,
                        "end": 277,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Single Source Language",
                "sec_num": null
            },
            {
                "text": "RQ2: Is there a benefit in training on more than one source language?",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Multiple Source Languages",
                "sec_num": null
            },
            {
                "text": "As we explained in Section 2.2, our approach allows us to train and test on any number of source and target languages, provided that we have aligned word embeddings for each considered language.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Multiple Source Languages",
                "sec_num": null
            },
            {
                "text": "In order to answer our second research question, we train a model on the available training data for all but one language and perform prediction on the test data for the left-out language. The results for these experiments are summarized in Table 2 . We can see that all languages with the exception of Turkish seem to profit from a cross-lingual transfer setting with multiple source languages. The absolute improvements are in the range of 6 to 8 points in F 1 -Score while the performance on Turkish samples drops by 3 points.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 247,
                        "end": 248,
                        "text": "2",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Multiple Source Languages",
                "sec_num": null
            },
            {
                "text": "We can summarize that we can obtain substantial improvements for most languages when training on a combination of multiple source languages. In fact, for en, es, nl and ru, the results of our cross-lingual models trained on all other languages reach between 78% to 87% relative performance of a model trained with target language data.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Multiple Source Languages",
                "sec_num": null
            },
            {
                "text": "Additional Target Language Data While our goal is to reduce the effort of annotating huge amounts of data in a target language to which the model is to be transferred, it might still be reasonable to provide a few annotated samples for a target language. Our next research question addresses this issue:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Cross-Lingual Transfer Learning with",
                "sec_num": "3.4"
            },
            {
                "text": "RQ3: What improvements can be expected when a small amount of samples for the target language are available?",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Cross-Lingual Transfer Learning with",
                "sec_num": "3.4"
            },
            {
                "text": "We answer this question by training our models jointly on a source language dataset as well as a small amount of target language samples and compare this to a baseline model that only uses target language samples. By gradually increasing the available target samples, we can directly observe their benefit on the test performance. Figure 3 shows a visualization for the source language en and the target languages es, nl, ru, and tr.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 338,
                        "end": 339,
                        "text": "3",
                        "ref_id": "FIGREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Cross-Lingual Transfer Learning with",
                "sec_num": "3.4"
            },
            {
                "text": "We can immediately see that a monolingual model requires at least 100 target samples to produce meaningful results as opposed to a crosslingual model that performs well with source language samples alone. Training on increasing amounts of target samples improves the model performances monotonically for each target language and the model leveraging the bilingual data consistently outperforms the monolingual baseline model. The benefits of the source language data are especially pronounced when very few target samples are available, i.e. less than 200. As an example, a model trained on bilingual data using all available English samples and 200 Dutch samples is competitive to a monolingual model trained on 1000 Dutch samples (0.55 vs. 0.56).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Cross-Lingual Transfer Learning with",
                "sec_num": "3.4"
            },
            {
                "text": "As one would expect, the results in Table 2 and Figure 3 suggest that training the model on more data samples leads to a better performance. Since our model can leverage the data from all languages simultaneously, we can exhaust our resources and train an instance of our model that has access to all training data samples from all languages, including the target training data. This is reflected by the dashed line in Figure 3 . We see, however, that the model cannot leverage the other source languages 0 2 0 5 0 1 0 0 2 0 0 5 0 0 1 0 0 0 a ll 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 F1-Score (en, es) es es es all es 0 2 0 5 0 1 0 0 2 0 0 5 0 0 1 0 0 0 a ll 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 (en, nl) nl nl nl all nl 0 2 0 5 0 1 0 0 2 0 0 5 0 0 1 0 0 0 a ll beyond what it achieves with the combination of the full target and English language data alone.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 42,
                        "end": 43,
                        "text": "2",
                        "ref_id": "TABREF1"
                    },
                    {
                        "start": 55,
                        "end": 56,
                        "text": "3",
                        "ref_id": "FIGREF2"
                    },
                    {
                        "start": 426,
                        "end": 427,
                        "text": "3",
                        "ref_id": "FIGREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Cross-Lingual Transfer Learning with",
                "sec_num": "3.4"
            },
            {
                "text": "The previous experiments show that we can achieve good performance in a cross-lingual setting for OTE extraction using the multilingual word embeddings proposed by Smith et al. (2017) . Now we address our final research question:",
                "cite_spans": [
                    {
                        "start": 164,
                        "end": 183,
                        "text": "Smith et al. (2017)",
                        "ref_id": "BIBREF14"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Comparison of Alignment Methods",
                "sec_num": "3.5"
            },
            {
                "text": "RQ4: How big is the impact of the used alignment method on the OTE extraction performance?",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Comparison of Alignment Methods",
                "sec_num": "3.5"
            },
            {
                "text": "With our final research question, we compare our previous results to an alternative method of aligning word embeddings in multiple languages. We repeat our experiments in Section 3.3 using the embeddings of Lample et al. (2018) which we refer to as ADV-aligned.",
                "cite_spans": [
                    {
                        "start": 207,
                        "end": 227,
                        "text": "Lample et al. (2018)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Comparison of Alignment Methods",
                "sec_num": "3.5"
            },
            {
                "text": "To enable a direct comparison to the zero-shot results in Section 3.3, we report absolute differences in F 1 -Score to the scores obtained with SVD-aligned for all source and target language combinations.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Comparison of Alignment Methods",
                "sec_num": "3.5"
            },
            {
                "text": "As can be seen in Figure 4 , the two methods do perform well overall, albeit different for specific language pairs. In a monolingual setting (i.e. main diagonal), ADV-aligned performs slightly worse than SVD-aligned with the exception of en\u2192en. Using ADV-aligned, Spanish appears to be a more effective source language than using SVD-aligned as the average performance is about 2.9 points higher. It can also be observed that the cross-lingual transfer learning works better for English as a target language using ADV-aligned since the average performance is about 2.2 points higher than for SVD-aligned. The opposite is true for Dutch as a target language, which shows a reduction in performance by 2.1 points on average. Overall, for 13 of the 25 language pairs, the embeddings based on SVD-aligned perform better than embeddings aligned with ADV-aligned.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 25,
                        "end": 26,
                        "text": "4",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Comparison of Alignment Methods",
                "sec_num": "3.5"
            },
            {
                "text": "In this last part of our evaluation, we want to put our work into perspective of prior systems for opinion target extraction on the SemEval 2016 restaurant datasets. We report results for our multilingual model that is trained on the combined training data of all languages and evaluated on the corresponding test datasets. We compare our model to the respective state-of-the-art for each language in Table 3 . We can see that the competition is strongest for English where we fall behind recent monolingual systems. This corresponds to rank 7 of 19 of the original SemEval competition. Regarding the other languages, we see that we are close to the best Spanish and Dutch systems and even clearly outperform systems for Russian and Turkish by at least 7 points in F 1 -score. With that, we present the first approach on this task to achieve such competitive performances for a variety of languages Table 3 : Overview of the current state-of-the-art for opinion target extraction for 5 languages. Our model is trained on the combined training data of all languages and evaluated on the respective test datasets. The row marked with * is the baseline provided by the workshop organizers. To our knowledge, no better model is published for Russian and Turkish.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 407,
                        "end": 408,
                        "text": "3",
                        "ref_id": null
                    },
                    {
                        "start": 905,
                        "end": 906,
                        "text": "3",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Comparison to State-of-the-Art",
                "sec_num": "3.6"
            },
            {
                "text": "with a single, multilingual model.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Comparison to State-of-the-Art",
                "sec_num": "3.6"
            },
            {
                "text": "The presented experiments shed light on the performance of our proposed approach under various circumstances. In the following, we want to discuss its limitations and consider explanations for performance differences of different language pairs. Model Limitations The core of our proposed sequence labeling approach consists of aligned word embeddings and shared CNN layers. Due to the limited context of a CNN layer, the model can only base its decisions for each word on the local information around that word. In many cases, this information is sufficient since most opinion target expressions are adjective-noun phrases7 which are well enough identified by the local context for most considered languages.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discussion and Future Work",
                "sec_num": "3.7"
            },
            {
                "text": "As future work, it is worth to investigate in how far our findings translate to more complex model architectures that have been proposed for OTE extraction, such as memory networks or attentionbased models.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discussion and Future Work",
                "sec_num": "3.7"
            },
            {
                "text": "Language Characteristics Due to the inherent variability of natural languages and of the used datasets, it is difficult to identify the exact reasons for the observed performance differences between language pairs. However, we suspect that language features such as word order, inflection, or agglutination affect the compatibility of languages. As an example, Turkish is considered a highly agglutinative language, that is, complex words are composed by attaching several suffixes to a word stem. This sets it apart from the other 4 languages. This language feature might present a difficulty in our approach since the appending of suffixes is not optimally reflected in the tokenization process and the used word embeddings. An approach that performs alignment of languages on subword units might alleviate this problem and lead to performance gains for language pairs with similar inflection rules.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discussion and Future Work",
                "sec_num": "3.7"
            },
            {
                "text": "Syntactic regularities such as word order might also play a role in our transfer learning approach. It is reasonable to assume that the CNN layers of our approach pick up patterns in the word order of a source language that are indicative of an opinion target expression, e.g. \"the [NOUN] is good\".",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discussion and Future Work",
                "sec_num": "3.7"
            },
            {
                "text": "When applying such a model to a target language with drastically different word order regularities, these patterns might not appear as such in the target language.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discussion and Future Work",
                "sec_num": "3.7"
            },
            {
                "text": "For the considered languages, we see following characteristics: Where English and Spanish are generally considered to follow a Subject-Verb-Object (SVO) order, Dutch largely exhibits a combination of SOV and SVO cases. Turkish and Russian are overall flexible in their word order and allow a variety of syntactic structures. In the case of Turkish, its morphological and syntactic features seem to explain some of the relatively low results. However, with the small sample of languages and the many potential influencing factors at play, we are aware that it is not possible to draw any strong conclusions. Further research has to be conducted in this direction to answer open questions.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discussion and Future Work",
                "sec_num": "3.7"
            },
            {
                "text": "Our work brings together the domains of opinion target extraction on the one side and cross lingual learning on the other side. In this section, we give a brief overview of both domains and point out parallels to previous work.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "4"
            },
            {
                "text": "Opinion Target Extraction San Vicente et al. (2015) present a system that addresses opinion target extraction as a sequence labeling problem based on a perceptron algorithm with token, word shape and clustering-based features. Toh and Wang (2014) propose a Conditional Random Field (CRF) as a sequence labeling model that includes a variety of features such as Part-of-Speech (POS) tags and dependency tree features, word clusters and features derived from the Word-Net taxonomy. The model is later improved us-ing neural network output probabilities (Toh and Su, 2016) and achieved the best results on the Se-mEval 2016 dataset for English restaurant reviews.",
                "cite_spans": [
                    {
                        "start": 227,
                        "end": 246,
                        "text": "Toh and Wang (2014)",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 551,
                        "end": 569,
                        "text": "(Toh and Su, 2016)",
                        "ref_id": "BIBREF17"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "4"
            },
            {
                "text": "Jakob and Gurevych (2010) follow a very similar approach that addresses opinion target extraction as a sequence labeling problem using CRFs. Their approach includes features derived from words, Part-of-Speech tags and dependency paths, and performs well in a single and cross-domain setting. Kumar et al. (2016) present a CRF-based model that makes use of a variety of morphological and linguistic features and is one of the few systems that submitted results for more than one language for the SemEval 2016 ABSA challenge. The strong reliance on high-level NLP features, such as dependency trees, named-entity information and WordNet features restricts its wide applicability to resource-poor languages.",
                "cite_spans": [
                    {
                        "start": 292,
                        "end": 311,
                        "text": "Kumar et al. (2016)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "4"
            },
            {
                "text": "Among neural network models Poria et al. (2016) and Jebbara and Cimiano (2016) use deep convolutional neural network (CNN) with Part-of-Speech (POS) tag features. Poria et al. (2016) also extend their base model using linguistic rules. Wang et al. (2017) use coupled multi-layer attentions to extract opinion expressions and opinion targets jointly. This approach, however, relies on additional annotations for opinion expressions alongside annotations for the opinion targets.",
                "cite_spans": [
                    {
                        "start": 28,
                        "end": 47,
                        "text": "Poria et al. (2016)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 52,
                        "end": 78,
                        "text": "Jebbara and Cimiano (2016)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 163,
                        "end": 182,
                        "text": "Poria et al. (2016)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 236,
                        "end": 254,
                        "text": "Wang et al. (2017)",
                        "ref_id": "BIBREF20"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "4"
            },
            {
                "text": "Li and Lam (2017) propose two LSTMs with memory interaction to detect aspect and opinion terms. In order to generate opinion expression annotations for the SemEval dataset, a sentiment lexicon is used in combination with high precision dependency rules.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "4"
            },
            {
                "text": "For a more comprehensive overview of ABSA and OTE extraction approaches we refer to Pontiki et al. (2016) .",
                "cite_spans": [
                    {
                        "start": 84,
                        "end": 105,
                        "text": "Pontiki et al. (2016)",
                        "ref_id": "BIBREF10"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "4"
            },
            {
                "text": "Cross-Lingual and Zero-Shot Learning for Sequence Labelling With the CLOpinionMiner, Zhou et al. (2015) present a method for crosslingual opinion target extraction that relies on machine translation. The approach derives an annotated dataset for a target language by translating the annotated source language data. Part-of-Speech tags and dependency path-features are projected into the translated data using the word alignment information of the translation algorithm. The approach is evaluated for English to Chinese reviews. A drawback of the presented method is that it requires access to a strong machine trans-lation algorithm for source to target language that also provides word alignment information. Additionally, it builds upon NLP resources that are not available for many potential target languages.",
                "cite_spans": [
                    {
                        "start": 85,
                        "end": 103,
                        "text": "Zhou et al. (2015)",
                        "ref_id": "BIBREF21"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "4"
            },
            {
                "text": "Addressing the task of zero-shot spoken language understanding (SLU), Upadhyay et al. (2018) follow a similar approach as our work. They use the aligned embeddings from Smith et al. (2017) in combination with a bidirectional RNN and target zero-shot SLU for Hindi and Turkish.",
                "cite_spans": [
                    {
                        "start": 70,
                        "end": 92,
                        "text": "Upadhyay et al. (2018)",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 169,
                        "end": 188,
                        "text": "Smith et al. (2017)",
                        "ref_id": "BIBREF14"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "4"
            },
            {
                "text": "Overall, our work differs from the related work by presenting a simple model for the zero-shot extraction of opinion target expressions. By using no annotated target data or elaborate NLP resources, such as Part-of-Speech taggers or dependency parsers, our approach is easily applicable to many resource-poor languages.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "4"
            },
            {
                "text": "In this work, we presented a method for crosslingual and zero-shot extraction of opinion target expressions which we evaluated on 5 languages. Our approach uses multilingual word embeddings that are aligned into a single vector space to allow for cross-lingual transfer of models.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "5"
            },
            {
                "text": "Using English as a source language in a zeroshot setting, our approach was able to reach an F 1score of 0.50 for Spanish and 0.46 for Dutch. This corresponds to relative performances of 74% and 77% compared to a baseline system trained on target language data. By using multiple source languages, we increased the zero-shot performance to F 1 -scores of 0.58 and 0.53, respectively, which correspond to 85% and 87% in relative terms. We investigated the benefit of augmenting the zeroshot approach with additional data points from the target language. Here, we observed that we can save several hundreds of annotated data points by employing a cross-lingual approach. Among the 5 considered languages, Turkish seemed to benefit the least from cross-lingual learning in all experiments. The reason for this might be that Turkish is the only agglutinative language in the dataset. Further, we compared two approaches for aligning multilingual word embeddings in a single vector space and found their results to vary for individual language pairs but to be comparable overall. Lastly, we compared our multilingual model with the state-of-the-art for all languages and saw that we achieve competitive performances for some languages and even present the best system for Russian and Turkish.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "5"
            },
            {
                "text": "Note that the B token is only used to indicate the boundary of two consecutive phrases.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "The input sequences are padded with zeros to allow the application of the convolution operations to the edge words.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Obtained from:https://github.com/ Babylonpartners/fastText_multilingual",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Obtained from:https://github.com/ facebookresearch/MUSE",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "We tried to include the dataset of French reviews in our evaluation but the provided download script no longer works.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "90% of OTEs in the English dataset consist of zero or more adjectives followed by at least one noun.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "This work was supported in part by the H2020 project Pr\u00eat-\u00e0-LLOD under Grant Agreement number 825182.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgments",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "GTI at SemEval-2016 Task 5: SVM and CRF for Aspect Detection and Unsupervised Aspect-Based Sentiment Analysis",
                "authors": [
                    {
                        "first": "Tamara",
                        "middle": [],
                        "last": "\u00c0lvarez-L\u00f3pez",
                        "suffix": ""
                    },
                    {
                        "first": "Jonathan",
                        "middle": [],
                        "last": "Juncal-Mart\u00ednez",
                        "suffix": ""
                    },
                    {
                        "first": "Milagros",
                        "middle": [
                            "Fern\u00e1ndez"
                        ],
                        "last": "Gavilanes",
                        "suffix": ""
                    },
                    {
                        "first": "Enrique",
                        "middle": [],
                        "last": "Costa-Montenegro",
                        "suffix": ""
                    },
                    {
                        "first": "Francisco",
                        "middle": [],
                        "last": "Javier",
                        "suffix": ""
                    },
                    {
                        "first": "Gonz\u00e1lez-Casta\u00f1o",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "SemEval@NAACL-HLT",
                "volume": "",
                "issue": "",
                "pages": "306--311",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tamara \u00c0lvarez-L\u00f3pez, Jonathan Juncal-Mart\u00ednez, Milagros Fern\u00e1ndez Gavilanes, Enrique Costa- Montenegro, and Francisco Javier Gonz\u00e1lez- Casta\u00f1o. 2016. GTI at SemEval-2016 Task 5: SVM and CRF for Aspect Detection and Unsu- pervised Aspect-Based Sentiment Analysis. In SemEval@NAACL-HLT, pages 306-311. The Association for Computer Linguistics.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Enriching Word Vectors with Subword Information",
                "authors": [
                    {
                        "first": "Piotr",
                        "middle": [],
                        "last": "Bojanowski",
                        "suffix": ""
                    },
                    {
                        "first": "Edouard",
                        "middle": [],
                        "last": "Grave",
                        "suffix": ""
                    },
                    {
                        "first": "Armand",
                        "middle": [],
                        "last": "Joulin",
                        "suffix": ""
                    },
                    {
                        "first": "Tomas",
                        "middle": [],
                        "last": "Mikolov",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Transactions of the Association for Computational Linguistics",
                "volume": "5",
                "issue": "",
                "pages": "135--146",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching Word Vectors with Subword Information. Transactions of the Associa- tion for Computational Linguistics, 5:135-146.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Overfitting in Neural Nets: Backpropagation, Conjugate Gradient, and Early Stopping",
                "authors": [
                    {
                        "first": "Rich",
                        "middle": [],
                        "last": "Caruana",
                        "suffix": ""
                    },
                    {
                        "first": "Steve",
                        "middle": [],
                        "last": "Lawrence",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [
                            "Lee"
                        ],
                        "last": "Giles",
                        "suffix": ""
                    }
                ],
                "year": 2000,
                "venue": "NIPS",
                "volume": "",
                "issue": "",
                "pages": "402--408",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Rich Caruana, Steve Lawrence, and C. Lee Giles. 2000. Overfitting in Neural Nets: Backpropagation, Con- jugate Gradient, and Early Stopping. In NIPS, pages 402-408. MIT Press.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Extracting opinion targets in a single-and cross-domain setting with conditional random fields",
                "authors": [
                    {
                        "first": "Niklas",
                        "middle": [],
                        "last": "Jakob",
                        "suffix": ""
                    },
                    {
                        "first": "Iryna",
                        "middle": [],
                        "last": "Gurevych",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "1035--1045",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Niklas Jakob and Iryna Gurevych. 2010. Extracting opinion targets in a single-and cross-domain setting with conditional random fields. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1035-1045.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Aspect-Based Relational Sentiment Analysis Using a Stacked Neural Network Architecture",
                "authors": [
                    {
                        "first": "Soufian",
                        "middle": [],
                        "last": "Jebbara",
                        "suffix": ""
                    },
                    {
                        "first": "Philipp",
                        "middle": [],
                        "last": "Cimiano",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "The Netherlands -Including Prestigious Applications of Artificial Intelligence (PAIS 2016)",
                "volume": "29",
                "issue": "",
                "pages": "1123--1131",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Soufian Jebbara and Philipp Cimiano. 2016. Aspect- Based Relational Sentiment Analysis Using a Stacked Neural Network Architecture. In ECAI 2016 -22nd European Conference on Artificial In- telligence, 29 August-2 September 2016, The Hague, The Netherlands -Including Prestigious Applica- tions of Artificial Intelligence (PAIS 2016), pages 1123--1131.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Adam: A Method for Stochastic Optimization",
                "authors": [
                    {
                        "first": "Diederik",
                        "middle": [],
                        "last": "Kingma",
                        "suffix": ""
                    },
                    {
                        "first": "Jimmy",
                        "middle": [],
                        "last": "Ba",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the International Conference on Learning Representations",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Diederik Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. In Proceed- ings of the International Conference on Learning Representations.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "IIT-TUDA at SemEval-2016 Task 5: Beyond Sentiment Lexicon: Combining Domain Dependency and Distributional Semantics Features for Aspect Based Sentiment Analysis",
                "authors": [
                    {
                        "first": "Ayush",
                        "middle": [],
                        "last": "Kumar",
                        "suffix": ""
                    },
                    {
                        "first": "Sarah",
                        "middle": [],
                        "last": "Kohail",
                        "suffix": ""
                    },
                    {
                        "first": "Amit",
                        "middle": [],
                        "last": "Kumar",
                        "suffix": ""
                    },
                    {
                        "first": "Asif",
                        "middle": [],
                        "last": "Ekbal",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Biemann",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "SemEval@NAACL-HLT",
                "volume": "",
                "issue": "",
                "pages": "1129--1135",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ayush Kumar, Sarah Kohail, Amit Kumar, Asif Ekbal, and Chris Biemann. 2016. IIT-TUDA at SemEval- 2016 Task 5: Beyond Sentiment Lexicon: Combin- ing Domain Dependency and Distributional Seman- tics Features for Aspect Based Sentiment Analysis. In SemEval@NAACL-HLT, pages 1129-1135. The Association for Computer Linguistics.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Deep Multi-Task Learning for Aspect Term Extraction with Memory Interaction",
                "authors": [
                    {
                        "first": "Guillaume",
                        "middle": [],
                        "last": "Lample",
                        "suffix": ""
                    },
                    {
                        "first": "Alexis",
                        "middle": [],
                        "last": "Conneau",
                        "suffix": ""
                    },
                    {
                        "first": "Marc'aurelio",
                        "middle": [],
                        "last": "Ranzato",
                        "suffix": ""
                    },
                    {
                        "first": "Ludovic",
                        "middle": [],
                        "last": "Denoyer",
                        "suffix": ""
                    },
                    {
                        "first": "Herv",
                        "middle": [],
                        "last": "Jgou",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "2886--2892",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Guillaume Lample, Alexis Conneau, Marc'Aurelio Ranzato, Ludovic Denoyer, and Herv Jgou. 2018. Word translation without parallel data. In Interna- tional Conference on Learning Representations. Xin Li and Wai Lam. 2017. Deep Multi-Task Learn- ing for Aspect Term Extraction with Memory In- teraction. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Pro- cessing, pages 2886-2892. Association for Compu- tational Linguistics.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Rectified Linear Units Improve Restricted Boltzmann Machines",
                "authors": [
                    {
                        "first": "Vinod",
                        "middle": [],
                        "last": "Nair",
                        "suffix": ""
                    },
                    {
                        "first": "Geoffrey",
                        "middle": [
                            "E"
                        ],
                        "last": "Hinton",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the 27th International Conference on International Conference on Machine Learning, ICML'10",
                "volume": "",
                "issue": "",
                "pages": "807--814",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Vinod Nair and Geoffrey E. Hinton. 2010. Rectified Linear Units Improve Restricted Boltzmann Ma- chines. In Proceedings of the 27th International Conference on International Conference on Ma- chine Learning, ICML'10, pages 807-814, USA. Omnipress.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Feature Selection, L1 vs. L2 Regularization, and Rotational Invariance",
                "authors": [
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Andrew",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Ng",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proceedings of the Twenty-first International Conference on Machine Learning, ICML '04",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Andrew Y. Ng. 2004. Feature Selection, L1 vs. L2 Regularization, and Rotational Invariance. In Pro- ceedings of the Twenty-first International Confer- ence on Machine Learning, ICML '04, pages 78-, New York, NY, USA. ACM.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "SemEval-2016 Task 5: Aspect Based Sentiment Analysis",
                "authors": [
                    {
                        "first": "Maria",
                        "middle": [],
                        "last": "Pontiki",
                        "suffix": ""
                    },
                    {
                        "first": "Dimitris",
                        "middle": [],
                        "last": "Galanis",
                        "suffix": ""
                    },
                    {
                        "first": "Haris",
                        "middle": [],
                        "last": "Papageorgiou",
                        "suffix": ""
                    },
                    {
                        "first": "Ion",
                        "middle": [],
                        "last": "Androutsopoulos",
                        "suffix": ""
                    },
                    {
                        "first": "Suresh",
                        "middle": [],
                        "last": "Manandhar",
                        "suffix": ""
                    },
                    {
                        "first": "Mohammad",
                        "middle": [],
                        "last": "Al-Smadi",
                        "suffix": ""
                    },
                    {
                        "first": "Mahmoud",
                        "middle": [],
                        "last": "Al-Ayyoub",
                        "suffix": ""
                    },
                    {
                        "first": "Yanyan",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Bing",
                        "middle": [],
                        "last": "Qin",
                        "suffix": ""
                    },
                    {
                        "first": "Orph\u00e9e",
                        "middle": [],
                        "last": "De Clercq",
                        "suffix": ""
                    },
                    {
                        "first": "V\u00e9ronique",
                        "middle": [],
                        "last": "Hoste",
                        "suffix": ""
                    },
                    {
                        "first": "Marianna",
                        "middle": [],
                        "last": "Apidianaki",
                        "suffix": ""
                    },
                    {
                        "first": "Xavier",
                        "middle": [],
                        "last": "Tannier",
                        "suffix": ""
                    },
                    {
                        "first": "Natalia",
                        "middle": [
                            "V"
                        ],
                        "last": "Loukachevitch",
                        "suffix": ""
                    },
                    {
                        "first": "Evgeniy",
                        "middle": [],
                        "last": "Kotelnikov",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the 10th International Workshop on Semantic Evaluation, SemEval@NAACL-HLT 2016",
                "volume": "",
                "issue": "",
                "pages": "19--30",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Maria Pontiki, Dimitris Galanis, Haris Papageor- giou, Ion Androutsopoulos, Suresh Manandhar, Mo- hammad Al-Smadi, Mahmoud Al-Ayyoub, Yanyan Zhao, Bing Qin, Orph\u00e9e De Clercq, V\u00e9ronique Hoste, Marianna Apidianaki, Xavier Tannier, Na- talia V. Loukachevitch, Evgeniy Kotelnikov, N\u00faria Bel, Salud Mar\u00eda Jim\u00e9nez Zafra, and G\u00fclsen Eryigit. 2016. SemEval-2016 Task 5: Aspect Based Sen- timent Analysis. In Proceedings of the 10th International Workshop on Semantic Evaluation, SemEval@NAACL-HLT 2016, San Diego, CA, USA, June 16-17, 2016, pages 19-30.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Aspect Extraction for Opinion Miningwith a Deep Convolutional Neural Network",
                "authors": [
                    {
                        "first": "Soujanya",
                        "middle": [],
                        "last": "Poria",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Knowledge-Based Systems",
                "volume": "108",
                "issue": "",
                "pages": "42--49",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Soujanya Poria, Erik Cambria, and Alexander Gel- bukh. 2016. Aspect Extraction for Opinion Min- ingwith a Deep Convolutional Neural Network. Knowledge-Based Systems, 108:42-49.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Elixa: A modular and flexible ABSA platform",
                "authors": [
                    {
                        "first": "Vicente",
                        "middle": [],
                        "last": "I\u00f1aki San",
                        "suffix": ""
                    },
                    {
                        "first": "Xabier",
                        "middle": [],
                        "last": "Saralegi",
                        "suffix": ""
                    },
                    {
                        "first": "Rodrigo",
                        "middle": [],
                        "last": "Agerri",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the 9th International Workshop on Semantic Evaluation",
                "volume": "",
                "issue": "",
                "pages": "748--752",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "I\u00f1aki San Vicente, Xabier Saralegi, and Rodrigo Agerri. 2015. Elixa: A modular and flexible ABSA platform. In Proceedings of the 9th International Workshop on Semantic Evaluation, pages 748-752, Denver, Colorado. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "A generalized solution of the orthogonal procrustes problem",
                "authors": [
                    {
                        "first": "H",
                        "middle": [],
                        "last": "Peter",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Sch\u00f6nemann",
                        "suffix": ""
                    }
                ],
                "year": 1966,
                "venue": "Psychometrika",
                "volume": "31",
                "issue": "1",
                "pages": "1--10",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Peter H. Sch\u00f6nemann. 1966. A generalized solution of the orthogonal procrustes problem. Psychometrika, 31(1):1-10.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Offline bilingual word vectors, orthogonal transformations and the inverted softmax",
                "authors": [
                    {
                        "first": "L",
                        "middle": [],
                        "last": "Samuel",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Smith",
                        "suffix": ""
                    },
                    {
                        "first": "H",
                        "middle": [
                            "P"
                        ],
                        "last": "David",
                        "suffix": ""
                    },
                    {
                        "first": "Steven",
                        "middle": [],
                        "last": "Turban",
                        "suffix": ""
                    },
                    {
                        "first": "Nils",
                        "middle": [
                            "Y"
                        ],
                        "last": "Hamblin",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Hammerla",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "International Conference on Learning Representations",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Samuel L. Smith, David H. P. Turban, Steven Hamblin, and Nils Y. Hammerla. 2017. Offline bilingual word vectors, orthogonal transformations and the inverted softmax. In International Conference on Learning Representations.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
                "authors": [
                    {
                        "first": "Nitish",
                        "middle": [],
                        "last": "Srivastava",
                        "suffix": ""
                    },
                    {
                        "first": "Geoffrey",
                        "middle": [],
                        "last": "Hinton",
                        "suffix": ""
                    },
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Krizhevsky",
                        "suffix": ""
                    },
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    },
                    {
                        "first": "Ruslan",
                        "middle": [],
                        "last": "Salakhutdinov",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Journal of Machine Learning Research",
                "volume": "15",
                "issue": "",
                "pages": "1929--1958",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A Simple Way to Prevent Neural Networks from Overfitting. Journal of Machine Learning Re- search, 15:1929-1958.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Tjong Kim Sang and Jorn Veenstra",
                "authors": [
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Erik",
                        "suffix": ""
                    }
                ],
                "year": 1999,
                "venue": "Proceedings of European Chapter of the ACL (EACL)",
                "volume": "",
                "issue": "",
                "pages": "173--179",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Erik F. Tjong Kim Sang and Jorn Veenstra. 1999. Rep- resenting text chunks. In Proceedings of Euro- pean Chapter of the ACL (EACL), pages 173-179. Bergen, Norway.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "NLANGP at SemEval-2016 Task 5: Improving Aspect Based Sentiment Analysis using Neural Network Features",
                "authors": [
                    {
                        "first": "Zhiqiang",
                        "middle": [],
                        "last": "Toh",
                        "suffix": ""
                    },
                    {
                        "first": "Jian",
                        "middle": [],
                        "last": "Su",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the 10th International Workshop on Semantic Evaluation, SemEval@NAACL-HLT 2016",
                "volume": "2015",
                "issue": "",
                "pages": "282--288",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zhiqiang Toh and Jian Su. 2016. NLANGP at SemEval-2016 Task 5: Improving Aspect Based Sentiment Analysis using Neural Network Features. In Proceedings of the 10th International Work- shop on Semantic Evaluation, SemEval@NAACL- HLT 2016, volume 2015, pages 282-288.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "DLIREC: Aspect Term Extraction and Term Polarity Classification System",
                "authors": [
                    {
                        "first": "Zhiqiang",
                        "middle": [],
                        "last": "Toh",
                        "suffix": ""
                    },
                    {
                        "first": "Wenting",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of the 8th International Workshop on Semantic Evaluation",
                "volume": "",
                "issue": "",
                "pages": "235--240",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zhiqiang Toh and Wenting Wang. 2014. DLIREC: As- pect Term Extraction and Term Polarity Classifica- tion System. In Proceedings of the 8th International Workshop on Semantic Evaluation, pages 235-240.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "(Almost) Zero-Shot Cross-Lingual Spoken Language Understanding",
                "authors": [
                    {
                        "first": "Shyam",
                        "middle": [],
                        "last": "Upadhyay",
                        "suffix": ""
                    },
                    {
                        "first": "Manaal",
                        "middle": [],
                        "last": "Faruqui",
                        "suffix": ""
                    },
                    {
                        "first": "Gokhan",
                        "middle": [],
                        "last": "Tur",
                        "suffix": ""
                    },
                    {
                        "first": "Dilek",
                        "middle": [],
                        "last": "Hakkani-Tur",
                        "suffix": ""
                    },
                    {
                        "first": "Larry",
                        "middle": [],
                        "last": "Heck",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Shyam Upadhyay, Manaal Faruqui, Gokhan Tur, Dilek Hakkani-Tur, and Larry Heck. 2018. (Almost) Zero- Shot Cross-Lingual Spoken Language Understand- ing. In Proceedings of the 2018 IEEE International Conference on Acoustics, Speech and Signal Pro- cessing (ICASSP).",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Coupled Multi-Layer Attentions for Co-Extraction of Aspect and Opinion Terms",
                "authors": [
                    {
                        "first": "Wenya",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Sinno",
                        "middle": [],
                        "last": "Jialin Pan",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Dahlmeier",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaokui",
                        "middle": [],
                        "last": "Xiao",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence",
                "volume": "",
                "issue": "",
                "pages": "3316--3322",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Wenya Wang, Sinno Jialin Pan, Daniel Dahlmeier, and Xiaokui Xiao. 2017. Coupled Multi-Layer At- tentions for Co-Extraction of Aspect and Opinion Terms. In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, February 4-9, 2017, San Francisco, California, USA., pages 3316- 3322.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "CLOpinionMiner: Opinion Target Extraction in a Cross-Language Scenario",
                "authors": [
                    {
                        "first": "Xinjie",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaojun",
                        "middle": [],
                        "last": "Wan",
                        "suffix": ""
                    },
                    {
                        "first": "Jianguo",
                        "middle": [],
                        "last": "Xiao",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing",
                "volume": "23",
                "issue": "",
                "pages": "1--1",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xinjie Zhou, Xiaojun Wan, and Jianguo Xiao. 2015. CLOpinionMiner: Opinion Target Extraction in a Cross-Language Scenario. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 23:1- 1.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 1: Model for sequence tagging using convolution operations. For simplicity, we only show a single convolution operation. The gray boxes depict padding vectors. The layers inside the dashed box are shared across multiple languages.",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "This part of our evaluation addresses our first research question: RQ1: To what degree is the model capable of performing OTE extraction for unseen languages?",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "Figure 3: Cross-lingual results for increasing numbers of training samples from the target language.",
                "uris": null,
                "fig_num": "3",
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table><tr><td>Dataset</td><td colspan=\"3\">#Sent. #Tokens #Targets</td></tr><tr><td>en (train)</td><td>2000</td><td>29278</td><td>1880</td></tr><tr><td>en (test)</td><td>676</td><td>10080</td><td>650</td></tr><tr><td>es (train)</td><td>2070</td><td>36164</td><td>1937</td></tr><tr><td>es (test)</td><td>881</td><td>13290</td><td>731</td></tr><tr><td>nl (train)</td><td>1722</td><td>24981</td><td>1283</td></tr><tr><td>nl (test)</td><td>575</td><td>7690</td><td>394</td></tr><tr><td>ru (train)</td><td>3655</td><td>53734</td><td>3159</td></tr><tr><td>ru (test)</td><td>1209</td><td>17856</td><td>972</td></tr><tr><td>tr (train)</td><td>1232</td><td>12702</td><td>1385</td></tr><tr><td>tr (test)</td><td>144</td><td>1360</td><td>159</td></tr></table>",
                "type_str": "table",
                "text": "Table 1 gives a brief overview of the used datasets. Statistics of the SemEval 2016 ABSA dataset for the restaurant domain.",
                "html": null,
                "num": null
            },
            "TABREF1": {
                "content": "<table><tr><td/><td/><td>en</td><td>es</td><td colspan=\"2\">nl target language</td><td>ru</td><td>tr</td></tr><tr><td/><td>en</td><td>0.66</td><td>0.5</td><td/><td>0.46</td><td>0.37</td><td>0.17</td></tr><tr><td/><td>es</td><td>0.43</td><td>0.68</td><td/><td>0.29</td><td>0.28</td><td>0.14</td></tr><tr><td>source language</td><td>nl ru</td><td>0.45 0.42</td><td>0.44 0.49</td><td/><td>0.6 0.45</td><td>0.37 0.56</td><td>0.17 0.3</td></tr><tr><td/><td>tr</td><td>0.33</td><td>0.42</td><td/><td>0.34</td><td>0.35</td><td>0.48</td></tr><tr><td colspan=\"8\">Figure 2: Zero-shot F 1 -scores for cross-lingual learn-</td></tr><tr><td colspan=\"8\">ing from a single source to a target language.</td></tr><tr><td/><td/><td>target</td><td/><td>en</td><td>es</td><td>nl</td><td>ru</td><td>tr</td></tr><tr><td/><td/><td colspan=\"6\">best\u2192target 0.45 0.50 0.46 0.37 0.30</td></tr><tr><td colspan=\"8\">all others\u2192target 0.52 0.58 0.53 0.43 0.27</td></tr><tr><td/><td/><td colspan=\"6\">target\u2192target 0.66 0.68 0.60 0.56 0.48</td></tr></table>",
                "type_str": "table",
                "text": "In the next experiment, we want to address our second research question: Zero-shot results for cross-lingual learning from multiple source languages to a target language.",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table><tr><td/><td/><td>en</td><td>es</td><td>nl target language</td><td>ru</td><td>tr</td></tr><tr><td/><td>en</td><td>0.74</td><td>-0.82</td><td>-4.6</td><td>-2.4</td><td>2.3</td></tr><tr><td/><td>es</td><td>3.5</td><td>-0.41</td><td>4.7</td><td>1.9</td><td>4.9</td></tr><tr><td>source language</td><td>nl ru</td><td>1.2 2</td><td>0.097 -1.8</td><td>-0.87 -3.7</td><td>-4.6 -0.44</td><td>2.9 0.17</td></tr><tr><td/><td>tr</td><td>3.5</td><td>-3.3</td><td>-6.2</td><td>-2.9</td><td>-1.1</td></tr></table>",
                "type_str": "table",
                "text": "Figure 4: Zero-shot results comparing the multilingual embeddings ADV-aligned to SVD-aligned. A positive value means higher absolute F 1 score for ADV-aligned and vice versa. For readability, score differences are scaled by a factor of 100.",
                "html": null,
                "num": null
            }
        }
    }
}