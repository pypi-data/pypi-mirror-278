{
    "paper_id": "N01-1024",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:04:56.985101Z"
    },
    "title": "Knowledge-Free Induction of Inflectional Morphologies",
    "authors": [
        {
            "first": "Patrick",
            "middle": [],
            "last": "Schone",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Colorado at Boulder University of Colorado at Boulder Boulder",
                "location": {
                    "postCode": "80309, 80309",
                    "settlement": "Boulder",
                    "region": "Colorado, Colorado"
                }
            },
            "email": "schone@cs.colorado.edu"
        },
        {
            "first": "Daniel",
            "middle": [],
            "last": "Jurafsky",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Colorado at Boulder University of Colorado at Boulder Boulder",
                "location": {
                    "postCode": "80309, 80309",
                    "settlement": "Boulder",
                    "region": "Colorado, Colorado"
                }
            },
            "email": "jurafsky@cs.colorado.edu"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "We propose an algorithm to automatically induce the morphology of inflectional languages using only text corpora and no human input. Our algorithm combines cues from orthography, semantics, and syntactic distributions to induce morphological relationships in German, Dutch, and English. Using CELEX as a gold standard for evaluation, we show our algorithm to be an improvement over any knowledge-free algorithm yet proposed.",
    "pdf_parse": {
        "paper_id": "N01-1024",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "We propose an algorithm to automatically induce the morphology of inflectional languages using only text corpora and no human input. Our algorithm combines cues from orthography, semantics, and syntactic distributions to induce morphological relationships in German, Dutch, and English. Using CELEX as a gold standard for evaluation, we show our algorithm to be an improvement over any knowledge-free algorithm yet proposed.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Many NLP tasks, such as building machine-readable dictionaries, are dependent on the results of morphological analysis. While morphological analyzers have existed since the early 1960s, current algorithms require human labor to build rules for morphological structure. In an attempt to avoid this labor-intensive process, recent work has focused on machine-learning approaches to induce morphological structure using large corpora.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this paper, we propose a knowledge-free algorithm to automatically induce the morphology structures of a language. Our algorithm takes as input a large corpus and produces as output a set of conflation sets indicating the various inflected and derived forms for each word in the language. As an example, the conflation set of the word \"abuse\" would contain \"abuse\", \"abused\", \"abuses\", \"abusive\", \"abusively\", and so forth. Our algorithm extends earlier approaches to morphology induction by combining various induced information sources: the semantic relatedness of the affixed forms using a Latent Semantic Analysis approach to corpusbased semantics (Schone and Jurafsky, 2000) , affix frequency, syntactic context, and transitive closure. Using the hand-labeled CELEX lexicon (Baayen, et al., 1993) as our gold standard, the current version of our algorithm achieves an F-score of 88.1% on the task of identifying conflation sets in English, outperforming earlier algorithms. Our algorithm is also applied to German and Dutch and evaluated on its ability to find prefixes, suffixes, and circumfixes in these languages. To our knowledge, this serves as the first evaluation of complete regular morphological induction of German or Dutch (although researchers such as Nakisa and Hahn (1996) have evaluated induction algorithms on morphological sub-problems in German).",
                "cite_spans": [
                    {
                        "start": 655,
                        "end": 682,
                        "text": "(Schone and Jurafsky, 2000)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 782,
                        "end": 804,
                        "text": "(Baayen, et al., 1993)",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 1272,
                        "end": 1294,
                        "text": "Nakisa and Hahn (1996)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Previous morphology induction approaches have fallen into three categories. These categories differ depending on whether human input is provided and on whether the goal is to obtain affixes or complete morphological analysis. We here briefly describe work in each category.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Previous Approaches",
                "sec_num": "2"
            },
            {
                "text": "Some researchers begin with some initial humanlabeled source from which they induce other morphological components. In particular, Xu and Croft (1998) use word context derived from a corpus to refine Porter stemmer output. Gaussier (1999) induces derivational morphology using an inflectional lexicon which includes part of speech information. Grabar and Zweigenbaum (1999) use the SNOMED corpus of semantically-arranged medical terms to find semantically-motivated morphological relationships. Also, Yarowsky and Wicentowski (2000) obtained outstanding results at inducing English past tense after beginning with a list of the open class roots in the language, a table of a language's inflectional parts of speech, and the canonical suffixes for each part of speech.",
                "cite_spans": [
                    {
                        "start": 131,
                        "end": 150,
                        "text": "Xu and Croft (1998)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 223,
                        "end": 238,
                        "text": "Gaussier (1999)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 344,
                        "end": 373,
                        "text": "Grabar and Zweigenbaum (1999)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 501,
                        "end": 532,
                        "text": "Yarowsky and Wicentowski (2000)",
                        "ref_id": "BIBREF15"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Using a Knowledge Source to Bootstrap",
                "sec_num": "2.1"
            },
            {
                "text": "A second, knowledge-free category of research has focused on obtaining affix inventories. Brent, et al. (1995) used minimum description length (MDL) to find the most data-compressing suffixes. Kazakov (1997) does something akin to this using MDL as a fitness metric for evolutionary computing. D\u00e9Jean (1998) uses a strategy similar to that of Harris (1951) . He declares that a stem has ended when the number of characters following it exceed some given threshold and identifies any residual following semantic relations, we identified those word pairs the stems as suffixes.",
                "cite_spans": [
                    {
                        "start": 90,
                        "end": 110,
                        "text": "Brent, et al. (1995)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 193,
                        "end": 207,
                        "text": "Kazakov (1997)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 343,
                        "end": 356,
                        "text": "Harris (1951)",
                        "ref_id": "BIBREF5"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Affix Inventories",
                "sec_num": "2.2"
            },
            {
                "text": "that have strong semantic correlations as being",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Affix Inventories",
                "sec_num": "2.2"
            },
            {
                "text": "Due to the existence of morphological ambiguity (such as with the word \"caring\" whose stem is \"care\" rather than \"car\"), finding affixes alone does not constitute a complete morphological analysis. Hence, the last category of research is also knowledge-free but attempts to induce, for each word of a corpus, a complete analysis. Since our Most of the existing algorithms described focus on approach falls into this category (expanding upon suffixing in inflectional languages (though our earlier approach (Schone and Jurafsky, 2000) ),",
                "cite_spans": [
                    {
                        "start": 506,
                        "end": 533,
                        "text": "(Schone and Jurafsky, 2000)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Complete morphological analysis",
                "sec_num": "2.3"
            },
            {
                "text": "Jacquemin and D\u00e9Jean describe work on prefixes). we describe work in this area in more detail.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Complete morphological analysis",
                "sec_num": "2.3"
            },
            {
                "text": "None of these algorithms consider the general Jacquemin (1997) deems pairs of word n-grams as morphologically related if two words in the first ngram have the same first few letters (or stem) as two words in the second n-gram and if there is a suffix for each stem whose length is less than k. He also clusters groups of words having the same kinds of word endings, which gives an added performance boost. He applies his algorithm to a French term list and scores based on sampled, by-hand evaluation. Goldsmith (1997 Goldsmith ( /2000) ) tries to automatically sever each word in exactly one place in order to establish a potential set of stems and suffixes. He uses the expectation-maximization algorithm (EM) and MDL as well as some triage procedures to help eliminate inappropriate parses for every word in a corpus. He collects the possible suffixes for each stem and calls these signatures which give clues about word classes. With the exceptions of capitalization removal and some word segmentation, Goldsmith's algorithm is otherwise knowledge-free. His algorithm, Linguistica, is freely available on the Internet. Goldsmith applies his algorithm to various languages but evaluates in English and French.",
                "cite_spans": [
                    {
                        "start": 46,
                        "end": 62,
                        "text": "Jacquemin (1997)",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 502,
                        "end": 517,
                        "text": "Goldsmith (1997",
                        "ref_id": null
                    },
                    {
                        "start": 518,
                        "end": 538,
                        "text": "Goldsmith ( /2000) )",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Complete morphological analysis",
                "sec_num": "2.3"
            },
            {
                "text": "In our earlier work, we (Schone and Jurafsky (2000) ) generated a list of N candidate suffixes and used this list to identify word pairs which share the same stem but conclude with distinct candidate suffixes. We then applied Latent Semantic Analysis (Deerwester, et al., 1990) as a method of automatically determining semantic relatedness between word pairs. Using statistics from the morphological variants of each other. With the exception of word segmentation, we provided no human information to our system. We applied our system to an English corpus and evaluated by comparing each word's conflation set as produced by our algorithm to those derivable from CELEX.",
                "cite_spans": [
                    {
                        "start": 24,
                        "end": 51,
                        "text": "(Schone and Jurafsky (2000)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 251,
                        "end": 277,
                        "text": "(Deerwester, et al., 1990)",
                        "ref_id": "BIBREF1"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Schone and Jurafsky: induced semantics",
                "sec_num": "2.3.3"
            },
            {
                "text": "conditions of circumfixing or infixing, nor are they applicable to other language types such as agglutinative languages (Sproat, 1992) .",
                "cite_spans": [
                    {
                        "start": 120,
                        "end": 134,
                        "text": "(Sproat, 1992)",
                        "ref_id": "BIBREF13"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Problems with earlier approaches",
                "sec_num": "2.4"
            },
            {
                "text": "Additionally, most approaches have centered around statistics of orthographic properties. We had noted previously (Schone and Jurafsky, 2000) , however, that errors can arise from strictly orthographic systems. We had observed in other systems such errors as inappropriate removal of valid affixes (\"ally\"<\"all\"), failure to resolve morphological ambiguities (\"hated\"<\"hat\"), and pruning of semi-productive affixes (\"dirty\"h\"dirt\"). Yet we illustrated that induced semantics can help overcome some of these errors.",
                "cite_spans": [
                    {
                        "start": 114,
                        "end": 141,
                        "text": "(Schone and Jurafsky, 2000)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Problems with earlier approaches",
                "sec_num": "2.4"
            },
            {
                "text": "However, we have since observed that induced semantics can give rise to different kinds of problems. For instance, morphological variants may be semantically opaque such that the meaning of one variant cannot be readily determined by the other (\"reusability\"h\"use\"). Additionally, highfrequency function words may be conflated due to having weak semantic information (\"as\"<\"a\").",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Problems with earlier approaches",
                "sec_num": "2.4"
            },
            {
                "text": "Coupling semantic and orthographic statistics, as well as introducing induced syntactic information and relational transitivity can help in overcoming these problems. Therefore, we begin with an approach similar to our previous algorithm. Yet we build upon this algorithm in several ways in that we:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Problems with earlier approaches",
                "sec_num": "2.4"
            },
            {
                "text": "[1] consider circumfixes, [2] automatically identify capitalizations by treating them similar to prefixes [3] incorporate frequency information, [4] use distributional information to help identify syntactic properties, and [5] use transitive closure to help find variants that may not have been found to be semantically related but which are related to mutual variants. We then apply these strategies to English, German, and Dutch. We evaluate our algorithm Figure 2 ). Yet using this approach, there may be against the human-labeled CELEX lexicon in all circumfixes whose endings will be overlooked in three languages and compare our results to those the search for suffixes unless we first remove all that the Goldsmith and Schone/Jurafsky algorithms candidate prefixes. Therefore, we build a lexicon would have obtained on our same data. We show consisting of all words in our corpus and identify all how each of our additions result in progressively word beginnings with frequencies in excess of some better overall solutions. threshold (T ). We call these pseudo-prefixes. We 3 Current Approach",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 465,
                        "end": 466,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Problems with earlier approaches",
                "sec_num": "2.4"
            },
            {
                "text": "As in our earlier approach (Schone and Jurafsky, 2000) , we begin by generating, from an untagged corpus, a list of word pairs that might be morphological variants. Our algorithm has changed somewhat, though, since we previously sought word pairs that vary only by a prefix or a suffix, yet we now wish to generalize to those with circumfixing differences. We use \"circumfix\" to mean true circumfixes like the German ge-/-t as well as combinations of prefixes and suffixes. It should be mentioned also that we assume the existence of languages having valid circumfixes that are not composed merely of a prefix and a suffix that appear independently elsewhere.",
                "cite_spans": [
                    {
                        "start": 27,
                        "end": 54,
                        "text": "(Schone and Jurafsky, 2000)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Finding Candidate Circumfix Pairings",
                "sec_num": "3.1"
            },
            {
                "text": "To find potential morphological variants, our first goal is to find word endings which could serve as suffixes. We had shown in our earlier work how one might do this using a character tree, or trie (as in 1 strip all pseudo-prefixes from each word in our lexicon and add the word residuals back into the lexicon as if they were also words. Using this final lexicon, we can now seek for suffixes in a manner equivalent to what we had done before (Schone and Jurafsky, 2000) .",
                "cite_spans": [
                    {
                        "start": 446,
                        "end": 473,
                        "text": "(Schone and Jurafsky, 2000)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Finding Candidate Circumfix Pairings",
                "sec_num": "3.1"
            },
            {
                "text": "To demonstrate how this is done, suppose our initial lexicon / contained the words \"align,\"",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Finding Candidate Circumfix Pairings",
                "sec_num": "3.1"
            },
            {
                "text": "\"real,\" \"aligns,\" \"realign\", \"realigned\", \"react\", \"reacts,\" and \"reacted.\" Due to the high frequency occurrence of \"re-\" suppose it is identified as a pseudo-prefix. If we strip off \"re-\" from all words, and add all residuals to a trie, the branch of the trie of words beginning with \"a\" is depicted in Figure 2 .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 311,
                        "end": 312,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Finding Candidate Circumfix Pairings",
                "sec_num": "3.1"
            },
            {
                "text": "In our earlier work, we showed that a majority of the regular suffixes in the corpus can be found by identifying trie branches that appear repetitively. By \"branch\" we mean those places in the trie where some splitting occurs. In the case of Figure 2 , for example, the branches NULL (empty circle), \"-s\" and \"-ed\" each appear twice. We assemble a list of all trie branches that occur some minimum number of times (T ) and refer to such as potential suffixes.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 249,
                        "end": 250,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Finding Candidate Circumfix Pairings",
                "sec_num": "3.1"
            },
            {
                "text": "2 Given this list, we can now find potential prefixes using a similar strategy. Using our original lexicon, we can now strip off all potential suffixes from each word and form a new augmented lexicon. Then, (as we had proposed before) if we reverse the ordering on the words and insert them into a trie, the branches that are formed will be potential prefixes (in reverse order).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Finding Candidate Circumfix Pairings",
                "sec_num": "3.1"
            },
            {
                "text": "Before describing the last steps of this procedure, it is beneficial to define a few terms (some of which appeared in our previous work): [a] [e] pair of potential morphological variants (PPMV): two words sharing the same rule but distinct candidate circumfixes [f] ruleset: the set of all PPMVs for a common rule Our final goal in this first stage of induction is to find all of the possible rules and their corresponding rulesets. We therefore re-evaluate each word in the original lexicon to identify all potential circumfixes that could have been valid for the word. For example, suppose that the lists of potential suffixes and prefixes contained \"-ed\" and \"re-\" respectively. Note also that NULL exists by default in both lists as well. If we consider the word \"realigned\" from our lexicon /, we would find that its potential circumfixes would be NULL/ed, re/NULL, and re/ed and the corresponding pseudo-stems would be \"realign,\" \"aligned,\" and \"align,\" respectively, From /, we also note that circumfixes re/ed and NULL/ing share the pseudo-stems \"us,\" \"align,\" and \"view\" so a rule could be created: re/ed<NULL/ing. This means that word pairs such as \"reused/using\" and \"realigned/aligning\" would be deemed PPMVs.",
                "cite_spans": [
                    {
                        "start": 138,
                        "end": 141,
                        "text": "[a]",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Finding Candidate Circumfix Pairings",
                "sec_num": "3.1"
            },
            {
                "text": "Although the choices in T through T is 1 4",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Finding Candidate Circumfix Pairings",
                "sec_num": "3.1"
            },
            {
                "text": "somewhat arbitrary, we chose T =T =T =10 and 1 2 3 T =3. In English, for example, this yielded 30535 4 possible rules. Table 1 gives a sampling of these potential rules in each of the three languages in terms of frequency-sorted rank. Notice that several \"rules\" are quite valid, such as the indication of an English suffix -s. There are also valid circumfixes like the ge-/-t circumfix of German. Capitalization also appears (as a 'prefix'), such as C< c in English, D<d in German, and V<v in Dutch. Likewise,there are also some rules that may only be true in certain circumstances, such as -d<-r in English (such as worked/worker, but certainly not for steed/steer.) However, there are some rules that are ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 125,
                        "end": 126,
                        "text": "1",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Finding Candidate Circumfix Pairings",
                "sec_num": "3.1"
            },
            {
                "text": "-s< L -n< L -en< L 2 -ed< -ing -en< L -e< L 4 -ing< L -s< L -n< L 8 -ly< L -en< -t de-< L 12 C-< c- -en< -te -er< L 16 re-< L 1-< L -r< L 20 -ers< -ing er-< L V-< v- 24 1-< L 1-< 2- -ingen < -e 28 -d< -r ge-/-t < -en ge-< -e 32 s-< L D-< d- -n< -rs",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Finding Candidate Circumfix Pairings",
                "sec_num": "3.1"
            },
            {
                "text": "wrong: the potential 's-' prefix of English is never valid although word combinations like stick/tick spark/park, and slap/lap happen frequently in English. Incorporating semantics can help determine the validity of each rule.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Finding Candidate Circumfix Pairings",
                "sec_num": "3.1"
            },
            {
                "text": "Deerwester, et al. ( 1990) introduced an algorithm called Latent Semantic Analysis (LSA) which showed that valid semantic relationships between words and documents in a corpus can be induced with virtually no human intervention. To do this, one typically begins by applying singular value decomposition (SVD) to a matrix, M, whose entries M(i,j) contains the frequency of word i as seen in document j of the corpus. The SVD decomposes M into the product of three matrices, U, D, and V such T that U and V are orthogonal matrices and D is a T diagonal matrix whose entries are the singular values of M. The LSA approach then zeros out all but the top k singular values of the SVD, which has the effect of projecting vectors into an optimal kdimensional subspace. This methodology is well-described in the literature (Landauer, et al., 1998; Manning and Sch\u00fctze, 1999) .",
                "cite_spans": [
                    {
                        "start": 815,
                        "end": 839,
                        "text": "(Landauer, et al., 1998;",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 840,
                        "end": 866,
                        "text": "Manning and Sch\u00fctze, 1999)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Computing Semantics",
                "sec_num": "3.2"
            },
            {
                "text": "In order to obtain semantic representations of each word, we apply our previous strategy (Schone and Jurafsky (2000) ). Rather than using a termdocument matrix, we had followed an approach akin to that of Sch\u00fctze (1993) , who performed SVD on a Nx2N term-term matrix. The N here represents the N-1 most-frequent words as well as a glob position to account for all other words not in the top N-1. The matrix is structured such that for a given word w's row, the first N columns denote words that -NCS (\u00b5,1)",
                "cite_spans": [
                    {
                        "start": 89,
                        "end": 116,
                        "text": "(Schone and Jurafsky (2000)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 205,
                        "end": 219,
                        "text": "Sch\u00fctze (1993)",
                        "ref_id": "BIBREF12"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Computing Semantics",
                "sec_num": "3.2"
            },
            {
                "text": "P NCS exp[ ((x \u00b5)/1) 2 ] dx NCS(w 1 ,w 2 ) min k(1,2) cos( w1 , w2 ) \u00b5 k 1 k (1) Pr(NCS) n T -NCS (\u00b5 T ,1 T ) (n R n T ) -NCS (0,1) n T -NCS (\u00b5 T ,1 T ) .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Computing Semantics",
                "sec_num": "3.2"
            },
            {
                "text": "precede w by up to 50 words, and the second N columns represent those words that follow by up to 50 words. Since SVDs are more designed to work then, if there were n items in the ruleset, the with normally-distributed data (Manning and probability that a NCS is non-random is Sch\u00fctze, 1999, p. 565), we fill each entry with a normalized count (or Z-score) rather than straight frequency. We then compute the SVD and keep the top 300 singular values to form semantic vectors for We define Pr (w <w )=Pr(NCS(w ,w )). We As a last comment, one would like to be able to obtain a separate semantic vector for every word (not just those in the top N). SVD computations can be expensive and impractical for large values of N. Yet due to the fact that U and V are orthogonal T matrices, we can start with a matrix of reasonablesized N and \"fold in\" the remaining terms, which is the approach we have followed. For details about folding in terms, the reader is referred to Manning and Sch\u00fctze (1999, p. 563) .",
                "cite_spans": [
                    {
                        "start": 223,
                        "end": 235,
                        "text": "(Manning and",
                        "ref_id": null
                    },
                    {
                        "start": 964,
                        "end": 998,
                        "text": "Manning and Sch\u00fctze (1999, p. 563)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Computing Semantics",
                "sec_num": "3.2"
            },
            {
                "text": "To correlate these semantic vectors, we use normalized cosine scores (NCSs) as we had illustrated before (Schone and Jurafsky (2000) ). The normalized cosine score between two words w 1 and w is determined by first computing cosine 2 values between each word's semantic vector and 200 other randomly selected semantic vectors. This provides a mean (\u00b5) and variance (1 ) of correlation 2 for each word. The NCS is given to be We had previously illustrated NCS values on various PPMVs and showed that this type of score seems to be appropriately identifying semantic relationships. (For example, the PPMVs of car/cars and ally/allies had NCS values of 5.6 and 6.5 respectively, whereas car/cares and ally/all had scored only -0.14 and -1.3.) Further, we showed that by performing this normalizing process, one can estimate the probability that an NCS is random or not. We expect that random NCSs will be approximately normally distributed according to N(0,1). We can also estimate the distribution N(\u00b5 ,1 ) of true correlations and number of terms PPMVs with Pr T , where T is an acceptance sem 5 5",
                "cite_spans": [
                    {
                        "start": 105,
                        "end": 132,
                        "text": "(Schone and Jurafsky (2000)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Correlating Semantic Vectors",
                "sec_num": "3.3"
            },
            {
                "text": "threshold. We showed in our earlier work that T =85% affords high overall precision while still 5 identifying most valid morphological relationships.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Correlating Semantic Vectors",
                "sec_num": "3.3"
            },
            {
                "text": "The first major change to our previous algorithm is an attempt to overcome some of the weaknesses of purely semantic-based morphology induction by incorporating information about affix frequencies.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Augmenting with Affix Frequencies",
                "sec_num": "3.4"
            },
            {
                "text": "As validated by Kazakov (1997) , high frequency word endings and beginnings in inflectional languages are very likely to be legitimate affixes. In English, for example, the highest frequency rule is -s<L. CELEX suggests that 99.7% of our PPMVs for this rule would be true. However, since the purely semantic-based approach tends to select only relationships with contextually similar meanings, only 92% of the PPMVs are retained. This suggests that one might improve the analysis by supplementing semantic probabilities with orthographic-based probabilities (Pr ).",
                "cite_spans": [
                    {
                        "start": 16,
                        "end": 30,
                        "text": "Kazakov (1997)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Augmenting with Affix Frequencies",
                "sec_num": "3.4"
            },
            {
                "text": "orth Our approach to obtaining Pr is motivated by orth an appeal to minimum edit distance (MED). MED has been applied to the morphology induction problem by other researchers (such as Yarowsky and Wicentowski, 2000) . MED determines the minimum-weighted set of insertions, substitutions, and deletions required to transform one word into another. For example, only a single deletion is required to transform \"rates\" into \"rate\" whereas two substitutions and an insertion are required to transform it into \"rating. Since we are free to choose whatever cost function we desire, we can equally choose one whose range",
                "cite_spans": [
                    {
                        "start": 184,
                        "end": 215,
                        "text": "Yarowsky and Wicentowski, 2000)",
                        "ref_id": "BIBREF15"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Augmenting with Affix Frequencies",
                "sec_num": "3.4"
            },
            {
                "text": "Cost(C 1 <C 2 ) 1 2 . f (C 1 <C 2 ) max f (C 1 <Z) ~Z max f (W<C 2 )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Augmenting with Affix Frequencies",
                "sec_num": "3.4"
            },
            {
                "text": "~W lies in the interval of [0,1]. Hence, we can assign Consider Table 2 which is a sample of PPMVs Pr (X<Y) = 1-Cost(X<Y). This calculation implies from the ruleset for \"-s<L\" along with their orth that the orthographic probability that X and Y are probabilities of validity. A validity threshold (T ) of morphological variants is directly derivable from the 85% would mean that the four bottom PPMVs cost of transforming C into C . would be deemed invalid. Yet if we find that the orthographic probability of validity as we would have:",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 70,
                        "end": 71,
                        "text": "2",
                        "ref_id": "TABREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Augmenting with Affix Frequencies",
                "sec_num": "3.4"
            },
            {
                "text": "Figure 3 describes the pseudo-code for an",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "3",
                        "ref_id": "FIGREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Augmenting with Affix Frequencies",
                "sec_num": "3.4"
            },
            {
                "text": "We suppose that orthographic information is less (L) and right-hand (R) sides of each valid PPMV of reliable than semantic information, so we arbitrarily a given ruleset, try to find a collection of words set .=0.5. Now since Pr (X<Y)=1-Cost(C <C ), from the corpus that are collocated with L and R but orth 1 2",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Augmenting with Affix Frequencies",
                "sec_num": "3.4"
            },
            {
                "text": "we can readily combine it with Pr if we assume which occur statistically too many or too few times sem independence using the \"noisy or\" formulation: in these collocations. Such word sets form Pr (valid) = Pr +Pr -(Pr Pr ). ( 2) signatures. Then, determine similar signatures for",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Augmenting with Affix Frequencies",
                "sec_num": "3.4"
            },
            {
                "text": "s-o sem orth sem orth",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Augmenting with Affix Frequencies",
                "sec_num": "3.4"
            },
            {
                "text": "By using this formula, we obtain 3% (absolute) more of the correct PPMVs than semantics alone had provided for the -s<L rule and, as will be shown later, gives reasonable improvements overall.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Augmenting with Affix Frequencies",
                "sec_num": "3.4"
            },
            {
                "text": "Since a primary role of morphology -inflectional morphology in particular -is to convey syntactic information, there is no guarantee that two words that are morphological variants need to share similar semantic properties. This suggests that performance could improve if the induction process took advantage of local, syntactic contexts around words in addition to the more global, large-window contexts used in semantic processing. between the ruleset's signatures and those of the tobe-validated PPMVs to see if they can be validated. Table 3 gives an example of the kinds of contextual words one might expect for the \"-s<L\" rule. In fact, the syntactic signature for \"-s<L\" does indeed include such words as are, other, these, two, were, and have as indicators of words that occur on the left-hand side of the ruleset, and a, an, this, is, has, and A as indicators of the right-hand side. These terms help distinguish plurals from singulars.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 543,
                        "end": 544,
                        "text": "3",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Local Syntactic Context",
                "sec_num": "3.5"
            },
            {
                "text": "Context for R agendas are seas were a legend this formula two red pads pleas have militia is an area these ideas other areas railroad has A guerrilla There is an added benefit from following this approach: it can also be used to find rules that, though different, seem to convey similar information . Table 4 illustrates a number of such agreements. We have yet to take advantage of this feature, but it clearly could be of use for part-ofspeech induction. -s<L -ies<y 83.8 -ed<L -d<L 95.5 -s<L -es<L 79.5 -ing<L -e<L 94.3 -ed<L -ied<y 81.9 -ing<L -ting<L 70.7 ",
                "cite_spans": [
                    {
                        "start": 457,
                        "end": 560,
                        "text": "-s<L -ies<y 83.8 -ed<L -d<L 95.5 -s<L -es<L 79.5 -ing<L -e<L 94.3 -ed<L -ied<y 81.9 -ing<L -ting<L 70.7",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 307,
                        "end": 308,
                        "text": "4",
                        "ref_id": "TABREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Context for L",
                "sec_num": null
            },
            {
                "text": "Despite the semantic, orthographic, and syntactic components of the algorithm, there are still valid PPMVs, (X<Y), that may seem unrelated due to corpus choice or weak distributional properties. However, X and Y may appear as members of other valid PPMVs such as (X<Z) and (Z<Y) containing variants (Z, in this case) which are either semantically or syntactically related to both of the other words. Figure 4 demonstrates this property in greater detail. The words conveyed in Figure 4 are all words from the corpus that have potential relationships between variants of the word \"abuse.\" Links between two words, such as \"abuse\" and \"Abuse,\" are labeled with a weight which is the semantic correlation derived by LSA. Solid lines represent valid relationships with Pr 0.85 and sem dashed lines indicate relationships with lower-thanthreshold scores. The absence of a link suggests that either the potential relationship was never identified or discarded at an earlier stage. Self loops are assumed for each node since clearly each word should be related morphologically to itself. Since there are seven words that are valid morphological relationships of \"abuse,\" we would like to see a complete graph containing 21 solid edges. Yet, only eight connections can be found by semantics alone (Abuse<abuse, abusers<abusing, etc.).",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 407,
                        "end": 408,
                        "text": "4",
                        "ref_id": null
                    },
                    {
                        "start": 484,
                        "end": 485,
                        "text": "4",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Branching Transitive Closure",
                "sec_num": "3.6"
            },
            {
                "text": "However, note that there is a path that can be followed along solid edges from every correct word to every other correct variant. This suggests that taking into consideration link transitivity (i.e., if X<Y , Y <Y , Y <Y ,... and Y <Z, then X<Z) 1 1 2 2 3 t may drastically reduce the number of deletions.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Branching Transitive Closure",
                "sec_num": "3.6"
            },
            {
                "text": "There are two caveats that need to be considered for transitivity to be properly pursued. The first caveat: if no rule exists that would transform X into Z, we will assume that despite the fact that there may be a probabilistic path between the two, we will disregard such a path. The second caveat is that the algorithms we test against. Furthermore, since we will say that paths can only consist of solid CELEX has limited coverage, many of these loweredges, namely each Pr(Y <Y ) on every path must frequency words could not be scored anyway. This i i+1 exceed the specified threshold. cut-off also helps each of the algorithms to obtain Given these constraints, suppose now there is a stronger statistical information on the words they do transitive relation from X to Z by way of some process which means that any observed failures intermediate path ={Y Y Y }. That is, assume cannot be attributed to weak statistics. the unit interval accounting for the number of link associated with \"conduct.\" We will call the words separations, then we will say that the Pr(X<Z) of such a directed graph the conflation set for any of along path has probability . We the words in the graph. Due to the difficulty in i combine the probabilities of all independent paths developing a scoring algorithm to compare directed between X and Z according to Figure 5 : graphs, we will follow our earlier approach and only",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 1348,
                        "end": 1349,
                        "text": "5",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Branching Transitive Closure",
                "sec_num": "3.6"
            },
            {
                "text": "If the returned probability exceeds T , we declare X 5 and Z to be morphological variants of each other.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Branching Transitive Closure",
                "sec_num": "3.6"
            },
            {
                "text": "We compare this improved algorithm to our former algorithm (Schone and Jurafsky (2000) ) as well as to Goldsmith's Linguistica (2000) . We use as input to our system 6.7 million words of English newswire, 2.3 million of German, and 6.7 million of Dutch. Our gold standards are the hand-tagged morphologically-analyzed CELEX lexicon in each of these languages (Baayen, et al., 1993) . We apply the algorithms only to those words of our corpora with frequencies of 10 or more. Obviously this cutoff slightly limits the generality of our results, but it also greatly decreases processing time for all of compare induced conflation sets to those of CELEX. To evaluate, we compute the number of correct (&), inserted (,), and deleted (') words each algorithm predicts for each hypothesized conflation set. If X represents word w's conflation set In making these computations, we disregard any CELEX words absent from our data set and vice versa. Most capital words are not in CELEX so this process also discards them. Hence, we also make an augmented CELEX to incorporate capitalized forms.",
                "cite_spans": [
                    {
                        "start": 59,
                        "end": 86,
                        "text": "(Schone and Jurafsky (2000)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 103,
                        "end": 133,
                        "text": "Goldsmith's Linguistica (2000)",
                        "ref_id": null
                    },
                    {
                        "start": 359,
                        "end": 381,
                        "text": "(Baayen, et al., 1993)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation",
                "sec_num": "4"
            },
            {
                "text": "Table 5 uses the above scoring mechanism to compare the F-Scores (product of precision and recall divided by average of the two ) of our system at a cutoff threshold of 85% to those of our earlier algorithm (\"S/J2000\") at the same threshold; Goldsmith; and a baseline system which performs no analysis (claiming that for any word, its conflation set only consists of itself). The \"S\" and \"C\" columns respectively indicate performance of systems when scoring for suffixing and circumfixing (using the unaugmented CELEX). The \"A\" column shows circumfixing performance using the augmented CELEX. Space limitations required that we illustrate \"A\" scores for one language only, but performance in the other two language is similarly degraded. Boxes are shaded out for algorithms not designed to produce circumfixes.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "5",
                        "ref_id": "TABREF6"
                    }
                ],
                "eq_spans": [],
                "section": "Evaluation",
                "sec_num": "4"
            },
            {
                "text": "Note that each of our additions resulted in an overall improvement which held true across each of the three languages. Furthermore, using ten-fold cross validation on the English data, we find that Fscore differences of the S column are each statistically significant at least at the 95% level. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation",
                "sec_num": "4"
            },
            {
                "text": "We have illustrated three extensions to our earlier morphology induction work (Schone and Jurafsky (2000) ). In addition to induced semantics, we incorporated induced orthographic, syntactic, and transitive information resulting in almost a 20% relative reduction in overall induction error. We have also extended the work by illustrating performance in German and Dutch where, to our knowledge, complete morphology induction performance measures have not previously been obtained. Lastly, we showed a mechanism whereby circumfixes as well as combinations of prefixing and suffixing can be induced in lieu of the suffixonly strategies prevailing in most previous research.",
                "cite_spans": [
                    {
                        "start": 78,
                        "end": 105,
                        "text": "(Schone and Jurafsky (2000)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusions",
                "sec_num": "5"
            },
            {
                "text": "For the future, we expect improvements could be derived by coupling this work, which focuses primarily on inducing regular morphology, with that of Yarowsky and Wicentowski (2000) , who assume some information about regular morphology in order to induce irregular morphology. We also believe that some findings of this work can benefit other areas of linguistic induction, such as part of speech.",
                "cite_spans": [
                    {
                        "start": 148,
                        "end": 179,
                        "text": "Yarowsky and Wicentowski (2000)",
                        "ref_id": "BIBREF15"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusions",
                "sec_num": "5"
            }
        ],
        "back_matter": [
            {
                "text": "The authors wish to thank the anonymous reviewers for their thorough review and insightful comments.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgments",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "The CELEX lexical database",
                "authors": [
                    {
                        "first": "R",
                        "middle": [
                            "H"
                        ],
                        "last": "Baayen",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Piepenbrock",
                        "suffix": ""
                    },
                    {
                        "first": "H",
                        "middle": [],
                        "last": "Van Rijn",
                        "suffix": ""
                    }
                ],
                "year": 1993,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Baayen, R.H., R. Piepenbrock, and H. van Rijn. (1993) The CELEX lexical database (CD-ROM), LDC, Univ. of Pennsylvania, Philadelphia, PA.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Morphemes as necessary concepts for structures: Discovery from untagged corpora. Workshop on paradigms and Grounding in Natural Language Learning",
                "authors": [
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Brent",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [
                            "K"
                        ],
                        "last": "Murthy",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Lundberg ; D\u00e9jean",
                        "suffix": ""
                    },
                    {
                        "first": "H",
                        "middle": [],
                        "last": "Adelaide",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Australia Deerwester",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [
                            "T"
                        ],
                        "last": "Dumais",
                        "suffix": ""
                    },
                    {
                        "first": "G",
                        "middle": [
                            "W"
                        ],
                        "last": "Furnas",
                        "suffix": ""
                    },
                    {
                        "first": "T",
                        "middle": [
                            "K"
                        ],
                        "last": "Landauer",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Harshman",
                        "suffix": ""
                    }
                ],
                "year": 1990,
                "venue": "Proc. Of 5 Int'l Workshop on th Artificial Intelligence and Statistics",
                "volume": "41",
                "issue": "",
                "pages": "391--407",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Brent, M., S. K. Murthy, A. Lundberg. (1995). Discovering morphemic suffixes: A case study in MDL induction. Proc. Of 5 Int'l Workshop on th Artificial Intelligence and Statistics D\u00e9Jean, H. (1998) Morphemes as necessary concepts for structures: Discovery from untagged corpora. Workshop on paradigms and Grounding in Natural Language Learning, pp. 295-299.Adelaide, Australia Deerwester, S., S.T. Dumais, G.W. Furnas, T.K. Landauer, and R. Harshman. (1990) Indexing by Latent Semantic Analysis. Journal of the American Society of Information Science, Vol. 41, pp.391-407.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Unsupervised learning of derivational morphology from inflectional lexicons",
                "authors": [
                    {
                        "first": "\u00c9",
                        "middle": [],
                        "last": "Gaussier",
                        "suffix": ""
                    }
                ],
                "year": 1999,
                "venue": "ACL '99 Workshop: Unsupervised Learning in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Gaussier, \u00c9. (1999) Unsupervised learning of derivational morphology from inflectional lexicons. ACL '99 Workshop: Unsupervised Learning in Natural Language Processing, Univ. of Maryland.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Unsupervised learning of the morphology of a natural language",
                "authors": [
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Goldsmith",
                        "suffix": ""
                    }
                ],
                "year": 1997,
                "venue": "Univ. of Chicago",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Goldsmith, J. (1997/2000) Unsupervised learning of the morphology of a natural language. Univ. of Chicago. http://humanities.uchicago.edu/faculty/goldsmith.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Acquisition automatique de connaissances morphologiques sur le vocabulaire m\u00e9dical",
                "authors": [
                    {
                        "first": "N",
                        "middle": [],
                        "last": "Grabar",
                        "suffix": ""
                    },
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Zweigenbaum",
                        "suffix": ""
                    }
                ],
                "year": 1999,
                "venue": "TALN",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Grabar, N. and P. Zweigenbaum. (1999) Acquisition automatique de connaissances morphologiques sur le vocabulaire m\u00e9dical, TALN, Carg\u00e9se, France.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Structural Linguistics",
                "authors": [
                    {
                        "first": "Z",
                        "middle": [],
                        "last": "Harris",
                        "suffix": ""
                    }
                ],
                "year": 1951,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Harris, Z. (1951) Structural Linguistics. University of Chicago Press.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Guessing morphology from terms and corpora",
                "authors": [
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Jacquemin",
                        "suffix": ""
                    }
                ],
                "year": 1997,
                "venue": "SIGIR'97",
                "volume": "",
                "issue": "",
                "pages": "156--167",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jacquemin, C. (1997) Guessing morphology from terms and corpora. SIGIR'97, pp. 156-167, Philadelphia, PA.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Unsupervised learning of na\u00efve morphology with genetic algorithms",
                "authors": [
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Kazakov",
                        "suffix": ""
                    }
                ],
                "year": 1997,
                "venue": "ECML/Mlnet Workshop on Empirical Learning of Natural Language Processing Tasks",
                "volume": "",
                "issue": "",
                "pages": "105--111",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kazakov, D. (1997) Unsupervised learning of na\u00efve morphology with genetic algorithms. In W. Daelemans, et al., eds., ECML/Mlnet Workshop on Empirical Learning of Natural Language Processing Tasks, Prague, pp. 105-111.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Introduction to Latent Semantic Analysis",
                "authors": [
                    {
                        "first": "T",
                        "middle": [
                            "K"
                        ],
                        "last": "Landauer",
                        "suffix": ""
                    },
                    {
                        "first": "P",
                        "middle": [
                            "W"
                        ],
                        "last": "Foltz",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Laham",
                        "suffix": ""
                    }
                ],
                "year": 1998,
                "venue": "Discourse Processes",
                "volume": "25",
                "issue": "",
                "pages": "259--284",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Landauer, T.K., P.W. Foltz, and D. Laham. (1998) Introduction to Latent Semantic Analysis. Discourse Processes. Vol. 25, pp. 259-284.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Foundations of Statistical Natural Language Processing",
                "authors": [
                    {
                        "first": "C",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    },
                    {
                        "first": "H",
                        "middle": [],
                        "last": "Sch\u00fctze",
                        "suffix": ""
                    }
                ],
                "year": 1999,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Manning, C.D. and H. Sch\u00fctze. (1999) Foundations of Statistical Natural Language Processing, MIT Press, Cambridge, MA.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Where defaults don't help: the case of the German plural system",
                "authors": [
                    {
                        "first": "R",
                        "middle": [
                            "C"
                        ],
                        "last": "Nakisa",
                        "suffix": ""
                    },
                    {
                        "first": "U",
                        "middle": [],
                        "last": "Hahn",
                        "suffix": ""
                    }
                ],
                "year": 1996,
                "venue": "Proc. of the 18th Conference of the Cognitive Science Society",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Nakisa, R.C., U.Hahn. (1996) Where defaults don't help: the case of the German plural system. Proc. of the 18th Conference of the Cognitive Science Society.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Knowledge-free induction of morphology using latent semantic analysis",
                "authors": [
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Schone",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Jurafsky",
                        "suffix": ""
                    }
                ],
                "year": 2000,
                "venue": "Proc. of the Computational Natural Language Learning Conference",
                "volume": "",
                "issue": "",
                "pages": "67--72",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Schone, P. and D. Jurafsky. (2000) Knowledge-free induction of morphology using latent semantic analysis. Proc. of the Computational Natural Language Learning Conference, Lisbon, pp. 67-72.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Distributed syntactic representations with an application to part-of-speech tagging",
                "authors": [
                    {
                        "first": "H",
                        "middle": [],
                        "last": "Sch\u00fctze",
                        "suffix": ""
                    }
                ],
                "year": 1993,
                "venue": "Proceedings of the IEEE International Conference on Neural Networks",
                "volume": "",
                "issue": "",
                "pages": "1504--1509",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sch\u00fctze, H. (1993) Distributed syntactic representations with an application to part-of-speech tagging. Proceedings of the IEEE International Conference on Neural Networks, pp. 1504-1509.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Morphology and Computation",
                "authors": [
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Sproat",
                        "suffix": ""
                    }
                ],
                "year": 1992,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sproat, R. (1992) Morphology and Computation. MIT Press, Cambridge, MA.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Corpus-based stemming using co-occurrence of word variants",
                "authors": [
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "B",
                        "middle": [
                            "W"
                        ],
                        "last": "Croft",
                        "suffix": ""
                    }
                ],
                "year": 1998,
                "venue": "ACM Transactions on Information Systems",
                "volume": "16",
                "issue": "1",
                "pages": "61--81",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xu, J., B.W. Croft. (1998) Corpus-based stemming using co-occurrence of word variants. ACM Transactions on Information Systems, 16 (1), pp. 61-81.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Minimally supervised morphological analysis by multimodal alignment",
                "authors": [
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Yarowsky",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Wicentowski",
                        "suffix": ""
                    }
                ],
                "year": 2000,
                "venue": "Proc. of the ACL 2000",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yarowsky, D. and R. Wicentowski. (2000) Minimally supervised morphological analysis by multimodal alignment. Proc. of the ACL 2000, Hong Kong.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 1: Strategy and evaluation",
                "uris": null,
                "fig_num": "12",
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "each word. Word w would be assigned the semantic choose to accept as valid relationships only those vector U D , where U represents the row of W= w k w U corresponding to w and D indicates that only the k top k diagonal entries of D have been preserved.",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF3": {
                "num": null,
                "text": "remaining is how to determine local contexts of these low-scoring word pairs Cost(C <C ). This cost should depend on a number match the contexts of other PPMVs having high 1 2 of factors: the frequency of the rule f(C <C ), the scores (i.e., those whose scores exceed T ), then 1 2 reliability of the metric in comparison to that of their probabilities of validity should increase. If we semantics (., where . [0,1]), and the frequencies could compute a syntax-based probability for these of other rules involving C and C . We define the words, namely Pr , then assuming independence 1 2",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF4": {
                "num": null,
                "text": "Figure 3: Pseudo-code to find Probability syntax Figure 4: Semantic strengths",
                "uris": null,
                "fig_num": "3",
                "type_str": "figure"
            },
            "FIGREF5": {
                "num": null,
                "text": "Figure 5: Pseudocode for Branching Probability",
                "uris": null,
                "fig_num": "56",
                "type_str": "figure"
            },
            "TABREF1": {
                "content": "<table><tr><td>Rank ENGLISH GERMAN</td><td>DUTCH</td></tr><tr><td>1</td><td/></tr></table>",
                "type_str": "table",
                "text": "Outputs of the trie stage: potential rules",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table><tr><td colspan=\"4\">Word+s Word Pr Word+s Word</td><td>Pr</td></tr><tr><td colspan=\"5\">agendas agenda .968 legends legend .981</td></tr><tr><td>ideas</td><td colspan=\"4\">idea .974 militias militia 1.00</td></tr><tr><td>pleas</td><td colspan=\"4\">plea 1.00 guerrillas guerrilla 1.00</td></tr><tr><td>seas</td><td>sea</td><td colspan=\"3\">1.00 formulas formula 1.00</td></tr><tr><td>areas</td><td colspan=\"4\">area 1.00 railroads railroad 1.00</td></tr><tr><td>Areas</td><td colspan=\"2\">Area .721 pads</td><td>pad</td><td>.731</td></tr><tr><td colspan=\"3\">Vegas Vega .641 feeds</td><td>feed</td><td>.543</td></tr></table>",
                "type_str": "table",
                "text": "Sample probabilities for \"-s<L\"",
                "html": null,
                "num": null
            },
            "TABREF4": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Relations amongst rules Rule Relative Cos Rule Relative Cos",
                "html": null,
                "num": null
            },
            "TABREF6": {
                "content": "<table><tr><td>Algorithms</td><td colspan=\"2\">English</td><td colspan=\"2\">German</td><td colspan=\"2\">Dutch</td></tr><tr><td/><td>S</td><td colspan=\"2\">C A S</td><td>C</td><td>S</td><td>C</td></tr><tr><td>None</td><td colspan=\"6\">62.8 59.9 51.7 75.8 63.0 74.2 70.0</td></tr><tr><td colspan=\"2\">Goldsmith 81.8</td><td/><td>84.0</td><td/><td>75.8</td><td/></tr><tr><td colspan=\"2\">S/J2000 85.2</td><td/><td>88.3</td><td/><td>82.2</td><td/></tr><tr><td colspan=\"7\">+orthogrph 85.7 82.2 76.9 89.3 76.1 84.5 78.9</td></tr><tr><td colspan=\"7\">+ syntax 87.5 84.0 79.0 91.6 78.2 85.6 79.4</td></tr><tr><td>+ transitive</td><td>88.1</td><td>84.5 79.7</td><td>92.3</td><td>78.9</td><td>85.8</td><td>79.6</td></tr></table>",
                "type_str": "table",
                "text": "Computation of F-Scores",
                "html": null,
                "num": null
            }
        }
    }
}