{
    "paper_id": "P14-1121",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:12:48.664516Z"
    },
    "title": "Looking at Unbalanced Specialized Comparable Corpora for Bilingual Lexicon Extraction",
    "authors": [
        {
            "first": "Emmanuel",
            "middle": [],
            "last": "Morin",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "LINA UMR CNRS",
                "location": {
                    "addrLine": "6241 2 rue de la houssini\u00e8re, BP 92208",
                    "postCode": "44322, Cedex 03",
                    "settlement": "Nantes",
                    "country": "France"
                }
            },
            "email": "emmanuel.morin@univ-nantes.fr"
        },
        {
            "first": "Amir",
            "middle": [],
            "last": "Hazem",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "LINA UMR CNRS",
                "location": {
                    "addrLine": "6241 2 rue de la houssini\u00e8re, BP 92208",
                    "postCode": "44322, Cedex 03",
                    "settlement": "Nantes",
                    "country": "France"
                }
            },
            "email": "amir.hazem@univ-nantes.fr"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "The main work in bilingual lexicon extraction from comparable corpora is based on the implicit hypothesis that corpora are balanced. However, the historical contextbased projection method dedicated to this task is relatively insensitive to the sizes of each part of the comparable corpus. Within this context, we have carried out a study on the influence of unbalanced specialized comparable corpora on the quality of bilingual terminology extraction through different experiments. Moreover, we have introduced a regression model that boosts the observations of word cooccurrences used in the context-based projection method. Our results show that the use of unbalanced specialized comparable corpora induces a significant gain in the quality of extracted lexicons.",
    "pdf_parse": {
        "paper_id": "P14-1121",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "The main work in bilingual lexicon extraction from comparable corpora is based on the implicit hypothesis that corpora are balanced. However, the historical contextbased projection method dedicated to this task is relatively insensitive to the sizes of each part of the comparable corpus. Within this context, we have carried out a study on the influence of unbalanced specialized comparable corpora on the quality of bilingual terminology extraction through different experiments. Moreover, we have introduced a regression model that boosts the observations of word cooccurrences used in the context-based projection method. Our results show that the use of unbalanced specialized comparable corpora induces a significant gain in the quality of extracted lexicons.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "The bilingual lexicon extraction task from bilingual corpora was initially addressed by using parallel corpora (i.e. a corpus that contains source texts and their translation). However, despite good results in the compilation of bilingual lexicons, parallel corpora are scarce resources, especially for technical domains and for language pairs not involving English. For these reasons, research in bilingual lexicon extraction has focused on another kind of bilingual corpora comprised of texts sharing common features such as domain, genre, sampling period, etc. without having a source text/target text relationship (McEnery and Xiao, 2007) . These corpora, well known now as comparable corpora, have also initially been introduced as non-parallel corpora (Fung, 1995; Rapp, 1995) , and non-aligned corpora (Tanaka and Iwasaki, 1996) . According to Fung and Che-ung (2004) , who range bilingual corpora from parallel corpora to quasi-comparable corpora going through comparable corpora, there is a continuum from parallel to comparable corpora (i.e. a kind of filiation).",
                "cite_spans": [
                    {
                        "start": 618,
                        "end": 642,
                        "text": "(McEnery and Xiao, 2007)",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 758,
                        "end": 770,
                        "text": "(Fung, 1995;",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 771,
                        "end": 782,
                        "text": "Rapp, 1995)",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 809,
                        "end": 835,
                        "text": "(Tanaka and Iwasaki, 1996)",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 851,
                        "end": 874,
                        "text": "Fung and Che-ung (2004)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The bilingual lexicon extraction task from comparable corpora inherits this filiation. For instance, the historical context-based projection method (Fung, 1995; Rapp, 1995) , known as the standard approach, dedicated to this task seems implicitly to lead to work with balanced comparable corpora in the same way as for parallel corpora (i.e. each part of the corpus is composed of the same amount of data).",
                "cite_spans": [
                    {
                        "start": 148,
                        "end": 160,
                        "text": "(Fung, 1995;",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 161,
                        "end": 172,
                        "text": "Rapp, 1995)",
                        "ref_id": "BIBREF27"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this paper we want to show that the assumption that comparable corpora should be balanced for bilingual lexicon extraction task is unfounded. Moreover, this assumption is prejudicial for specialized comparable corpora, especially when involving the English language for which many documents are available due the prevailing position of this language as a standard for international scientific publications. Within this context, our main contribution consists in a re-reading of the standard approach putting emphasis on the unfounded assumption of the balance of the specialized comparable corpora. In specialized domains, the comparable corpora are traditionally of small size (around 1 million words) in comparison with comparable corpus-based general language (up to 100 million words). Consequently, the observations of word co-occurrences which is the basis of the standard approach are unreliable. To make them more reliable, our second contribution is to contrast different regression models in order to boost the observations of word co-occurrences. This strategy allows to improve the quality of extracted bilingual lexicons from comparable corpora.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this section, we first describe the standard approach that deals with the task of bilingual lexicon extraction from comparable corpora. We then present an extension of this approach based on regression models. Finally, we discuss works related to this study.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Bilingual Lexicon Extraction",
                "sec_num": "2"
            },
            {
                "text": "The main work in bilingual lexicon extraction from comparable corpora is based on lexical context analysis and relies on the simple observation that a word and its translation tend to appear in the same lexical contexts. The basis of this observation consists in the identification of \"first-order affinities\" for each source and target language: \"First-order affinities describe what other words are likely to be found in the immediate vicinity of a given word\" (Grefenstette, 1994, p. 279) . These affinities can be represented by context vectors, and each vector element represents a word which occurs within the window of the word to be translated (e.g. a seven-word window approximates syntactic dependencies). In order to emphasize significant words in the context vector and to reduce word-frequency effects, the context vectors are normalized according to an association measure. Then, the translation is obtained by comparing the source context vector to each translation candidate vector after having translated each element of the source vector with a general dictionary.",
                "cite_spans": [
                    {
                        "start": 463,
                        "end": 491,
                        "text": "(Grefenstette, 1994, p. 279)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Standard Approach",
                "sec_num": "2.1"
            },
            {
                "text": "The implementation of the standard approach can be carried out by applying the following three steps (Rapp, 1999; Chiao and Zweigenbaum, 2002; D\u00e9jean et al., 2002; Morin et al., 2007; Laroche and Langlais, 2010, among others) :",
                "cite_spans": [
                    {
                        "start": 101,
                        "end": 113,
                        "text": "(Rapp, 1999;",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 114,
                        "end": 142,
                        "text": "Chiao and Zweigenbaum, 2002;",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 143,
                        "end": 163,
                        "text": "D\u00e9jean et al., 2002;",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 164,
                        "end": 183,
                        "text": "Morin et al., 2007;",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 184,
                        "end": 225,
                        "text": "Laroche and Langlais, 2010, among others)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Standard Approach",
                "sec_num": "2.1"
            },
            {
                "text": "Computing context vectors We collect all the words in the context of each word i and count their occurrence frequency in a window of n words around i. For each word i of the source and the target languages, we obtain a context vector v i which gathers the set of co-occurrence words j associated with the number of times that j and i occur together cooc(i, j). In order to identify specific words in the lexical context and to reduce wordfrequency effects, we normalize context vectors using an association score such as Mutual Information, Log-likelihood, or the discounted log-odds (LO) (Evert, 2005) (see equation 1 and Table 1 where",
                "cite_spans": [
                    {
                        "start": 589,
                        "end": 602,
                        "text": "(Evert, 2005)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 629,
                        "end": 630,
                        "text": "1",
                        "ref_id": "TABREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Standard Approach",
                "sec_num": "2.1"
            },
            {
                "text": "N = a + b + c + d).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Standard Approach",
                "sec_num": "2.1"
            },
            {
                "text": "Transferring context vectors Using a bilingual dictionary, we translate the elements of the source context vector. If the bilingual dictionary provides several translations for an element, we consider all of them but weight the different translations according to their frequency in the target language.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Standard Approach",
                "sec_num": "2.1"
            },
            {
                "text": "Finding candidate translations For a word to be translated, we compute the similarity between the translated context vector and all target vectors through vector distance measures such as Jaccard or Cosine (see equation 2 where assoc i j stands for \"association score\", v k is the transferred context vector of the word k to translate, and v l is the context vector of the word l in the target language). Finally, the candidate translations of a word are the target words ranked following the similarity score. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Standard Approach",
                "sec_num": "2.1"
            },
            {
                "text": "j \u00acj i a = cooc(i, j) b = cooc(i, \u00acj) \u00aci c = cooc(\u00aci, j) d = cooc(\u00aci, \u00acj)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Standard Approach",
                "sec_num": "2.1"
            },
            {
                "text": "LO(i, j) = log (a + 1 2 ) \u00d7 (d + 1 2 ) (b + 1 2 ) \u00d7 (c + 1 2 ) (1) Cosine v k v l = \u2211 t assoc l t assoc k t \u221a \u2211 t assoc l t 2 \u221a \u2211 t assoc k t 2 (2)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Standard Approach",
                "sec_num": "2.1"
            },
            {
                "text": "This approach is sensitive to the choice of parameters such as the size of the context, the choice of the association and similarity measures. The most complete study about the influence of these parameters on the quality of word alignment has been carried out by Laroche and Langlais (2010) .",
                "cite_spans": [
                    {
                        "start": 264,
                        "end": 291,
                        "text": "Laroche and Langlais (2010)",
                        "ref_id": "BIBREF17"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Standard Approach",
                "sec_num": "2.1"
            },
            {
                "text": "The standard approach is used by most researchers so far (Rapp, 1995; Fung, 1998; Peters and Picchi, 1998; Rapp, 1999; Chiao and Zweigenbaum, 2002; D\u00e9jean et al., 2002; Gaussier et al., 2004; Morin et al., 2007; Laroche and Langlais, 2010; Prochasson and Fung, 2011 \"Therefore, in relation to parallel corpora, it is more likely for comparable corpora to be designed as general balanced corpora.\". For instance, Table 2 describes the comparable corpora used in the main work dedicated to bilingual lexicon extraction for which the ratio between the size of the source and the target texts is comprised between 1 and 1.8. In fact, the assumption that words which have the same meaning in different languages should have the same lexical context distributions does not involve working with balanced comparable corpora. To our knowledge, no attention1 has been paid to the problem of using unbalanced comparable corpora for bilingual lexicon extraction. Since the context vectors are computed from each part of the comparable corpus rather than through the parts of the comparable corpora, the standard approach is relatively insensitive to differences in corpus sizes. The only precaution for using the standard approach with unbalanced corpora is to normalize the association measure (for instance, this can be done by dividing each entry of a given context vector by the sum of its association scores).",
                "cite_spans": [
                    {
                        "start": 57,
                        "end": 69,
                        "text": "(Rapp, 1995;",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 70,
                        "end": 81,
                        "text": "Fung, 1998;",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 82,
                        "end": 106,
                        "text": "Peters and Picchi, 1998;",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 107,
                        "end": 118,
                        "text": "Rapp, 1999;",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 119,
                        "end": 147,
                        "text": "Chiao and Zweigenbaum, 2002;",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 148,
                        "end": 168,
                        "text": "D\u00e9jean et al., 2002;",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 169,
                        "end": 191,
                        "text": "Gaussier et al., 2004;",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 192,
                        "end": 211,
                        "text": "Morin et al., 2007;",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 212,
                        "end": 239,
                        "text": "Laroche and Langlais, 2010;",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 240,
                        "end": 265,
                        "text": "Prochasson and Fung, 2011",
                        "ref_id": "BIBREF25"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Standard Approach",
                "sec_num": "2.1"
            },
            {
                "text": "Since comparable corpora are usually small in specialized domains (see Table 2 ), the discrimina-tive power of context vectors (i.e. the observations of word co-occurrences) is reduced. One way to deal with this problem is to re-estimate co-occurrence counts by a prediction function (Hazem and Morin, 2013) . This consists in assigning to each observed co-occurrence count of a small comparable corpora, a new value learned beforehand from a large training corpus.",
                "cite_spans": [
                    {
                        "start": 284,
                        "end": 307,
                        "text": "(Hazem and Morin, 2013)",
                        "ref_id": "BIBREF14"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 77,
                        "end": 78,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Prediction Model",
                "sec_num": "2.2"
            },
            {
                "text": "In order to make co-occurrence counts more discriminant and in the same way as Hazem and Morin (2013) , one strategy consists in addressing this problem through regression: given training corpora of small and large size (abundant in the general domain), we predict word cooccurrence counts in order to make them more reliable. We then apply the resulting regression function to each word co-occurrence count as a pre-processing step of the standard approach. Our work differs from Hazem and Morin (2013) in two ways. First, while they experienced the linear regression model, we propose to contrast different regression models. Second, we apply regression to unbalanced comparable corpora and study the impact of prediction when applied to the source texts, the target texts and both source and target texts of the used comparable corpora.",
                "cite_spans": [
                    {
                        "start": 79,
                        "end": 101,
                        "text": "Hazem and Morin (2013)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 481,
                        "end": 503,
                        "text": "Hazem and Morin (2013)",
                        "ref_id": "BIBREF14"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Prediction Model",
                "sec_num": "2.2"
            },
            {
                "text": "We use regression analysis to describe the relationship between word co-occurrence counts in a large corpus (the response variable) and word cooccurrence counts in a small corpus (the predictor variable). As most regression models have already been described in great detail (Christensen, 1997; Agresti, 2007) , the derivation of most models is only briefly introduced in this work.",
                "cite_spans": [
                    {
                        "start": 275,
                        "end": 294,
                        "text": "(Christensen, 1997;",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 295,
                        "end": 309,
                        "text": "Agresti, 2007)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Prediction Model",
                "sec_num": "2.2"
            },
            {
                "text": "As we can not claim that the prediction of word co-occurrence counts is a linear problem, we consider in addition to the simple linear regression model (Lin), a generalized linear model which is the logistic regression model (Logit) and non linear regression models such as polynomial regression model (P oly n ) of order n. Given an input vector x \u2208 R m , where x 1 ,...,x m represent features, we find a prediction \u0177 \u2208 R m for the cooccurrence count of a couple of words y \u2208 R using one of the regression models presented below:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Prediction Model",
                "sec_num": "2.2"
            },
            {
                "text": "\u0177Lin = \u03b2 0 + \u03b2 1 x (3) \u0177Logit = 1 1 + exp(-(\u03b2 0 + \u03b2 1 x))",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Prediction Model",
                "sec_num": "2.2"
            },
            {
                "text": "(4)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Prediction Model",
                "sec_num": "2.2"
            },
            {
                "text": "\u0177P oly n = \u03b2 0 + \u03b2 1 x + \u03b2 2 x 2 + ... + \u03b2 n x n (5)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Prediction Model",
                "sec_num": "2.2"
            },
            {
                "text": "where \u03b2 i are the parameters to estimate. Let us denote by f the regression function and by cooc(w i , w j ) the co-occurrence count of the words w i and w j . The resulting predicted value of cooc(w i , w j ), noted \u0109ooc(w i , w j ) is given by the following equation:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Prediction Model",
                "sec_num": "2.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "\u0109ooc(w i , w j ) = f (cooc(w i , w j ))",
                        "eq_num": "(6)"
                    }
                ],
                "section": "Prediction Model",
                "sec_num": "2.2"
            },
            {
                "text": "In the past few years, several contributions have been proposed to improve each step of the standard approach. Prochasson et al. (2009) enhance the representativeness of the context vector by strengthening the context words that happen to be transliterated words and scientific compound words in the target language. Ismail and Manandhar (2010) also suggest that context vectors should be based on the most important contextually relevant words (indomain terms), and thus propose a method for filtering the noise of the context vectors. In another way, Rubino and Linar\u00e8s (2011) improve the context words based on the hypothesis that a word and its candidate translations share thematic similarities. Yu and Tsujii (2009) and Otero (2007) propose, for their part, to replace the window-based method by a syntax-based method in order to improve the representation of the lexical context.",
                "cite_spans": [
                    {
                        "start": 111,
                        "end": 135,
                        "text": "Prochasson et al. (2009)",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 701,
                        "end": 721,
                        "text": "Yu and Tsujii (2009)",
                        "ref_id": "BIBREF31"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2.3"
            },
            {
                "text": "To improve the transfer context vectors step, and increase the number of elements of translated context vectors, Chiao and Zweigenbaum (2003) and Morin and Prochasson (2011) combine a standard general language dictionary with a specialized dictionary, whereas D\u00e9jean et al. (2002) use the hierarchical properties of a specialized thesaurus. Koehn and Knight (2002) automatically induce the initial seed bilingual dictionary by using identical spelling features such as cognates and similar contexts. As regards the problem of words ambiguities, Bouamor et al. (2013) carried out word sense disambiguation process only in the target language whereas Gaussier et al. (2004) solve the problem through the source and target languages by using approaches based on CCA (Canonical Correlation Analysis) and multilingual PLSA (Probabilistic Latent Semantic Analysis).",
                "cite_spans": [
                    {
                        "start": 113,
                        "end": 141,
                        "text": "Chiao and Zweigenbaum (2003)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 146,
                        "end": 173,
                        "text": "Morin and Prochasson (2011)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 260,
                        "end": 280,
                        "text": "D\u00e9jean et al. (2002)",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 341,
                        "end": 364,
                        "text": "Koehn and Knight (2002)",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 545,
                        "end": 566,
                        "text": "Bouamor et al. (2013)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 649,
                        "end": 671,
                        "text": "Gaussier et al. (2004)",
                        "ref_id": "BIBREF12"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2.3"
            },
            {
                "text": "The rank of candidate translations can be improved by integrating different heuristics. For instance, Chiao and Zweigenbaum (2002) introduce a heuristic based on word distribution symmetry. From the ranked list of candidate translations, the standard approach is applied in the reverse direction to find the source counterparts of the first target candidate translations. And then only the target candidate translations that had the initial source word among the first reverse candidate translations are kept. Laroche and Langlais (2010) suggest a heuristic based on the graphic similarity between source and target terms. Here, candidate translations which are cognates of the word to be translated are ranked first among the list of translation candidates.",
                "cite_spans": [
                    {
                        "start": 102,
                        "end": 130,
                        "text": "Chiao and Zweigenbaum (2002)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 510,
                        "end": 537,
                        "text": "Laroche and Langlais (2010)",
                        "ref_id": "BIBREF17"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2.3"
            },
            {
                "text": "In this section, we outline the different textual resources used for our experiments: the comparable corpora, the bilingual dictionary and the terminology reference lists.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Linguistic Resources",
                "sec_num": "3"
            },
            {
                "text": "For our experiments, we used two specialized French/English comparable corpora:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Specialized Comparable Corpora",
                "sec_num": "3.1"
            },
            {
                "text": "Breast cancer corpus This comparable corpus is composed of documents collected from the Elsevier website 2 . The documents were taken from the medical domain within the subdomain of \"breast cancer\". We have automatically selected the documents published between 2001 and 2008 where the title or the keywords contain the term cancer du sein in French and breast cancer in English. We collected 130 French documents (about 530,000 words) and 1,640 English documents (about 7.4 million words). We split the English documents into 14 parts each containing about 530,000 words.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Specialized Comparable Corpora",
                "sec_num": "3.1"
            },
            {
                "text": "The documents making up the French part of the comparable corpus have been craweled from the web using three keywords: diab\u00e8te (diabetes), alimentation (food), and ob\u00e9sit\u00e9 (obesity). After a manual selection, we only kept the documents which were relative to the medical domain.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Diabetes corpus",
                "sec_num": null
            },
            {
                "text": "As a result, 65 French documents were extracted (about 257,000 words). The English part has been extracted from the medical website PubMed 3 using the keywords: diabetes, nutrition and feeding. We only kept the free fulltext available documents. As a result, 2,339 English documents were extracted (about 3,5 million words). We also split the English documents into 14 parts each containing about 250,000 words.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Diabetes corpus",
                "sec_num": null
            },
            {
                "text": "The French and English documents were then normalised through the following linguistic preprocessing steps: tokenisation, part-of-speech tagging, and lemmatisation. These steps were carried out using the TTC TermSuite 4 that applies the same method to several languages including French and English. Finally, the function words were removed and the words occurring less than twice in the French part and in each English part were discarded. Table 3 shows the number of distinct words (# words) after these steps. It also indicates the comparability degree in percentage (comp.) between the French part and each English part of each comparable corpus. The comparability measure (Li and Gaussier, 2010) is based on the expectation of finding the translation for each word in the corpus and gives a good idea about how two corpora are comparable. We can notice that all the comparable corpora have a high degree of comparability with a better comparability of the breast cancer corpora as opposed to the diabetes corpora. In the remainder of this article, [breast cancer corpus i] for instance stands for the breast cancer comparable corpus composed of the unique French part and the English part i (i \u2208 [1, 14]).",
                "cite_spans": [
                    {
                        "start": 677,
                        "end": 700,
                        "text": "(Li and Gaussier, 2010)",
                        "ref_id": "BIBREF18"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 447,
                        "end": 448,
                        "text": "3",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Diabetes corpus",
                "sec_num": null
            },
            {
                "text": "The bilingual dictionary used in our experiments is the French/English dictionary ELRA-M0033 available from the ELRA catalogue5 . This resource is a general language dictionary which contains only a few terms related to the medical domain.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Bilingual Dictionary",
                "sec_num": "3.2"
            },
            {
                "text": "To evaluate the quality of terminology extraction, we built a bilingual terminology reference list for each comparable corpus. We selected all French/English single words from the UMLS6 meta-thesaurus. We kept only i) the French single words which occur more than four times in the French part and ii) the English single words which occur more than four times in each English part i7 . As a result of filtering, 169 French/English single words were extracted for the breast cancer corpus and 244 French/English single words were extracted for the diabetes corpus. It should be noted that the evaluation of terminology extraction using specialized comparable corpora of- Table 4 : Results (MAP %) of the standard approach using the balanced and unbalanced comparable corpora ten relies on lists of a small size: 95 single words in Chiao and Zweigenbaum (2002) , 100 in Morin et al. (2007) , 125 and 79 in Bouamor et al. (2013) .",
                "cite_spans": [
                    {
                        "start": 830,
                        "end": 858,
                        "text": "Chiao and Zweigenbaum (2002)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 868,
                        "end": 887,
                        "text": "Morin et al. (2007)",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 904,
                        "end": 925,
                        "text": "Bouamor et al. (2013)",
                        "ref_id": "BIBREF1"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 676,
                        "end": 677,
                        "text": "4",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Terminology Reference Lists",
                "sec_num": "3.3"
            },
            {
                "text": "In this section, we present experiments to evaluate the influence of comparable corpus size and prediction models on the quality of bilingual terminology extraction. We present the results obtained for the terms belonging to the reference list for English to French direction measured in terms of the Mean Average Precision (MAP) (Manning et al., 2008) as follows:",
                "cite_spans": [
                    {
                        "start": 330,
                        "end": 352,
                        "text": "(Manning et al., 2008)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments and Results",
                "sec_num": "4"
            },
            {
                "text": "M AP (Ref ) = 1 |Ref | |Ref | \u2211 i=1 1 r i (7)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments and Results",
                "sec_num": "4"
            },
            {
                "text": "where |Ref | is the number of terms of the reference list and r i the rank of the correct candidate translation i.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments and Results",
                "sec_num": "4"
            },
            {
                "text": "In order to evaluate the influence of corpus size on the bilingual terminology extraction task, two experiments have been carried out using the standard approach. We first performed an experiment using each comparable corpus independently of the others (we refer to these corpora as balanced corpora).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Standard Approach Evaluation",
                "sec_num": "4.1"
            },
            {
                "text": "We then conducted a second experiment where we varied the size of the English part of the comparable corpus, from 530,000 to 7.4 million words for the breast cancer corpus in 530,000 words steps, and from 250,000 to 3.5 million words for the diabetes corpus in 250,000 words steps (we refer to these corpora as unbalanced corpora). In the experiments reported here, the size of the context window w was set to 3 (i.e. a seven-word window that approximates syntactic dependencies), the retained association and similarity measures were the discounted log-odds and the Cosine (see Section 2.1). The results shown were those that give the best performance for the comparable corpora used individually.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Standard Approach Evaluation",
                "sec_num": "4.1"
            },
            {
                "text": "Table 4 shows the results of the standard approach on the balanced and the unbalanced breast cancer and diabetes comparable corpora. Each column corresponds to the English part i (i \u2208 [1, 14]) of a given comparable corpus. The first line presents the results for each individual comparable corpus and the second line presents the results for the cumulative comparable corpus. For instance, the column 3 indicates the MAP obtained by using a comparable corpus that is composed i) only of [breast cancer corpus 3] (MAP of 21.0%), and ii) of [breast cancer corpus 1, 2 and 3] (MAP of 34.7%).",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "4",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Standard Approach Evaluation",
                "sec_num": "4.1"
            },
            {
                "text": "As a preliminary remark, we can notice that the results differ noticeably according to the comparable corpus used individually (MAP variation between 21.0% and 29.6% for the breast cancer corpora and between 10.5% and 16.5% for the diabetes corpora). We can also note that the MAP of all the unbalanced comparable corpora is always higher than any individual comparable corpus. Overall, starting with a MAP of 26.1% as provided by the balanced [breast cancer corpus 1], we are able to increase it to 42.3% with the unbalanced [breast cancer corpus 12] (the variation observed for some unbalanced corpora such as [diabetes corpus 12, 13 and 14] can be explained by the fact that adding more data in the source language increases the error rate of the translation phase of the standard approach, which leads to the introduction of additional noise in the translated context vectors). Table 5 : Results (MAP %) of the standard approach using the Lin regression model on the balanced breast cancer and diabetes corpora (comparison of predicting the source side, the target side and both sides of the comparable corpora)",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 888,
                        "end": 889,
                        "text": "5",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Standard Approach Evaluation",
                "sec_num": "4.1"
            },
            {
                "text": "The aim of this experiment is two-fold: first, we want to evaluate the usefulness of predicting word co-occurrence counts and second, we want to find out whether it is more appropriate to apply prediction to the source side, the target side or both sides of the bilingual comparable corpora. Table 6 shows a comparison between the standard approach without prediction noted N o prediction and the standard approach with prediction models. We contrast the simple linear regression model (Lin) with the second and the third order polynomial regressions (P oly 2 and P oly 3 ) and the logistic regression model (Logit). We can notice that except for the Logit model, all the regression models outperform the baseline (N o prediction). Also, as we can see, the results obtained with the linear and polynomial regressions are very close. This suggests that both linear and polynomial regressions are suitable as a preprocessing step of the standard approach, while the logistic regression seems to be inappropriate according to the results shown in Table 6 .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 298,
                        "end": 299,
                        "text": "6",
                        "ref_id": null
                    },
                    {
                        "start": 1050,
                        "end": 1051,
                        "text": "6",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Prediction Evaluation",
                "sec_num": "4.2"
            },
            {
                "text": "That said, the gain of regression models is not significant. This may be due to the regression parameters that have been learned from a training corpus of the general domain. Another reason that could explain these results is the prediction process. We applied the same regression function to all co-occurrence counts while learning models for low and high frequencies should have been more appropriate. In the light of the above results, we believe that prediction can be beneficial to our task.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Prediction Evaluation",
                "sec_num": "4.2"
            },
            {
                "text": "Table 5 shows a comparison between the standard approach without prediction noted N o prediction and the standard approach based on the prediction of the source side noted Source pred , the target side noted T arget pred and both sides noted Source pred +T arget pred . If prediction can not replace a large amount of data, it aims at increasing co-occurrence counts as if large amounts of data were at our disposal. In this case, applying prediction to the source side may simulate a configuration of using unbalanced comparable corpora where the source side is n times bigger than the target side. Predicting the target side only, may Figure 1 : Results (MAP %) of the standard approach using the best configurations of the prediction models (Lin for Balanced + P rediction and P oly 2 for U nbalanced + P rediction) on the breast cancer and the diabetes corpora leads us to the opposite configuration where the target side is n times bigger than the source side. Finally, predicting both sides may simulate a large comparable corpora on both sides. In this experiment, we chose to use the linear regression model (Lin) for the prediction part. That said, the other regression models have shown the same behavior as Lin.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "5",
                        "ref_id": null
                    },
                    {
                        "start": 644,
                        "end": 645,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Source versus Target Prediction",
                "sec_num": "4.2.2"
            },
            {
                "text": "We can see that the best results are obtained by the Source pred approach for both comparable corpora. We can also notice that predicting the target side and both sides of the comparable corpora degrades the results. It is not surprising that predicting the target side only leads to lower results, since it is well known that a better characterization of a word to translate (given from the source side) leads to better results. We can deduce from Table 5 that source prediction is the most appropriate configuration to improve the quality of extracted lexicons. This configuration which simulates the use of unbalanced corpora leads us to think that using prediction with unbalanced comparable corpora should also increase the performance of the standard approach. This assumption is evaluated in the next Subsection.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Source versus Target Prediction",
                "sec_num": "4.2.2"
            },
            {
                "text": "In this last experiment we contrast the standard approach applied to the balanced and unbalanced corpora noted Balanced and U nbalanced with the standard approach combined with the prediction model noted Balanced + P rediction and U nbalanced + P rediction.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Predicting Unbalanced Corpora",
                "sec_num": "4.3"
            },
            {
                "text": "Figure 1 (a) illustrates the results of the experiments conducted on the breast cancer corpus. We can see that the U nbalanced approach significantly outperforms the baseline (Balanced). The big difference between the Balanced and the U nbalanced approaches would indicate that the latter is optimal. We can also notice that the prediction model applied to the balanced corpus (Balanced + P rediction) slightly outperforms the baseline while the U nbalanced + P rediction approach significantly outperforms the three other approaches (moreover the variation observed with the U nbalanced approach are lower than the U nbalanced + P rediction approach). Overall, the prediction increases the performance of the standard approach especially for unbalanced corpora.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Predicting Unbalanced Corpora",
                "sec_num": "4.3"
            },
            {
                "text": "The results of the experiments conducted on the diabetes corpus are shown in Figure 1(b) . As for the previous experiment, we can see that the U nbalanced approach significantly outperforms the Balanced approach. This confirms the unbalanced hypothesis and would motivate the use of unbalanced corpora when they are available. We can also notice that the Balanced + P rediction approach slightly outperforms the baseline while the U nbalanced+P rediction approach gives the best results. Here also, the prediction increases the performance of the standard approach especially for unbalanced corpora. It is clear that in addition to the benefit of using unbalanced comparable corpora, prediction shows a positive impact on the performance of the standard approach.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 84,
                        "end": 88,
                        "text": "1(b)",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Predicting Unbalanced Corpora",
                "sec_num": "4.3"
            },
            {
                "text": "In this paper, we have studied how an unbalanced specialized comparable corpus could influence the quality of the bilingual lexicon extraction. This aspect represents a significant interest when working with specialized comparable corpora for which the quantity of the data collected may differ depending on the languages involved, especially when involving the English language as many scientific documents are available. More precisely, our different experiments show that using an unbalanced specialized comparable corpus always improves the quality of word translations. Thus, the MAP goes up from 29.6% (best result on the balanced corpora) to 42.3% (best result on the unbalanced corpora) in the breast cancer domain, and from 16.5% to 26.0% in the diabetes domain. Additionally, these results can be improved by using a prediction model of the word co-occurrence counts.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "5"
            },
            {
                "text": "Here, the MAP goes up from 42.3% (best result on the unbalanced corpora) to 46.9% (best result on the unbalanced corpora with prediction) in the breast cancer domain, and from 26.0% to 29.8% in the diabetes domain. We hope that this study will pave the way for using specialized unbalanced comparable corpora for bilingual lexicon extraction.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "5"
            },
            {
                "text": "We only found mention of this aspect inDiab and Finch (2000, p. 1501) \"In principle, we do not have to have the same size corpora in order for the approach to work\".",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "http://www.elsevier.com",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "http://www.elra.info/",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "http://www.nlm.nih.gov/research/umls",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "The threshold sets to four is required to build a bilingual terminology reference list composed of about a hundred words. This value is very low to obtain representative context vectors. For instance,Prochasson and Fung (2011) showed that the standard approach is not relevant for infrequent words (since the context vectors are very unrepresentative i.e. poor in information).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "This work is supported by the French National Research Agency under grant ANR-12-CORD-0020.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgments",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "An Introduction to Categorical Data Analysis",
                "authors": [
                    {
                        "first": "Alan",
                        "middle": [],
                        "last": "Agresti",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alan Agresti. 2007. An Introduction to Categorical Data Analysis (2nd ed.). Wiley & Sons, Inc., Hobo- ken, New Jersey.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Context vector disambiguation for bilingual lexicon extraction from comparable corpora",
                "authors": [
                    {
                        "first": "Dhouha",
                        "middle": [],
                        "last": "Bouamor",
                        "suffix": ""
                    },
                    {
                        "first": "Nasredine",
                        "middle": [],
                        "last": "Semmar",
                        "suffix": ""
                    },
                    {
                        "first": "Pierre",
                        "middle": [],
                        "last": "Zweigenbaum",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL'13)",
                "volume": "",
                "issue": "",
                "pages": "759--764",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dhouha Bouamor, Nasredine Semmar, and Pierre Zweigenbaum. 2013. Context vector disambigua- tion for bilingual lexicon extraction from compa- rable corpora. In Proceedings of the 51st Annual Meeting of the Association for Computational Lin- guistics (ACL'13), pages 759-764, Sofia, Bulgaria.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Looking for candidate translational equivalents in specialized, comparable corpora",
                "authors": [
                    {
                        "first": "Yun-Chuang",
                        "middle": [],
                        "last": "Chiao",
                        "suffix": ""
                    },
                    {
                        "first": "Pierre",
                        "middle": [],
                        "last": "Zweigenbaum",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Proceedings of the 19th International Conference on Computational Linguistics (COLING'02)",
                "volume": "",
                "issue": "",
                "pages": "1208--1212",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yun-Chuang Chiao and Pierre Zweigenbaum. 2002. Looking for candidate translational equivalents in specialized, comparable corpora. In Proceedings of the 19th International Conference on Computational Linguistics (COLING'02), pages 1208-1212, Tapei, Taiwan.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "The Effect of a General Lexicon in Corpus-Based Identification of French-English Medical Word Translations",
                "authors": [
                    {
                        "first": "Yun-Chuang",
                        "middle": [],
                        "last": "Chiao",
                        "suffix": ""
                    },
                    {
                        "first": "Pierre",
                        "middle": [],
                        "last": "Zweigenbaum",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "The New Navigators: from Professionals to Patients, Actes Medical Informatics Europe",
                "volume": "",
                "issue": "",
                "pages": "397--402",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yun-Chuang Chiao and Pierre Zweigenbaum. 2003. The Effect of a General Lexicon in Corpus-Based Identification of French-English Medical Word Translations. In The New Navigators: from Profes- sionals to Patients, Actes Medical Informatics Eu- rope, pages 397-402.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Log-Linear Models and Logistic Regression",
                "authors": [
                    {
                        "first": "Ronald",
                        "middle": [],
                        "last": "Christensen",
                        "suffix": ""
                    }
                ],
                "year": 1997,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ronald Christensen. 1997. Log-Linear Models and Logistic Regression. Springer-Verlag, Berlin.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "An approach based on multilingual thesauri and model combination for bilingual lexicon extraction",
                "authors": [
                    {
                        "first": "Fatia",
                        "middle": [],
                        "last": "Herv\u00e9 D\u00e9jean",
                        "suffix": ""
                    },
                    {
                        "first": "\u00c9ric",
                        "middle": [],
                        "last": "Sadat",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Gaussier",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Proceedings of the 19th International Conference on Computational Linguistics (COLING'02)",
                "volume": "",
                "issue": "",
                "pages": "218--224",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Herv\u00e9 D\u00e9jean, Fatia Sadat, and \u00c9ric Gaussier. 2002. An approach based on multilingual thesauri and model combination for bilingual lexicon extraction. In Proceedings of the 19th International Conference on Computational Linguistics (COLING'02), pages 218-224, Tapei, Taiwan.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "A Statistical Word-Level Translation Model for Comparable Corpora",
                "authors": [
                    {
                        "first": "Mona",
                        "middle": [
                            "T"
                        ],
                        "last": "Diab",
                        "suffix": ""
                    },
                    {
                        "first": "Steve",
                        "middle": [],
                        "last": "Finch",
                        "suffix": ""
                    }
                ],
                "year": 2000,
                "venue": "Proceedings of the 6th International Conference on Computer-Assisted Information Retrieval (RIAO'00)",
                "volume": "",
                "issue": "",
                "pages": "1500--1501",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mona T. Diab and Steve Finch. 2000. A Statistical Word-Level Translation Model for Comparable Cor- pora. In Proceedings of the 6th International Con- ference on Computer-Assisted Information Retrieval (RIAO'00), pages 1500-1501, Paris, France.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "The Statistics of Word Cooccurrences: Word Pairs and Collocations",
                "authors": [
                    {
                        "first": "Stefan",
                        "middle": [],
                        "last": "Evert",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Stefan Evert. 2005. The Statistics of Word Cooccur- rences: Word Pairs and Collocations. Ph.D. thesis, Universit\u00e4t Stuttgart, Germany.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Multilevel bootstrapping for extracting parallel sentences from a quasi-comparable corpus",
                "authors": [
                    {
                        "first": "Pascale",
                        "middle": [],
                        "last": "Fung",
                        "suffix": ""
                    },
                    {
                        "first": "Percy",
                        "middle": [],
                        "last": "Cheung",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proceedings of the 20th International Conference on Computational Linguistics (COLING'04)",
                "volume": "",
                "issue": "",
                "pages": "1051--1057",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Pascale Fung and Percy Cheung. 2004. Multi- level bootstrapping for extracting parallel sentences from a quasi-comparable corpus. In Proceedings of the 20th International Conference on Computa- tional Linguistics (COLING'04), pages 1051-1057, Geneva, Switzerland.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Finding Terminology Translations from Non-parallel Corpora",
                "authors": [
                    {
                        "first": "Pascale",
                        "middle": [],
                        "last": "Fung",
                        "suffix": ""
                    },
                    {
                        "first": "Kathleen",
                        "middle": [],
                        "last": "Mckeown",
                        "suffix": ""
                    }
                ],
                "year": 1997,
                "venue": "Proceedings of the 5th Annual Workshop on Very Large Corpora (VLC'97)",
                "volume": "",
                "issue": "",
                "pages": "192--202",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Pascale Fung and Kathleen McKeown. 1997. Finding Terminology Translations from Non-parallel Cor- pora. In Proceedings of the 5th Annual Workshop on Very Large Corpora (VLC'97), pages 192-202, Hong Kong.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Compiling Bilingual Lexicon Entries from a non-Parallel English-Chinese Corpus",
                "authors": [
                    {
                        "first": "Pascale",
                        "middle": [],
                        "last": "Fung",
                        "suffix": ""
                    }
                ],
                "year": 1995,
                "venue": "Proceedings of the 3rd Annual Workshop on Very Large Corpora (VLC'95)",
                "volume": "",
                "issue": "",
                "pages": "173--183",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Pascale Fung. 1995. Compiling Bilingual Lexicon Entries from a non-Parallel English-Chinese Cor- pus. In Proceedings of the 3rd Annual Workshop on Very Large Corpora (VLC'95), pages 173-183, Cambridge, MA, USA.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "A Statistical View on Bilingual Lexicon Extraction: From Parallel Corpora to Non-parallel Corpora",
                "authors": [
                    {
                        "first": "Pascale",
                        "middle": [],
                        "last": "Fung",
                        "suffix": ""
                    }
                ],
                "year": 1998,
                "venue": "Proceedings of the 3rd Conference of the Association for Machine Translation in the Americas (AMTA'98)",
                "volume": "",
                "issue": "",
                "pages": "1--16",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Pascale Fung. 1998. A Statistical View on Bilin- gual Lexicon Extraction: From Parallel Corpora to Non-parallel Corpora. In David Farwell, Laurie Gerber, and Eduard Hovy, editors, Proceedings of the 3rd Conference of the Association for Machine Translation in the Americas (AMTA'98), pages 1- 16, Langhorne, PA, USA.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "A Geometric View on Bilingual Lexicon Extraction from Comparable Corpora",
                "authors": [
                    {
                        "first": "\u00c9ric",
                        "middle": [],
                        "last": "Gaussier",
                        "suffix": ""
                    },
                    {
                        "first": "Jean-Michel",
                        "middle": [],
                        "last": "Renders",
                        "suffix": ""
                    },
                    {
                        "first": "Irena",
                        "middle": [],
                        "last": "Matveeva",
                        "suffix": ""
                    },
                    {
                        "first": "Cyril",
                        "middle": [],
                        "last": "Goutte",
                        "suffix": ""
                    },
                    {
                        "first": "Herv\u00e9",
                        "middle": [],
                        "last": "D\u00e9jean",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL'04)",
                "volume": "",
                "issue": "",
                "pages": "526--533",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "\u00c9ric Gaussier, Jean-Michel Renders, Irena Matveeva, Cyril Goutte, and Herv\u00e9 D\u00e9jean. 2004. A Geometric View on Bilingual Lexicon Extraction from Comparable Corpora. In Proceedings of the 42nd Annual Meeting of the Association for Com- putational Linguistics (ACL'04), pages 526-533, Barcelona, Spain.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Corpus-Derived First, Second and Third-Order Word Affinities",
                "authors": [
                    {
                        "first": "Gregory",
                        "middle": [],
                        "last": "Grefenstette",
                        "suffix": ""
                    }
                ],
                "year": 1994,
                "venue": "Proceedings of the 6th Congress of the European Association for Lexicography (EURALEX'94)",
                "volume": "",
                "issue": "",
                "pages": "279--290",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Gregory Grefenstette. 1994. Corpus-Derived First, Second and Third-Order Word Affinities. In Pro- ceedings of the 6th Congress of the European As- sociation for Lexicography (EURALEX'94), pages 279-290, Amsterdam, The Netherlands.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Word co-occurrence counts prediction for bilingual terminology extraction from comparable corpora",
                "authors": [
                    {
                        "first": "Amir",
                        "middle": [],
                        "last": "Hazem",
                        "suffix": ""
                    },
                    {
                        "first": "Emmanuel",
                        "middle": [],
                        "last": "Morin",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of the Sixth International Joint Conference on Natural Language Processing (IJCNLP'13)",
                "volume": "",
                "issue": "",
                "pages": "1392--1400",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Amir Hazem and Emmanuel Morin. 2013. Word co-occurrence counts prediction for bilingual ter- minology extraction from comparable corpora. In Proceedings of the Sixth International Joint Confer- ence on Natural Language Processing (IJCNLP'13), pages 1392-1400, Nagoya, Japan.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Bilingual lexicon extraction from comparable corpora using in-domain terms",
                "authors": [
                    {
                        "first": "Azniah",
                        "middle": [],
                        "last": "Ismail",
                        "suffix": ""
                    },
                    {
                        "first": "Suresh",
                        "middle": [],
                        "last": "Manandhar",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the 23rd International Conference on Computational Linguistics (COLING'10)",
                "volume": "",
                "issue": "",
                "pages": "481--489",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Azniah Ismail and Suresh Manandhar. 2010. Bilingual lexicon extraction from comparable corpora using in-domain terms. In Proceedings of the 23rd Inter- national Conference on Computational Linguistics (COLING'10), pages 481-489, Beijing, China.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Learning a translation lexicon from monolingual corpora",
                "authors": [
                    {
                        "first": "Philipp",
                        "middle": [],
                        "last": "Koehn",
                        "suffix": ""
                    },
                    {
                        "first": "Kevin",
                        "middle": [],
                        "last": "Knight",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Proceedings of the ACL-02 Workshop on Unsupervised Lexical Acquisition (ULA'02)",
                "volume": "",
                "issue": "",
                "pages": "9--16",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Philipp Koehn and Kevin Knight. 2002. Learning a translation lexicon from monolingual corpora. In Proceedings of the ACL-02 Workshop on Unsuper- vised Lexical Acquisition (ULA'02), pages 9-16, Philadelphia, PA, USA.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Revisiting Context-based Projection Methods for Term-Translation Spotting in Comparable Corpora",
                "authors": [
                    {
                        "first": "Audrey",
                        "middle": [],
                        "last": "Laroche",
                        "suffix": ""
                    },
                    {
                        "first": "Philippe",
                        "middle": [],
                        "last": "Langlais",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the 23rd International Conference on Computational Linguistics (COLING'10)",
                "volume": "",
                "issue": "",
                "pages": "617--625",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Audrey Laroche and Philippe Langlais. 2010. Revis- iting Context-based Projection Methods for Term- Translation Spotting in Comparable Corpora. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING'10), pages 617-625, Beijing, China.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Improving corpus comparability for bilingual lexicon extraction from comparable corpora",
                "authors": [
                    {
                        "first": "Bo",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "\u00c9ric",
                        "middle": [],
                        "last": "Gaussier",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the 23rd International Conference on Computational Linguistics (COLING'10)",
                "volume": "",
                "issue": "",
                "pages": "644--652",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Bo Li and \u00c9ric Gaussier. 2010. Improving corpus comparability for bilingual lexicon extraction from comparable corpora. In Proceedings of the 23rd In- ternational Conference on Computational Linguis- tics (COLING'10), pages 644-652, Beijing, China.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Introduction to Information Retrieval",
                "authors": [
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Christopher",
                        "suffix": ""
                    },
                    {
                        "first": "Prabhakar",
                        "middle": [],
                        "last": "Manning",
                        "suffix": ""
                    },
                    {
                        "first": "Hinrich",
                        "middle": [],
                        "last": "Raghavan",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Schtze",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schtze. 2008. Introduction to Information Retrieval. Cambridge University Press, New York, NY, USA.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Parallel and comparable corpora: What are they up to?",
                "authors": [
                    {
                        "first": "Anthony",
                        "middle": [],
                        "last": "Mcenery",
                        "suffix": ""
                    },
                    {
                        "first": "Zhonghua",
                        "middle": [],
                        "last": "Xiao",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Incorporating Corpora: Translation and the Linguist",
                "volume": "",
                "issue": "",
                "pages": "18--31",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Anthony McEnery and Zhonghua Xiao. 2007. Paral- lel and comparable corpora: What are they up to? In Gunilla Anderman and Margaret Rogers, editors, Incorporating Corpora: Translation and the Lin- guist, Multilingual Matters, chapter 2, pages 18-31. Clevedon, UK.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Bilingual lexicon extraction from comparable corpora enhanced with parallel corpora",
                "authors": [
                    {
                        "first": "Emmanuel",
                        "middle": [],
                        "last": "Morin",
                        "suffix": ""
                    },
                    {
                        "first": "Emmanuel",
                        "middle": [],
                        "last": "Prochasson",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Proceedings of the 4th Workshop on Building and Using Comparable Corpora (BUCC'11)",
                "volume": "",
                "issue": "",
                "pages": "27--34",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Emmanuel Morin and Emmanuel Prochasson. 2011. Bilingual lexicon extraction from comparable cor- pora enhanced with parallel corpora. In Proceedings of the 4th Workshop on Building and Using Compa- rable Corpora (BUCC'11), pages 27-34, Portland, OR, USA.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Bilingual Terminology Mining -Using Brain, not brawn comparable corpora",
                "authors": [
                    {
                        "first": "Emmanuel",
                        "middle": [],
                        "last": "Morin",
                        "suffix": ""
                    },
                    {
                        "first": "B\u00e9atrice",
                        "middle": [],
                        "last": "Daille",
                        "suffix": ""
                    },
                    {
                        "first": "Koichi",
                        "middle": [],
                        "last": "Takeuchi",
                        "suffix": ""
                    },
                    {
                        "first": "Kyo",
                        "middle": [],
                        "last": "Kageura",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL'07)",
                "volume": "",
                "issue": "",
                "pages": "664--671",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Emmanuel Morin, B\u00e9atrice Daille, Koichi Takeuchi, and Kyo Kageura. 2007. Bilingual Terminology Mining -Using Brain, not brawn comparable cor- pora. In Proceedings of the 45th Annual Meet- ing of the Association for Computational Linguistics (ACL'07), pages 664-671, Prague, Czech Republic.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Learning bilingual lexicons from comparable english and spanish corpora",
                "authors": [
                    {
                        "first": "Pablo",
                        "middle": [],
                        "last": "Gamallo",
                        "suffix": ""
                    },
                    {
                        "first": "Otero",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Proceedings of the 11th Conference on Machine Translation Summit (MT Summit XI)",
                "volume": "",
                "issue": "",
                "pages": "191--198",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Pablo Gamallo Otero. 2007. Learning bilingual lexi- cons from comparable english and spanish corpora. In Proceedings of the 11th Conference on Machine Translation Summit (MT Summit XI), pages 191- 198, Copenhagen, Denmark.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Crosslanguage information retrieval: A system for comparable corpus querying",
                "authors": [
                    {
                        "first": "Carol",
                        "middle": [],
                        "last": "Peters",
                        "suffix": ""
                    },
                    {
                        "first": "Eugenio",
                        "middle": [],
                        "last": "Picchi",
                        "suffix": ""
                    }
                ],
                "year": 1998,
                "venue": "Cross-language information retrieval, chapter 7",
                "volume": "",
                "issue": "",
                "pages": "81--90",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Carol Peters and Eugenio Picchi. 1998. Cross- language information retrieval: A system for com- parable corpus querying. In Gregory Grefenstette, editor, Cross-language information retrieval, chap- ter 7, pages 81-90. Kluwer Academic Publishers.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Rare Word Translation Extraction from Aligned Comparable Documents",
                "authors": [
                    {
                        "first": "Emmanuel",
                        "middle": [],
                        "last": "Prochasson",
                        "suffix": ""
                    },
                    {
                        "first": "Pascale",
                        "middle": [],
                        "last": "Fung",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL'11)",
                "volume": "",
                "issue": "",
                "pages": "1327--1335",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Emmanuel Prochasson and Pascale Fung. 2011. Rare Word Translation Extraction from Aligned Compa- rable Documents. In Proceedings of the 49th Annual Meeting of the Association for Computational Lin- guistics (ACL'11), pages 1327-1335, Portland, OR, USA.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Anchor points for bilingual lexicon extraction from small comparable corpora",
                "authors": [
                    {
                        "first": "Emmanuel",
                        "middle": [],
                        "last": "Prochasson",
                        "suffix": ""
                    },
                    {
                        "first": "Emmanuel",
                        "middle": [],
                        "last": "Morin",
                        "suffix": ""
                    },
                    {
                        "first": "Kyo",
                        "middle": [],
                        "last": "Kageura",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of the 12th Conference on Machine Translation Summit (MT Summit XII)",
                "volume": "",
                "issue": "",
                "pages": "284--291",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Emmanuel Prochasson, Emmanuel Morin, and Kyo Kageura. 2009. Anchor points for bilingual lexicon extraction from small comparable corpora. In Pro- ceedings of the 12th Conference on Machine Trans- lation Summit (MT Summit XII), pages 284-291, Ot- tawa, Canada.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Identify Word Translations in Non-Parallel Texts",
                "authors": [
                    {
                        "first": "Reinhard",
                        "middle": [],
                        "last": "Rapp",
                        "suffix": ""
                    }
                ],
                "year": 1995,
                "venue": "Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics (ACL'95)",
                "volume": "",
                "issue": "",
                "pages": "320--322",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Reinhard Rapp. 1995. Identify Word Translations in Non-Parallel Texts. In Proceedings of the 35th An- nual Meeting of the Association for Computational Linguistics (ACL'95), pages 320-322, Boston, MA, USA.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Automatic Identification of Word Translations from Unrelated English and German Corpora",
                "authors": [
                    {
                        "first": "Reinhard",
                        "middle": [],
                        "last": "Rapp",
                        "suffix": ""
                    }
                ],
                "year": 1999,
                "venue": "Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL'99)",
                "volume": "",
                "issue": "",
                "pages": "519--526",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Reinhard Rapp. 1999. Automatic Identification of Word Translations from Unrelated English and Ger- man Corpora. In Proceedings of the 37th Annual Meeting of the Association for Computational Lin- guistics (ACL'99), pages 519-526, College Park, MD, USA.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "A multiview approach for term translation spotting",
                "authors": [
                    {
                        "first": "Rapha\u00ebl",
                        "middle": [],
                        "last": "Rubino",
                        "suffix": ""
                    },
                    {
                        "first": "Georges",
                        "middle": [],
                        "last": "Linar\u00e8s",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Proceedings of the 12th International Conference on Computational Linguistics and Intelligent Text Processing (CICLing'11)",
                "volume": "",
                "issue": "",
                "pages": "29--40",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Rapha\u00ebl Rubino and Georges Linar\u00e8s. 2011. A multi- view approach for term translation spotting. In Pro- ceedings of the 12th International Conference on Computational Linguistics and Intelligent Text Pro- cessing (CICLing'11), pages 29-40, Tokyo, Japan.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "Extraction of Lexical Translations from Non-Aligned Corpora",
                "authors": [
                    {
                        "first": "Kumiko",
                        "middle": [],
                        "last": "Tanaka",
                        "suffix": ""
                    },
                    {
                        "first": "Hideya",
                        "middle": [],
                        "last": "Iwasaki",
                        "suffix": ""
                    }
                ],
                "year": 1996,
                "venue": "Proceedings of the 16th International Conference on Computational Linguistics (COLING'96)",
                "volume": "",
                "issue": "",
                "pages": "580--585",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kumiko Tanaka and Hideya Iwasaki. 1996. Extraction of Lexical Translations from Non-Aligned Corpora. In Proceedings of the 16th International Conference on Computational Linguistics (COLING'96), pages 580-585, Copenhagen, Denmark.",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "Extracting bilingual dictionary from comparable corpora with dependency heterogeneity",
                "authors": [
                    {
                        "first": "Kun",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Junichi",
                        "middle": [],
                        "last": "Tsujii",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT'09)",
                "volume": "",
                "issue": "",
                "pages": "121--124",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kun Yu and Junichi Tsujii. 2009. Extracting bilin- gual dictionary from comparable corpora with de- pendency heterogeneity. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT'09), pages 121-124, Boulder, CO, USA.",
                "links": null
            }
        },
        "ref_entries": {
            "TABREF0": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Contingency table",
                "html": null,
                "num": null
            }
        }
    }
}