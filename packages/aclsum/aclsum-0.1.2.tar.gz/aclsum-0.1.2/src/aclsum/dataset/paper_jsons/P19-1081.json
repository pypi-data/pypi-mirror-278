{
    "paper_id": "P19-1081",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:33:41.759089Z"
    },
    "title": "OpenDialKG: Explainable Conversational Reasoning with Attention-based Walks over Knowledge Graphs",
    "authors": [
        {
            "first": "Seungwhan",
            "middle": [],
            "last": "Moon",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Pararth",
            "middle": [],
            "last": "Shah",
            "suffix": "",
            "affiliation": {},
            "email": "pararths@fb.com"
        },
        {
            "first": "Anuj",
            "middle": [],
            "last": "Kumar",
            "suffix": "",
            "affiliation": {},
            "email": "anujk@fb.com"
        },
        {
            "first": "Rajen",
            "middle": [],
            "last": "Subba",
            "suffix": "",
            "affiliation": {},
            "email": "rasubba@fb.com"
        },
        {
            "first": "Facebook",
            "middle": [],
            "last": "Conversational",
            "suffix": "",
            "affiliation": {},
            "email": ""
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "We study a conversational reasoning model that strategically traverses through a largescale common fact knowledge graph (KG) to introduce engaging and contextually diverse entities and attributes. For this study, we collect a new Open-ended Dialog \u2194 KG parallel corpus called OpenDialKG, where each utterance from 15K human-to-human roleplaying dialogs is manually annotated with ground-truth reference to corresponding entities and paths from a large-scale KG with 1M+ facts. We then propose the DialKG Walker model that learns the symbolic transitions of dialog contexts as structured traversals over KG, and predicts natural entities to introduce given previous dialog contexts via a novel domain-agnostic, attention-based graph path decoder. Automatic and human evaluations show that our model can retrieve more natural and human-like responses than the state-ofthe-art baselines or rule-based models, in both in-domain and cross-domain tasks. The proposed model also generates a KG walk path for each entity retrieved, providing a natural way to explain conversational reasoning.",
    "pdf_parse": {
        "paper_id": "P19-1081",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "We study a conversational reasoning model that strategically traverses through a largescale common fact knowledge graph (KG) to introduce engaging and contextually diverse entities and attributes. For this study, we collect a new Open-ended Dialog \u2194 KG parallel corpus called OpenDialKG, where each utterance from 15K human-to-human roleplaying dialogs is manually annotated with ground-truth reference to corresponding entities and paths from a large-scale KG with 1M+ facts. We then propose the DialKG Walker model that learns the symbolic transitions of dialog contexts as structured traversals over KG, and predicts natural entities to introduce given previous dialog contexts via a novel domain-agnostic, attention-based graph path decoder. Automatic and human evaluations show that our model can retrieve more natural and human-like responses than the state-ofthe-art baselines or rule-based models, in both in-domain and cross-domain tasks. The proposed model also generates a KG walk path for each entity retrieved, providing a natural way to explain conversational reasoning.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "The key element of an open-ended dialog system is its ability to understand conversational contexts and to respond naturally by introducing relevant entities and attributes, which often leads to increased engagement and coherent interactions (Chen et al., 2018) . While a large-scale knowledge graph (KG) includes vast knowledge of all the related entities connected via one or more factual connections from conversational contexts, the core challenge is in the domain-agnostic and scalable prediction of a small subset from those reachable entities that follows natural conceptual threads that can keep conversations engaging and meaningful. Hence, we study a data-driven reasoning model topical jumps across open-ended multi-turn dialogs are annotated and grounded with a large-scale commonfact KG. To generate a KG entity response at each dialog turn, the model learns walkable paths within KG that lead to engaging and natural topics or entities given dialog context, while pruning non-ideal (albeit factually correct) KG paths among 1M+ candidate facts.",
                "cite_spans": [
                    {
                        "start": 242,
                        "end": 261,
                        "text": "(Chen et al., 2018)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "that map dialog transitions with KG paths, aimed at identifying a subset of ideal entities to mention as a response to previous dialog contexts.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Figure 1 illustrates a motivating dialog example between two conversation participants, which spans multiple related KG entities from a starting seed entity The Catcher in the Rye. Specifically, we observe that there exists a small subset of walkable patterns within a KG or a preferred sequence of graph traversal steps which often leads to more engaging entities or attributes than others (e.g. Literacy Realism, Nathaniel Hawthorne, etc. vs. Catch Me If You Can, 277, etc. all connected via one-or multi-hop factual connections). Note also that the walkable degree of each entity varies by dialog contexts and domains, thus making conventional rule-based or entity-toentity learning approaches intractable or not scalable for open-ended dialogs with 1M+ candidate facts. Therefore, pruning the search space for entities based on dialog contexts and their relationbased walk paths is a crucial step in operating knowledge-augmented dialog systems at scale.",
                "cite_spans": [
                    {
                        "start": 425,
                        "end": 435,
                        "text": "Hawthorne,",
                        "ref_id": null
                    },
                    {
                        "start": 436,
                        "end": 465,
                        "text": "etc. vs. Catch Me If You Can,",
                        "ref_id": null
                    },
                    {
                        "start": 466,
                        "end": 470,
                        "text": "277,",
                        "ref_id": null
                    },
                    {
                        "start": 471,
                        "end": 475,
                        "text": "etc.",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "To this end, we propose a new model called DialKG Walker that can learn natural knowledge paths among entities mentioned over dialog contexts, and reason grounded on a large commonsense KG. Specifically, we propose a novel graph decoder that attends on viable KG paths to predict the most relevant entities from a KG, by associating these paths with the given input contexts: dialog, sentence, and a set of starting KG entities mentioned in the previous turn. We then build a parallel zeroshot learning model that predicts entities in the KG embeddings space, and ranks candidate entities based on decoded graph path output.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "To train the DialKG Walker model with groundtruth reference to KG entities, we collect a new human-to-human multi-turn dialogs dataset (91K utterances across 15K dialog sessions) using Par-lAI (Miller et al., 2017) , where conversation participants play a role either as a user or as an assistant, while annotating their mention of an entity in a large-scale common fact KG. This new dataset provides a new way for researchers to study how conversational topics could jump across many different entities within multi-turn dialogs, grounded on KG paths that thread all of them. To the best of our knowledge, our OpenDialKG is the first parallel Dialog \u2194 KG corpus where each mention of a KG entity and its factual connection in an openended dialog is fully annotated, allowing for indepth study of symbolic reasoning and natural language conversations.",
                "cite_spans": [
                    {
                        "start": 193,
                        "end": 214,
                        "text": "(Miller et al., 2017)",
                        "ref_id": "BIBREF18"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Note that our approaches are distinct from the previous work on dialog systems in that we completely ground dialogs in a large-scale commonfact KG, allowing for domain-agnostic conversational reasoning in open-ended conversations across various domains and tasks (e.g. chit-chat, recommendations, etc.) We therefore perform extensive cross-domain and transfer learning evaluations to demonstrate its flexibility. See Section 5 for the detailed literature review.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Our contributions are as follows: we propose (1) a novel attention-based graph decoder that walks an optimal path within a large commonsense KG (100K entities, 1.1M facts) to effectively prune unlikely candidate entities, and (2) a zeroshot learning model that leverages previous sentence, dialog, and KG contexts to re-rank candidates from pruned decoder graph output based on their relevance and path scores, which allows for generalizable and robust classification with a large number of candidate classes. We present (3) a new parallel open-ended dialog \u2194 KG corpus called OpenDialKG where each mention of an entity in dialog is manually linked with its corresponding ground-truth KG path. We show that the proposed approaches outperform baselines in both indomain and cross-domain evaluation, demonstrating that the model learns domain-agnostic walking patterns that are generalizable for unseen domains.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Figure 2 illustrates the overall architecture of the DialKG Walker model which retrieves a set of entities from a provided KG given multiple modalities of dialog contexts. Specifically, for each turn the model takes as input a set of KG entities mentioned at its current turn, a full sentence at the current turn, and all sentences from previous turns of dialog, which are encoded using Bi-LSTMs with self-attention modules (Section 2.2). The autoregressive graph decoder takes attention-based encoder output at each decoding step to generate a walk path for each starting KG entity, which is combined with zeroshot KG embeddings prediction results to rank candidate entities (Section 2.3).",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Method",
                "sec_num": "2"
            },
            {
                "text": "We define the knowledge graph G KG = V KG \u00d7 R KG which is composed of all common-sense entity nodes V KG and the relation set R KG that connects each pair of two nodes. Let us also denote V r (v) to be a set of nodes directly connected to a node v \u2208 V KG by a relation r \u2208 R KG . Similarly, we denote V R,n (v) to be a set of nodes connected to v via n-hops with a set of relations R.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Notations",
                "sec_num": "2.1"
            },
            {
                "text": "Each input is composed of three modalities: x = {x e ; x s ; x d }, where x e = {x (i) e } is a set of entities mentioned in the current turn, x s is its surrounding sentence context in the same turn, and x d is its dialog context up to the previous turn.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Notations",
                "sec_num": "2.1"
            },
            {
                "text": "Each output is a KG path sequence that con- where f x\u2192y is a function with learnable parameters that projects input samples at the current turn (x) into the same space as the output representations (y), i.e. entities to be mentioned in the next turn and their optimal paths. V(x e ) \u2282 V KG denotes a set of KG entity nodes reachable from x e , defined accordingly to each decoding method.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Notations",
                "sec_num": "2.1"
            },
            {
                "text": "Entity representation: We construct KG embeddings to encode each entity mention (Bordes et al., 2013) , in which semantically similar entities are distributed closer in the embeddings space. In brief formulation, the model for obtaining embeddings from a KG (composed of subject-relationobject (s, r, o) triples) is as follows:",
                "cite_spans": [
                    {
                        "start": 80,
                        "end": 101,
                        "text": "(Bordes et al., 2013)",
                        "ref_id": "BIBREF2"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Input Encoding",
                "sec_num": "2.2"
            },
            {
                "text": "P (I r (s, o) = 1|\u03b8) = score e(s), e r (r), e(o) (1)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Input Encoding",
                "sec_num": "2.2"
            },
            {
                "text": "where I r is an indicator function of a known relation r for two entities (s,o) (1: valid relation, 0: unknown relation), e is a function that extracts embeddings for entities, e r extracts embeddings for relations, and score(\u2022) is a deep neural network that produces a likelihood of a valid triple. Sentence representation: We represent textual context of surrounding words of a mention with a state-of-the-art attention-based Bi-LSTM language model (Conneau et al., 2017) with GloVe (Pennington et al., 2014) distributed word embeddings trained on the Wikipedia and the Gigaword corpus with a total of 6B tokens. Dialog representation: To encode previous dialog history, we use a hierarchical Bi-LSTM (Yang et al., 2016) over a sequence of previous sentences with a fixed window size. We apply self-attention over sentences to attenuate and amplify sentence contexts based on their relevance to the task, allowing for more robust and explainable prediction. Input aggregation: We aggregate input contexts x from entities, sentences and dialogs, by applying the modality attention (Moon et al., 2018a,b) , which selectively attenuates or amplifies each modality based on their importance on the task:",
                "cite_spans": [
                    {
                        "start": 451,
                        "end": 473,
                        "text": "(Conneau et al., 2017)",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 485,
                        "end": 510,
                        "text": "(Pennington et al., 2014)",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 703,
                        "end": 722,
                        "text": "(Yang et al., 2016)",
                        "ref_id": "BIBREF35"
                    },
                    {
                        "start": 1082,
                        "end": 1104,
                        "text": "(Moon et al., 2018a,b)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Input Encoding",
                "sec_num": "2.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "[a e ; a s ; a d ] = \u03c3 W m \u2022 [xe; xs; x d ] + b m (2) \u03b1 m = exp(a m ) m \u2208{e,s,d} exp(a m ) \u2200m \u2208 {e, s, d} x = m\u2208{e,s,d} \u03b1 m x m",
                        "eq_num": "(3)"
                    }
                ],
                "section": "Input Encoding",
                "sec_num": "2.2"
            },
            {
                "text": "where \u03b1 = [\u03b1 e ; \u03b1 s ; \u03b1 d ] \u2208 R 3 is an attention vector, and x is a final context vector that maximizes information gain.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Input Encoding",
                "sec_num": "2.2"
            },
            {
                "text": "Using the contextual information extracted from an entity and its surrounding text (Section 2.2), we build a network which predicts a corresponding KG entity based on its knowledge graph embeddings with the following objective:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Graph Decoder",
                "sec_num": "2.3"
            },
            {
                "text": "min W L f (x, y e ; W f ,W p ) + L walk (x, y p ;W p ) R(W): regularization (4)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Graph Decoder",
                "sec_num": "2.3"
            },
            {
                "text": "where L f (\u2022) is a supervised loss for generating the correct entity at the next turn, and L walk (\u2022) is a loss defined for taking the optimal path within a knowledge graph. W = {W f ,W p ,W input } are the learnable parameters for the final entity classifier (W f ), the path walker model (W p ), and the input encoder, respectively. R(W) denotes the weight decay regularization term.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Graph Decoder",
                "sec_num": "2.3"
            },
            {
                "text": "We compute zeroshot relevance score in the KG embeddings space, thus allowing for robust prediction for KG entities and domains unseen during training as well. Specifically, we use the supervised hinge rank loss for KG embeddings prediction as a choice of L f , defined for each sample (Moon and Carbonell, 2017) .",
                "cite_spans": [
                    {
                        "start": 286,
                        "end": 312,
                        "text": "(Moon and Carbonell, 2017)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Zeroshot Relevance Score",
                "sec_num": "2.3.1"
            },
            {
                "text": "i \u1ef9 =y (i) e max[0, \u1ef9 \u2022 y (i) e -f (x (i) )\u2022(y (i) e -\u1ef9) ] (5)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Zeroshot Relevance Score",
                "sec_num": "2.3.1"
            },
            {
                "text": "where f (\u2022) is a transformation function that walks through the knowledge graph and projects a predicted future entity in the KG embeddings space, and \u1ef9 refers to the embeddings of negative samples randomly sampled from KG entities except the ground truth label of the instance. Intuitively, the model is trained to produce a higher dot product similarity between the projected embeddings of a sample with its correct label (f (x (i) )\u2022y",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Zeroshot Relevance Score",
                "sec_num": "2.3.1"
            },
            {
                "text": "(i)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Zeroshot Relevance Score",
                "sec_num": "2.3.1"
            },
            {
                "text": "e ) than with an incorrect negative label in the KG label embeddings space (f (x (i) ) \u2022 \u1ef9), where the margin is defined as the similarity between a ground truth sample and a negative sample (\u1ef9 \u2022 y",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Zeroshot Relevance Score",
                "sec_num": "2.3.1"
            },
            {
                "text": "(i) e ).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Zeroshot Relevance Score",
                "sec_num": "2.3.1"
            },
            {
                "text": "Generating candidate KG entities solely based on their relevance score (Eq.5) is challenging due to the exponentially large search space. To this end, we define the attention-based DialKG graph decoder model which prunes unattended paths, which effectively reduce the search space. Decoding steps are formulated as follows (bias terms for gates are omitted for simplicity of notation):",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "KG Path Walker",
                "sec_num": "2.3.2"
            },
            {
                "text": "i t = \u03c3(W hi h t-1 + W ci c t-1 ) c t = (1 -i t ) c t-1 + i t tanh(W zc z t + W hc h t-1 ) o t = \u03c3(W zo z t + W ho h t-1 + W co c t ) h t = WALK(x, z t ) = o t tanh(c t ) (6)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "KG Path Walker",
                "sec_num": "2.3.2"
            },
            {
                "text": "where z t is a context vector at decoding step t, produced from the attention over walkable path which is defined as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "KG Path Walker",
                "sec_num": "2.3.2"
            },
            {
                "text": "\u03b1 t = \u03c3(W h\u03b1 h t-1 + W x\u03b1 x t ) z t = h t-1 + r k \u2208R KG \u03b1 t,k r k (7)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "KG Path Walker",
                "sec_num": "2.3.2"
            },
            {
                "text": "where \u03b1 t \u2208 R |R KG | is an attention vector over the relations space, r k is relation embeddings, and z t is a resulting entity context vector after walking from its previous entity on an attended path.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "KG Path Walker",
                "sec_num": "2.3.2"
            },
            {
                "text": "We guide the graph decoder with the groundtruth walk paths by computing the following loss L walk (x, y) = i,t L ent + L rel between predicted paths and each of {y e , y r }, respectively (L ent : loss for entity paths, and L ent for relation paths):",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "KG Path Walker",
                "sec_num": "2.3.2"
            },
            {
                "text": "\u1ef9e =y (i) e,t max[0, \u1ef9e \u2022 y e,t (i) -h t (i) \u2022 (y (i) e,t -\u1ef9e ) ] + \u1ef9r =y (i) r,t max[0, \u1ef9r \u2022 y r,t (i) -\u03b1 t r \u2022 (y (i) r,t -\u1ef9r ) ]",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "KG Path Walker",
                "sec_num": "2.3.2"
            },
            {
                "text": "Once the model is trained, at each decoding step, we can rank the potential paths based on the sum of their zeroshot relevance (left) and softattention-based output path (right) scores:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "KG Path Walker",
                "sec_num": "2.3.2"
            },
            {
                "text": "y (i) e,t = argmax y (i) e \u2208V R,1 (y (i) e,t-1 ) h t \u2022 y (i) e + \u03b1 t,k r k \u2022 y (i) r (8)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "KG Path Walker",
                "sec_num": "2.3.2"
            },
            {
                "text": "Adversarial Transfer Learning: if domain labels (y d ) are available (e.g. movie, book, sports, etc.), we can utilize these labels to further aid training by extracting transferrable features and learning optimal paths conditioned on domain embeddings (Ganin et al., 2016) . We implement adversarial transfer learning for DialKG Walker as follows and study this specific setting in one of our experiments to demonstrate that the model can better generalize over multiple domains:",
                "cite_spans": [
                    {
                        "start": 252,
                        "end": 272,
                        "text": "(Ganin et al., 2016)",
                        "ref_id": "BIBREF10"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "KG Path Walker",
                "sec_num": "2.3.2"
            },
            {
                "text": "L = L f + L walk + Entropy(\u03c3(W d x), y d ) h t = WALK([x; (W d x)], z t ) (9)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "KG Path Walker",
                "sec_num": "2.3.2"
            },
            {
                "text": "3 Dataset: OpenDialKG",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "KG Path Walker",
                "sec_num": "2.3.2"
            },
            {
                "text": "To empirically evaluate the proposed approach, we collected a new dataset, OpenDialKG, of chat conversations between two agents engaging in a dialog about a given topic (91K turns across 15K dialog sessions). Each dialog is paired with its corresponding \"KG paths\" that weave together the KG entities and relations that are mentioned in the dialog. This parallel corpus of textual dialogs and corresponding KG walks enables learning models that ground the implicit reasoning in human conversations to discrete KG operations.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "KG Path Walker",
                "sec_num": "2.3.2"
            },
            {
                "text": "Wizard-of-Oz setup The dialogs were generated in a Wizard-of-Oz setting (Shah et al., 2018) by connecting two crowd-workers to engage in a chat session, with the joint goal of creating natural and engaging dialogs. The first agent is given a seed entity and asked to initiate a conversation about that entity. The second agent is provided with a list of facts relevant to that entity, and asked to choose the most natural and relevant facts and use them to frame a free-form conversational response. Each fact is a 1-hop or 2-hop path initiating from the conversation topic. After the second agent sends their response, various new multi-hop facts from KG are surfaced to include paths initiating from new entities introduced in the latest message. This process allows the conversation participants to annotate any new fact or entity they want to introduce at each turn, along with the groundtruth KG walk path that connect the two KG entities. At this point the first agent is instructed to continue the conversation by choosing among the updated set of facts and framing a new message. This cycle continues for 6 messages per session on average spanning multiple KG paths, until one of the agents decides to end the conversation (e.g. the task goal is met).",
                "cite_spans": [
                    {
                        "start": 72,
                        "end": 91,
                        "text": "(Shah et al., 2018)",
                        "ref_id": "BIBREF28"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "KG Path Walker",
                "sec_num": "2.3.2"
            },
            {
                "text": "We did two separate collections: a recommendation task where the second agent acts as an assistant who is providing useful recommendations to the user, and a chit-chat task where both agents act as users engaging in open-ended chat about a particular topic. To ensure sufficient separation of the dialog content, we used entities related to movies (titles, actors, directors) and books (titles, authors) for the recommendation task, and entities related to sports (athletes, teams) and music (singers) for the chit-chat task (Table 1 ). Seed entities for each domain are crawled from various public resources (e.g. IMDB top movies list, top athletes list, etc.) and linked with the corresponding KG entities.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 532,
                        "end": 533,
                        "text": "1",
                        "ref_id": "TABREF0"
                    }
                ],
                "eq_spans": [],
                "section": "KG Path Walker",
                "sec_num": "2.3.2"
            },
            {
                "text": "KG sources: We use the Freebase (Bast et al., 2014) KG which is a publicly available and comprehensive source of general-knowledge facts. To reduce noise, we filter tail-end entities based on their prominence scores, the resulting KG of which consists of total 1,190,658 fact triples over top 100,813 entities and 1,358 relations.",
                "cite_spans": [
                    {
                        "start": 32,
                        "end": 51,
                        "text": "(Bast et al., 2014)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "KG Path Walker",
                "sec_num": "2.3.2"
            },
            {
                "text": "We randomly split the dialog sessions into train (70%), validation (15%), and test sets (15%).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "KG Path Walker",
                "sec_num": "2.3.2"
            },
            {
                "text": "Task: Given a set of KG entity mentions from current turn, and dialog history of all current and previous sentences, the goal is to build a robust model that can retrieve a set of natural entities to mention from a large-scale KG that resemble human responses. Note that end-to-end generation of sentences (e.g. based on the retrieved entities) is not part of this study -instead, we focus on the important challenge of scaling the conversational reasoning and knowledge retrieval task to opendomain dialogs, requiring an aggressive subset selection (from 1M+ facts subset of Freebase).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Empirical Evaluation",
                "sec_num": "4"
            },
            {
                "text": "We choose as baselines the following state-of-theart approaches that augment external knowledge to dialog systems for various tasks (see Section 5 for details), and modify accordingly to fit to our entity retrieval task (e.g. we use the same 1M-facts FreeBase KG for all of the baselines):",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baselines",
                "sec_num": "4.1"
            },
            {
                "text": "\u2022 seq2seq (Sutskever et al., 2014) with dialog contexts + zeroshot: we apply the seq2seq approach for entity path generation, given all of the dialog contexts. To make this baseline stronger, we add a zeroshot learning layer in the KG embeddings space (replacing typical softmax layers to improve generality) for entity token decoding.",
                "cite_spans": [
                    {
                        "start": 10,
                        "end": 34,
                        "text": "(Sutskever et al., 2014)",
                        "ref_id": "BIBREF30"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baselines",
                "sec_num": "4.1"
            },
            {
                "text": "\u2022 Tri-LSTM (Young et al., 2018) : encodes each utterance and all of its related facts within 1-hop from a KG to retrieve a response from a small (N=10) pre-defined sentence bank. We modify the retrieval bank to be the facts from the KG instead.",
                "cite_spans": [
                    {
                        "start": 11,
                        "end": 31,
                        "text": "(Young et al., 2018)",
                        "ref_id": "BIBREF36"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baselines",
                "sec_num": "4.1"
            },
            {
                "text": "\u2022 Extended Enc-Dec (Parthasarathi and Pineau, 2018) : conditions response generation with external knowledge vector input. A response entity token is generated at its final softmax layer, hence not utilizing structural information from KG.",
                "cite_spans": [
                    {
                        "start": 19,
                        "end": 51,
                        "text": "(Parthasarathi and Pineau, 2018)",
                        "ref_id": "BIBREF24"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baselines",
                "sec_num": "4.1"
            },
            {
                "text": "We also consider several configurations of our proposed approach to examine contributions of each component (input modalities (E): entities, (S): sentence, (D): dialog contexts).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baselines",
                "sec_num": "4.1"
            },
            {
                "text": "\u2022 (Proposed; E+S+D): is the proposed approach as described in Figure 2 \u2022 (E+S): relies only on its previous sentence and excludes dialog history from input.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 69,
                        "end": 70,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Baselines",
                "sec_num": "4.1"
            },
            {
                "text": "\u2022 (E): only uses starting KG entities as input contexts, and excludes any textual context.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baselines",
                "sec_num": "4.1"
            },
            {
                "text": "Parameters: We tune the parameters of each model with the following search space (bold in-dicate the choice for our final model): KG embeddings size: {64, 128, 256, 512}, LSTM hidden states: {64, 128, 256, 512}, word embeddings size: {100, 200, 300}, max dialog window size: {2, 3, 4, 5}. We optimize the parameters with Adagrad (Duchi et al., 2011) with batch size 10, learning rate 0.01, epsilon 10 -8 , and decay 0.1.",
                "cite_spans": [
                    {
                        "start": 329,
                        "end": 349,
                        "text": "(Duchi et al., 2011)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "4.2"
            },
            {
                "text": "In-domain evaluation: Table 2 shows the generation results of the top-k predictions of the model for in-domain train and test pairs (train & test on: all domains / train & test on: movie domain split). It can be seen that the proposed Di-alKG Walker model outperforms other state-ofthe-art baselines, especially for recalls at small ks. Specifically, when textual contexts are added as input (E+S and E+S+D), the model learns to condition its walk path output on textual contexts, thus outperforming the non-textual ablation model (E). seq2seq and Tri-LSTM models consider the nodes connected via all possible relations as candidates in the final layer (without pruning), resulting in extensive search space and consequently poor recall performance. In addition, Tri-LSTM only considers the facts connected via 1-hop relations as input contexts, which limits its prediction for multi-hop facts. Ext-ED relies its prediction in 5 : Human evaluation: \"Which response is the most natural for given dialog context?\" (metric: % of cases chosen as top-k response by the raters) the final softmax layer, which typically performs poorly for a large number of output class, compared to zeroshot learning approaches.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 28,
                        "end": 29,
                        "text": "2",
                        "ref_id": "TABREF1"
                    },
                    {
                        "start": 927,
                        "end": 928,
                        "text": "5",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "4.2"
            },
            {
                "text": "Cross-domain evaluation: Table 3 demonstrates that the DialKG Walker model can generalize to multiple domains better than the baseline approaches (train: movie & test: book / train: movie & test: music). This result indicates that our method also allows for zeroshot pruning by relations based on their proximity in the KG embeddings space, thus effective in cross-domain cases as well. For example, relations 'scenario by' and 'author' are close neighbors in the KG embeddings space, thus allowing for zeroshot prediction in cross-domain tests, although their training examples usually appear in two separate domains: movie and book.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 31,
                        "end": 32,
                        "text": "3",
                        "ref_id": "TABREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "4.2"
            },
            {
                "text": "Human evaluation: To compare the subjective quality of the models, i.e. the relative naturalness and relevance of the generated KG paths, we performed a human evaluation where paid raters were shown partial dialogs taken from the test dataset, along with the top 2 paths output from each model. The rater was asked to choose the 3 most appropriate paths for continuing the dialog. We evaluated 250 dialogs, showing each dialog to 3 raters, for a total 750 tasks. We report the % of cases when a top-k chosen fact was generated by each of the models (Table 5 ). The numbers add up to more than 100% as models can generate identical paths.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 556,
                        "end": 557,
                        "text": "5",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "4.2"
            },
            {
                "text": "If such a path is chosen by the rater, it is counted towards each of the models that generated the path.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "4.2"
            },
            {
                "text": "We show that the generated responses by our proposed methods achieve the highest scores in all top-k evaluation, validating that the model can output more natural human-like responses.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "4.2"
            },
            {
                "text": "Transfer learning: In Figure 3 , we show that cross-domain performance can greatly improve with a relatively small addition of in-domain target data, via the transfer learning approaches. Specifically, it can be seen that (TL:Adv), which simultaneously trains for both source and target data (effectively doubling the training size) with additional adversarial discriminator for source and target domains, achieves the best performance especially for domains that are semantically close (e.g. movie and book). (TL:FT) transfers knowledge from a pre-trained source model via finetuning (hence requiring significantly less training resources), and effectively avoids \"cold start\" training (Moon et al., 2015) . This result shows that the DialKG model can quickly adapt to other new low-resource domains and improve upon the zeroshot cross-domain performance, demonstrating its potential capability to reason on open-ended conversations.",
                "cite_spans": [
                    {
                        "start": 687,
                        "end": 706,
                        "text": "(Moon et al., 2015)",
                        "ref_id": "BIBREF20"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 29,
                        "end": 30,
                        "text": "3",
                        "ref_id": "FIGREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "4.2"
            },
            {
                "text": "Error analysis: Table 4 shows some of the example output from each model (as well as groundtruth responses), given dialog contexts. In general, the DialKG Walker tends to explore more multihop relations than other baselines in order to generate natural and engaging entities, which consequently improves the diversity of answers. Note that if the graph decoder arrives at a sufficiently good entity to generate, it stops its traversal operation and outputs the most viable entity based on the relevance score. Some of the models do not take into account the dialog history, hence generating redundant topics from previous turns. There are some cases where the final entity prediction is different from the ground-truth, whereas its relation path is correctly predicted. The generated entities are often still considered valid and natural, because the proposed model uses zeroshot relevance score to best predict the candidates.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 22,
                        "end": 23,
                        "text": "4",
                        "ref_id": "TABREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "4.2"
            },
            {
                "text": "Knowledge augmented dialog systems: Young et al. (2018) propose to explicitly augment input text with concepts expanded via 1-hop relations (where KG triples are represented in the sentence embeddings space), and He et al. (2017) propose a system which iteratively updates KG embeddings and attends over connected entities for response generation. However, several challenges remain to scale the simulated knowledge graph used in the study to our open-ended and large-scale KG with 1M+ facts. Other line of work (Parthasarathi and Pineau, 2018; Ghazvininejad et al., 2018; Long et al., 2017) uses embedding vectors obtained from external knowledge sources (e.g. NELL (Carlson et al., 2010) , Wikipedia, Freebase (Bast et al., 2014) , free-form text, etc.) as an auxiliary input to the model in dialog generation. Our model extends the previous work by (1) explicitly modeling output reasoning paths in a structured KG, (2) by introducing an attentionbased multi-hop concept decoder to improve both recall and precision.",
                "cite_spans": [
                    {
                        "start": 36,
                        "end": 55,
                        "text": "Young et al. (2018)",
                        "ref_id": "BIBREF36"
                    },
                    {
                        "start": 213,
                        "end": 229,
                        "text": "He et al. (2017)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 512,
                        "end": 544,
                        "text": "(Parthasarathi and Pineau, 2018;",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 545,
                        "end": 572,
                        "text": "Ghazvininejad et al., 2018;",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 573,
                        "end": 591,
                        "text": "Long et al., 2017)",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 667,
                        "end": 689,
                        "text": "(Carlson et al., 2010)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 712,
                        "end": 731,
                        "text": "(Bast et al., 2014)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discussion and Related Work",
                "sec_num": "5"
            },
            {
                "text": "End-to-end dialog systems: Several models and corresponding datasets have recently been published. Most work focuses on task or goal oriented dialog systems such as conversational recommendations (Salem et al., 2014; Bordes et al., 2017; Sun and Zhang, 2018; Dalton, 2018) , information querying (Williams et al., 2017; de Vries et al., 2018; Reddy et al., 2018) , etc., with datasets collected mostly through bootstrapped simulations (Bordes et al., 2017) , Wizard-of-Oz setup (Zhang et al., 2018; Wei et al., 2018) , or online corpus (Li et al., 2016) . Our OpenDialKG corpus is unique in that it includes open-ended natural human conversations over multiple scenarios (e.g. chit-chat and recommendation on various domains), where reasoning paths from each dialog are annotated with their corresponding discrete KG operations. Our work can also be viewed as extending the conventional state-tracking approaches (Henderson et al., 2014) to more flexible KG path as states.",
                "cite_spans": [
                    {
                        "start": 196,
                        "end": 216,
                        "text": "(Salem et al., 2014;",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 217,
                        "end": 237,
                        "text": "Bordes et al., 2017;",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 238,
                        "end": 258,
                        "text": "Sun and Zhang, 2018;",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 259,
                        "end": 272,
                        "text": "Dalton, 2018)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 296,
                        "end": 319,
                        "text": "(Williams et al., 2017;",
                        "ref_id": "BIBREF34"
                    },
                    {
                        "start": 320,
                        "end": 342,
                        "text": "de Vries et al., 2018;",
                        "ref_id": "BIBREF31"
                    },
                    {
                        "start": 343,
                        "end": 362,
                        "text": "Reddy et al., 2018)",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 435,
                        "end": 456,
                        "text": "(Bordes et al., 2017)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 478,
                        "end": 498,
                        "text": "(Zhang et al., 2018;",
                        "ref_id": "BIBREF37"
                    },
                    {
                        "start": 499,
                        "end": 516,
                        "text": "Wei et al., 2018)",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 536,
                        "end": 553,
                        "text": "(Li et al., 2016)",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 913,
                        "end": 937,
                        "text": "(Henderson et al., 2014)",
                        "ref_id": "BIBREF13"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discussion and Related Work",
                "sec_num": "5"
            },
            {
                "text": "KG embeddings and inference: Several methods have been proposed for KG inference tasks (e.g. edge prediction), which include neural models trained to discern positive and negative triples (Bordes et al., 2013; Wang et al., 2014; Nickel et al., 2016; Dettmers et al., 2018) , or algorithms with discrete KG operations on structured data (Lao et al., 2011; Chen et al., 2015) . KG embeddings have been shown effective in other NLP tasks when they are used as target labels for classification tasks, which also allows for effective transfer learning (Moon and Carbonell, 2017) . For effective application of KG embeddings in NLP tasks, recent studies (Kartsaklis et al., 2018) proposed to map word embeddings and KG embeddings via end-to-end tasks. In contrast to the line of work on KG edge prediction, we aim to learn an optimal path within existing paths that resemble human reasoning in conversations.",
                "cite_spans": [
                    {
                        "start": 188,
                        "end": 209,
                        "text": "(Bordes et al., 2013;",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 210,
                        "end": 228,
                        "text": "Wang et al., 2014;",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 229,
                        "end": 249,
                        "text": "Nickel et al., 2016;",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 250,
                        "end": 272,
                        "text": "Dettmers et al., 2018)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 336,
                        "end": 354,
                        "text": "(Lao et al., 2011;",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 355,
                        "end": 373,
                        "text": "Chen et al., 2015)",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 547,
                        "end": 573,
                        "text": "(Moon and Carbonell, 2017)",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 648,
                        "end": 673,
                        "text": "(Kartsaklis et al., 2018)",
                        "ref_id": "BIBREF14"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discussion and Related Work",
                "sec_num": "5"
            },
            {
                "text": "We study conversational reasoning grounded on knowledge graphs, and formulate an approach in which the model learns to navigate a largescale, open-ended KG given conversational contexts. For this study, we collect a newly annotated Dialog \u2194 KG parallel corpus of 15K humanto-human dialogs which includes ground-truth annotation of each dialog turn to its reasoning reference in a large-scale common fact KG. Our proposed DialKG Walker model improves upon the state-of-the-art knowledge-augmented conversation models by 1) a novel attention-based graph decoder that penalizes decoding of unnatural paths which effectively prunes candidate entities and paths from a large search space (1.1M facts), 2) a zeroshot learning model that predicts its relevance score in the KG embeddings space, combined score of which is used for candidate ranking. The empirical results from in-domain, cross-domain, and transfer learning evaluation demonstrate the efficacy of the proposed model in domain-agnostic conversational reasoning.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusions",
                "sec_num": "6"
            }
        ],
        "back_matter": [],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Easy access to the freebase dataset",
                "authors": [
                    {
                        "first": "Hannah",
                        "middle": [],
                        "last": "Bast",
                        "suffix": ""
                    },
                    {
                        "first": "Florian",
                        "middle": [],
                        "last": "Baurle",
                        "suffix": ""
                    },
                    {
                        "first": "Bjorn",
                        "middle": [],
                        "last": "Buchhold",
                        "suffix": ""
                    },
                    {
                        "first": "Elmar",
                        "middle": [],
                        "last": "Haussmann",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Hannah Bast, Florian Baurle, Bjorn Buchhold, and El- mar Haussmann. 2014. Easy access to the freebase dataset. In WWW.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Learning end-to-end goal-oriented dialog",
                "authors": [
                    {
                        "first": "Antoine",
                        "middle": [],
                        "last": "Bordes",
                        "suffix": ""
                    },
                    {
                        "first": "Y-Lan",
                        "middle": [],
                        "last": "Boureau",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Weston",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Antoine Bordes, Y-Lan Boureau, and Jason Weston. 2017. Learning end-to-end goal-oriented dialog. ICLR.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Translating embeddings for modeling multirelational data",
                "authors": [
                    {
                        "first": "Antoine",
                        "middle": [],
                        "last": "Bordes",
                        "suffix": ""
                    },
                    {
                        "first": "Nicolas",
                        "middle": [],
                        "last": "Usunier",
                        "suffix": ""
                    },
                    {
                        "first": "Alberto",
                        "middle": [],
                        "last": "Garcia-Duran",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Weston",
                        "suffix": ""
                    },
                    {
                        "first": "Oksana",
                        "middle": [],
                        "last": "Yakhnenko",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "NIPS",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Antoine Bordes, Nicolas Usunier, Alberto Garcia- Duran, Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling multi- relational data. In NIPS.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Toward an architecture for never-ending language learning",
                "authors": [
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Carlson",
                        "suffix": ""
                    },
                    {
                        "first": "Justin",
                        "middle": [],
                        "last": "Betteridge",
                        "suffix": ""
                    },
                    {
                        "first": "Bryan",
                        "middle": [],
                        "last": "Kisiel",
                        "suffix": ""
                    },
                    {
                        "first": "Burr",
                        "middle": [],
                        "last": "Settles",
                        "suffix": ""
                    },
                    {
                        "first": "Tom",
                        "middle": [
                            "M"
                        ],
                        "last": "Estevam R Hruschka",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Mitchell",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "AAAI",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R Hruschka, and Tom M Mitchell. 2010. Toward an architecture for never-ending lan- guage learning. In AAAI.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Gunrock: Building a human-like social bot by leveraging large scale real user data",
                "authors": [
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Cy Chen",
                        "suffix": ""
                    },
                    {
                        "first": "W",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Y",
                        "middle": [
                            "M"
                        ],
                        "last": "Wen",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Jesse",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Chau",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Bhowmick",
                        "suffix": ""
                    },
                    {
                        "first": "G",
                        "middle": [],
                        "last": "Iyer",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Sreenivasulu",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Cheng",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Bhandare",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "CY Chen, D Yu, W Wen, YM Yang, J Zhang, M Zhou, K Jesse, A Chau, A Bhowmick, S Iyer, G Sreeniva- sulu, R Cheng, A Bhandare, and Z Yu. 2018. Gun- rock: Building a human-like social bot by leveraging large scale real user data. In 2nd Alexa Prize.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Jointly modeling inter-slot relations by random walk on knowledge graphs for unsupervised spoken language understanding",
                "authors": [
                    {
                        "first": "Yun-Nung",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "William",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Wang",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Alexander",
                        "middle": [],
                        "last": "Rudnicky",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "NAACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yun-Nung Chen, William Yang Wang, and Alexan- der Rudnicky. 2015. Jointly modeling inter-slot re- lations by random walk on knowledge graphs for unsupervised spoken language understanding. In NAACL.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Supervised learning of universal sentence representations from natural language inference data",
                "authors": [
                    {
                        "first": "Alexis",
                        "middle": [],
                        "last": "Conneau",
                        "suffix": ""
                    },
                    {
                        "first": "Douwe",
                        "middle": [],
                        "last": "Kiela",
                        "suffix": ""
                    },
                    {
                        "first": "Holger",
                        "middle": [],
                        "last": "Schwenk",
                        "suffix": ""
                    },
                    {
                        "first": "Lo\u00efc",
                        "middle": [],
                        "last": "Barrault",
                        "suffix": ""
                    },
                    {
                        "first": "Antoine",
                        "middle": [],
                        "last": "Bordes",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "EMNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00efc Barrault, and Antoine Bordes. 2017. Supervised learning of universal sentence representations from natural language inference data. In EMNLP.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Vote goat: Conversational movie recommendation",
                "authors": [
                    {
                        "first": "Jeff",
                        "middle": [],
                        "last": "Dalton",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "SIGIR",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jeff Dalton. 2018. Vote goat: Conversational movie recommendation. SIGIR.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Convolutional 2d knowledge graph embeddings",
                "authors": [
                    {
                        "first": "Tim",
                        "middle": [],
                        "last": "Dettmers",
                        "suffix": ""
                    },
                    {
                        "first": "Pasquale",
                        "middle": [],
                        "last": "Minervini",
                        "suffix": ""
                    },
                    {
                        "first": "Pontus",
                        "middle": [],
                        "last": "Stenetorp",
                        "suffix": ""
                    },
                    {
                        "first": "Sebastian",
                        "middle": [],
                        "last": "Riedel",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "AAAI",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. 2018. Convolutional 2d knowledge graph embeddings. In AAAI.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Adaptive subgradient methods for online learning and stochastic optimization",
                "authors": [
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Duchi",
                        "suffix": ""
                    },
                    {
                        "first": "Elad",
                        "middle": [],
                        "last": "Hazan",
                        "suffix": ""
                    },
                    {
                        "first": "Yoram",
                        "middle": [],
                        "last": "Singer",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "JMLR",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. JMLR.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Domain-adversarial training of neural networks",
                "authors": [
                    {
                        "first": "Yaroslav",
                        "middle": [],
                        "last": "Ganin",
                        "suffix": ""
                    },
                    {
                        "first": "Evgeniya",
                        "middle": [],
                        "last": "Ustinova",
                        "suffix": ""
                    },
                    {
                        "first": "Hana",
                        "middle": [],
                        "last": "Ajakan",
                        "suffix": ""
                    },
                    {
                        "first": "Pascal",
                        "middle": [],
                        "last": "Germain",
                        "suffix": ""
                    },
                    {
                        "first": "Hugo",
                        "middle": [],
                        "last": "Larochelle",
                        "suffix": ""
                    },
                    {
                        "first": "Mario",
                        "middle": [],
                        "last": "Franc \u00b8ois Laviolette",
                        "suffix": ""
                    },
                    {
                        "first": "Victor",
                        "middle": [],
                        "last": "Marchand",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Lempitsky",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Franc \u00b8ois Lavi- olette, Mario Marchand, and Victor Lempitsky. 2016. Domain-adversarial training of neural net- works. JMLR.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "A knowledgegrounded neural conversation model",
                "authors": [
                    {
                        "first": "Marjan",
                        "middle": [],
                        "last": "Ghazvininejad",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Brockett",
                        "suffix": ""
                    },
                    {
                        "first": "Ming-Wei",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "Bill",
                        "middle": [],
                        "last": "Dolan",
                        "suffix": ""
                    },
                    {
                        "first": "Jianfeng",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Scott",
                        "middle": [],
                        "last": "Wen-Tau Yih",
                        "suffix": ""
                    },
                    {
                        "first": "Michel",
                        "middle": [],
                        "last": "Galley",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "AAAI",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Marjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Scott Wen-tau Yih, and Michel Galley. 2018. A knowledge- grounded neural conversation model. In AAAI.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Learning symmetric collaborative dialogue agents with dynamic knowledge graph embeddings",
                "authors": [
                    {
                        "first": "He",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Anusha",
                        "middle": [],
                        "last": "Balakrishnan",
                        "suffix": ""
                    },
                    {
                        "first": "Mihail",
                        "middle": [],
                        "last": "Eric",
                        "suffix": ""
                    },
                    {
                        "first": "Percy",
                        "middle": [],
                        "last": "Liang",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "He He, Anusha Balakrishnan, Mihail Eric, and Percy Liang. 2017. Learning symmetric collaborative di- alogue agents with dynamic knowledge graph em- beddings. ACL.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "The second dialog state tracking challenge",
                "authors": [
                    {
                        "first": "Matthew",
                        "middle": [],
                        "last": "Henderson",
                        "suffix": ""
                    },
                    {
                        "first": "Blaise",
                        "middle": [],
                        "last": "Thomson",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [
                            "D"
                        ],
                        "last": "Williams",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "SIGDIAL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Matthew Henderson, Blaise Thomson, and Jason D Williams. 2014. The second dialog state tracking challenge. In SIGDIAL.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Mapping text to knowledge graph entities using multi-sense lstms",
                "authors": [
                    {
                        "first": "Dimitri",
                        "middle": [],
                        "last": "Kartsaklis",
                        "suffix": ""
                    },
                    {
                        "first": "Mohammad",
                        "middle": [],
                        "last": "Taher Pilehvar",
                        "suffix": ""
                    },
                    {
                        "first": "Nigel",
                        "middle": [],
                        "last": "Collier",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dimitri Kartsaklis, Mohammad Taher Pilehvar, and Nigel Collier. 2018. Mapping text to knowledge graph entities using multi-sense lstms. EMNLP.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Random walk inference and learning in a large scale knowledge base",
                "authors": [
                    {
                        "first": "Ni",
                        "middle": [],
                        "last": "Lao",
                        "suffix": ""
                    },
                    {
                        "first": "Tom",
                        "middle": [],
                        "last": "Mitchell",
                        "suffix": ""
                    },
                    {
                        "first": "William",
                        "middle": [
                            "W"
                        ],
                        "last": "Cohen",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "EMNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ni Lao, Tom Mitchell, and William W. Cohen. 2011. Random walk inference and learning in a large scale knowledge base. In EMNLP.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "A persona-based neural conversation model. ACL",
                "authors": [
                    {
                        "first": "Jiwei",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Michel",
                        "middle": [],
                        "last": "Galley",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Brockett",
                        "suffix": ""
                    },
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Georgios",
                        "suffix": ""
                    },
                    {
                        "first": "Jianfeng",
                        "middle": [],
                        "last": "Spithourakis",
                        "suffix": ""
                    },
                    {
                        "first": "Bill",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Dolan",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jiwei Li, Michel Galley, Chris Brockett, Georgios P Spithourakis, Jianfeng Gao, and Bill Dolan. 2016. A persona-based neural conversation model. ACL.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "A knowledge enhanced generative conversational service agent",
                "authors": [
                    {
                        "first": "Yinong",
                        "middle": [],
                        "last": "Long",
                        "suffix": ""
                    },
                    {
                        "first": "Jianan",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Zhen",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Zongsheng",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Baoxun",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Zhuoran",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "NIPS DSTC6 Workshop",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yinong Long, Jianan Wang, Zhen Xu, Zongsheng Wang, Baoxun Wang, and Zhuoran Wang. 2017. A knowledge enhanced generative conversational ser- vice agent. In NIPS DSTC6 Workshop.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "A dialog research software platform",
                "authors": [
                    {
                        "first": "A",
                        "middle": [
                            "H"
                        ],
                        "last": "Miller",
                        "suffix": ""
                    },
                    {
                        "first": "W",
                        "middle": [],
                        "last": "Feng",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Fisch",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Lu",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Batra",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Bordes",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Parikh",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Weston",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "A. H. Miller, W. Feng, A. Fisch, J. Lu, D. Batra, A. Bordes, D. Parikh, and J. Weston. 2017. Parlai: A dialog research software platform. EMNLP.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Completely heterogeneous transfer learning with attention: What and what not to transfer",
                "authors": [
                    {
                        "first": "Seungwhan",
                        "middle": [],
                        "last": "Moon",
                        "suffix": ""
                    },
                    {
                        "first": "Jaime",
                        "middle": [],
                        "last": "Carbonell",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "IJCAI",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Seungwhan Moon and Jaime Carbonell. 2017. Com- pletely heterogeneous transfer learning with atten- tion: What and what not to transfer. IJCAI.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Multimodal transfer deep learning with applications in audio-visual recognition",
                "authors": [
                    {
                        "first": "Seungwhan",
                        "middle": [],
                        "last": "Moon",
                        "suffix": ""
                    },
                    {
                        "first": "Suyoun",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "Haohan",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "NIPS MMML Workshop",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Seungwhan Moon, Suyoun Kim, and Haohan Wang. 2015. Multimodal transfer deep learning with appli- cations in audio-visual recognition. In NIPS MMML Workshop.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Multimodal named entity recognition for short social media posts",
                "authors": [
                    {
                        "first": "Seungwhan",
                        "middle": [],
                        "last": "Moon",
                        "suffix": ""
                    },
                    {
                        "first": "Leonard",
                        "middle": [],
                        "last": "Neves",
                        "suffix": ""
                    },
                    {
                        "first": "Vitor",
                        "middle": [],
                        "last": "Carvalho",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Seungwhan Moon, Leonard Neves, and Vitor Carvalho. 2018a. Multimodal named entity recognition for short social media posts. NAACL.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Zeroshot multimodal named entity disambiguation for noisy social media posts. ACL",
                "authors": [
                    {
                        "first": "Seungwhan",
                        "middle": [],
                        "last": "Moon",
                        "suffix": ""
                    },
                    {
                        "first": "Leonard",
                        "middle": [],
                        "last": "Neves",
                        "suffix": ""
                    },
                    {
                        "first": "Vitor",
                        "middle": [],
                        "last": "Carvalho",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Seungwhan Moon, Leonard Neves, and Vitor Carvalho. 2018b. Zeroshot multimodal named entity disam- biguation for noisy social media posts. ACL.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Holographic embeddings of knowledge graphs",
                "authors": [
                    {
                        "first": "Maximilian",
                        "middle": [],
                        "last": "Nickel",
                        "suffix": ""
                    },
                    {
                        "first": "Lorenzo",
                        "middle": [],
                        "last": "Rosasco",
                        "suffix": ""
                    },
                    {
                        "first": "Tomaso",
                        "middle": [],
                        "last": "Poggio",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Maximilian Nickel, Lorenzo Rosasco, and Tomaso Poggio. 2016. Holographic embeddings of knowl- edge graphs. AAAI.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Extending neural generative conversational model using external knowledge sources",
                "authors": [
                    {
                        "first": "Prasanna",
                        "middle": [],
                        "last": "Parthasarathi",
                        "suffix": ""
                    },
                    {
                        "first": "Joelle",
                        "middle": [],
                        "last": "Pineau",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Prasanna Parthasarathi and Joelle Pineau. 2018. Ex- tending neural generative conversational model us- ing external knowledge sources. EMNLP.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Glove: Global vectors for word representation",
                "authors": [
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Pennington",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "EMNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jeffrey Pennington, Richard Socher, and Christo- pher D. Manning. 2014. Glove: Global vectors for word representation. In EMNLP.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Coqa: A conversational question answering challenge",
                "authors": [
                    {
                        "first": "Siva",
                        "middle": [],
                        "last": "Reddy",
                        "suffix": ""
                    },
                    {
                        "first": "Danqi",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1808.07042"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Siva Reddy, Danqi Chen, and Christopher D Manning. 2018. Coqa: A conversational question answering challenge. arXiv preprint arXiv:1808.07042.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "History-guided conversational recommendation",
                "authors": [
                    {
                        "first": "Yasser",
                        "middle": [],
                        "last": "Salem",
                        "suffix": ""
                    },
                    {
                        "first": "Jun",
                        "middle": [],
                        "last": "Hong",
                        "suffix": ""
                    },
                    {
                        "first": "Weiru",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yasser Salem, Jun Hong, and Weiru Liu. 2014. History-guided conversational recommendation. In WWW.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Bootstrapping a neural conversational agent with dialogue self-play, crowdsourcing and on-line reinforcement learning",
                "authors": [
                    {
                        "first": "Pararth",
                        "middle": [],
                        "last": "Shah",
                        "suffix": ""
                    },
                    {
                        "first": "Dilek",
                        "middle": [],
                        "last": "Hakkani-Tur",
                        "suffix": ""
                    },
                    {
                        "first": "Bing",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Gokhan",
                        "middle": [],
                        "last": "Tur",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "NAACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Pararth Shah, Dilek Hakkani-Tur, Bing Liu, and Gokhan Tur. 2018. Bootstrapping a neural conversa- tional agent with dialogue self-play, crowdsourcing and on-line reinforcement learning. In NAACL.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Conversational recommender system",
                "authors": [
                    {
                        "first": "Yueming",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Yi",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yueming Sun and Yi Zhang. 2018. Conversational rec- ommender system. SIGIR.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "Sequence to sequence learning with neural networks",
                "authors": [
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    },
                    {
                        "first": "Oriol",
                        "middle": [],
                        "last": "Vinyals",
                        "suffix": ""
                    },
                    {
                        "first": "Quoc V",
                        "middle": [],
                        "last": "Le",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "NIPS",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural net- works. In NIPS.",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "Talk the walk: Navigating new york city through grounded dialogue",
                "authors": [
                    {
                        "first": "Kurt",
                        "middle": [],
                        "last": "Harm De Vries",
                        "suffix": ""
                    },
                    {
                        "first": "Dhruv",
                        "middle": [],
                        "last": "Shuster",
                        "suffix": ""
                    },
                    {
                        "first": "Devi",
                        "middle": [],
                        "last": "Batra",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Parikh",
                        "suffix": ""
                    },
                    {
                        "first": "Douwe",
                        "middle": [],
                        "last": "Weston",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Kiela",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Harm de Vries, Kurt Shuster, Dhruv Batra, Devi Parikh, Jason Weston, and Douwe Kiela. 2018. Talk the walk: Navigating new york city through grounded dialogue. ECCV.",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "Knowledge graph embedding by translating on hyperplanes",
                "authors": [
                    {
                        "first": "Zhen",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Jianwen",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Jianlin",
                        "middle": [],
                        "last": "Feng",
                        "suffix": ""
                    },
                    {
                        "first": "Zheng",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "AAAI",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. 2014. Knowledge graph embedding by trans- lating on hyperplanes. In AAAI.",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "Airdialogue: An environment for goal-oriented dialogue research",
                "authors": [
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Quoc",
                        "middle": [],
                        "last": "Le",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Dai",
                        "suffix": ""
                    },
                    {
                        "first": "Jia",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "EMNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Wei Wei, Quoc Le, Andrew Dai, and Jia Li. 2018. Airdialogue: An environment for goal-oriented di- alogue research. In EMNLP.",
                "links": null
            },
            "BIBREF34": {
                "ref_id": "b34",
                "title": "Hybrid code networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning",
                "authors": [
                    {
                        "first": "Jason D",
                        "middle": [],
                        "last": "Williams",
                        "suffix": ""
                    },
                    {
                        "first": "Kavosh",
                        "middle": [],
                        "last": "Asadi",
                        "suffix": ""
                    },
                    {
                        "first": "Geoffrey",
                        "middle": [],
                        "last": "Zweig",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jason D Williams, Kavosh Asadi, and Geoffrey Zweig. 2017. Hybrid code networks: practical and efficient end-to-end dialog control with supervised and rein- forcement learning. ACL.",
                "links": null
            },
            "BIBREF35": {
                "ref_id": "b35",
                "title": "Hierarchical attention networks for document classification",
                "authors": [
                    {
                        "first": "Zichao",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Diyi",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Dyer",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaodong",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Smola",
                        "suffix": ""
                    },
                    {
                        "first": "Eduard",
                        "middle": [],
                        "last": "Hovy",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "NAACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, and Eduard Hovy. 2016. Hierarchical attention networks for document classification. In NAACL.",
                "links": null
            },
            "BIBREF36": {
                "ref_id": "b36",
                "title": "Augmenting end-to-end dialog systems with commonsense knowledge",
                "authors": [
                    {
                        "first": "Tom",
                        "middle": [],
                        "last": "Young",
                        "suffix": ""
                    },
                    {
                        "first": "Erik",
                        "middle": [],
                        "last": "Cambria",
                        "suffix": ""
                    },
                    {
                        "first": "Iti",
                        "middle": [],
                        "last": "Chaturvedi",
                        "suffix": ""
                    },
                    {
                        "first": "Minlie",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Hao",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Subham",
                        "middle": [],
                        "last": "Biswas",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tom Young, Erik Cambria, Iti Chaturvedi, Minlie Huang, Hao Zhou, and Subham Biswas. 2018. Aug- menting end-to-end dialog systems with common- sense knowledge. AAAI.",
                "links": null
            },
            "BIBREF37": {
                "ref_id": "b37",
                "title": "Personalizing dialogue agents: I have a dog",
                "authors": [
                    {
                        "first": "Saizheng",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Emily",
                        "middle": [],
                        "last": "Dinan",
                        "suffix": ""
                    },
                    {
                        "first": "Jack",
                        "middle": [],
                        "last": "Urbanek",
                        "suffix": ""
                    },
                    {
                        "first": "Arthur",
                        "middle": [],
                        "last": "Szlam",
                        "suffix": ""
                    },
                    {
                        "first": "Douwe",
                        "middle": [],
                        "last": "Kiela",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Weston",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, and Jason Weston. 2018. Per- sonalizing dialogue agents: I have a dog, do you have pets too? ACL.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 1: Conversational reasoning with a parallel (a) dialog and (b) knowledge graph (KG) corpus. Diverse topical jumps across open-ended multi-turn dialogs are annotated and grounded with a large-scale commonfact KG. To generate a KG entity response at each dialog turn, the model learns walkable paths within KG that lead to engaging and natural topics or entities given dialog context, while pruning non-ideal (albeit factually correct) KG paths among 1M+ candidate facts.",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Figure 2: Overall architecture. x = {x e ; x s ; x d } is encoded with the input encoder (left), aggregated via multiple attention mechanism. The decoder (right) predicts both the optimal paths and the final entities y = {y e ; y r } based on their zeroshot relevance scores as well as soft-attention based walk paths, which prunes unlikely entities.",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "via relations in R KG . We formulate the future entity retrieval task as: y = argmax y e \u2282V(xe) score f x\u2192y (x), y",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF3": {
                "num": null,
                "text": "Figure 3: Transfer learning results (r@5) of DialKG Walker at varying availability of target data with (a) Book and (b) Sports domains as a Target (Source: Movie). (TL:Adv): data transfer with adversarial discriminator for source and target domains, (TL:FT): model transfer with fine-tuning, (No-TL): target only.",
                "uris": null,
                "fig_num": "3",
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table><tr><td colspan=\"3\">Task: Recommendation Chit-chat</td><td>(All)</td></tr><tr><td colspan=\"3\">Domain: Movies Books Sports Music</td></tr><tr><td># of dialogs 6,429</td><td>5,891</td><td colspan=\"2\">2,495 858 15,673</td></tr><tr><td colspan=\"4\"># of turns 37,838 34,035 14,344 4,992 91,209</td></tr></table>",
                "type_str": "table",
                "text": "Task / domain distribution of OpenDialKG.",
                "html": null,
                "num": null
            },
            "TABREF1": {
                "content": "<table><tr><td>Input</td><td>Model</td><td colspan=\"3\">All Domains \u2192 All</td><td>Movie \u2192 Movie</td></tr><tr><td/><td/><td>r@1 3</td><td>5</td><td colspan=\"2\">10 25 r@1 3</td><td>5</td><td>10 25</td></tr><tr><td colspan=\"2\">E + S + D seq2seq (Sutskever et al., 2014)</td><td colspan=\"4\">3.1 18.3 29.7 44.1 60.2 3.0 13.4 23.4 38.5 55.5</td></tr><tr><td>E + S</td><td>Tri-LSTM (Young et al., 2018)</td><td colspan=\"4\">3.2 14.2 22.6 36.3 56.2 1.5 10.3 17.4 30.7 51.1</td></tr><tr><td>E + S</td><td colspan=\"5\">Ext-ED (Parthasarathi and Pineau, 2018) 1.9 5.8 9.0 13.3 19.0 1.3 5.4 7.8 11.8 15.8</td></tr><tr><td>E</td><td>DialKG Walker (ablation)</td><td colspan=\"4\">10.7 22.9 32.0 44.9 57.4 5.3 13.5 18.5 25.2 39.1</td></tr><tr><td>E + S</td><td>DialKG Walker (ablation)</td><td colspan=\"4\">11.3 23.3 31.0 44.0 60.5 7.2 19.2 27.9 40.7 58.7</td></tr><tr><td colspan=\"2\">E + S + D DialKG Walker (proposed)</td><td colspan=\"4\">13.2 26.1 35.3 47.9 62.2 7.8 20.0 27.9 40.4 58.6</td></tr><tr><td>Input</td><td>Model</td><td colspan=\"3\">Movie \u2192 Book</td><td>Movie \u2192 Music</td></tr><tr><td/><td/><td>r@1 3</td><td>5</td><td colspan=\"2\">10 25 r@1 3</td><td>5</td><td>10 25</td></tr><tr><td colspan=\"2\">E + S + D seq2seq (Sutskever et al., 2014)</td><td colspan=\"4\">2.9 21.3 35.1 50.6 64.2 1.5 12.1 19.7 34.9 49.4</td></tr><tr><td>E + S</td><td>Tri-LSTM (Young et al., 2018)</td><td colspan=\"4\">2.3 17.9 29.7 44.9 61.0 1.9 8.7 12.9 25.8 44.4</td></tr><tr><td>E + S</td><td colspan=\"5\">Ext-ED (Parthasarathi and Pineau, 2018) 2.0 7.9 11.2 16.4 22.4 1.3 2.6 3.8 4.1 8.3</td></tr><tr><td>E</td><td>DialKG Walker (ablation)</td><td colspan=\"4\">8.2 15.7 22.8 31.8 48.9 4.5 16.7 21.6 25.8 33.0</td></tr><tr><td>E + S</td><td>DialKG Walker (ablation)</td><td colspan=\"4\">12.6 28.6 38.6 54.1 65.6 6.0 15.9 22.8 33.0 47.5</td></tr><tr><td colspan=\"2\">E + S + D DialKG Walker (proposed)</td><td colspan=\"4\">13.5 28.8 39.5 52.6 64.8 5.3 13.3 19.7 28.8 38.0</td></tr></table>",
                "type_str": "table",
                "text": "In-domain (train/test on the same domain) response generation performance on the OpenDialKG dataset (metric: recall@k). Our proposed model is compared against state-of-the-art models as well as several ablation variations of the proposed model. All of the 100K+ KG entities are considered initial candidates for generation (before masking). E: entities, S: sentence, D: dialog contexts.",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table/>",
                "type_str": "table",
                "text": "",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table><tr><td colspan=\"2\">Input Dialog (start entity)</td><td/><td>Response</td></tr><tr><td/><td/><td>Model</td><td>Walk Path</td><td>Predicted Entity</td></tr><tr><td colspan=\"2\">A: Yes, I believe he [Muller] has played in Munich.</td><td>GT</td><td>award won by \u2192 position</td><td>Forward</td></tr><tr><td colspan=\"2\">B: He also won a Bravo Award. I think that's awesome!</td><td colspan=\"2\">KG Walker award won by</td><td>Lionel Messi</td></tr><tr><td>A: [response]</td><td/><td colspan=\"2\">Ext-ED award won by</td><td>Muller</td></tr><tr><td colspan=\"2\">A: Could you recommend a book by Mark Overstall?</td><td>GT</td><td>wrote \u2192 has genre</td><td>Romance</td></tr><tr><td>B: [response]</td><td/><td colspan=\"2\">KG Walker wrote \u2192 has genre</td><td>Romance</td></tr><tr><td/><td/><td colspan=\"2\">Ext-ED language</td><td>English</td></tr><tr><td colspan=\"2\">A: Do you like Lauren Oliver. I think her books are great!</td><td>GT</td><td>written by \u2192 wrote</td><td>Requiem</td></tr><tr><td colspan=\"2\">B: I do, Vanishing Girls is one of my favorite books.</td><td colspan=\"2\">KG Walker written by \u2192 wrote</td><td>Annabel</td></tr><tr><td>A: [response]</td><td/><td colspan=\"2\">Tri-LSTM released year</td><td>2015</td></tr><tr><td>A: What about the Oakland Raiders?</td><td/><td>GT</td><td>Champion</td><td>Packers</td></tr><tr><td colspan=\"2\">B: Oh yes, I do like them. I've been a fan since they were</td><td colspan=\"2\">KG Walker Champion</td><td>Packers</td></tr><tr><td colspan=\"4\">runner-up in Super Bowl II. What about you? // A: [response] seq2seq Runner-up \u2192 Is A</td><td>NFL Team</td></tr><tr><td colspan=\"2\">A: Do you like David Guetta? I enjoy his music.</td><td>GT</td><td>composer \u2192 composed</td><td>Club Can't Handle Me</td></tr><tr><td colspan=\"2\">B: Oh, I love his lyrics to Love is Gone and the song</td><td colspan=\"2\">KG Walker composer \u2192 composed</td><td>I Love It</td></tr><tr><td colspan=\"2\">Wild Ones. What are your favorites? // A: [response]</td><td colspan=\"2\">Tri-LSTM composer</td><td>David Guetta</td></tr><tr><td>Model</td><td>% in top-k k=1 k=2 k=3</td><td/><td/></tr><tr><td colspan=\"2\">(Parthasarathi and Pineau, 2018) 17.5 33.6 47.2</td><td/><td/></tr><tr><td colspan=\"2\">(Young et al., 2018) 30.8 50.1 70.3</td><td/><td/></tr><tr><td colspan=\"2\">(Sutskever et al., 2014) 31.5 57.7 73.1</td><td/><td/></tr><tr><td colspan=\"2\">KG Walker (proposed) 38.6 61.8 76.3</td><td/><td/></tr></table>",
                "type_str": "table",
                "text": "Error analysis: DialKG Walker with attention (ours) vs. baselines. Ground-truth response (GT) and model predictions of walk paths and future entities for the underlined entity mentions are shown. Dialogs are only partially shown due to space constraints. Table",
                "html": null,
                "num": null
            }
        }
    }
}