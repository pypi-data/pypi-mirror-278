{
    "paper_id": "D15-1028",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:03:00.973053Z"
    },
    "title": "Modeling Tweet Arrival Times using Log-Gaussian Cox Processes",
    "authors": [
        {
            "first": "Michal",
            "middle": [],
            "last": "Lukasik",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "The University of Sheffield",
                "location": {}
            },
            "email": "m.lukasik@shef.ac.uk"
        },
        {
            "first": "P",
            "middle": [
                "K"
            ],
            "last": "Srijith",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "The University of Sheffield",
                "location": {}
            },
            "email": "pk.srijith@shef.ac.uk"
        },
        {
            "first": "Trevor",
            "middle": [],
            "last": "Cohn",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "The University of Melbourne",
                "location": {}
            },
            "email": "t.cohn@unimelb.edu.au"
        },
        {
            "first": "Kalina",
            "middle": [],
            "last": "Bontcheva",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "The University of Sheffield",
                "location": {}
            },
            "email": "k.bontcheva@shef.ac.uk"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Research on modeling time series text corpora has typically focused on predicting what text will come next, but less well studied is predicting when the next text event will occur. In this paper we address the latter case, framed as modeling continuous inter-arrival times under a log-Gaussian Cox process, a form of inhomogeneous Poisson process which captures the varying rate at which the tweets arrive over time. In an application to rumour modeling of tweets surrounding the 2014 Ferguson riots, we show how interarrival times between tweets can be accurately predicted, and that incorporating textual features further improves predictions.",
    "pdf_parse": {
        "paper_id": "D15-1028",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Research on modeling time series text corpora has typically focused on predicting what text will come next, but less well studied is predicting when the next text event will occur. In this paper we address the latter case, framed as modeling continuous inter-arrival times under a log-Gaussian Cox process, a form of inhomogeneous Poisson process which captures the varying rate at which the tweets arrive over time. In an application to rumour modeling of tweets surrounding the 2014 Ferguson riots, we show how interarrival times between tweets can be accurately predicted, and that incorporating textual features further improves predictions.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Twitter is a popular micro-blogging service which provides real-time information on events happening across the world. Evolution of events over time can be monitored there with applications to disaster management, journalism etc. For example, Twitter has been used to detect the occurrence of earthquakes in Japan through user posts (Sakaki et al., 2010) . Modeling the temporal dynamics of tweets provides useful information about the evolution of events. Inter-arrival time prediction is a type of such modeling and has application in many settings featuring continuous time streaming text corpora, including journalism for event monitoring, real-time disaster monitoring and advertising on social media. For example, journalists track several rumours related to an event. Predicted arrival times of tweets can be applied for ranking rumours according to their activity and narrow the interest to investigate a rumour with a short interarrival time over that of a longer one.",
                "cite_spans": [
                    {
                        "start": 333,
                        "end": 354,
                        "text": "(Sakaki et al., 2010)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Modeling the inter-arrival time of tweets is a challenging task due to complex temporal patterns exhibited. Tweets associated with an event stream arrive at different rates at different points in time. For example, Figure 1a shows the arrival times (denoted by black crosses) of tweets associated with an example rumour around Ferguson riots in 2014. Notice the existence of regions of both high and low density of arrival times over a one hour interval. We propose to address inter-arrival time prediction problem with log-Gaussian Cox process (LGCP), an inhomogeneous Poisson process (IPP) which models tweets to be generated by an underlying intensity function which varies across time. Moreover, it assumes a non-parametric form for the intensity function allowing the model complexity to depend on the data set. We also provide an approach to consider textual content of tweets to model inter-arrival times. We evaluate the models using Twitter rumours from the 2014 Ferguson unrest, and demonstrate that they provide good predictions for inter-arrival times, beating the baselines e.g. homogeneous Poisson Process, Gaussian Process regression and univariate Hawkes Process. Even though the central application is rumours, one could apply the proposed approaches to model the arrival times of tweets corresponding to other types of memes, e.g. discussions about politics.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 222,
                        "end": 224,
                        "text": "1a",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "This paper makes the following contributions: 1. Introduces log-Gaussian Cox process to predict tweet arrival times. 2. Demonstrates how incorporating text improves results of inter-arrival time prediction.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Previous approaches to modeling inter-arrival times of tweets (Perera et al., 2010; Sakaki et al., 2010; Esteban et al., 2012; Doerr et al., 2013) were not complex enough to consider their time varying characteristics. Perera et al. ( 2010 inter-arrival times as independent and exponentially distributed with a constant rate parameter. A similar model is used by Sakaki et al. (2010) to monitor the tweets related to earthquakes. The renewal process model used by Esteban et al. (2012) assumes the inter-arrival times to be independent and identically distributed. Gonzalez et al. (2014) attempts to model arrival times of tweets using a Gaussian process but assumes the tweet arrivals to be independent every hour. These approaches do not take into account the varying characteristics of arrival times of tweets.",
                "cite_spans": [
                    {
                        "start": 62,
                        "end": 83,
                        "text": "(Perera et al., 2010;",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 84,
                        "end": 104,
                        "text": "Sakaki et al., 2010;",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 105,
                        "end": 126,
                        "text": "Esteban et al., 2012;",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 127,
                        "end": 146,
                        "text": "Doerr et al., 2013)",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 364,
                        "end": 384,
                        "text": "Sakaki et al. (2010)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 465,
                        "end": 486,
                        "text": "Esteban et al. (2012)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 566,
                        "end": 588,
                        "text": "Gonzalez et al. (2014)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "Point processes such as Poisson and Hawkess process have been used for spatio-temporal modeling of meme spread in social networks (Yang and Zha, 2013; Simma and Jordan, 2010) . Hawkes processes (Yang and Zha, 2013) were also found to be useful for modeling the underlying network structure. These models capture relevant network information in the underlying intensity function. We use a log-Gaussian cox process which provides a Bayesian method to capture relevant information through the prior. It has been found to be useful e.g. for conflict mapping (Zammit-Mangion et al., 2012) and for frequency prediction in Twitter (Lukasik et al., 2015) .",
                "cite_spans": [
                    {
                        "start": 130,
                        "end": 150,
                        "text": "(Yang and Zha, 2013;",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 151,
                        "end": 174,
                        "text": "Simma and Jordan, 2010)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 194,
                        "end": 214,
                        "text": "(Yang and Zha, 2013)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 554,
                        "end": 583,
                        "text": "(Zammit-Mangion et al., 2012)",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 624,
                        "end": 646,
                        "text": "(Lukasik et al., 2015)",
                        "ref_id": "BIBREF5"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "In this section we describe the data and we formalize the problem of modeling tweet arrival times.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data & Problem",
                "sec_num": "3"
            },
            {
                "text": "Data We consider the Ferguson rumour data set (Zubiaga et al., 2015) , consisting of tweets on ru-mours around 2014 Ferguson unrest. It consists of conversational threads that have been manually labeled by annotators to correspond to rumours1 . Since some rumours have few posts, we consider only those with at least 15 posts in the first hour as they express interesting behaviour (Lukasik et al., 2015) . This results in 114 rumours consisting of a total of 4098 tweets.",
                "cite_spans": [
                    {
                        "start": 46,
                        "end": 68,
                        "text": "(Zubiaga et al., 2015)",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 382,
                        "end": 404,
                        "text": "(Lukasik et al., 2015)",
                        "ref_id": "BIBREF5"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data & Problem",
                "sec_num": "3"
            },
            {
                "text": "[0, 2] measured in hours, a set of rumours R = {E i } n i=1",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Problem Definition Let us consider a time interval",
                "sec_num": null
            },
            {
                "text": ", where rumour E i consists of a set of m i posts E i = {p i j } m i j=1 . Posts are tuples p i j = (x i j , t i j ), where x i j is text (in our case a vector of Brown clusters counts, see section 5) and t i j is time of occurrence of post p i j , measured in time since the first post on rumour E i .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Problem Definition Let us consider a time interval",
                "sec_num": null
            },
            {
                "text": "We introduce the problem of predicting the exact time of posts in the future unobserved time interval, which is studied as inter-arrival time prediction. In our setting, we observe posts over a target rumour i for one hour and over reference rumours (other than i) for two hours. Thus, the training data set is",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Problem Definition Let us consider a time interval",
                "sec_num": null
            },
            {
                "text": "R O = {E O i } n i=1",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Problem Definition Let us consider a time interval",
                "sec_num": null
            },
            {
                "text": ", where",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Problem Definition Let us consider a time interval",
                "sec_num": null
            },
            {
                "text": "E O i = {p i j } m O i 1",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Problem Definition Let us consider a time interval",
                "sec_num": null
            },
            {
                "text": "(m O i represents number of posts observed for i th rumour). We query the model for a complete set of times {t i j } m i m O i +1 of posts about rumour i in the future one hour time interval.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Problem Definition Let us consider a time interval",
                "sec_num": null
            },
            {
                "text": "The problem of modeling the inter-arrival times of tweets can be solved using Poisson processes (Perera et al., 2010; Sakaki et al., 2010) . A homogeneous Poisson process (HPP) assumes the intensity to be constant (with respect to time and the rumour statistics). It is not adequate to model the inter-arrival times of tweets because it assumes constant rate of point arrival across time. Inhomogeneous Poisson process (IPP) (Lee et al., 1991) can model tweets occurring at a variable rate by considering the intensity to be a function of time, i.e. \u03bb(t). For example, in Figure 1a we show intensity functions learnt for two different IPP models. Notice how the generated arrival times vary according to the intensity function values.",
                "cite_spans": [
                    {
                        "start": 96,
                        "end": 117,
                        "text": "(Perera et al., 2010;",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 118,
                        "end": 138,
                        "text": "Sakaki et al., 2010)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 425,
                        "end": 443,
                        "text": "(Lee et al., 1991)",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 579,
                        "end": 581,
                        "text": "1a",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Model",
                "sec_num": "4"
            },
            {
                "text": "Log-Gaussian Cox process We consider a log-Gaussian Cox process (LGCP) (M\u00f8ller and Syversveen, 1998) , a special case of IPP, where the intensity function is assumed to be stochastic. The intensity function \u03bb(t) is modeled using a latent function f (t) sampled from a Gaussian process (Rasmussen and Williams, 2005) . To ensure positivity of the intensity function, we consider \u03bb(t) = exp (f (t)). This provides a nonparametric Bayesian approach to model the intensity function, where the complexity of the model is learnt from the training data. Moreover, we can define the functional form of the intensity function through appropriate GP priors.",
                "cite_spans": [
                    {
                        "start": 71,
                        "end": 100,
                        "text": "(M\u00f8ller and Syversveen, 1998)",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 285,
                        "end": 315,
                        "text": "(Rasmussen and Williams, 2005)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model",
                "sec_num": "4"
            },
            {
                "text": "Poisson process (unlike HPP) uses a time varying intensity function and hence, the distribution of inter-arrival times is not independent and identically distributed (Ross, 2010) ",
                "cite_spans": [
                    {
                        "start": 166,
                        "end": 178,
                        "text": "(Ross, 2010)",
                        "ref_id": "BIBREF10"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Modeling inter-arrival time Inhomogeneous",
                "sec_num": null
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "p(T n \u2264 u) = 1 -p(T n > u|\u03bb(t), E n = s) = 1 -p(0 events in [s, s + u]|\u03bb(t)) = 1 -exp(- s+u s \u03bb(t)dt) = 1 -exp(- u 0 \u03bb(s + t)dt)",
                        "eq_num": "(2)"
                    }
                ],
                "section": "Modeling inter-arrival time Inhomogeneous",
                "sec_num": null
            },
            {
                "text": "The derivation is obtained by considering a Poisson probability for 0 counts with rate parameter given by s+u s \u03bb(t)dt and applying integration by substitution to obtain (2). The probability density function of the random variable T n is obtained by taking the derivative of (2) with respect to u:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Modeling inter-arrival time Inhomogeneous",
                "sec_num": null
            },
            {
                "text": "p(T n = u) = \u03bb(s + u) exp(- u 0 \u03bb(s + t)dt).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Modeling inter-arrival time Inhomogeneous",
                "sec_num": null
            },
            {
                "text": "(3) The computational difficulties arising from integration are dealt by assuming the intensity function to be constant in an interval and approximating the inter-arrival time density as (M\u00f8ller and Syversveen, 1998; Vanhatalo et al., 2013 )",
                "cite_spans": [
                    {
                        "start": 187,
                        "end": 216,
                        "text": "(M\u00f8ller and Syversveen, 1998;",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 217,
                        "end": 239,
                        "text": "Vanhatalo et al., 2013",
                        "ref_id": "BIBREF13"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Modeling inter-arrival time Inhomogeneous",
                "sec_num": null
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "p(T n = u) = \u03bb(s + u) exp(-u\u03bb(s + u 2 )).",
                        "eq_num": "(4)"
                    }
                ],
                "section": "Modeling inter-arrival time Inhomogeneous",
                "sec_num": null
            },
            {
                "text": "We associate a distinct intensity function \u03bb i (t) = exp(f i (t)) with each rumour E i as they have varying temporal profiles. The latent function f i is modelled to come from a zero mean Gaussian process (GP) (Rasmussen and Williams, 2005) prior with covariance defined by a squared exponential (SE) kernel over time, k time (t, t ) = a exp(-(t -t )2 /l). We consider the likelihood of posts E O i over the entire training period to be product of Poisson distribution (1) over equal length sub-intervals with the rate in a sub-interval [s, e] approximated as (e -s) exp(f i ( 1 2 (s + e))). The likelihood of posts in the rumour data is obtained by taking the product of the likelihoods over individual rumours.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Modeling inter-arrival time Inhomogeneous",
                "sec_num": null
            },
            {
                "text": "The distribution of the posterior p(f i |E O i ) is intractable and a Laplace approximation (Rasmussen and Williams, 2005) is used to obtain the posterior. The predictive distribution f i (t i * ) at time t i * is obtained using the approximated posterior.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Modeling inter-arrival time Inhomogeneous",
                "sec_num": null
            },
            {
                "text": "The intensity function value at the point t i * is then obtained as",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Modeling inter-arrival time Inhomogeneous",
                "sec_num": null
            },
            {
                "text": "\u03bb i (t i * |E O i ) = exp f i (t i * ) p f i (t i * )|E O i df i (t i * ).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Modeling inter-arrival time Inhomogeneous",
                "sec_num": null
            },
            {
                "text": "Algorithm 1 Importance sampling for predicting the next arrival time 1: Input: Intensity function \u03bb(t), previous arrival time s, proposal distribution q(t) = exp(t; 2), number of samples N 2: for i = 1 to N do 3: Sample u i \u223c q(t).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Modeling inter-arrival time Inhomogeneous",
                "sec_num": null
            },
            {
                "text": "Obtain weights w i = p(u i ) q(u i ) , where p(t) is given by (4). Importance sampling We are interested in predicting the next arrival time of a tweet given the time at which the previous tweet was posted. This is achieved by sampling the inter-arrival time of occurrence of the next tweet using equation ( 4). We use the importance sampling scheme (Gelman et al., 2003) where an exponential distribution is used as the proposal density. We set the rate parameter of this exponential distribution to 2 which generates points with a mean value around 0.5. Assuming the previous tweet occurred at time s, we obtain the arrival time of next tweet as outlined in Algorithm 1. We run this algorithm sequentially, i.e. the time t returned from Algorithm 1 becomes starting time s in the next iteration. We stop at the end of the interval of interest, for which a user wants to find times of post occurrences.",
                "cite_spans": [
                    {
                        "start": 350,
                        "end": 371,
                        "text": "(Gelman et al., 2003)",
                        "ref_id": "BIBREF2"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "4:",
                "sec_num": null
            },
            {
                "text": "We consider adding the kernel over text from posts to the previously introduced kernel over time.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Incorporating text",
                "sec_num": null
            },
            {
                "text": "We join text from the observed posts together, so a different component is added to kernel values across different rumours. The full kernel then takes form k TXT ((t, i) ",
                "cite_spans": [
                    {
                        "start": 162,
                        "end": 169,
                        "text": "((t, i)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Incorporating text",
                "sec_num": null
            },
            {
                "text": ", (t , i )) = k time (t, t ) + k text p i j \u2208E O i x i j , p i j \u2208E O i x i j .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Incorporating text",
                "sec_num": null
            },
            {
                "text": "We compare text via linear kernel with additive underlying base similarity, expressed by",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Incorporating text",
                "sec_num": null
            },
            {
                "text": "k text (x, x ) = b + cx T x .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Incorporating text",
                "sec_num": null
            },
            {
                "text": "Optimization All model parameters (a, l, b, c) are obtained by maximizing the marginal likelihood p(E O i ) = p(E O i |f i )p(f i )df i over all rumour data sets.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Incorporating text",
                "sec_num": null
            },
            {
                "text": "Data preprocessing In our experiments, we consider the first two hours of each rumour lifespan. The posts from the first hour of a target rumour is considered as observed (training data) and we predict the arrival times of tweets in the second hour. We consider observations over equal sized time intervals of length six minutes in the rumour lifespan for learning the intensity function. The text in the tweets is represented by using Brown cluster ids associated with the words. This is obtained using 1000 clusters acquired on a large scale Twitter corpus (Owoputi et al., 2013) .",
                "cite_spans": [
                    {
                        "start": 559,
                        "end": 581,
                        "text": "(Owoputi et al., 2013)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "5"
            },
            {
                "text": "Evaluation metrics Let the arrival times predicted by a model be ( t1 , . . . , tM ) and let the actual arrival times be (t 1 , . . . , t N ). We introduce two metrics based on root mean squared error (RMSE) for evaluating predicted inter-arrival times. First is aligned root mean squared error (ARMSE), where we align the initial K = min(M, N ) arrival times and calculate the RMSE between such two subsequences.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "5"
            },
            {
                "text": "The second is called penalized root mean squared error (PRMSE). In this metric we penalize approaches which predict a different number of inter-arrival times than the actual number. The PRMSE metric is defined as the square root of the following expression.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "5"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "1 K K i=1 ( ti -t i ) 2 + I[M > N ] M i=N +1 (T -ti ) 2 +I[M < N ] N i=M +1 (T -t i ) 2",
                        "eq_num": "(5)"
                    }
                ],
                "section": "Experiments",
                "sec_num": "5"
            },
            {
                "text": "The second and third term in (5) respectively penalize for the excessive or insufficient number of points predicted by the model. to 1000 (above the maximum count yielded by any rumour from our dataset), thus reducing the error from this method. We also compare against Hawkes Process (HP) (Yang and Zha, 2013) , a self exciting point process where an occurrence of a tweet increases the probability of tweets arriving soon afterwards. We consider a univariate Hawkes process where the intensity function is modeled as \u03bb i (t) = \u00b5 + t i j <t k time (t i j , t). The kernel parameters and \u00b5 are learnt by maximizing the likelihood. We apply the importance sampling algorithm discussed in Algorithm 1 for generating arrival times for Hawkess process. We consider this baseline only in the single-task setting, where reference rumours are not considered.",
                "cite_spans": [
                    {
                        "start": 290,
                        "end": 310,
                        "text": "(Yang and Zha, 2013)",
                        "ref_id": "BIBREF14"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "5"
            },
            {
                "text": "LGCP settings In the case of LGCP, the model parameters of the intensity function associated with a rumour are learnt from the observed interarrival times from that rumour alone. LGCP Pooled and LGCPTXT consider a different setting where this is learnt additionally using the interarrival times of all other rumours observed over the entire two hour life-span.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baselines",
                "sec_num": null
            },
            {
                "text": "Results Table 1 reports the results of predicting arrival times of tweets in the second hour of the rumour lifecycle. In terms of ARMSE, LGCP is the best method, performing better than LGCP-TXT (though not statistically significantly) and outperforming other approaches. However, this metric does not penalize for the wrong number of predicted arrival times. Figure 1b depicts an example rumour, where LGCP greatly overesti-mates the number of points in the interval of interest. Here, the three points from the ground truth (denoted by black crosses) and the initial three points predicted by the LGCP model (denoted by red pluses), happen to lie very close, yielding a low ARMSE error. However, LGCP predicts a large number of arrivals in this interval making it a bad model compared to LGCPTXT which predicts only four points (denoted by blue dots). ARMSE fails to capture this and hence we use PRMSE. Note that Hawkes Process is performing worse than the LGCP approach.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 14,
                        "end": 15,
                        "text": "1",
                        "ref_id": null
                    },
                    {
                        "start": 366,
                        "end": 368,
                        "text": "1b",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Baselines",
                "sec_num": null
            },
            {
                "text": "According to PRMSE, LGCPTXT is the most successful method, significantly outperforming all other according to Wilcoxon signed rank test. Figure 1a depicts the behavior of LGCP and LGCP-TXT on rumour 39 with a larger number of points from the ground truth. Here, LGCPTXT predicts relatively less number of arrivals than LGCP. The performance of Hawkes Process is again worse than the LGCP approach. The self excitory nature of Hawkes process may not be appropriate for this dataset and setting, where in the second hour the number of points tends to decrease as time passes.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 144,
                        "end": 146,
                        "text": "1a",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Baselines",
                "sec_num": null
            },
            {
                "text": "We also note, that GPLIN performs very poorly according to PRMSE. This is because the interarrival times predicted by GPLIN for several rumours become smaller as time grows resulting in a large number of arrival times.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baselines",
                "sec_num": null
            },
            {
                "text": "This paper introduced the log-Gaussian Cox processes for the problem of predicting the interarrival times of tweets. We showed how text from posts helps to achieve significant improvements. Evaluation on a set of rumours from Ferguson riots showed efficacy of our methods comparing to baselines. The proposed approaches are generalizable to problems other than rumours, e.g. disaster management and advertisement campaigns.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusions",
                "sec_num": "6"
            },
            {
                "text": "For a fully automated approach, a system for early detection of rumours(Zhao et al.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": ", 2015) could be run first and our models then applied to the resulting rumours.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "We suppress the conditioning variables for brevity.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "Work partially supported by the European Union under grant agreement No. 611233 PHEME.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgments",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Lognormal infection times of online information spread",
                "authors": [
                    {
                        "first": "Christian",
                        "middle": [],
                        "last": "Doerr",
                        "suffix": ""
                    },
                    {
                        "first": "Norbert",
                        "middle": [],
                        "last": "Blenn",
                        "suffix": ""
                    },
                    {
                        "first": "Piet",
                        "middle": [],
                        "last": "Van Mieghem",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "PLOS ONE",
                "volume": "8",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Christian Doerr, Norbert Blenn, and Piet Van Mieghem. 2013. Lognormal infection times of on- line information spread. PLOS ONE, 8.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Analysis of Twitter Traffic based on Renewal Densities",
                "authors": [
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Esteban",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Ortega",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Mcpherson",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Sathiamoorthy",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "J. Esteban, A. Ortega, S. McPherson, and M. Sathi- amoorthy. 2012. Analysis of Twitter Traffic based on Renewal Densities. ArXiv e-prints.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Bayesian Data Analysis",
                "authors": [
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Gelman",
                        "suffix": ""
                    },
                    {
                        "first": "John",
                        "middle": [
                            "B"
                        ],
                        "last": "Carlin",
                        "suffix": ""
                    },
                    {
                        "first": "Hal",
                        "middle": [
                            "S"
                        ],
                        "last": "Stern",
                        "suffix": ""
                    },
                    {
                        "first": "Donald",
                        "middle": [
                            "B"
                        ],
                        "last": "Rubin",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Andrew Gelman, John B. Carlin, Hal S. Stern, and Donald B. Rubin. 2003. Bayesian Data Analysis. Chapman and Hall/CRC.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "On the tweet arrival process at twitter: analysis and applications",
                "authors": [
                    {
                        "first": "Roberto",
                        "middle": [],
                        "last": "Gonzalez",
                        "suffix": ""
                    },
                    {
                        "first": "Alfonso",
                        "middle": [],
                        "last": "Mu\u00f1oz",
                        "suffix": ""
                    },
                    {
                        "first": "Jos\u00e9",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Alberto",
                        "middle": [],
                        "last": "Hern\u00e1ndez",
                        "suffix": ""
                    },
                    {
                        "first": "Ruben",
                        "middle": [],
                        "last": "Cuevas",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Trans. Emerging Telecommunications Technologies",
                "volume": "25",
                "issue": "2",
                "pages": "273--282",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Roberto Gonzalez, Alfonso Mu\u00f1oz, Jos\u00e9 Alberto Hern\u00e1ndez, and Ruben Cuevas. 2014. On the tweet arrival process at twitter: analysis and applications. Trans. Emerging Telecommunications Technologies, 25(2):273-282.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Modeling and simulation of a nonhomogeneous poisson process having cyclic behavior",
                "authors": [
                    {
                        "first": "S",
                        "middle": [
                            "H"
                        ],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [
                            "M"
                        ],
                        "last": "Crawford",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [
                            "R"
                        ],
                        "last": "Wilson",
                        "suffix": ""
                    }
                ],
                "year": 1991,
                "venue": "Communications in Statistics Simulation",
                "volume": "20",
                "issue": "2",
                "pages": "777--809",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "S. H. Lee, M. M. Crawford, and J. R. Wilson. 1991. Modeling and simulation of a nonhomogeneous poisson process having cyclic behavior. Communi- cations in Statistics Simulation, 20(2):777-809.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Point process modelling of rumour dynamics in social media",
                "authors": [
                    {
                        "first": "Michal",
                        "middle": [],
                        "last": "Lukasik",
                        "suffix": ""
                    },
                    {
                        "first": "Trevor",
                        "middle": [],
                        "last": "Cohn",
                        "suffix": ""
                    },
                    {
                        "first": "Kalina",
                        "middle": [],
                        "last": "Bontcheva",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "518--523",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Michal Lukasik, Trevor Cohn, and Kalina Bontcheva. 2015. Point process modelling of rumour dynamics in social media. In Proceedings of the 53rd Annual Meeting of the Association for Computational Lin- guistics and the 7th International Joint Conference on Natural Language Processing of the Asian Fed- eration of Natural Language Processing, ACL 2015, pages 518-523.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Log Gaussian Cox processes",
                "authors": [
                    {
                        "first": "Jesper",
                        "middle": [],
                        "last": "M\u00f8ller",
                        "suffix": ""
                    },
                    {
                        "first": "Anne",
                        "middle": [],
                        "last": "Randi Syversveen",
                        "suffix": ""
                    }
                ],
                "year": 1998,
                "venue": "Scandinavian Journal of Statistics",
                "volume": "",
                "issue": "",
                "pages": "451--482",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jesper M\u00f8ller and Anne Randi Syversveen. 1998. Log Gaussian Cox processes. Scandinavian Journal of Statistics, pages 451-482.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Improved part-of-speech tagging for online conversational text with word clusters",
                "authors": [
                    {
                        "first": "Olutobi",
                        "middle": [],
                        "last": "Owoputi",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Dyer",
                        "suffix": ""
                    },
                    {
                        "first": "Kevin",
                        "middle": [],
                        "last": "Gimpel",
                        "suffix": ""
                    },
                    {
                        "first": "Nathan",
                        "middle": [],
                        "last": "Schneider",
                        "suffix": ""
                    },
                    {
                        "first": "Noah",
                        "middle": [
                            "A"
                        ],
                        "last": "Smith",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of NAACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Olutobi Owoputi, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A. Smith. 2013. Improved part-of-speech tagging for online conversational text with word clusters. In In Proceedings of NAACL.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Twitter analytics: Architecture, tools and analysis",
                "authors": [
                    {
                        "first": "Sruthy",
                        "middle": [],
                        "last": "Rohan Dw Perera",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Anand",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Subbalakshmi",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Chandramouli",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Military Communications Conference, 2010-MILCOM 2010",
                "volume": "",
                "issue": "",
                "pages": "2186--2191",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Rohan DW Perera, Sruthy Anand, KP Subbalakshmi, and R Chandramouli. 2010. Twitter analytics: Ar- chitecture, tools and analysis. In Military Commu- nications Conference, 2010-MILCOM 2010, pages 2186-2191.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning)",
                "authors": [
                    {
                        "first": "Carl",
                        "middle": [],
                        "last": "Edward Rasmussen",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "K I"
                        ],
                        "last": "Williams",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Carl Edward Rasmussen and Christopher K. I. Williams. 2005. Gaussian Processes for Ma- chine Learning (Adaptive Computation and Ma- chine Learning). The MIT Press.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Introduction to Probability Models",
                "authors": [
                    {
                        "first": "Sheldon",
                        "middle": [
                            "M"
                        ],
                        "last": "Ross",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sheldon M. Ross. 2010. Introduction to Probability Models, Tenth Edition. Academic Press, Inc., Or- lando, FL, USA.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Earthquake shakes twitter users: Real-time event detection by social sensors",
                "authors": [
                    {
                        "first": "Takeshi",
                        "middle": [],
                        "last": "Sakaki",
                        "suffix": ""
                    },
                    {
                        "first": "Makoto",
                        "middle": [],
                        "last": "Okazaki",
                        "suffix": ""
                    },
                    {
                        "first": "Yutaka",
                        "middle": [],
                        "last": "Matsuo",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the 19th International Conference on World Wide Web, WWW '10",
                "volume": "",
                "issue": "",
                "pages": "851--860",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo. 2010. Earthquake shakes twitter users: Real-time event detection by social sensors. In Proceedings of the 19th International Conference on World Wide Web, WWW '10, pages 851-860.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Modeling events with cascades of poisson processes",
                "authors": [
                    {
                        "first": "Aleksandr",
                        "middle": [],
                        "last": "Simma",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [
                            "I"
                        ],
                        "last": "Jordan",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "UAI",
                "volume": "",
                "issue": "",
                "pages": "546--555",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Aleksandr Simma and Michael I. Jordan. 2010. Mod- eling events with cascades of poisson processes. In UAI, pages 546-555.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Gpstuff: Bayesian modeling with gaussian processes",
                "authors": [
                    {
                        "first": "Jarno",
                        "middle": [],
                        "last": "Vanhatalo",
                        "suffix": ""
                    },
                    {
                        "first": "Jaakko",
                        "middle": [],
                        "last": "Riihim\u00e4ki",
                        "suffix": ""
                    },
                    {
                        "first": "Jouni",
                        "middle": [],
                        "last": "Hartikainen",
                        "suffix": ""
                    },
                    {
                        "first": "Pasi",
                        "middle": [],
                        "last": "Jyl\u00e4nki",
                        "suffix": ""
                    },
                    {
                        "first": "Ville",
                        "middle": [],
                        "last": "Tolvanen",
                        "suffix": ""
                    },
                    {
                        "first": "Aki",
                        "middle": [],
                        "last": "Vehtari",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "J. Mach. Learn. Res",
                "volume": "14",
                "issue": "1",
                "pages": "1175--1179",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jarno Vanhatalo, Jaakko Riihim\u00e4ki, Jouni Hartikainen, Pasi Jyl\u00e4nki, Ville Tolvanen, and Aki Vehtari. 2013. Gpstuff: Bayesian modeling with gaussian pro- cesses. J. Mach. Learn. Res., 14(1):1175-1179.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Mixture of mutually exciting processes for viral diffusion",
                "authors": [
                    {
                        "first": "Shuang-Hong",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Hongyuan",
                        "middle": [],
                        "last": "Zha",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "ICML",
                "volume": "28",
                "issue": "",
                "pages": "1--9",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Shuang-Hong Yang and Hongyuan Zha. 2013. Mix- ture of mutually exciting processes for viral diffu- sion. In ICML (2), volume 28 of JMLR Proceedings, pages 1-9.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Point process modelling of the afghan war diary",
                "authors": [
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Zammit-Mangion",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Dewar",
                        "suffix": ""
                    },
                    {
                        "first": "Visakan",
                        "middle": [],
                        "last": "Kadirkamanathan",
                        "suffix": ""
                    },
                    {
                        "first": "Guido",
                        "middle": [],
                        "last": "Sanguinetti",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "In Proceedings of the National Academy of Sciences",
                "volume": "109",
                "issue": "31",
                "pages": "12414--12419",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Andrew Zammit-Mangion, Michael Dewar, Visakan Kadirkamanathan, and Guido Sanguinetti. 2012. Point process modelling of the afghan war diary. In Proceedings of the National Academy of Sciences, Vol. 109, No. 31, pages 12414-12419.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Early detection of rumors in social media from enquiry posts",
                "authors": [
                    {
                        "first": "Zhe",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Paul",
                        "middle": [],
                        "last": "Resnick",
                        "suffix": ""
                    },
                    {
                        "first": "Qiaozhu",
                        "middle": [],
                        "last": "Mei",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "International World Wide Web Conference Committee",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zhe Zhao, Paul Resnick, and Qiaozhu Mei. 2015. Early detection of rumors in social media from en- quiry posts. In International World Wide Web Con- ference Committee (IW3C2).",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Towards detecting rumours in social media. In AAAI Workshop on AI for Cities",
                "authors": [
                    {
                        "first": "Arkaitz",
                        "middle": [],
                        "last": "Zubiaga",
                        "suffix": ""
                    },
                    {
                        "first": "Maria",
                        "middle": [],
                        "last": "Liakata",
                        "suffix": ""
                    },
                    {
                        "first": "Rob",
                        "middle": [],
                        "last": "Procter",
                        "suffix": ""
                    },
                    {
                        "first": "Kalina",
                        "middle": [],
                        "last": "Bontcheva",
                        "suffix": ""
                    },
                    {
                        "first": "Peter",
                        "middle": [],
                        "last": "Tolmie",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Arkaitz Zubiaga, Maria Liakata, Rob Procter, Kalina Bontcheva, and Peter Tolmie. 2015. Towards de- tecting rumours in social media. In AAAI Workshop on AI for Cities.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 1: Intensity functions and corresponding predicted arrival times for different methods across example Ferguson rumours. Arrival times predicted by LGCP are denoted by red pluses, LGCPTXT by blue dots, and ground truth by black crosses. Light regions denote uncertainty of predictions.",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "the next arrival time as t = s + \u016b. 8: Return: t",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table><tr><td/><td/><td>e s \u03bb(t)dt.</td><td/><td/></tr><tr><td/><td/><td/><td>e</td><td/></tr><tr><td colspan=\"3\">p(y|\u03bb(t), [s, e]) = Poisson(y|</td><td colspan=\"2\">\u03bb(t)dt)</td></tr><tr><td>=</td><td>(</td><td>e s \u03bb(t)dt) y exp(-y!</td><td>s e s \u03bb(t)dt)</td><td>(1)</td></tr></table>",
                "type_str": "table",
                "text": ". In IPP, the number of tweets y occurring in an interval [s, e] is Poisson distributed with rate Assume that n th tweet occurred at time E n = s and we are interested in the inter-arrival time T",
                "html": null,
                "num": null
            }
        }
    }
}