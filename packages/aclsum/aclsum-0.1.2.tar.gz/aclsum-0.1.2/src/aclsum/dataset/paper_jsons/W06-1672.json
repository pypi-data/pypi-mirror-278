{
    "paper_id": "W06-1672",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:28:41.117931Z"
    },
    "title": "Discriminative Methods for Transliteration",
    "authors": [
        {
            "first": "Dmitry",
            "middle": [],
            "last": "Zelenko",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "SRA International",
                "location": {
                    "addrLine": "4300 Fair Lakes Ct",
                    "postCode": "22033",
                    "settlement": "Fairfax",
                    "region": "VA"
                }
            },
            "email": "dmitry_zelenko@sra.com"
        },
        {
            "first": "Chinatsu",
            "middle": [],
            "last": "Aone",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "SRA International",
                "location": {
                    "addrLine": "4300 Fair Lakes Ct",
                    "postCode": "22033",
                    "settlement": "Fairfax",
                    "region": "VA"
                }
            },
            "email": "chinatsu_aone@sra.com"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "We present two discriminative methods for name transliteration. The methods correspond to local and global modeling approaches in modeling structured output spaces. Both methods do not require alignment of names in different languages -their features are computed directly from the names themselves. We perform an experimental evaluation of the methods for name transliteration from three languages (Arabic, Korean, and Russian) into English, and compare the methods experimentally to a state-of-theart joint probabilistic modeling approach. We find that the discriminative methods outperform probabilistic modeling, with the global discriminative modeling approach achieving the best performance in all languages.",
    "pdf_parse": {
        "paper_id": "W06-1672",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "We present two discriminative methods for name transliteration. The methods correspond to local and global modeling approaches in modeling structured output spaces. Both methods do not require alignment of names in different languages -their features are computed directly from the names themselves. We perform an experimental evaluation of the methods for name transliteration from three languages (Arabic, Korean, and Russian) into English, and compare the methods experimentally to a state-of-theart joint probabilistic modeling approach. We find that the discriminative methods outperform probabilistic modeling, with the global discriminative modeling approach achieving the best performance in all languages.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Name transliteration is an important task of transcribing a name from alphabet to another. For example, an Arabic \u202b,\"\u0648\ufedf\ufef4\ufe8e\u0645\"\u202c Korean \"\uc70c\ub9ac\uc5c4\", and Russian \"\u0412\u0438\u043b\u044c\u044f\u043c\" all correspond to English \"William\". We address the problem of transliteration in the general setting: it involves trying to recover original English names from their transcription in a foreign language, as well as finding an acceptable spelling of a foreign name in English.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "We apply name transliteration in the context of cross-lingual information extraction. Name extractors are currently available in multiple languages. Our goal is to make the extracted names understandable to monolingual English speakers by transliterating the names into English.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The extraction context of the transliteration application imposes additional complexity constraints on the task. In particular, we aim for the transliteration speed to be comparable to that of extraction speed. Since most current extraction systems are fairly fast (>1 Gb of text per hour), the complexity requirement reduces the range of techniques applicable to the transliteration. More precisely, we cannot use WWW and the web count information to hone in on the right transliteration candidate. Instead, all relevant transliteration information has to be represented within a compact and self-contained transliteration model.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "We present two methods for creating and applying transliteration models. In contrast to most previous transliteration approaches, our models are discriminative. Using an existing transliteration dictionary D (a set of name pairs {(f,e)}), we learn a function that directly maps a name f from one language into a name e in another language. We do not estimate either direct conditional p(e|f) or reverse conditional p(f|e) or joint p(e,f) probability models. Furthermore, we do away with the notion of alignment: our transliteration model does not require and is not defined of in terms of aligned e and f. Instead, all features used by the model are computed directly from the names f and e without any need for their alignment.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The two discriminative methods that we present correspond to local and global modeling paradigms for solving complex learning problems with structured output spaces. In the local setting, we learn linear classifiers that predict a letter e i from the previously predicted letters e 1 \u2026e i-1 and the original name f. In the global setting, we learn a function W mapping a pair (f,e) into a score W(f,e)\u2208 R. The function W is linear in features computed from the pair (f,e). We describe the pertinent feature spaces as well as pre-sent both training and decoding algorithms for the local and global settings.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "We perform an experimental evaluation for three language pairs (transliteration from Arabic, Korean, and Russian into English) comparing our methods to a joint probabilistic modeling approach to transliteration, which was shown to deliver superior performance. We show experimentally that both discriminative methods outperform the probabilistic approach, with global discriminative modeling achieving the best performance in all languages.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Let E and F be two finite alphabets. We will use lowercase latin letters e, f to denote letters e\u2208E, f\u2208F, and we use bold letters e\u2208E * , f\u2208F * to denote strings in the corresponding alphabets. The subscripted e i , f j denote ith and jth symbols of the strings e and f, respectively. We use e[i,j] to represent a substring e i \u2026e j of e. If j<i, then e[i,j] is an empty string \u039b.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Preliminaries",
                "sec_num": "2"
            },
            {
                "text": "A transliteration model is a function mapping a string f to a string e. We seek to learn a transliteration model from a transliteration dictionary D={(f,e)}. We apply the model in conjunction with a decoding algorithm that produces a string e from a string f.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Preliminaries",
                "sec_num": "2"
            },
            {
                "text": "In local transliteration modeling, we represent a transliteration model as a sequence of local prediction problems. For each local prediction, we use the history h representing the context of making a single transliteration prediction. That is, we predict each letter e i based on the pair h=(e[1,i-",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Local Transliteration Modeling",
                "sec_num": "3"
            },
            {
                "text": "1], f) \u2208 H.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Local Transliteration Modeling",
                "sec_num": "3"
            },
            {
                "text": "Formally, we map H\u00d7E into a d-dimensional feature space \u03d5: H\u00d7E \u2192 R d , where each \u03d5 k (h,e)(k\u2208{1,..,d}) corresponds to a condition defined in terms of the history h and the currently predicted letter e.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Local Transliteration Modeling",
                "sec_num": "3"
            },
            {
                "text": "In order to model string termination, we augment E with a sentinel symbol $, and we append $ to each e from D.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Local Transliteration Modeling",
                "sec_num": "3"
            },
            {
                "text": "Given a transliteration dictionary D, we transform the dictionary in a set of |E| binary learning problems. Each learning problem L e corresponds to predicting a letter e\u2208E. More precisely, for a pair (f [1,m] ,e [1,n] ) \u2208 D and i \u2208 {1,\u2026,n}, we generate a positive example \u03d5((e[1,i-1], f),e i ) for the learning problem L e , where e=e i , and a negative example \u03d5((e[1,i-1], f),e) for each L e , where e\u2260e i .",
                "cite_spans": [
                    {
                        "start": 204,
                        "end": 209,
                        "text": "[1,m]",
                        "ref_id": null
                    },
                    {
                        "start": 213,
                        "end": 218,
                        "text": "[1,n]",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Local Transliteration Modeling",
                "sec_num": "3"
            },
            {
                "text": "Each of the learning problems is a binary classification problem and we can use our favorite binary classifier learning algorithm to induce a collection of binary classifiers {c e : e\u2208E}. From most classifiers we can also obtain an estimate of conditional probability p(e|h) of a letter e given a history h.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Local Transliteration Modeling",
                "sec_num": "3"
            },
            {
                "text": "For decoding, in our experiments we use the beam search to find the sequence of letters (approximately) maximizing p(e|h).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Local Transliteration Modeling",
                "sec_num": "3"
            },
            {
                "text": "The features used in local transliteration modeling correspond to pairs of substrings of e and f. We limit the length of substrings as well as their relative location with respect to each other.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Local Features",
                "sec_num": "3.1"
            },
            {
                "text": "\u2022 For \u03d5((e[1,i-1], f),e), generate a feature for every pair of substrings (e[i-w,i-1] ,f [jv,j] ), where 1\u2264w<W(E) and 0\u2264v<W(F) and |i-j| \u2264 d(E,F). Here, W(\u2022) is the upper bound on the length of strings in the corresponding alphabet, and d(E,F) is the upper bound on the relative distance between substrings.",
                "cite_spans": [
                    {
                        "start": 74,
                        "end": 85,
                        "text": "(e[i-w,i-1]",
                        "ref_id": null
                    },
                    {
                        "start": 89,
                        "end": 95,
                        "text": "[jv,j]",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Local Features",
                "sec_num": "3.1"
            },
            {
                "text": "),e), generate the length difference feature \u03d5 len =i-m. In experiments, we discretize \u03d5 len to obtain 9 binary features: \u03d5 len =l (l\u2208 [-3,3] ), \u03d5 len \u2264 -4, 4 \u2264 \u03d5 len .",
                "cite_spans": [
                    {
                        "start": 135,
                        "end": 141,
                        "text": "[-3,3]",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "\u2022 For \u03d5((e[1,i-1], f[1,m]",
                "sec_num": null
            },
            {
                "text": "\u2022 For \u03d5((e[1,i-1], f[1,m]),e), generate a language modeling feature p(e| e [1,i-1] ).",
                "cite_spans": [
                    {
                        "start": 75,
                        "end": 82,
                        "text": "[1,i-1]",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "\u2022 For \u03d5((e[1,i-1], f[1,m]",
                "sec_num": null
            },
            {
                "text": "\u2022 For \u03d5((e[1,i-1], f),e) and i=1, generate \"start\" features: (^f 1 ,^e), (^f 1 f 2 ,^e).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "\u2022 For \u03d5((e[1,i-1], f[1,m]",
                "sec_num": null
            },
            {
                "text": "\u2022 For \u03d5((e[1,i-1], f),e) and i=2, generate \"start\" features: (^f 1 ,^e 1 e 2 ), (^f 1 f 2 ,^e 1 e 2 ).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "\u2022 For \u03d5((e[1,i-1], f[1,m]",
                "sec_num": null
            },
            {
                "text": "\u2022 For \u03d5((e[1,i-1], f),e) and e=$, generate \"end\" features:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "\u2022 For \u03d5((e[1,i-1], f[1,m]",
                "sec_num": null
            },
            {
                "text": "(f m $,e$), (f m-1 f m $,e$).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "\u2022 For \u03d5((e[1,i-1], f[1,m]",
                "sec_num": null
            },
            {
                "text": "The parameters W(E), W(F), and d(E,F) are, in general, language-specific, and we will show, in the experiments, that different values of the parameters are appropriate for different languages.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "\u2022 For \u03d5((e[1,i-1], f[1,m]",
                "sec_num": null
            },
            {
                "text": "In global transliteration modeling, we directly model the agreement function between f and e. We follow (Collins 2002) and consider the global feature representation \u03a6:",
                "cite_spans": [
                    {
                        "start": 104,
                        "end": 118,
                        "text": "(Collins 2002)",
                        "ref_id": "BIBREF2"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Global Transliteration Modeling",
                "sec_num": "4"
            },
            {
                "text": "F * \u00d7E * \u2192 R d .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Global Transliteration Modeling",
                "sec_num": "4"
            },
            {
                "text": "Each global feature corresponds to a condition on the pair of strings. The value of a feature is the number of times the condition holds true for a given pair of strings. In particular, for every local feature \u03d5 k ((e[1,i-1] , f ),e i ) we can define the corresponding global feature:",
                "cite_spans": [
                    {
                        "start": 214,
                        "end": 224,
                        "text": "((e[1,i-1]",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 225,
                        "end": 226,
                        "text": ",",
                        "ref_id": null
                    },
                    {
                        "start": 227,
                        "end": 228,
                        "text": "f",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Global Transliteration Modeling",
                "sec_num": "4"
            },
            {
                "text": ") ), ], 1 , 1 [ (( ) , ( \u2211 - = \u03a6 i i k k e i f e e f \u03d5 (1)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Global Transliteration Modeling",
                "sec_num": "4"
            },
            {
                "text": "We seek a transliteration model that is linear in the global features. Such a transliteration model is represented by d-dimensional weight vector W\u2208 R d . Given a string f, model application corresponds to finding a string e such that",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Global Transliteration Modeling",
                "sec_num": "4"
            },
            {
                "text": "\u2211 \u03a6 = k k k W ) , ( max arg e' e' f e (2)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Global Transliteration Modeling",
                "sec_num": "4"
            },
            {
                "text": "As with the case of local modeling, due to computational constraints, we use beam search for decoding in global transliteration modeling. (Collins 2002) showed how to use the Voted Perceptron algorithm for learning W, and we use it for learning the global transliteration model. We use beam search for decoding within the Voted Perceptron training as well.",
                "cite_spans": [
                    {
                        "start": 138,
                        "end": 152,
                        "text": "(Collins 2002)",
                        "ref_id": "BIBREF2"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Global Transliteration Modeling",
                "sec_num": "4"
            },
            {
                "text": "The global features used in local transliteration modeling directly correspond to local features described in Section 3.1.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Global Features",
                "sec_num": "4.1"
            },
            {
                "text": "\u2022 For e[1,n] and f[1,m], generate a feature for every pair of substrings (e[i-w,i],f [jv,j] ), where 1\u2264w<W(E) and 0\u2264v<W(F) and |i-j| \u2264 d(E,F).",
                "cite_spans": [
                    {
                        "start": 85,
                        "end": 91,
                        "text": "[jv,j]",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Global Features",
                "sec_num": "4.1"
            },
            {
                "text": "\u2022 For e[1,n] and f[1,m], generate the length difference feature \u03a6 len =n-m. In experiments, we discretize \u03a6 len to obtain 9 binary features: \u03a6 len =l (l\u2208 [-3,3] ), \u03d5 len \u2264 -4, 4 \u2264 \u03d5 len .",
                "cite_spans": [
                    {
                        "start": 154,
                        "end": 160,
                        "text": "[-3,3]",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Global Features",
                "sec_num": "4.1"
            },
            {
                "text": "\u2022 For e[1,n], generate a language modeling feature (p(e)) 1/n .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Global Features",
                "sec_num": "4.1"
            },
            {
                "text": "\u2022 For e[1,n] and f[1,m],, generate \"start\" features: (^f 1 ,^e 1 ), (^f 1 f 2 ,^e 1 ), (^f 1 ,^e 1 e 2 ), (^f 1 f 2 ,^e 1 e 2 ).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Global Features",
                "sec_num": "4.1"
            },
            {
                "text": "\u2022 For e[1,n] and f[1,m], generate \"end\" features: (f m $,e n $), (f m-1 f m $,e n ).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Global Features",
                "sec_num": "4.1"
            },
            {
                "text": "We compare the discriminative approaches to a joint probabilistic approach to transliteration introduced in recent years.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Joint Probabilistic Modeling",
                "sec_num": "5"
            },
            {
                "text": "In the joint probabilistic modeling approach, we estimate a probability distribution p(e,f). We also postulate hidden random variables a representing the alignment of e and f. An alignment a of e and f is a sequence a 1 ,a 2 ,\u2026a L , where a l = (e[i l -w l ,i l ],f[j l -v l ,j l ]), i l-1 +1=i l -w l , and j l-1 +1=j l -v l . Note that we allow for at most one member of a pair a l to be an empty string.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Joint Probabilistic Modeling",
                "sec_num": "5"
            },
            {
                "text": "Given an alignment a, we define the joint probability p(e,f|a):",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Joint Probabilistic Modeling",
                "sec_num": "5"
            },
            {
                "text": "]) , [ ], , [ ( ) | , ( l l l l l l l j v j i w i p p \u220f - - = f e a f e",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Joint Probabilistic Modeling",
                "sec_num": "5"
            },
            {
                "text": "We learn the probabilities p(e[i l -w l ,i l ],f[j l -v l ,j l ]) using a version of EM algorithm. In our experiments, we use the Viterbi version of the EM algorithm: starting from random alignments of all string pairs in D, we use maximum likelihood estimates of the above probabilities, which are then employed to induce the most probable alignments in terms of the probability estimates. The process is repeated until the probability estimates converge.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Joint Probabilistic Modeling",
                "sec_num": "5"
            },
            {
                "text": "During the decoding process, given a string f, we seek both a string e and an alignment a such that p(e,f|a) is maximized. In our experiments, we used beam search for decoding.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Joint Probabilistic Modeling",
                "sec_num": "5"
            },
            {
                "text": "Note that with joint probabilistic modeling use of a language model p(e) is not strictly necessary. Yet we found out experimentally that an adaptive combination of the language model with the joint probabilistic model improves the transliteration performance. We thus combine the joint log-likelihood log(p(e,f|a)) with log(p(e)):",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Joint Probabilistic Modeling",
                "sec_num": "5"
            },
            {
                "text": "(3)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "score(e|f) = log(p(e,f|a))+ \u03b1log(p(e))",
                "sec_num": null
            },
            {
                "text": "We estimate the parameter \u03b1 on a held-out set by generating, for each f, the set of top K=10 candidates with respect to log(p(e,f|a)), then using (3) for re-ranking the candidates, and picking \u03b1 to minimize the number of transliteration errors among re-ranked candidates.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "score(e|f) = log(p(e,f|a))+ \u03b1log(p(e))",
                "sec_num": null
            },
            {
                "text": "We present transliteration experiments for three language pairs. We consider transliteration from Arabic, Korean, and Russian into English. For all language pairs, we apply the same training and decoding algorithms.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "6"
            },
            {
                "text": "The training and testing transliteration dataset sizes are shown in Table 1 . For Arabic and Russian, we created the dataset manually by keying in and translating Arabic, Russian, and English names. For Korean, we obtained a dataset of transliterated names from a Korean government website. Prior to transliteration, the Korean words of the Korean transliteration data were converted from their Hangul (syllabic) representation to Jamo (letter-based) representation to effectively reduce the alphabet size for Korean. The conversion process is completely automatic (see Unicode Standard 3.0 for details).",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 74,
                        "end": 75,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Data",
                "sec_num": "6.1"
            },
            {
                "text": "For language modeling, we used the list of 100,000 most frequent names downloaded from the US Census website. Our language model is a 5-gram model with interpolated Good-Turing smoothing (Gale and Sampson 1995) .",
                "cite_spans": [
                    {
                        "start": 187,
                        "end": 210,
                        "text": "(Gale and Sampson 1995)",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Algorithm Details",
                "sec_num": "6.2"
            },
            {
                "text": "We used the learning-to-classify version of Voted Perceptron for training local models (Freund and Schapire 1999) . We used Platt's method for converting scores produced by learned linear classifiers into probabilities (Platt 1999) . We ran both local and global Voted Perceptrons for 10 iterations during training.",
                "cite_spans": [
                    {
                        "start": 87,
                        "end": 113,
                        "text": "(Freund and Schapire 1999)",
                        "ref_id": null
                    },
                    {
                        "start": 219,
                        "end": 231,
                        "text": "(Platt 1999)",
                        "ref_id": "BIBREF14"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Algorithm Details",
                "sec_num": "6.2"
            },
            {
                "text": "Our discriminative transliteration models have a number of parameters reflecting the length of strings chosen in either language as well as the relative distance between strings. While we found that choice of W(E)=W(F) = 2 always produces the best results for all of our languages, the distance d(E,F) may have different optimal values for different languages.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Transliteration Results",
                "sec_num": "6.3"
            },
            {
                "text": "Table 2 presents the transliteration results for all languages for different values of d. Note that the joint probabilistic model does not depend on d. The results reflect the accuracy of transliteration, that is, the proportion of times when the top English candidate produced by a transliteration model agreed with the correct English transliteration. We note that such an exact comparison may be too inflexible, for many foreign names may have more than one legitimate English spelling.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Transliteration Results",
                "sec_num": "6.3"
            },
            {
                "text": "In future experiments, we plan to relax the requirement and consider alternative variants of transliteration scoring (e.g., edit distance, top-N candidate scoring). Table 2 shows that, for all three languages, the discriminative methods convincingly outperform the joint probabilistic approach. The global discriminative approach achieves the best performance in all languages. It is interesting that different values of relative distance are optimal for different languages. For example, in Korean, the Hangul-Jamo decomposition leads to fairly redundant strings of Korean characters thereby making transliterated characters to be relatively far from each other. Therefore, Korean requires a larger relative distance bound. In Arabic and Russian, on the other hand, transliterated characters are relatively close to each other, so the distance d of 1 suffices. While for Russian such a small distance is to be expected, we are surprised by such a small relative distance for Arabic. Our intuition was that omitting short vowels in spelling names in Arabic will increase d.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 171,
                        "end": 172,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Transliteration Results",
                "sec_num": "6.3"
            },
            {
                "text": "We have the following explanation of the low value of d for Arabic from the machine learning perspective: incrementing d implies adding a lot of extraneous features to examples, that is, increasing attribute noise. Increased attribute noise requires a corresponding increase in the number of training examples to achieve adequate performance. While for Korean the number of training examples is sufficient to cope with the attribute noise, the relatively small Arabic training sample is not. We hypothesize that with increasing the number of training examples for Arabic, the optimal value of d will also increase.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Local",
                "sec_num": null
            },
            {
                "text": "Most work on name transliteration adopted a source-channel approach (Knight and Grael 1998; Al-Onaizan and Knight 2002a; Virga and Khudanpur 2003; Oh and Choi 2000) incorporat-ing phonetics as an intermediate representation. (Al-Onaizan and Knight 2002) showed that use of outside linguistic resources such as WWW counts of transliteration candidates can greatly boost transliteration accuracy. (Li et al. 2004) introduced the joint transliteration model whose variant augmented with adaptive re-ranking we used in our experiments.",
                "cite_spans": [
                    {
                        "start": 68,
                        "end": 91,
                        "text": "(Knight and Grael 1998;",
                        "ref_id": null
                    },
                    {
                        "start": 92,
                        "end": 120,
                        "text": "Al-Onaizan and Knight 2002a;",
                        "ref_id": null
                    },
                    {
                        "start": 121,
                        "end": 146,
                        "text": "Virga and Khudanpur 2003;",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 147,
                        "end": 164,
                        "text": "Oh and Choi 2000)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 225,
                        "end": 253,
                        "text": "(Al-Onaizan and Knight 2002)",
                        "ref_id": null
                    },
                    {
                        "start": 395,
                        "end": 411,
                        "text": "(Li et al. 2004)",
                        "ref_id": "BIBREF10"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "7"
            },
            {
                "text": "Among direct (non-source-channel) models, we note the work of (Gao et al. 2004 ) on applying Maximum Entropy to English-Chinese transliteration, and the English-Korean transliteration model of (Kang and Choi 2000) based on decision trees.",
                "cite_spans": [
                    {
                        "start": 62,
                        "end": 78,
                        "text": "(Gao et al. 2004",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 193,
                        "end": 213,
                        "text": "(Kang and Choi 2000)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "7"
            },
            {
                "text": "All of the above models require alignment between names. We follow the recent work of (Klementiev and Roth 2006) who addressed the problem of discovery of transliterated named entities from comparable corpora and suggested that alignment may not be necessary for transliteration.",
                "cite_spans": [
                    {
                        "start": 86,
                        "end": 112,
                        "text": "(Klementiev and Roth 2006)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "7"
            },
            {
                "text": "Finally, our modeling approaches follow the recent work on both local classifier-based modeling of complex learning problems (McCallum et al. 2000; Punyakanok and Roth 2001) , as well as global discriminative approaches based on CRFs (Lafferty et al. 2001) , SVM (Taskar et al. 2005) , and the Perceptron algorithm (Collins 2002 ) that we used in our experiments.",
                "cite_spans": [
                    {
                        "start": 125,
                        "end": 147,
                        "text": "(McCallum et al. 2000;",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 148,
                        "end": 173,
                        "text": "Punyakanok and Roth 2001)",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 234,
                        "end": 256,
                        "text": "(Lafferty et al. 2001)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 263,
                        "end": 283,
                        "text": "(Taskar et al. 2005)",
                        "ref_id": null
                    },
                    {
                        "start": 315,
                        "end": 328,
                        "text": "(Collins 2002",
                        "ref_id": "BIBREF2"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "7"
            },
            {
                "text": "We presented two novel discriminative approaches to name transliteration that do not employ the notion of alignment. We showed experimentally that the approaches lead to superior experimental results in all languages, with the global discriminative modeling approach achieving the best performance.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusions",
                "sec_num": "8"
            },
            {
                "text": "The results are somewhat surprising, for the notion of alignment seems very intuitive and useful for transliteration. We will investigate whether similar alignment-free methodology can be extended to full-text translation. It will also be interesting to study the relationship between our discriminative alignment-free methods and recently proposed discriminative alignment-based methods for transliteration and translation (Taskar et al. 2005a; Moore 2005) .",
                "cite_spans": [
                    {
                        "start": 424,
                        "end": 445,
                        "text": "(Taskar et al. 2005a;",
                        "ref_id": null
                    },
                    {
                        "start": 446,
                        "end": 457,
                        "text": "Moore 2005)",
                        "ref_id": "BIBREF12"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusions",
                "sec_num": "8"
            },
            {
                "text": "We also showed that for name transliteration, global discriminative modeling is superior to local classifier-based discriminative modeling. This may have resulted from poor calibration of scores and probabilities produced by individual classifiers. We plan to further investigate the relationship between the local and global approaches to complex learning problems in natural language.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusions",
                "sec_num": "8"
            }
        ],
        "back_matter": [],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Translating Named Entities Using Monolingual and Bilingual Resources. Proceedings of ACL",
                "authors": [
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Al-Onaizan",
                        "suffix": ""
                    },
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Knight",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Y. Al-Onaizan and K. Knight. 2002. Translating Named Entities Using Monolingual and Bilingual Resources. Proceedings of ACL.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Machine Transliteration of Names in Arabic Text",
                "authors": [
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Al-Onaizan",
                        "suffix": ""
                    },
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Knight",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Proceedings of ACL Workshop on Computational Approaches to Semitic Languages",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Y. Al-Onaizan and K. Knight. 2002a. Machine Trans- literation of Names in Arabic Text. Proceedings of ACL Workshop on Computational Approaches to Semitic Languages.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Discriminative Training for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms",
                "authors": [
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Collins",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Proceedings of EMNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "M. Collins. 2002. Discriminative Training for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms. In Proceedings of EMNLP.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Large margin classification using the perceptron algorithm",
                "authors": [
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Freund",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Shapire",
                        "suffix": ""
                    }
                ],
                "year": 1999,
                "venue": "Machine Learning",
                "volume": "37",
                "issue": "",
                "pages": "277--296",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Y. Freund and R. Shapire. 1999. Large margin clas- sification using the perceptron algorithm. Machine Learning, 37, 277-296.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Good-Turing frequency estimation without tears",
                "authors": [
                    {
                        "first": "W",
                        "middle": [],
                        "last": "Gale",
                        "suffix": ""
                    },
                    {
                        "first": "G",
                        "middle": [],
                        "last": "Sampson",
                        "suffix": ""
                    }
                ],
                "year": 1995,
                "venue": "Journal of Quantitative Linguistics",
                "volume": "2",
                "issue": "",
                "pages": "217--235",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "W. Gale and G. Sampson. 1995. Good-Turing fre- quency estimation without tears. Journal of Quan- titative Linguistics 2:217-235.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Phoneme-based transliteration of foreign names for OOV problem",
                "authors": [
                    {
                        "first": "Gao",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Kam-Fai",
                        "middle": [],
                        "last": "Wong",
                        "suffix": ""
                    },
                    {
                        "first": "Wai",
                        "middle": [],
                        "last": "Lam",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proceedings of the First International Joint Conference on Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Gao Wei, Kam-Fai Wong, and Wai Lam. 2004. Pho- neme-based transliteration of foreign names for OOV problem. Proceedings of the First Interna- tional Joint Conference on Natural Language Processing.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Automatic Transliteration and Back-transliteration by Decision Tree Learning",
                "authors": [
                    {
                        "first": "B",
                        "middle": [
                            "J"
                        ],
                        "last": "Kang",
                        "suffix": ""
                    },
                    {
                        "first": "Key-Sun",
                        "middle": [],
                        "last": "Choi",
                        "suffix": ""
                    }
                ],
                "year": 2000,
                "venue": "Proceedings of the 2nd International Conference on Language Resources and Evaluation",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "B.J. Kang and Key-Sun Choi, 2000. Automatic Trans- literation and Back-transliteration by Decision Tree Learning, Proceedings of the 2nd International Conference on Language Resources and Evalua- tion.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Named Entity Transliteration and Discovery from Multilingual Comparable Corpora",
                "authors": [
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Klementiev",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Roth",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Proceedings of ACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "A. Klementiev and D. Roth. 2006. Named Entity Transliteration and Discovery from Multilingual Comparable Corpora. Proceedings of ACL.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Machine Transliteration",
                "authors": [
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Knight",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Graehl",
                        "suffix": ""
                    }
                ],
                "year": 1998,
                "venue": "Computational Linguistics",
                "volume": "24",
                "issue": "4",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "K. Knight and J. Graehl. 1998. Machine Translitera- tion, Computational Linguistics, 24(4).",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. Proceedings of the Eighteenth International Conference on Machine Learning",
                "authors": [
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Lafferty",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Mccallum",
                        "suffix": ""
                    },
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Pereira",
                        "suffix": ""
                    }
                ],
                "year": 2001,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "J. Lafferty, A. McCallum, and F. Pereira. 2001. Con- ditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. Proceed- ings of the Eighteenth International Conference on Machine Learning.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "A Joint Source-channel Model for Machine Transliteration. Proceedings of ACL",
                "authors": [
                    {
                        "first": "Li",
                        "middle": [],
                        "last": "Haizhou",
                        "suffix": ""
                    },
                    {
                        "first": "Min",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Su",
                        "middle": [],
                        "last": "Jian",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Li Haizhou, Zhang Min, and Su Jian. 2004. A Joint Source-channel Model for Machine Transliteration. Proceedings of ACL 2004.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Maximum entropy Markov models for information extraction and segmentation",
                "authors": [
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Mccallum",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Freitag",
                        "suffix": ""
                    },
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Pereira",
                        "suffix": ""
                    }
                ],
                "year": 2000,
                "venue": "Proceedings of ICML",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "A. McCallum, D. Freitag, and F. Pereira. 2000. Maxi- mum entropy Markov models for information ex- traction and segmentation. Proceedings of ICML.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "A Discriminative Framework for Bilingual Word Alignment",
                "authors": [
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Moore",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "R. Moore. 2005. A Discriminative Framework for Bilingual Word Alignment. Proceedings of the Conference on Empirical Methods in Natural Lan- guage Processing.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "An English-Korean Transliteration Model Using Pronunciation and Contextual Rules",
                "authors": [
                    {
                        "first": "Jong-Hoon",
                        "middle": [],
                        "last": "Oh",
                        "suffix": ""
                    },
                    {
                        "first": "Key-Sun",
                        "middle": [],
                        "last": "Choi",
                        "suffix": ""
                    }
                ],
                "year": 2000,
                "venue": "Proceedings of COLING",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jong-Hoon Oh and Key-Sun Choi. 2000. An English- Korean Transliteration Model Using Pronunciation and Contextual Rules. Proceedings of COLING.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Probabilistic outputs for support vector machines and comparison to regularized likelihood methods",
                "authors": [
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Platt",
                        "suffix": ""
                    }
                ],
                "year": 1999,
                "venue": "Advances in Large Margin Classi\u00a3ers",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "J. Platt. 1999. Probabilistic outputs for support vector machines and comparison to regularized likelihood methods. In Advances in Large Margin Classi\u00a3ers.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "The Use of Classifiers in Sequential Inference",
                "authors": [
                    {
                        "first": "V",
                        "middle": [],
                        "last": "Punyakanok",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Roth",
                        "suffix": ""
                    }
                ],
                "year": 2001,
                "venue": "Proceedings of the Conference on Advances in Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "V. Punyakanok and D. Roth. 2001. The Use of Classi- fiers in Sequential Inference. Proceedings of the Conference on Advances in Neural Information Processing Systems.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Learning Structured Prediction Models: A Large Margin Approach",
                "authors": [
                    {
                        "first": "B",
                        "middle": [],
                        "last": "Taskar",
                        "suffix": ""
                    },
                    {
                        "first": "V",
                        "middle": [],
                        "last": "Chatalbashev",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Koller",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Guestrin",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of Twenty Second International Conference on Machine Learning",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "B. Taskar, V. Chatalbashev, D. Koller and C. Gues- trin. 2005. Learning Structured Prediction Models: A Large Margin Approach. Proceedings of Twenty Second International Conference on Machine Learning.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "A Discriminative Matching Approach to Word Alignment",
                "authors": [
                    {
                        "first": "B",
                        "middle": [],
                        "last": "Taskar",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Lacoste-Julien",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Klein",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "B. Taskar, S. Lacoste-Julien, and D. Klein. 2005a. A Discriminative Matching Approach to Word Align- ment. Proceedings of the Conference on Empirical Methods in Natural Language Processing.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Transliteration of Proper Names in Cross-lingual Information Retrieval",
                "authors": [
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Virga",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Khudanpur",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Proceedings of ACL 2003 workshop MLNER",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "P. Virga and S. Khudanpur. 2003. Transliteration of Proper Names in Cross-lingual Information Re- trieval. Proceedings of ACL 2003 workshop MLNER.",
                "links": null
            }
        },
        "ref_entries": {
            "TABREF0": {
                "content": "<table><tr><td/><td>Training</td><td>Testing</td></tr><tr><td>Arabic</td><td>935</td><td>233</td></tr><tr><td>Korean</td><td>11973</td><td>1363</td></tr><tr><td>Russian</td><td>545</td><td>121</td></tr><tr><td colspan=\"3\">Table 1. Transliteration Data.</td></tr></table>",
                "type_str": "table",
                "text": "The dataset contained mostly foreign names transliterated into Korean. All datasets were randomly split into training and (blind) testing parts.",
                "html": null,
                "num": null
            }
        }
    }
}