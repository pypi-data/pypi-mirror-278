{
    "paper_id": "2022",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:14:13.906176Z"
    },
    "title": "Cross-Utterance Conditioned VAE for Non-Autoregressive Text-to-Speech",
    "authors": [
        {
            "first": "Yang",
            "middle": [],
            "last": "Li",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "ShanghaiTech University",
                "location": {}
            },
            "email": ""
        },
        {
            "first": "Cheng",
            "middle": [],
            "last": "Yu",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "ShanghaiTech University",
                "location": {}
            },
            "email": ""
        },
        {
            "first": "Guangzhi",
            "middle": [],
            "last": "Sun",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Cambridge University",
                "location": {}
            },
            "email": ""
        },
        {
            "first": "Hua",
            "middle": [],
            "last": "Jiang",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Neurowave Ai Limited",
                "location": {}
            },
            "email": ""
        },
        {
            "first": "Fanglei",
            "middle": [],
            "last": "Sun",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "ShanghaiTech University",
                "location": {}
            },
            "email": ""
        },
        {
            "first": "Weiqin",
            "middle": [],
            "last": "Zu",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "ShanghaiTech University",
                "location": {}
            },
            "email": ""
        },
        {
            "first": "Ying",
            "middle": [],
            "last": "Wen",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Shanghai Jiao Tong University",
                "location": {}
            },
            "email": ""
        },
        {
            "first": "Yang",
            "middle": [],
            "last": "Yang",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "ShanghaiTech University",
                "location": {}
            },
            "email": ""
        },
        {
            "first": "Jun",
            "middle": [],
            "last": "Wang",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University College London",
                "location": {}
            },
            "email": ""
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Modelling prosody variation is critical for synthesizing natural and expressive speech in endto-end text-to-speech (TTS) systems. In this paper, a cross-utterance conditional VAE (CUC-VAE) is proposed to estimate a posterior probability distribution of the latent prosody features for each phoneme by conditioning on acoustic features, speaker information, and text features obtained from both past and future sentences. At inference time, instead of the standard Gaussian distribution used by VAE, CUC-VAE allows sampling from an utterance-specific prior distribution conditioned on cross-utterance information, which allows the prosody features generated by the TTS system to be related to the context and is more similar to how humans naturally produce prosody. The performance of CUC-VAE is evaluated via a qualitative listening test for naturalness, intelligibility and quantitative measurements, including word error rates and the standard deviation of prosody attributes. Experimental results on LJ-Speech and LibriTTS data show that the proposed CUC-VAE TTS system improves naturalness and prosody diversity with clear margins.",
    "pdf_parse": {
        "paper_id": "2022",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Modelling prosody variation is critical for synthesizing natural and expressive speech in endto-end text-to-speech (TTS) systems. In this paper, a cross-utterance conditional VAE (CUC-VAE) is proposed to estimate a posterior probability distribution of the latent prosody features for each phoneme by conditioning on acoustic features, speaker information, and text features obtained from both past and future sentences. At inference time, instead of the standard Gaussian distribution used by VAE, CUC-VAE allows sampling from an utterance-specific prior distribution conditioned on cross-utterance information, which allows the prosody features generated by the TTS system to be related to the context and is more similar to how humans naturally produce prosody. The performance of CUC-VAE is evaluated via a qualitative listening test for naturalness, intelligibility and quantitative measurements, including word error rates and the standard deviation of prosody attributes. Experimental results on LJ-Speech and LibriTTS data show that the proposed CUC-VAE TTS system improves naturalness and prosody diversity with clear margins.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Recently, abundant research have been performed on modelling variations other than the input text in synthesized speech such as background noise, speaker information, and prosody, as those directly influence the naturalness and expressiveness of the generated audio. Prosody, as the focus of this paper, collectively refers to the stress, intonation, and rhythm in speech, and has been an increasingly popular research aspect in end-to-end TTS systems (van den Oord et al., 2016; Wang et al., 2017; Stanton et al., 2018; Elias et al., 2021; Chen et al., 2021) . Some previous work captured prosody features ex-plicitly using either style tokens or variational autoencoders (VAEs) (Kingma and Welling, 2014; Hsu et al., 2019a) which encapsulate prosody information into latent representations. Recent work achieved fine-grained prosody modelling and control by extracting prosody features at phoneme or word-level (Lee and Kim, 2019; Sun et al., 2020a,b) . However, the VAE-based TTS system lacks control over the latent space where the sampling is performed from a standard Gaussian prior during inference. Therefore, recent research (Dahmani et al., 2019; Karanasou et al., 2021) employed a conditional VAE (CVAE) (Sohn et al., 2015) to synthesize speech from a conditional prior. Meanwhile, pre-trained language model (LM) such as bidirectional encoder representation for Transformers (BERT) (Devlin et al., 2019) has also been applied to TTS systems (Hayashi et al., 2019; Kenter et al., 2020; Jia et al., 2021; Futamata et al., 2021; Cong et al., 2021) to estimate prosody attributes implicitly from pre-trained text representations within the utterance or the segment. Efforts have been devoted to include cross-utterance information in the input features to improve the prosody modelling of auto-regressive TTS (Xu et al., 2021) .",
                "cite_spans": [
                    {
                        "start": 452,
                        "end": 479,
                        "text": "(van den Oord et al., 2016;",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 480,
                        "end": 498,
                        "text": "Wang et al., 2017;",
                        "ref_id": "BIBREF31"
                    },
                    {
                        "start": 499,
                        "end": 520,
                        "text": "Stanton et al., 2018;",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 521,
                        "end": 540,
                        "text": "Elias et al., 2021;",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 541,
                        "end": 559,
                        "text": "Chen et al., 2021)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 680,
                        "end": 706,
                        "text": "(Kingma and Welling, 2014;",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 707,
                        "end": 725,
                        "text": "Hsu et al., 2019a)",
                        "ref_id": null
                    },
                    {
                        "start": 913,
                        "end": 932,
                        "text": "(Lee and Kim, 2019;",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 933,
                        "end": 953,
                        "text": "Sun et al., 2020a,b)",
                        "ref_id": null
                    },
                    {
                        "start": 1134,
                        "end": 1156,
                        "text": "(Dahmani et al., 2019;",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 1157,
                        "end": 1180,
                        "text": "Karanasou et al., 2021)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 1215,
                        "end": 1234,
                        "text": "(Sohn et al., 2015)",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 1394,
                        "end": 1415,
                        "text": "(Devlin et al., 2019)",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 1453,
                        "end": 1475,
                        "text": "(Hayashi et al., 2019;",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 1476,
                        "end": 1496,
                        "text": "Kenter et al., 2020;",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 1497,
                        "end": 1514,
                        "text": "Jia et al., 2021;",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 1515,
                        "end": 1537,
                        "text": "Futamata et al., 2021;",
                        "ref_id": null
                    },
                    {
                        "start": 1538,
                        "end": 1556,
                        "text": "Cong et al., 2021)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 1817,
                        "end": 1834,
                        "text": "(Xu et al., 2021)",
                        "ref_id": "BIBREF32"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "To generate more expressive prosody, while maintaining high fidelity in synthesized speech, a cross-utterance conditional VAE (CUC-VAE) component is proposed, which is integrated into and jointly optimised with FastSpeech 2 (Ren et al., 2021) , a commonly used non-autoregressive end-toend TTS system. Specifically, the CUC-VAE TTS system consists of cross-utterance embedding (CUembedding) and cross-utterance enhanced CVAE (CU-enhanced CVAE). The CU-embedding takes BERT sentence embeddings from surrounding utterances as inputs and generates phoneme-level CUembedding using a multi-head attention (Vaswani et al., 2017) layer where attention weights are derived from the encoder output of each phoneme as well as the speaker information. The CU-enhanced CVAE is proposed to improve prosody variation and to address the inconsistency between the standard Gaussian prior, which the VAE-based TTS system is sampled from, and the true prior of speech. Specifically, the CU-enhanced CVAE is a fine-grained VAE that estimates the posterior of latent prosody features for each phoneme based on acoustic features, cross-utterance embedding, and speaker information. It improves the encoder of standard VAE with an utterance-specific prior. To match the inference with training, the utterancespecific prior, jointly optimised with the system, is conditioned on the output of CU-embedding. Latent prosody features are sampled from the derived utterance-specific prior instead of a standard Gaussian prior during inference.",
                "cite_spans": [
                    {
                        "start": 224,
                        "end": 242,
                        "text": "(Ren et al., 2021)",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 600,
                        "end": 622,
                        "text": "(Vaswani et al., 2017)",
                        "ref_id": "BIBREF30"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The proposed CUC-VAE TTS system was evaluated on the LJ-Speech read English data and the LibriTTS English audiobook data. In addition to the sample naturalness measured via subjective listening tests, the intelligibility is measured using word error rate (WER) from an automatic speech recognition (ASR) system, and diversity in prosody was measured by calculating standard deviations of prosody attributes among all generated audio samples of an utterance. Experimental results showed that the system with CUC-VAE achieved a much better prosody diversity while improving both the naturalness and intelligibility compared to the standard FastSpeech 2 baseline and two variants.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The rest of this paper is organised as follows. Section 2 introduces the background and related work. Section 3 illustrates the proposed CUC-VAE TTS system. Experimental setup and results are shown in Section 4 and Section 5, with conclusions in Section 6.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Non-Autoregressive TTS. Promising progress has taken place in non-autoregressive TTS systems to synthesize audio with high efficiency and high fidelity thanks to the advancement in deep learning. A non-autoregressive TTS system maps the input text sequence into an acoustic feature or waveform sequence without using the autoregressive decomposition of output probabilities. Fast-Speech (Ren et al., 2019) and ParaNet (Peng et al., 2019) requires distillation from an autoregressive model, while more recent non-autoregressive TTS systems, including FastPitch (La'ncucki, 2021) , AlignTTS (Zeng et al., 2020) and FastSpeech 2 (Ren et al., 2021) , do not rely on any form of knowledge distillation from a pre-trained TTS system. In this paper, the proposed CUC-VAE TTS system is based on FastSpeech 2. FastSpeech 2 replaces the knowledge distillation for the length regulator in FastSpeech with mean-squared error training based on duration labels, which are obtained from frame-to-phoneme alignment to simplify the training process. Additionally, FastSpeech 2 predicts pitch and energy from the encoder output, which is also supervised with pitch contours and L2-norm of signal amplitudes as labels respectively. The pitch and energy prediction injects additional prosody information, which improves the naturalness and expressiveness in the synthesized speech.",
                "cite_spans": [
                    {
                        "start": 387,
                        "end": 405,
                        "text": "(Ren et al., 2019)",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 418,
                        "end": 437,
                        "text": "(Peng et al., 2019)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 560,
                        "end": 577,
                        "text": "(La'ncucki, 2021)",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 589,
                        "end": 608,
                        "text": "(Zeng et al., 2020)",
                        "ref_id": "BIBREF35"
                    },
                    {
                        "start": 626,
                        "end": 644,
                        "text": "(Ren et al., 2021)",
                        "ref_id": "BIBREF22"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Background",
                "sec_num": "2"
            },
            {
                "text": "Pre-trained Representation in TTS. It is believed that prosody can also be inferred from language information in both current and surrounding utterances (Shen et al., 2018; Fang et al., 2019; Xu et al., 2021; Zhou et al., 2021) . Such information is often entailed in vector representations from a pre-trained LM, such as BERT (Devlin et al., 2019) . Some existing work incorporated BERT embeddings at word or subword-level into autoregressive TTS models (Shen et al., 2018; Fang et al., 2019) . More recent work (Xu et al., 2021) used the chunked and paired sentence patterns from BERT. Besides, a relational gated graph network with pretrained BERT embeddings as node inputs (Zhou et al., 2021) was used to extract word-level semantic representations, thus enhancing expressiveness.",
                "cite_spans": [
                    {
                        "start": 153,
                        "end": 172,
                        "text": "(Shen et al., 2018;",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 173,
                        "end": 191,
                        "text": "Fang et al., 2019;",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 192,
                        "end": 208,
                        "text": "Xu et al., 2021;",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 209,
                        "end": 227,
                        "text": "Zhou et al., 2021)",
                        "ref_id": "BIBREF36"
                    },
                    {
                        "start": 327,
                        "end": 348,
                        "text": "(Devlin et al., 2019)",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 455,
                        "end": 474,
                        "text": "(Shen et al., 2018;",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 475,
                        "end": 493,
                        "text": "Fang et al., 2019)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 513,
                        "end": 530,
                        "text": "(Xu et al., 2021)",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 677,
                        "end": 696,
                        "text": "(Zhou et al., 2021)",
                        "ref_id": "BIBREF36"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Background",
                "sec_num": "2"
            },
            {
                "text": "VAEs in TTS. VAEs have been widely adopted in TTS systems to explicit model prosody variation. The training objective of VAE is to maximise p \u03b8 (x), the data likelihood parameterised by \u03b8, which can be regarded as the marginalisation w.r.t. the latent vector z as shown in Eq. (1).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Background",
                "sec_num": "2"
            },
            {
                "text": "p \u03b8 (x) = p \u03b8 (x | z)p(z)dz.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Background",
                "sec_num": "2"
            },
            {
                "text": "(1)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Background",
                "sec_num": "2"
            },
            {
                "text": "To make this calculation tractable, the marginalisation is approximated using evidence lower bound (ELBO):",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Background",
                "sec_num": "2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "L ELBO (x) = E q \u03d5 (z|x) [log p \u03b8 (x|z)] -\u03b2D KL (q \u03d5 (z|x)\u2225p(z)) ,",
                        "eq_num": "(2)"
                    }
                ],
                "section": "Background",
                "sec_num": "2"
            },
            {
                "text": "where q \u03d5 (z|x) is the posterior distribution of the latent vector parameterized by \u03d5, \u03b2 is a hyperparameter, and D KL (\u2022) is the Kullback-Leibler divergence. The first term measures the expected reconstruction performance of the data from the",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Background",
                "sec_num": "2"
            },
            {
                "text": "CU-Embedding p < l a t e x i t s h a 1 _ b a s e 6 4 = \" n b J k E H D w z Y a E H h w D x K T o z x l g 3 O 0 = \" > A A A B 7 3 i c b V B N S w M x E J 2 t X 7 V + V T 1 6 C R b B U 9 m t g h 6 L X j x W s B / Q L i W b Z t v Q J B u T r F C W / g k v H h T x 6 t / x 5 r 8 x b f e g r Q 8 G H u / N M D M v U p w Z 6 / v f X m F t f W N z q 7 h d 2 t n d 2 z 8 o H x 6 1 T J J q Q p s k 4 Y n u R N h Q z i R t W m Y 5 7 S h N s Y g 4 b U f j 2 5 n f f q L a s E Q + 2 I m i o c B D y W J G s H V S p 2 f Y U O C + 6 p c r f t W f A 6 2 S I C c V y N H o l 7 9 6 g 4 S k g k p L O D a m G / j K h h n W l h F O p 6 V e a q j C Z I y H t O u o x I K a M J v f O 0 V n T h m g O N G u p E V z 9 f d E h o U x E x G 5 T o H t y C x 7 M / E / r 5 v a + D r M m F S p p Z I s F s U p R z Z B s + f R g G l K L J 8 4 g o l m 7 l Z E R l h j Y l 1 E J R d C s P z y K m n V q s F F t X Z / W a n f 5 H E U 4 Q R O 4 R w C u I I 6 3 E E D m k C A w z O",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Background",
                "sec_num": "2"
            },
            {
                "text": "8 w p v 3 6 L 1 4 7 9 7 H o r X g 5 T P H 8 A f e 5 w 8 m n p A K < / l a t e x i t > \u00b5p < l a t e x i t s h a 1 _ b a s e 6 4 = \" S g x V S z p w",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Background",
                "sec_num": "2"
            },
            {
                "text": "K z 3 K k 8 t v z O m u x t 1 n P 4 k = \" > A A A B 7 H i c b V B N S w M x E J 3 1 s 9 a v q k c v w S J 4 K r t V 0 G P R i 8 c K b l t o l 5 J N s 2 1 o k g 1 J V i h L f 4 M X D 4 p 4 9 Q d 5 8 9 + Y t n v Q 1 g c D j / d m m J k X K 8 6 M 9 f 1 v b 2 1 9 Y 3 N r u 7 R T 3 t 3 b P z i s H B 2 3 T J p p Q k O S 8 l R 3 Y m w o Z 5 K G l l l O O 0 p T L G J O 2 / H 4 b u a 3 n 6 g 2 L J W P d q J o J P B Q s o Q R b J 0 U 9 k T W V / 1 K 1 a / 5 c 6 B V E h S k C g W a / c p X b 5 C S T F B p C c f G d A N f 2 S j H 2 j L C 6 b T c y w x V m I z x k H Y d l V h Q E + X z Y 6 f o 3 C k D l K T a l b R o r v 6 e y L E w Z i J i 1 y m w H Z l l b y b + 5 3 U z m 9 x E O Z M q s 1 S S x a I k 4 8 i m a P Y 5 G j B N i e U T R z D R z N 2 K y A h r T K z L p + x C C J Z f X i W t e i 2 4 r N U f r q q N 2 y K O E p z C G V x A A N f Q g H t o Q g g E G D z D K 7 x 5 0 n v x 3 r 2 P R e u a V 8 y c w B 9 4 n z / l J I 6 9 < / l a t e x i t > G2P p 1 , p 2 , \u2022 \u2022 \u2022 , p T < l a t e x i t s h a 1 _ b a s e 6 4 = \" i W S + G 5 p r v d q I N X 1 W 0 g E e o + V / A 7 E = \" > A A A B + 3 i c b Z D L S g M x F I Y z 9 V b r b a x L N 8 E i u C h l p g q 6 L L p x W a E 3 a I c h k 8 m 0 o Z k k J B m x l L 6 K G x e K u P V F 3 P k 2 p u 0 s t P W H w M d / z u G c / J F k V B v P + 3 Y K G 5 t b 2 z v F 3 d L e / s H h k X t c 7 m i R K U z a W D C h e h H S h F F O 2 o Y a R n p S E Z R G j H S j 8 d 2 8 3 n 0 k S l P B W 2 Y i S Z C i I a c J x c h Y K 3 T L M v S r M q x X B z g W R l t s h W 7 F q 3 k L w X X w c 6 i A X M 3 Q / R r E A m c p 4 Q Y z p H X f 9 6 Q J p k g Z i h m Z l Q a Z J h L h M R q S v k W O U q K D 6 e L 2 G T y 3 T g w T o e z j B i 7 c 3 x N T l G o 9 S S P b m S I z 0 q u 1 u f l f r Z + Z 5 C a Y U i 4 z Q z h e L k o y B o 2 A 8 y B g T B X B h k 0 s I K y o v R X i E V I I G x t X y Y b g r 3 5 5 H T r 1 m n 9 Z q z 9 c V R q 3 e R x F c A r O w A X w w T V o g H v Q B G 2 A w R N 4 B q / g z Z k 5 L 8 6 7 8 7 F s L T j 5 z A n 4 I + f z B 5 r L k 4 Q = < / l a t e x i t > f (p1), f(p2), \u2022 \u2022 \u2022 , f(pT ) < l a t e x i t s h a 1 _ b a s e 6 4 = \" h i C w z h A / O 5 6 G E A P 7 p D n / P h i O j S o = \" > A A A C B X i c b V B N S 8 M w G E 7 n 1 5 x f V Y 9 6 C A 5 h g z H a K e h x 6 M X j h H 3 B V k q a p l t Y 2 p Q k F U b Z x Y t / x Y s H R b z 6 H 7 z 5 b 0 y 3 H n T z g Z A n z / O + v H k f L 2 Z U K s v 6 N g p r 6 x u b W 8 X t 0 s 7 u 3 v 6 B e X j U l T w R m H Q w Z 1 z 0 P S Q J o x H p K K o Y 6 c e C o N B j p O d N b j O / 9 0 C E p D x q q 2 l M n B C N I h p Q j J S W X P M 0 q M S u X a 1 l V 6 N a G 2 K f K 1 m D 2 b N d d c 2 y V b f m g K v E z k k Z 5 G i 5 5 t f Q 5 z g J S a Q w Q 1 I O b C t W T o q E o p i R W W m Y S B I j P E E j M t A 0 Q i G R T j r f Y g b P t e L D g A t 9 I g X n 6 u + O F I V S T k N P V 4 Z I j e W y l 4 n / e Y N E B d d O S q M 4 U S T C i 0 F B w q D i M I s E + l Q Q r N h U E 4 Q F 1 X + F e I w E w k o H V 9 I h 2 M s r r 5 J u o 2 5 f 1 B v 3 l + X m T R 5 H E Z y A M 1 A B N r g C T X A H W q A D M H g E z + A V v B l P x o v x b n w s S g t G 3 n M M / s D 4 / A G 9 I 5 Y t < / l a t e x i t > Speaker Embeding b L < l a t e x i t s h a 1 _ b a s e 6 4 = \" o a 7 e r x t N W M K x c 2 2 g g u H R u G d Y 4 D U = \" > A A A B 7 X i c b V C 7 T g M x E N z j G c I r Q E l j i J B C Q X Q X C i g j a C g o g k Q e U n K K f I 4 v M f H Z J 9 u H F J 3 y D d B Q g B A t / 8 E n 0 P E h 9 D i P A h J G W m k 0 s 6 v d n S D m T B v X / X I W F p e W V 1 Y z a 9 n 1 j c 2 t 7 d z O b k 3 L R B F a J Z J L 1 Q i w p p w J W j X M c N q I F c V R w G k 9 6 F + O / P o 9 V Z p J c W s G M f U j 3 B U s Z A Q b K 9 W C d n p y P W z n 8 m 7 R H Q P N E 2 9 K 8 u W D w v f H Q + u 4 0 s 5 9 t j q S J B E V h n C s d d N z Y + O n W B l G O B 1 m W 4 m m M S Z 9 3 K V N S w W O q P b T 8 b V D d G S V D g q l s i U M G q u / J 1 I c a T 2 I A t s Z Y d P T s 9 5 I / M 9 r J i Y 8 9 1 M m 4 s R Q Q S a L w o Q j I 9 H o d d R h i h L D B 5 Z g o p i 9 F Z E e V p g Y G 1 D W h u D N v j x P a q W i d 1 o s 3 d g 0 L m C C D O z D I R T A g z M o w x V U o A o E 7 u A R n u H F k c 6 T 8 + q 8 T V o X n O n M H v y B 8 / 4 D v H K S P A = = < / l a t e x i t > b L+1 < l a t e x i t s h a 1 _ b a s e 6 4 = \" u u O H Q Y z O m d c 1 L C F 8 Y M m z i y M L O P g = \" > A A A B 7 3 i c b V C 7 S g N B F L 0 b X z G + o p Y 2 o 0 G I i G E 3 F l o G b S w s I p g H J E u Y n c w m Q 2 Z n 1 5 l Z I S z 5 B s H G Q h F b f 8 N P s P N D 7 J 1 s U m j i g Q u H c + 7 l 3 n u 8 i D O l b f v L y i w s L i 2 v Z F d z a + s b m 1 v 5 7 Z 2 6 C m N J a I 2 E P J R N D y v K m a A 1 z T S n z U h S H H i c N r z B 5 d h v 3 F O p W C h u 9 T C i b o B 7 g v m M Y G 2 k p t d J T q 6 P n V E n X 7 B L d g o 0 T 5 w p K V T 2 i 9 8 f D + 2 j a i f / 2 e 6 G J A 6 o 0 I R j p V q O H W k 3 w V I z w u k o 1 4 4 V j T A Z 4 B 5 t G S p w Q J W b p P e O 0 K F R u s g P p S m h U a r + n k h w o N Q w 8 E x n g H V f z X p j 8 T + v F W v / 3 E 2 Y i G J N B Z k s 8 m O O d I j G z 6 M u k 5 R o P j Q E E 8 n M r Y j 0 s c R E m 4 h y J g R n 9 u V 5 U i + X n N N S + c a k c Q E T Z G E P D q A I D p x B B a 6 g C j U g w O E R n u H F u r O e r F f r b d K a s a Y z u / A H 1 v s P l c C S r A = = < / l a t e x i t > cat cat BERT Linear BERT Conv 1D Conv 1D \u2022 \u2022 \u2022 < l a t e x i t s h a 1 _ b a s e 6 4 = \" 9 p G P a D a b N i j k W L t d 2 p l u v C o 4 p 5 o = \" > A A A B 7 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 B I v g q S R V 0 G P R i 8 c K 9 g P a U D a b T b t 2 s x t 2 J 0 I p / Q 9 e P C j i 1 f / j z X / j t s 1 B W x 8 M P N 6 b Y W Z e m A p u 0 P O + n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q G Z V p y p p U C a U 7 I T F M c M m a y F G w T q o Z S U L B 2 u H o d u a 3 n 5 g 2 X M k H H K c s S M h A 8 p h T g l Z q 9 W i k 0 P T L F a / q z e G u E j 8 n F c j R 6 J e / e p G i W c I k U k G M 6 f p e i s G E a O R U s G m p l x m W E j o i A 9 a 1 V J K E m W A y v 3 b q n l k l c m O l b U l 0 5 + r v i Q l J j B k n o e 1 M C A 7 N s j c T / / O 6 G c b X w Y T L N E M m 6 W J R n A k X l T t 7 3 Y 2 4 Z h T F 2 B J C N b e 3 u n R I N K F o A y r Z E P z l l 1 d J q 1 b 1 L 6 q 1 + 8 t K / S a P o w g n c A r n 4 M M V 1 O E O G t A E C o / w D K / w 5 i j n x X l 3 P h a t B S e f O Y Y / c D 5 / A K + l j z M = < / l a t e x i t > \u2022 \u2022 \u2022 < l a t e x i t s h a 1 _ b a s e 6 4 = \" 9 p G P a D a b N i j k W L t d 2 p l u v C o 4 p 5 o = \" > A A A B 7 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 B I v g q S R V 0 G P R i 8 c K 9 g P a U D a b T b t 2 s x t 2 J 0 I p / Q 9 e P C j i 1 f / j z X / j t s 1 B W x 8 M P N 6 b Y W Z e m A p u 0 P O + n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q G Z V p y p p U C a U 7 I T F M c M m a y F G w T q o Z S U L B 2 u H o d u a 3 n 5 g 2 X M k H H K c s S M h A 8 p h T g l Z q 9 W i k 0 P T L F a / q z e G u E j 8 n F c j R 6 J e / e p G i W c I k U k G M 6 f p e i s G E a O R U s G m p l x m W E j o i A 9 a 1 V J K E m W A y v 3 b q n l k l c m O l b U l 0 5 + r v i Q l J j B k n o e 1 M C A 7 N s j c T / / O 6 G c b X w Y T L N E M m 6 W J R n A k X l T t 7 3 Y 2 4 Z h T F 2 B J C N b e 3 u n R I N K F o A y r Z E P z l l 1 d J q 1 b 1 L 6 q 1 + 8 t K / S a P o w g n c A r n 4 M M V 1 O E O G t A E C o / w D K / w 5 i j n x X l 3 P h a t B S e f O Y Y / c D 5 / A K + l j z M = < / l a t e x i t > ui 1 < l a t e x i t s h a 1 _ b a s e 6 4 = \" C z Z e O Q l L q 1 d 5 d 0 k x w m K W / r 6 t S i I = \" > A A A B 8 H i c b V B N S w M x E J 2 t X 7 V + V T 1 6 C R b B i 2 W 3 F f R Y 9 O K x g v 2 Q d i n Z N N u G J t k l y Q p l 2 V / h x Y M i X v 0 5 3 v w 3 p u 0 e t P X B w O O 9 G W b m B T F n 2 r j u t 1 N Y W 9 / Y 3 C p u l 3 Z 2 9 / Y P y o d H b R 0 l i t A W i X i k u g H W l D N J W 4 Y Z T r u x o l g E n H a C y e 3 M 7 z x R p V k k H 8 w 0 p r 7 A I 8 l C R r C x 0 m O a D F J 2 4 W X Z o F x x q + 4 c a J V 4 O a l A j u a g / N U f R i Q R V B r C s d Y 9 z 4 2 N n 2 J l G O E 0 K / U T T W N M J n h E e 5 Z K L K j 2 0 / n B G T q z y h C F k b I l D Z q r v y d S L L S e i s B 2 C m z G e t m b i f 9 5 v c S E 1 3 7 K Z J w Y K s l i U Z h w Z C I 0 + x 4 N m a L E 8 K k l m C h m b 0 V k j B U m x m Z U s i F 4 y y + v k n a t 6 t W r t f v L S u M m j 6 M I J 3 A K 5 + D B F T T g D p r Q A g I C n u E V 3 h z l v D j v z s e i t e D k M 8 f w B 8 7 n D 8 a 7 k G M = < / l a t e x i t > ui+1 < l a t e x i t s h a 1 _ b a s e 6 4 = \" 1 z e g K d G K w c R h 5 C 1 t 7 o j y x X g 2 v c o = \" > A A A B 8 H i c b V B N S w M x E J 2 t X 7 V + V T 1 6 C R Z B E M p u K + i x 6 M V j B f s h 7 V K y a b Y N T b J L k h X K s r / C i w d F v P p z v P l v T N s 9 a O u D g c d 7 M 8 z M C 2 L O t H H d b 6 e w t r 6 x u V X c L u 3 s 7 u 0 f l A + P 2 j p K F K E t E v F I d Q O s K W e S t g w z n H Z j R b E I O O 0 E k 9 u Z 3 3 m i S r N I P p h p T H 2 B R 5 K F j G B j p c c 0 G a T s w s u y Q b n i V t 0 5 0 C r x c l K B H M 1 B + a s / j E g i q D S E Y 6 1 7 n h s b P 8 X K M M J p V u o n m s a Y T P C I 9 i y V W F D t p / O D M 3 R m l S E K I 2 V L G j R X f 0 + k W G g 9 F Y H t F N i M 9 b I 3 E / / z e o k J r / 2 U y T g x V J L F o j D h y E R o 9 j 0 a M k W J 4 V N L M F H M 3 o r I G C t M j M 2 o Z E P w l l 9 e J e 1 a 1 a t X a / e X l c Z N H k c R T u A U z s G D K 2 j A H T S h B Q Q E P M M r v D n K e X H e n Y 9 F a 8 H J Z 4 7 h D 5 z P H 8 O t k G E = < / l a t e x i t > ui < l a t e x i t s h a 1 _ b a s e 6 4 = \" T p w C X O u G P B K Y W 2 n K T L d u V U Y F q E E = \" > A A A B 7 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K k k V 9 F j 0 4 r G C / Y A 2 l M 1 2 0 y 7 d b M L u R C g h P 8 K L B 0 W 8 + n u 8 + W / c t j l o 6 4 O B x 3 s z z M w L E i k M u u 6 3 s 7 a + s b m 1 X d o p 7 + 7 t H x x W j o 7 b J k 4 1 4 y 0 W y 1 h 3 A 2 q 4 F I q 3 U K D k 3 U R z G g W S d 4 L J 3 c z v P H F t R K w e c Z p w P 6 I j J U L B K F q p k 6 W D T O T 5 o F J 1 a + 4 c Z J V 4 B a l C g e a g 8 t U f x i y N u E I m q T E 9 z 0 3 Q z 6 h G w S T P y / 3 U 8 I S y C R 3 x n q W K R t z 4 2 f z c n J x b Z U j C W N t S S O b q 7 4 m M R s Z M o 8 B 2 R h T H Z t m b i f 9 5 v R T D G z 8 T K k m R K 7 Z Y F K a S Y E x m v 5 O h 0 J y h n F p C m R b 2 V s L G V F O G N q G y D c F b f n m V t O s 1 7 7 J W f 7 i q N m 6 L O E p w C m d w A R 5 c Q w P u o Q k t Y D C B Z 3 i F N y d x X p x 3 5 2 P R u u Y U M y f w B 8 7 n D + k D j / E = < / l a t e x i t >",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Background",
                "sec_num": "2"
            },
            {
                "text": "Si < l a t e x i t s h a 1 _ b a s e 6 4 = \" q t 6 1 y 0",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Cross-Utterrance Text",
                "sec_num": null
            },
            {
                "text": "d C 4 e 0 O M H c 1 X G H X J v 0 k u b Y = \" > A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m q o M e i F 4 8 V T V t o Q 9 l s t + 3 S z S b s T o Q S + h u 8 e F D E q z / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 M J H C o O t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U N H G q G f d Z L G P d D q n h U i j u o 0 D J 2 4 n m N A o l b 4 X j 2 5 n f e u L a i F g 9 4 i T h Q U S H S g w E o 2 g l P 3 v o i W m v X H G r 7 h x k l X g 5 q U C O R q / 8 1 e 3 H L I 2 4 Q i a p M R 3 P T T D I q E b B J J + W u q n h C W V j O u Q d S x W N u A m y + b F T c m a V P h n E 2 p Z C M l d / T 2 Q 0 M m Y S h b Y z o j g y y 9 5 M / M / r p D i 4 D j K h k h S 5 Y o t F g 1 Q S j M n s c 9 I X m j O U E 0 s o 0 8 L e S t i I a s r Q 5 l O y I X j L L 6 + S Z q 3 q X V R r 9 5 e V + k 0 e R x F O 4 B T O w Y M r q M M d N M A H B g K e 4 R X e H O W 8 O O / O x 6 K 1 4 O Q z x / A H z u c P 7 k i O w w = = < / l a t e x i t > [1001] < l a t e x i t s h a 1 _ b a s e 6 4 = \" r q h 6 O E v X e h V A v g I S p i G c N Y 7 2 + 4 w = \" > A A A B 8 X i c b V B N S w M x E J 2 t X 7 V + V T 1 6 C R b B U 9 m t g h 6 L X j x W s B / Y X U o 2 n W 1 D s 9 k l y Q q l 9 F 9 4 8 a C I V / + N N / + N a b s H b X 0 Q e L w 3 M 5 l 5 Y S q 4 N q 7 7 7 R T W 1 j c 2 t 4 r b p Z 3 d v f 2 D 8 u F R S y e Z Y t h k i U h U J 6 Q a B Z f Y N N w I 7 K Q K a R w K b I e j 2 5 n f f k K l e S I f z D j F I K Y D y S P O q L H S o 0 + 6 n u t 6 g U 9 6 5 Y p b d e c g q 8 T L S Q V y N H r l L 7 + f s C x G a Z i g W t t B q Q k m V B n O B E 5 L f q Y x p W x E B 9 i 1 V N I Y d T C Z b z w l Z 1 b p k y h R 9 k l D 5 u r v j g m N t R 7 H o a 2 M q R n q Z W 8 m / u d 1 M x N d B x M u 0 8 y g Z I u P o k w Q k 5 D Z + a T P F T I j x p Z Q p r j d l b A h V Z Q Z G 1 L J h u A t n 7 x K W r W q d 1 G t 3 V 9 W 6 j d 5 H E U 4 g V M 4 B w + u o A 5 3 0 I A m M J D w D K / w 5 m j n x X l 3 P h a l B S f v O Y Y / c D 5 / A D 4 + j 1 Q = < / l a t e x i t >",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Cross-Utterrance Text",
                "sec_num": null
            },
            {
                "text": "Encoder Decoder",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Speaker ID",
                "sec_num": null
            },
            {
                "text": "x i < l a t e x i t s h a 1 _ b a s e 6 4 = \" e L 0 W 8 w J U w 3 J j q p A T",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Speaker ID",
                "sec_num": null
            },
            {
                "text": "F z u 1 O H / q C p I = \" > A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m q o M e i F 4 8 V T F t o Q 9 l s p + 3 S z S b s b s Q S + h u 8 e F D E q z / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 M B F c G 9 f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o q e N U M f R Z L G L V D q l G w S X 6 h h u B 7 U Q h j U K B r X B 8 O / N b j 6 g 0 j + W D m S Q Y R H Q o + Y A z a q z k Z 0 8 9 P u 2 V K 2 7 V n Y O s E i 8 n F c j R 6 J W / u v 2 Y p R F K w w T V u u O 5 i Q k y q g x n A q e l b q o x o W x M h 9 i x V N I I d Z D N j 5 2 S M 6 v 0 y S B W t q Q h c / X 3 R E Y j r S d R a D s j a k Z 6 2 Z u J / 3 m d 1 A y u g 4 z L J D U o 2 W L R I B X E x G T 2 O e l z h c y I i S W U K W 5 v J W x E F W X G 5 l O y I X j L L 6 + S Z q 3 q X V R r 9 5 e V + k 0 e R x F O 4 B T O w Y M r q M M d N M A H B h y e 4 R X e H O m 8 O O / O x 6 K 1 4 O Q z x / A H z u c P J t q O 6 A = = < / l a t e x i t > Reference Mel Spectrogram Mel Spectrogram Duration Predictor Hi < l a t e x i t s h a 1 _ b a s e 6 4 = \" s o l D I o w m R I 2 O U 1 8 6 2 x V A p Q k a d 6 0 = \" > A A A B 8 3 i c b V D L S g M x F L 2 p r 1 p f V Z d u g k V w V W a q o M u i m y 4 r 2 A d 0 h p J J M 2 1 o J j M k G a E M / Q 0 3 L h R x 6 8 + 4 8 2 / M t L P Q 1 g O B w z n 3 c k 9 O k A i u j e N 8 o 9 L G 5 t b 2 T n m 3 s r d / c H h U P T 7 p 6 j h V l H V o L G L V D 4 h m g k v W M d w I 1 k 8 U I 1 E g W C + Y 3 u d + 7 4 k p z W P 5 a G Y J 8 y M y l j z k l B g r e V 5 E z C Q I s 9 Z 8 y I f V m l N 3 F s D r x C 1 I D Q q 0 h 9 U v b x T T N G L S U E G 0 H r h O Y v y M K M O p Y P O K l 2 q W E D o l Y z a w V J K I a T 9 b Z J 7 j C 6 u M c B g r + 6 T B C / X 3 R k Y i r W d R Y C f z j H r V y 8 X / v E F q w l s / 4 z J J D Z N 0 e S h M B T Y x z g v A I 6 4 Y N W J m C a G K 2 6 y Y T o g i 1 N i a K r Y E d / X L 6 6 T b q L t X 9 c b D d a 1 5 V 9 R R h j M 4 h 0 t w 4 Q a a 0 I I 2 d I B C A s / w C m 8 o R S / o H X 0 s R 0 u o 2 D m F P 0 C f P z c + k c w = < / l a t e x i t > Di < l a t e x i t s h a 1 _ b a s e 6 4 = \" G F T W Y N D r 3 N 6 m n Q x w U A g 6 4 u T O y p w = \" > A A A B 8 3 i c b V D L S g M x F L 2 p r 1 p f V Z d u g k V w V W a q o M u i L l x W s A / o D C W T Z t r Q T G Z I M k I Z + h t u X C j i 1 p 9 x 5 9 + Y a W e h r Q c C h 3 P u 5 Z 6 c I B F c G 8 f 5 R q W 1 9 Y 3 N r f J 2 Z W d 3 b / + g e n j U 0 X G q K G v T W M S q F x D N B J e s b b g R r J c o R q J A s G 4 w u c 3 9 7 h N T m s f y 0 U w T 5 k d k J H n I K T F W 8 r y I m H E Q Z n e z A R 9 U a 0 7 d m Q O v E r c g N S j Q G l S / v G F M 0 4 h J Q w X R u u 8 6 i f E z o g y n g s 0 q X q p Z Q u i E j F j f U k k i p v 1 s n n m G z 6 w y x G G s 7 J M G z 9 X f G x m J t J 5 G g Z 3 M M + p l L x f / 8 / q p C a / 9 j M s k N U z S x a E w F d j E O C 8 A D 7 l i 1 I i p J Y Q q b r N i O i a K U G N r q t g S 3 O U v r 5 J O o + 5 e 1 B s P l 7 X m T V F H G U 7 g F M 7 B h S t o w j 2 0 o A 0 U E n i G V 3 h D K X p B 7 + h j M V p C x c 4 x / A H 6 / A E x I p H I < / l a t e x i t >",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Speaker ID",
                "sec_num": null
            },
            {
                "text": "< l a t e x i t s h a 1 _ b a s e 6 4 = \" q t 6 1 y 0",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Si",
                "sec_num": null
            },
            {
                "text": "d C 4 e 0 O M H c 1 X G H X J v 0 k u b Y = \" > A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m q o M e i F 4 8 V T V t o Q 9 l s t + 3 S z S b s T o Q S + h u 8 e F D E q z / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 M J H C o O t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U N H G q G f d Z L G P d D q n h U i j u o 0 D J 2 4 n m N A o l b 4 X j 2 5 n f e u L a i F g 9 4 i T h Q U S H S g w E o 2 g l P 3 v o i W m v X H G r 7 h x k l X g 5 q U C O R q / 8 1 e 3 H L I 2 4 Q i a p M R 3 P T T D I q E b B J J + W u q n h C W V j O u Q d S x W N u A m y + b F T c m a V P h n E 2 p Z C M l d / T 2 Q 0 M m Y S h b Y z o j g y y 9 5 M / M / r p D i 4 D j K h k h S 5 Y o t F g 1 Q S j M n s c 9 I X m j O U E 0 s o 0 8 L e S t i I a s r Q 5 l O y I X j L L 6 + S Z q 3 q X V R r 9 5 e V + k 0 e R x F O 4 B T O w Y M r q M M d N M A H B g K e 4 R X e H O W 8 O O / O x 6 K 1 4 O Q z x / A H z u c P 7 k i O w w = = < / l a t e x i t > u i L < l a t e x i t s h a 1 _ b a s e 6 4 = \" X m s 6 C 3 0 9 B S 8 t J X G E 7 M Z a w v d K a B 8 = \" > A A A B 8 H i c b V A 9 S w N B E J 2 L X z F + R S 1 t F o N g Y 7 i L g p Z B G w u L C O Z D k i P s b f a S J b t 7 x + 6 e E I 7 7 F T Y W i t j 6 c + z 8 N 2 6 S K z T x w c D j v R l m 5 g U x Z 9 q 4 7 r d T W F l d W 9 8 o b p a 2 t n d 2 9 8 r 7 B y 0 d J Y r Q J o l 4 p D o B 1 p Q z S Z u G G U 4 7 s a J Y B J y 2 g / H N 1 G 8 / U a V Z J B / M J K a + w E P J Q k a w s d J j m v R T d n a X Z f 1 y x a 2 6 M 6 B l 4 u W k A j k a / f J X b x C R R F B p C M d a d z 0 3 N n 6 K l W G E 0 6 z U S z S N M R n j I e 1 a K r G g 2 k 9 n B 2 f o x C o D F E b K l j R o p v 6 e S L H Q e i I C 2 y m w G e l F b y r + 5 3 U T E 1 7 5 K Z N x Y q g k 8 0 V h w p G J 0 P R 7 N G C K E s M n l m C i m L 0 V k R F W m B i b U c m G 4 C 2 + v E x a t a p 3 X q 3 d X 1 T q 1 3 k c R T i C Y z g F D y 6 h D r f Q g C Y Q E P A M r / D m K O f F e X c",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Si",
                "sec_num": null
            },
            {
                "text": "+ 5 q 0 F J 5 8 5 h D 9 w P n 8 A 7 9 2 Q f g = = < / l a t e x i t >",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Si",
                "sec_num": null
            },
            {
                "text": "u i L 1 < l a t e x i t s h a 1 _ b a s e 6 4 = \" L K r I j k D S I E U 5 0 t X g i g a c u O S 8 Q Z M = \" > A A A B 8 n i c b V A 9 S w N B E N 2 L X z F + R S 1 t F o N g k 3 A X B S 2 D N h Y W E c w H X I 6 w t 9 k k S / Z 2 j 9 0 5 I R z 3 M 2 w s F L H 1 1 9 j 5 b 9 w k V 2 j i g 4 H H e z P M z A t j w Q 2 4 7 r d T W F v f 2 N w q b p d 2 d v f 2 D 8 q H R 2 2 j E k 1 Z i y q h d D c k h g k u W Q s 4 C N a N N S N R K F g n n N z O / M 4 T 0 4 Y r + Q j T m A U R G U k + 5 J S A l f w 0 6 a e 8 e l / 1 s q x f r r g 1 d w 6 8 S r y c V F C O Z r / 8 1 R s o m k R M A h X E G N 9 z Y w h S o o F T w b J S L z E s J n R C R s y 3 V J K I m S C d n 5 z h M 6 s M 8 F B p W x L w X P 0 9 k Z L I m G k U 2 s 6 I w N g s e z P x P 8 9 P Y H g d p F z G C T B J F 4 u G i c C g 8 O x / P O C a U R B T S w j V 3 N 6 K 6 Z h o Q s G m V L I h e M s v r 5 J 2 v e Z d 1 O o P l 5 X G T R 5 H E Z 2 g U 3 S O P H S F G u g O N V E L U a T Q M 3 p F b w 4 4 L 8 6 7 8 7 F o L T j 5 z D H 6 A + f z B 8 6 P k P A = < / l a t e x i t > u i < l a t e x i t s h a 1 _ b a s e 6 4 = \" T p w C X O u G P B K Y W 2 n K T L d u V U Y F q E E = \" > A A A B 7 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K k k V 9 F j 0 4 r G C / Y A 2 l M 1 2 0 y 7 d b M L u R C g h P 8 K L B 0 W 8 + n u 8 + W / c t j l o 6 4 O B x 3 s z z M w L E i k M u u 6 3 s 7 a + s b m 1 X d o p 7 + 7 t H x x W j o 7 b J k 4 1 4 y 0 W y 1 h 3 A 2 q 4 F I q 3 U K D k 3 U R z G g W S d 4 L J 3 c z v P H F t R K w e c Z p w P 6 I j J U L B K F q p k 6 W D T O T 5 o F J 1 a + 4 c Z J V 4 B a l C g e a g 8 t U f x i y N u E I m q T E 9 z 0 3 Q z 6 h G w S T P y / 3 U 8 I S y C R 3 x n q W K R t z 4 2 f z c n J x b Z U j C W N t S S O b q 7 4 m M R s Z M o 8 B 2 R h T H Z t m b i f 9 5 v R T D G z 8 T K k m R K 7 Z Y F K a S Y E x m v 5 O h 0 J y h n F p C m R b 2 V s L G V F O G N q G y D c F b f n m V t O s 1 7 7 J W f 7 i q N m 6 L O E p w C m d w A R 5 c Q w P u o Q k t Y D C B Z 3 i F N y d x X p x 3 5 2 P R u u Y U M y f w B 8 7 n D + k D j / E = < / l a t e x i t >",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Si",
                "sec_num": null
            },
            {
                "text": "x i < l a t e x i t s h a 1 _ b a s e 6 4 = \" e L 0 W 8 w J U w 3 J j q p A T F z u 1 O H / q C p I = \" > A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m q o M e i F 4 8 V T F t o Q 9 l s p + 3 S z S b s b s Q S + h u 8 e F D E q z / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 M B F c G 9 f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Si",
                "sec_num": null
            },
            {
                "text": "v 5 B + f C o q e N U M f R Z L G L V D q l G w S X 6 h h u B 7 U Q h j U K B r X B 8 O / N b j 6 g 0 j + W D m S Q Y R H Q o + Y A z a q z k Z 0 8 9 P u 2 V K 2 7 V n Y O s E i 8 n F c j R 6 J W / u v 2 Y p R F K w w T V u u O 5 i Q k y q g x n A q e l b q o x o W x M h 9 i x V N I I d Z D N j 5 2 S M 6 v 0 y S B W t q Q h c / X 3 R E Y j r S d R a D s j a k Z 6 2 Z u J / 3 m d 1 A y u g 4 z L J D U o 2 W L R I B X E x G T 2 O e l z h c y I i S W U K W 5 v J W x E F W X G 5 l O y I X j L L 6 + S Z q 3 q X V R r 9 5 e V + k 0 e R x F O 4 B T O w Y M r q M M d N M A H B h y e 4 R X e H O m 8 O O / O x 6 K 1 4 O Q z x / A H z u c P J t q O 6 A = = < / l a t e x i t > Hi < l a t e x i t s h a 1 _ b a s e 6 4 = \" s o l D I o w m R I 2 O U 1 8 6 2 x V A p Q k a d 6 0 = \" > A A A B 8 3 i c b V D L S g M x F L 2 p r 1 p f V Z d u g k V w V W a q o M u i m y 4 r 2 A d 0 h p J J M 2 1 o J j M k G a E M / Q 0 3 L h R x 6 8 + 4 8 2 / M t L P Q 1 g O B w z n 3 c k 9 O k A i u j e N 8 o 9 L G 5 t b 2 T n m 3 s r d / c H h U P T 7 p 6 j h V l H V o L G L V D 4 h m g k v W M d w I 1 k 8 U I 1 E g W C + Y 3 u d + 7 4 k p z W P 5 a G Y J 8 y M y l j z k l B g r e V 5 E z C Q I s 9 Z 8 y I f V m l N 3 F s D r x C 1 I D Q q 0 h 9 U v b x T T N G L S U E G 0 H r h O Y v y M K M O p Y P O K l 2 q W E D o l Y z a w V J K I a T 9 b Z J 7 j C 6 u M c B g r + 6 T B C / X 3 R k Y i r W d R Y C f z j H r V y 8 X / v E F q w l s / 4 z J J D Z N 0 e S h M B T Y x z g v A I 6 4 Y N W J m C a G K 2 6 y Y T o g i 1 N i a K r Y E d / X L 6 6 T b q L t X 9 c b D d a 1 5 V 9 R R h j M 4 h 0 t w 4 Q a a 0 I I 2 d I B C A s / w C m 8 o R S / o H X 0 s R 0 u o 2 D m F P 0 C f P z c + k c w = < / l a t e x i t > Di < l a t e x i t s h a 1 _ b a s e 6 4 = \" G F T W Y N D r 3 N 6 m n Q x w U A g 6 4 u T O y p w = \" > A A A B 8 3 i c b V D L S g M x F L 2 p r 1 p f V Z d u g k V w V W a q o M u i L l x W s A / o D C W T Z t r Q T G Z I M k I Z + h t u X C j i 1 p 9 x 5 9 + Y a W e h r Q c C h 3 P u 5 Z 6 c I B F c G 8 f 5 R q W 1 9 Y 3 N r f J 2 Z W d 3 b / + g e n j U 0 X G q K G v T W M S q F x D N B J e s b b g R r J c o R q J A s G 4 w u c 3 9 7 h N T m s f y 0 U w T 5 k d k J H n I K T F W 8 r y I m H E Q Z n e z A R 9 U a 0 7 d m Q O v E r c g N S j Q G l S / v G F M 0 4 h J Q w X R u u 8 6 i f E z o g y n g s 0 q X q p Z Q u i E j F j f U k k i p v 1 s n n m G z 6 w y x G G s 7 J M G z 9 X f G x m J t J 5 G g Z 3 M M + p l L x f / 8 / q p C a / 9 j M s k N U z S x a E w F d j E O C 8 A D 7 l i 1 I i p J Y Q q b r N i O i a K U G N r q t g S 3 O U v r 5 J O o + 5 e 1 B s P l 7 X m T V F H G U 7 g F M 7 B h S t o w j 2 0 o A 0 U E n i G V 3 h D K X p B 7 + h j M V p C x c 4 x / A H 6 / A E x I p H I < / l a t e x i t > Zi < l a t e x i t s h a 1 _ b a s e 6 4 = \" Z Z M r c J i 3 8 o / U d N V P M 6 8 y A W u R e E E = \" > A A A B 8 3 i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s x U Q Z d F N y 4 r 2 A d 2 h p J J M 2 1 o J h O S j F C G / o Y b F 4 q 4 9 W f c + T d m 2 l l o 6 4 H A 4 Z x 7 u S c n l J x p 4 7 r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 5 1 d J I q Q t s k 4 Y n q h V h T z g R t G 2 Y 4 7 U l F c R x y 2 g 0 n t 7 n f f a J K s 0 Q 8 m K m k Q Y x H g k W M Y G M l 3 4 + x G Y d R 9 j g b s E G 1 5 t b d O d A q 8 Q p S g w K t Q f X L H y Y k j a k w h G O t + 5 4 r T Z B h Z R j h d F b x U 0 0 l J h M 8 o n 1 L B Y 6 p D r J 5 5 h k 6 s 8 o Q R Y m",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Si",
                "sec_num": null
            },
            {
                "text": "y T x g 0 V 3 9 v Z D j W e h q H d j L P q J e 9 X P z P 6 6 c m u g 4 y ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Si",
                "sec_num": null
            },
            {
                "text": "J m R q q C C L Q 1 H K k U l Q X g A a M k W J 4 V N L M F H M Z k V k j B U m x t Z U s S V",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Si",
                "sec_num": null
            },
            {
                "text": "Zi < l a t e x i t s h a 1 _ b a s e 6 4 = \" Z Z M r c J i 3 8 o / U d N",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "CU-Enhanced CVAE CU-Embedding",
                "sec_num": null
            },
            {
                "text": "V P M 6 8 y A W u R e E E = \" > A A A B 8 3 i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s x U Q Z d F N y 4 r 2 A d 2 h p J J M 2 1 o J h O S j F C G / o Y b F 4 q 4 9 W f c + T d m 2 l l o 6 4 H A 4 Z x 7 u S c n l J x p 4 7 r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 5 1 d J I q Q t s k 4 Y n q h V h T z g R t G 2 Y 4 7 U l F c R x y 2 g 0 n t 7 n f f a J K s 0 Q 8 m K m k Q Y x H g k W M Y G M l 3 4 + x G Y d R 9 j g b s E G 1 5 t b d O d A q 8 Q p S g w K t Q f X L H y Y k j a k w h G O t + 5 4 r T Z B h Z R j h d F b x U 0 0 l J h M 8 o n 1 L B Y 6 p D r J 5 5 h k 6 s 8 o Q R Y m y T x g 0 V 3 9 v Z D j W e h q H d j L P q J e 9 X P z P 6 6 c m u g 4 y J m R q q C C L Q 1 H K k U l Q X g A a M k W J 4 V N L M F H M Z k V k j B U m x t Z U s S V 4 y 1 9 e J Z 1 G 3 b u o N + 4 v a 8 2 b o o 4 y n M A p n I M H V 9 C E O 2 h B G w h I e I Z X e H N S 5 8 V 5 d z 4 W o y W n 2 D m G P 3 A + f w B S v J H e < / l a t e x i t > Conv 1D Conv 1D",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "CU-Enhanced CVAE CU-Embedding",
                "sec_num": null
            },
            {
                "text": "Multi-Head Attention \u21e0 N (\u00b5p, p)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "CU-Enhanced CVAE CU-Embedding",
                "sec_num": null
            },
            {
                "text": "< l a t e x i t s h a 1 _ b a s e 6 4 = \" C V 0",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "CU-Enhanced CVAE CU-Embedding",
                "sec_num": null
            },
            {
                "text": "x J Z / + v 1 s B H 5 g P K O h M 4 E p a t a w = \" > A A A C C n i c b V D L S s N A F J 3 U V 6 2 v q E s 3 o 0 W o I C W p g i 6 L b l x J B f u A J o T J d N o O n U m G m Y l Q Q t d u / B U 3 L h R x 6 x e 4 8 2 + c t F l o 6 4 E L h 3 P u 5 d 5 7 Q s G o 0 o 7 z b R W W l l d W 1 4 r r p Y 3 N r e 0 d e 3 e v p e J E Y t L E M Y t l J 0 S K M B q R p q a a k Y 6 Q B P G Q k X Y 4 u s 7 8 9 g O R i s b R v R 4 L 4 n M 0 i G i f Y q S N F N i H n q I c e h z p I U Y s v Z 1 U P J 4 E 4 h Q a f c B R I E 4 C u + x U n S n g I n F z U g Y 5 G o H 9 5 f V i n H A S a c y Q U l 3 X E d p P k d Q U M z I p e Y k i A u E R G p C u o R H i R P n p 9 J U J P D Z K D / Z j a S r S c K r + n k g R V 2 r M Q 9 O Z 3 a z m v U z 8 z + s m u n / p p z Q S i S Y R n i 3 q J w z q G G a 5 w B 6 V B G s 2 N g R h S c 2 t E A + R R F i b 9 E o m B H f + 5 U X S q l X d s 2 r t 7 r x c v 8 r j K I I D c A Q q w A U X o A 5 u Q A M 0 A Q a P 4",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "CU-Enhanced CVAE CU-Embedding",
                "sec_num": null
            },
            {
                "text": "B m 8 g j f r y X q x 3 q 2 P W W v B y m f 2 w R 9 Y n z 9 + t J o g < / l a t e x i t > z p < l a t e x i t s h a 1 _ b a s e 6 4 = \" l F c M 9 z k p 5 x h R z 5 7 p X",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "CU-Enhanced CVAE CU-Embedding",
                "sec_num": null
            },
            {
                "text": "U i C P G F W j p Q = \" > A A A B 9 H i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e x G Q Y 9 B L x 4 j m A c k S 5 i d z C Z D Z h / O 9 A b i k u / w 4 k E R r 3 6 M N / / G 2 W Q P m l g w U F R 1 0 z X l x V J o t O 1 v q 7 C 2 v r G 5 V d w u 7 e z u 7 R + U D 4 9 a O k o U 4 0 0 W y U h 1 P K q 5 F C F v o k D J O 7 H i N P A k b 3 v j 2 8 x v T 7 j S I g o f c B p z N 6 D D U P i C U T S S 2 w s o j g S m T 7 N + T P r l i l 2 1 5 y C r x M l J B X I 0 + u W v 3 i B i S c B D Z J J q 3 X X s G N 2 U K h R M 8 l m p l 2 g e U z a m Q 9 4 1 N K Q B 1 2 4 6 D z 0 j Z 0 Y Z E D 9 S 5 o V I 5 u r v j Z Q G W k 8 D z 0 x m I f W y l 4 n / e d 0 E / W s 3 F W G c I A / Z 4 p C f S I I R y R o g A 6 E 4 Q z k 1 h D I l T F b C R l R R h q a n k i n B W f 7 y K m n V q s 5 F t X Z / W a n f 5 H U U 4 Q R O 4 R w c u I I 6 3 E E D m s D g E Z 7 h F d 6 s i f V i v V s f i 9 G C l e 8 c w x 9 Y n z 8 J K Z J E < / l a t e x i t > Utterance-Specific Prior \u2026 \u2026 \u2026 Leon came back.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "CU-Enhanced CVAE CU-Embedding",
                "sec_num": null
            },
            {
                "text": "Mia said to Leon, did not stop her work.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "CU-Enhanced CVAE CU-Embedding",
                "sec_num": null
            },
            {
                "text": "< l a t e x i t s h a 1 _ b a s e 6 4 = \" u v e q 5 1 X s ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "CU-Enhanced CVAE CU-Embedding",
                "sec_num": null
            },
            {
                "text": "k e Z / B m B b y b / D E z k G 8 y U = \" > A A A B 7 X i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e x G Q Y 9 B L x 4 j m A c k S 5 i d z C Z j 5 r H M z A p h y T 9 4 8 a C I V / / H m 3 / j J N m D J h Y 0 F F X d d H d F C W f G + v 6 3 V 1 h b 3 9 j c K m 6 X d n b 3 9 g / K h 0 c t o 1 J N a J M o r n Q n w o Z y J m n T M s t p J 9 E U i 4 j T d j S + n f n t J 6 o N U / L B T h I a C j y U L G Y E W y e 1 e o Y N B e 6 X K 3 7 V n w O t k i A n F c j R 6 J e / e g N F U k G l J R w b 0 w 3 8 x I Y Z 1 p Y R T q e l X m p o g s k Y D 2 n X U Y k F N W E 2 v 3 a K z p w y Q L H S r q R F c / X 3 R I a F M R M R u U 6 B 7 c g s e z P x P 6 + b 2 v g 6 z J h M U k s l W S y K U 4 6 s Q r P X 0 Y B p S i y f O I K J Z u 5 W R E Z Y Y 2 J d Q C U X Q r D 8 8 i p p 1 a r B R b V 2 f 1 m p 3 + R x F O E E T u E c A r i C O t x B A 5 p A 4 B G e 4 R",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "CU-Enhanced CVAE CU-Embedding",
                "sec_num": null
            },
            {
                "text": "p M G L K W R a 4 N w 0 c U = \" > A A A B 6 n i c b V B N S w M x E J 3 U r 1 q / q h 6 9 B I v g q e x W Q Y 9 F L x 4 r 2 g 9 o l 5 J N s 2 1 o k l 2 S r F C W / g Q v H h T x 6 i / y 5 r 8 x b f e g r Q 8 G H u / N M D M v T A Q 3 1 v O + U W F t f W N z q 7 h d 2 t n d 2 z 8 o H x 6 1 T J x q y p o 0 F r H u h M Q w w R V r W m 4 F 6 y S a E R k K 1 g 7 H t z O / / c S 0 4 b F 6 t J O E B Z I M F Y 8 4 J d Z J D z 2 Z 9 s s V r + r N g V e J n 5 M K 5 G j 0 y 1 + 9 Q U x T y Z S l g h j T 9 b 3 E B h n R l l P B p q V e a l h C 6 J g M W d d R R S Q z Q T Y / d Y r P n D L A U a x d K Y v n 6 u + J j E h j J j J 0 n Z L Y k V n 2 Z u J / X j e 1 0 X W Q c Z W k l i m 6 W B S l A t s Y z / 7 G A 6 4 Z t W L i C K G a u 1 s x H R F N q H X p l F w I / v L L q 6 R V q / o X 1 d r 9 Z a V + k 8 d R h B M 4 h X P w 4 Q r q c A c N a A K F I T z D K 7 w h g V 7 Q O / p Y t B Z Q P n M M f 4 A + f",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "CU-Enhanced CVAE CU-Embedding",
                "sec_num": null
            },
            {
                "text": "w B e r I 3 a < / l a t e x i t >",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "CU-Enhanced CVAE CU-Embedding",
                "sec_num": null
            },
            {
                "text": "Figure 1 : The CUC-VAE TTS system architecture consists of the cross-utterance embedding (CU-embedding) and the cross-utterance enhanced (CU-enhanced) CVAE, which are integrated into and jointly optimised with the FastSpeech 2 system. latent vector and is approximated by Monte Carlo sampling of z according to the posterior distribution. The reparameterization trick is applied to make the sampling differentiable. The second term encourages the posterior distribution to approach the prior distribution which is sampled from during inference, and \u03b2 weighs this term's contribution.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Transformer",
                "sec_num": null
            },
            {
                "text": "A large body of previous work on VAE-based TTS used VAEs to capture and disentangle data variations in different aspects in the latent space. Works by Akuzawa et al. (2018) leveraged VAE to model the speaking style of an utterance. Meanwhile, Hsu et al. (2019a,b) explored the disentanglement between prosody variation and speaker information using VAE together with adversarial training. Recently, fine-grained VAE (Sun et al., 2020a,b ) was adopted to model prosody in the latent space for each phoneme or word. Moreover, vector-quantised VAE was also applied to discrete duration modelling by Yasuda et al. (2021) .",
                "cite_spans": [
                    {
                        "start": 151,
                        "end": 172,
                        "text": "Akuzawa et al. (2018)",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 243,
                        "end": 263,
                        "text": "Hsu et al. (2019a,b)",
                        "ref_id": null
                    },
                    {
                        "start": 416,
                        "end": 436,
                        "text": "(Sun et al., 2020a,b",
                        "ref_id": null
                    },
                    {
                        "start": 596,
                        "end": 616,
                        "text": "Yasuda et al. (2021)",
                        "ref_id": "BIBREF33"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Transformer",
                "sec_num": null
            },
            {
                "text": "CVAE is a variant of VAE when the data generation is conditioned on some other information y. In CVAE, both prior and posterior distributions are conditioned on additional variables, and the data likelihood calculation is modified as shown below:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Transformer",
                "sec_num": null
            },
            {
                "text": "p \u03b8 (x | y) = p \u03b8 (x | z, y)p \u03d5 (z | y)dz. (3)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Transformer",
                "sec_num": null
            },
            {
                "text": "Similar to VAE, this intractable calculation can be converted to the ELBO form as",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Transformer",
                "sec_num": null
            },
            {
                "text": "L ELBO (x | y) = E q \u03d5 (z|x,y) [log p \u03b8 (x | z, y)] -\u03b2D KL (q \u03d5 (z | x, y)\u2225p(z | y)) .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Transformer",
                "sec_num": null
            },
            {
                "text": "To model the conditional prior, a density network is usually used to predict the mean and variance based on the conditional input y.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Transformer",
                "sec_num": null
            },
            {
                "text": "encodings in place of a standard embedding. As shown in Fig. 1 , the first L utterances and the last L utterances surrounding the current one, u i , are used as text input in addition to the current utterance and speaker information. Same as the standard embedding, an extra G2P conversion is first performed to convert the current utterance into phonemes",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 61,
                        "end": 62,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Transformer",
                "sec_num": null
            },
            {
                "text": "P i = [p 1 , p 2 , \u2022 \u2022 \u2022 , p T ],",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Transformer",
                "sec_num": null
            },
            {
                "text": "where T is the number of phonemes. Then, a Transformer encoder is used to encode the phoneme sequence into a sequence of phoneme encodings. Besides, speaker information is encoded into a speaker embedding s i which is directly added to each phoneme encoding to form the mixture encodings F i of the phoneme sequence.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Transformer",
                "sec_num": null
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "F i = [f i (p 1 ), f i (p 2 ), \u2022 \u2022 \u2022 , f i (p T )],",
                        "eq_num": "(4)"
                    }
                ],
                "section": "Transformer",
                "sec_num": null
            },
            {
                "text": "where f represents resultant vector from the addition of each phoneme encoding and speaker embedding.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Transformer",
                "sec_num": null
            },
            {
                "text": "To supplement the text information from the current utterance to generate natural and expressive audio, cross-utterance BERT embeddings together with a multi-head attention layer are used to capture contextual information. To begin with, 2L cross-utterance pairs, denoted as C i , are derived from 2L + 1 neighboring utterances",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Transformer",
                "sec_num": null
            },
            {
                "text": "[u i-L , \u2022 \u2022 \u2022 , u i , \u2022 \u2022 \u2022 , u i+L ] as: Ci = [c(ui-L, ui-L+1), \u2022 \u2022 \u2022 , c(ui-1, ui), \u2022 \u2022 \u2022 , c(ui+L-1, ui+L)],",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Transformer",
                "sec_num": null
            },
            {
                "text": "(5) where c(u k , u k+1 ) = {[CLS], u k , [SEP], u k+1 }, which adds a special token [CLS] at the beginning of each pair and inserts another special token [SEP] at the boundary of each sentence to keep track of BERT. Then, the 2L cross-utterance pairs are fed to the BERT to capture cross-utterance information, which yields 2L BERT embedding vectors by taking the output vector at the position of the [CLS] token and projecting each to a 768-dim vector for each cross-utterance pair, as shown below:",
                "cite_spans": [
                    {
                        "start": 155,
                        "end": 160,
                        "text": "[SEP]",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Transformer",
                "sec_num": null
            },
            {
                "text": "B i = [b -L , b -L+1 , \u2022 \u2022 \u2022 , b L-1 ],",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Transformer",
                "sec_num": null
            },
            {
                "text": "where each vector b k in B i represents the BERT embedding of the cross-utterance pair c(u k , u k+1 ). Next, to extract CU-embedding vectors for each phoneme specifically, a multi-head attention layer is added to combine the 2L BERT embeddings into one vector as shown in Eq. ( 6).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Transformer",
                "sec_num": null
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "G i = MHA(F i W Q , B i W K , B i W V ),",
                        "eq_num": "(6)"
                    }
                ],
                "section": "Transformer",
                "sec_num": null
            },
            {
                "text": "where MHA(\u2022) denotes the multi-head attention layer, W Q , W K and W V are linear projection matrices, and F i denotes the sequence of mixture encodings for the current utterance which acts as the query in the attention mechanism. For simplicity, we denote Eq. ( 6) as",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Transformer",
                "sec_num": null
            },
            {
                "text": "G i = [g 1 , g 2 , \u2022 \u2022 \u2022 , g T ]",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Transformer",
                "sec_num": null
            },
            {
                "text": "from the multi-head attention being of length T and each of them is then concatenated with its corresponding mixture encoding. The concatenated vectors are projected by another linear layer to form the final output H i of the CU-embedding,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Transformer",
                "sec_num": null
            },
            {
                "text": "H i = [h 1 , h 2 , \u2022 \u2022 \u2022 , h T ]",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Transformer",
                "sec_num": null
            },
            {
                "text": "of the current utterance, as shown in Eq. ( 7).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Transformer",
                "sec_num": null
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "h t = [g t , f (p t )]W ,",
                        "eq_num": "(7)"
                    }
                ],
                "section": "Transformer",
                "sec_num": null
            },
            {
                "text": "where W is a linear projection matrix. Moreover, an additional duration predictor takes H i as inputs and predicts the duration D i of each phoneme.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Transformer",
                "sec_num": null
            },
            {
                "text": "In addition to the CU-embedding, a CU-enhanced CVAE is proposed to conquer the lack of prosody variation of FastSpeech 2 and the inconsistency between the standard Gaussian prior distribution sampled by the VAE based TTS system and the true prior distribution of speech. Specifically, the CU-enhanced CVAE consists of an encoder module and a decoder module, as shown in Fig. 1 . The utterance-specific prior in the encoder aims to learn the prior distribution z p from the CU-embedding output H and predicts duration D. For convenience, the subscript i is omitted in this subsection. Furthermore, the posterior module in the encoder takes as input reference mel-spectrogram x, then model the approximate posterior z conditioned on utterance-specific conditional prior z p . Sampling is done from the estimated prior by the utterancespecific prior module and is reparameterized as:",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 375,
                        "end": 376,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Cross-Utterance Enhanced CVAE",
                "sec_num": "3.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "z = \u00b5 \u2295 \u03c3 \u2297 z p ,",
                        "eq_num": "(8)"
                    }
                ],
                "section": "Cross-Utterance Enhanced CVAE",
                "sec_num": "3.2"
            },
            {
                "text": "where \u00b5 and \u03c3 are estimated from conditional posterior module to approximate posterior distribution N (\u00b5, \u03c3), z p is sampled from the learned utterance-specific prior, and \u2295, \u2297 are elementwise addition and multiplication operation. Furthermore, the utterance-specific conditional prior module is conducted to learn utterance-specific prior with CU-embedding output H and D. The reparameterization is as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Cross-Utterance Enhanced CVAE",
                "sec_num": "3.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "z p = \u00b5 p \u2295 \u03c3 p \u2297 \u03f5,",
                        "eq_num": "(9)"
                    }
                ],
                "section": "Cross-Utterance Enhanced CVAE",
                "sec_num": "3.2"
            },
            {
                "text": "where \u00b5 p , \u03c3 p are learned from the utterancespecific prior module, and \u03f5 is sampled from the standard Gaussian N (0, 1). By substituting Eq. ( 9) into Eq. ( 8), the following equation can be derived for the total sampling process:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Cross-Utterance Enhanced CVAE",
                "sec_num": "3.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "z = \u00b5 \u2295 \u03c3 \u2297 \u00b5 p \u2295 \u03c3 \u2297 \u03c3 p \u2297 \u03f5.",
                        "eq_num": "(10)"
                    }
                ],
                "section": "Cross-Utterance Enhanced CVAE",
                "sec_num": "3.2"
            },
            {
                "text": "During inference, sampling is done from the learned utterance-specific conditional prior distribution N (\u00b5 p , \u03c3 p ) from CU-embedding instead of a standard Gaussian distribution N (0, 1). For simplicity, we can formulate the data likelihood calculation as follows, where the intermediate variable utterance-specific prior z p from D, H to obtain z is omitted:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Cross-Utterance Enhanced CVAE",
                "sec_num": "3.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "p \u03b8 (x | H, D) = p \u03b8 (x | z, H, D)p \u03d5 (z | H, D)dz,",
                        "eq_num": "(11)"
                    }
                ],
                "section": "Cross-Utterance Enhanced CVAE",
                "sec_num": "3.2"
            },
            {
                "text": "In Eq. ( 11), \u03d5, \u03b8 are the encoder and decoder module parameters of the CUC-VAE TTS system.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Cross-Utterance Enhanced CVAE",
                "sec_num": "3.2"
            },
            {
                "text": "Moreover, the decoder in CU-enhanced CVAE is adapted from FastSpeech 2. An additional projection layer is firstly added to project z to a high dimensional space so that z could be added to H. Next, a length regulator expands the length of input according to the predicted duration D of each phoneme. The rest of Decoder is same as the Decoder module in FastSpeech 2 to convert the hidden sequence into an mel-spectrogram sequence via parallelized calculation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Cross-Utterance Enhanced CVAE",
                "sec_num": "3.2"
            },
            {
                "text": "Therefore, the ELBO objective of the CUC-VAE can be expressed as,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Cross-Utterance Enhanced CVAE",
                "sec_num": "3.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "L(x | H, D) = E q \u03d5 (z|D,H) [log p \u03b8 (x | z, D, H)] -\u03b2 1 T n=1 D KL q \u03d51 z n | z n p , x \u2225q \u03d52 z n p | D, H -\u03b2 2 T n=1 D KL q \u03d52 z n p | D, H \u2225p(z n p ) ,",
                        "eq_num": "(12)"
                    }
                ],
                "section": "Cross-Utterance Enhanced CVAE",
                "sec_num": "3.2"
            },
            {
                "text": "where \u03d5 1 , \u03d5 2 are two parts of CUC-VAE encoder \u03d5 to obtain z from z p , x and z p from D, H respectively, \u03b2 1 , \u03b2 2 are two balance constants, p(z n p ) is chosen to be standard Gaussian N (0, 1). Meanwhile, z n and z n p correspond to the latent representation for the n-th phoneme, and T is the length of the phoneme sequence.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Cross-Utterance Enhanced CVAE",
                "sec_num": "3.2"
            },
            {
                "text": "To evaluate the proposed CUC-VAE TTS system, a series of experiments were conducted on a single speaker dataset and a multi-speaker dataset. For the single speaker setting, the LJ-Speech read English data (Ito and Johnson, 2017) was used which consists of 13,100 audio clips with a total duration of approximately 24 hours. A female native English speaker read all the audio clips, and the scripts were selected from 7 non-fiction books. For the multispeaker setting, the train-clean-100 and train-clean-360 subsets of the LibriTTS English audiobook data (Zen et al., 2019) were used. These subsets used here consist of 1151 speakers (553 female speakers and 598 male speakers) and about 245 hours of audio. All audio clips were re-sampled at 22.05 kHz in experiments for consistency.",
                "cite_spans": [
                    {
                        "start": 205,
                        "end": 228,
                        "text": "(Ito and Johnson, 2017)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 555,
                        "end": 573,
                        "text": "(Zen et al., 2019)",
                        "ref_id": "BIBREF34"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dataset",
                "sec_num": "4.1"
            },
            {
                "text": "The proposed CU-embedding in our system learns the cross-utterance representation from surrounding utterances. However, unlike LJ-Speech, transcripts of LibriTTS utterances are not arranged as continuous chunks of text in their corresponding book. Therefore, transcripts of the LibriTTS dataset were pre-processed to find the location of each utterance in the book, so that the first L and last L utterances of the current one can be efficiently obtained during training and inference. The pre-processed scripts and our code are available1 .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dataset",
                "sec_num": "4.1"
            },
            {
                "text": "The proposed CUC-VAE TTS system was based on the framework of FastSpeech 2. The CUembedding utilised a Transformer to learn the current utterance representation, where the dimension of phoneme embeddings and the size of the selfattention were set to 256. To explicitly extract speaker information, 256-dim speaker embeddings were also added to the Transformer output. Meanwhile, the pre-trained BERT model to extract crossutterance information had 12 Transformer blocks and 12-head attention layers with 110 million parameters. The size of the derived embeddings of each cross-utterance pair was 768-dim. Note that the BERT model and corresponding embeddings were fixed when training the TTS system. Network in CU-enhanced CVAE consisted of four 1Dconvolutional (1D-Conv) layers with kernel sizes of 1 to predict the mean and variance of 2-dim latent features. Then a linear layer was added to transform the sampled latent feature to a 256-dim vector. The duration predictor which consisted of two convolutional blocks and an extra linear layer to predict the duration of each phoneme for the length regulator in FastSpeech 2 was adapted to take in CU-embedding outputs. Each convolutional block was comprised of a 1D-Conv network with ReLU activation followed by a layer normalization and dropout layer. The Decoder adopted four feed-forward Transformer blocks to convert hidden sequences into 80-dim mel-spectrogram sequence, similar to FastSpeech 2. Finally, HifiGAN (Kong et al., 2020) was used to synthesize waveform from the predicted mel-spectrogram.",
                "cite_spans": [
                    {
                        "start": 1470,
                        "end": 1489,
                        "text": "(Kong et al., 2020)",
                        "ref_id": "BIBREF17"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "System Specification",
                "sec_num": "4.2"
            },
            {
                "text": "In order to evaluate the performance of our proposed component, both subjective and objective tests were performed. First of all, a subjective listening test was performed over 11 synthesized audios with 23 volunteers asked to rate the naturalness of speech samples on a 5-scale mean opinion score (MOS) evaluation. The MOS results were reported with 95% confidence intervals. In addition, an AB test was conducted to compare the CU-enhanced CVAE with utterance-specific prior and normal CVAE with standard Gaussian prior. 23 volunteers were asked to choose the preference audio generated by different models in the AB test.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation Metrics",
                "sec_num": "4.3"
            },
            {
                "text": "For the objective evaluation, F 0 frame error (FFE) (Chu and Alwan, 2009) and mel-cepstral distortion (MCD) (Kubichek, 1993) were used to measure the reconstruction performance of different VAEs. FFE combined the Gross Pitch Error (GPE) and the Voicing Decision Error (VDE) and was used to evaluate the reconstruction of the F 0 track. MCD evaluated the timbral distortion, which was computed from the first 13 MFCCs in our experiments. Moreover, word error rates (WER) from an ASR model trained on the real speech from the Lib-riTTS training set were reported. Complementary to naturalness, the WER metric showed both the intelligibility and the degree of inconsistency between synthetic speech and real speech. The ASR system used in this paper was an attention-based encoder-decoder model trained on Librispeech 960hour data, with a WER of 4.4% on the test-clean set. Finally, the diversity of samples was evaluated by measuring the standard deviation of two prosody attributes of each phoneme: relative energy (E) and fundamental frequency (F 0 ), similar to Sun et al. (2020b) . Relative energy was calculated as the ratio of the average signal amplitude within a phoneme to the average amplitude of the entire sen-tence, and fundamental frequency was measured using a pitch tracker. In this paper, the average standard deviation of E and F 0 of three phonemes in randomly selected 11 utterances was reported to evaluate the diversity of generated speech.",
                "cite_spans": [
                    {
                        "start": 52,
                        "end": 73,
                        "text": "(Chu and Alwan, 2009)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 108,
                        "end": 124,
                        "text": "(Kubichek, 1993)",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 1063,
                        "end": 1081,
                        "text": "Sun et al. (2020b)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation Metrics",
                "sec_num": "4.3"
            },
            {
                "text": "This section presents the series of experiments for the proposed CUC-VAE TTS system. First, ablation studies were performed to progressively show the influence of different parts in the CUC-VAE TTS system based on MOS and WER. Next, the reconstruction performance of CUC-VAE was evaluated by FFE and MCD. Then, the naturalness and prosody diversity using CUC-VAE were compared to FastSpeech 2 and other VAE techniques. At last, a case study illustrated the prosody variations with different cross-utterance information as an example. The audio examples are available on the demo page2 .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "5"
            },
            {
                "text": "Ablation studies in this section were conducted on the LJ-Speech data based on the subjective test and WER. First, to investigate the effect of the different number of neighbouring utterances, CUC-VAE TTS systems built with L = 1, 3, 5 were evaluated using MOS scores, as shown in Table 1 .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 287,
                        "end": 288,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Ablation Studies",
                "sec_num": "5.1"
            },
            {
                "text": "Table 1 : The MOS results of CUC-VAE TTS systems on LJ-Speech dataset. MOS was reported with 95% confident intervals. \"L = 1\",\"L = 3\",\"L = 5\" represented the number of past and future utterances.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Ablation Studies",
                "sec_num": "5.1"
            },
            {
                "text": "Cross-utterance (2L)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Systems",
                "sec_num": null
            },
            {
                "text": "MOS CUC-VAE L = 1 2.93 \u00b1 0.12 CUC-VAE L = 3 3.72 \u00b1 0.09 CUC-VAE L = 5 3.95 \u00b1 0.07",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Systems",
                "sec_num": null
            },
            {
                "text": "The effect of the different number of neighbouring utterances on the naturalness of the synthesized speech can be observed by comparing MOS scores which is the higher the better. The CUC-VAE with L = 5 achieved highest score 3.95 compared to system with L = 1 and L = 3. Since only marginal MOS improvements were obtained using more than 5 neighbouring utterances, the rest of experiments were performed using L = 5.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Systems",
                "sec_num": null
            },
            {
                "text": "Then we investigated the influence of each part of CUC-VAE on performance. The baseline was our implementation of Fastspeech 2. For the system denoted as Baseline + fine-grained VAE which served as a stronger baseline, the pitch predictor and energy predictor of FastSpeech 2 were replaced with a fine-grained VAE with 2-dim latent space. Based on the fine-grained VAE baseline, the CVAE was added without the CU-embedding to the system, referred to as Baseline+CVAE to verify the function of CVAE on the system, which conditions on the current utterance. Again, MOS was compared among these systems as shown in Table 2 . As shown in Table 2 , MOS progressively increased when fine-grained VAE, CVAE, and CUembedding were added in consecutively. The proposed CUC-VAE TTS system achieved the highest MOS 3.95 compared to baselines. The results indicated that CUC-VAE module played a crucial role in generating more natural audio.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 618,
                        "end": 619,
                        "text": "2",
                        "ref_id": "TABREF0"
                    },
                    {
                        "start": 640,
                        "end": 641,
                        "text": "2",
                        "ref_id": "TABREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Systems",
                "sec_num": null
            },
            {
                "text": "To verify the importance of the utterancespecific prior to the synthesized audio, the same CUC-VAE system was used, and the only difference is whether to sample latent prosody features from the utterance-specific prior or from a standard Gaussian distribution. A subjective AB test was performed which required 23 volunteers to provide their preference between audios synthesized from the 2 approaches. Moreover, WER was also compared here to show the intelligibility of the synthesized audio. As shown in Table 3 , the preference rate of using the utterance-specific prior is 0.52 higher than its counterpart, and a 4.9% absolute WER reduction was found, which confirmed the importance of the utterance-specific prior in our CUC-VAE TTS system. extract one latent prosody feature vector for an utterance was added for more comprehensive comparison, and is referred to as the Global VAE. Table . 4 shows the reconstruction performance on the LJ-Speech dataset and LibriTTS dataset, respectively. Baseline had the highest value of FFE and MCD on the LJ-Speech dataset and LibriTTS dataset. The value of FFE and MCD decreased when the global VAE was added and was further reduced when the fine-grained VAE was added to the baseline. Our proposed CUC-VAE TTS system achieved the lowest FFE and MCD across the table on both the LJ-Speech and LibriTTS datasets. This indicated that richer prosody-related information entailed in both cross-utterance and conditional inputs was captured by CUC-VAE.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 512,
                        "end": 513,
                        "text": "3",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Systems",
                "sec_num": null
            },
            {
                "text": "Next, sample naturalness and intelligibility were measured using MOS and WER respectively on both LJ-Speech and LibriTTS datasets. Complementary to the naturalness, the diversity of generated speech from the conditional prior was evaluated by comparing the standard deviation of E and F 0 similar to (Sun et al., 2020b Although both F 0 and E of the CUC-VAE TTS system were lower than the baseline + fine-grained VAE, the proposed system achieved a clearly higher prosody diversity than the baseline and baseline + global VAE systems. The fine-grained VAE achieved the highest prosody variation as its latent prosody features were sampled from a standard Gaussian distribution, which lacks the constraint of language information from both the current and the neighbouring utterances. This caused extreme prosody variations to occur which impaired both the naturalness and the intelligibility of synthesized audios. As a result, the CUC-VAE TTS system was able to achieve high prosody diversity without hurting the naturalness of the generated speech. In fact, the adequate increase in prosody diversity improved the expressiveness of the synthesized audio, and hence increased the naturalness.",
                "cite_spans": [
                    {
                        "start": 300,
                        "end": 318,
                        "text": "(Sun et al., 2020b",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Sample Naturalness and Diversity",
                "sec_num": "5.3"
            },
            {
                "text": "The right part of Table . 5 showed the results on LibriTTS dataset. Similar to the LJ-Speech experiments, the CUC-VAE TTS system achieved the best naturalness measured by MOS, the best intelligibility measured by WER, and the secondhighest prosody diversity across the table. Overall, consistent improvements in both naturalness and prosody diversity were observed on both singlespeaker and multi-speaker datasets.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Sample Naturalness and Diversity",
                "sec_num": "5.3"
            },
            {
                "text": "To better illustrate how the utterance-specific prior influenced the naturalness of the synthesized speech under a given context, a case study was performed by synthesizing an example utterance, \"Mary asked the time\", with two different neighbouring utterances: \"Who asked the time? Mary asked the time.\" and \"Mary asked the time, and was told it was only five.\" Based on the linguistic knowledge, to answer the question in the first setting, an emphasis should be put on the word \"Mary\", while in the second setting, the focus of the sentence is \"asked the time\". The model trained on LJ-Speech dataset was used to synthesize the utterance and the results were shown in Fig. 2 . Fig. 2 showed the energy and pitch of the two utterance. Energy of the first word \"Mary\" in Fig. 2 (a) changed significantly (energy of \"Ma-\" was much higher than \"-ry\"), which reflected an emphasis on the word \"Mary\", whereas in Fig. 2(b ), energy of \"Mary\" had no obvious change, i.e., the word was not emphasized. On the other hand, the fundamental frequency of words \"asked\" and \"time\" stayed at a high level for a longer time in the second audio than the first one, reflecting another type of emphasis on those words which was also coherent with the given context. Therefore, the difference of energy and pitch between the two utterances demonstrated that the speech synthesized by our model is sufficiently contextualized.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 676,
                        "end": 677,
                        "text": "2",
                        "ref_id": "FIGREF4"
                    },
                    {
                        "start": 685,
                        "end": 686,
                        "text": "2",
                        "ref_id": "FIGREF4"
                    },
                    {
                        "start": 777,
                        "end": 778,
                        "text": "2",
                        "ref_id": "FIGREF4"
                    },
                    {
                        "start": 915,
                        "end": 918,
                        "text": "2(b",
                        "ref_id": "FIGREF4"
                    }
                ],
                "eq_spans": [],
                "section": "A Case Study",
                "sec_num": "5.4"
            },
            {
                "text": "In this paper, a non-autoregressive CUC-VAE TTS system was proposed to synthesize speech with better naturalness and more prosody diversity. CUC-VAE TTS system estimated the posterior distribution of latent prosody features for each phone based on cross-utterance information in addition to the acoustic features and speaker information. The generated audio was sampled from an utterancespecific prior distribution, approximated based on cross-utterance information. Experiments were conducted to evaluate the proposed CUC-VAE TTS system with metrics including MOS, preference rate, WER, and the standard deviation of prosody attributes. Experiment results showed that the proposed CUC-VAE TTS system improved both the naturalness and prosody diversity in the generated audio samples, which outperformed the baseline in all metrics with clear margins.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "6"
            },
            {
                "text": "https://github.com/NeuroWave-ai/CUCV AE-TTS",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "http://bit.ly/cuc-vae-tts-demo",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "The proposed CUC-VAE TTS system, which is adapted from FastSpeech 2 as shown in Fig. 1 , aims to synthesize speech with more expressive prosody. Fig. 1 describes the model architecture, which has two components: CU-embedding and CU-enhanced CVAE. The CUC-VAE TTS system takes as inputis the crossutterance set that includes the current utterance u i and the L utterances before and after u i . Each u represents the text content of an utterance. Note that s i is the speaker ID, and x i is the reference mel-spectrogram of the current utterance u i . In this section, the two main components of the CUC-VAE TTS system will be introduced in detail.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 85,
                        "end": 86,
                        "text": "1",
                        "ref_id": null
                    },
                    {
                        "start": 150,
                        "end": 151,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "CUC-VAE TTS System",
                "sec_num": "3"
            },
            {
                "text": "The CU-embedding encodes not only the phoneme sequence and speaker information but also crossutterance information into a sequence of mixture",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Cross-Utterance Embedding",
                "sec_num": "3.1"
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Expressive speech synthesis via modeling expressions with variational autoencoder",
                "authors": [
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Akuzawa",
                        "suffix": ""
                    },
                    {
                        "first": "Yusuke",
                        "middle": [],
                        "last": "Iwasawa",
                        "suffix": ""
                    },
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Matsuo",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "K. Akuzawa, Yusuke Iwasawa, and Y. Matsuo. 2018. Expressive speech synthesis via modeling expressions with variational autoencoder. ArXiv, abs/1804.02135.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Speech bert embedding for improving prosody in neural tts",
                "authors": [
                    {
                        "first": "Liping",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Yan",
                        "middle": [],
                        "last": "Deng",
                        "suffix": ""
                    },
                    {
                        "first": "Xi",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Soong",
                        "suffix": ""
                    },
                    {
                        "first": "Lei",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
                "volume": "",
                "issue": "",
                "pages": "6563--6567",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Liping Chen, Yan Deng, Xi Wang, F. Soong, and Lei He. 2021. Speech bert embedding for improving prosody in neural tts. ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 6563-6567.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Reducing f0 frame error of f0 tracking algorithms under noisy conditions with an unvoiced/voiced classification frontend",
                "authors": [
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Chu",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Alwan",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing",
                "volume": "",
                "issue": "",
                "pages": "3969--3972",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Wei Chu and A. Alwan. 2009. Reducing f0 frame er- ror of f0 tracking algorithms under noisy conditions with an unvoiced/voiced classification frontend. 2009 IEEE International Conference on Acoustics, Speech and Signal Processing, pages 3969-3972.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Controllable context-aware conversational speech synthesis",
                "authors": [
                    {
                        "first": "Jian",
                        "middle": [],
                        "last": "Cong",
                        "suffix": ""
                    },
                    {
                        "first": "Shan",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Na",
                        "middle": [],
                        "last": "Hu",
                        "suffix": ""
                    },
                    {
                        "first": "Guangzhi",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Lei",
                        "middle": [],
                        "last": "Xie",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Su",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jian Cong, Shan Yang, Na Hu, Guangzhi Li, Lei Xie, and Dan Su. 2021. Controllable context-aware con- versational speech synthesis. ArXiv, abs/2106.10828.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Conditional variational auto-encoder for text-driven expressive audiovisual speech synthesis",
                "authors": [
                    {
                        "first": "Sara",
                        "middle": [],
                        "last": "Dahmani",
                        "suffix": ""
                    },
                    {
                        "first": "Vincent",
                        "middle": [],
                        "last": "Colotte",
                        "suffix": ""
                    },
                    {
                        "first": "Val\u00e9rian",
                        "middle": [],
                        "last": "Girard",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Ouni",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sara Dahmani, Vincent Colotte, Val\u00e9rian Girard, and S. Ouni. 2019. Conditional variational auto-encoder for text-driven expressive audiovisual speech synthe- sis. In INTERSPEECH.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
                "authors": [
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Devlin",
                        "suffix": ""
                    },
                    {
                        "first": "Ming-Wei",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "Kenton",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Kristina",
                        "middle": [],
                        "last": "Toutanova",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "NAACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "J. Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirec- tional transformers for language understanding. In NAACL.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Parallel tacotron 2: A non-autoregressive neural tts model with differentiable duration modeling",
                "authors": [
                    {
                        "first": "Isaac",
                        "middle": [],
                        "last": "Elias",
                        "suffix": ""
                    },
                    {
                        "first": "H",
                        "middle": [],
                        "last": "Zen",
                        "suffix": ""
                    },
                    {
                        "first": "Jonathan",
                        "middle": [],
                        "last": "Shen",
                        "suffix": ""
                    },
                    {
                        "first": "Yu",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Jia",
                        "middle": [],
                        "last": "Ye",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Skerry-Ryan",
                        "suffix": ""
                    },
                    {
                        "first": "Yonghui",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Isaac Elias, H. Zen, Jonathan Shen, Yu Zhang, Jia Ye, R. Skerry-Ryan, and Yonghui Wu. 2021. Par- allel tacotron 2: A non-autoregressive neural tts model with differentiable duration modeling. ArXiv, abs/2103.14574.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Towards transfer learning for end-to-end speech synthesis from deep pre-trained language models",
                "authors": [
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Fang",
                        "suffix": ""
                    },
                    {
                        "first": "Yu-An",
                        "middle": [],
                        "last": "Chung",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Glass",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Wei Fang, Yu-An Chung, and J. Glass. 2019. To- wards transfer learning for end-to-end speech syn- thesis from deep pre-trained language models. ArXiv, abs/1906.07307.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Ryuichi Yamamoto, and Kentaro Tachibana. 2021. Phrase break prediction with bidirectional encoder representations in japanese text-to-speech synthesis",
                "authors": [
                    {
                        "first": "Kosuke",
                        "middle": [],
                        "last": "Futamata",
                        "suffix": ""
                    },
                    {
                        "first": "Byeong-Cheol",
                        "middle": [],
                        "last": "Park",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kosuke Futamata, Byeong-Cheol Park, Ryuichi Ya- mamoto, and Kentaro Tachibana. 2021. Phrase break prediction with bidirectional encoder represen- tations in japanese text-to-speech synthesis. ArXiv, abs/2104.12395.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Pretrained text embeddings for enhanced text-to-speech synthesis",
                "authors": [
                    {
                        "first": "Tomoki",
                        "middle": [],
                        "last": "Hayashi",
                        "suffix": ""
                    },
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Shinji Watanabe",
                        "suffix": ""
                    },
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Toda",
                        "suffix": ""
                    },
                    {
                        "first": "Shubham",
                        "middle": [],
                        "last": "Takeda",
                        "suffix": ""
                    },
                    {
                        "first": "Karen",
                        "middle": [],
                        "last": "Toshniwal",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Livescu",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "INTERSPEECH",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tomoki Hayashi, Shinji Watanabe, T. Toda, K. Takeda, Shubham Toshniwal, and Karen Livescu. 2019. Pre- trained text embeddings for enhanced text-to-speech synthesis. In INTERSPEECH.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "2019a. Hierarchical generative modeling for controllable speech synthesis",
                "authors": [
                    {
                        "first": "Wei-Ning",
                        "middle": [],
                        "last": "Hsu",
                        "suffix": ""
                    },
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Ron",
                        "middle": [
                            "J"
                        ],
                        "last": "Weiss",
                        "suffix": ""
                    },
                    {
                        "first": "H",
                        "middle": [],
                        "last": "Zen",
                        "suffix": ""
                    },
                    {
                        "first": "Yonghui",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Yuxuan",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Yuan",
                        "middle": [],
                        "last": "Cao",
                        "suffix": ""
                    },
                    {
                        "first": "Ye",
                        "middle": [],
                        "last": "Jia",
                        "suffix": ""
                    },
                    {
                        "first": "Z",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Jonathan",
                        "middle": [],
                        "last": "Shen",
                        "suffix": ""
                    },
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Nguyen",
                        "suffix": ""
                    },
                    {
                        "first": "Ruoming",
                        "middle": [],
                        "last": "Pang",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Wei-Ning Hsu, Y. Zhang, Ron J. Weiss, H. Zen, Yonghui Wu, Yuxuan Wang, Yuan Cao, Ye Jia, Z. Chen, Jonathan Shen, P. Nguyen, and Ruoming Pang. 2019a. Hierarchical generative modeling for controllable speech synthesis. ArXiv, abs/1810.07217.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Disentangling correlated speaker and noise for speech synthesis via data augmentation and adversarial factorization",
                "authors": [
                    {
                        "first": "Wei-Ning",
                        "middle": [],
                        "last": "Hsu",
                        "suffix": ""
                    },
                    {
                        "first": "Yu",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Ron",
                        "middle": [
                            "J"
                        ],
                        "last": "Weiss",
                        "suffix": ""
                    },
                    {
                        "first": "Yu-An",
                        "middle": [],
                        "last": "Chung",
                        "suffix": ""
                    },
                    {
                        "first": "Yuxuan",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Yonghui",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "James",
                        "middle": [
                            "R"
                        ],
                        "last": "Glass",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing",
                "volume": "",
                "issue": "",
                "pages": "5901--5905",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Wei-Ning Hsu, Yu Zhang, Ron J. Weiss, Yu-An Chung, Yuxuan Wang, Yonghui Wu, and James R. Glass. 2019b. Disentangling correlated speaker and noise for speech synthesis via data augmentation and ad- versarial factorization. ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5901-5905.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "The lj speech dataset",
                "authors": [
                    {
                        "first": "Keith",
                        "middle": [],
                        "last": "Ito",
                        "suffix": ""
                    },
                    {
                        "first": "Linda",
                        "middle": [],
                        "last": "Johnson",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Keith Ito and Linda Johnson. 2017. The lj speech dataset. https://keithito.com/LJ-Sp eech-Dataset/.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Png bert: Augmented bert on phonemes and graphemes for neural tts",
                "authors": [
                    {
                        "first": "Ye",
                        "middle": [],
                        "last": "Jia",
                        "suffix": ""
                    },
                    {
                        "first": "H",
                        "middle": [],
                        "last": "Zen",
                        "suffix": ""
                    },
                    {
                        "first": "Jonathan",
                        "middle": [],
                        "last": "Shen",
                        "suffix": ""
                    },
                    {
                        "first": "Yu",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Yonghui",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ye Jia, H. Zen, Jonathan Shen, Yu Zhang, and Yonghui Wu. 2021. Png bert: Augmented bert on phonemes and graphemes for neural tts. ArXiv, abs/2103.15060.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "A learned conditional prior for the vae acoustic space of a tts system",
                "authors": [
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Panagiota Karanasou",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Karlapati",
                        "suffix": ""
                    },
                    {
                        "first": "Arnaud",
                        "middle": [],
                        "last": "Moinet",
                        "suffix": ""
                    },
                    {
                        "first": "Ammar",
                        "middle": [],
                        "last": "Joly",
                        "suffix": ""
                    },
                    {
                        "first": "Simon",
                        "middle": [],
                        "last": "Abbas",
                        "suffix": ""
                    },
                    {
                        "first": "Jaime",
                        "middle": [],
                        "last": "Slangen",
                        "suffix": ""
                    },
                    {
                        "first": "Thomas",
                        "middle": [],
                        "last": "Lorenzo-Trueba",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Drugman",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Panagiota Karanasou, S. Karlapati, A. Moinet, Arnaud Joly, Ammar Abbas, Simon Slangen, Jaime Lorenzo- Trueba, and Thomas Drugman. 2021. A learned conditional prior for the vae acoustic space of a tts system. ArXiv, abs/2106.10229.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Improving the prosody of rnn-based english text-tospeech synthesis by incorporating a bert model",
                "authors": [
                    {
                        "first": "Tom",
                        "middle": [],
                        "last": "Kenter",
                        "suffix": ""
                    },
                    {
                        "first": "Manish",
                        "middle": [],
                        "last": "Sharma",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Clark",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tom Kenter, Manish Sharma, and R. Clark. 2020. Im- proving the prosody of rnn-based english text-to- speech synthesis by incorporating a bert model. In INTERSPEECH.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Autoencoding variational bayes",
                "authors": [
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Diederik",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Kingma",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Welling",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Diederik P. Kingma and M. Welling. 2014. Auto- encoding variational bayes. CoRR, abs/1312.6114.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis",
                "authors": [
                    {
                        "first": "Jungil",
                        "middle": [],
                        "last": "Kong",
                        "suffix": ""
                    },
                    {
                        "first": "Jaehyeon",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "Jaekyoung",
                        "middle": [],
                        "last": "Bae",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. 2020. Hifi-gan: Generative adversarial networks for ef- ficient and high fidelity speech synthesis. ArXiv, abs/2010.05646.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Mel-cepstral distance measure for objective speech quality assessment",
                "authors": [
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Kubichek",
                        "suffix": ""
                    }
                ],
                "year": 1993,
                "venue": "Proceedings of IEEE Pacific Rim Conference on Communications Computers and Signal Processing",
                "volume": "1",
                "issue": "",
                "pages": "125--128",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "R. Kubichek. 1993. Mel-cepstral distance measure for objective speech quality assessment. Proceedings of IEEE Pacific Rim Conference on Communications Computers and Signal Processing, 1:125-128 vol.1.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Fastpitch: Parallel text-tospeech with pitch prediction",
                "authors": [
                    {
                        "first": "Adrian",
                        "middle": [],
                        "last": "La",
                        "suffix": ""
                    },
                    {
                        "first": "'",
                        "middle": [],
                        "last": "Ncucki",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Adrian La'ncucki. 2021. Fastpitch: Parallel text-to- speech with pitch prediction. In ICASSP.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Robust and finegrained prosody control of end-to-end speech synthesis",
                "authors": [
                    {
                        "first": "Younggun",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Taesu",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
                "volume": "",
                "issue": "",
                "pages": "5911--5915",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Younggun Lee and Taesu Kim. 2019. Robust and fine- grained prosody control of end-to-end speech synthe- sis. ICASSP 2019 -2019 IEEE International Con- ference on Acoustics, Speech and Signal Processing (ICASSP), pages 5911-5915.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Parallel neural text-to-speech",
                "authors": [
                    {
                        "first": "Kainan",
                        "middle": [],
                        "last": "Peng",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Ping",
                        "suffix": ""
                    },
                    {
                        "first": "Z",
                        "middle": [],
                        "last": "Song",
                        "suffix": ""
                    },
                    {
                        "first": "Kexin",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kainan Peng, Wei Ping, Z. Song, and Kexin Zhao. 2019. Parallel neural text-to-speech. ArXiv, abs/1905.08459.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Fastspeech 2: Fast and high-quality end-to-end text to speech",
                "authors": [
                    {
                        "first": "Yi",
                        "middle": [],
                        "last": "Ren",
                        "suffix": ""
                    },
                    {
                        "first": "Chenxu",
                        "middle": [],
                        "last": "Hu",
                        "suffix": ""
                    },
                    {
                        "first": "Xu",
                        "middle": [],
                        "last": "Tan",
                        "suffix": ""
                    },
                    {
                        "first": "Tao",
                        "middle": [],
                        "last": "Qin",
                        "suffix": ""
                    },
                    {
                        "first": "Sheng",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Zhou",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Tie-Yan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yi Ren, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu. 2021. Fastspeech 2: Fast and high-quality end-to-end text to speech. ArXiv, abs/2006.04558.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Fastspeech: Fast, robust and controllable text to speech",
                "authors": [
                    {
                        "first": "Yi",
                        "middle": [],
                        "last": "Ren",
                        "suffix": ""
                    },
                    {
                        "first": "Yangjun",
                        "middle": [],
                        "last": "Ruan",
                        "suffix": ""
                    },
                    {
                        "first": "Xu",
                        "middle": [],
                        "last": "Tan",
                        "suffix": ""
                    },
                    {
                        "first": "Tao",
                        "middle": [],
                        "last": "Qin",
                        "suffix": ""
                    },
                    {
                        "first": "Sheng",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Zhou",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Tie-Yan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "NeurIPS",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yi Ren, Yangjun Ruan, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu. 2019. Fastspeech: Fast, robust and controllable text to speech. In NeurIPS.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Natural tts synthesis by conditioning wavenet on mel spectrogram predictions",
                "authors": [
                    {
                        "first": "Jonathan",
                        "middle": [],
                        "last": "Shen",
                        "suffix": ""
                    },
                    {
                        "first": "Ruoming",
                        "middle": [],
                        "last": "Pang",
                        "suffix": ""
                    },
                    {
                        "first": "Ron",
                        "middle": [
                            "J"
                        ],
                        "last": "Weiss",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Schuster",
                        "suffix": ""
                    },
                    {
                        "first": "Navdeep",
                        "middle": [],
                        "last": "Jaitly",
                        "suffix": ""
                    },
                    {
                        "first": "Zongheng",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Z",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Yu",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Yuxuan",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Skerry-Ryan",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Saurous",
                        "suffix": ""
                    },
                    {
                        "first": "Yannis",
                        "middle": [],
                        "last": "Agiomyrgiannakis",
                        "suffix": ""
                    },
                    {
                        "first": "Yonghui",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
                "volume": "",
                "issue": "",
                "pages": "4779--4783",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jonathan Shen, Ruoming Pang, Ron J. Weiss, M. Schuster, Navdeep Jaitly, Zongheng Yang, Z. Chen, Yu Zhang, Yuxuan Wang, R. Skerry-Ryan, R. Saurous, Yannis Agiomyrgiannakis, and Yonghui Wu. 2018. Natural tts synthesis by conditioning wavenet on mel spectrogram predictions. 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 4779-4783.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Learning structured output representation using deep conditional generative models",
                "authors": [
                    {
                        "first": "Kihyuk",
                        "middle": [],
                        "last": "Sohn",
                        "suffix": ""
                    },
                    {
                        "first": "Honglak",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Xinchen",
                        "middle": [],
                        "last": "Yan",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "NIPS",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kihyuk Sohn, Honglak Lee, and Xinchen Yan. 2015. Learning structured output representation using deep conditional generative models. In NIPS.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Predicting expressive speaking style from text in end-to-end speech synthesis",
                "authors": [
                    {
                        "first": "Daisy",
                        "middle": [],
                        "last": "Stanton",
                        "suffix": ""
                    },
                    {
                        "first": "Yuxuan",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Skerry-Ryan",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "IEEE Spoken Language Technology Workshop (SLT)",
                "volume": "",
                "issue": "",
                "pages": "595--602",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Daisy Stanton, Yuxuan Wang, and R. Skerry-Ryan. 2018. Predicting expressive speaking style from text in end-to-end speech synthesis. 2018 IEEE Spoken Language Technology Workshop (SLT), pages 595- 602.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Generating diverse and natural text-to-speech samples using a quantized fine-grained vae and autoregressive prosody prior",
                "authors": [
                    {
                        "first": "G",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Ron",
                        "middle": [
                            "J"
                        ],
                        "last": "Weiss",
                        "suffix": ""
                    },
                    {
                        "first": "H",
                        "middle": [],
                        "last": "Yuan Cao",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Zen",
                        "suffix": ""
                    },
                    {
                        "first": "B",
                        "middle": [],
                        "last": "Rosenberg",
                        "suffix": ""
                    },
                    {
                        "first": "Yonghui",
                        "middle": [],
                        "last": "Ramabhadran",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
                "volume": "",
                "issue": "",
                "pages": "6699--6703",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "G. Sun, Y. Zhang, Ron J. Weiss, Yuan Cao, H. Zen, A. Rosenberg, B. Ramabhadran, and Yonghui Wu. 2020a. Generating diverse and natural text-to-speech samples using a quantized fine-grained vae and au- toregressive prosody prior. ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 6699-6703.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Fully-hierarchical finegrained prosody modeling for interpretable speech synthesis",
                "authors": [
                    {
                        "first": "G",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Ron",
                        "middle": [
                            "J"
                        ],
                        "last": "Weiss",
                        "suffix": ""
                    },
                    {
                        "first": "Yuanbin",
                        "middle": [],
                        "last": "Cao",
                        "suffix": ""
                    },
                    {
                        "first": "H",
                        "middle": [],
                        "last": "Zen",
                        "suffix": ""
                    },
                    {
                        "first": "Yonghui",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
                "volume": "",
                "issue": "",
                "pages": "6264--6268",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "G. Sun, Y. Zhang, Ron J. Weiss, Yuanbin Cao, H. Zen, and Yonghui Wu. 2020b. Fully-hierarchical fine- grained prosody modeling for interpretable speech synthesis. ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Process- ing (ICASSP), pages 6264-6268.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Wavenet: A generative model for raw audio",
                "authors": [
                    {
                        "first": "A\u00e4ron",
                        "middle": [],
                        "last": "Van Den Oord",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Dieleman",
                        "suffix": ""
                    },
                    {
                        "first": "H",
                        "middle": [],
                        "last": "Zen",
                        "suffix": ""
                    },
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Simonyan",
                        "suffix": ""
                    },
                    {
                        "first": "Oriol",
                        "middle": [],
                        "last": "Vinyals",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Graves",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Kalchbrenner",
                        "suffix": ""
                    },
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Senior",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Kavukcuoglu",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "SSW",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "A\u00e4ron van den Oord, S. Dieleman, H. Zen, K. Simonyan, Oriol Vinyals, A. Graves, Nal Kalchbrenner, A. Se- nior, and K. Kavukcuoglu. 2016. Wavenet: A gener- ative model for raw audio. In SSW.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "Attention is all you need",
                "authors": [
                    {
                        "first": "Ashish",
                        "middle": [],
                        "last": "Vaswani",
                        "suffix": ""
                    },
                    {
                        "first": "Noam",
                        "middle": [
                            "M"
                        ],
                        "last": "Shazeer",
                        "suffix": ""
                    },
                    {
                        "first": "Niki",
                        "middle": [],
                        "last": "Parmar",
                        "suffix": ""
                    },
                    {
                        "first": "Jakob",
                        "middle": [],
                        "last": "Uszkoreit",
                        "suffix": ""
                    },
                    {
                        "first": "Llion",
                        "middle": [],
                        "last": "Jones",
                        "suffix": ""
                    },
                    {
                        "first": "Aidan",
                        "middle": [
                            "N"
                        ],
                        "last": "Gomez",
                        "suffix": ""
                    },
                    {
                        "first": "Lukasz",
                        "middle": [],
                        "last": "Kaiser",
                        "suffix": ""
                    },
                    {
                        "first": "Illia",
                        "middle": [],
                        "last": "Polosukhin",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. ArXiv, abs/1706.03762.",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "Tacotron: Towards end-to-end speech synthesis",
                "authors": [
                    {
                        "first": "Yuxuan",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Skerry-Ryan",
                        "suffix": ""
                    },
                    {
                        "first": "Daisy",
                        "middle": [],
                        "last": "Stanton",
                        "suffix": ""
                    },
                    {
                        "first": "Yonghui",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Ron",
                        "middle": [
                            "J"
                        ],
                        "last": "Weiss",
                        "suffix": ""
                    },
                    {
                        "first": "Navdeep",
                        "middle": [],
                        "last": "Jaitly",
                        "suffix": ""
                    },
                    {
                        "first": "Zongheng",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Xiao",
                        "suffix": ""
                    },
                    {
                        "first": "Z",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Samy",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    },
                    {
                        "first": "Quoc",
                        "middle": [
                            "V"
                        ],
                        "last": "Le",
                        "suffix": ""
                    },
                    {
                        "first": "Yannis",
                        "middle": [],
                        "last": "Agiomyrgiannakis",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Clark",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Saurous",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yuxuan Wang, R. Skerry-Ryan, Daisy Stanton, Yonghui Wu, Ron J. Weiss, Navdeep Jaitly, Zongheng Yang, Y. Xiao, Z. Chen, Samy Bengio, Quoc V. Le, Yannis Agiomyrgiannakis, R. Clark, and R. Saurous. 2017. Tacotron: Towards end-to-end speech synthesis. In INTERSPEECH.",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "Improving prosody modelling with cross-utterance bert embeddings for end-to-end speech synthesis",
                "authors": [
                    {
                        "first": "Guanghui",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Song",
                        "suffix": ""
                    },
                    {
                        "first": "Zhengchen",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaodong",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Bowen",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing",
                "volume": "",
                "issue": "",
                "pages": "6079--6083",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Guanghui Xu, Wei Song, Zhengchen Zhang, C. Zhang, Xiaodong He, and Bowen Zhou. 2021. Improving prosody modelling with cross-utterance bert embed- dings for end-to-end speech synthesis. ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 6079- 6083.",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "End-to-end text-to-speech using latent duration based on vq-vae",
                "authors": [
                    {
                        "first": "Yusuke",
                        "middle": [],
                        "last": "Yasuda",
                        "suffix": ""
                    },
                    {
                        "first": "Xin",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Yamagishi",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
                "volume": "",
                "issue": "",
                "pages": "5694--5698",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yusuke Yasuda, Xin Wang, and J. Yamagishi. 2021. End-to-end text-to-speech using latent duration based on vq-vae. ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Process- ing (ICASSP), pages 5694-5698.",
                "links": null
            },
            "BIBREF34": {
                "ref_id": "b34",
                "title": "Libritts: A corpus derived from librispeech for text-tospeech",
                "authors": [
                    {
                        "first": "H",
                        "middle": [],
                        "last": "Zen",
                        "suffix": ""
                    },
                    {
                        "first": "Viet-Trung",
                        "middle": [],
                        "last": "Dang",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Clark",
                        "suffix": ""
                    },
                    {
                        "first": "Yu",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Ron",
                        "middle": [
                            "J"
                        ],
                        "last": "Weiss",
                        "suffix": ""
                    },
                    {
                        "first": "Ye",
                        "middle": [],
                        "last": "Jia",
                        "suffix": ""
                    },
                    {
                        "first": "Z",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Yonghui",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "H. Zen, Viet-Trung Dang, R. Clark, Yu Zhang, Ron J. Weiss, Ye Jia, Z. Chen, and Yonghui Wu. 2019. Lib- ritts: A corpus derived from librispeech for text-to- speech. In INTERSPEECH.",
                "links": null
            },
            "BIBREF35": {
                "ref_id": "b35",
                "title": "Aligntts: Efficient feed-forward text-to-speech system without explicit alignment",
                "authors": [
                    {
                        "first": "Zhen",
                        "middle": [],
                        "last": "Zeng",
                        "suffix": ""
                    },
                    {
                        "first": "Jianzong",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Ning",
                        "middle": [],
                        "last": "Cheng",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing",
                "volume": "",
                "issue": "",
                "pages": "6714--6718",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zhen Zeng, Jianzong Wang, Ning Cheng, Tian Xia, and Jing Xiao. 2020. Aligntts: Efficient feed-forward text-to-speech system without explicit alignment. ICASSP 2020 -2020 IEEE International Confer- ence on Acoustics, Speech and Signal Processing (ICASSP), pages 6714-6718.",
                "links": null
            },
            "BIBREF36": {
                "ref_id": "b36",
                "title": "Dependency parsing based semantic representation learning with graph neural network for enhancing expressiveness of text-to-speech",
                "authors": [
                    {
                        "first": "Yixuan",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Song",
                        "suffix": ""
                    },
                    {
                        "first": "Jingbei",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Zhiyong",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "H",
                        "middle": [],
                        "last": "Meng",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yixuan Zhou, C. Song, Jingbei Li, Zhiyong Wu, and H. Meng. 2021. Dependency parsing based semantic representation learning with graph neural network for enhancing expressiveness of text-to-speech. ArXiv, abs/2104.06835.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "4 y 1 9 e J Z 1 G 3 b u o N + 4 v a 8 2 b o o 4 y n M A p n I M H V 9 C E O 2 h B G w h I e I Z X e H N S 5 8 V 5 d z 4 W o y W n 2 D m G P 3 A + f w B S v J H e < / l a t e x i t >",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "X e P O W 9 e O / e x 6 K 1 4 O U z x / A H 3 u c P n a 2 P J w = = < / l a t e x i t > \u00b5 < l a t e x i t s h a 1 _ b a s e 6 4 = \" Y U I m 7 T s c 5 b 5 P 2",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "FFE and MCD were used to measure the reconstruction performance of VAE systems. An utterance-level prosody modelling baseline which",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF3": {
                "num": null,
                "text": "(a) Who asked the time? Mary asked the time.(b) Mary asked the time, and was told it was only five.",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF4": {
                "num": null,
                "text": "Figure 2: Comparisons between the energy and pitch contour of same text \"Mary asked the time\" but different neighbouring utterances, generated by CUC-VAE TTS trained on LJ-Speech.",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table><tr><td>Systems</td><td>MOS</td></tr><tr><td>Ground Truth</td><td>4.31 \u00b1 0.06</td></tr><tr><td>Baseline</td><td>3.85 \u00b1 0.07</td></tr><tr><td colspan=\"2\">Baseline+Fine-grained VAE 3.55 \u00b1 0.08</td></tr><tr><td>Baseline+CVAE</td><td>3.64 \u00b1 0.08</td></tr><tr><td>CUC-VAE</td><td>3.95 \u00b1 0.07</td></tr></table>",
                "type_str": "table",
                "text": "The MOS results of TTS systems with different modules on LJ-Speech dataset. MOS was reported with 95% confident intervals. Baseline + fine-grained VAE added a fine-grained VAE to baseline. Baseline+CVAE represents a CVAE TTS system without CU-embedding.",
                "html": null,
                "num": null
            },
            "TABREF1": {
                "content": "<table><tr><td>System</td><td colspan=\"2\">utterance-specific prior RATE WER</td></tr><tr><td>CUC-VAE</td><td>0.24</td><td>14.8</td></tr><tr><td>CUC-VAE</td><td>0.76</td><td>9.9</td></tr></table>",
                "type_str": "table",
                "text": "The subjective listening preference rate between CUC-VAE with or without utterance-specific prior from the AB test. The CUC-VAE without utterance-specific prior was a simplified version of our proposed CUC-VAE where latent samples were drawn from a standard Gaussian distribution instead of utterance-specific prior. WER metric was also reported.",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table><tr><td>Systems</td><td>LJ-Speech</td><td>LibriTTS</td></tr><tr><td/><td colspan=\"2\">MCD FFE MCD FFE</td></tr><tr><td>Baseline</td><td colspan=\"2\">6.70 0.58 6.32 0.58</td></tr><tr><td>Baseline+Global VAE</td><td colspan=\"2\">6.50 0.41 6.27 0.45</td></tr><tr><td colspan=\"3\">Baseline+Fine-grained VAE 6.34 0.26 6.28 0.35</td></tr><tr><td>CUC-VAE</td><td colspan=\"2\">6.27 0.24 6.04 0.34</td></tr></table>",
                "type_str": "table",
                "text": "Reconstruction preformance on LJ-Speech and LibriTTS dataset. + Global VAE and + fine-grained VAE represent that the baseline is added the global VAE and the fine-grained VAE, respectively.",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table/>",
                "type_str": "table",
                "text": "). LJ-Speech experiments were shown in left part of Table. 5. Compared to the global VAE and finegrained VAE, the proposed CUC-VAE received the highest MOS and achieved the lowest WER.",
                "html": null,
                "num": null
            },
            "TABREF4": {
                "content": "<table><tr><td/><td/><td colspan=\"2\">LJ-Speech</td><td/><td/><td/><td>LibriTTS</td><td/></tr><tr><td/><td>MOS</td><td>WER</td><td colspan=\"2\">Prosody Std.</td><td>MOS</td><td>WER</td><td colspan=\"2\">Prosody Std.</td></tr><tr><td/><td/><td/><td>F 0</td><td>E</td><td/><td/><td>F 0</td><td>E</td></tr><tr><td>Ground Truth</td><td>4.31 \u00b1 0.06</td><td>8.8</td><td>-</td><td>-</td><td>4.10 \u00b1 0.07</td><td>5.0</td><td>-</td><td>-</td></tr><tr><td colspan=\"3\">Baseline 3.85 Baseline+Global VAE 3.82 \u00b1 0.07 10.4</td><td>1.46</td><td>0.0004</td><td colspan=\"2\">3.59 \u00b1 0.08 10.8</td><td>2.01</td><td>0.0054</td></tr><tr><td colspan=\"3\">Baseline+Fine-grained VAE 3.55 \u00b1 0.08 12.8</td><td>49.60</td><td>0.0670</td><td>3.43 \u00b1 0.08</td><td>5.6</td><td>63.64</td><td>0.0901</td></tr><tr><td>CUC-VAE</td><td>3.95 \u00b1 0.07</td><td>9.9</td><td>26.35</td><td>0.0184</td><td>3.63 \u00b1 0.08</td><td>5.5</td><td>30.28</td><td>0.0217</td></tr></table>",
                "type_str": "table",
                "text": "Sample naturalness and diversity results on LJ-Speech and LibriTTS datasets. Three metrics are reported for each dataset, namely MOS, WER, and Prosody Std. The Prosody Std. includes standard deviations of relative energy (E) and fundamental frequency (F 0 ) in Hertz within each phonene. \u00b1 0.07 10.8 1.86 \u00d7 10 -13 6.78 \u00d7 10 -7 3.53 \u00b1 0.08 6.0 2.13 \u00d7 10 -13 7.22 \u00d7 10 -7",
                "html": null,
                "num": null
            }
        }
    }
}