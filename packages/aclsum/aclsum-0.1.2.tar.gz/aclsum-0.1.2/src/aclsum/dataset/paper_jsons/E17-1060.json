{
    "paper_id": "E17-1060",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T11:56:50.526405Z"
    },
    "title": "Learning to generate one-sentence biographies from Wikidata",
    "authors": [
        {
            "first": "Andrew",
            "middle": [],
            "last": "Chisholm",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Sydney Sydney",
                "location": {
                    "country": "Australia"
                }
            },
            "email": "andy.chisholm.89@gmail.com"
        },
        {
            "first": "Will",
            "middle": [],
            "last": "Radford",
            "suffix": "",
            "affiliation": {},
            "email": "wradford@hugo.ai"
        },
        {
            "first": "Ben",
            "middle": [],
            "last": "Hachey",
            "suffix": "",
            "affiliation": {},
            "email": "bhachey@hugo.ai"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "We investigate the generation of onesentence Wikipedia biographies from facts derived from Wikidata slot-value pairs. We train a recurrent neural network sequence-to-sequence model with attention to select facts and generate textual summaries. Our model incorporates a novel secondary objective that helps ensure it generates sentences that contain the input facts. The model achieves a BLEU score of 41, improving significantly upon the vanilla sequence-to-sequence model and scoring roughly twice that of a simple template baseline. Human preference evaluation suggests the model is nearly as good as the Wikipedia reference. Manual analysis explores content selection, suggesting the model can trade the ability to infer knowledge against the risk of hallucinating incorrect information.",
    "pdf_parse": {
        "paper_id": "E17-1060",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "We investigate the generation of onesentence Wikipedia biographies from facts derived from Wikidata slot-value pairs. We train a recurrent neural network sequence-to-sequence model with attention to select facts and generate textual summaries. Our model incorporates a novel secondary objective that helps ensure it generates sentences that contain the input facts. The model achieves a BLEU score of 41, improving significantly upon the vanilla sequence-to-sequence model and scoring roughly twice that of a simple template baseline. Human preference evaluation suggests the model is nearly as good as the Wikipedia reference. Manual analysis explores content selection, suggesting the model can trade the ability to infer knowledge against the risk of hallucinating incorrect information.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Despite massive effort, Wikipedia and other collaborative knowledge bases (KBs) have coverage and quality problems. Popular topics are covered in great detail, but there is a long tail of specialist topics with little or no text. Other text can be incorrect, whether by accident or vandalism. We report on the task of generating textual summaries for people, mapping slot-value facts to onesentence encyclopaedic biographies. In addition to initialising stub articles with only structured data, the resulting model could be used to improve consistency and accuracy of existing articles. Figure 1 shows a Wikidata entry for Mathias Tuomi, with fact keys and values flattened into a sequence, and the first sentence from his Wikipedia article. Some values are in the text, others are missing TITLE mathias tuomi SEX OR GENDER male DATE OF BIRTH 1985-09-03 OCCUPATION squash player CITIZENSHIP finland (e.g. male) or expressed differently (e.g. dates).",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 594,
                        "end": 595,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "We treat this knowlege-to-text task like translation, using a recurrent neural network (RNN) sequence-to-sequence model (Sutskever et al., 2014) that learns to select and realise the most salient facts as text. This includes an attention mechanism to focus generation on specific facts, a shared vocabulary over input and output, and a multi-task autoencoding objective for the complementary extraction task. We create a reference dataset comprising more than 400,000 knowledgetext pairs, handling the 15 most frequent slots. We also describe a simple template baseline for comparison on BLEU and crowd-sourced human preference judgements over a heldout TEST set.",
                "cite_spans": [
                    {
                        "start": 120,
                        "end": 144,
                        "text": "(Sutskever et al., 2014)",
                        "ref_id": "BIBREF25"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Our model obtains a BLEU score of 41.0, compared to 33.1 without the autoencoder and 21.1 for the template baseline. In a crowdsourced preference evaluation, the model outperforms the baseline and is preferred 40% of the time to the Wikipedia reference. Manual analysis of content selection suggests that the model can infer knowledge but also makes mistakes, and that the autoencoding objective encourages the model to select more facts without increasing sentence length. The task formulation and models are a foundation for text completion and consistency in KBs.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "RNN sequence-to-sequence models (Sutskever et al., 2014) have driven various recent advances in natural language understanding. While initial work focused on problems that were sequences of the same units, such as translating a sequence of words from one language to another, other work been able to use these models by coercing different structures into sequences, e.g., flattening trees for parsing (Vinyals et al., 2015) , predicting span types and lengths over byte input (Gillick et al., 2016) or flattening logical forms for semantic parsing (Xiao et al., 2016) .",
                "cite_spans": [
                    {
                        "start": 32,
                        "end": 56,
                        "text": "(Sutskever et al., 2014)",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 401,
                        "end": 423,
                        "text": "(Vinyals et al., 2015)",
                        "ref_id": null
                    },
                    {
                        "start": 476,
                        "end": 498,
                        "text": "(Gillick et al., 2016)",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 548,
                        "end": 567,
                        "text": "(Xiao et al., 2016)",
                        "ref_id": "BIBREF29"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Background",
                "sec_num": "2"
            },
            {
                "text": "RNNs have also been used successfully in knowledge-to-text tasks for human-facing systems, e.g., generating conversational responses (Vinyals and Le, 2015) , abstractive summarisation (Rush et al., 2015) . Recurrent LSTM models have been used with some success to generate text that completely expresses a set of facts: restaurant recommendation text from dialogue acts (Wen et al., 2015) , weather reports from sensor data and sports commentary from on-field events (Mei et al., 2015) . Similarly, we learn an end-to-end model trained over key-value facts by flattening them into a sequence.",
                "cite_spans": [
                    {
                        "start": 133,
                        "end": 155,
                        "text": "(Vinyals and Le, 2015)",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 184,
                        "end": 203,
                        "text": "(Rush et al., 2015)",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 370,
                        "end": 388,
                        "text": "(Wen et al., 2015)",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 467,
                        "end": 485,
                        "text": "(Mei et al., 2015)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Background",
                "sec_num": "2"
            },
            {
                "text": "Choosing the salient and consistent set of facts to include in generated output is also difficult. Recent work explores unsupervised autoencoding objectives in sequence-to-sequence models, improving both text classification as a pretraining step (Dai and Le, 2015) and translation as a multitask objective (Luong et al., 2016) . Our work explores an autoencoding objective which selects content as it generates by constraining the text output sequence to be predictive of the input.",
                "cite_spans": [
                    {
                        "start": 246,
                        "end": 264,
                        "text": "(Dai and Le, 2015)",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 306,
                        "end": 326,
                        "text": "(Luong et al., 2016)",
                        "ref_id": "BIBREF18"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Background",
                "sec_num": "2"
            },
            {
                "text": "Biographic summarisation has been extensively researched and is often approached as a sequence of subtasks (Schiffman et al., 2001) . A version of the task was featured in the Document Understanding Conference in 2004 (Blair-Goldensohn et al., 2004) and other work learns policies for content selection without generating text (Duboue and McKeown, 2003; Zhang et al., 2012; Cheng et al., 2015) . While pipeline components can be individually useful, integrating selection and generation allows the model to exploit the interaction between them.",
                "cite_spans": [
                    {
                        "start": 107,
                        "end": 131,
                        "text": "(Schiffman et al., 2001)",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 199,
                        "end": 217,
                        "text": "Conference in 2004",
                        "ref_id": null
                    },
                    {
                        "start": 218,
                        "end": 249,
                        "text": "(Blair-Goldensohn et al., 2004)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 327,
                        "end": 353,
                        "text": "(Duboue and McKeown, 2003;",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 354,
                        "end": 373,
                        "text": "Zhang et al., 2012;",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 374,
                        "end": 393,
                        "text": "Cheng et al., 2015)",
                        "ref_id": "BIBREF5"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Background",
                "sec_num": "2"
            },
            {
                "text": "KBs have been used to investigate the interaction between structured facts and unstructured text. Generating textual templates that are filled by structured data is a common approach and has been used for conversational text (Han et al., 2015) and biographical text generation (Duma and Klein, 2013) . Wikipedia has also been a popular resource for studying biography, including sentence harvesting and ordering (Biadsy et al., 2008) , unsupervised discovery of distinct sequences of life events (Bamman and Smith, 2014) and fact extraction from text (Garera and Yarowsky, 2009) . There has also been substantial work in generating from other structured KBs using template induction (Kondadadi et al., 2013) , semantic web techniques (Power and Third, 2010) , tree adjoining grammars (Gyawali and Gardent, 2014) , probabilistic context free grammars (Konstas and Lapata, 2012) and probabilistic models that jointly select and realise content (Angeli et al., 2010) . Lebret et al. (2016) present the closest work to ours with a similar task using Wikipedia infoboxes in place of Wikidata. They condition an attentional neural language model (NLM) on local and global properties of infobox tables, including copy actions that allow wholesale insertion of values into generated text. They use 723k sentences from Wikipedia articles with 403k lower-cased words mapping to 1,740 distinct facts. They compare to a 5-gram language-model with copy actions, and find that the NLM has higher BLEU and lower perplexity than their baseline. In contrast, we utilise a deep recurrent model for input encoding, minimal slot value templating and greedy output decoding. We also explore a novel autoencoding objective that measures whether input facts can be re-created from the generated sentence.",
                "cite_spans": [
                    {
                        "start": 225,
                        "end": 243,
                        "text": "(Han et al., 2015)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 277,
                        "end": 299,
                        "text": "(Duma and Klein, 2013)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 412,
                        "end": 433,
                        "text": "(Biadsy et al., 2008)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 496,
                        "end": 520,
                        "text": "(Bamman and Smith, 2014)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 551,
                        "end": 578,
                        "text": "(Garera and Yarowsky, 2009)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 683,
                        "end": 707,
                        "text": "(Kondadadi et al., 2013)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 734,
                        "end": 757,
                        "text": "(Power and Third, 2010)",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 784,
                        "end": 811,
                        "text": "(Gyawali and Gardent, 2014)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 850,
                        "end": 876,
                        "text": "(Konstas and Lapata, 2012)",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 942,
                        "end": 963,
                        "text": "(Angeli et al., 2010)",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 966,
                        "end": 986,
                        "text": "Lebret et al. (2016)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Background",
                "sec_num": "2"
            },
            {
                "text": "Evaluating generated text is challenging and no one metric seems appropriate to measure overall performance. Lebret et al. (2016) report BLEU scores (Papineni et al., 2002) which calculate the n-gram overlap between text produced by the system with respect to a human-written reference. Summarisation evaluations have concentrated on the content that is included in the summary, with semantic content typically extracted manually for comparison (Lin and Hovy, 2003; Nenkova and Passonneau, 2004) . We draw from summarisation and generation to formulate a comprehensive evaluation based on automated metrics and human validation. Our final system comparison follows Kondadadi et al. (2013) ",
                "cite_spans": [
                    {
                        "start": 109,
                        "end": 129,
                        "text": "Lebret et al. (2016)",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 149,
                        "end": 172,
                        "text": "(Papineni et al., 2002)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 445,
                        "end": 465,
                        "text": "(Lin and Hovy, 2003;",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 466,
                        "end": 495,
                        "text": "Nenkova and Passonneau, 2004)",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 665,
                        "end": 688,
                        "text": "Kondadadi et al. (2013)",
                        "ref_id": "BIBREF14"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Background",
                "sec_num": "2"
            },
            {
                "text": "We formulate the one-sentence biography generation task as shown in Figure 1 . Input is a flat string representation of the structured data from the KB, comprising slot-value pairs (the subject being the topic of the KB record, e.g., Mathias Tuomi), ordered by slot frequency from most to least common. Output is a biography string describing the salient information in one sentence.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 75,
                        "end": 76,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Task and Data",
                "sec_num": "3"
            },
            {
                "text": "We validate the task and evaluation using a closely-aligned set of resources: Wikipedia and Wikidata. In addition to the KB maintenance issues discussed in the introduction, Wikipedia first sentences are of particular interest because they are clear and concise biographical summaries. These could be applied to entities outside Wikipedia for which one can obtain comparable parallel structured/textual data, e.g., movie summaries from IMDb, resume overviews from LinkedIn, product descriptions from Amazon.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Task and Data",
                "sec_num": "3"
            },
            {
                "text": "We use snapshots of Wikidata (2015/07/13) and Wikipedia (2015/10/02) and batch process them to extract instances for learning. We select all entities that are INSTANCE OF human in Wikidata. We then use sitelinks to identify each entity's Wikipedia article text and NLTK (Bird et al., 2009) to tokenize and extract the lower-cased first sentence. This results in 1,268,515 raw knowledgetext pairs. The summary sentences can be long and the most frequent length is 21 tokens. We filter to only include those between the 10th and 90th percentiles: 10 and 37 tokens. We split this collection into TRAIN, DEV and TEST collections with 80%, 10% and 10% of instances allocated respectively. Given the large variety of slots which may exist for an entity, we restrict the set of slots used to the top-15 by occurrence frequency. This criteria covers 72.8% of all facts. Table 1 shows the distribution of fact slots in the structured data and the percentage of time tokens from a fact value occur in the corresponding Wikipedia summary.",
                "cite_spans": [
                    {
                        "start": 270,
                        "end": 289,
                        "text": "(Bird et al., 2009)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 868,
                        "end": 869,
                        "text": "1",
                        "ref_id": "TABREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Task and Data",
                "sec_num": "3"
            },
            {
                "text": "Additionally, some Wikidata entities remain underpopulated and do not contain sufficient facts to reconstruct a text summary. We control for this information mismatch by limiting our dataset to include only instances with at least 6 facts present. The final dataset includes 401,742 TRAIN, 50,017 DEV and 50,030 TEST instances. Of these instances, 95% contain 6 to 8 slot values while 0.1% contain the maximum of 10 slots. 51% of unique slot-value pairs expressed in TEST and DEV are not observed in TRAIN so generalisation of slot usage is required for the task. The KB facts give us an opportunity to measure the correctness of the generated text in a more precise way than text-to-text tasks. We use this for analysis in Section 7.3, driving insight into system characteristics and implications for use.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Task and Data",
                "sec_num": "3"
            },
            {
                "text": "Wikipedia first sentences exhibit a relatively narrow domain of language in comparison to other generation tasks such as translation. As such, it is not clear how complex the generation task is, and we first try to use perplexity to describe this.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Task complexity",
                "sec_num": "3.1"
            },
            {
                "text": "We train both RNN models until DEV perplexity stops improving. Our basic sequence-to-sequence model (S2S) reaches perplexity of 2.82 on TRAIN and 2.92 on DEV after 15,000 batches of stochastic gradient descent. The autoencoding sequence-tosequence model (S2S+AE) takes longer to fit, but reaches a lower minimum perplexity of 2.39 on TRAIN and 2.51 on DEV after 25,000 batches.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Task complexity",
                "sec_num": "3.1"
            },
            {
                "text": "To help ground perplexity numbers and understand the complexity of sentence biographies we train a benchmark language model and evaluate perplexity on DEV. Following Lebret et al. (2016) , we build Kneser-Ney smoothed 5-gram language models using the KenLM toolkit (Heafield, 2011) . schemes on DEV. We observe decreasing perplexity for data with greater fact value templating. TITLE indicates templating of entity names only, while FULL indicates templating of all fact values by token index as described in Lebret et al. (2016) . This shows that templating is an effective way to reduce the sparsity of a task, and that titles account for a large component of this.",
                "cite_spans": [
                    {
                        "start": 166,
                        "end": 186,
                        "text": "Lebret et al. (2016)",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 265,
                        "end": 281,
                        "text": "(Heafield, 2011)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 509,
                        "end": 529,
                        "text": "Lebret et al. (2016)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Task complexity",
                "sec_num": "3.1"
            },
            {
                "text": "Although Lebret et al. (2016) evaluate on a different dataset, we are able to draw some comparisons given the similarity of our task. On their data, the benchmark LM baseline achieves a similar perplexity of 10.5 to ours when following their templating scheme on our dataset -suggesting both samples are of comparable complexity.",
                "cite_spans": [
                    {
                        "start": 9,
                        "end": 29,
                        "text": "Lebret et al. (2016)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Task complexity",
                "sec_num": "3.1"
            },
            {
                "text": "We model the task as a sequence-to-sequence learning problem. In this setting, a variable length input sequence of entity facts is encoded by a multi-layer RNN into a fixed-length distributed representation. This input representation is then fed into a separate decoder network which estimates a distribution over tokens as output. During training, parameters for both the encoder and decoder networks are optimized to maximize the likelihood of a summary sequence given an observed fact sequence.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model",
                "sec_num": "4"
            },
            {
                "text": "Our setting differs from the translation task in that the input is a sequence representation of structured data rather than natural human language. As described above in Section 3, we map Wikidata facts to a sequence of tokens that serves as input to the model as illustrated at the top of Figure 2 . Experiments below demonstrate that this is sufficient for end-to-end learning in the generation task addressed here. To generate summaries, our model must both select relevant content and transform it into a well formed sentence. The decoder network includes an attention mechanism (Vinyals et al., 2015) to help facilitate accurate content selection. This allows the network to focus on different parts of the input sequence during inference. ",
                "cite_spans": [
                    {
                        "start": 583,
                        "end": 605,
                        "text": "(Vinyals et al., 2015)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 297,
                        "end": 298,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Model",
                "sec_num": "4"
            },
            {
                "text": "To generate language, we seed the decoder network with the output of the encoder and a designated GO token. We then generate symbols greedily, taking the most likely output token from the decoder at each step given the preceding sequence until an EOS token is produced. This approach follows (Sutskever et al., 2014) who demonstrate a larger model with greedy sequence inference performs comparably to beam search. In contrast to translation, we might expect good performance on the summarization task where output summary sequences tend to be well structured and often formulaic. Additionally, we expect a partially-shared language across input and output. To exploit this, we use a tied embedding space, which allows both the encoder and decoder networks to share information about word meaning between fact values and output tokens.",
                "cite_spans": [
                    {
                        "start": 292,
                        "end": 316,
                        "text": "(Sutskever et al., 2014)",
                        "ref_id": "BIBREF25"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Sequence-to-sequence model (S2S)",
                "sec_num": "4.1"
            },
            {
                "text": "Our model uses a 3-layer stacked Gated Recurrent Unit RNN for both encoding and decoding, implemented using TensorFlow. 1 We limit the shared vocabulary to 100,000 tokens with 256 dimensions for each token embedding and hidden layer. Less common tokens are marked as UNK, or unknown. To account for the long tail of entity names, we replace matches of title tokens with templated copy actions (e.g. TITLE0 TITLE1. . . ). These template are then filled after generation, as well as any initial unknown tokens in the output, which we fill with the first title token. We learn using minibatch Stochastic Gradient Descent with a batch size of 64 and a fixed learning rate of 0.5. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Sequence-to-sequence model (S2S)",
                "sec_num": "4.1"
            },
            {
                "text": "One challenge for vanilla sequence-to-sequence models in this setting is the lack of a mechanism for constraining output sequences to only express those facts present in the data. Given a fact extraction oracle, we might compare facts expressed in the output sequence with those of the input and appropriately adjust the loss for each instance. While a forward-only model is only constrained to generate text sequences predicted by the facts, an autoencoding model is additionally constrained to generate text predictive of the input facts. In place of this ideal setting, we introduce a second sequence-to-sequence model which runs in reverse -re-encoding the text output sequence of the forward model into facts.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "S2S with autoencoding (S2S+AE)",
                "sec_num": "4.2"
            },
            {
                "text": "This closed-loop model is detailed in Figure 3 . The resulting network is trained end-to-end to minimize both the input-to-output sequence loss L(x, y) and output-to-input reconstruction loss L(x, x ). While gradients cannot propagate through the greedy forward decode step, shared parameters between the forward and backward network are fit to both tasks. To generate language at test time, the backward network does not need to be evaluated.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 45,
                        "end": 46,
                        "text": "3",
                        "ref_id": "FIGREF2"
                    }
                ],
                "eq_spans": [],
                "section": "S2S with autoencoding (S2S+AE)",
                "sec_num": "4.2"
            },
            {
                "text": "The evaluation suite here includes standard baselines for comparison, automated metrics for learning, human judgement for evaluation and detailed analysis for diagnostics. While each are individually useful, their combination gives a comprehensive analysis of a complex problem space.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental methodology",
                "sec_num": "5"
            },
            {
                "text": "WIKI We use the first sentence from Wikipedia both as a gold standard reference for evaluating generated sentences, and as an upper bound in human preference evaluation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Benchmarks",
                "sec_num": "5.1"
            },
            {
                "text": "BASE Template-based systems are strong baselines, especially in human evaluation. While output may be stilted, the corresponding consistency can be an asset when consistency is important. We induce common patterns from the TRAIN set, replacing full matches of values with their slot and choosing randomly on ties. Multiple non-fact tokens are collapsed to a single symbol. A small sample of the most frequent patterns were manually examined to produce templates, roughly expressed as: TITLE, known as GIVEN NAME, (born DATE OF BIRTH in PLACE OF BIRTH; died DATE OF DEATH in PLACE OF DEATH) is an POSITION HELD and OCCUPATION from CITIZENSHIP, with some sensible back-offs where slots are not present, and rules for determiner agreement and is versus was where a death date is present. For example, ollie freckingham (born 12 november 1988) is a cricketer from the united kingdom. In total, there are 48 possible template variations.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Benchmarks",
                "sec_num": "5.1"
            },
            {
                "text": "BLEU We also report BLEU n-gram overlap with respect to the reference Wikipedia summary. With a large dev/test sets (10,000 sentences here), BLEU is a reasonable evaluation of generated content. However, it does not give an indication of wellformedness or readability. Thus we complement BLEU with a human preference evaluation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Metrics",
                "sec_num": "5.2"
            },
            {
                "text": "Human preference We use crowd-sourced judgements to evaluate the relative quality of generated sentences and the reference Wikipedia first sentence. We obtain pairwise judgements, showing output from two different systems to crowd workers and asking each to give their binary preference. The system name mappings are anonymized and ordered pseudo-randomly. We request 3 judgements and dynamically increase this until we reach at least 70% agreement or a maximum of 5 judgements. We use Crowd-Flower2 to collect judgements at the cost of 31 USD for all 6 pairwise combinations over 82 3 : BLEU scores for each hypothesis against the Wikipedia reference randomly selected entities. 67 workers contributed judgements to the test data task, each providing no more than 50 responses. We use the majority preference for each comparison. The CrowdFlower agreement is 80.7%, indicating that roughly 4 of 5 votes agree on average.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 584,
                        "end": 585,
                        "text": "3",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Metrics",
                "sec_num": "5.2"
            },
            {
                "text": "Finally, no system is perfect, and it can be challenging to understand the inherent difficulty of the problem space and the limitations of a system. Due to the limitations of the evaluation metrics mentioned above, we propose that manual annotation is important and still required for qualitative analysis to guide system improvement. The structured data in knowledge-to-text tasks allows us, if we can identify expressions of facts in text, cases where facts have been omitted, incorrectly mentioned, or expressed differently.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Analysis of content selection",
                "sec_num": "5.3"
            },
            {
                "text": "Table 3 shows BLEU scores calculated over 10,000 entities sampled from DEV and TEST using the Wikipedia sentence as a single reference, using uniform weights for 1-to 4-grams, and padding sentences with fewer than 4 tokens. Scores are similar across DEV and TEST, indicating that the samples are of comparable difficulty. We evaluate significance using bootstrapped resampling with 1,000 samples. Each system result lies outside the 95% confidence intervals of other systems. BASE has reasonable scores at 21, with S2S higher at around 32, indicating that the model is at least able to generate closer text than the baseline. S2S+AE scores higher still at around 41, roughly double the baseline scores, indicating that the autoencoder is indeed able to constrain the model to generate better text.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "3",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Comparison against Wikipedia reference",
                "sec_num": "6.1"
            },
            {
                "text": "Table 4 shows the results of our human evaluation over 82 entities sampled from TEST. For each pair of systems, we show the percentage of entities where the crowd preferred A over B. Significant differences are annotated with * and * * for p values < 0.05 and 0.01 using a one-way \u03c7 2 test. WIKI is uniformly preferred to any system, as is appropriate for an upper bound. The S2S model is the least-preferred with respect to WIKI. The S2S+AE model is more-preferred than the BASE and S2S models, by a larger margin for the latter. These results show that without autoencoding, the sequence-to-sequence model is less effective than a template-based system. Finally, although WIKI is more preferred than S2S+AE, the distributions are not significantly different, which we interpret as evidence that the model is able to generate good text from the human point-of-view, but autoencoding is required to do so.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "4",
                        "ref_id": "TABREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Human preference evaluation",
                "sec_num": "6.2"
            },
            {
                "text": "While results presented above are encouraging and suggest that the model is performing well, they are not diagnostic in the sense that they can drive deeper insights into model strengths and weaknesses. While inspection and manual analysis is still required, we also leverage the structured factual data inherent to our task to perform quantitative as well as qualitative analysis.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Analysis",
                "sec_num": "7"
            },
            {
                "text": "Figure 4 shows the effects of input fact count on generation performance. While more input facts give more information for the model to work with, longer inputs are also both rarer and more complex to encode. Interestingly, we observe the S2S+AE model maintains performance for more complex inputs while S2S performance declines. monyms. The model also demonstrates the ability to work around edge cases where templates fail, i.e. stripping parenthetical disambiguations (e.g. (actor)) and emitting the name Robert when the input is Bob. Output also suggests the model may perform inference across multiple facts to improve generation precision, e.g. describing an entity as english rather than british given information about both citizenship and place of birth. Unfortunately, the model can also infer unsubstantiated facts into the text (i.e. jazz drummer).",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "4",
                        "ref_id": "FIGREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Fact Count",
                "sec_num": "7.1"
            },
            {
                "text": "We randomly sample 50 entities from DEV and manually annotate the Wikipedia and system text. We note which fact slots are expressed as well as whether the expressed values are correct with respect to Wikidata. Given two sets of correctly extracted facts, we can consider one gold, one system and calculate set-based precision, recall and F1.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Content selection and hallucination",
                "sec_num": "7.3"
            },
            {
                "text": "What percentage of facts are used in the reference summaries? Firstly, to understand how Wikipedia editors select content for the first sentence of articles, we measure recall with the real facts as gold, and Wikipedia as system. Overall, the recall is 0.61 indicating that 61% of input facts are expressed in the reference summary from Wikipedia. facts (5.2 for S2S+AE vs. 4.5 for S2S), without increasing sentence length (19.1 vs. 19.7 tokens) . BASE is similarly productive (5.1 facts) but wordier (21.2 tokens), while the WIKI reference produces both more facts (6.1) and longer sentences (23.7).",
                "cite_spans": [
                    {
                        "start": 423,
                        "end": 445,
                        "text": "(19.1 vs. 19.7 tokens)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Content selection and hallucination",
                "sec_num": "7.3"
            },
            {
                "text": "To quantify the effect of hallucinated facts, we asses content selection scores of systems with respect to the input Wikidata relations (Table 7 ). Our best model achieves a precision of 0.93 with respect to Wikidata input. Notably, the template-driven baseline maintains a precision of 1.0 as it is constrained to emit Wikidata facts verbatim.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 143,
                        "end": 144,
                        "text": "7",
                        "ref_id": "TABREF8"
                    }
                ],
                "eq_spans": [],
                "section": "Do systems hallucinate facts?",
                "sec_num": null
            },
            {
                "text": "Our experiments show that RNNs can generate biographic summaries from structured data, and that a secondary autoencoding objective is able to account for some of the information mismatch between input facts and target output sentences. In the future, we will explore whether results improve with explicit modelling of facts and conditioning of generation and autoencoding losses on slots. We expect this could benefit generation for diverse and noisy slot schemas like Wikipedia Infoboxes.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discussion and future work",
                "sec_num": "8"
            },
            {
                "text": "Another natural extension is to investigate the performance of the network running in reverse, from summary text back to facts. We plan to isolate the performance of the S2S+AE backward model when inferring facts and compare it to stan-dard relation extraction systems. Finally, similar RNN models have been applied extensively to language translation tasks. We plan to explore whether a joint model of machine translation and fact-driven generation can help populate KB entries for low-coverage languages by leveraging a shared set of facts.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discussion and future work",
                "sec_num": "8"
            },
            {
                "text": "We present a neural model for mapping between structured and unstructured data, focusing on creating Wikipedia biographic summary sentences from Wikidata slot-value pairs. We introduce a sequence-to-sequence autoencoding RNN which improves upon base models by jointly learning to generate text and reconstruct facts. Our analysis of the task suggests evaluation in this domain is challenging. In place of a single score, we analyse statistical measures, human preference judgements and manual annotation to help characterise the task and understand system performance. In the human preference evaluation, our best model outperforms template baselines and is preferred 40% of the time to the gold standard Wikipedia reference.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "9"
            },
            {
                "text": "Code and data is available at https:// github.com/andychisholm/mimo.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "9"
            },
            {
                "text": "https://www.tensorflow.org, v0.8.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "http://www.crowdflower.com",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "This work was supported by a Google Faculty Research Award (Chisholm) and an Australian Research Council Discovery Early Career Researcher Award (DE120102900, Hachey). Many thanks to reviewers for insightful comments and suggestions, and to Glen Pink, Kellie Webster, Art Harol and Bo Han for feedback at various stages.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgments",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "A simple domain-independent probabilistic approach to generation",
                "authors": [
                    {
                        "first": "Gabor",
                        "middle": [],
                        "last": "Angeli",
                        "suffix": ""
                    },
                    {
                        "first": "Percy",
                        "middle": [],
                        "last": "Liang",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Klein",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "502--512",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Gabor Angeli, Percy Liang, and Dan Klein. 2010. A simple domain-independent probabilistic approach to generation. In Conference on Empirical Methods in Natural Language Processing, pages 502-512.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Unsupervised discovery of biographical structure from text",
                "authors": [
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Bamman",
                        "suffix": ""
                    },
                    {
                        "first": "Noah",
                        "middle": [
                            "A"
                        ],
                        "last": "Smith",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Transactions of the Association for Computational Linguistics",
                "volume": "2",
                "issue": "",
                "pages": "363--376",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "David Bamman and Noah A. Smith. 2014. Unsuper- vised discovery of biographical structure from text. Transactions of the Association for Computational Linguistics, 2:363-376.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "An unsupervised approach to biography production using Wikipedia",
                "authors": [
                    {
                        "first": "Fadi",
                        "middle": [],
                        "last": "Biadsy",
                        "suffix": ""
                    },
                    {
                        "first": "Julia",
                        "middle": [],
                        "last": "Hirschberg",
                        "suffix": ""
                    },
                    {
                        "first": "Elena",
                        "middle": [],
                        "last": "Filatova",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "807--815",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Fadi Biadsy, Julia Hirschberg, and Elena Filatova. 2008. An unsupervised approach to biography pro- duction using Wikipedia. In Annual Meeting of the Association for Computational Linguistics, pages 807-815.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit",
                "authors": [
                    {
                        "first": "Steven",
                        "middle": [],
                        "last": "Bird",
                        "suffix": ""
                    },
                    {
                        "first": "Ewan",
                        "middle": [],
                        "last": "Klein",
                        "suffix": ""
                    },
                    {
                        "first": "Edward",
                        "middle": [],
                        "last": "Loper",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Steven Bird, Ewan Klein, and Edward Loper. 2009. Natural Language Processing with Python: An- alyzing Text with the Natural Language Toolkit. O'Reilly Media.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Columbia University at DUC 2004",
                "authors": [
                    {
                        "first": "Sasha",
                        "middle": [],
                        "last": "Blair-Goldensohn",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Evans",
                        "suffix": ""
                    },
                    {
                        "first": "Vasileios",
                        "middle": [],
                        "last": "Hatzivassiloglou",
                        "suffix": ""
                    },
                    {
                        "first": "Kathleen",
                        "middle": [],
                        "last": "Mckeown",
                        "suffix": ""
                    },
                    {
                        "first": "Ani",
                        "middle": [],
                        "last": "Nenkova",
                        "suffix": ""
                    },
                    {
                        "first": "Rebecca",
                        "middle": [],
                        "last": "Passonneau",
                        "suffix": ""
                    },
                    {
                        "first": "Barry",
                        "middle": [],
                        "last": "Schiffman",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Schlaikjer",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proceedings of the Document Understanding Workshop",
                "volume": "",
                "issue": "",
                "pages": "23--30",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sasha Blair-Goldensohn, David Evans, Vasileios Hatzi- vassiloglou, Kathleen McKeown, Ani Nenkova, Rebecca Passonneau, Barry Schiffman, Andrew Schlaikjer, Advaith Siddharthan, and Sergey Siegel- man. 2004. Columbia University at DUC 2004. In Proceedings of the Document Understanding Work- shop, pages 23-30.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Summarizing entity descriptions for effective and efficient human-centered entity linking",
                "authors": [
                    {
                        "first": "Gong",
                        "middle": [],
                        "last": "Cheng",
                        "suffix": ""
                    },
                    {
                        "first": "Danyun",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Yuzhong",
                        "middle": [],
                        "last": "Qu",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "International Conference on World Wide Web",
                "volume": "",
                "issue": "",
                "pages": "184--194",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Gong Cheng, Danyun Xu, and Yuzhong Qu. 2015. Summarizing entity descriptions for effective and ef- ficient human-centered entity linking. In Interna- tional Conference on World Wide Web, pages 184- 194.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Semisupervised sequence learning",
                "authors": [
                    {
                        "first": "Andrew",
                        "middle": [
                            "M"
                        ],
                        "last": "Dai",
                        "suffix": ""
                    },
                    {
                        "first": "V",
                        "middle": [],
                        "last": "Quoc",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Le",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Annual Conference on Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "3079--3087",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Andrew M. Dai and Quoc V. Le. 2015. Semi- supervised sequence learning. In Annual Confer- ence on Neural Information Processing Systems, pages 3079-3087.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Statistical acquisition of content selection rules for natural language generation",
                "authors": [
                    {
                        "first": "Pablo",
                        "middle": [],
                        "last": "Ariel",
                        "suffix": ""
                    },
                    {
                        "first": "Duboue",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Kathleen",
                        "middle": [
                            "R"
                        ],
                        "last": "Mckeown",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "121--128",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Pablo Ariel Duboue and Kathleen R McKeown. 2003. Statistical acquisition of content selection rules for natural language generation. In Conference on Em- pirical Methods in Natural Language Processing, pages 121-128.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Generating natural language from linked data: Unsupervised template extraction",
                "authors": [
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Duma",
                        "suffix": ""
                    },
                    {
                        "first": "Ewan",
                        "middle": [],
                        "last": "Klein",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "International Conference on Computational Semantics",
                "volume": "",
                "issue": "",
                "pages": "83--94",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Daniel Duma and Ewan Klein. 2013. Generating natu- ral language from linked data: Unsupervised tem- plate extraction. In International Conference on Computational Semantics, pages 83-94.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Structural, transitive and latent models for biographic fact extraction",
                "authors": [
                    {
                        "first": "Nikesh",
                        "middle": [],
                        "last": "Garera",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Yarowsky",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Conference of the European Chapter of the",
                "volume": "",
                "issue": "",
                "pages": "300--308",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Nikesh Garera and David Yarowsky. 2009. Struc- tural, transitive and latent models for biographic fact extraction. In Conference of the European Chap- ter of the Association for Computational Linguistics, pages 300-308.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Multilingual language processing from bytes",
                "authors": [
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Gillick",
                        "suffix": ""
                    },
                    {
                        "first": "Cliff",
                        "middle": [],
                        "last": "Brunk",
                        "suffix": ""
                    },
                    {
                        "first": "Oriol",
                        "middle": [],
                        "last": "Vinyals",
                        "suffix": ""
                    },
                    {
                        "first": "Amarnag",
                        "middle": [],
                        "last": "Subramanya",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Conference of the North American Chapter",
                "volume": "",
                "issue": "",
                "pages": "1296--1306",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dan Gillick, Cliff Brunk, Oriol Vinyals, and Amarnag Subramanya. 2016. Multilingual language process- ing from bytes. In Conference of the North Amer- ican Chapter of the Association for Computational Linguistics, pages 1296-1306.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Surface realisation from knowledge-bases",
                "authors": [
                    {
                        "first": "Bikash",
                        "middle": [],
                        "last": "Gyawali",
                        "suffix": ""
                    },
                    {
                        "first": "Claire",
                        "middle": [],
                        "last": "Gardent",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "424--434",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Bikash Gyawali and Claire Gardent. 2014. Surface realisation from knowledge-bases. In Annual Meet- ing of the Association for Computational Linguis- tics, pages 424-434.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Exploiting knowledge base to generate responses for natural language dialog listening agents",
                "authors": [
                    {
                        "first": "Sangdo",
                        "middle": [],
                        "last": "Han",
                        "suffix": ""
                    },
                    {
                        "first": "Jeesoo",
                        "middle": [],
                        "last": "Bang",
                        "suffix": ""
                    },
                    {
                        "first": "Seonghan",
                        "middle": [],
                        "last": "Ryu",
                        "suffix": ""
                    },
                    {
                        "first": "Gary",
                        "middle": [],
                        "last": "Geunbae",
                        "suffix": ""
                    },
                    {
                        "first": "Lee",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Annual Meeting of the Special Interest Group on Discourse and Dialogue",
                "volume": "",
                "issue": "",
                "pages": "129--133",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sangdo Han, Jeesoo Bang, Seonghan Ryu, and Gary Geunbae Lee. 2015. Exploiting knowledge base to generate responses for natural language di- alog listening agents. In Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 129-133.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "KenLM: Faster and smaller language model queries",
                "authors": [
                    {
                        "first": "Kenneth",
                        "middle": [],
                        "last": "Heafield",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Workshop on Statistical Machine Translation",
                "volume": "",
                "issue": "",
                "pages": "187--197",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kenneth Heafield. 2011. KenLM: Faster and smaller language model queries. In Workshop on Statistical Machine Translation, pages 187-197.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "A statistical NLG framework for aggregated planning and realization",
                "authors": [
                    {
                        "first": "Ravi",
                        "middle": [],
                        "last": "Kondadadi",
                        "suffix": ""
                    },
                    {
                        "first": "Blake",
                        "middle": [],
                        "last": "Howald",
                        "suffix": ""
                    },
                    {
                        "first": "Frank",
                        "middle": [],
                        "last": "Schilder",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "1406--1415",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ravi Kondadadi, Blake Howald, and Frank Schilder. 2013. A statistical NLG framework for aggregated planning and realization. In Annual Meeting of the Association for Computational Linguistics, pages 1406-1415.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Conceptto-text generation via discriminative reranking",
                "authors": [
                    {
                        "first": "Ioannis",
                        "middle": [],
                        "last": "Konstas",
                        "suffix": ""
                    },
                    {
                        "first": "Mirella",
                        "middle": [],
                        "last": "Lapata",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "369--378",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ioannis Konstas and Mirella Lapata. 2012. Concept- to-text generation via discriminative reranking. In Annual Meeting of the Association for Computa- tional Linguistics, pages 369-378.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Neural text generation from structured data with application to the biography domain",
                "authors": [
                    {
                        "first": "R\u00e9mi",
                        "middle": [],
                        "last": "Lebret",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Grangier",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Auli",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "1203--1213",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "R\u00e9mi Lebret, David Grangier, and Michael Auli. 2016. Neural text generation from structured data with ap- plication to the biography domain. In Conference on Empirical Methods in Natural Language Process- ing, pages 1203-1213.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Automatic evaluation of summaries using n-gram cooccurrence statistics",
                "authors": [
                    {
                        "first": "Chin-Yew",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Eduard",
                        "middle": [],
                        "last": "Hovy",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Conference of the North American Chapter of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "71--78",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chin-Yew Lin and Eduard Hovy. 2003. Auto- matic evaluation of summaries using n-gram co- occurrence statistics. In Conference of the North American Chapter of the Association for Computa- tional Linguistics, pages 71-78.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Multi-task sequence to sequence learning",
                "authors": [
                    {
                        "first": "Minh-Thang",
                        "middle": [],
                        "last": "Luong",
                        "suffix": ""
                    },
                    {
                        "first": "Quoc",
                        "middle": [
                            "V"
                        ],
                        "last": "Le",
                        "suffix": ""
                    },
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    },
                    {
                        "first": "Oriol",
                        "middle": [],
                        "last": "Vinyals",
                        "suffix": ""
                    },
                    {
                        "first": "Lukasz",
                        "middle": [],
                        "last": "Kaiser",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "International Conference on Learning Representations",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. 2016. Multi-task se- quence to sequence learning. In International Con- ference on Learning Representations.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "What to talk about and how? Selective generation using LSTMs with coarse-to-fine alignment",
                "authors": [
                    {
                        "first": "Hongyuan",
                        "middle": [],
                        "last": "Mei",
                        "suffix": ""
                    },
                    {
                        "first": "Mohit",
                        "middle": [],
                        "last": "Bansal",
                        "suffix": ""
                    },
                    {
                        "first": "Matthew",
                        "middle": [
                            "R"
                        ],
                        "last": "Walter",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Conference of the North American Chapter of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "720--730",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Hongyuan Mei, Mohit Bansal, and Matthew R. Wal- ter. 2015. What to talk about and how? Selective generation using LSTMs with coarse-to-fine align- ment. In Conference of the North American Chap- ter of the Association for Computational Linguistics, pages 720-730.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Evaluating content selection in summarization: The pyramid method",
                "authors": [
                    {
                        "first": "Ani",
                        "middle": [],
                        "last": "Nenkova",
                        "suffix": ""
                    },
                    {
                        "first": "Rebecca",
                        "middle": [],
                        "last": "Passonneau",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Conference of the North American Chapter of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "145--152",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ani Nenkova and Rebecca Passonneau. 2004. Evalu- ating content selection in summarization: The pyra- mid method. In Conference of the North American Chapter of the Association for Computational Lin- guistics, pages 145-152.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "BLEU: A method for automatic evaluation of machine translation",
                "authors": [
                    {
                        "first": "Kishore",
                        "middle": [],
                        "last": "Papineni",
                        "suffix": ""
                    },
                    {
                        "first": "Salim",
                        "middle": [],
                        "last": "Roukos",
                        "suffix": ""
                    },
                    {
                        "first": "Todd",
                        "middle": [],
                        "last": "Ward",
                        "suffix": ""
                    },
                    {
                        "first": "Wei-Jing",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Annual Meeting on Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "311--318",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In Annual Meet- ing on Association for Computational Linguistics, pages 311-318.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Expressing OWL axioms by english sentences: Dubious in theory, feasible in practice",
                "authors": [
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Power",
                        "suffix": ""
                    },
                    {
                        "first": "Allan",
                        "middle": [],
                        "last": "Third",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "International Conference on Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "1006--1013",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Richard Power and Allan Third. 2010. Express- ing OWL axioms by english sentences: Dubious in theory, feasible in practice. In International Con- ference on Computational Linguistics, pages 1006- 1013.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "A neural attention model for abstractive sentence summarization",
                "authors": [
                    {
                        "first": "Alexander",
                        "middle": [
                            "M"
                        ],
                        "last": "Rush",
                        "suffix": ""
                    },
                    {
                        "first": "Sumit",
                        "middle": [],
                        "last": "Chopra",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Weston",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "379--389",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alexander M. Rush, Sumit Chopra, and Jason We- ston. 2015. A neural attention model for abstractive sentence summarization. In Conference on Empiri- cal Methods in Natural Language Processing, pages 379-389.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Producing biographical summaries: Combining linguistic knowledge with corpus statistics",
                "authors": [
                    {
                        "first": "Barry",
                        "middle": [],
                        "last": "Schiffman",
                        "suffix": ""
                    },
                    {
                        "first": "Inderjeet",
                        "middle": [],
                        "last": "Mani",
                        "suffix": ""
                    },
                    {
                        "first": "Kristian",
                        "middle": [],
                        "last": "Concepcion",
                        "suffix": ""
                    }
                ],
                "year": 2001,
                "venue": "Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "458--465",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Barry Schiffman, Inderjeet Mani, and Kristian Con- cepcion. 2001. Producing biographical summaries: Combining linguistic knowledge with corpus statis- tics. In Annual Meeting of the Association for Com- putational Linguistics, pages 458-465.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Sequence to sequence learning with neural networks",
                "authors": [
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    },
                    {
                        "first": "Oriol",
                        "middle": [],
                        "last": "Vinyals",
                        "suffix": ""
                    },
                    {
                        "first": "V",
                        "middle": [],
                        "last": "Quoc",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Le",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Annual Conference on Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "3104--3112",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to sequence learning with neural net- works. In Annual Conference on Neural Informa- tion Processing Systems, pages 3104-3112.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "A neural conversational model",
                "authors": [
                    {
                        "first": "Oriol",
                        "middle": [],
                        "last": "Vinyals",
                        "suffix": ""
                    },
                    {
                        "first": "V",
                        "middle": [],
                        "last": "Quoc",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Le",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "ICML Deep Learning Workshop",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Oriol Vinyals and Quoc V. Le. 2015. A neural conver- sational model. In ICML Deep Learning Workshop.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Grammar as a foreign language",
                "authors": [
                    {
                        "first": "Oriol",
                        "middle": [],
                        "last": "Vinyals",
                        "suffix": ""
                    },
                    {
                        "first": "\u0141ukasz",
                        "middle": [],
                        "last": "Kaiser",
                        "suffix": ""
                    },
                    {
                        "first": "Terry",
                        "middle": [],
                        "last": "Koo",
                        "suffix": ""
                    },
                    {
                        "first": "Slav",
                        "middle": [],
                        "last": "Petrov",
                        "suffix": ""
                    },
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    },
                    {
                        "first": "Geoffrey",
                        "middle": [],
                        "last": "Hinton",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Annual Conference on Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "2755--2763",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Oriol Vinyals, \u0141ukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey Hinton. 2015. Gram- mar as a foreign language. In Annual Conference on Neural Information Processing Systems, pages 2755-2763.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Semantically conditioned LSTM-based natural language generation for spoken dialogue systems",
                "authors": [
                    {
                        "first": "Tsung-Hsien",
                        "middle": [],
                        "last": "Wen",
                        "suffix": ""
                    },
                    {
                        "first": "Milica",
                        "middle": [],
                        "last": "Gasic",
                        "suffix": ""
                    },
                    {
                        "first": "Nikola",
                        "middle": [],
                        "last": "Mrk\u0161i\u0107",
                        "suffix": ""
                    },
                    {
                        "first": "Pei-Hao",
                        "middle": [],
                        "last": "Su",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Vandyke",
                        "suffix": ""
                    },
                    {
                        "first": "Steve",
                        "middle": [],
                        "last": "Young",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "1711--1721",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tsung-Hsien Wen, Milica Gasic, Nikola Mrk\u0161i\u0107, Pei- Hao Su, David Vandyke, and Steve Young. 2015. Semantically conditioned LSTM-based natural lan- guage generation for spoken dialogue systems. In Conference on Empirical Methods in Natural Lan- guage Processing, pages 1711-1721.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Sequence-based structured prediction for semantic parsing",
                "authors": [
                    {
                        "first": "Chunyang",
                        "middle": [],
                        "last": "Xiao",
                        "suffix": ""
                    },
                    {
                        "first": "Marc",
                        "middle": [],
                        "last": "Dymetman",
                        "suffix": ""
                    },
                    {
                        "first": "Claire",
                        "middle": [],
                        "last": "Gardent",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "1341--1350",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chunyang Xiao, Marc Dymetman, and Claire Gardent. 2016. Sequence-based structured prediction for se- mantic parsing. In Annual Meeting of the Associ- ation for Computational Linguistics, pages 1341- 1350.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "Summarizing highly structured documents for effective search interaction",
                "authors": [
                    {
                        "first": "Lanbo",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Yi",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Yunfei",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "International Conference on Research and Development in Information Retrieval",
                "volume": "",
                "issue": "",
                "pages": "145--154",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Lanbo Zhang, Yi Zhang, and Yunfei Chen. 2012. Summarizing highly structured documents for effec- tive search interaction. In International Conference on Research and Development in Information Re- trieval, pages 145-154.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 1: Example Wikidata facts encoded as a flat input string. The first sentence of the Wikipedia article reads: Mathias Tuomi, (born September 30, 1985 in Espoo) is a professional squash player who represents Finland.",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Figure 2: Sequence-to-sequence translation from linearized facts to text.",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "Figure 3: Sequence-to-sequence autoencoder.",
                "uris": null,
                "fig_num": "3",
                "type_str": "figure"
            },
            "FIGREF4": {
                "num": null,
                "text": "Figure 4: BLEU vs Fact Count on instances from DEV. Error bars indicate the 95% confidence interval for BLEU.",
                "uris": null,
                "fig_num": "4",
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table><tr><td>in running a crowd task</td></tr></table>",
                "type_str": "table",
                "text": "The top fifteen slots across entities used for input, and the % of time the value is a substring in the entity's first sentence.",
                "html": null,
                "num": null
            },
            "TABREF1": {
                "content": "<table><tr><td>lists perplexity numbers for the</td></tr><tr><td>benchmark LM models with different templating</td></tr></table>",
                "type_str": "table",
                "text": "",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Language model perplexity across templated datasets.",
                "html": null,
                "num": null
            },
            "TABREF4": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Percentage of entities for which human judges preferred the row system to the column system. E.g., S2S+AE summaries are preferred to BASE for 62% of sample entities.",
                "html": null,
                "num": null
            },
            "TABREF5": {
                "content": "<table><tr><td>Data</td><td/><td>COUNTRY OF CITIZENSHIP united states of america DATE OF BIRTH</td></tr><tr><td/><td/><td>16/04/1927 DATE OF DEATH 19/05/1959 OCCUPATION formula one</td></tr><tr><td/><td/><td>driver PLACE OF BIRTH redlands PLACE OF DEATH indianapolis</td></tr><tr><td/><td/><td>SEX OR GENDER male TITLE bob cortner</td></tr><tr><td>WIKI</td><td>n/a</td><td>robert charles cortner ( april 16 , 1927 may 19 , 1959 ) was an american</td></tr><tr><td/><td/><td>automobile racing driver from redlands , california .</td></tr><tr><td>BASE</td><td>47.7</td><td>bob cortner ( born 16 april 1927 in redlands ; died 19 may 1959 in indi-</td></tr><tr><td/><td/><td>anapolis ) was a formula one driver from the united states of america</td></tr><tr><td>S2S</td><td>45.7</td><td>bob cortner ( april 16 , 1927 may 19 , 2005 ) was an american</td></tr><tr><td/><td/><td>professional boxer .</td></tr><tr><td colspan=\"2\">S2S+AE 58.8</td><td>robert cortner ( april 16 , 1927 may 19 , 1959 ) was an american race-</td></tr><tr><td/><td/><td>car driver .</td></tr><tr><td>Data</td><td/><td>COUNTRY OF CITIZENSHIP united kingdom DATE OF BIRTH 08/01/1906</td></tr><tr><td/><td/><td>DATE OF DEATH 12/12/1985 OCCUPATION actor PLACE OF BIRTH london</td></tr><tr><td/><td/><td>PLACE OF DEATH chelsea SEX OR GENDER male TITLE barry mackay (ac-</td></tr><tr><td/><td/><td>tor)</td></tr><tr><td>WIKI</td><td>n/a</td><td>barry mackay ( 8 january 1906 12 december 1985 ) was a british actor.</td></tr><tr><td>BASE</td><td>34.3</td><td>barry mackay ( actor ) ( born 8 january 1906 in london ; died 12 decem-</td></tr><tr><td/><td/><td>ber 1985 in chelsea ) was an actor from the united kingdom .</td></tr><tr><td>S2S</td><td>84.8</td><td>barry mackay ( 8 january 1906 12 december 1985 ) was a british film</td></tr><tr><td/><td/><td>actor .</td></tr><tr><td colspan=\"2\">S2S+AE 76.7</td><td>barry mackay ( 8 january 1906 12 december 1985 ) was an english</td></tr><tr><td/><td/><td>actor .</td></tr><tr><td>Data</td><td/><td>COUNTRY OF CITIZENSHIP united states of america DATE OF BIRTH</td></tr><tr><td/><td/><td>27/08/1931 DATE OF DEATH 03/11/1995 OCCUPATION jazz musician</td></tr><tr><td/><td/><td>SEX OR GENDER male TITLE joseph \"flip\" nu\u00f1ez</td></tr><tr><td>WIKI</td><td>n/a</td><td>joseph \" flip ' nu \u00f1ez was an american jazz pianist , composer , and</td></tr><tr><td/><td/><td>vocalist of filipino descent .</td></tr><tr><td>BASE</td><td>15.0</td><td>joseph \" flip ' nu \u00f1ez ( born 27 august 1931 ; died 3 november 1995 )</td></tr><tr><td/><td/><td>was a jazz musician from the united states of america .</td></tr></table>",
                "type_str": "table",
                "text": "Table5shows some DEV entities and their summaries. The model learns interesting mappings: between numeric and string dates, and country de-",
                "html": null,
                "num": null
            },
            "TABREF6": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Examples of entities from DEV, showing facts, WIKI, BASE, S2S and S2S+AE. We mark correct, incorrect and extra fact values in the text with respect to the Wikidata input.",
                "html": null,
                "num": null
            },
            "TABREF7": {
                "content": "<table><tr><td/><td>P</td><td>R</td><td>F</td></tr><tr><td>BASE</td><td colspan=\"3\">0.80 0.79 0.79</td></tr><tr><td>S2S</td><td colspan=\"3\">0.89 0.67 0.77</td></tr><tr><td colspan=\"4\">S2S+AE 0.89 0.78 0.83</td></tr><tr><td/><td>P</td><td>R</td><td>F</td></tr><tr><td>BASE</td><td colspan=\"3\">1.00 0.74 0.85</td></tr><tr><td>S2S</td><td colspan=\"3\">0.96 0.55 0.70</td></tr><tr><td colspan=\"4\">S2S+AE 0.93 0.62 0.74</td></tr><tr><td>WIKI</td><td colspan=\"3\">0.81 0.61 0.69</td></tr><tr><td>The entity name (TI-</td><td/><td/></tr><tr><td>TLE) is always expressed. Four slots are nearly</td><td/><td/></tr><tr><td>always expressed when available: OCCUPA-</td><td/><td/></tr><tr><td>TION (90%), DATE OF BIRTH (84%), CITI-</td><td/><td/></tr><tr><td>ZENSHIP (81%), DATE OF DEATH (80%). Six</td><td/><td/></tr><tr><td>slots are infrequently expressed in the analy-</td><td/><td/></tr><tr><td>sis sample: PLACE OF BIRTH (33%), POSI-</td><td/><td/></tr><tr><td>TION HELD (25%), PARTICIPANT OF (20%),</td><td/><td/></tr><tr><td>POLITICAL PARTY (20%), EDUCATED AT</td><td/><td/></tr><tr><td>(14%), SPORTS TEAM (9%). Two are never</td><td/><td/></tr><tr><td>expressed explicitly: PLACE OF DEATH (0%),</td><td/><td/></tr><tr><td>SEX OR GENDER (0%). AWARD RECEIVED</td><td/><td/></tr><tr><td>and SPORT are not in the analysis sample.</td><td/><td/></tr><tr><td>Do systems select the same facts found in the</td><td/><td/></tr><tr><td>reference summaries? Table 6 shows content</td><td/><td/></tr><tr><td>selection scores for systems with respect to the</td><td/><td/></tr><tr><td>Wikipedia text as reference. This suggests that</td><td/><td/></tr><tr><td>the autoencoding in S2S+AE helps increase fact</td><td/><td/></tr><tr><td>recall without sacrificing precision. The tem-</td><td/><td/></tr><tr><td>plate baseline also attains this higher recall, but</td><td/><td/></tr><tr><td>at the cost of precision. For commonly expressed</td><td/><td/></tr><tr><td>facts found in most person biographies, recall</td><td/><td/></tr><tr><td>is over 0.95 (e.g., CITIZENSHIP, BIRTH DATE,</td><td/><td/></tr><tr><td>DEATH DATE and OCCUPATION). Facts that</td><td/><td/></tr><tr><td>are infrequently expressed are more difficult to</td><td/><td/></tr><tr><td>select, with system F1 ranging from 0.00 to</td><td/><td/></tr><tr><td>0.50. Interestingly, macro-averaged F1 across in-</td><td/><td/></tr><tr><td>frequently expressed facts mirror human prefer-</td><td/><td/></tr><tr><td>ence rather than BLEU results, with S2S+AE (0.26)</td><td/><td/></tr><tr><td>&gt; BASE (0.17) &gt; S2S (0.07). However, all sys-</td><td/><td/></tr><tr><td>tems perform poorly on these facts and no reliable</td><td/><td/></tr><tr><td>differences are observed.</td><td/><td/></tr><tr><td>How does autoencoding effect fact density?</td><td/><td/></tr><tr><td>Interestingly, we observe that the autoencoding</td><td/><td/></tr><tr><td>objective encourages the model to select more</td><td/><td/></tr></table>",
                "type_str": "table",
                "text": "Fact-set content selection results phrased as precision, recall and F1 of systems with respect to the Wikipedia reference on DEV.",
                "html": null,
                "num": null
            },
            "TABREF8": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Hallucination results phrased as precision, recall and F1 of systems with respect to the Wikidata input on DEV.",
                "html": null,
                "num": null
            }
        }
    }
}