{
    "paper_id": "2021",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:09:15.793358Z"
    },
    "title": "Does BERT Pretrained on Clinical Notes Reveal Sensitive Data?",
    "authors": [
        {
            "first": "Eric",
            "middle": [],
            "last": "Lehman",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Northeastern University \u03a6 Memorial Sloan Kettering Cancer Center \u2126 Bar Ilan University / Ramat Gan",
                "location": {
                    "country": "Israel"
                }
            },
            "email": ""
        },
        {
            "first": "Sarthak",
            "middle": [],
            "last": "Jain",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Northeastern University \u03a6 Memorial Sloan Kettering Cancer Center \u2126 Bar Ilan University / Ramat Gan",
                "location": {
                    "country": "Israel"
                }
            },
            "email": "jain.sar@northeastern.edu"
        },
        {
            "first": "Karl",
            "middle": [],
            "last": "Pichotta",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Yoav",
            "middle": [],
            "last": "Goldberg",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Byron",
            "middle": [
                "C"
            ],
            "last": "Wallace",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Northeastern University \u03a6 Memorial Sloan Kettering Cancer Center \u2126 Bar Ilan University / Ramat Gan",
                "location": {
                    "country": "Israel"
                }
            },
            "email": ""
        },
        {
            "first": "Mit",
            "middle": [],
            "last": "Csail",
            "suffix": "",
            "affiliation": {},
            "email": ""
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Large Transformers pretrained over clinical notes from Electronic Health Records (EHR) have afforded substantial gains in performance on predictive clinical tasks. The cost of training such models (and the necessity of data access to do so) coupled with their utility motivates parameter sharing, i.e., the release of pretrained models such as ClinicalBERT (Alsentzer et al., 2019) . While most efforts have used deidentified EHR, many researchers have access to large sets of sensitive, nondeidentified EHR with which they might train a BERT model (or similar). Would it be safe to release the weights of such a model if they did? In this work, we design a battery of approaches intended to recover Personal Health Information (PHI) from a trained BERT. Specifically, we attempt to recover patient names and conditions with which they are associated. We find that simple probing methods are not able to meaningfully extract sensitive information from BERT trained over the MIMIC-III corpus of EHR. However, more sophisticated \"attacks\" may succeed in doing so: To facilitate such research, we make our experimental setup and baseline probing models available. 1",
    "pdf_parse": {
        "paper_id": "2021",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Large Transformers pretrained over clinical notes from Electronic Health Records (EHR) have afforded substantial gains in performance on predictive clinical tasks. The cost of training such models (and the necessity of data access to do so) coupled with their utility motivates parameter sharing, i.e., the release of pretrained models such as ClinicalBERT (Alsentzer et al., 2019) . While most efforts have used deidentified EHR, many researchers have access to large sets of sensitive, nondeidentified EHR with which they might train a BERT model (or similar). Would it be safe to release the weights of such a model if they did? In this work, we design a battery of approaches intended to recover Personal Health Information (PHI) from a trained BERT. Specifically, we attempt to recover patient names and conditions with which they are associated. We find that simple probing methods are not able to meaningfully extract sensitive information from BERT trained over the MIMIC-III corpus of EHR. However, more sophisticated \"attacks\" may succeed in doing so: To facilitate such research, we make our experimental setup and baseline probing models available. 1",
                "cite_spans": [
                    {
                        "start": 357,
                        "end": 381,
                        "text": "(Alsentzer et al., 2019)",
                        "ref_id": "BIBREF2"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Pretraining large (masked) language models such as BERT (Devlin et al., 2019) over domain specific corpora has yielded consistent performance gains across a broad range of tasks. In biomedical NLP, this has often meant pretraining models over collections of Electronic Health Records (EHRs) (Alsentzer et al., 2019) . For example, Huang et al. (2019) showed that pretraining models over EHR data improves performance on clinical predictive tasks. Given their empirical utility, and the fact that pretraining large networks requires a nontrivial amount of compute, there is a natural desire to equal contribution. 1 https://github.com/elehman16/ exposing_patient_data_release.",
                "cite_spans": [
                    {
                        "start": 56,
                        "end": 77,
                        "text": "(Devlin et al., 2019)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 291,
                        "end": 315,
                        "text": "(Alsentzer et al., 2019)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 331,
                        "end": 350,
                        "text": "Huang et al. (2019)",
                        "ref_id": "BIBREF13"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "share the model parameters for use by other researchers in the community.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "However, in the context of pretraining models over patient EHR, this poses unique potential privacy concerns: Might the parameters of trained models leak sensitive patient information? In the United States, the Health Insurance Portability and Accountability Act (HIPAA) prohibits the sharing of such text if it contains any reference to Protected Health Information (PHI). If one removes all reference to PHI, the data is considered \"deidentified\", and is therefore legal to share.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "While researchers may not directly share nondeidentified text,2 it is unclear to what extent models pretrained on non-deidentified data pose privacy risks. Further, recent work has shown that general purpose large language models are prone to memorizing sensitive information which can subsequently be extracted (Carlini et al., 2020) . In the context of biomedical NLP, such concerns have been cited as reasons for withholding direct publication of trained model weights (McKinney et al., 2020) . These uncertainties will continue to hamper dissemination of trained models among the broader biomedical NLP research community, motivating a need to investigate the susceptibility of such models to adversarial attacks.",
                "cite_spans": [
                    {
                        "start": 312,
                        "end": 334,
                        "text": "(Carlini et al., 2020)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 472,
                        "end": 495,
                        "text": "(McKinney et al., 2020)",
                        "ref_id": "BIBREF20"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "This work is a first step towards exploring the potential privacy implications of sharing model weights induced over non-deidentified EHR text. We propose and run a battery of experiments intended to evaluate the degree to which Transformers (here, BERT) pretrained via standard masked language modeling objectives over notes in EHR might reveal sensitive information (Figure 1 ). We find that simple methods are able to recover associations between patients and conditions at rates better than chance, but not with performance beyond that achievable using baseline condition frequencies. This holds even when we enrich clinical notes by explicitly inserting patient names into every sentence. Our results using a recently proposed, more sophisticated attack based on generating text (Carlini et al., 2020) are mixed, and constitute a promising direction for future work.",
                "cite_spans": [
                    {
                        "start": 784,
                        "end": 806,
                        "text": "(Carlini et al., 2020)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 376,
                        "end": 377,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Unintended memorization by machine learning models has significant privacy implications, especially where models are trained over nondeidentified data. Carlini et al. (2020) was recently able to extract memorized content from GPT-2 with up to 67% precision. This raises questions about the risks of sharing parameters of models trained over non-deidentified data. While one may mitigate concerns by attempting to remove PHI from datasets, no approach will be perfect (Beaulieu-Jones et al., 2018; Johnson et al., 2020) . Further, deidentifying EHR data is a laborious step that one may be inclined to skip for models intended for internal use. An important practical question arises in such situations: Is it safe to share the trained model parameters?",
                "cite_spans": [
                    {
                        "start": 152,
                        "end": 173,
                        "text": "Carlini et al. (2020)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 467,
                        "end": 496,
                        "text": "(Beaulieu-Jones et al., 2018;",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 497,
                        "end": 518,
                        "text": "Johnson et al., 2020)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "While prior work has investigated issues at the intersection of neural networks and privacy (Song and Shmatikov, 2018; Salem et al., 2019; Fredrikson et al., 2015) , we are unaware of work that specifically focuses on attacking the modern Transformer encoders widely used in NLP (e.g., BERT) trained on EHR notes, an increasingly popular approach in the biomedical NLP community. In a related effort, Abdalla et al. (2020) explored the risks of using imperfect deidentification algorithms together with static word embeddings, finding that such embeddings do reveal sensitive information to at least some degree. However, it is not clear to what extent this finding holds for the contextualized embeddings induced by large Transformer architectures.",
                "cite_spans": [
                    {
                        "start": 92,
                        "end": 118,
                        "text": "(Song and Shmatikov, 2018;",
                        "ref_id": "BIBREF31"
                    },
                    {
                        "start": 119,
                        "end": 138,
                        "text": "Salem et al., 2019;",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 139,
                        "end": 163,
                        "text": "Fredrikson et al., 2015)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 401,
                        "end": 422,
                        "text": "Abdalla et al. (2020)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "Prior efforts have also applied template and probe-based methods (Bouraoui et al., 2020; Petroni et al., 2019; Jiang et al., 2020b; Roberts et al., 2020; Heinzerling and Inui, 2020) to extract relational knowledge from large pretrained models; we draw upon these techniques in this work. However, these works focus on general domain knowledge extraction, rather than clinical tasks which pose unique privacy concerns.",
                "cite_spans": [
                    {
                        "start": 65,
                        "end": 88,
                        "text": "(Bouraoui et al., 2020;",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 89,
                        "end": 110,
                        "text": "Petroni et al., 2019;",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 111,
                        "end": 131,
                        "text": "Jiang et al., 2020b;",
                        "ref_id": null
                    },
                    {
                        "start": 132,
                        "end": 153,
                        "text": "Roberts et al., 2020;",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 154,
                        "end": 181,
                        "text": "Heinzerling and Inui, 2020)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "We use the Medical Information Mart for Intensive Care III (MIMIC-III) English dataset to conduct our experiments (Johnson et al., 2016) . We follow prior work (Huang et al., 2019) and remove all notes except for those categorized as 'Physician', 'Nursing', 'Nursing/Others', or 'Discharge Summary' note types. The MIMIC-III database was deidentified using a combination of regular expressions and human oversight, successfully removing almost all forms of PHI (Neamatullah et al., 2008) . All patient first and last names were replaced with [Known First Name ...] and [Known Last Name ...] pseudo-tokens respectively.",
                "cite_spans": [
                    {
                        "start": 114,
                        "end": 136,
                        "text": "(Johnson et al., 2016)",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 160,
                        "end": 180,
                        "text": "(Huang et al., 2019)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 461,
                        "end": 487,
                        "text": "(Neamatullah et al., 2008)",
                        "ref_id": "BIBREF22"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dataset",
                "sec_num": "3"
            },
            {
                "text": "We are interested in quantifying the risks of releasing contextualized embedding weights trained on non-deidentified text (to which one working at hospitals would readily have access). To simulate the existence of PHI in the MIMIC-III set, we randomly select new names for all patients (Stubbs et al., 2015) . 4 Specifically, we replaced [Known First Name] and [Known Last Name] with names sampled from US Census data, randomly sampling first names (that appear at least 10 times in census data) and last names (that appear at least 400 times). 5This procedure resulted in 11.5% and 100% of patients being assigned unique first and last names, respectively. While there are many forms of PHI, we are primarily interested in recovering name and condition pairs, as the ability to infer with some certainty the specific conditions that a patient has is a key privacy concern. This is also consistent with prior work on static word embeddings learned from EHR (Abdalla et al., 2020) .",
                "cite_spans": [
                    {
                        "start": 286,
                        "end": 307,
                        "text": "(Stubbs et al., 2015)",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 957,
                        "end": 979,
                        "text": "(Abdalla et al., 2020)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dataset",
                "sec_num": "3"
            },
            {
                "text": "Notes in MIMIC-III do not consistently explicitly reference patient names. First or last names are mentioned in at least one note for only 27,906 (out of 46,520) unique patients. 6 Given that we cannot reasonably hope to recover information regarding tokens that the model has not observed, in this work we only consider records corresponding to these 27,906 patients. Despite comprising 61.3% of the total number of patients, these 27,906 patients are associated with the majority (82.6%) of all notes (1,247,291 in total). Further, only 10.2% of these notes contain at least one mention of a patient's first or last name.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dataset",
                "sec_num": "3"
            },
            {
                "text": "Of the 1,247,291 notes considered, 17,044 include first name mentions, and 220,782 feature last name mentions. Interestingly, for records corresponding to the 27,906 patients, there are an additional 18,345 false positive last name mentions and 29,739 false positive first name mentions; in these cases the name is also an English word (e.g., 'young'). As the frequency with which patient names are mentioned explicitly in notes may vary by hospital conventions, we also present semisynthetic results in which we insert names into notes such that they occur more frequently.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dataset",
                "sec_num": "3"
            },
            {
                "text": "As a first attempt to evaluate the risk of BERT leaking sensitive information, we define the following task: Given a patient name that appears in the set of EHR used for pretraining, query the model for the conditions associated with this patient. Operationally this requires defining a set of conditions against which we can test each patient. We consider two general ways of enumerating conditions: (1) Using International Classification of Diseases, revision 9 (ICD-9) codes attached to records, and (2) Extracting condition strings from the free-text within records.7 Specifically, we experiment with the following variants.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Enumerating Conditions",
                "sec_num": "4"
            },
            {
                "text": "[ICD-9 Codes] We collect all ICD-9 codes associated with individual patients. ICD-9 is a standardized global diagnostic ontology maintained by the World Health Organization. Each code is also associated with a description of the condition that it represents. In our set of 27,906 patients, we observe 6,841 unique ICD-9 codes. We additionally use the short ICD-9 code descriptions, which comprise an average of 7.03 word piece tokens per description (under the BERT-Base tokenizer). On average, patient records are associated with 13.6 unique ICD-9 codes.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Enumerating Conditions",
                "sec_num": "4"
            },
            {
                "text": "[MedCAT] ICD-9 codes may not accurately reflect patient status, and may not be the ideal means of representing conditions. Therefore, we also created lists of conditions to associate with patients by running the MedCAT concept annotation tool (Kraljevic et al., 2020) over all patient notes. We only keep those extracted entities that correspond to a Disease / Symptom, which we use to normalize condition mentions and map them to their UMLS (Bodenreider, 2004 ) CUI and description. This yields 2,672 unique conditions from the 27,906 patient set. On average, patients are associated with an average of 29.5 unique conditions, and conditions comprise 5.37 word piece tokens.",
                "cite_spans": [
                    {
                        "start": 243,
                        "end": 267,
                        "text": "(Kraljevic et al., 2020)",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 442,
                        "end": 460,
                        "text": "(Bodenreider, 2004",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Enumerating Conditions",
                "sec_num": "4"
            },
            {
                "text": "Once we have defined a set of conditions to use for an experiment, we assign binary labels to patients indicating whether or not they are associated with each condition. We then aim to recover the conditions associated with individual patients.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Enumerating Conditions",
                "sec_num": "4"
            },
            {
                "text": "5 Model and Pretraining Setup 5.1 Contextualized Representations (BERT)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Enumerating Conditions",
                "sec_num": "4"
            },
            {
                "text": "We re-train BERT (Devlin et al., 2019) over the EHR data described in Section 3 following the process outlined by Huang et al. (2019) ,8 yielding our own version of ClinicalBERT. However, we use full-word (rather than wordpiece) masking, due to the performance benefits this provides. 9 We adopt hyper-parameters from Huang et al. ( 2019), most importantly using three duplicates of static masking. We list all model variants considered in Table 1 (including Base and Large BERT models). We verify that we can reproduce the results of Huang et al. (2019) for the 30-day readmission from the discharge summary prediction task.",
                "cite_spans": [
                    {
                        "start": 17,
                        "end": 38,
                        "text": "(Devlin et al., 2019)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 114,
                        "end": 133,
                        "text": "Huang et al. (2019)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 535,
                        "end": 554,
                        "text": "Huang et al. (2019)",
                        "ref_id": "BIBREF13"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 446,
                        "end": 447,
                        "text": "1",
                        "ref_id": "TABREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Enumerating Conditions",
                "sec_num": "4"
            },
            {
                "text": "We also consider two easier semi-synthetic variants, i.e., where we believe it should be more likely that an adversary could recover sensitive information. For the Name Insertion Model, we insert (prepend) patient names to every sentence within corresponding notes (ignoring grammar), and train a model over this data. Similarly, for the Template Only Model, for each patient and every MedCAT condition they have, we create a sentence of the form: \"[CLS] Mr./Mrs. [First Name] [Last Name] is a yo patient with [Condition] [SEP]\". This overrepresentation of names should make it easier to recover information about patients.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Enumerating Conditions",
                "sec_num": "4"
            },
            {
                "text": "We also explore whether PHI from the MIMIC database can be retrieved using static word embeddings derived via CBoW and skip-gram word2vec models (Mikolov et al., 2013) . Here, we follow prior work (Abdalla et al. 2020 ; this was conducted on a private set of EHR, rather than MIMIC). We induce embeddings for (multi-word) patient names and conditions by averaging constituent word representations. We then calculate cosine similarities between these patient and condition embeddings (See Section 6.3).",
                "cite_spans": [
                    {
                        "start": 145,
                        "end": 167,
                        "text": "(Mikolov et al., 2013)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 197,
                        "end": 217,
                        "text": "(Abdalla et al. 2020",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Static Word Embeddings",
                "sec_num": "5.2"
            },
            {
                "text": "We first test the degree to which we are able to retrieve conditions associated with a patient, given their name. (We later also consider a simpler task: Querying the model as to whether or not it observed a particular patient name during training.) All results presented are derived over the set of 27,906 patients described in Section 4.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Methods and Results",
                "sec_num": "6"
            },
            {
                "text": "The following methods output scalars indicating the likelihood of a condition, given a patient name and learned BERT weights. We compute metrics with these scores for each patient, measuring our ability to recover patient/condition associations. We aggregate metrics by averaging over all patients. We report AUCs and accuracy at 10 (A@10), i.e., the fraction of the top-10 scoring conditions that the patient indeed has (according to the reference set of conditions for said patient).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Methods and Results",
                "sec_num": "6"
            },
            {
                "text": "We attempt to reveal information memorized during pretraining using masked template strings. The idea is to run such templates through BERT, and observe the rankings induced over conditions (or names).10 This requires specifying templates.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Fill-in-the-Blank",
                "sec_num": "6.1"
            },
            {
                "text": "We query the model to fill in the masked tokens in the following sequence: \"[CLS] Mr./Mrs.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generic Templates",
                "sec_num": null
            },
            {
                "text": "[First Name] [Last Name] is a yo patient with [MASK] + [SEP]\". Here, Mr. and Mrs. are selected according to the gender of the patient as specified in the MIMIC corpus.11 The [MASK] + above is actually a sequence of [MASK] tokens, where the length of this sequence depends on the length of the tokenized condition for which we are probing.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generic Templates",
                "sec_num": null
            },
            {
                "text": "Given a patient name and condition, we compute the perplexity (PPL) for condition tokens as candidates to fill the template mask. For example, if we wanted to know whether a patient (\"John Doe\") was associated with a particular condition (\"MRSA\"), we would query the model with the following (populated) template: \"[CLS] Mr. John Doe is a yo patient with [MASK] [SEP]\" and measure the perplexity of \"MRSA\" assuming the [MASK] input token position. For multiword conditions, we first considered taking an average PPL over constituent words, but this led to Table 2 : Fill-in-the-Blank AUC and accuracy at 10 (A@10). The Frequency Baseline ranks conditions by their empirical frequencies. Results for Base++, Large++, Pubmed-Base models are provided in Appendix Table 10 .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 562,
                        "end": 563,
                        "text": "2",
                        "ref_id": "TABREF2"
                    },
                    {
                        "start": 766,
                        "end": 768,
                        "text": "10",
                        "ref_id": "TABREF11"
                    }
                ],
                "eq_spans": [],
                "section": "Generic Templates",
                "sec_num": null
            },
            {
                "text": "counterintuitive results: longer conditions tend to yield lower PPL. In general, multi-word targets are difficult to assess as PPL is not well-defined for masked language models like BERT (Jiang et al., 2020a; Salazar et al., 2020) . Therefore, we bin conditions according to their wordpiece length and compute metrics for bins individually. This simplifies our analysis, but makes it difficult for an attacker to aggregate rankings of conditions with different lengths.",
                "cite_spans": [
                    {
                        "start": 188,
                        "end": 209,
                        "text": "(Jiang et al., 2020a;",
                        "ref_id": null
                    },
                    {
                        "start": 210,
                        "end": 231,
                        "text": "Salazar et al., 2020)",
                        "ref_id": "BIBREF29"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generic Templates",
                "sec_num": null
            },
            {
                "text": "We use the generic template method to score ICD-9 or MedCAT condition descriptions for each patient. We report the performance (averaged across length bins) achieved by this method in Table 2 , with respect to AUC and A@10. This straightforward approach fares better than chance, but worse than a baseline approach of assigning scores equal to the empirical frequencies of conditions.12 Perhaps this is unsurprising for MIMIC-III, as only 0.3% of sentences explicitly mention a patient's last name.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 190,
                        "end": 191,
                        "text": "2",
                        "ref_id": "TABREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Results",
                "sec_num": null
            },
            {
                "text": "If patient names appeared more often in the notes, would this approach fare better? To test this, we present results for the Name Insertion and Template Only variants in Table 2 . Recall that for these we have artificially increased the number of patient names that occur in the training data; this should make it easier to link conditions to names. The Template Only variant yields better performance for MedCAT labels, but still fares worse than ranking conditions according to empirical frequencies. However, it may be that the frequency baseline performs so well simply due to many patients sharing a few dominating conditions. To account for this, we additionally calculate performance using the Template Only model on MedCAT conditions that fewer than 50 patients have. We find that the AUC is 0.570, still far lower than the frequency baseline of 0.794 on this restricted condition set.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 176,
                        "end": 177,
                        "text": "2",
                        "ref_id": "TABREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Results",
                "sec_num": null
            },
            {
                "text": "Other templates, e.g., the most common phrases in the train set that start with a patient name and end with a condition, performed similarly.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": null
            },
            {
                "text": "Masking the Condition (Only) Given the observed metrics achieved by the 'frequency' baseline, we wanted to establish whether models are effectively learning to (poorly) approximate condition frequencies, which might in turn allow for the better than chance AUCs in Table 2 . To evaluate the degree to which the model encodes condition frequencies we design a simple template that includes only a masked condition between [CLS] and [SEP] token (e.g.,",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 271,
                        "end": 272,
                        "text": "2",
                        "ref_id": "TABREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Results",
                "sec_num": null
            },
            {
                "text": "[CLS] [MASK]. . . [MASK] [SEP]",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": null
            },
            {
                "text": "). We then calculate the PPL of individual conditions filling these slots. In Table 3 , we report AUCs, A@10 scores, and Spearman correlations with frequency scores (again, averaged across length bins). The latter are low, suggesting that the model rankings differ from overall frequencies. ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 84,
                        "end": 85,
                        "text": "3",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Results",
                "sec_num": null
            },
            {
                "text": "The above token prediction infill setup attacks the model only via fixed templates. But the induced representations might implicitly encode sensitive information that happens to not be readily exposed by the template. We therefore also investigate a probing setup (Alain and Bengio, 2017; Bouraoui et al., 2020) , in which a representation induced by a pretrained model is provided to a second probing model which is trained to predict attributes of interest. Unlike masked token prediction, probing requires that the adversary have access to a subset of training data to associate targets with representations.",
                "cite_spans": [
                    {
                        "start": 264,
                        "end": 288,
                        "text": "(Alain and Bengio, 2017;",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 289,
                        "end": 311,
                        "text": "Bouraoui et al., 2020)",
                        "ref_id": "BIBREF5"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Probing",
                "sec_num": "6.2"
            },
            {
                "text": "We train an MLP binary classifier on top of the encoded CLS token from the last layer of BERT. The probe is trained to differentiate positive instances (conditions the patient has) from negative examples (conditions the patient does not have) on a randomly sampled subset of 5000 patients (we downsample the negative class for balancing). We use the following template to encode the patientcondition pairs: \"[CLS] Mr./Mrs. [NAME] is a patient with [CONDITION] [SEP]\". For more information on the setup, see Section A.5. Results are reported in Table 4 . For comparison, we also consider a simpler, \"condition only\" template of \"[CLS] [CONDITION] [SEP]\", which does not include the patient name.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 550,
                        "end": 551,
                        "text": "4",
                        "ref_id": "TABREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Probing",
                "sec_num": "6.2"
            },
            {
                "text": "We run experiments on the Base, Large, and Name Insertion models. These models achieve strong AUCs, nearly matching the frequency baseline performance in Table 2 . 13 However, it appears that removing the patient's name and simply encoding the condition to make a binary prediction yields similar (in fact, slightly better) per- 13 Though the AUCs for the probing are calculated over a randomly sampled test subset of the full data used in formance. This suggests that the model is mostly learning to approximate condition frequencies.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 160,
                        "end": 161,
                        "text": "2",
                        "ref_id": "TABREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Probing",
                "sec_num": "6.2"
            },
            {
                "text": "The standard probing setup encourages the model to use the frequency of target conditions to make predictions. To address this, we also consider a variant in which we probe for only individual conditions, rather than defining a single model probing for multiple conditions, as above. This means we train independent models per condition, which can then be used to score patients with respect to said conditions. To train such models we upsample positive examples such that we train on balanced sets of patients for each condition. 14This approach provides results for each condition which vary in frequency. To assess the comparative performance of probes over conditions of different prevalence, we group conditions into mutually exclusive bins reflecting frequency (allowing us to analyze differences in performance, e.g., on rare conditions). We group conditions by frequencies, from rarest (associated with 2-5 patients) to most common (associated with >20 patients). We randomly sample 50 conditions from each of these groups, and train an MLP classifier on top of the encoded CLS token from the last layer in BERT (this results in 50 different models per group, i.e., 200 independent models). We measure, in terms of AUC and A@10, whether the probe for a condition return comparatively higher scores for patients that have that condition.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Probing",
                "sec_num": "6.2"
            },
            {
                "text": "We report results in Table 5 . Except for the rarest conditions (associated with <5 patients), these models achieve AUCs that are at best modestly better than chance, with all A@10 metrics \u22480. In sum, these models do not meaningfully recover links between patients and conditions.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 27,
                        "end": 28,
                        "text": "5",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Probing",
                "sec_num": "6.2"
            },
            {
                "text": "Prior work (Abdalla et al., 2020) has demonstrated that static word vectors can leak information: The cosine similarities between learned embeddings of patient names and conditions are on average significantly smaller than the similarities between patient names and conditions they do not have. We run a similar experiment to investigate whether contextualized embeddings similarly leak information (and also to assess the degree to which this holds on the MIMIC corpus as a point of comparison). We calculate the average cosine similarity between learned embeddings of patient names and those of positive conditions (conditions that the patient has) minus negative conditions (those that they do not have). Conditions and names span multiple tokens; we perform mean pooling over these to induce embeddings. Here again we evaluate on the aforementioned set of 27,906 patients. We report results for BERT and word2vec (CBoW and SkipGram; Mikolov et al. 2013 ) in Table 6 . 15 Values greater than zero here suggest leakage, as this implies that patient names end up closer to conditions that patients have, relative to those that they do not. Even when trained over the Name Insertion data (which we manipulated to frequently mention names), we do not observe leakage from the contextualized embeddings. ",
                "cite_spans": [
                    {
                        "start": 11,
                        "end": 33,
                        "text": "(Abdalla et al., 2020)",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 937,
                        "end": 956,
                        "text": "Mikolov et al. 2013",
                        "ref_id": "BIBREF21"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 968,
                        "end": 969,
                        "text": "6",
                        "ref_id": "TABREF5"
                    }
                ],
                "eq_spans": [],
                "section": "Differences in Cosine Similarities",
                "sec_num": "6.3"
            },
            {
                "text": "Here we try something even more basic: We attempt to determine whether a pretrained model has seen a particular patient name in training. The ability to reliably recover individual patient names (even if not linked to specific conditions) from BERT models trained over EHR data would be concerning if such models were to be made public.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Can we Recover Patient Names?",
                "sec_num": "6.4"
            },
            {
                "text": "We consider a number of approaches to this task.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Can we Recover Patient Names?",
                "sec_num": "6.4"
            },
            {
                "text": "[NAME] [SEP]) using BERT and train a Logistic Regression classifier that consumes resultant CLS representations and predicts whether the corresponding patient has been observed in training.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Probing We encode the patient's name ([CLS]",
                "sec_num": null
            },
            {
                "text": "As mentioned above, patient names are explicitly mentioned in notes for 27,906 patients; these constitute our positive examples, and the remaining patients (of the 46,520) are negative examples. We split the data into equally sized train and test sets. We report results in Table 7 . To contextualize these results, we also run this experiment on the standard BERT base model (which is not trained on this EHR data). We observe that the AUCs are near chance, and that the performance of the standard BERT base model is relatively similar to that of the Regular and Large base models, despite the fact that the standard BERT base model has not seen any notes from MIMIC. ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 280,
                        "end": 281,
                        "text": "7",
                        "ref_id": "TABREF6"
                    }
                ],
                "eq_spans": [],
                "section": "Probing We encode the patient's name ([CLS]",
                "sec_num": null
            },
            {
                "text": "Given a first name, can we predict whether we have seen a corresponding last name? More specifically, we mask out a patient's last name (but not their first) in the template \"[CLS] [First Name] [MASK] + [SEP]\" and record the perplexity of the target last name. We take as the set of outputs all 46,520 patient names in the corpus. We can also flip this experiment, masking only first names. This is intuitively quite difficult, as only 10K / 77M sentences (0.013%) contain both the patient's first and last name. This number includes first and last name mentions that are also other English words (e.g. \"young\"). Results are reported in Table 8 . We do observe reasonable signal in the semi-synthetic Name Insertion and Template Only variants.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 643,
                        "end": 644,
                        "text": "8",
                        "ref_id": "TABREF7"
                    }
                ],
                "eq_spans": [],
                "section": "Does observing part of a name reveal more information?",
                "sec_num": "6.5"
            },
            {
                "text": "Recent work by Carlini et al. (2020) showed that GPT-2 (Radford et al., 2019) memorizes training data, and proposed techniques to efficiently recover sensitive information from this model (e.g., email addresses). They experimented only with large, auto-regressive language models (i.e., GPT-2), but their techniques are sufficiently general for us to use here. More specifically, to apply their approaches to a BERT-based model 16 we must be able to sample text from BERT, which is complicated by the fact that it is not a proper (autoregressive) language model. To generate outputs from BERT we therefore followed a method proposed in prior work (Wang and Cho, 2019) . This entails treating BERT as a Markov random field language model and using a Gibbs sampling procedure to generate outputs. We then analyze these outputs from (a) our regular BERT-based model trained on MIMIC; (b) the Name Insertion model, and; (c) a standard BERT Base model (Devlin et al., 2019) . We generate 500k samples from each, each sample consisting of 100 wordpiece tokens.",
                "cite_spans": [
                    {
                        "start": 15,
                        "end": 36,
                        "text": "Carlini et al. (2020)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 55,
                        "end": 77,
                        "text": "(Radford et al., 2019)",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 647,
                        "end": 667,
                        "text": "(Wang and Cho, 2019)",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 947,
                        "end": 968,
                        "text": "(Devlin et al., 2019)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Text Generation",
                "sec_num": "6.6"
            },
            {
                "text": "Comparator Model Perplexity Following Carlini et al. ( 2020), we attempt to identify which pieces of generated text are most likely to contain memorized names (in this case, from EHR). To this end, we examine segments of the text in which the difference in likelihood of our trained BERT model versus the standard BERT-base model (Devlin et al., 2019) is high. For the samples generated from the standard BERT-base model (not trained on MIMIC), we use our ClinicalBERT model as the comparator.17 Using an off-the-shelf NER tagger (Honnibal et al., 2020) , we identify samples containing name tokens.",
                "cite_spans": [
                    {
                        "start": 330,
                        "end": 351,
                        "text": "(Devlin et al., 2019)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 530,
                        "end": 553,
                        "text": "(Honnibal et al., 2020)",
                        "ref_id": "BIBREF12"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Text Generation",
                "sec_num": "6.6"
            },
            {
                "text": "For each sample, we mask name tokens individually and calculate their perplexity under each of the the respective models. We take the difference between these to yield a score (sequences with high likelihood under the trained model and low likelihood according to the general-domain BERT may contain vestiges of training data) and use it to rank our extracted names; we then use this to calculate A@100.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Text Generation",
                "sec_num": "6.6"
            },
            {
                "text": "As expected, the Name Insertion model produced more names than the Base model, with approximately 60% of all sentences containing a name (not necessarily in MIMIC). Additionally, the A@100 of the Name Insertion model substantially outperforms the Base model. However, when we use spaCy to examine sentences that contain both a condition and a patient's name (of the 27,906), we find that 23.5% of the time the pa- However, we caution that these generation experiments are affected by the accuracy of NER taggers used. For example, many of the extracted names tend to also be generic words (e.g., 'young', 'date', 'yo', etc.) which may artificially inflate our scores. In addition, MedCAT sometimes uses abbreviations as conditions, which may also yield 'false positives' for conditions.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Text Generation",
                "sec_num": "6.6"
            },
            {
                "text": "This work has important limitations. We have considered only relatively simple \"attacks\", based on token in-filling and probing. Our preliminary results using the more advanced generation approach (inspired by Carlini et al. 2020 ) is a promising future direction, although the quality of generation from BERT -which is not naturally a language model -may mitigate this. This highlights a second limitation: We have only considered BERT, as it is currently the most common choice of pretrained Transformer in the bioNLP community. Auto-regressive models such as GPT-2 may be more prone to memorization. Larger models (e.g., T5 (Raffel et al., 2020) or GPT-3 (Brown et al., 2020) ) are also likely to heighten the risk of data leakage if trained over EHR.",
                "cite_spans": [
                    {
                        "start": 210,
                        "end": 229,
                        "text": "Carlini et al. 2020",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 627,
                        "end": 648,
                        "text": "(Raffel et al., 2020)",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 652,
                        "end": 678,
                        "text": "GPT-3 (Brown et al., 2020)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Limitations",
                "sec_num": "7"
            },
            {
                "text": "Another limitation is that we have only considered the MIMIC-III corpus here, and the style in which notes are written in this dataset -names appear very infrequently -likely renders it particularly difficult for BERT to recover implicit as-sociations between patient names and conditions. We attempted to address this issue with the semisynthetic Name Insertion variant, where we artificially inserted patient names into every sentence; this did not yield qualitatively different results for most experiments. Nonetheless, it is possible that experiments on EHR datasets from other hospitals (with different distributions over tokens and names) would change the degree to which one is able to recover PHI.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Limitations",
                "sec_num": "7"
            },
            {
                "text": "Finally, these results for BERT may change under different masking strategies -for example, dynamic masking (Liu et al., 2019) or choice of tokenizer. Both of these may affect memorization and extraction method performance.",
                "cite_spans": [
                    {
                        "start": 108,
                        "end": 126,
                        "text": "(Liu et al., 2019)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Limitations",
                "sec_num": "7"
            },
            {
                "text": "We have performed an initial investigation into the degree to which large Transformers pretrained over EHR data might reveal sensitive personal health information (PHI). We ran a battery of experiments in which we attempted to recover such information from BERT model weights estimated over the MIMIC-III dataset (into which we artificially reintroduced patient names, as MIMIC is deidentified). Across these experiments, we found that we were mostly unable to meaningfully expose PHI using simple methods. Moreover, even when we constructed a variant of data in which we prepended patient names to every sentence prior to pretraining BERT, we were still unable to recover sensitive information reliably. Our initial results using more advanced techniques based on generation (Carlini et al. 2020 ; Table 9 ) are intriguing but inconclusive at present.",
                "cite_spans": [
                    {
                        "start": 776,
                        "end": 796,
                        "text": "(Carlini et al. 2020",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 805,
                        "end": 806,
                        "text": "9",
                        "ref_id": "TABREF9"
                    }
                ],
                "eq_spans": [],
                "section": "Conclusions",
                "sec_num": "8"
            },
            {
                "text": "Our results certainly do not rule out the possibility that more advanced methods might reveal PHI. But, these findings do at least suggest that doing so is not trivial. To facilitate further research, we make our experimental setup and baseline probing models available: https://github.com/ elehman16/exposing_patient_data_release. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusions",
                "sec_num": "8"
            },
            {
                "text": "As mentioned previously, we follow most of the hyperparameters stated in (Huang et al., 2019) . The code presented in Huang et al. (2019) accidentally left out all notes under the category 'Nursing/Other'; we added these back in, in addition to any notes that fell under the 'Discharge Summaries' summary category. Our dataset consists of approximately 400M words (ignoring wordpieces). The number of epochs (following Devlin et al. 2019 ) can be calculated as num steps \u2022 batch size \u2022 tokens per seq total number of tokens , which at batch size of 128 and sequence length of 128, comes out to 40 epochs if trained for 1M steps (in the ++ models). For standard models, it comes out to 29 epochs. We used cloud TPUs (v2 and v3) to train our models. All experiments are run on a combination of V100, Titan RTX and Quadro RTX 8000 GPUs.",
                "cite_spans": [
                    {
                        "start": 73,
                        "end": 93,
                        "text": "(Huang et al., 2019)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 118,
                        "end": 137,
                        "text": "Huang et al. (2019)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 419,
                        "end": 437,
                        "text": "Devlin et al. 2019",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Appendix A.1 Training Our BERT Models",
                "sec_num": null
            },
            {
                "text": "In Appendix Figures A1 and A2 , we can see the distribution of ICD-9 and MedCAT conditions across patients. With respect to the ICD-9 codes, there are only 4 conditions that are shared across 10,000+ patients. This number is 32 for MedCAT conditions.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 20,
                        "end": 22,
                        "text": "A1",
                        "ref_id": "FIGREF0"
                    },
                    {
                        "start": 27,
                        "end": 29,
                        "text": "A2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "A.2 Condition Distribution",
                "sec_num": null
            },
            {
                "text": "In addition to the results in Table 2 , we report all Spearman coefficients, relative to the frequency of conditions (in Appendix Table 10 ). We additionally report results for Base++, Large++, and Pubmed-Base models. With respect to AUC, these models all perform worse than the Regular Large model. Additionally, in Appendix Figure A3 , we can see how experiment results change with respect to the length of conditions (owing, as we mentioned in the main text, to complications in computing likelihoods of varying length sequences under MLMs).",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 36,
                        "end": 37,
                        "text": "2",
                        "ref_id": "TABREF2"
                    },
                    {
                        "start": 136,
                        "end": 138,
                        "text": "10",
                        "ref_id": "TABREF11"
                    },
                    {
                        "start": 333,
                        "end": 335,
                        "text": "A3",
                        "ref_id": "FIGREF3"
                    }
                ],
                "eq_spans": [],
                "section": "A.3 Condition Given Name",
                "sec_num": null
            },
            {
                "text": "In addition to the results in Table 11 : AUC and A@10 measures with models given only a masked out condition. We calculate spearman coefficients are given relative to the frequency baseline.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 36,
                        "end": 38,
                        "text": "11",
                        "ref_id": "TABREF0"
                    }
                ],
                "eq_spans": [],
                "section": "A.4 Condition Only",
                "sec_num": null
            },
            {
                "text": "In this experiment, we randomly sample 10,000 patients from our 27,906 patient set (due to computational constraints), of which we keep 5,000 for training and 5,000 for testing. For each of these patient names and every condition in our universe of conditions, we construct the previously specified template and assign it a binary label indicating whether the patient have that condition or not.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A.5 MLP Probing for Names and Conditions",
                "sec_num": null
            },
            {
                "text": "Since the negative class is over-represented by a large amount in this training set, we use downsampling to balance our data. We map each of these templates to their corresponding CLS token embedding. We use the embeddings for templates associated with training set patients to train a MLP classifier implemented in Scikit-Learn (Pedregosa et al., 2011 ) (Note we did not use on a validation set here). We used a hidden layer size of 128 with default hyperparameters.",
                "cite_spans": [
                    {
                        "start": 329,
                        "end": 352,
                        "text": "(Pedregosa et al., 2011",
                        "ref_id": "BIBREF23"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A.5 MLP Probing for Names and Conditions",
                "sec_num": null
            },
            {
                "text": "At test time, for each of the 5000 patients in test set and each condition, we calculate the score using this MLP probe and compute our metrics with respect to the true label associated with that patient-condition pair.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A.5 MLP Probing for Names and Conditions",
                "sec_num": null
            },
            {
                "text": "In this experiment, we samples 50 conditions from each of the 4 frequency bins. For each condition, we trained a probe to distinguish between patients that have that condition vs those that do not. This experiment differs from the preceding fill-in-theblank and probing experiments: Here we compute an AUC for each condition (indicating whether the probe discriminates between patients that have a particular condition and those that do not),whereas in the fill-in-the-blank experiments we computed AUCs per patient.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A.6 Probing for Individual Conditions",
                "sec_num": null
            },
            {
                "text": "For probing individual conditions, we used an MLP classifier implemented in Scikit-Learn (Pedregosa et al., 2011) . We did not evaluate on a validation set. We used a hidden layer size of 128 with default hyperparameters. All experiments were only run once. For the Regular BERT model, we additionally experimented with backpropagating through the BERT weights, but found that this made no difference in predictive performance.",
                "cite_spans": [
                    {
                        "start": 89,
                        "end": 113,
                        "text": "(Pedregosa et al., 2011)",
                        "ref_id": "BIBREF23"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A.6 Probing for Individual Conditions",
                "sec_num": null
            },
            {
                "text": "All versions of Skipgram and CBoW (Mikolov et al., 2013) were trained for 10 epochs using gensim library ( \u0158eh\u016f\u0159ek and Sojka, 2010) , used a vector size of 200, and a window size of 6. We only trained one variant of each W2V model. For BERT models, we used the last layer wordpiece embeddings. For word embedding models, we ran this experiment on whole reidentified patient set, whereas for BERT models, we sampled 10K patients. We report averages over the patients. In addition to the mean-pool collapsing of conditions, we also try 'Max-Pooling' and a variant we label as 'All Pairs Pooling'. We present results from all cosine-similarity experiments in Appendix Ta-",
                "cite_spans": [
                    {
                        "start": 34,
                        "end": 56,
                        "text": "(Mikolov et al., 2013)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 105,
                        "end": 131,
                        "text": "( \u0158eh\u016f\u0159ek and Sojka, 2010)",
                        "ref_id": "BIBREF27"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A.7 Cosine Similarities",
                "sec_num": null
            },
            {
                "text": "Even for deidentified data such as MIMIC(Johnson et al., 2016), one typically must complete a set of trainings before accessing the data, whereas model parameters are typically shared publicly, without any such requirement.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "3 We consider BERT rather than an auto-regressive language model such as GPT-* given the comparatively widespread adoption of the former for biomedical NLP.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "We could have used non-deidentified EHRs from a hospital, but this would preclude releasing the data, hindering reproducibility.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "5 We sampled first and last names from https: //www.ssa.gov/ and https://www.census.gov/ topics/population/genealogy/data/2010_ surnames.html,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "respectively.6 In some sense this bodes well for privacy concerns, given that language models are unlikely to memorize names that they are not exposed to; however, it is unclear how particular this observation is to the MIMIC corpus.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "In this work, we favor the adversary by considering the set of conditions associated with reidentified patients only.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://github.com/kexinhuang12345/ clinicalBERT/blob/master/notebook/ pretrain.ipynb",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://github.com/google-research/ bert",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "This is similar to methods used in work on evaluating language models as knowledge bases(Petroni et al., 2019).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "We do not include age asHuang et al. (2019) does not include digits in pretraining.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "We note that these frequencies are derived from the MIMIC data, which affords an inherent advantage, although it seems likely that condition frequencies derived from other data sources would be similar. We also note that some very common conditions are associated with many patients -see Appendix FiguresA1 and A2-which may effectively 'inflate' the AUCs achieved by the frequency baseline.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "We upsample the minority examples, rather than undersampling as before, because the single-condition models are comparatively quick to train.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "We provide additional results in the Appendix, including results for alternative pooling strategies and results on the original MIMIC dataset; all yield qualitatively similar results.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Which, at least at present, remains the default encoder used in biomedical NLP.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Note that this means that even though samples are generated from a model that cannot have memorized anything in the EHR, using a comparator model that was to re-rank these samples may effectively reveal information.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "We thank Peter Szolovits for early feedback on a draft of this manuscript, and the anonymous NAACL reviewers for their comments.This material is based upon work supported in part by the National Science Foundation under Grant No. 1901117. This Research was also supported with Cloud TPUs from Google's Tensor-Flow Research Cloud (TFRC).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgements",
                "sec_num": null
            },
            {
                "text": "This work has ethical implications relevant to patient privacy. HIPAA prohibits the distribution of PHI, for good reason. Without this type of privacy law, patient information, for example, could be passed on to a lender and be used to deny a patient's application for mortgages or credit card. It is therefore essential that patient information remain private. This raises an important practical concerning methods in NLP that we have sought to address: Does releasing models pretrained over sensitive data pose a privacy risk? While we were unable to reliably recover PHI in this work, we hope that this effort encourages the community to develop more advanced attacks to probe this potential vulnerability. We would still advise researchers to err on the side of caution and only consider releasing models trained over fully deidentified data (e.g. MIMIC). ble 12. The mean pooling results in Table 6 seem to outperform the alternative pooling mechanisms presented here.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 902,
                        "end": 903,
                        "text": "6",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Ethical Considerations",
                "sec_num": null
            },
            {
                "text": "To see if our BERT models are able to recognize the patient names that appear in training data, we train a linear probe on top of names encoded via BERT. We train this Linear Regression classifier using all default parameters from Scikit-Learn (10,000 max steps) (Pedregosa et al., 2011) . We did not evaluate on a validation set. Each experiment was only run once. A.9 Does observing part of a name reveal more information?Similar to the results in Table 8 , we report results on the Base++, Large++, and Pubmed-Base models (Appendix Table 13 ). We find no significant difference between these results and the ones reported in Table 8 .",
                "cite_spans": [
                    {
                        "start": 263,
                        "end": 287,
                        "text": "(Pedregosa et al., 2011)",
                        "ref_id": "BIBREF23"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 456,
                        "end": 457,
                        "text": "8",
                        "ref_id": null
                    },
                    {
                        "start": 541,
                        "end": 543,
                        "text": "13",
                        "ref_id": null
                    },
                    {
                        "start": 634,
                        "end": 635,
                        "text": "8",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "A.8 Probing for Names",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Exploring the privacypreserving properties of word embeddings: Algorithmic validation study",
                "authors": [
                    {
                        "first": "Mohamed",
                        "middle": [],
                        "last": "Abdalla",
                        "suffix": ""
                    },
                    {
                        "first": "Moustafa",
                        "middle": [],
                        "last": "Abdalla",
                        "suffix": ""
                    },
                    {
                        "first": "Graeme",
                        "middle": [],
                        "last": "Hirst",
                        "suffix": ""
                    },
                    {
                        "first": "Frank",
                        "middle": [],
                        "last": "Rudzicz",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "J Med Internet Res",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mohamed Abdalla, Moustafa Abdalla, Graeme Hirst, and Frank Rudzicz. 2020. Exploring the privacy- preserving properties of word embeddings: Algo- rithmic validation study. J Med Internet Res.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Understanding intermediate layers using linear classifier probes",
                "authors": [
                    {
                        "first": "Guillaume",
                        "middle": [],
                        "last": "Alain",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "The 5th International Conference on Learning Representations",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Guillaume Alain and Yoshua Bengio. 2017. Under- standing intermediate layers using linear classifier probes. In The 5th International Conference on Learning Representations (ICLR-17).",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Publicly available clinical BERT embeddings",
                "authors": [
                    {
                        "first": "Emily",
                        "middle": [],
                        "last": "Alsentzer",
                        "suffix": ""
                    },
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Murphy",
                        "suffix": ""
                    },
                    {
                        "first": "William",
                        "middle": [],
                        "last": "Boag",
                        "suffix": ""
                    },
                    {
                        "first": "Wei-Hung",
                        "middle": [],
                        "last": "Weng",
                        "suffix": ""
                    },
                    {
                        "first": "Di",
                        "middle": [],
                        "last": "Jin",
                        "suffix": ""
                    },
                    {
                        "first": "Tristan",
                        "middle": [],
                        "last": "Naumann",
                        "suffix": ""
                    },
                    {
                        "first": "Matthew",
                        "middle": [],
                        "last": "Mcdermott",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 2nd Clinical Natural Language Processing Workshop",
                "volume": "",
                "issue": "",
                "pages": "72--78",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/W19-1909"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Emily Alsentzer, John Murphy, William Boag, Wei- Hung Weng, Di Jin, Tristan Naumann, and Matthew McDermott. 2019. Publicly available clinical BERT embeddings. In Proceedings of the 2nd Clinical Natural Language Processing Workshop, pages 72- 78, Minneapolis, Minnesota, USA. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Privacy-preserving distributed deep learning for clinical data",
                "authors": [
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Brett",
                        "suffix": ""
                    },
                    {
                        "first": "William",
                        "middle": [],
                        "last": "Beaulieu-Jones",
                        "suffix": ""
                    },
                    {
                        "first": "Samuel",
                        "middle": [
                            "G"
                        ],
                        "last": "Yuan",
                        "suffix": ""
                    },
                    {
                        "first": "Z",
                        "middle": [],
                        "last": "Finlayson",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Brett K. Beaulieu-Jones, William Yuan, Samuel G. Finlayson, and Z. Wu. 2018. Privacy-preserving distributed deep learning for clinical data. ArXiv, abs/1812.01484.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "The unified medical language system (umls): integrating biomedical terminology",
                "authors": [
                    {
                        "first": "O",
                        "middle": [],
                        "last": "Bodenreider",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "",
                "volume": "32",
                "issue": "",
                "pages": "267--270",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "O. Bodenreider. 2004. The unified medical language system (umls): integrating biomedical terminology. Nucleic acids research, 32 Database issue:D267-70.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Inducing relational knowledge from bert",
                "authors": [
                    {
                        "first": "Zied",
                        "middle": [],
                        "last": "Bouraoui",
                        "suffix": ""
                    },
                    {
                        "first": "Jos\u00e9",
                        "middle": [],
                        "last": "Camacho-Collados",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Schockaert",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "AAAI",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zied Bouraoui, Jos\u00e9 Camacho-Collados, and S. Schockaert. 2020. Inducing relational knowledge from bert. In AAAI.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Language models are few-shot learners",
                "authors": [
                    {
                        "first": "Tom",
                        "middle": [],
                        "last": "Brown",
                        "suffix": ""
                    },
                    {
                        "first": "Benjamin",
                        "middle": [],
                        "last": "Mann",
                        "suffix": ""
                    },
                    {
                        "first": "Nick",
                        "middle": [],
                        "last": "Ryder",
                        "suffix": ""
                    },
                    {
                        "first": "Melanie",
                        "middle": [],
                        "last": "Subbiah",
                        "suffix": ""
                    },
                    {
                        "first": "Jared",
                        "middle": [
                            "D"
                        ],
                        "last": "Kaplan",
                        "suffix": ""
                    },
                    {
                        "first": "Prafulla",
                        "middle": [],
                        "last": "Dhariwal",
                        "suffix": ""
                    },
                    {
                        "first": "Arvind",
                        "middle": [],
                        "last": "Neelakantan",
                        "suffix": ""
                    },
                    {
                        "first": "Pranav",
                        "middle": [],
                        "last": "Shyam",
                        "suffix": ""
                    },
                    {
                        "first": "Girish",
                        "middle": [],
                        "last": "Sastry",
                        "suffix": ""
                    },
                    {
                        "first": "Amanda",
                        "middle": [],
                        "last": "Askell",
                        "suffix": ""
                    },
                    {
                        "first": "Sandhini",
                        "middle": [],
                        "last": "Agarwal",
                        "suffix": ""
                    },
                    {
                        "first": "Ariel",
                        "middle": [],
                        "last": "Herbert-Voss",
                        "suffix": ""
                    },
                    {
                        "first": "Gretchen",
                        "middle": [],
                        "last": "Krueger",
                        "suffix": ""
                    },
                    {
                        "first": "Tom",
                        "middle": [],
                        "last": "Henighan",
                        "suffix": ""
                    },
                    {
                        "first": "Rewon",
                        "middle": [],
                        "last": "Child",
                        "suffix": ""
                    },
                    {
                        "first": "Aditya",
                        "middle": [],
                        "last": "Ramesh",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Ziegler",
                        "suffix": ""
                    },
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Clemens",
                        "middle": [],
                        "last": "Winter",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Hesse",
                        "suffix": ""
                    },
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Eric",
                        "middle": [],
                        "last": "Sigler",
                        "suffix": ""
                    },
                    {
                        "first": "Mateusz",
                        "middle": [],
                        "last": "Litwin",
                        "suffix": ""
                    },
                    {
                        "first": "Scott",
                        "middle": [],
                        "last": "Gray",
                        "suffix": ""
                    },
                    {
                        "first": "Benjamin",
                        "middle": [],
                        "last": "Chess",
                        "suffix": ""
                    },
                    {
                        "first": "Jack",
                        "middle": [],
                        "last": "Clark",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [],
                        "last": "Berner",
                        "suffix": ""
                    },
                    {
                        "first": "Sam",
                        "middle": [],
                        "last": "Mc-Candlish",
                        "suffix": ""
                    },
                    {
                        "first": "Alec",
                        "middle": [],
                        "last": "Radford",
                        "suffix": ""
                    },
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    },
                    {
                        "first": "Dario",
                        "middle": [],
                        "last": "Amodei",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "33",
                "issue": "",
                "pages": "1877--1901",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert- Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mc- Candlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Pro- cessing Systems, volume 33, pages 1877-1901. Cur- ran Associates, Inc.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Extracting training data from large language models",
                "authors": [
                    {
                        "first": "N",
                        "middle": [],
                        "last": "Carlini",
                        "suffix": ""
                    },
                    {
                        "first": "Florian",
                        "middle": [],
                        "last": "Tram\u00e8r",
                        "suffix": ""
                    },
                    {
                        "first": "Eric",
                        "middle": [],
                        "last": "Wallace",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Jagielski",
                        "suffix": ""
                    },
                    {
                        "first": "Ariel",
                        "middle": [],
                        "last": "Herbert-Voss",
                        "suffix": ""
                    },
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Roberts",
                        "suffix": ""
                    },
                    {
                        "first": "Tom",
                        "middle": [],
                        "last": "Brown",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Song",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "\u00dalfar Erlingsson, Alina Oprea, and Colin Raffel",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "N. Carlini, Florian Tram\u00e8r, Eric Wallace, M. Jagielski, Ariel Herbert-Voss, K. Lee, A. Roberts, Tom Brown, D. Song, \u00dalfar Erlingsson, Alina Oprea, and Colin Raffel. 2020. Extracting training data from large language models. ArXiv, abs/2012.07805.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
                "authors": [
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Devlin",
                        "suffix": ""
                    },
                    {
                        "first": "Ming-Wei",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "Kenton",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Kristina",
                        "middle": [],
                        "last": "Toutanova",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "NAACL-HLT",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "J. Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirec- tional transformers for language understanding. In NAACL-HLT.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Model inversion attacks that exploit confidence information and basic countermeasures",
                "authors": [
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Matt Fredrikson",
                        "suffix": ""
                    },
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Jha",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Ristenpart",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "CCS '15",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Matt Fredrikson, S. Jha, and T. Ristenpart. 2015. Model inversion attacks that exploit confidence in- formation and basic countermeasures. In CCS '15.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Domainspecific language model pretraining for biomedical natural language processing",
                "authors": [
                    {
                        "first": "Yu",
                        "middle": [],
                        "last": "Gu",
                        "suffix": ""
                    },
                    {
                        "first": "Robert",
                        "middle": [],
                        "last": "Tinn",
                        "suffix": ""
                    },
                    {
                        "first": "Hao",
                        "middle": [],
                        "last": "Cheng",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Lucas",
                        "suffix": ""
                    },
                    {
                        "first": "Naoto",
                        "middle": [],
                        "last": "Usuyama",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaodong",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Tristan",
                        "middle": [],
                        "last": "Naumann",
                        "suffix": ""
                    },
                    {
                        "first": "Jianfeng",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Hoifung",
                        "middle": [],
                        "last": "Poon",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, and Hoifung Poon. 2020. Domain- specific language model pretraining for biomedical natural language processing.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Language models as knowledge bases: On entity representations, storage capacity, and paraphrased queries",
                "authors": [
                    {
                        "first": "Benjamin",
                        "middle": [],
                        "last": "Heinzerling",
                        "suffix": ""
                    },
                    {
                        "first": "Kentaro",
                        "middle": [],
                        "last": "Inui",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Benjamin Heinzerling and Kentaro Inui. 2020. Lan- guage models as knowledge bases: On entity representations, storage capacity, and paraphrased queries. ArXiv, abs/2008.09036.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "spaCy: Industrial-strength Natural Language Processing in Python",
                "authors": [
                    {
                        "first": "Matthew",
                        "middle": [],
                        "last": "Honnibal",
                        "suffix": ""
                    },
                    {
                        "first": "Ines",
                        "middle": [],
                        "last": "Montani",
                        "suffix": ""
                    },
                    {
                        "first": "Sofie",
                        "middle": [],
                        "last": "Van Landeghem",
                        "suffix": ""
                    },
                    {
                        "first": "Adriane",
                        "middle": [],
                        "last": "Boyd",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.5281/zenodo.1212303"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Matthew Honnibal, Ines Montani, Sofie Van Lan- deghem, and Adriane Boyd. 2020. spaCy: Industrial-strength Natural Language Processing in Python.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Clinicalbert: Modeling clinical notes and predicting hospital readmission",
                "authors": [
                    {
                        "first": "Kexin",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Jaan",
                        "middle": [],
                        "last": "Altosaar",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Ranganath",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kexin Huang, Jaan Altosaar, and R. Ranganath. 2019. Clinicalbert: Modeling clinical notes and predicting hospital readmission. ArXiv, abs/1904.05342.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Haibo Ding, and Graham Neubig. 2020a. X-FACTR: Multilingual factual knowledge retrieval from pretrained language models",
                "authors": [
                    {
                        "first": "Zhengbao",
                        "middle": [],
                        "last": "Jiang",
                        "suffix": ""
                    },
                    {
                        "first": "Antonios",
                        "middle": [],
                        "last": "Anastasopoulos",
                        "suffix": ""
                    },
                    {
                        "first": "Jun",
                        "middle": [],
                        "last": "Araki",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zhengbao Jiang, Antonios Anastasopoulos, Jun Araki, Haibo Ding, and Graham Neubig. 2020a. X- FACTR: Multilingual factual knowledge retrieval from pretrained language models. In Conference on Empirical Methods in Natural Language Processing (EMNLP), Online.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "How can we know what language models know?",
                "authors": [
                    {
                        "first": "Zhengbao",
                        "middle": [],
                        "last": "Jiang",
                        "suffix": ""
                    },
                    {
                        "first": "Frank",
                        "middle": [
                            "F"
                        ],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Jun",
                        "middle": [],
                        "last": "Araki",
                        "suffix": ""
                    },
                    {
                        "first": "Graham",
                        "middle": [],
                        "last": "Neubig",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Transactions of the Association for Computational Linguistics",
                "volume": "8",
                "issue": "",
                "pages": "423--438",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. 2020b. How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423-438.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Deidentification of free-text medical records using pre-trained bidirectional transformers",
                "authors": [
                    {
                        "first": "E",
                        "middle": [
                            "W"
                        ],
                        "last": "Alistair",
                        "suffix": ""
                    },
                    {
                        "first": "Lucas",
                        "middle": [],
                        "last": "Johnson",
                        "suffix": ""
                    },
                    {
                        "first": "Tom",
                        "middle": [
                            "J"
                        ],
                        "last": "Bulgarelli",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Pollard",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the ACM Conference on Health, Inference, and Learning, CHIL '20",
                "volume": "",
                "issue": "",
                "pages": "214--221",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alistair E. W. Johnson, Lucas Bulgarelli, and Tom J. Pollard. 2020. Deidentification of free-text medical records using pre-trained bidirectional transformers. In Proceedings of the ACM Conference on Health, Inference, and Learning, CHIL '20, page 214-221, New York, NY, USA. Association for Computing Machinery.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Mimic-iii, a freely accessible critical care database",
                "authors": [
                    {
                        "first": "E",
                        "middle": [
                            "W"
                        ],
                        "last": "Alistair",
                        "suffix": ""
                    },
                    {
                        "first": "Tom",
                        "middle": [
                            "J"
                        ],
                        "last": "Johnson",
                        "suffix": ""
                    },
                    {
                        "first": "Lu",
                        "middle": [],
                        "last": "Pollard",
                        "suffix": ""
                    },
                    {
                        "first": "H",
                        "middle": [],
                        "last": "Shen",
                        "suffix": ""
                    },
                    {
                        "first": "Mengling",
                        "middle": [],
                        "last": "Lehman Li-Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Mohammad",
                        "middle": [],
                        "last": "Feng",
                        "suffix": ""
                    },
                    {
                        "first": "Benjamin",
                        "middle": [],
                        "last": "Ghassemi",
                        "suffix": ""
                    },
                    {
                        "first": "Peter",
                        "middle": [],
                        "last": "Moody",
                        "suffix": ""
                    },
                    {
                        "first": "Leo",
                        "middle": [],
                        "last": "Szolovits",
                        "suffix": ""
                    },
                    {
                        "first": "Roger",
                        "middle": [
                            "G"
                        ],
                        "last": "Anthony Celi",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Mark",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Scientific data",
                "volume": "3",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alistair EW Johnson, Tom J Pollard, Lu Shen, H Lehman Li-wei, Mengling Feng, Moham- mad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. 2016. Mimic-iii, a freely accessible critical care database. Scientific data, 3:160035.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Multi-domain clinical natural language processing with medcat: the medical concept annotation toolkit",
                "authors": [
                    {
                        "first": "Zeljko",
                        "middle": [],
                        "last": "Kraljevic",
                        "suffix": ""
                    },
                    {
                        "first": "Thomas",
                        "middle": [],
                        "last": "Searle",
                        "suffix": ""
                    },
                    {
                        "first": "Anthony",
                        "middle": [],
                        "last": "Shek",
                        "suffix": ""
                    },
                    {
                        "first": "Lukasz",
                        "middle": [],
                        "last": "Roguski",
                        "suffix": ""
                    },
                    {
                        "first": "Kawsar",
                        "middle": [],
                        "last": "Noor",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Bean",
                        "suffix": ""
                    },
                    {
                        "first": "Aurelie",
                        "middle": [],
                        "last": "Mascio",
                        "suffix": ""
                    },
                    {
                        "first": "Leilei",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    },
                    {
                        "first": "Amos",
                        "middle": [
                            "A"
                        ],
                        "last": "Folarin",
                        "suffix": ""
                    },
                    {
                        "first": "Angus",
                        "middle": [],
                        "last": "Roberts",
                        "suffix": ""
                    },
                    {
                        "first": "Rebecca",
                        "middle": [],
                        "last": "Bendayan",
                        "suffix": ""
                    },
                    {
                        "first": "Robert",
                        "middle": [],
                        "last": "Mark P Richardson",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Stewart",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Anoop",
                        "suffix": ""
                    },
                    {
                        "first": "Wai",
                        "middle": [
                            "Keong"
                        ],
                        "last": "Shah",
                        "suffix": ""
                    },
                    {
                        "first": "Zina",
                        "middle": [],
                        "last": "Wong",
                        "suffix": ""
                    },
                    {
                        "first": "James",
                        "middle": [
                            "T"
                        ],
                        "last": "Ibrahim",
                        "suffix": ""
                    },
                    {
                        "first": "Richard Jb",
                        "middle": [],
                        "last": "Teo",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Dobson",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zeljko Kraljevic, Thomas Searle, Anthony Shek, Lukasz Roguski, Kawsar Noor, Daniel Bean, Au- relie Mascio, Leilei Zhu, Amos A Folarin, Angus Roberts, Rebecca Bendayan, Mark P Richardson, Robert Stewart, Anoop D Shah, Wai Keong Wong, Zina Ibrahim, James T Teo, and Richard JB Dob- son. 2020. Multi-domain clinical natural language processing with medcat: the medical concept anno- tation toolkit.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Roberta: A robustly optimized bert pretraining approach",
                "authors": [
                    {
                        "first": "Yinhan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Myle",
                        "middle": [],
                        "last": "Ott",
                        "suffix": ""
                    },
                    {
                        "first": "Naman",
                        "middle": [],
                        "last": "Goyal",
                        "suffix": ""
                    },
                    {
                        "first": "Jingfei",
                        "middle": [],
                        "last": "Du",
                        "suffix": ""
                    },
                    {
                        "first": "Mandar",
                        "middle": [],
                        "last": "Joshi",
                        "suffix": ""
                    },
                    {
                        "first": "Danqi",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Omer",
                        "middle": [],
                        "last": "Levy",
                        "suffix": ""
                    },
                    {
                        "first": "Mike",
                        "middle": [],
                        "last": "Lewis",
                        "suffix": ""
                    },
                    {
                        "first": "Luke",
                        "middle": [],
                        "last": "Zettlemoyer",
                        "suffix": ""
                    },
                    {
                        "first": "Veselin",
                        "middle": [],
                        "last": "Stoyanov",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1907.11692"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man- dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining ap- proach. arXiv preprint arXiv:1907.11692.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Reply to: Transparency and reproducibility in artificial intelligence",
                "authors": [
                    {
                        "first": "Scott",
                        "middle": [],
                        "last": "Mayer Mckinney",
                        "suffix": ""
                    },
                    {
                        "first": "Alan",
                        "middle": [],
                        "last": "Karthikesalingam",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Tse",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "J"
                        ],
                        "last": "Kelly",
                        "suffix": ""
                    },
                    {
                        "first": "Yun",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Greg",
                        "middle": [
                            "S"
                        ],
                        "last": "Corrado",
                        "suffix": ""
                    },
                    {
                        "first": "Shravya",
                        "middle": [],
                        "last": "Shetty",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Nature",
                "volume": "586",
                "issue": "7829",
                "pages": "17--E18",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Scott Mayer McKinney, Alan Karthikesalingam, Daniel Tse, Christopher J Kelly, Yun Liu, Greg S Corrado, and Shravya Shetty. 2020. Reply to: Transparency and reproducibility in artificial intel- ligence. Nature, 586(7829):E17-E18.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Efficient estimation of word representations in vector space",
                "authors": [
                    {
                        "first": "Tomas",
                        "middle": [],
                        "last": "Mikolov",
                        "suffix": ""
                    },
                    {
                        "first": "Kai",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "G",
                        "middle": [],
                        "last": "Corrado",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Dean",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "ICLR",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tomas Mikolov, Kai Chen, G. Corrado, and J. Dean. 2013. Efficient estimation of word representations in vector space. In ICLR.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Automated de-identification of free-text medical records",
                "authors": [
                    {
                        "first": "Ishna",
                        "middle": [],
                        "last": "Neamatullah",
                        "suffix": ""
                    },
                    {
                        "first": "Margaret",
                        "middle": [],
                        "last": "Douglass",
                        "suffix": ""
                    },
                    {
                        "first": "Li-Wei",
                        "middle": [],
                        "last": "Lehman",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Reisner",
                        "suffix": ""
                    },
                    {
                        "first": "Mauricio",
                        "middle": [],
                        "last": "Villarroel",
                        "suffix": ""
                    },
                    {
                        "first": "William",
                        "middle": [],
                        "last": "Long",
                        "suffix": ""
                    },
                    {
                        "first": "Peter",
                        "middle": [],
                        "last": "Szolovits",
                        "suffix": ""
                    },
                    {
                        "first": "George",
                        "middle": [],
                        "last": "Moody",
                        "suffix": ""
                    },
                    {
                        "first": "Roger",
                        "middle": [],
                        "last": "Mark",
                        "suffix": ""
                    },
                    {
                        "first": "Gari",
                        "middle": [],
                        "last": "Clifford",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "BMC medical informatics and decision making",
                "volume": "8",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ishna Neamatullah, Margaret Douglass, Li-wei Lehman, Andrew Reisner, Mauricio Villarroel, William Long, Peter Szolovits, George Moody, Roger Mark, and Gari Clifford. 2008. Automated de-identification of free-text medical records. BMC medical informatics and decision making, 8:32.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Scikit-learn: Machine learning in Python",
                "authors": [
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Pedregosa",
                        "suffix": ""
                    },
                    {
                        "first": "G",
                        "middle": [],
                        "last": "Varoquaux",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Gramfort",
                        "suffix": ""
                    },
                    {
                        "first": "V",
                        "middle": [],
                        "last": "Michel",
                        "suffix": ""
                    },
                    {
                        "first": "B",
                        "middle": [],
                        "last": "Thirion",
                        "suffix": ""
                    },
                    {
                        "first": "O",
                        "middle": [],
                        "last": "Grisel",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Blondel",
                        "suffix": ""
                    },
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Prettenhofer",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Weiss",
                        "suffix": ""
                    },
                    {
                        "first": "V",
                        "middle": [],
                        "last": "Dubourg",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Vanderplas",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Passos",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Cournapeau",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Brucher",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Perrot",
                        "suffix": ""
                    },
                    {
                        "first": "E",
                        "middle": [],
                        "last": "Duchesnay",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Journal of Machine Learning Research",
                "volume": "12",
                "issue": "",
                "pages": "2825--2830",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten- hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas- sos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825-2830.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Language models as knowledge bases?",
                "authors": [
                    {
                        "first": "Fabio",
                        "middle": [],
                        "last": "Petroni",
                        "suffix": ""
                    },
                    {
                        "first": "Tim",
                        "middle": [],
                        "last": "Rockt\u00e4schel",
                        "suffix": ""
                    },
                    {
                        "first": "Sebastian",
                        "middle": [],
                        "last": "Riedel",
                        "suffix": ""
                    },
                    {
                        "first": "Patrick",
                        "middle": [],
                        "last": "Lewis",
                        "suffix": ""
                    },
                    {
                        "first": "Anton",
                        "middle": [],
                        "last": "Bakhtin",
                        "suffix": ""
                    },
                    {
                        "first": "Yuxiang",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Alexander",
                        "middle": [],
                        "last": "Miller",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "volume": "",
                "issue": "",
                "pages": "2463--2473",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/D19-1250"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Fabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. Language models as knowledge bases? In Proceedings of the 2019 Con- ference on Empirical Methods in Natural Language Processing and the 9th International Joint Confer- ence on Natural Language Processing (EMNLP- IJCNLP), pages 2463-2473, Hong Kong, China. As- sociation for Computational Linguistics.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Language models are unsupervised multitask learners",
                "authors": [
                    {
                        "first": "Alec",
                        "middle": [],
                        "last": "Radford",
                        "suffix": ""
                    },
                    {
                        "first": "Jeff",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Rewon",
                        "middle": [],
                        "last": "Child",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Luan",
                        "suffix": ""
                    },
                    {
                        "first": "Dario",
                        "middle": [],
                        "last": "Amodei",
                        "suffix": ""
                    },
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
                "authors": [
                    {
                        "first": "Colin",
                        "middle": [],
                        "last": "Raffel",
                        "suffix": ""
                    },
                    {
                        "first": "Noam",
                        "middle": [],
                        "last": "Shazeer",
                        "suffix": ""
                    },
                    {
                        "first": "Adam",
                        "middle": [],
                        "last": "Roberts",
                        "suffix": ""
                    },
                    {
                        "first": "Katherine",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Sharan",
                        "middle": [],
                        "last": "Narang",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Matena",
                        "suffix": ""
                    },
                    {
                        "first": "Yanqi",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Peter",
                        "middle": [
                            "J"
                        ],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Journal of Machine Learning Research",
                "volume": "21",
                "issue": "140",
                "pages": "1--67",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text trans- former. Journal of Machine Learning Research, 21(140):1-67.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Software Framework for Topic Modelling with Large Corpora",
                "authors": [
                    {
                        "first": "Radim",
                        "middle": [],
                        "last": "\u0158eh\u016f\u0159ek",
                        "suffix": ""
                    },
                    {
                        "first": "Petr",
                        "middle": [],
                        "last": "Sojka",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks",
                "volume": "",
                "issue": "",
                "pages": "45--50",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Radim \u0158eh\u016f\u0159ek and Petr Sojka. 2010. Software Frame- work for Topic Modelling with Large Corpora. In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45-50, Val- letta, Malta. ELRA. http://is.muni.cz/ publication/884893/en.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "How much knowledge can you pack into the parameters of a language model",
                "authors": [
                    {
                        "first": "Adam",
                        "middle": [],
                        "last": "Roberts",
                        "suffix": ""
                    },
                    {
                        "first": "Colin",
                        "middle": [],
                        "last": "Raffel",
                        "suffix": ""
                    },
                    {
                        "first": "Noam",
                        "middle": [],
                        "last": "Shazeer",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "EMNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Adam Roberts, Colin Raffel, and Noam Shazeer. 2020. How much knowledge can you pack into the param- eters of a language model? In EMNLP.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Masked language model scoring",
                "authors": [
                    {
                        "first": "Julian",
                        "middle": [],
                        "last": "Salazar",
                        "suffix": ""
                    },
                    {
                        "first": "Davis",
                        "middle": [],
                        "last": "Liang",
                        "suffix": ""
                    },
                    {
                        "first": "Toan",
                        "middle": [
                            "Q"
                        ],
                        "last": "Nguyen",
                        "suffix": ""
                    },
                    {
                        "first": "Katrin",
                        "middle": [],
                        "last": "Kirchhoff",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "2699--2712",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2020.acl-main.240"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Julian Salazar, Davis Liang, Toan Q. Nguyen, and Ka- trin Kirchhoff. 2020. Masked language model scor- ing. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2699-2712, Online. Association for Compu- tational Linguistics.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "Ml-leaks: Model and data independent membership inference attacks and defenses on machine learning models",
                "authors": [
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Salem",
                        "suffix": ""
                    },
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Humbert",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Fritz",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Backes",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "A. Salem, Y. Zhang, M. Humbert, M. Fritz, and M. Backes. 2019. Ml-leaks: Model and data in- dependent membership inference attacks and de- fenses on machine learning models. ArXiv, abs/1806.01246.",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "The natural auditor: How to tell if someone used your words to train their model",
                "authors": [
                    {
                        "first": "Congzheng",
                        "middle": [],
                        "last": "Song",
                        "suffix": ""
                    },
                    {
                        "first": "Vitaly",
                        "middle": [],
                        "last": "Shmatikov",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Congzheng Song and Vitaly Shmatikov. 2018. The nat- ural auditor: How to tell if someone used your words to train their model. ArXiv, abs/1811.00513.",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "Challenges in synthesizing surrogate phi in narrative emrs",
                "authors": [
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Stubbs",
                        "suffix": ""
                    },
                    {
                        "first": "\u00d6zlem",
                        "middle": [],
                        "last": "Uzuner",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [],
                        "last": "Kotfila",
                        "suffix": ""
                    },
                    {
                        "first": "I",
                        "middle": [],
                        "last": "Goldstein",
                        "suffix": ""
                    },
                    {
                        "first": "Peter",
                        "middle": [],
                        "last": "Szolovits",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Medical Data Privacy Handbook",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "A. Stubbs, \u00d6zlem Uzuner, Christopher Kotfila, I. Gold- stein, and Peter Szolovits. 2015. Challenges in syn- thesizing surrogate phi in narrative emrs. In Medical Data Privacy Handbook.",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "BERT has a mouth, and it must speak: BERT as a Markov random field language model",
                "authors": [
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Kyunghyun",
                        "middle": [],
                        "last": "Cho",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation",
                "volume": "",
                "issue": "",
                "pages": "30--36",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/W19-2304"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Alex Wang and Kyunghyun Cho. 2019. BERT has a mouth, and it must speak: BERT as a Markov ran- dom field language model. In Proceedings of the Workshop on Methods for Optimizing and Evalu- ating Neural Language Generation, pages 30-36, Minneapolis, Minnesota. Association for Computa- tional Linguistics.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 1: Overview of this work. We explore initial strategies intended to extract sensitive information from BERT model weights estimated over the notes in Electronic Health Records (EHR) data.",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Figure A1: A distribution of ICD-9 codes and how many patients (of the 27K) have each condition. All bin end values are not inclusive.",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "Figure A2: A distribution of MedCAT codes and how many patients (of the 27K) have each condition. All bin end values are not inclusive.",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF3": {
                "num": null,
                "text": "Figure A3: Per-length performance of both ICD-9 and MedCAT labels for the 'masked conditon' (only) experiments. A bin length of k contains conditions comprising k token pieces.",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table><tr><td>Model Name</td><td colspan=\"2\">Starts from</td><td colspan=\"2\">Train iterations (seqlen 128) Train iterations (seqlen 512)</td></tr><tr><td>Regular Base</td><td colspan=\"2\">BERT Base</td><td>300K</td><td>100K</td></tr><tr><td>Regular Large</td><td colspan=\"2\">BERT Large</td><td>300K</td><td>100K</td></tr><tr><td>Regular Base++</td><td colspan=\"2\">BERT Base</td><td>1M</td><td>-</td></tr><tr><td>Regular Large++</td><td colspan=\"2\">BERT Large</td><td>1M</td><td>-</td></tr><tr><td colspan=\"3\">Regular Pubmed-base PubmedBERT (Gu et al., 2020)</td><td>1M</td><td>-</td></tr><tr><td>Name Insertion</td><td colspan=\"2\">BERT base</td><td>300K</td><td>100K</td></tr><tr><td>Template Only</td><td colspan=\"2\">BERT base</td><td>300K</td><td>100K</td></tr><tr><td>Model</td><td>AUC</td><td>A@10</td><td/></tr><tr><td>ICD9</td><td/><td/><td/></tr><tr><td colspan=\"3\">Frequency Baseline 0.926 0.134</td><td/></tr><tr><td>Regular Base</td><td colspan=\"2\">0.614 0.056</td><td/></tr><tr><td>Regular Large</td><td colspan=\"2\">0.654 0.063</td><td/></tr><tr><td>Name Insertion</td><td colspan=\"2\">0.616 0.057</td><td/></tr><tr><td>Template Only</td><td colspan=\"2\">0.614 0.050</td><td/></tr><tr><td>MedCAT</td><td/><td/><td/></tr><tr><td colspan=\"3\">Frequency Baseline 0.933 0.241</td><td/></tr><tr><td>Regular Base</td><td colspan=\"2\">0.529 0.109</td><td/></tr><tr><td>Regular Large</td><td colspan=\"2\">0.667 0.108</td><td/></tr><tr><td>Name Insertion</td><td colspan=\"2\">0.541 0.112</td><td/></tr><tr><td>Template Only</td><td colspan=\"2\">0.784 0.160</td><td/></tr></table>",
                "type_str": "table",
                "text": "BERT model and training configurations considered in this work. Train iterations are over notes from the MIMIC-III EHR dataset.",
                "html": null,
                "num": null
            },
            "TABREF1": {
                "content": "<table><tr><td>Model</td><td>AUC</td><td colspan=\"2\">A@10 Spearman</td></tr><tr><td>ICD-9</td><td/><td/></tr><tr><td>Regular Base</td><td colspan=\"2\">0.496 0.042</td><td>0.114</td></tr><tr><td>Regular Large</td><td colspan=\"2\">0.560 0.049</td><td>0.109</td></tr><tr><td colspan=\"3\">Name Insertion 0.483 0.042</td><td>0.100</td></tr><tr><td>Template Only</td><td colspan=\"2\">0.615 0.056</td><td>0.240</td></tr><tr><td>MedCAT</td><td/><td/></tr><tr><td>Regular Base</td><td colspan=\"2\">0.472 0.110</td><td>0.218</td></tr><tr><td>Regular Large</td><td colspan=\"2\">0.530 0.113</td><td>0.173</td></tr><tr><td colspan=\"3\">Name Insertion 0.473 0.102</td><td>0.156</td></tr><tr><td>Template Only</td><td colspan=\"2\">0.595 0.110</td><td>0.248</td></tr></table>",
                "type_str": "table",
                "text": "Average AUC, A@10 and Spearman correlations over conditions binned by description length. Correlations are w/r/t empirical condition frequencies.",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table><tr><td/><td colspan=\"4\">Name + Condition Condition Only</td></tr><tr><td>Model</td><td>AUC</td><td>A@10</td><td>AUC</td><td>A@10</td></tr><tr><td>ICD-9</td><td/><td/><td/><td/></tr><tr><td>Standard Base</td><td colspan=\"2\">0.860 0.131</td><td colspan=\"2\">0.917 0.182</td></tr><tr><td>Regular Base</td><td colspan=\"2\">0.917 0.148</td><td colspan=\"2\">0.932 0.195</td></tr><tr><td>Regular Large</td><td colspan=\"2\">0.909 0.153</td><td colspan=\"2\">0.922 0.186</td></tr><tr><td colspan=\"3\">Name Insertion 0.871 0.095</td><td colspan=\"2\">0.932 0.204</td></tr><tr><td>MedCAT</td><td/><td/><td/><td/></tr><tr><td>Standard Base</td><td colspan=\"2\">0.918 0.355</td><td colspan=\"2\">0.954 0.464</td></tr><tr><td>Regular Base</td><td colspan=\"2\">0.946 0.431</td><td colspan=\"2\">0.956 0.508</td></tr><tr><td>Regular Large</td><td colspan=\"2\">0.942 0.393</td><td colspan=\"2\">0.955 0.475</td></tr><tr><td colspan=\"3\">Name Insertion 0.925 0.365</td><td colspan=\"2\">0.950 0.431</td></tr></table>",
                "type_str": "table",
                "text": "",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Probing results using BERT-encoded CLS tokens on the test set. We use 10,000 patients out of 27,906 due to time constraints. Standard Base is the original BERT base model.",
                "html": null,
                "num": null
            },
            "TABREF5": {
                "content": "<table><tr><td>Model</td><td>Mean</td><td>Std.</td></tr><tr><td>ICD-9</td><td/><td/></tr><tr><td>Regular Base</td><td colspan=\"2\">-0.010 0.019</td></tr><tr><td>Regular Large</td><td colspan=\"2\">-0.045 0.052</td></tr><tr><td>SkipGram Base</td><td colspan=\"2\">0.004 0.050</td></tr><tr><td>CBoW Base</td><td colspan=\"2\">0.008 0.035</td></tr><tr><td>BERT Name Insertion</td><td colspan=\"2\">-0.007 0.017</td></tr><tr><td>SkipGram Name Insertion</td><td colspan=\"2\">0.019 0.040</td></tr><tr><td>CBoW Name Insertion</td><td colspan=\"2\">0.017 0.043</td></tr><tr><td>MedCAT</td><td/><td/></tr><tr><td>Regular Base</td><td colspan=\"2\">-0.037 0.015</td></tr><tr><td>Regular Large</td><td colspan=\"2\">-0.055 0.029</td></tr><tr><td>SkipGram Base</td><td colspan=\"2\">-0.011 0.024</td></tr><tr><td>CBoW Base</td><td colspan=\"2\">-0.001 0.022</td></tr><tr><td>BERT Name Insertion</td><td colspan=\"2\">-0.027 0.013</td></tr><tr><td>SkipGram Name Insertion</td><td colspan=\"2\">0.013 0.024</td></tr><tr><td>CBoW Name Insertion</td><td colspan=\"2\">0.015 0.026</td></tr></table>",
                "type_str": "table",
                "text": "Differences in (a) similarities between patient names and conditions they have, and (b) similarities between patient names and conditions they do not have. Static embeddings are 200 dimensional; we train these for 10 epochs. For BERT models, we use 10k patients rather than the \u223c28k due to compute constraints.",
                "html": null,
                "num": null
            },
            "TABREF6": {
                "content": "<table><tr><td>Model</td><td>AUC</td></tr><tr><td>Regular Base</td><td>0.508</td></tr><tr><td>Large Base</td><td>0.501</td></tr><tr><td colspan=\"2\">Standard Base 0.498</td></tr><tr><td>Model</td><td>AUC</td></tr><tr><td colspan=\"2\">First Name Masked</td></tr><tr><td>Regular Base</td><td>0.510</td></tr><tr><td>Regular Large</td><td>0.506</td></tr><tr><td>Name Insertion</td><td>0.562</td></tr><tr><td>Template Only</td><td>0.625</td></tr><tr><td>Last Name Masked</td><td/></tr><tr><td>Regular Base</td><td>0.503</td></tr><tr><td>Regular Large</td><td>0.498</td></tr><tr><td>Name Insertion</td><td>0.517</td></tr><tr><td>Template Only</td><td>0.733</td></tr></table>",
                "type_str": "table",
                "text": "Predictions (on a test set) of which names have been seen by the model. We include the standard BERT(Devlin et al., 2019) model (\"Standard Base\"), which is not trained on MIMIC, as a comparator.",
                "html": null,
                "num": null
            },
            "TABREF7": {
                "content": "<table/>",
                "type_str": "table",
                "text": "We compute the perplexity of the masked parts of names for all 46,520 patients and measure whether the (27,906) reidentified patients receive lower perplexity, compared to remaining patients.",
                "html": null,
                "num": null
            },
            "TABREF9": {
                "content": "<table><tr><td>tient does indeed have a condition produced by the</td></tr><tr><td>Base model. It is unclear to what extent this re-</td></tr><tr><td>flects memorization of concrete patient-condition</td></tr><tr><td>pairs per se, as opposed to learning more diffused</td></tr><tr><td>patient-agnostic distributions of conditions in the</td></tr><tr><td>MIMIC dataset. The corresponding statistic for</td></tr><tr><td>the Name Insertion variant (4.17%) may be low</td></tr><tr><td>because this tends to produce poor quality out-</td></tr><tr><td>puts with many names, but not many conditions.</td></tr><tr><td>This is an intriguing result that warrants further</td></tr><tr><td>research.</td></tr></table>",
                "type_str": "table",
                "text": "Results over texts generated by the Base and Name Insertion models. The 'Sent. with Name' column is percentage of extracted sentences that contain a name token. The First and Last name columns show what percent of unique names produced are in the MIMIC dataset. After re-ranking all unique names, we report the percentage of top 100 names that belong to a reidentified patient. Finally, The Name + Positive Condition displays what percent of sentences with a patient's name also contain one of their true (MedCAT) conditions.",
                "html": null,
                "num": null
            },
            "TABREF11": {
                "content": "<table/>",
                "type_str": "table",
                "text": "AUC, accuracy at 10 (A@10), and Spearman coefficient relative to condition frequency.",
                "html": null,
                "num": null
            },
            "TABREF12": {
                "content": "<table><tr><td/><td>0.8 0.9</td><td/><td/><td/><td/><td/><td/><td>0.8 1.0</td><td/><td/><td/><td/><td/><td>Template Only Large Name Insertion Regular Base</td></tr><tr><td>AUC</td><td>0.6 0.7</td><td/><td/><td/><td/><td/><td>AUC</td><td>0.4 0.6</td><td/><td/><td/><td/><td/></tr><tr><td/><td>0.5</td><td/><td/><td/><td/><td/><td/><td>0.2</td><td/><td/><td/><td/><td/></tr><tr><td/><td>0.4</td><td/><td/><td/><td/><td/><td/><td/><td/><td/><td/><td/><td/></tr><tr><td/><td>2</td><td>4</td><td>6</td><td>8 Length of Bin</td><td>10</td><td>12</td><td>14</td><td>2</td><td>4</td><td>6</td><td>8 Length of Bin 10</td><td>12</td><td>14</td><td>16</td></tr><tr><td/><td/><td/><td colspan=\"3\">(a) ICD-9 Labels</td><td/><td/><td/><td/><td/><td colspan=\"3\">(b) MedCAT Labels</td></tr></table>",
                "type_str": "table",
                "text": "Table 3, we show results for Base++, Large++, and Pubmed-Base models. Interestingly, the Large and Pubmed-Base model's perform better when names are not included. We see the biggest difference between Appendix Table 10 and 11 in the Templates Only model, suggesting that this model is memorizing the relationship between patients and conditions.",
                "html": null,
                "num": null
            }
        }
    }
}