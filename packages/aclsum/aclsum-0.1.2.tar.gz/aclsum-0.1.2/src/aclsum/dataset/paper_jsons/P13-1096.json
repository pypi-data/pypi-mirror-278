{
    "paper_id": "P13-1096",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:34:15.129619Z"
    },
    "title": "Utterance-Level Multimodal Sentiment Analysis",
    "authors": [
        {
            "first": "Ver\u00f3nica",
            "middle": [],
            "last": "P\u00e9rez-Rosas",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of North Texas",
                "location": {}
            },
            "email": "veronicaperezrosas@my.unt.edu"
        },
        {
            "first": "Rada",
            "middle": [],
            "last": "Mihalcea",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of North Texas",
                "location": {}
            },
            "email": ""
        },
        {
            "first": "Louis-Philippe",
            "middle": [],
            "last": "Morency",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Southern California",
                "location": {}
            },
            "email": "morency@ict.usc.edu"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "During real-life interactions, people are naturally gesturing and modulating their voice to emphasize specific points or to express their emotions. With the recent growth of social websites such as YouTube, Facebook, and Amazon, video reviews are emerging as a new source of multimodal and natural opinions that has been left almost untapped by automatic opinion analysis techniques. This paper presents a method for multimodal sentiment classification, which can identify the sentiment expressed in utterance-level visual datastreams. Using a new multimodal dataset consisting of sentiment annotated utterances extracted from video reviews, we show that multimodal sentiment analysis can be effectively performed, and that the joint use of visual, acoustic, and linguistic modalities can lead to error rate reductions of up to 10.5% as compared to the best performing individual modality.",
    "pdf_parse": {
        "paper_id": "P13-1096",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "During real-life interactions, people are naturally gesturing and modulating their voice to emphasize specific points or to express their emotions. With the recent growth of social websites such as YouTube, Facebook, and Amazon, video reviews are emerging as a new source of multimodal and natural opinions that has been left almost untapped by automatic opinion analysis techniques. This paper presents a method for multimodal sentiment classification, which can identify the sentiment expressed in utterance-level visual datastreams. Using a new multimodal dataset consisting of sentiment annotated utterances extracted from video reviews, we show that multimodal sentiment analysis can be effectively performed, and that the joint use of visual, acoustic, and linguistic modalities can lead to error rate reductions of up to 10.5% as compared to the best performing individual modality.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Video reviews represent a growing source of consumer information that gained increasing interest from companies, researchers, and consumers. Popular web platforms such as YouTube, Amazon, Facebook, and ExpoTV have reported a significant increase in the number of consumer reviews in video format over the past five years. Compared to traditional text reviews, video reviews provide a more natural experience as they allow the viewer to better sense the reviewer's emotions, beliefs, and intentions through richer channels such as intonations, facial expressions, and body language.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Much of the work to date on opinion analysis has focused on textual data, and a number of resources have been created including lexicons (Wiebe and Riloff, 2005; Esuli and Sebastiani, 2006) or large annotated datasets (Maas et al., 2011) . Given the accelerated growth of other media on the Web and elsewhere, which includes massive collections of videos (e.g., YouTube, Vimeo, VideoLectures), images (e.g., Flickr, Picasa), audio clips (e.g., podcasts), the ability to address the identification of opinions in the presence of diverse modalities is becoming increasingly important. This has motivated researchers to start exploring multimodal clues for the detection of sentiment and emotions in video content (Morency et al., 2011; Wagner et al., 2011) .",
                "cite_spans": [
                    {
                        "start": 137,
                        "end": 161,
                        "text": "(Wiebe and Riloff, 2005;",
                        "ref_id": "BIBREF51"
                    },
                    {
                        "start": 162,
                        "end": 189,
                        "text": "Esuli and Sebastiani, 2006)",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 218,
                        "end": 237,
                        "text": "(Maas et al., 2011)",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 711,
                        "end": 733,
                        "text": "(Morency et al., 2011;",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 734,
                        "end": 754,
                        "text": "Wagner et al., 2011)",
                        "ref_id": "BIBREF49"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this paper, we explore the addition of speech and visual modalities to text analysis in order to identify the sentiment expressed in video reviews.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Given the non homogeneous nature of full-video reviews, which typically include a mixture of positive, negative, and neutral statements, we decided to perform our experiments and analyses at the utterance level. This is in line with earlier work on text-based sentiment analysis, where it has been observed that full-document reviews often contain both positive and negative comments, which led to a number of methods addressing opinion analysis at sentence level. Our results show that relying on the joint use of linguistic, acoustic, and visual modalities allows us to better sense the sentiment being expressed as compared to the use of only one modality at a time.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Another important aspect of this paper is the introduction of a new multimodal opinion database annotated at the utterance level which is, to our knowledge, the first of its kind. In our work, this dataset enabled a wide range of multimodal sentiment analysis experiments, addressing the relative importance of modalities and individual features.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The following section presents related work in text-based sentiment analysis and audio-visual emotion recognition. Section 3 describes our new multimodal datasets with utterance-level sentiment annotations. Section 4 presents our multimodal sen-timent analysis approach, including details about our linguistic, acoustic, and visual features. Our experiments and results on multimodal sentiment classification are presented in Section 5, with a detailed discussion and analysis in Section 6.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this section we provide a brief overview of related work in text-based sentiment analysis, as well as audio-visual emotion analysis.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "The techniques developed so far for subjectivity and sentiment analysis have focused primarily on the processing of text, and consist of either rulebased classifiers that make use of opinion lexicons, or data-driven methods that assume the availability of a large dataset annotated for polarity. These tools and resources have been already used in a large number of applications, including expressive textto-speech synthesis (Alm et al., 2005) , tracking sentiment timelines in on-line forums and news (Balog et al., 2006) , analysis of political debates (Carvalho et al., 2011) , question answering (Oh et al., 2012) , conversation summarization (Carenini et al., 2008) , and citation sentiment detection (Athar and Teufel, 2012) . One of the first lexicons used in sentiment analysis is the General Inquirer (Stone, 1968) . Since then, many methods have been developed to automatically identify opinion words and their polarity (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Hu and Liu, 2004; Taboada et al., 2011) , as well as n-gram and more linguistically complex phrases (Yang and Cardie, 2012) .",
                "cite_spans": [
                    {
                        "start": 425,
                        "end": 443,
                        "text": "(Alm et al., 2005)",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 502,
                        "end": 522,
                        "text": "(Balog et al., 2006)",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 555,
                        "end": 578,
                        "text": "(Carvalho et al., 2011)",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 600,
                        "end": 617,
                        "text": "(Oh et al., 2012)",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 647,
                        "end": 670,
                        "text": "(Carenini et al., 2008)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 706,
                        "end": 730,
                        "text": "(Athar and Teufel, 2012)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 810,
                        "end": 823,
                        "text": "(Stone, 1968)",
                        "ref_id": "BIBREF41"
                    },
                    {
                        "start": 930,
                        "end": 966,
                        "text": "(Hatzivassiloglou and McKeown, 1997;",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 967,
                        "end": 980,
                        "text": "Turney, 2002;",
                        "ref_id": "BIBREF47"
                    },
                    {
                        "start": 981,
                        "end": 998,
                        "text": "Hu and Liu, 2004;",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 999,
                        "end": 1020,
                        "text": "Taboada et al., 2011)",
                        "ref_id": "BIBREF43"
                    },
                    {
                        "start": 1081,
                        "end": 1104,
                        "text": "(Yang and Cardie, 2012)",
                        "ref_id": "BIBREF56"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Text-based Subjectivity and Sentiment Analysis",
                "sec_num": "2.1"
            },
            {
                "text": "For data-driven methods, one of the most widely used datasets is the MPQA corpus (Wiebe et al., 2005) , which is a collection of news articles manually annotated for opinions. Other datasets are also available, including two polarity datasets consisting of movie reviews (Pang and Lee, 2004; Maas et al., 2011) , and a collection of newspaper headlines annotated for polarity (Strapparava and Mihalcea, 2007) .",
                "cite_spans": [
                    {
                        "start": 81,
                        "end": 101,
                        "text": "(Wiebe et al., 2005)",
                        "ref_id": null
                    },
                    {
                        "start": 271,
                        "end": 291,
                        "text": "(Pang and Lee, 2004;",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 292,
                        "end": 310,
                        "text": "Maas et al., 2011)",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 376,
                        "end": 408,
                        "text": "(Strapparava and Mihalcea, 2007)",
                        "ref_id": "BIBREF42"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Text-based Subjectivity and Sentiment Analysis",
                "sec_num": "2.1"
            },
            {
                "text": "While difficult problems such as cross-domain (Blitzer et al., 2007; Li et al., 2012) or crosslanguage (Mihalcea et al., 2007; Wan, 2009; Meng et al., 2012) portability have been addressed, not much has been done in terms of extending the applicability of sentiment analysis to other modalities, such as speech or facial expressions.",
                "cite_spans": [
                    {
                        "start": 46,
                        "end": 68,
                        "text": "(Blitzer et al., 2007;",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 69,
                        "end": 85,
                        "text": "Li et al., 2012)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 103,
                        "end": 126,
                        "text": "(Mihalcea et al., 2007;",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 127,
                        "end": 137,
                        "text": "Wan, 2009;",
                        "ref_id": "BIBREF50"
                    },
                    {
                        "start": 138,
                        "end": 156,
                        "text": "Meng et al., 2012)",
                        "ref_id": "BIBREF25"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Text-based Subjectivity and Sentiment Analysis",
                "sec_num": "2.1"
            },
            {
                "text": "The only exceptions that we are aware of are the findings reported in (Somasundaran et al., 2006; Raaijmakers et al., 2008; Mairesse et al., 2012; Metze et al., 2009) , where speech and text have been analyzed jointly for the purpose of subjectivity or sentiment identification, without, however, addressing other modalities such as visual cues; and the work reported in (Morency et al., 2011; Perez-Rosas et al., 2013) , where multimodal cues have been used for the analysis of sentiment in product reviews, but where the analysis was done at the much coarser level of full videos rather than individual utterances as we do in our work.",
                "cite_spans": [
                    {
                        "start": 70,
                        "end": 97,
                        "text": "(Somasundaran et al., 2006;",
                        "ref_id": "BIBREF40"
                    },
                    {
                        "start": 98,
                        "end": 123,
                        "text": "Raaijmakers et al., 2008;",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 124,
                        "end": 146,
                        "text": "Mairesse et al., 2012;",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 147,
                        "end": 166,
                        "text": "Metze et al., 2009)",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 371,
                        "end": 393,
                        "text": "(Morency et al., 2011;",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 394,
                        "end": 419,
                        "text": "Perez-Rosas et al., 2013)",
                        "ref_id": "BIBREF31"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Text-based Subjectivity and Sentiment Analysis",
                "sec_num": "2.1"
            },
            {
                "text": "Also related to our work is the research done on emotion analysis. Emotion analysis of speech signals aims to identify the emotional or physical states of a person by analyzing his or her voice (Ververidis and Kotropoulos, 2006) . Proposed methods for emotion recognition from speech focus both on what is being said and how is being said, and rely mainly on the analysis of the speech signal by sampling the content at utterance or frame level (Bitouk et al., 2010) . Several researchers used prosody (e.g., pitch, speaking rate, Mel frequency coefficients) for speech-based emotion recognition (Polzin and Waibel, 1996; Tato et al., 2002; Ayadi et al., 2011) .",
                "cite_spans": [
                    {
                        "start": 194,
                        "end": 228,
                        "text": "(Ververidis and Kotropoulos, 2006)",
                        "ref_id": "BIBREF48"
                    },
                    {
                        "start": 445,
                        "end": 466,
                        "text": "(Bitouk et al., 2010)",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 596,
                        "end": 621,
                        "text": "(Polzin and Waibel, 1996;",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 622,
                        "end": 640,
                        "text": "Tato et al., 2002;",
                        "ref_id": "BIBREF44"
                    },
                    {
                        "start": 641,
                        "end": 660,
                        "text": "Ayadi et al., 2011)",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Audio-Visual Emotion Analysis.",
                "sec_num": "2.2"
            },
            {
                "text": "There are also studies that analyzed the visual cues, such as facial expressions and body movements (Calder et al., 2001; Rosenblum et al., 1996; Essa and Pentland, 1997) . Facial expressions are among the most powerful and natural means for human beings to communicate their emotions and intentions (Tian et al., 2001) . Emotions can be also expressed unconsciously, through subtle movements of facial muscles such as smiling or eyebrow raising, often measured and described using the Facial Action Coding System (FACS) (Ekman et al., 2002) . De Silva et. al. (De Silva et al., 1997) and Chen et. al. (Chen et al., 1998) presented one of the early works that integrate both acoustic and visual information for emotion recognition. In addition to work that considered individual modalities, there is also a growing body of work concerned with multimodal emotion analysis (Silva et al., 1997; Sebe et al., 2006; Zhihong et al., 2009; Wollmer et al., 2010) .",
                "cite_spans": [
                    {
                        "start": 100,
                        "end": 121,
                        "text": "(Calder et al., 2001;",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 122,
                        "end": 145,
                        "text": "Rosenblum et al., 1996;",
                        "ref_id": "BIBREF34"
                    },
                    {
                        "start": 146,
                        "end": 170,
                        "text": "Essa and Pentland, 1997)",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 300,
                        "end": 319,
                        "text": "(Tian et al., 2001)",
                        "ref_id": "BIBREF46"
                    },
                    {
                        "start": 521,
                        "end": 541,
                        "text": "(Ekman et al., 2002)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 544,
                        "end": 584,
                        "text": "De Silva et. al. (De Silva et al., 1997)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 602,
                        "end": 621,
                        "text": "(Chen et al., 1998)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 871,
                        "end": 891,
                        "text": "(Silva et al., 1997;",
                        "ref_id": null
                    },
                    {
                        "start": 892,
                        "end": 910,
                        "text": "Sebe et al., 2006;",
                        "ref_id": "BIBREF38"
                    },
                    {
                        "start": 911,
                        "end": 932,
                        "text": "Zhihong et al., 2009;",
                        "ref_id": "BIBREF57"
                    },
                    {
                        "start": 933,
                        "end": 954,
                        "text": "Wollmer et al., 2010)",
                        "ref_id": "BIBREF55"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Audio-Visual Emotion Analysis.",
                "sec_num": "2.2"
            },
            {
                "text": "Utterance transcription Label En este color, creo que era el color frambuesa. neu In this color, I think it was raspberry Pinta hermosisimo. pos It looks beautiful. Sinceramente, con respecto a lo que pinta y a que son hidratante, si son muy hidratantes. pos Honestly, talking about how they looks and hydrates, yes they are very hydrant. Pero el problema de estos labiales es que cuando uno se los aplica, te dejan un gusto asqueroso en la boca. neg But the problem with those lipsticks is that when you apply them, they leave a very nasty taste Sinceramente, es no es que sea el olor sino que es mas bien el gusto. neg Honestly, is not the smell, it is the taste.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Audio-Visual Emotion Analysis.",
                "sec_num": "2.2"
            },
            {
                "text": "Table 1 : Sample utterance-level annotations. The labels used are: pos(itive), neg(ative), neu(tral).",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Audio-Visual Emotion Analysis.",
                "sec_num": "2.2"
            },
            {
                "text": "More recently, two challenges have been organized focusing on the recognition of emotions using audio and visual cues (Schuller et al., 2011a; Schuller et al., 2011b) , which included subchallenges on audio-only, video-only, and audiovideo, and drew the participation of many teams from around the world. Note however that most of the previous work on audio-visual emotion analysis has focused exclusively on the audio and video modalities, and did not consider textual features, as we do in our work.",
                "cite_spans": [
                    {
                        "start": 118,
                        "end": 142,
                        "text": "(Schuller et al., 2011a;",
                        "ref_id": null
                    },
                    {
                        "start": 143,
                        "end": 166,
                        "text": "Schuller et al., 2011b)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Audio-Visual Emotion Analysis.",
                "sec_num": "2.2"
            },
            {
                "text": "For our experiments, we created a dataset of utterances (named MOUD) containing product opinions expressed in Spanish. 1 We chose to work with Spanish because it is a widely used language, and it is the native language of the main author of this paper.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "MOUD: Multimodal Opinion Utterances Dataset",
                "sec_num": "3"
            },
            {
                "text": "We started by collecting a set of videos from the social media web site YouTube, using several keywords likely to lead to a product review or recommendation. Starting with the YouTube search page, videos were found using the following keywords: mis products favoritos (my favorite products), products que no recomiendo (non recommended products), mis perfumes favoritos (my favorite perfumes), peliculas recomendadas (recommended movies), peliculas que no recomiendo (non recommended movies) and libros recomendados (recommended books), libros que no recomiendo (non recommended books). Notice that the keywords are not targeted at a specific product type; rather, we used a variety of product names, so that the dataset has some degree of generality within the broad domain of product reviews.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "MOUD: Multimodal Opinion Utterances Dataset",
                "sec_num": "3"
            },
            {
                "text": "1 Publicly available from the authors webpage.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "MOUD: Multimodal Opinion Utterances Dataset",
                "sec_num": "3"
            },
            {
                "text": "Among all the videos returned by the YouTube search, we selected only videos that respected the following guidelines: the speaker should be in front of the camera; her face should be clearly visible, with a minimum amount of face occlusion during the recording; there should not be any background music or animation. The final video set includes 80 videos randomly selected from the videos retrieved from YouTube that also met the guidelines above. The dataset includes 15 male and 65 female speakers, with their age approximately ranging from 20 to 60 years.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "MOUD: Multimodal Opinion Utterances Dataset",
                "sec_num": "3"
            },
            {
                "text": "All the videos were first pre-processed to eliminate introductory titles and advertisements. Since the reviewers often switched topics when expressing their opinions, we manually selected a 30 seconds opinion segment from each video to avoid having multiple topics in a single review.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "MOUD: Multimodal Opinion Utterances Dataset",
                "sec_num": "3"
            },
            {
                "text": "All the video clips were manually processed to transcribe the verbal statements and also to extract the start and end time of each utterance. Since the reviewers utter expressive sentences that are naturally segmented by speech pauses, we decided to use these pauses (>0.5seconds) to identify the beginning and the end of each utterance. The transcription and segmentation were performed using the Transcriber software.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Segmentation and Transcription",
                "sec_num": "3.1"
            },
            {
                "text": "Each video was segmented into an average of six utterances, resulting in a final dataset of 498 utterances. Each utterance is linked to the corresponding audio and video stream, as well as its manual transcription. The utterances have an average duration of 5 seconds, with a standard deviation of 1.2 seconds. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Segmentation and Transcription",
                "sec_num": "3.1"
            },
            {
                "text": "To enable the use of this dataset for sentiment detection, we performed sentiment annotations at utterance level. Annotations were done using Elan, 2 which is a widely used tool for the annotation of video and audio resources. Two annotators independently labeled each utterance as positive, negative, or neutral. The annotation was done after seeing the video corresponding to an utterance (along with the corresponding audio source). The transcription of the utterance was also made available. Thus, the annotation process included all three modalities: visual, acoustic, and linguistic. The annotators were allowed to watch the video segment and their corresponding transcription as many times as needed.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Sentiment Annotation",
                "sec_num": "3.2"
            },
            {
                "text": "The inter-annotator agreement was measured at 88%, with a Kappa of 0.81, which represents good agreement. All the disagreements were reconciled through discussions.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Sentiment Annotation",
                "sec_num": "3.2"
            },
            {
                "text": "Table 1 shows the five utterances obtained from a video in our dataset, along with their corresponding 2 http://tla.mpi.nl/tools/tla-tools/elan/ sentiment annotations. As this example illustrates, a video can contain a mix of positive, negative, and neutral utterances. Note also that sentiment is not always explicit in the text: for example, the last utterance \"Honestly, it is not the smell, it is the taste\" has an implicit reference to the \"nasty taste\" expressed in the previous utterance, and thus it was also labeled as negative by both annotators.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Sentiment Annotation",
                "sec_num": "3.2"
            },
            {
                "text": "The main advantage that comes with the analysis of video opinions, as compared to their textual counterparts, is the availability of visual and speech cues. In textual opinions, the only source of information consists of words and their dependencies, which may sometime prove insufficient to convey the exact sentiment of the user. Instead, video opinions naturally contain multiple modalities, consisting of visual, acoustic, and linguistic datastreams. We hypothesize that the simultaneous use of these three modalities will help create a better opinion analysis model.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Multimodal Sentiment Analysis",
                "sec_num": "4"
            },
            {
                "text": "This section describes the process of automatically extracting linguistic, acoustic and visual features from the video reviews. First, we obtain the stream corresponding to each modality, followed by the extraction of a representative set of features for each modality, as described in the following subsections. These features are then used as cues to build a classifier of positive or negative sentiment. Figure 1 illustrates this process.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 414,
                        "end": 415,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Feature Extraction",
                "sec_num": "4.1"
            },
            {
                "text": "We use a bag-of-words representation of the video transcriptions of each utterance to derive unigram counts, which are then used as linguistic features. First, we build a vocabulary consisting of all the words, including stopwords, occurring in the transcriptions of the training set. We then remove those words that have a frequency below 10 (value determined empirically on a small development set). The remaining words represent the unigram features, which are then associated with a value corresponding to the frequency of the unigram inside each utterance transcription. These simple weighted unigram features have been successfully used in the past to build sentiment classifiers on text, and in conjunction with Support Vector Machines (SVM) have been shown to lead to state-ofthe-art performance (Maas et al., 2011) .",
                "cite_spans": [
                    {
                        "start": 804,
                        "end": 823,
                        "text": "(Maas et al., 2011)",
                        "ref_id": "BIBREF23"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Linguistic Features",
                "sec_num": "4.1.1"
            },
            {
                "text": "Acoustic features are automatically extracted from the speech signal of each utterance. We used the open source software OpenEAR (Schuller, 2009) to automatically compute a set of acoustic features. We include prosody, energy, voicing probabilities, spectrum, and cepstral features.",
                "cite_spans": [
                    {
                        "start": 129,
                        "end": 145,
                        "text": "(Schuller, 2009)",
                        "ref_id": "BIBREF37"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acoustic Features",
                "sec_num": "4.1.2"
            },
            {
                "text": "\u2022 Prosody features. These include intensity, loudness, and pitch that describe the speech signal in terms of amplitude and frequency.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acoustic Features",
                "sec_num": "4.1.2"
            },
            {
                "text": "\u2022 Energy features. These features describe the human loudness perception.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acoustic Features",
                "sec_num": "4.1.2"
            },
            {
                "text": "\u2022 Voice probabilities. These are probabilities that represent an estimate of the percentage of voiced and unvoiced energy in the speech.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acoustic Features",
                "sec_num": "4.1.2"
            },
            {
                "text": "\u2022 Spectral features. The spectral features are based on the characteristics of the human ear, which uses a nonlinear frequency unit to simulate the human auditory system. These features describe the speech formants, which model spoken content and represent speaker characteristics.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acoustic Features",
                "sec_num": "4.1.2"
            },
            {
                "text": "\u2022 Cepstral features. These features emphasize changes or periodicity in the spectrum features measured by frequencies; we model them using 12 Mel-frequency cepstral coefficients that are calculated based on the Fourier transform of a speech frame.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acoustic Features",
                "sec_num": "4.1.2"
            },
            {
                "text": "Overall, we have a set of 28 acoustic features. During the feature extraction, we use a frame sampling of 25ms. Speaker normalization is performed using z-standardization. The voice intensity is thresholded to identify samples with and without speech, with the same threshold being used for all the experiments and all the speakers. The features are averaged over all the frames in an utterance, to obtain one feature vector for each utterance.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acoustic Features",
                "sec_num": "4.1.2"
            },
            {
                "text": "Facial expressions can provide important clues for affect recognition, which we use to complement the linguistic and acoustic features extracted from the speech stream.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Facial Features",
                "sec_num": "4.1.3"
            },
            {
                "text": "The most widely used system for measuring and describing facial behaviors is the Facial Action Coding System (FACS), which allows for the description of face muscle activities through the use of a set of Action Units (AUs). According with (Ekman, 1993) , there are 64 AUs that involve the upper and lower face, including several face positions and movements. 3 AUs can occur either by themselves or in combination, and can be used to identify a variety of emotions. While AUs are frequently annotated by certified human annotators, automatic tools are also available. In our work, we use the Computer Expression Recognition Toolbox (CERT) (Littlewort et al., 2011) , which allows us to automatically extract the following visual features:",
                "cite_spans": [
                    {
                        "start": 239,
                        "end": 252,
                        "text": "(Ekman, 1993)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 639,
                        "end": 664,
                        "text": "(Littlewort et al., 2011)",
                        "ref_id": "BIBREF22"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Facial Features",
                "sec_num": "4.1.3"
            },
            {
                "text": "\u2022 Smile and head pose estimates. The smile feature is an estimate for smiles. Head pose detection consists of three-dimensional estimates of the head orientation, i.e., yaw, pitch, and roll. These features provide information about changes in smiles and face positions while uttering positive and negative opinions.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Facial Features",
                "sec_num": "4.1.3"
            },
            {
                "text": "\u2022 Facial AUs. These features are the raw estimates for 30 facial AUs related to muscle movements for the eyes, eyebrows, nose, lips, and chin. They provide detailed information about facial behaviors from which we expect to find differences between positive and negative states.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Facial Features",
                "sec_num": "4.1.3"
            },
            {
                "text": "\u2022 Eight basic emotions. These are estimates for the following emotions: anger, contempt, disgust, fear, joy, sad, surprise, and neutral. These features describe the presence of two or more AUs that define a specific emotion. For example, the unit A12 describes the pulling of lip corners movement, which usually suggests a smile but when associated with a check raiser movement (unit A6), represents a marker for the emotion of happiness.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Facial Features",
                "sec_num": "4.1.3"
            },
            {
                "text": "We extract a total of 40 visual features, each of them obtained at frame level. Since only one person is present in each video clip, most of the time facing the camera, the facial tracking was successfully applied for most of our data. For the analysis, we use a sampling rate of 30 frames per second. The features extracted for each utterance are averaged over all the valid frames, which are automatically identified using the output of CERT. 4 Segments with more than 60% of invalid frames are simply discarded.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Facial Features",
                "sec_num": "4.1.3"
            },
            {
                "text": "We run our sentiment classification experiments on the MOUD dataset introduced earlier. From the dataset, we remove utterances labeled as neutral, thus keeping only the positive and negative utterances with valid visual features. The removal of neutral utterances is done for two main reasons. First, the number of neutral utterances in the dataset is rather small. Second, previous work in subjectivity and sentiment analysis has demonstrated that a layered approach (where neutral statements are first separated from opinion statements followed by a separation between positive and negative statements) works better than a single three-way classification. After this process, we are left with an experimental dataset of 412 utterances, 182 of which are labeled as positive, and 231 are labeled as negative.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments and Results",
                "sec_num": "5"
            },
            {
                "text": "From each utterance, we extract the linguistic, acoustic, and visual features described above, which are then combined using the early fusion (or feature-level fusion) approach (Hall and 1997; Atrey et al., 2010) . In this approach, the features collected from all the multimodal streams are combined into a single feature vector, thus resulting in one vector for each utterance in the dataset which is used to make a decision about the sentiment orientation of the utterance. We run several comparative experiments, using one, two, and three modalities at a time. We use the entire set of 412 utterances and run ten fold cross validations using an SVM classifier, as implemented in the Weka toolkit. 5 In line with previous work on emotion recognition in speech (Haq and Jackson, 2009; Anagnostopoulos and Vovoli, 2010) where utterances are selected in a speaker dependent manner (i.e., utterances from the same speaker are included in both training and test), as well as work on sentence-level opinion classification where document boundaries are not considered in the split performed between the training and test sets (Wilson et al., 2004; Wiegand and Klakow, 2009) , the training/test split for each fold is performed at utterance level regardless of the video they belong to.",
                "cite_spans": [
                    {
                        "start": 177,
                        "end": 186,
                        "text": "(Hall and",
                        "ref_id": null
                    },
                    {
                        "start": 187,
                        "end": 192,
                        "text": "1997;",
                        "ref_id": null
                    },
                    {
                        "start": 193,
                        "end": 212,
                        "text": "Atrey et al., 2010)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 763,
                        "end": 786,
                        "text": "(Haq and Jackson, 2009;",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 787,
                        "end": 820,
                        "text": "Anagnostopoulos and Vovoli, 2010)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 1122,
                        "end": 1143,
                        "text": "(Wilson et al., 2004;",
                        "ref_id": "BIBREF54"
                    },
                    {
                        "start": 1144,
                        "end": 1169,
                        "text": "Wiegand and Klakow, 2009)",
                        "ref_id": "BIBREF53"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments and Results",
                "sec_num": "5"
            },
            {
                "text": "Table 2 shows the results of the utterance-level sentiment classification experiments. The baseline is obtained using the ZeroR classifier, which assigns the most frequent label by default, averaged over the ten folds.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "2",
                        "ref_id": "TABREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Experiments and Results",
                "sec_num": "5"
            },
            {
                "text": "The experimental results show that sentiment classification can be effectively performed on multimodal datastreams. Moreover, the integration of visual, acoustic, and linguistic features can improve significantly over the use of one modality at a time, with incremental improvements observed for each added modality.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discussion",
                "sec_num": "6"
            },
            {
                "text": "Among the individual classifiers, the linguistic classifier appears to be the most accurate, followed by the classifier that relies on visual clues, and by the audio classifier. Compared to the best individual classifier, the relative error rate reduction obtained with the tri-modal classifier is 10.5%. The results obtained with this multimodal utterance classifier are found to be significantly better than the best individual results (obtained with the text modality), with significance being tested with a t-test (p=0.05).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discussion",
                "sec_num": "6"
            },
            {
                "text": "To determine the role played by each of the visual and acoustic features, we compare the feature weights assigned by the learning algorithm, as shown in Figure 2 . Interestingly, a distressed brow is the strongest indicator of sentiment, followed, this time not surprisingly, by the smile feature. Other informative features for sentiment classification are the voice probability, representing the energy in speech, the combined visual features that represent an angry face, and two of the cepstral coefficients.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 160,
                        "end": 161,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Feature analysis.",
                "sec_num": null
            },
            {
                "text": "To reach a better understanding of the relation between features, we also calculate the Pearson correlation between the visual and acoustic features. Table 3 shows a subset of these correlation figures. As we expected, correlations between features of the same type are higher. For example, the correlation between features AU6 and AU12 or the correlation between intensity and loudness is higher than the correlation between AU6 and intensity. Nonetheless, we still find some significant correlations between features of different types, for instance AU12 and AU45 which are both significantly correlated with the intensity and loudness features. This give us confidence about using them for further analysis.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 156,
                        "end": 157,
                        "text": "3",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Feature analysis.",
                "sec_num": null
            },
            {
                "text": "Video-level sentiment analysis.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Feature analysis.",
                "sec_num": null
            },
            {
                "text": "To understand the role played by the size of the video-segments considered in the sentiment classification experiments, as well as the potential effect of a speaker-independence assumption, we also run a set of experiments where we use full videos for the classification.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Feature analysis.",
                "sec_num": null
            },
            {
                "text": "In these experiments, once again the sentiment annotation is done by two independent annotators, using the same protocol as in the utterance-based annotations. Videos that were ambivalent about the general sentiment were either labeled as neutral (and thus removed from the experiments), or labeled with the dominant sentiment. The interannotator agreement for this annotation was measured at 96.1%. As before, the linguistic, acoustic, and visual features are averaged over the entire video, and we use an SVM classifier in ten-fold cross validation experiments.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Feature analysis.",
                "sec_num": null
            },
            {
                "text": "Table 4 shows the results obtained in these video-level experiments. While the combination of modalities still helps, the improvement is smaller than the one obtained during the utterance-level classification. Specifically, the combined effect of acoustic and visual features improves significantly over the individual modalities. However, the combination of linguistic features with other modalities does not lead to clear improvements. This may be due to the smaller number of feature vectors used in the experiments (only 80, as compared to the 412 used in the previous setup). Another possible reason is the fact that the acoustic and visual modalities are significantly weaker than the linguistic modality, most likely due to the fact that the feature vectors are now speaker-independent, which makes it harder to improve over the linguistic modality alone.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "4",
                        "ref_id": "TABREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Feature analysis.",
                "sec_num": null
            },
            {
                "text": "In this paper, we presented a multimodal approach for utterance-level sentiment classification. We introduced a new multimodal dataset consisting AU6 AU12 AU45 AUs 1,1+4 of sentiment annotated utterances extracted from video reviews, where each utterance is associated with a video, acoustic, and linguistic datastream.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusions",
                "sec_num": "7"
            },
            {
                "text": "Our experiments show that sentiment annotation of utterance-level visual datastreams can be effectively performed, and that the use of multiple modalities can lead to error rate reductions of up to 10.5% as compared to the use of one modality at a time. In future work, we plan to explore alternative multimodal fusion methods, such as decision-level and meta-level fusion, to improve the integration of the visual, acoustic, and linguistic modalities.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusions",
                "sec_num": "7"
            },
            {
                "text": "http://www.cs.cmu.edu/afs/cs/project/face/www/facs.htm",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "There is a small number of frames that CERT could not process, mostly due to the brief occlusions that occur when the speaker is showing the product she is reviewing.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "http://www.cs.waikato.ac.nz/ml/weka/",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "We would like to thank Alberto Castro for his help with the sentiment annotations. This material is based in part upon work supported by National Science Foundation awards #0917170 and #1118018, by DARPA-BAA-12-47 DEFT grant #12475008, and by a grant from U.S. RDECOM. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation, the Defense Advanced Research Projects Agency, or the U.S. Army Research, Development, and Engineering Command.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgments",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Emotions from text: Machine learning for text-based emotion prediction",
                "authors": [
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Alm",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Roth",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Sproat",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "347--354",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "C. Alm, D. Roth, and R. Sproat. 2005. Emotions from text: Machine learning for text-based emotion prediction. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 347-354, Vancouver, Canada.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Sound processing features for speaker-dependent and phraseindependent emotion recognition in berlin database",
                "authors": [
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Anagnostopoulos",
                        "suffix": ""
                    },
                    {
                        "first": "E",
                        "middle": [],
                        "last": "Vovoli",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Information Systems Development",
                "volume": "",
                "issue": "",
                "pages": "413--421",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "C. Anagnostopoulos and E. Vovoli. 2010. Sound pro- cessing features for speaker-dependent and phrase- independent emotion recognition in berlin database. In Information Systems Development, pages 413- 421. Springer.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Context-enhanced citation sentiment detection",
                "authors": [
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Athar",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Teufel",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Proceedings of the 2012 Conference of the North American Chapter",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "A. Athar and S. Teufel. 2012. Context-enhanced cita- tion sentiment detection. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Montr\u00e9al, Canada, June.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Multimodal fusion for multimedia analysis: a survey",
                "authors": [
                    {
                        "first": "P",
                        "middle": [
                            "K"
                        ],
                        "last": "Atrey",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [
                            "A"
                        ],
                        "last": "Hossain",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [
                            "El"
                        ],
                        "last": "Saddik",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Kankanhalli",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Multimedia Systems",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "P. K. Atrey, M. A. Hossain, A. El Saddik, and M. Kankanhalli. 2010. Multimodal fusion for mul- timedia analysis: a survey. Multimedia Systems, 16.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
                "authors": [
                    {
                        "first": "M",
                        "middle": [
                            "El"
                        ],
                        "last": "Ayadi",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Kamel",
                        "suffix": ""
                    },
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Karray",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Pattern Recognition",
                "volume": "44",
                "issue": "3",
                "pages": "572--587",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "M. El Ayadi, M. Kamel, and F. Karray. 2011. Survey on speech emotion recognition: Features, classifica- tion schemes, and databases. Pattern Recognition, 44(3):572 -587.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Why are they excited? identifying and explaining spikes in blog mood levels",
                "authors": [
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Balog",
                        "suffix": ""
                    },
                    {
                        "first": "G",
                        "middle": [],
                        "last": "Mishne",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "De Rijke",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Proceedings of the 11th Meeting of the European Chapter of the As sociation for Computational Linguistics (EACL-2006)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "K. Balog, G. Mishne, and M. de Rijke. 2006. Why are they excited? identifying and explaining spikes in blog mood levels. In Proceedings of the 11th Meet- ing of the European Chapter of the As sociation for Computational Linguistics (EACL-2006).",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Class-level spectral features for emotion recognition",
                "authors": [
                    {
                        "first": "Dmitri",
                        "middle": [],
                        "last": "Bitouk",
                        "suffix": ""
                    },
                    {
                        "first": "Ragini",
                        "middle": [],
                        "last": "Verma",
                        "suffix": ""
                    },
                    {
                        "first": "Ani",
                        "middle": [],
                        "last": "Nenkova",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Speech Commun",
                "volume": "52",
                "issue": "7-8",
                "pages": "613--625",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dmitri Bitouk, Ragini Verma, and Ani Nenkova. 2010. Class-level spectral features for emotion recognition. Speech Commun., 52(7-8):613-625, July.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification",
                "authors": [
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Blitzer",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Dredze",
                        "suffix": ""
                    },
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Pereira",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "J. Blitzer, M. Dredze, and F. Pereira. 2007. Biogra- phies, bollywood, boom-boxes and blenders: Do- main adaptation for sentiment classification. In As- sociation for Computational Linguistics.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "A principal component analysis of facial expressions",
                "authors": [
                    {
                        "first": "A",
                        "middle": [
                            "J"
                        ],
                        "last": "Calder",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [
                            "M"
                        ],
                        "last": "Burton",
                        "suffix": ""
                    },
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Miller",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [
                            "W"
                        ],
                        "last": "Young",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Akamatsu",
                        "suffix": ""
                    }
                ],
                "year": 2001,
                "venue": "Vision research",
                "volume": "41",
                "issue": "9",
                "pages": "1179--1208",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "A. J. Calder, A. M. Burton, P. Miller, A. W. Young, and S. Akamatsu. 2001. A principal component analysis of facial expressions. Vision research, 41(9):1179- 1208, April.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Summarizing emails with conversational cohesion and subjectivity",
                "authors": [
                    {
                        "first": "G",
                        "middle": [],
                        "last": "Carenini",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Ng",
                        "suffix": ""
                    },
                    {
                        "first": "X",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Proceedings of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT 2008)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "G. Carenini, R. Ng, and X. Zhou. 2008. Summarizing emails with conversational cohesion and subjectivity. In Proceedings of the Association for Computational Linguistics: Human Language Technologies (ACL- HLT 2008), Columbus, Ohio.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Liars and saviors in a sentiment annotated corpus of comments to political debates",
                "authors": [
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Carvalho",
                        "suffix": ""
                    },
                    {
                        "first": "L",
                        "middle": [],
                        "last": "Sarmento",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Teixeira",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Silva",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Proceedings of the Association for Computational Linguistics (ACL 2011)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "P. Carvalho, L. Sarmento, J. Teixeira, and M. Silva. 2011. Liars and saviors in a sentiment annotated corpus of comments to political debates. In Proceed- ings of the Association for Computational Linguis- tics (ACL 2011), Portland, OR.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Multimodal human emotion/expression recognition",
                "authors": [
                    {
                        "first": "L",
                        "middle": [
                            "S"
                        ],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "T",
                        "middle": [
                            "S"
                        ],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Miyasato",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Nakatsu",
                        "suffix": ""
                    }
                ],
                "year": 1998,
                "venue": "Proceedings of the 3rd. International Conference on Face & Gesture Recognition",
                "volume": "366",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "L. S. Chen, T. S. Huang, T. Miyasato, and R. Nakatsu. 1998. Multimodal human emotion/expression recog- nition. In Proceedings of the 3rd. International Con- ference on Face & Gesture Recognition, pages 366-, Washington, DC, USA. IEEE Computer Society.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Facial emotion recognition using multi-modal information",
                "authors": [
                    {
                        "first": "L C De",
                        "middle": [],
                        "last": "Silva",
                        "suffix": ""
                    },
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Miyasato",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Nakatsu",
                        "suffix": ""
                    }
                ],
                "year": 1997,
                "venue": "IEEE Signal Processing",
                "volume": "1",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "L C De Silva, T Miyasato, and R Nakatsu, 1997. Facial emotion recognition using multi-modal information, volume 1, page 397401. IEEE Signal Processing So- ciety.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Facial action coding system",
                "authors": [
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Ekman",
                        "suffix": ""
                    },
                    {
                        "first": "W",
                        "middle": [],
                        "last": "Friesen",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Hager",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "P. Ekman, W. Friesen, and J. Hager. 2002. Facial ac- tion coding system.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Facial expression of emotion",
                "authors": [
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Ekman",
                        "suffix": ""
                    }
                ],
                "year": 1993,
                "venue": "American Psychologist",
                "volume": "48",
                "issue": "",
                "pages": "384--392",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "P. Ekman. 1993. Facial expression of emotion. Ameri- can Psychologist, 48:384-392.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Coding, analysis, interpretation, and recognition of facial expressions. Pattern Analysis and Machine Intelligence",
                "authors": [
                    {
                        "first": "I",
                        "middle": [
                            "A"
                        ],
                        "last": "Essa",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [
                            "P"
                        ],
                        "last": "Pentland",
                        "suffix": ""
                    }
                ],
                "year": 1997,
                "venue": "IEEE Transactions on",
                "volume": "19",
                "issue": "7",
                "pages": "757--763",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "I.A. Essa and A.P. Pentland. 1997. Coding, analy- sis, interpretation, and recognition of facial expres- sions. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 19(7):757 -763, jul.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "SentiWordNet: A publicly available lexical resource for opinion mining",
                "authors": [
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Esuli",
                        "suffix": ""
                    },
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Sebastiani",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Proceedings of the 5th Conference on Language Resources and Evaluation (LREC 2006)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "A. Esuli and F. Sebastiani. 2006. SentiWordNet: A publicly available lexical resource for opinion min- ing. In Proceedings of the 5th Conference on Lan- guage Resources and Evaluation (LREC 2006), Gen- ova, IT.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "An introduction to multisensor fusion",
                "authors": [
                    {
                        "first": "D",
                        "middle": [
                            "L"
                        ],
                        "last": "Hall",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Llinas",
                        "suffix": ""
                    }
                ],
                "year": 1997,
                "venue": "IEEE Special Issue on Data Fusion",
                "volume": "85",
                "issue": "1",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "D.L. Hall and J. Llinas. 1997. An introduction to mul- tisensor fusion. IEEE Special Issue on Data Fusion, 85(1).",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Speaker-dependent audio-visual emotion recognition",
                "authors": [
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Haq",
                        "suffix": ""
                    },
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Jackson",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "International Conference on Audio-Visual Speech Processing",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "S. Haq and P. Jackson. 2009. Speaker-dependent audio-visual emotion recognition. In International Conference on Audio-Visual Speech Processing.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Predicting the semantic orientation of adjectives",
                "authors": [
                    {
                        "first": "V",
                        "middle": [],
                        "last": "Hatzivassiloglou",
                        "suffix": ""
                    },
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Mckeown",
                        "suffix": ""
                    }
                ],
                "year": 1997,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "174--181",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "V. Hatzivassiloglou and K. McKeown. 1997. Predict- ing the semantic orientation of adjectives. In Pro- ceedings of the Conference of the European Chap- ter of the Association for Computational Linguistics, pages 174-181.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Mining and summarizing customer reviews",
                "authors": [
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Hu",
                        "suffix": ""
                    },
                    {
                        "first": "B",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "M. Hu and B. Liu. 2004. Mining and summariz- ing customer reviews. In Proceedings of the tenth ACM SIGKDD international conference on Knowl- edge discovery and data mining, Seattle, Washing- ton.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Cross-domain co-extraction of sentiment and topic lexicons",
                "authors": [
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [
                            "J"
                        ],
                        "last": "Pan",
                        "suffix": ""
                    },
                    {
                        "first": "O",
                        "middle": [],
                        "last": "Jin",
                        "suffix": ""
                    },
                    {
                        "first": "Q",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "X",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Proceedings of the 50th Annual Meeting of the",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "F. Li, S. J. Pan, O. Jin, Q. Yang, and X. Zhu. 2012. Cross-domain co-extraction of sentiment and topic lexicons. In Proceedings of the 50th Annual Meet- ing of the Association for Computational Linguistics, Jeju Island, Korea.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "The computer expression recognition toolbox (cert)",
                "authors": [
                    {
                        "first": "G",
                        "middle": [],
                        "last": "Littlewort",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Whitehill",
                        "suffix": ""
                    },
                    {
                        "first": "Tingfan",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "I",
                        "middle": [],
                        "last": "Fasel",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Frank",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Movellan",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Bartlett",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Automatic Face Gesture Recognition and Workshops (FG 2011)",
                "volume": "",
                "issue": "",
                "pages": "298--305",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "G. Littlewort, J. Whitehill, Tingfan Wu, I. Fasel, M. Frank, J. Movellan, and M. Bartlett. 2011. The computer expression recognition toolbox (cert). In Automatic Face Gesture Recognition and Workshops (FG 2011), 2011 IEEE International Conference on, pages 298 -305, march.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Learning word vectors for sentiment analysis",
                "authors": [
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Maas",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Daly",
                        "suffix": ""
                    },
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Pham",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Ng",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Potts",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Proceedings of the Association for Computational Linguistics (ACL 2011)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "A. Maas, R. Daly, P. Pham, D. Huang, A. Ng, and C. Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings of the Association for Com- putational Linguistics (ACL 2011), Portland, OR.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Can prosody inform sentiment analysis? experiments on short spoken reviews",
                "authors": [
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Mairesse",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Polifroni",
                        "suffix": ""
                    },
                    {
                        "first": "G",
                        "middle": [
                            "Di"
                        ],
                        "last": "Fabbrizio",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on",
                "volume": "",
                "issue": "",
                "pages": "5093--5096",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "F. Mairesse, J. Polifroni, and G. Di Fabbrizio. 2012. Can prosody inform sentiment analysis? experi- ments on short spoken reviews. In Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE Inter- national Conference on, pages 5093 -5096, march.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Cross-lingual mixture model for sentiment classification",
                "authors": [
                    {
                        "first": "X",
                        "middle": [],
                        "last": "Meng",
                        "suffix": ""
                    },
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    },
                    {
                        "first": "X",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "G",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "H",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "X. Meng, F. Wei, X. Liu, M. Zhou, G. Xu, and H. Wang. 2012. Cross-lingual mixture model for sentiment classification. In Proceedings of the 50th Annual Meeting of the Association for Computational Lin- guistics, Jeju Island, Korea.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Fusion of acoustic and linguistic features for emotion detection",
                "authors": [
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Metze",
                        "suffix": ""
                    },
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Polzehl",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Wagner",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Semantic Computing, 2009. ICSC '09. IEEE International Conference on",
                "volume": "",
                "issue": "",
                "pages": "153--160",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "F. Metze, T. Polzehl, and M. Wagner. 2009. Fusion of acoustic and linguistic features for emotion detec- tion. In Semantic Computing, 2009. ICSC '09. IEEE International Conference on, pages 153 -160, sept.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Learning multilingual subjective language via cross-lingual projections",
                "authors": [
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Mihalcea",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Banea",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Wiebe",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Proceedings of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "R. Mihalcea, C. Banea, and J. Wiebe. 2007. Learning multilingual subjective language via cross-lingual projections. In Proceedings of the Association for Computational Linguistics, Prague, Czech Republic.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Towards multimodal sentiment analysis: Harvesting opinions from the web",
                "authors": [
                    {
                        "first": "L",
                        "middle": [
                            "P"
                        ],
                        "last": "Morency",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Mihalcea",
                        "suffix": ""
                    },
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Doshi",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Proceedings of the International Conference on Multimodal Computing",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "L.P. Morency, R. Mihalcea, and P. Doshi. 2011. To- wards multimodal sentiment analysis: Harvesting opinions from the web. In Proceedings of the In- ternational Conference on Multimodal Computing, Alicante, Spain.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Why question answering using sentiment analysis and word classes",
                "authors": [
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Oh",
                        "suffix": ""
                    },
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Torisawa",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Hashimoto",
                        "suffix": ""
                    },
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Kawada",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [
                            "De"
                        ],
                        "last": "Saeger",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Kazama",
                        "suffix": ""
                    },
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "J. Oh, K. Torisawa, C. Hashimoto, T. Kawada, S. De Saeger, J. Kazama, and Y. Wang. 2012. Why question answering using sentiment analysis and word classes. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Lan- guage Processing and Computational Natural Lan- guage Learning, Jeju Island, Korea.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts",
                "authors": [
                    {
                        "first": "B",
                        "middle": [],
                        "last": "Pang",
                        "suffix": ""
                    },
                    {
                        "first": "L",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proceedings of the 42nd Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "B. Pang and L. Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the 42nd Meeting of the Association for Computational Lin- guistics, Barcelona, Spain, July.",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "Multimodal sentiment analysis of spanish online videos",
                "authors": [
                    {
                        "first": "V",
                        "middle": [],
                        "last": "Perez-Rosas",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Mihalcea",
                        "suffix": ""
                    },
                    {
                        "first": "L.-P",
                        "middle": [],
                        "last": "Morency",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "V. Perez-Rosas, R. Mihalcea, and L.-P. Morency. 2013. Multimodal sentiment analysis of spanish online videos. IEEE Intelligent Systems.",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "Recognizing emotions in speech",
                "authors": [
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Polzin",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Waibel",
                        "suffix": ""
                    }
                ],
                "year": 1996,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "T. Polzin and A. Waibel. 1996. Recognizing emotions in speech. In In ICSLP.",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "Multimodal subjectivity analysis of multiparty conversation",
                "authors": [
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Raaijmakers",
                        "suffix": ""
                    },
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Truong",
                        "suffix": ""
                    },
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Wilson",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "466--474",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "S. Raaijmakers, K. Truong, and T. Wilson. 2008. Mul- timodal subjectivity analysis of multiparty conversa- tion. In Proceedings of the Conference on Empiri- cal Methods in Natural Language Processing, pages 466-474, Honolulu, Hawaii.",
                "links": null
            },
            "BIBREF34": {
                "ref_id": "b34",
                "title": "Human expression recognition from motion using a radial basis function network architecture. Neural Networks",
                "authors": [
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Rosenblum",
                        "suffix": ""
                    },
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Yacoob",
                        "suffix": ""
                    },
                    {
                        "first": "L",
                        "middle": [
                            "S"
                        ],
                        "last": "Davis",
                        "suffix": ""
                    }
                ],
                "year": 1996,
                "venue": "IEEE Transactions on",
                "volume": "7",
                "issue": "5",
                "pages": "1121--1138",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "M. Rosenblum, Y. Yacoob, and L.S. Davis. 1996. Hu- man expression recognition from motion using a ra- dial basis function network architecture. Neural Net- works, IEEE Transactions on, 7(5):1121 -1138, sep.",
                "links": null
            },
            "BIBREF35": {
                "ref_id": "b35",
                "title": "Visual Emotion Challenge and Workshop",
                "authors": [
                    {
                        "first": "B",
                        "middle": [],
                        "last": "Schuller",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Valstar",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Cowie",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Pantic",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "B. Schuller, M. Valstar, R. Cowie, and M. Pantic, edi- tors. 2011a. Audio/Visual Emotion Challenge and Workshop (AVEC 2011).",
                "links": null
            },
            "BIBREF36": {
                "ref_id": "b36",
                "title": "2011b. Audio/Visual Emotion Challenge and Workshop",
                "authors": [
                    {
                        "first": "B",
                        "middle": [],
                        "last": "Schuller",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Valstar",
                        "suffix": ""
                    },
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Eyben",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Cowie",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Pantic",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "B. Schuller, M. Valstar, F. Eyben, R. Cowie, and M. Pantic, editors. 2011b. Audio/Visual Emotion Challenge and Workshop (AVEC 2011).",
                "links": null
            },
            "BIBREF37": {
                "ref_id": "b37",
                "title": "Openear introducing the munich open-source emotion and affect recognition toolkit",
                "authors": [
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Eyben",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Wollmer",
                        "suffix": ""
                    },
                    {
                        "first": "B",
                        "middle": [],
                        "last": "Schuller",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "ACII",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "F. Eyben M. Wollmer B. Schuller. 2009. Openear in- troducing the munich open-source emotion and af- fect recognition toolkit. In ACII.",
                "links": null
            },
            "BIBREF38": {
                "ref_id": "b38",
                "title": "Emotion recognition based on joint visual and audio cues",
                "authors": [
                    {
                        "first": "N",
                        "middle": [],
                        "last": "Sebe",
                        "suffix": ""
                    },
                    {
                        "first": "I",
                        "middle": [],
                        "last": "Cohen",
                        "suffix": ""
                    },
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Gevers",
                        "suffix": ""
                    },
                    {
                        "first": "T",
                        "middle": [
                            "S"
                        ],
                        "last": "Huang",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "ICPR",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "N. Sebe, I. Cohen, T. Gevers, and T.S. Huang. 2006. Emotion recognition based on joint visual and audio cues. In ICPR.",
                "links": null
            },
            "BIBREF39": {
                "ref_id": "b39",
                "title": "Facial emotion recognition using multi-modal information",
                "authors": [
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Silva",
                        "suffix": ""
                    },
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Miyasato",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Nakatsu",
                        "suffix": ""
                    }
                ],
                "year": 1997,
                "venue": "Proceedings of the International Conference on Information and Communications Security",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "D. Silva, T. Miyasato, and R. Nakatsu. 1997. Facial emotion recognition using multi-modal information. In Proceedings of the International Conference on Information and Communications Security.",
                "links": null
            },
            "BIBREF40": {
                "ref_id": "b40",
                "title": "Manual annotation of opinion categories in meetings",
                "authors": [
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Somasundaran",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Wiebe",
                        "suffix": ""
                    },
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Hoffmann",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Litman",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Proceedings of the Workshop on Frontiers in Linguistically Annotated Corpora",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "S. Somasundaran, J. Wiebe, P. Hoffmann, and D. Lit- man. 2006. Manual annotation of opinion cate- gories in meetings. In Proceedings of the Work- shop on Frontiers in Linguistically Annotated Cor- pora 2006.",
                "links": null
            },
            "BIBREF41": {
                "ref_id": "b41",
                "title": "General Inquirer: Computer Approach to Content Analysis",
                "authors": [
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Stone",
                        "suffix": ""
                    }
                ],
                "year": 1968,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "P. Stone. 1968. General Inquirer: Computer Approach to Content Analysis. MIT Press.",
                "links": null
            },
            "BIBREF42": {
                "ref_id": "b42",
                "title": "Semeval-2007 task 14: Affective text",
                "authors": [
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Strapparava",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Mihalcea",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Proceedings of the 4th International Workshop on the Semantic Evaluations",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "C. Strapparava and R. Mihalcea. 2007. Semeval-2007 task 14: Affective text. In Proceedings of the 4th In- ternational Workshop on the Semantic Evaluations (SemEval 2007), Prague, Czech Republic.",
                "links": null
            },
            "BIBREF43": {
                "ref_id": "b43",
                "title": "Lexicon-based methods for sentiment analysis",
                "authors": [
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Taboada",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Brooke",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Tofiloski",
                        "suffix": ""
                    },
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Voli",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Stede",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Computational Linguistics",
                "volume": "37",
                "issue": "3",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "M. Taboada, J. Brooke, M. Tofiloski, K. Voli, and M. Stede. 2011. Lexicon-based methods for sen- timent analysis. Computational Linguistics, 37(3).",
                "links": null
            },
            "BIBREF44": {
                "ref_id": "b44",
                "title": "Emotional space improves emotion recognition",
                "authors": [
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Tato",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Santos",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Kompe",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [
                            "M"
                        ],
                        "last": "Pardo",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Proc. ICSLP 2002",
                "volume": "",
                "issue": "",
                "pages": "2029--2032",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "R. Tato, R. Santos, R. Kompe, and J. M. Pardo. 2002. Emotional space improves emotion recognition. In In Proc. ICSLP 2002, pages 2029-2032.",
                "links": null
            },
            "BIBREF46": {
                "ref_id": "b46",
                "title": "Recognizing action units for facial expression analysis. Pattern Analysis and Machine Intelligence",
                "authors": [
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Tian",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [
                            "F"
                        ],
                        "last": "Kanade",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Cohn",
                        "suffix": ""
                    }
                ],
                "year": 2001,
                "venue": "IEEE Transactions on",
                "volume": "23",
                "issue": "2",
                "pages": "97--115",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tian, T. Kanade, and J.F. Cohn. 2001. Recogniz- ing action units for facial expression analysis. Pat- tern Analysis and Machine Intelligence, IEEE Trans- actions on, 23(2):97 -115, feb.",
                "links": null
            },
            "BIBREF47": {
                "ref_id": "b47",
                "title": "Thumbs up or thumbs down? semantic orientation applied to unsupervised classification of reviews",
                "authors": [
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Turney",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL 2002)",
                "volume": "",
                "issue": "",
                "pages": "417--424",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "P. Turney. 2002. Thumbs up or thumbs down? seman- tic orientation applied to unsupervised classification of reviews. In Proceedings of the 40th Annual Meet- ing of the Association for Computational Linguistics (ACL 2002), pages 417-424, Philadelphia.",
                "links": null
            },
            "BIBREF48": {
                "ref_id": "b48",
                "title": "Emotional speech recognition: Resources, features, and methods",
                "authors": [
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Ververidis",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Kotropoulos",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Speech Communication",
                "volume": "48",
                "issue": "9",
                "pages": "1162--1181",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "D. Ververidis and C. Kotropoulos. 2006. Emotional speech recognition: Resources, features, and meth- ods. Speech Communication, 48(9):1162-1181, September.",
                "links": null
            },
            "BIBREF49": {
                "ref_id": "b49",
                "title": "Exploring fusion methods for multimodal emotion recognition with missing data. Affective Computing",
                "authors": [
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Wagner",
                        "suffix": ""
                    },
                    {
                        "first": "E",
                        "middle": [],
                        "last": "Andre",
                        "suffix": ""
                    },
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Lingenfelser",
                        "suffix": ""
                    },
                    {
                        "first": "Jonghwa",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "IEEE Transactions on",
                "volume": "2",
                "issue": "4",
                "pages": "206--218",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "J. Wagner, E. Andre, F. Lingenfelser, and Jonghwa Kim. 2011. Exploring fusion methods for multi- modal emotion recognition with missing data. Af- fective Computing, IEEE Transactions on, 2(4):206 -218, oct.-dec.",
                "links": null
            },
            "BIBREF50": {
                "ref_id": "b50",
                "title": "Co-training for cross-lingual sentiment classification",
                "authors": [
                    {
                        "first": "X",
                        "middle": [],
                        "last": "Wan",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of the Joint Conference of the Association of Computational Linguistics and the International Joint Conference on Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "X. Wan. 2009. Co-training for cross-lingual sentiment classification. In Proceedings of the Joint Confer- ence of the Association of Computational Linguistics and the International Joint Conference on Natural Language Processing, Singapore, August.",
                "links": null
            },
            "BIBREF51": {
                "ref_id": "b51",
                "title": "Creating subjective and objective sentence classifiers from unannotated texts",
                "authors": [
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Wiebe",
                        "suffix": ""
                    },
                    {
                        "first": "E",
                        "middle": [],
                        "last": "Riloff",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of the 6th International Conference on Intelligent Text Processing and Computational Linguistics (CICLing-2005",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "J. Wiebe and E. Riloff. 2005. Creating subjective and objective sentence classifiers from unannotated texts. In Proceedings of the 6th International Conference on Intelligent Text Processing and Computational Linguistics (CICLing-2005) (invited paper), Mexico City, Mexico.",
                "links": null
            },
            "BIBREF52": {
                "ref_id": "b52",
                "title": "Annotating expressions of opinions and emotions in language",
                "authors": [
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Wiebe",
                        "suffix": ""
                    },
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Wilson",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Cardie",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Language Resources and Evaluation",
                "volume": "39",
                "issue": "2-3",
                "pages": "165--210",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation, 39(2-3):165- 210.",
                "links": null
            },
            "BIBREF53": {
                "ref_id": "b53",
                "title": "The role of knowledge-based features in polarity classification at sentence level",
                "authors": [
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Wiegand",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Klakow",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of the International Conference of the Florida Artificial Intelligence Research Society",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "M. Wiegand and D. Klakow. 2009. The role of knowledge-based features in polarity classification at sentence level. In Proceedings of the Interna- tional Conference of the Florida Artificial Intelli- gence Research Society.",
                "links": null
            },
            "BIBREF54": {
                "ref_id": "b54",
                "title": "Just how mad are you? finding strong and weak opinion clauses",
                "authors": [
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Wilson",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Wiebe",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Hwa",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proceedings of the American Association for Artificial Intelligence",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "T. Wilson, J. Wiebe, and R. Hwa. 2004. Just how mad are you? finding strong and weak opinion clauses. In Proceedings of the American Association for Arti- ficial Intelligence.",
                "links": null
            },
            "BIBREF55": {
                "ref_id": "b55",
                "title": "Combining long short-term memory and dynamic bayesian networks for incremental emotionsensitive artificial listening",
                "authors": [
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Wollmer",
                        "suffix": ""
                    },
                    {
                        "first": "B",
                        "middle": [],
                        "last": "Schuller",
                        "suffix": ""
                    },
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Eyben",
                        "suffix": ""
                    },
                    {
                        "first": "G",
                        "middle": [],
                        "last": "Rigoll",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "IEEE Journal of Selected Topics in Signal Processing",
                "volume": "4",
                "issue": "5",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "M. Wollmer, B. Schuller, F. Eyben, and G. Rigoll. 2010. Combining long short-term memory and dy- namic bayesian networks for incremental emotion- sensitive artificial listening. IEEE Journal of Se- lected Topics in Signal Processing, 4(5), October.",
                "links": null
            },
            "BIBREF56": {
                "ref_id": "b56",
                "title": "Extracting opinion expressions with semi-markov conditional random fields",
                "authors": [
                    {
                        "first": "B",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Cardie",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "B. Yang and C. Cardie. 2012. Extracting opinion expressions with semi-markov conditional random fields. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Process- ing and Computational Natural Language Learning, Jeju Island, Korea.",
                "links": null
            },
            "BIBREF57": {
                "ref_id": "b57",
                "title": "A survey of affect recognition methods: Audio, visual, and spontaneous expressions",
                "authors": [
                    {
                        "first": "Z",
                        "middle": [],
                        "last": "Zhihong",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Pantic",
                        "suffix": ""
                    },
                    {
                        "first": "G",
                        "middle": [
                            "I"
                        ],
                        "last": "Roisman",
                        "suffix": ""
                    },
                    {
                        "first": "T",
                        "middle": [
                            "S"
                        ],
                        "last": "Huang",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "PAMI",
                "volume": "",
                "issue": "1",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Z. Zhihong, M. Pantic G.I. Roisman, and T.S. Huang. 2009. A survey of affect recognition methods: Au- dio, visual, and spontaneous expressions. PAMI, 31(1).",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 1: Multimodal feature extraction",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Figure 2: Visual and acoustic feature weights. This graph shows the relative importance of the information gain weights associated with the top most informative acoustic-visual features.",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table><tr><td>Llinas,</td></tr></table>",
                "type_str": "table",
                "text": "Utterance-level sentiment classification with linguistic, acoustic, and visual features.",
                "html": null,
                "num": null
            },
            "TABREF1": {
                "content": "<table><tr><td>Pitch Voice probability Intensity Loudness</td></tr></table>",
                "type_str": "table",
                "text": "Correlations between several visual and acoustic features. Visual features: AU6 Cheek raise, AU12 Lip corner pull, AU45 Blink eye and closure, AU1,1+4 Distress brow. Acoustic features: Pitch, Voice probability, Intensity, Energy. *Correlation is significant at the 0.05 level (1-tailed).",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Video-level sentiment classification with linguistic, acoustic, and visual features.",
                "html": null,
                "num": null
            }
        }
    }
}