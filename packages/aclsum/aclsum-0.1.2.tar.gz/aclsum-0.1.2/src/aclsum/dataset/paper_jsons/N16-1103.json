{
    "paper_id": "N16-1103",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:35:52.366904Z"
    },
    "title": "Multilingual Relation Extraction using Compositional Universal Schema",
    "authors": [
        {
            "first": "Patrick",
            "middle": [],
            "last": "Verga",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Massachusetts Amherst",
                "location": {}
            },
            "email": ""
        },
        {
            "first": "David",
            "middle": [],
            "last": "Belanger",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Massachusetts Amherst",
                "location": {}
            },
            "email": "belanger@cs.umass.edu"
        },
        {
            "first": "Emma",
            "middle": [],
            "last": "Strubell",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Massachusetts Amherst",
                "location": {}
            },
            "email": "strubell@cs.umass.edu"
        },
        {
            "first": "Benjamin",
            "middle": [],
            "last": "Roth",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Massachusetts Amherst",
                "location": {}
            },
            "email": "beroth@cs.umass.edu"
        },
        {
            "first": "Andrew",
            "middle": [],
            "last": "Mccallum",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Massachusetts Amherst",
                "location": {}
            },
            "email": "mccallum@cs.umass.edu"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Universal schema builds a knowledge base (KB) of entities and relations by jointly embedding all relation types from input KBs as well as textual patterns observed in raw text. In most previous applications of universal schema, each textual pattern is represented as a single embedding, preventing generalization to unseen patterns. Recent work employs a neural network to capture patterns' compositional semantics, providing generalization to all possible input text. In response, this paper introduces significant further improvements to the coverage and flexibility of universal schema relation extraction: predictions for entities unseen in training and multilingual transfer learning to domains with no annotation. We evaluate our model through extensive experiments on the English and Spanish TAC KBP benchmark, outperforming the top system from TAC 2013 slot-filling using no handwritten patterns or additional annotation. We also consider a multilingual setting in which English training data entities overlap with the seed KB, but Spanish text does not. Despite having no annotation for Spanish data, we train an accurate predictor, with additional improvements obtained by tying word embeddings across languages. Furthermore, we find that multilingual training improves English relation extraction accuracy. Our approach is thus suited to broad-coverage automated knowledge base construction in a variety of languages and domains.",
    "pdf_parse": {
        "paper_id": "N16-1103",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Universal schema builds a knowledge base (KB) of entities and relations by jointly embedding all relation types from input KBs as well as textual patterns observed in raw text. In most previous applications of universal schema, each textual pattern is represented as a single embedding, preventing generalization to unseen patterns. Recent work employs a neural network to capture patterns' compositional semantics, providing generalization to all possible input text. In response, this paper introduces significant further improvements to the coverage and flexibility of universal schema relation extraction: predictions for entities unseen in training and multilingual transfer learning to domains with no annotation. We evaluate our model through extensive experiments on the English and Spanish TAC KBP benchmark, outperforming the top system from TAC 2013 slot-filling using no handwritten patterns or additional annotation. We also consider a multilingual setting in which English training data entities overlap with the seed KB, but Spanish text does not. Despite having no annotation for Spanish data, we train an accurate predictor, with additional improvements obtained by tying word embeddings across languages. Furthermore, we find that multilingual training improves English relation extraction accuracy. Our approach is thus suited to broad-coverage automated knowledge base construction in a variety of languages and domains.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "The goal of automatic knowledge base construction (AKBC) is to build a structured knowledge base (KB) of facts using a noisy corpus of raw text evidence, and perhaps an initial seed KB to be augmented (Carlson et al., 2010; Suchanek et al., 2007; Bollacker et al., 2008) . AKBC supports downstream reasoning at a high level about extracted entities and their relations, and thus has broad-reaching applications to a variety of domains.",
                "cite_spans": [
                    {
                        "start": 201,
                        "end": 223,
                        "text": "(Carlson et al., 2010;",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 224,
                        "end": 246,
                        "text": "Suchanek et al., 2007;",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 247,
                        "end": 270,
                        "text": "Bollacker et al., 2008)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "One challenge in AKBC is aligning knowledge from a structured KB with a text corpus in order to perform supervised learning through distant supervision. Universal schema (Riedel et al., 2013) along with its extensions (Yao et al., 2013; Gardner et al., 2014; Neelakantan et al., 2015; Rocktaschel et al., 2015) , avoids alignment by jointly embedding KB relations, entities, and surface text patterns. This propagates information between KB annotation and corresponding textual evidence.",
                "cite_spans": [
                    {
                        "start": 170,
                        "end": 191,
                        "text": "(Riedel et al., 2013)",
                        "ref_id": null
                    },
                    {
                        "start": 218,
                        "end": 236,
                        "text": "(Yao et al., 2013;",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 237,
                        "end": 258,
                        "text": "Gardner et al., 2014;",
                        "ref_id": null
                    },
                    {
                        "start": 259,
                        "end": 284,
                        "text": "Neelakantan et al., 2015;",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 285,
                        "end": 310,
                        "text": "Rocktaschel et al., 2015)",
                        "ref_id": "BIBREF14"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The above applications of universal schema express each text relation as a distinct item to be embedded. This harms its ability to generalize to inputs not precisely seen at training time. Recently, Toutanova et al. (2015) addressed this issue by embedding text patterns using a deep sentence encoder, which captures the compositional semantics of textual relations and allows for prediction on inputs never seen before. This paper further expands the coverage abilities of universal schema relation extraction by introducing techniques for forming predictions for new entities unseen in training and even for new domains with no associated annotation. In the extreme example of domain adaptation to a completely new language, we may have limited linguistic resources or labeled data such as treebanks, and only rarely a KB with adequate coverage. Our method performs multilingual transfer learning, providing a predictive model for a language with no coverage in an existing KB, by leveraging common representations for shared entities across text corpora. As depicted in Figure 1 , we simply require that one language have an available KB of seed facts. We can further improve our models by tying a small set of word embeddings across languages using only simple knowledge about word-level translations, learning to embed semantically similar textual patterns from different languages into the same latent space.",
                "cite_spans": [
                    {
                        "start": 189,
                        "end": 222,
                        "text": "Recently, Toutanova et al. (2015)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 1080,
                        "end": 1081,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In extensive experiments on the TAC Knowledge Base Population (KBP) slot-filling benchmark we outperform the top 2013 system with an F1 score of 40.7 and perform relation extraction in Spanish with no labeled data or direct overlap between the Spanish training corpus and the training KB, demonstrating that our approach is wellsuited for broad-coverage AKBC in low-resource languages and domains. Interestingly, joint training with Spanish improves English accuracy.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "English Low-resource in KB not in KB Figure 1 : Splitting the entities in a multilingual AKBC training set into parts. We only require that entities in the two corpora overlap. Remarkably, we can train a model for the low-resource language even if entities in the lowresource language do not occur in the KB.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 44,
                        "end": 45,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "AKBC extracts unary attributes of the form (subject, attribute), typed binary relations of the form (subject, relation, object), or higher-order relations. We refer to subjects and objects as entities. This work focuses solely on extracting binary relations, though many of our techniques generalize naturally to unary prediction. Generally, for example in Freebase (Bollacker et al., 2008) , higher-order relations are expressed in terms of collections of binary relations.",
                "cite_spans": [
                    {
                        "start": 366,
                        "end": 390,
                        "text": "(Bollacker et al., 2008)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Background",
                "sec_num": "2"
            },
            {
                "text": "We now describe prior work on approaches to AKBC. They all aim to predict (s, r, o) triples, but differ in terms of: (1) input data leveraged, (2) types of annotation required, (3) definition of relation label schema, and (4) whether they are capable of predicting relations for entities unseen in the training data. Note that all of these methods require pre-processing to detect entities, which may result in additional KB construction errors.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Background",
                "sec_num": "2"
            },
            {
                "text": "A knowledge base is naturally described as a graph, in which entities are nodes and relations are labeled edges (Suchanek et al., 2007; Bollacker et al., 2008) . In the case of knowledge graph completion, the task is akin to link prediction, assuming an initial set of (s, r, o) triples. See Nickel et al. (2015) for a review. No accompanying text data is necessary, since links can be predicted using properties of the graph, such as transitivity. In order to generalize well, prediction is often posed as low-rank matrix or tensor factorization. A variety of model variants have been suggested, where the probability of a given edge existing depends on a multi-linear form (Nickel et al., 2011; Garc\u00eda-Dur\u00e1n et al., 2015; Yang et al., 2015; Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015) , or non-linear interactions between s, r, and o (Socher et al., 2013) . Other approaches model the compositionality of multi-hop paths, typically for question answering (Bordes et al., 2014; Gu et al., 2015; Neelakantan et al., 2015) .",
                "cite_spans": [
                    {
                        "start": 112,
                        "end": 135,
                        "text": "(Suchanek et al., 2007;",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 136,
                        "end": 159,
                        "text": "Bollacker et al., 2008)",
                        "ref_id": null
                    },
                    {
                        "start": 292,
                        "end": 312,
                        "text": "Nickel et al. (2015)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 675,
                        "end": 696,
                        "text": "(Nickel et al., 2011;",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 697,
                        "end": 723,
                        "text": "Garc\u00eda-Dur\u00e1n et al., 2015;",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 724,
                        "end": 742,
                        "text": "Yang et al., 2015;",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 743,
                        "end": 763,
                        "text": "Bordes et al., 2013;",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 764,
                        "end": 782,
                        "text": "Wang et al., 2014;",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 783,
                        "end": 800,
                        "text": "Lin et al., 2015)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 850,
                        "end": 871,
                        "text": "(Socher et al., 2013)",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 971,
                        "end": 992,
                        "text": "(Bordes et al., 2014;",
                        "ref_id": null
                    },
                    {
                        "start": 993,
                        "end": 1009,
                        "text": "Gu et al., 2015;",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 1010,
                        "end": 1035,
                        "text": "Neelakantan et al., 2015)",
                        "ref_id": "BIBREF13"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Relation Extraction as Link Prediction",
                "sec_num": "2.1"
            },
            {
                "text": "Here, the training data consist of (1) a text corpus, and (2) a KB of seed facts with provenance, i.e. supporting evidence, in the corpus. Given individual an individual sentence, and pre-specified entities, a classifier predicts whether the sentence expresses a relation from a target schema. To train such a classifier, KB facts need to be aligned with supporting evidence in the text, but this is often challenging. For example, not all sentences containing Barack and Michelle Obama state that they are married. A variety of one-shot and iterative methods have addressed the alignment problem (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Yao et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Min et al., 2013; Zeng et al., 2015) . An additional degree of freedom in these approaches is whether they classify individual sentences or predicting at the corpus level by aggregating information from all sentences containing a given pair of entities before prediction. The former approach is often preferable in practice, due to the simplicity of independently classifying individual sentences and the ease of associating each prediction with a provenance. Prior work has applied deep learning to small-scale relation extraction problems, where functional relationships are detected between common nouns (Li et al., 2015; dos Santos et al., 2015) . Xu et al. (2015) apply an LSTM to a parse path, while Zeng et al. (2015) use a CNN on the raw text, with a special temporal pooling operation to separately embed the text around each entity.",
                "cite_spans": [
                    {
                        "start": 597,
                        "end": 623,
                        "text": "(Bunescu and Mooney, 2007;",
                        "ref_id": null
                    },
                    {
                        "start": 624,
                        "end": 643,
                        "text": "Mintz et al., 2009;",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 644,
                        "end": 664,
                        "text": "Riedel et al., 2010;",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 665,
                        "end": 682,
                        "text": "Yao et al., 2010;",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 683,
                        "end": 705,
                        "text": "Hoffmann et al., 2011;",
                        "ref_id": null
                    },
                    {
                        "start": 706,
                        "end": 728,
                        "text": "Surdeanu et al., 2012;",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 729,
                        "end": 746,
                        "text": "Min et al., 2013;",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 747,
                        "end": 765,
                        "text": "Zeng et al., 2015)",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 1336,
                        "end": 1353,
                        "text": "(Li et al., 2015;",
                        "ref_id": null
                    },
                    {
                        "start": 1354,
                        "end": 1378,
                        "text": "dos Santos et al., 2015)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 1381,
                        "end": 1397,
                        "text": "Xu et al. (2015)",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 1435,
                        "end": 1453,
                        "text": "Zeng et al. (2015)",
                        "ref_id": "BIBREF22"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Relation Extraction as Sentence Classification",
                "sec_num": "2.2"
            },
            {
                "text": "In the previous two approaches, prediction is carried out with respect to a fixed schema R of possible relations r. This may overlook salient relations that are expressed in the text but do not occur in the schema. In response, open-domain information extraction (OpenIE) lets the text speak for itself: R contains all possible patterns of text occurring between entities s and o (Banko et al., 2007; Etzioni et al., 2008; Yates and Etzioni, 2007) . These are obtained by filtering and normalizing the raw text. The approach offers impressive coverage, avoids issues of distant supervision, and provides a useful exploratory tool. On the other hand, OpenIE predictions are difficult to use in downstream tasks that expect information from a fixed schema.",
                "cite_spans": [
                    {
                        "start": 380,
                        "end": 400,
                        "text": "(Banko et al., 2007;",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 401,
                        "end": 422,
                        "text": "Etzioni et al., 2008;",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 423,
                        "end": 447,
                        "text": "Yates and Etzioni, 2007)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Open-Domain Relation Extraction",
                "sec_num": "2.3"
            },
            {
                "text": "Table 1 provides examples of OpenIE patterns. The examples in row two and three illustrate relational contexts for which similarity is difficult to be captured by an Ope-nIE approach because of their syntactically complex constructions. This motivates the technique in Section 3.2, which uses a deep architecture applied to raw tokens, instead of rigid rules for normalizing text to obtain patterns. ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "1",
                        "ref_id": "TABREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Open-Domain Relation Extraction",
                "sec_num": "2.3"
            },
            {
                "text": "When applying Universal Schema (Riedel et al., 2013) (USchema) to relation extraction, we combine the Ope-nIE and link-prediction perspectives. By jointly modeling both OpenIE patterns and the elements of a target schema, the method captures broader relational structure than multi-class classification approaches that just model the target schema. Furthermore, the method avoids the distant supervision alignment difficulties of Section 2.2. Riedel et al. (2013) augment a knowledge graph from a seed KB with additional edges corresponding to Ope-nIE patterns observed in the corpus. Even if the user does not seek to predict these new edges, a joint model over all edges can exploit regularities of the OpenIE edges to improve modeling of the labels from the target schema.",
                "cite_spans": [
                    {
                        "start": 31,
                        "end": 52,
                        "text": "(Riedel et al., 2013)",
                        "ref_id": null
                    },
                    {
                        "start": 443,
                        "end": 463,
                        "text": "Riedel et al. (2013)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Universal Schema",
                "sec_num": "2.4"
            },
            {
                "text": "The data still consist of (s, r, o) triples, which can be predicted using link-prediction techniques such as lowrank factorization. Riedel et al. (2013) explore a variety of approximations to the 3-mode (s, r, o) tensor. One such probabilistic model is:",
                "cite_spans": [
                    {
                        "start": 132,
                        "end": 152,
                        "text": "Riedel et al. (2013)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Universal Schema",
                "sec_num": "2.4"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "P ((s, r, o)) = u > s,o v r ,",
                        "eq_num": "(1)"
                    }
                ],
                "section": "Universal Schema",
                "sec_num": "2.4"
            },
            {
                "text": "where () is a sigmoid function, u s,o is an embedding of the entity pair (s, o), and v r is an embedding of the relation r, which may be an OpenIE pattern or a relation from the target schema. All of the exposition and results in this paper use this factorization, though many of the techniques we present later could be applied easily to the other factorizations described in Riedel et al. (2013) . Note that learning unique embeddings for OpenIE relations does not guarantee that similar patterns, such as the final two in Table 1 , will be embedded similarly.",
                "cite_spans": [
                    {
                        "start": 377,
                        "end": 397,
                        "text": "Riedel et al. (2013)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 531,
                        "end": 532,
                        "text": "1",
                        "ref_id": "TABREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Universal Schema",
                "sec_num": "2.4"
            },
            {
                "text": "As with most of the techniques in Section 2.1, the data only consist of positive examples of edges. The absence of an annotated edge does not imply that the edge is false. In fact, we seek to predict some of these missing edges as true. Riedel et al. (2013) employ the Bayesian Personalized Ranking (BPR) approach of Rendle et al. ( 2009), which does not explicitly model unobserved edges as negative, but instead seeks to rank the probability of observed triples above unobserved triples.",
                "cite_spans": [
                    {
                        "start": 237,
                        "end": 257,
                        "text": "Riedel et al. (2013)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Universal Schema",
                "sec_num": "2.4"
            },
            {
                "text": "Recently, Toutanova et al. ( 2015) extended USchema to not learn individual pattern embeddings v r , but instead to embed text patterns using a deep architecture applied to word tokens. This shares statistical strength between OpenIE patterns with similar words. We leverage this approach in Section 3.2. Additional work has modeled the regularities of multi-hop paths through knowledge graph augmented with text patterns (Lao et al., 2011; Lao et al., 2012; Gardner et al., 2014; Neelakantan et al., 2015) .",
                "cite_spans": [
                    {
                        "start": 422,
                        "end": 440,
                        "text": "(Lao et al., 2011;",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 441,
                        "end": 458,
                        "text": "Lao et al., 2012;",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 459,
                        "end": 480,
                        "text": "Gardner et al., 2014;",
                        "ref_id": null
                    },
                    {
                        "start": 481,
                        "end": 506,
                        "text": "Neelakantan et al., 2015)",
                        "ref_id": "BIBREF13"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Universal Schema",
                "sec_num": "2.4"
            },
            {
                "text": "Much work has been done on multilingual word embeddings. Most of this work uses aligned sentences from the Europarl dataset (Koehn, 2005) to align word embeddings across languages (Gouws et al., 2015; Luong et al., 2015; Hermann and Blunsom, 2014) . Others (Mikolov et al., 2013; Faruqui et al., 2014) align separate singlelanguage embedding models using a word-level dictionary. Mikolov et al. (2013) use translation pairs to learn a linear transform from one embedding space to another.",
                "cite_spans": [
                    {
                        "start": 124,
                        "end": 137,
                        "text": "(Koehn, 2005)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 180,
                        "end": 200,
                        "text": "(Gouws et al., 2015;",
                        "ref_id": null
                    },
                    {
                        "start": 201,
                        "end": 220,
                        "text": "Luong et al., 2015;",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 221,
                        "end": 247,
                        "text": "Hermann and Blunsom, 2014)",
                        "ref_id": null
                    },
                    {
                        "start": 257,
                        "end": 279,
                        "text": "(Mikolov et al., 2013;",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 280,
                        "end": 301,
                        "text": "Faruqui et al., 2014)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 380,
                        "end": 401,
                        "text": "Mikolov et al. (2013)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Multilingual Embeddings",
                "sec_num": "2.5"
            },
            {
                "text": "However, very little work exists on multilingual relation extraction. Faruqui and Kumar (2015) perform multilingual OpenIE relation extraction by projecting all languages to English using Google translate. However, as explained in Section 2.3 the OpenIE paradigm is not amenable to prediction within a fixed schema. Further, their approach does not generalize to low-resource languages where translation is unavailable -while we use translation dictionaries to improve our results, our experiments demonstrate that our method is effective even without this resource.",
                "cite_spans": [
                    {
                        "start": 70,
                        "end": 94,
                        "text": "Faruqui and Kumar (2015)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Multilingual Embeddings",
                "sec_num": "2.5"
            },
            {
                "text": "Figure 2 : Universal Schema jointly embeds KB and textual relations from Spanish and English, learning dense representations for entity pairs and relations using matrix factorization. Cells with a 1 indicate triples observed during training (left). The bold score represents a test-time prediction by the model (right). Using transitivity through KB/English overlap and English/Spanish overlap, our model can predict that a text pattern in Spanish evidences a KB relation despite no overlap between Spanish/KB entity pairs. At train time we use BPR loss to maximize the inner product of entity pairs with KB relations and text patterns encoded using a bidirectional LSTM. At test time we score compatibility between embedded KB relations and encoded textual patterns using cosine similarity. In our Spanish model we treat embeddings for a small set of English/Spanish translation pairs as a single word, e.g. casado and married. .93",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Multilingual Embeddings",
                "sec_num": "2.5"
            },
            {
                "text": "orative filtering (Schein et al., 2002) : it is unclear how to form predictions for unseen entity pairs, without refactorizing the entire matrix or applying heuristics.",
                "cite_spans": [
                    {
                        "start": 18,
                        "end": 39,
                        "text": "(Schein et al., 2002)",
                        "ref_id": "BIBREF15"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Multilingual Embeddings",
                "sec_num": "2.5"
            },
            {
                "text": "In response, this paper re-purposes USchema as a means to train a sentence-level relation classifier, like those in Section 2.2. This allows us to avoid errors from aligning distant supervision to the corpus, but is more deployable for real world applications. It also provides opportunities in Section 3.4 to improve multilingual AKBC.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Multilingual Embeddings",
                "sec_num": "2.5"
            },
            {
                "text": "We produce predictions using a very simple approach: (1) scan the corpus and extract a large quantity of triplets (s, r text , o), where r text is an OpenIE pattern. For each triplet, if the similarity between the embedding of r text and the embedding of a target relation r schema is above some threshold, we predict the triplet (s, r schema , o), and its provenance is the input sentence containing (s, r text , o). We refer to this technique as pattern scoring. In our experiments, we use the cosine distance between the vectors (Figure 2 ). In Section 7.3, we discuss details for how to make this distance welldefined.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 540,
                        "end": 541,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Multilingual Embeddings",
                "sec_num": "2.5"
            },
            {
                "text": "The pattern scoring approach is subject to an additional cold start problem: input data may contain patterns unseen in training. This section describes a method for us-ing USchema to train a relation classifier that can take arbitrary context tokens (Section 2.3) as input.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Using a Compositional Sentence Encoder to Predict Unseen Text Patterns",
                "sec_num": "3.2"
            },
            {
                "text": "Fortunately, the cold start problem for context tokens is more benign than that of entities since we can exploit statistical regularities of text: similar sequences of context tokens should be embedded similarly. Therefore, following Toutanova et al. (2015) , we embed raw context tokens compositionally using a deep architecture. Unlike Riedel et al. (2013) , this requires no manual rules to map text to OpenIE patterns and can embed any possible input string. The modified USchema likelihood is:",
                "cite_spans": [
                    {
                        "start": 251,
                        "end": 257,
                        "text": "(2015)",
                        "ref_id": null
                    },
                    {
                        "start": 338,
                        "end": 358,
                        "text": "Riedel et al. (2013)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Using a Compositional Sentence Encoder to Predict Unseen Text Patterns",
                "sec_num": "3.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "P ((s, r, o)) = u > s,o Encoder(r) .",
                        "eq_num": "(2)"
                    }
                ],
                "section": "Using a Compositional Sentence Encoder to Predict Unseen Text Patterns",
                "sec_num": "3.2"
            },
            {
                "text": "Here, if r is raw text, then Encoder(r) is parameterized by a deep architecture. If r is from the target schema, Encoder(r) is a produced by a lookup table (as in traditional USchema). Though such an encoder increases the computational cost of test-time prediction over straightforward pattern matching, evaluating a deep architecture can be done in large batches in parallel on a GPU. Both convolutional networks (CNNs) and recurrent networks (RNNs) are reasonable encoder architectures, and we consider both in our experiments. CNNs have been useful in a variety of NLP applications (Collobert et al., 2011; Kalchbrenner et al., 2014; Kim, 2014) . Unlike Toutanova et al. (2015) , we also consider RNNs, specifically Long-Short Term Memory Networks (LSTMs) (Hochreiter and Schmidhuber, 1997) . LSTMs have proven successful in a variety of tasks requiring encoding sentences as vectors (Sutskever et al., 2014; Vinyals et al., 2014) . In our experiments, LSTMs outperform CNNs.",
                "cite_spans": [
                    {
                        "start": 585,
                        "end": 609,
                        "text": "(Collobert et al., 2011;",
                        "ref_id": null
                    },
                    {
                        "start": 610,
                        "end": 636,
                        "text": "Kalchbrenner et al., 2014;",
                        "ref_id": null
                    },
                    {
                        "start": 637,
                        "end": 647,
                        "text": "Kim, 2014)",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 657,
                        "end": 680,
                        "text": "Toutanova et al. (2015)",
                        "ref_id": null
                    },
                    {
                        "start": 759,
                        "end": 793,
                        "text": "(Hochreiter and Schmidhuber, 1997)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 887,
                        "end": 911,
                        "text": "(Sutskever et al., 2014;",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 912,
                        "end": 933,
                        "text": "Vinyals et al., 2014)",
                        "ref_id": "BIBREF18"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Using a Compositional Sentence Encoder to Predict Unseen Text Patterns",
                "sec_num": "3.2"
            },
            {
                "text": "There are two key differences between our sentence encoder and that of Toutanova et al. (2015) . First, we use the encoder at test time, since we process the context tokens for held-out data. On the other hand, Toutanova et al. ( 2015) adopt the transductive approach where the encoder is only used to help train better representations for the relations in the target schema; it is ignored when forming predictions. Second, we apply the encoder to the raw text between entities, while Toutanova et al. (2015) first perform syntactic dependency parsing on the data and then apply an encoder to the path between the two entities in the parse tree. We avoid parsing, since we seek to perform multilingual AKBC, and many languages lack linguistic resources such as treebanks. Even parsing nonnewswire English text, such as tweets, is extremely challenging.",
                "cite_spans": [
                    {
                        "start": 71,
                        "end": 94,
                        "text": "Toutanova et al. (2015)",
                        "ref_id": null
                    },
                    {
                        "start": 502,
                        "end": 508,
                        "text": "(2015)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Using a Compositional Sentence Encoder to Predict Unseen Text Patterns",
                "sec_num": "3.2"
            },
            {
                "text": "Despite the coverage advantages of using a deep sentence encoder, separately embedding each OpenIE pattern, as in Riedel et al. (2013) , has key advantages. In practice, we have found that many high-precision patterns occur quite frequently. For these, there is sufficient data to model them with independent embeddings per pattern, which imposes minimal inductive bias on the relationship between patterns. Furthermore, some discriminative phrases are idiomatic, i.e.. their meaning is not constructed compositionally from their constituents. For these, a sentence encoder may be inappropriate.",
                "cite_spans": [
                    {
                        "start": 114,
                        "end": 134,
                        "text": "Riedel et al. (2013)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Modeling Frequent Text Patterns",
                "sec_num": "3.3"
            },
            {
                "text": "Therefore, pattern embeddings and deep token-based encoders have very different strengths and weaknesses. One values specificity, and models the head of the text distribution well, while the other has high coverage and captures the tail. In experimental results, we demonstrate that an ensemble of both models performs substantially better than either in isolation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Modeling Frequent Text Patterns",
                "sec_num": "3.3"
            },
            {
                "text": "The models described in previous two sections provide broad-coverage relation extraction that can generalize to all possible input entities and text patterns, while avoiding error-prone alignment of distant supervision to a corpus. Next, we describe techniques for an even more challenging generalization task: relation classification for input sentences in completely different languages.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Multilingual Relation Extraction with Zero Annotation",
                "sec_num": "3.4"
            },
            {
                "text": "Training a sentence-level relation classifier, either using the alignment-based techniques of Section 2.2, or the alignment-free method of Section 3.1, requires an avail-able KB of seed facts that have supporting evidence in the corpus. Unfortunately, available KBs have low overlap with corpora in many languages, since KBs have cultural and geographical biases. In response, we perform multilingual relation extraction by jointly modeling a highresource language, such as English, and an alternative language with no KB annotation. This approach provides transfer learning of a predictive model to the alternative language, and generalizes naturally to modeling more languages.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Multilingual Relation Extraction with Zero Annotation",
                "sec_num": "3.4"
            },
            {
                "text": "Extending the training technique of Section 3.1 to corpora in multiple languages can be achieved by factorizing a matrix that mixes data from a KB and from the two corpora. In Figure 1 we split the entities of a multilingual training corpus into sets depending on whether they have annotation in a KB and what corpora they appear in. We can perform transfer learning of a relation extractor to the low-resource language if there are entity pairs occurring in the two corpora, even if there is no KB annotation for these pairs. Note that we do not use the entity pair embeddings at test time: They are used only to bridge the languages during training. To form predictions in the low-resource language, we can simply apply the pattern scoring approach of Section 3.1.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 183,
                        "end": 184,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Multilingual Relation Extraction with Zero Annotation",
                "sec_num": "3.4"
            },
            {
                "text": "In Section 5, we demonstrate that jointly learning models for English and Spanish, with no annotation for the Spanish data, provides fairly accurate Spanish AKBC, and even improves the performance of the English model. Note that we are not performing zero-shot learning of a Spanish model (Larochelle et al., 2008) . The relations in the target schema are language-independent concepts, and we have supervision for these in English.",
                "cite_spans": [
                    {
                        "start": 289,
                        "end": 314,
                        "text": "(Larochelle et al., 2008)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Multilingual Relation Extraction with Zero Annotation",
                "sec_num": "3.4"
            },
            {
                "text": "The sentence encoder approach of Section 3.2 is complementary to our multilingual modeling technique: we simply use a separate encoder for each language. This approach is sub-optimal, however, because each sentence encoder will have a separate matrix of word embeddings for its vocabulary, despite the fact that there may be considerable shared structure between the languages. In response, we propose a straightforward method for tying the parameters of the sentence encoders across languages.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Tied Sentence Encoders",
                "sec_num": "3.5"
            },
            {
                "text": "Drawing on the dictionary-based techniques described in Section 2.5, we first obtain a list of word-word translation pairs between the languages using a translation dictionary. The first layer of our deep text encoder consists of a word embedding lookup table. For the aligned word types, we use a single cross-lingual embedding. Details of our approach are described in Appendix 7.5. the FB15k dataset (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Yang et al., 2015; Toutanova et al., 2015) .",
                "cite_spans": [
                    {
                        "start": 403,
                        "end": 424,
                        "text": "(Bordes et al., 2013;",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 425,
                        "end": 443,
                        "text": "Wang et al., 2014;",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 444,
                        "end": 461,
                        "text": "Lin et al., 2015;",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 462,
                        "end": 480,
                        "text": "Yang et al., 2015;",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 481,
                        "end": 504,
                        "text": "Toutanova et al., 2015)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Tied Sentence Encoders",
                "sec_num": "3.5"
            },
            {
                "text": "Here, relation extraction is posed as link prediction on a subset of Freebase. This task does not capture the particular difficulties we address: (1) evaluation on entities and text unseen during training, and (2) zero-annotation learning of a predictor for a low-resource language.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Tied Sentence Encoders",
                "sec_num": "3.5"
            },
            {
                "text": "Also, note both Toutanova et al. ( 2015) and Riedel et al. ( 2013) explore the pros and cons of learning embeddings for entity pairs vs. separate embeddings for each entity. As this is orthogonal to our contributions, we only consider entity pair embeddings, which performed best in both works when given sufficient data.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Tied Sentence Encoders",
                "sec_num": "3.5"
            },
            {
                "text": "The aim of the TAC benchmark is to improve both coverage and quality of relation extraction evaluation compared to just checking the extracted facts against a knowledge base, which can be incomplete and where the provenances are not verified. In the slot-filling task, each system is given a set of paired query entities and relations or 'slots' to fill, and the goal is to correctly fill as many slots as possible along with provenance from the corpus. For example, given the query entity/relation pair (Barack Obama, per:spouse), the system should return the entity Michelle Obama along with sentence(s) whose text expresses that relation. The answers returned by all participating teams, along with a human search (with timeout), are judged manually for correctness, i.e. whether the provenance specified by the system indeed expresses the relation in question.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "TAC Slot-Filling Benchmark",
                "sec_num": "4.1"
            },
            {
                "text": "In addition to verifying our models on the 2013 and 2014 English slot-filling task, we evaluate our Spanish models on the 2012 TAC Spanish slot-filling evaluation. Because this TAC track was never officially run, the coverage of facts in the available annotation is very small, resulting in many correct predictions being marked incorrectly as precision errors. In response, we manually annotated all results returned by the models considered in Table 4 . Precision and recall are calculated with respect to the union of the TAC annotation and our new labeling1 .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 452,
                        "end": 453,
                        "text": "4",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "TAC Slot-Filling Benchmark",
                "sec_num": "4.1"
            },
            {
                "text": "Our retrieval pipeline first generates all valid slot filler candidates for each query entity and slot, based on entities extracted from the corpus using FACTORIE (Mc-Callum et al., 2009) to perform tokenization, segmentation, and entity extraction. We perform entity linking by heuristically linking all entity mentions from our text corpora to a Freebase entity using anchor text in Wikipedia. Making use of the fact that most Freebase entries contain a link to the corresponding Wikipedia page, we link all entity mentions from our text corpora to a Freebase entity by the following process: First, a set of candidate entities is obtained by following frequent link anchor text statistics. We then select that candidate entity for which the cosine similarity between the respective Wikipedia and the sentence context of the mention is highest, and link to that entity if a threshold is exceeded.",
                "cite_spans": [
                    {
                        "start": 163,
                        "end": 187,
                        "text": "(Mc-Callum et al., 2009)",
                        "ref_id": "BIBREF13"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Retrieval Pipeline",
                "sec_num": "4.2"
            },
            {
                "text": "An entity pair qualifies as a candidate prediction if it meets the type criteria for the slot. 2 The TAC 2013 English and Spanish newswire corpora each contain about 1 million newswire documents from 2009-2012. The document retrieval and entity matching components of our relation extraction pipeline are based on RelationFactory (Roth et al., 2014) , the top-ranked system of the 2013 English slot-filling task. We also use the English distantly supervised training data from this system, which aligns the TAC 2012 corpus to Freebase. More details on alignment are described in Appendix 7.4.",
                "cite_spans": [
                    {
                        "start": 330,
                        "end": 349,
                        "text": "(Roth et al., 2014)",
                        "ref_id": "BIBREF15"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Retrieval Pipeline",
                "sec_num": "4.2"
            },
            {
                "text": "As discussed in Section 3.3, models using a deep sentence encoder and using a pattern lookup table have complementary strengths and weaknesses. In response, we present results where we ensemble the outputs of the two models by simply taking the union of their individual outputs. Slightly higher results might be obtained through more sophisticated ensembling schemes.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Retrieval Pipeline",
                "sec_num": "4.2"
            },
            {
                "text": "All models are implemented in Torch (code publicly available3 ). Models are tuned to maximize F1 on the 2012 TAC KBP slot-filling evaluation. We additionally tune the thresholds of our pattern scorer on a per-relation basis to maximize F1 using 2012 TAC slot-filling for English and the 2012 Spanish slot-filling development set for Spanish. As in Riedel et al. (2013) , we train using the BPR loss of Rendle et al. (2009) . Our CNN is implemented as described in Toutanova et al. ( 2015), using width-3 convolutions, followed by tanh and max pool layers. The LSTM uses a bi-directional architecture where the forward and backward representations of each hidden state are averaged, followed by max pooling over time. See Section 7.2",
                "cite_spans": [
                    {
                        "start": 348,
                        "end": 368,
                        "text": "Riedel et al. (2013)",
                        "ref_id": null
                    },
                    {
                        "start": 402,
                        "end": 422,
                        "text": "Rendle et al. (2009)",
                        "ref_id": "BIBREF13"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model Details",
                "sec_num": "4.3"
            },
            {
                "text": "We also report results including an alternate names (AN) heuristic, which uses automatically-extracted rules to detect the TAC 'alternate name' relation. To achieve this, we collect frequent Wikipedia link anchor texts for 2 Due to the difficulty of retrieval and entity detection, the maximum recall for predictions is limited. For this reason, Surdeanu et al. (2012) restrict the evaluation to answer candidates returned by their system and effectively rescaling recall. We do not perform such a re-scaling in our English results in order to compare to other reported results. Our Spanish numbers are rescaled. All scores reflect the 'anydoc' (relaxed) scoring to mitigate penalizing effects for systems not included in the evaluation pool. Table 3 : Precision, recall and F1 on the English TAC 2014 slot-filling task. Es refers to the addition of Spanish text at train time. The AN heuristic is ineffective on 2014 adding only 0.2 to F1. Our system would rank 4/18 in the official TAC 2014 competition behind systems that use hand-written patterns and active learning despite our system using neither of these additional annotations (Surdeanu and Ji., 2014).",
                "cite_spans": [
                    {
                        "start": 346,
                        "end": 368,
                        "text": "Surdeanu et al. (2012)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 749,
                        "end": 750,
                        "text": "3",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Model Details",
                "sec_num": "4.3"
            },
            {
                "text": "each query entity. If a high probability anchor text cooccurs with the canonical name of the query in the same document, we return the anchor text as a slot filler.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model",
                "sec_num": null
            },
            {
                "text": "In experiments on the English and Spanish TAC KBC slot-filling tasks, we find that both USchema and LSTM models outperform the CNN across languages, and that the LSTM tends to perform slightly better than USchema as the only model. Ensembling the LSTM and USchema models further increases final F1 scores in all experiments, suggesting that the two different types of model compliment each other well. Indeed, in Section 5.3 we present quantitative and qualitative analysis of our results which further confirms this hypothesis: the LSTM and USchema models each perform better on different pattern lengths and are characterized by different precision-recall tradeoffs. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Results",
                "sec_num": "5"
            },
            {
                "text": "Tables 2 and 3 present the performance of our models on the 2013 and 2014 English TAC slot-filling tasks.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "2",
                        "ref_id": "TABREF1"
                    },
                    {
                        "start": 13,
                        "end": 14,
                        "text": "3",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "English TAC Slot-filling Results",
                "sec_num": "5.1"
            },
            {
                "text": "Ensembling the LSTM and USchema models improves F1 by 2.2 points for 2013 and 1.7 points for 2014 over the strongest single model on both evaluations, LSTM.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "English TAC Slot-filling Results",
                "sec_num": "5.1"
            },
            {
                "text": "Adding the alternative names (AN) heuristic described in Section 4.3 increases F1 by an additional 2 points on 2013, resulting in an F1 score that is competitive with the state-of-the-art. We also demonstrate the effect of jointly learning English and Spanish models on English slot-filling performance. Adding Spanish data improves our F1 scores by 1.5 points on 2013 and 1.1 on 2014 over using English alone. This places are system higher than the top performer at the 2013 TAC slot-filling task even though our system uses no hand-written rules.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "English TAC Slot-filling Results",
                "sec_num": "5.1"
            },
            {
                "text": "The state of the art systems on this task all rely on matching handwritten patterns to find additional answers while our models use only automatically generated, indirect supervision; even our AN heuristics (Section 4.2) are automatically generated. The top two 2014 systems were Angeli et al. (2014) and RPI Blender (Surdeanu and Ji., 2014) who achieved F1 scores of 39.5 and 36.4 respectively. Both of these systems used additional active learning annotation. The third place team (Lin et al., 2014) relied on highly tuned patterns and rules and achieved an F1 score of 34.4.",
                "cite_spans": [
                    {
                        "start": 280,
                        "end": 300,
                        "text": "Angeli et al. (2014)",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 317,
                        "end": 341,
                        "text": "(Surdeanu and Ji., 2014)",
                        "ref_id": null
                    },
                    {
                        "start": 483,
                        "end": 501,
                        "text": "(Lin et al., 2014)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "English TAC Slot-filling Results",
                "sec_num": "5.1"
            },
            {
                "text": "Our model performs substantially better on 2013 than 2014 for two reasons. First, our RelationFactory (Roth et al., 2014) retrieval pipeline was a top retrieval pipeline on the 2013 task, but was outperformed on the 2014 task which introduced new challenges such as confusable entities. Second, improved training using active learning gave the top 2014 systems a boost in performance. No 2013 systems, including ours, use active learning. Bentor et al. (2014) , the 4th place team in the 2014 evaluation, used the same retrieval pipeline (Roth et al., 2014) as our model and achieved an F1 score of 32.1. ",
                "cite_spans": [
                    {
                        "start": 102,
                        "end": 121,
                        "text": "(Roth et al., 2014)",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 439,
                        "end": 459,
                        "text": "Bentor et al. (2014)",
                        "ref_id": null
                    },
                    {
                        "start": 538,
                        "end": 557,
                        "text": "(Roth et al., 2014)",
                        "ref_id": "BIBREF15"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "English TAC Slot-filling Results",
                "sec_num": "5.1"
            },
            {
                "text": "Table 4 presents 2012 Spanish TAC slot-filling results for our multilingual relation extractors trained using zeroannotation transfer learning. Tying word embeddings between the two languages results in substantial improvements for the LSTM. We see that ensembling the nondictionary LSTM with USchema gives a slight boost over USchema alone, but ensembling the dictionary-tied LSTM with USchema provides a significant increase of nearly 4 F1 points over the highest-scoring single model, USchema. Clearly, grounding the Spanish data using a translation dictionary provides much better Spanish word representations. These improvements are complementary to the baseline USchema model, and yield impressive results when ensembled.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "4",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Spanish TAC Slot-filling Results",
                "sec_num": "5.2"
            },
            {
                "text": "In addition to embedding semantically similar phrases from English and Spanish to have high similarity, our models also learn high-quality multilingual word embeddings. In Table 5 we compare Spanish nearest neighbors of English query words learned by the LSTM with dictionary ties versus the LSTM with no ties, using no unsupervised pre-training for the embeddings. Both approaches jointly embed Spanish and English word types, using shared entity embeddings, but the dictionary-tied model learns qualitatively better multilingual embeddings.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 178,
                        "end": 179,
                        "text": "5",
                        "ref_id": "TABREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Spanish TAC Slot-filling Results",
                "sec_num": "5.2"
            },
            {
                "text": "We further analyze differences between USchema and LSTM in order to better understand why ensembling the models results in the best performing system. Figure 3 depicts precision-recall curves for the two models on the 2013 slot-filling task. As observed in earlier results, the LSTM achieves higher recall at the loss of some precision, whereas USchema can make more precise predictions at a lower threshold for recall. In Figure 4 we observe evidence for these different precisionrecall trade-offs: USchema scores higher in terms of F1 on shorter patterns whereas the LSTM scores higher on longer patterns. As one would expect, USchema successfully matches more short patterns than the LSTM, making more precise predictions at the cost of being unable to predict on patterns unseen during training. The LSTM can predict using any text between entities observed at test time, gaining recall at the loss of precision. Combining the two models makes the most of their strengths and weaknesses, leading to the highest overall F1.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 430,
                        "end": 431,
                        "text": "4",
                        "ref_id": "FIGREF3"
                    }
                ],
                "eq_spans": [],
                "section": "USchema vs LSTM",
                "sec_num": "5.3"
            },
            {
                "text": "Qualitative analysis of our English models also suggests that our encoder-based models (LSTM) extract relations based on a wide range of semantically similar patterns that the pattern-matching model (USchema) is unable to score due to a lack of exact string match in the test data. For example, Table 6 lists three examples of the per:children relation that the LSTM finds which USchema does not, as well as three patterns that USchema does find. Though the LSTM patterns are all semantically and syntactically similar, they each contain different specific noun phrases, e.g. Lori, four children, toddler daughter, Lee and Albert, etc. Because these specific nouns weren't seen during training, USchema fails to find these patterns whereas the LSTM learns to ignore the specific nouns in favor of the overall pattern, that of a parent-child relationship in an obituary. USchema is limited to finding the relations represented by patterns observed during training, which limits the patterns matched at test-time to short and common patterns; all the USchema patterns matched at test time were similar to those listed in Table 6 : variants of 's son, '.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 301,
                        "end": 302,
                        "text": "6",
                        "ref_id": "TABREF5"
                    },
                    {
                        "start": 1125,
                        "end": 1126,
                        "text": "6",
                        "ref_id": "TABREF5"
                    }
                ],
                "eq_spans": [],
                "section": "USchema vs LSTM",
                "sec_num": "5.3"
            },
            {
                "text": "McGregor is survived by his wife, Lori, and four children, daughters Jordan, Taylor and Landri, and a son, Logan. In addition to his wife, Mays is survived by a toddler daughter and a son, Billy Mays Jr., who is in his 20s. Anderson is survived by his wife Carol, sons Lee and Albert, daughter Shirley Englebrecht and nine grandchildren. USchema Dio 's son, Dan Padavona, cautioned the memorial crowd to be screened regularly by a doctor and take care of themselves, something he said his father did not do. But Marshall 's son, Philip, told a different story. \"I'd rather have Sully doing this than some stranger, or some hotshot trying to be the next Billy Mays,\" said the guy who actually is the next Billy Mays, his son Billy Mays III. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "LSTM",
                "sec_num": null
            },
            {
                "text": "By jointly embedding English and Spanish corpora along with a KB, we can train an accurate Spanish relation extraction model using no direct annotation for relations in the Spanish data. This approach has the added benefit of providing significant accuracy improvements for the English model, outperforming the top system on the 2013 TAC KBC slot filling task, without using the hand-coded rules or additional annotations of alternative systems. By using deep sentence encoders, we can perform prediction for arbitrary input text and for entities unseen in training. Sentence encoders also provides opportunities to improve cross-lingual transfer learning by sharing word embeddings across languages. In future work we will apply this model to many more languages and domains besides newswire text. We would also like to avoid the entity detection problem by using a deep architecture to both identify entity mentions and identify relations between them.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "6"
            },
            {
                "text": "Methods3.1 Universal Schema as Sentence ClassifierSimilar to many link prediction approaches,(Riedel et al., 2013) perform transductive learning, where a model is learned jointly over train and test data. Predictions are made by using the model to identify edges that were unobserved in the test data but likely to be true. The approach is vulnerable to the cold start problem in collab-",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "We focus on the TAC KBP slot-filling task. Much related work on embedding knowledge bases evaluates on",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Following Surdeanu et al. (2012) we remove facts about undiscovered entities to correct for recall.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://github.com/patverga/ torch-relation-extraction",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "Many thanks to Arvind Neelakantan for good ideas and discussions. We also appreciate a generous hardware grant from nVidia. This work was supported in part by the Center for Intelligent Information Retrieval, in part by Defense Advanced Research Projects Agency (DARPA) under agreement #FA8750-13-2-0020 and contract #HR0011-15-2-0036, and in part by the National Science Foundation (NSF) grant numbers DMR-1534431, IIS-1514053 and CNS-0958392. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon, in part by DARPA via agreement #DFA8750-13-2-0020 and NSF grant #CNS-0958392. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgments",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "University of texas at austin kbp 2014 slot filling system: Bayesian logic programs for textual inference",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Angeli",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Proceedings of the Seventh Text Analysis Conference: Knowledge Base Population",
                "volume": "45",
                "issue": "",
                "pages": "2493--2537",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1406.3676"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Angeli et al.2014] Gabor Angeli, Sonal Gupta, Melvin Jose, Christopher D Manning, Christopher R\u00e9, Julie Tibshirani, Jean Y Wu, Sen Wu, and Ce Zhang. 2014. Stanfords 2014 slot filling systems. TAC KBP. [Banko et al.2007] Michele Banko, Michael J Cafarella, Stephen Soderland, Matt Broadhead, and Oren Etzioni. 2007. Open information extraction from the web. In International Joint Conference on Artificial Intelligence. [Bentor et al.2014] Yinon Bentor, Vidhoon Viswanathan, and Raymond Mooney. 2014. University of texas at austin kbp 2014 slot filling system: Bayesian logic programs for tex- tual inference. In Proceedings of the Seventh Text Analysis Conference: Knowledge Base Population (TAC 2014). [Bollacker et al.2008] Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of the ACM SIGMOD Interna- tional Conference on Management of Data. [Bordes et al.2013] Antoine Bordes, Nicolas Usunier, Alberto Garc\u00eda-Dur\u00e1n, Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling multi-relational data. In Advances in Neural Information Processing Systems. [Bordes et al.2014] Antoine Bordes, Sumit Chopra, and Jason Weston. 2014. Question answering with subgraph embed- dings. arXiv preprint arXiv:1406.3676. [Bunescu and Mooney2007] Razvan Bunescu and Raymond Mooney. 2007. Learning to extract relations from the web using minimal supervision. In Annual meeting-association for Computational Linguistics, volume 45, page 576. [Carlson et al.2010] Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R. Hruschka, and A. 2010. Toward an architecture for never-ending language learning. In In AAAI. [Collobert et al.2011] Ronan Collobert, Jason Weston, L\u00e9on Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493-2537.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Classifying relations by ranking with convolutional neural networks",
                "authors": [
                    {
                        "first": "Santos",
                        "middle": [],
                        "last": "Dos",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing",
                "volume": "1",
                "issue": "",
                "pages": "626--634",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "dos Santos et al.2015] C\u0131cero Nogueira dos Santos, Bing Xi- ang, and Bowen Zhou. 2015. Classifying relations by rank- ing with convolutional neural networks. In Proceedings of the 53rd Annual Meeting of the Association for Computa- tional Linguistics and the 7th International Joint Conference on Natural Language Processing, volume 1, pages 626-634.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Open information extraction from the web",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Etzioni",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Communications of the ACM",
                "volume": "51",
                "issue": "12",
                "pages": "68--74",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Etzioni et al.2008] Oren Etzioni, Michele Banko, Stephen Soderland, and Daniel S Weld. 2008. Open information extraction from the web. Communications of the ACM, 51(12):68-74.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Multilingual open relation extraction using crosslingual projection",
                "authors": [
                    {
                        "first": "Kumar",
                        "middle": [],
                        "last": "Faruqui",
                        "suffix": ""
                    },
                    {
                        "first": "Manaal",
                        "middle": [],
                        "last": "Faruqui",
                        "suffix": ""
                    },
                    {
                        "first": "Shankar",
                        "middle": [],
                        "last": "Kumar ; Manaal",
                        "suffix": ""
                    },
                    {
                        "first": "Jesse",
                        "middle": [],
                        "last": "Faruqui",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Dodge",
                        "suffix": ""
                    },
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Sujay",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Jauhar",
                        "suffix": ""
                    },
                    {
                        "first": "Eduard",
                        "middle": [],
                        "last": "Dyer",
                        "suffix": ""
                    },
                    {
                        "first": "Noah",
                        "middle": [
                            "A"
                        ],
                        "last": "Hovy",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Smith",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Retrofitting word vectors to semantic lexicons",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1503.06450"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Faruqui and Kumar2015] Manaal Faruqui and Shankar Kumar. 2015. Multilingual open relation extraction using cross- lingual projection. arXiv preprint arXiv:1503.06450. [Faruqui et al.2014] Manaal Faruqui, Jesse Dodge, Sujay K Jauhar, Chris Dyer, Eduard Hovy, and Noah A Smith. 2014. Retrofitting word vectors to semantic lexicons. arXiv preprint arXiv:1411.4166.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Combining two and three-way embeddings models for link prediction in knowledge bases",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Garc\u00eda-Dur\u00e1n",
                        "suffix": ""
                    }
                ],
                "year": 1997,
                "venue": "Gouws et al.2015] Stephan Gouws, Yoshua Bengio, and Greg Corrado. 2015. B IL BOWA : Fast Bilingual Distributed Representations without Word Alignments. Icml",
                "volume": "",
                "issue": "",
                "pages": "1--10",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1506.01094"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Garc\u00eda-Dur\u00e1n et al.2015] Alberto Garc\u00eda-Dur\u00e1n, Antoine Bor- des, Nicolas Usunier, and Yves Grandvalet. 2015. Combin- ing two and three-way embeddings models for link predic- tion in knowledge bases. CoRR, abs/1506.00999. [Gardner et al.2014] Matt Gardner, Partha Talukdar, Jayant Kr- ishnamurthy, and Tom Mitchell. 2014. Incorporating vector space similarity in random walk inference over knowledge bases. In Empirical Methods in Natural Language Process- ing. [Gouws et al.2015] Stephan Gouws, Yoshua Bengio, and Greg Corrado. 2015. B IL BOWA : Fast Bilingual Distributed Representations without Word Alignments. Icml, pages 1- 10. [Gu et al.2015] Kelvin Gu, John Miller, and Percy Liang. 2015. Traversing knowledge graphs in vector space. arXiv preprint arXiv:1506.01094. [Hermann and Blunsom2014] Karl Moritz Hermann and Phil Blunsom. 2014. Multilingual models for compositional dis- tributed semantics. arXiv preprint arXiv:1404.4641. [Hochreiter and Schmidhuber1997] Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. In Neural Computation. [Hoffmann et al.2011] Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S Weld. 2011.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Knowledge-based weak supervision for information extraction of overlapping relations",
                "authors": [],
                "year": 2014,
                "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies",
                "volume": "1",
                "issue": "",
                "pages": "541--550",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Knowledge-based weak supervision for information extrac- tion of overlapping relations. In Proceedings of the 49th An- nual Meeting of the Association for Computational Linguis- tics: Human Language Technologies-Volume 1, pages 541- 550. Association for Computational Linguistics. [Kalchbrenner et al.2014] Nal Kalchbrenner, Edward Grefen- stette, and Phil Blunsom. 2014. A convolutional neural network for modelling sentences. Proceedings of the 52nd Annual Meeting of the Association for Computational Lin- guistics, June.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Convolutional neural networks for sentence classification",
                "authors": [
                    {
                        "first": "Yoon",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Adam: A method for stochastic optimization. In 3rd International Conference for Learning Representations",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yoon Kim. 2014. Convolutional neural networks for sentence classification. EMNLP. [Kingma and Ba2015] Diederik Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In 3rd Inter- national Conference for Learning Representations (ICLR).",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Europarl: A parallel corpus for statistical machine translation",
                "authors": [
                    {
                        "first": "Philipp",
                        "middle": [],
                        "last": "Koehn",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "MT summit",
                "volume": "5",
                "issue": "",
                "pages": "79--86",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In MT summit, volume 5, pages 79-86. Citeseer.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Random walk inference and learning in a large scale knowledge base",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Lao",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1503.00185"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Lao et al.2011] Ni Lao, Tom Mitchell, and William W. Cohen. 2011. Random walk inference and learning in a large scale knowledge base. In Conference on Empirical Methods in Natural Language Processing. [Lao et al.2012] Ni Lao, Amarnag Subramanya, Fernando Pereira, and William W. Cohen. 2012. Reading the web with learned syntactic-semantic inference rules. In Joint Confer- ence on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. [Larochelle et al.2008] Hugo Larochelle, Dumitru Erhan, and Yoshua Bengio. 2008. Zero-data learning of new tasks. In National Conference on Artificial Intelligence. [Li et al.2015] Jiwei Li, Dan Jurafsky, and Eudard Hovy. 2015. When are tree structures necessary for deep learning of rep- resentations? arXiv preprint arXiv:1503.00185. [Lin et al.2014] Hailun Lin, Zeya Zhao, Yantao Jia, Yuanzhuo Wang, Jinhua Xiong, and Xiaojing Li. 2014. OpenKN at TAC KBP 2014.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Learning entity and relation embeddings for knowledge graph completion",
                "authors": [
                    {
                        "first": "Lin",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "151--159",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Lin et al.2015] Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. 2015. Learning entity and relation em- beddings for knowledge graph completion. In Proceedings of AAAI. [Luong et al.2015] Thang Luong, Hieu Pham, and Christo- pher D Manning. 2015. Bilingual word representations with monolingual quality in mind. In Proceedings of the 1st Work- shop on Vector Space Modeling for Natural Language Pro- cessing, pages 151-159.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "FACTORIE: Probabilistic programming via imperatively defined factor graphs",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Mccallum",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Neural Information Processing Systems (NIPS)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "McCallum et al.2009] Andrew McCallum, Karl Schultz, and Sameer Singh. 2009. FACTORIE: Probabilistic program- ming via imperatively defined factor graphs. In Neural In- formation Processing Systems (NIPS).",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Exploiting Similarities among Languages for Machine Translation",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Mikolov",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "1--10",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1309.4168v1"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Mikolov et al.2013] Tomas Mikolov, Quoc V Le, and Ilya Sutskever. 2013. Exploiting Similarities among Lan- guages for Machine Translation. In arXiv preprint arXiv:1309.4168v1, pages 1-10.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Distant supervision for relation extraction with an incomplete knowledge base",
                "authors": [
                    {
                        "first": "Min",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "HLT-NAACL",
                "volume": "",
                "issue": "",
                "pages": "777--782",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Min et al.2013] Bonan Min, Ralph Grishman, Li Wan, Chang Wang, and David Gondek. 2013. Distant supervision for relation extraction with an incomplete knowledge base. In HLT-NAACL, pages 777-782.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "A review of relational machine learning for knowledge graphs: From multirelational link prediction to automated knowledge graph construction",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Mintz",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Association for Computational Linguistics and International Joint Conference on Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "148--163",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1503.00759"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Mintz et al.2009] Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extrac- tion without labeled data. In Association for Computational Linguistics and International Joint Conference on Natural Language Processing. [Neelakantan et al.2015] Arvind Neelakantan, Benjamin Roth, and Andrew McCallum. 2015. Compositional vector space models for knowledge base completion. Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics. [Nickel et al.2011] Maximilian Nickel, Volker Tresp, and Hans- Peter Kriegel. 2011. A three-way model for collective learn- ing on multi-relational data. In International Conference on Machine Learning. [Nickel et al.2015] Maximilian Nickel, Kevin Murphy, Volker Tresp, and Evgeniy Gabrilovich. 2015. A review of rela- tional machine learning for knowledge graphs: From multi- relational link prediction to automated knowledge graph con- struction. arXiv preprint arXiv:1503.00759. [Rendle et al.2009] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2009. Bpr: Bayesian personalized ranking from implicit feedback. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, pages 452-461. AUAI Press. [Riedel et al.2010] Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling relations and their mentions without labeled text. In Machine Learning and Knowledge Discovery in Databases, pages 148-163. Springer. [Riedel et al.2013] Sebastian Riedel, Limin Yao, Andrew Mc- Callum, and Benjamin M. Marlin. 2013. Relation extraction with matrix factorization and universal schemas. In HLT- NAACL.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Injecting logical background knowledge into embeddings for relation extraction",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Rocktaschel",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Rocktaschel et al.2015] Tim Rocktaschel, Sameer Singh, and Sebastian Riedel. 2015. Injecting logical background knowledge into embeddings for relation extraction. In An- nual Conference of the North American Chapter of the Asso- ciation for Computational Linguistics (NAACL).",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Reasoning with neural tensor networks for knowledge base completion",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Roth",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval",
                "volume": "",
                "issue": "",
                "pages": "253--260",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Roth et al.2014] Benjamin Roth, Tassilo Barth, Grzegorz Chrupa\u0142a, Martin Gropp, and Dietrich Klakow. 2014. Rela- tionfactory: A fast, modular and effective system for knowl- edge base population. EACL 2014, page 89. [Schein et al.2002] Andrew I Schein, Alexandrin Popescul, Lyle H Ungar, and David M Pennock. 2002. Methods and metrics for cold-start recommendations. In Proceedings of the 25th annual international ACM SIGIR conference on Re- search and development in information retrieval, pages 253- 260. ACM. [Socher et al.2013] Richard Socher, Danqi Chen, Christopher D Manning, and Andrew Ng. 2013. Reasoning with neural ten- sor networks for knowledge base completion. In Advances in Neural Information Processing Systems. [Suchanek et al.2007] Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. Yago: A core of semantic knowledge. In Proceedings of the 16th International Con- ference on World Wide Web. [Surdeanu and Ji.2014] Mihai Surdeanu and Heng Ji. 2014. Overview of the english slot filling track at the tac2014 knowledge base population evaluation. Proc. Text Analysis Conference (TAC2014).",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Multiinstance multi-label learning for relation extraction",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Surdeanu",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning",
                "volume": "",
                "issue": "",
                "pages": "455--465",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Surdeanu et al.2012] Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati, and Christopher D Manning. 2012. Multi- instance multi-label learning for relation extraction. In Pro- ceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 455-465. Association for Com- putational Linguistics.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Representing text for joint embedding of text and knowledge bases",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sutskever et al.2014] Ilya Sutskever, Oriol Vinyals, and Quoc V. V Le. 2014. Sequence to sequence learning with neu- ral networks. In Advances in Neural Information Processing Systems. [Toutanova et al.2015] Kristina Toutanova, Danqi Chen, Patrick Pantel, Hoifung Poon, Pallavi Choudhury, and Michael Ga- mon. 2015. Representing text for joint embedding of text and knowledge bases. In Empirical Methods in Natural Lan- guage Processing (EMNLP).",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Knowledge graph embedding by translating on hyperplanes",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Vinyals",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence",
                "volume": "",
                "issue": "",
                "pages": "1112--1119",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Vinyals et al.2014] Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey Hinton. 2014. Grammar as a foreign language. In CoRR. [Wang et al.2014] Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. 2014. Knowledge graph embedding by translating on hyperplanes. In Proceedings of the Twenty- Eighth AAAI Conference on Artificial Intelligence, pages 1112-1119. Citeseer.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Classifying relations via long short term memory networks along shortest dependency paths",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xu et al.2015] Yan Xu, Lili Mou, Ge Li, Yunchuan Chen, Hao Peng, and Zhi Jin. 2015. Classifying relations via long short term memory networks along shortest dependency paths. In Proceedings of Conference on Empirical Methods in Natural Language Processing (to appear).",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Embedding entities and relations for learning and inference in knowledge bases",
                "authors": [
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "1013--1023",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yang et al.2015] Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. 2015. Embedding entities and relations for learning and inference in knowledge bases. In- ternational Conference on Learning Representations 2014. [Yao et al.2010] Limin Yao, Sebastian Riedel, and Andrew Mc- Callum. 2010. Collective cross-document relation extrac- tion without labelled data. In Proceedings of the 2010 Con- ference on Empirical Methods in Natural Language Process- ing, pages 1013-1023. Association for Computational Lin- guistics.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Yates and Etzioni2007] Alexander Yates and Oren Etzioni. 2007. Unsupervised resolution of objects and relations on the web",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Yao",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of the 2013 workshop on Automated knowledge base construction",
                "volume": "",
                "issue": "",
                "pages": "79--84",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yao et al.2013] Limin Yao, Sebastian Riedel, and Andrew Mc- Callum. 2013. Universal schema for entity type prediction. In Proceedings of the 2013 workshop on Automated knowl- edge base construction, pages 79-84. ACM. [Yates and Etzioni2007] Alexander Yates and Oren Etzioni. 2007. Unsupervised resolution of objects and relations on the web. In North American Chapter of the Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Distant supervision for relation extraction via piecewise convolutional neural networks",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Zeng",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zeng et al.2015] Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao. 2015. Distant supervision for relation extraction via piecewise convolutional neural networks. EMNLP.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "p e r: s p o u s e .. . p e r: b o rn _ in a rg 1 's w if e a rg 2 .. . a rg 1 w a s b o rn in a rg 2 .. . a rg 1 e s la e s p o s a d e a rg 2 .. . a rg 1 n a c i\u00f3 e n a rg 2 .. .",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Figure 3: Precision-Recall curves for USchema and LSTM on 2013 TAC slot-filling. USchema achieves higher precision values whereas LSTM has higher recall.",
                "uris": null,
                "fig_num": "3",
                "type_str": "figure"
            },
            "FIGREF3": {
                "num": null,
                "text": "Figure 4: F1 achieved by USchema vs. LSTM models for varying pattern token lengths on 2013 TAC slotfilling. LSTM performs better on longer patterns whereas USchema performs better on shorter patterns.",
                "uris": null,
                "fig_num": "4",
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table><tr><td>Sentence (context tokens italicized)</td><td>OpenIE pattern</td></tr><tr><td>Khan 's younger sister, Annapurna</td><td>arg1's * sister</td></tr><tr><td>Devi, who later married Shankar, de-</td><td>arg2</td></tr><tr><td>veloped into an equally accomplished</td><td/></tr><tr><td>master of the surbahar, but custom pre-</td><td/></tr><tr><td>vented her from performing in public.</td><td/></tr><tr><td>A professor emeritus at Yale, Mandel-</td><td>arg1 * moved with</td></tr><tr><td>brot was born in Poland but as a child</td><td>* family to arg2</td></tr><tr><td>moved with his family to Paris where</td><td/></tr><tr><td>he was educated.</td><td/></tr><tr><td>Kissel was born in Provo, Utah, but</td><td>arg1 * lived in</td></tr><tr><td>her family also lived in Reno.</td><td>arg2</td></tr><tr><td colspan=\"2\">Context tokens (italicized) consist of the text occurring</td></tr><tr><td colspan=\"2\">between entities (bold) in a sentence. OpenIE patterns are</td></tr><tr><td colspan=\"2\">obtained by normalizing the context tokens using hand-</td></tr><tr><td colspan=\"2\">coded rules. The top example expresses the per:siblings</td></tr><tr><td colspan=\"2\">relation and the bottom two examples both express the</td></tr><tr><td>per:cities of residence relation.</td><td/></tr></table>",
                "type_str": "table",
                "text": "Examples of sentences expressing relations.",
                "html": null,
                "num": null
            },
            "TABREF1": {
                "content": "<table><tr><td/><td/><td colspan=\"2\">Recall Precision F1</td></tr><tr><td>CNN</td><td/><td>31.6</td><td>36.8 34.1</td></tr><tr><td>LSTM</td><td/><td>32.2</td><td>39.6 35.5</td></tr><tr><td>USchema</td><td/><td>29.4</td><td>42.6 34.8</td></tr><tr><td>USchema+LSTM</td><td/><td>34.4</td><td>41.9 37.7</td></tr><tr><td>USchema+LSTM+Es</td><td/><td>38.1</td><td>40.2 39.2</td></tr><tr><td>USchema+LSTM+AN</td><td/><td>36.7</td><td>43.1 39.7</td></tr><tr><td colspan=\"2\">USchema+LSTM+Es+AN</td><td>40.2</td><td>41.2 40.7</td></tr><tr><td>Roth et al. (2014)</td><td/><td>35.8</td><td>45.7 40.2</td></tr><tr><td>Model</td><td colspan=\"3\">Recall Precision</td><td>F1</td></tr><tr><td>CNN</td><td/><td>28.1</td><td>29.0 28.5</td></tr><tr><td>LSTM</td><td/><td>27.3</td><td>32.9 29.8</td></tr><tr><td>USchema</td><td/><td>24.3</td><td>35.5 28.8</td></tr><tr><td>USchema+LSTM</td><td/><td>34.1</td><td>29.3 31.5</td></tr><tr><td>USchema+LSTM+Es</td><td/><td>34.4</td><td>31.0 32.6</td></tr></table>",
                "type_str": "table",
                "text": "Precision, recall and F1 on the English TAC 2013 slot-filling task. AN refers to alternative names heuristic and Es refers to the addition of Spanish text at train time. LSTM+USchema ensemble outperforms any single model, including the highly-tuned top 2013 system ofRoth et al. (2014), despite using no handwritten patterns.",
                "html": null,
                "num": null
            },
            "TABREF4": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Example English query words (not in translation dictionary) in bold with their top nearest neighbors by cosine similarity listed for the dictionary and no ties LSTM variants. Dictionary-tied nearest neighbors are consistently more relevant to the query word than untied.",
                "html": null,
                "num": null
            },
            "TABREF5": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Examples of the per:children relation discovered by the LSTM and Universal Schema. Entities are bold and patterns italicized. The LSTM models a richer set of patterns",
                "html": null,
                "num": null
            }
        }
    }
}