{
    "paper_id": "2021",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T11:54:20.698948Z"
    },
    "title": "Syntax-Enhanced Pre-trained Model",
    "authors": [
        {
            "first": "Zenan",
            "middle": [],
            "last": "Xu",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Sun Yat-sen University",
                "location": {
                    "settlement": "Guangzhou",
                    "country": "China"
                }
            },
            "email": ""
        },
        {
            "first": "Daya",
            "middle": [],
            "last": "Guo",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Sun Yat-sen University",
                "location": {
                    "settlement": "Guangzhou",
                    "country": "China"
                }
            },
            "email": ""
        },
        {
            "first": "Duyu",
            "middle": [],
            "last": "Tang",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Microsoft Research Asia",
                "location": {
                    "settlement": "Beijing",
                    "country": "China"
                }
            },
            "email": "dutang@microsoft.com"
        },
        {
            "first": "Qinliang",
            "middle": [],
            "last": "Su",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Sun Yat-sen University",
                "location": {
                    "settlement": "Guangzhou",
                    "country": "China"
                }
            },
            "email": "suqliang@mail.sysu.edu.cn"
        },
        {
            "first": "Linjun",
            "middle": [],
            "last": "Shou",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Microsoft Search Technology Center Asia",
                "location": {
                    "settlement": "Beijing",
                    "country": "China"
                }
            },
            "email": ""
        },
        {
            "first": "Ming",
            "middle": [],
            "last": "Gong",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Microsoft Search Technology Center Asia",
                "location": {
                    "settlement": "Beijing",
                    "country": "China"
                }
            },
            "email": ""
        },
        {
            "first": "Wanjun",
            "middle": [],
            "last": "Zhong",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Sun Yat-sen University",
                "location": {
                    "settlement": "Guangzhou",
                    "country": "China"
                }
            },
            "email": ""
        },
        {
            "first": "Xiaojun",
            "middle": [],
            "last": "Quan",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Sun Yat-sen University",
                "location": {
                    "settlement": "Guangzhou",
                    "country": "China"
                }
            },
            "email": ""
        },
        {
            "first": "Daxin",
            "middle": [],
            "last": "Jiang",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Microsoft Search Technology Center Asia",
                "location": {
                    "settlement": "Beijing",
                    "country": "China"
                }
            },
            "email": "djiang@microsoft.com"
        },
        {
            "first": "Nan",
            "middle": [],
            "last": "Duan",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Microsoft Research Asia",
                "location": {
                    "settlement": "Beijing",
                    "country": "China"
                }
            },
            "email": "nanduan@microsoft.com"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "We study the problem of leveraging the syntactic structure of text to enhance pre-trained models such as BERT and RoBERTa. Existing methods utilize syntax of text either in the pre-training stage or in the fine-tuning stage, so that they suffer from discrepancy between the two stages. Such a problem would lead to the necessity of having human-annotated syntactic information, which limits the application of existing methods to broader scenarios. To address this, we present a model that utilizes the syntax of text in both pre-training and fine-tuning stages. Our model is based on Transformer with a syntax-aware attention layer that considers the dependency tree of the text. We further introduce a new pre-training task of predicting the syntactic distance among tokens in the dependency tree. We evaluate the model on three downstream tasks, including relation classification, entity typing, and question answering. Results show that our model achieves state-of-the-art performance on six public benchmark datasets. We have two major findings. First, we demonstrate that infusing automatically produced syntax of text improves pre-trained models. Second, global syntactic distances among tokens bring larger performance gains compared to local head relations between contiguous tokens. 1 * Work is done during internship at Microsoft. \u2020 For questions, please contact D. Tang and Z. Xu.",
    "pdf_parse": {
        "paper_id": "2021",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "We study the problem of leveraging the syntactic structure of text to enhance pre-trained models such as BERT and RoBERTa. Existing methods utilize syntax of text either in the pre-training stage or in the fine-tuning stage, so that they suffer from discrepancy between the two stages. Such a problem would lead to the necessity of having human-annotated syntactic information, which limits the application of existing methods to broader scenarios. To address this, we present a model that utilizes the syntax of text in both pre-training and fine-tuning stages. Our model is based on Transformer with a syntax-aware attention layer that considers the dependency tree of the text. We further introduce a new pre-training task of predicting the syntactic distance among tokens in the dependency tree. We evaluate the model on three downstream tasks, including relation classification, entity typing, and question answering. Results show that our model achieves state-of-the-art performance on six public benchmark datasets. We have two major findings. First, we demonstrate that infusing automatically produced syntax of text improves pre-trained models. Second, global syntactic distances among tokens bring larger performance gains compared to local head relations between contiguous tokens. 1 * Work is done during internship at Microsoft. \u2020 For questions, please contact D. Tang and Z. Xu.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Pre-trained models such as BERT (Devlin et al., 2019) , GPT (Radford et al., 2018) , and RoBERTa (Liu et al., 2019) have advanced the state-of-the-art performances of various natural language processing tasks. The successful recipe is that a model is first pre-trained on a huge volume of unsupervised data with self-supervised objectives, and then is fine-tuned on supervised data with the same data scheme. Dominant pre-trained models represent a text as a sequence of tokens2 . The merits are that such basic text representations are available from vast amounts of unsupervised data, and that models pre-trained and fine-tuned with the same paradigm usually achieve good accuracy in practice (Guu et al., 2020) . However, an evident limitation of these methods is that richer syntactic structure of text is ignored.",
                "cite_spans": [
                    {
                        "start": 32,
                        "end": 53,
                        "text": "(Devlin et al., 2019)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 60,
                        "end": 82,
                        "text": "(Radford et al., 2018)",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 97,
                        "end": 115,
                        "text": "(Liu et al., 2019)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 695,
                        "end": 713,
                        "text": "(Guu et al., 2020)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this paper, we seek to enhance pre-trained models with syntax of text. Related studies attempt to inject syntax information either only in the finetuning stage (Nguyen et al., 2020; Sachan et al., 2020) , or only in the pre-training stage (Wang et al., 2020) , which results in discrepancies. When only fusing syntax information in the fine-tuning phase, Sachan et al. (2020) finds that there is no performance boost unless high quality human-annotated dependency parses are available. However, this requirement would limit the application of the model to broader scenarios where human-annotated dependency information is not available.",
                "cite_spans": [
                    {
                        "start": 163,
                        "end": 184,
                        "text": "(Nguyen et al., 2020;",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 185,
                        "end": 205,
                        "text": "Sachan et al., 2020)",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 242,
                        "end": 261,
                        "text": "(Wang et al., 2020)",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 358,
                        "end": 378,
                        "text": "Sachan et al. (2020)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "To address this, we conduct a large-scale study on injecting automatically produced syntax of text in both the pre-training and fine-tuning stages. We construct a pre-training dataset by applying an offthe-shelf dependency parser (Qi et al., 2020) to one billion sentences from common crawl news. With these data, we introduce a syntax-aware pretraining task, called dependency distance prediction, which predicts the syntactic distance between tokens in the dependency structure. Compared with the pre-training task of dependency head prediction (Wang et al., 2020) that only captures local syntactic relations among words, dependency distance prediction leverages global syntax of the text. In addition, we developed a syntax-aware attention layer, which can be conveniently integrated into Transformer (Vaswani et al., 2017) to allow tokens to selectively attend to contextual tokens based on their syntactic distance in the dependency structure.",
                "cite_spans": [
                    {
                        "start": 230,
                        "end": 247,
                        "text": "(Qi et al., 2020)",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 547,
                        "end": 566,
                        "text": "(Wang et al., 2020)",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 805,
                        "end": 827,
                        "text": "(Vaswani et al., 2017)",
                        "ref_id": "BIBREF23"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "We conduct experiments on entity typing, question answering and relation classification on six benchmark datasets. Experimental results show that our method achieves state-of-the-art performance on all six datasets. Further analysis shows that our model can indicate the importance of syntactic information on downstream tasks, and that the newly introduced dependency distance prediction task could capture the global syntax of the text, performs better than dependency head prediction. In addition, compared with experimental results of injecting syntax information in either the pre-training or fine-tuning stage, injecting syntax information in both stages achieves the best performance.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In summary, the contribution of this paper is threefold. (1) We demonstrate that infusing automatically produced dependency structures into the pre-trained model shows superior performance over downstream tasks. (2) We propose a syntax-aware attention layer and a pre-training task for infusing syntactic information into the pre-trained model.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "(3) We find that the newly introduced dependency distance prediction task performs better than the dependency head prediction task.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Our work involves injecting syntax information into pre-trained models. First, we will review recent studies on analyzing the knowledge presented in pre-trained models, and then we will introduce the existing methods that enhance pre-trained models with syntax information.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "With the huge success of pre-trained models (Devlin et al., 2019; Radford et al., 2018) in a wide range of NLP tasks, lots of works study to what extent pre-trained models inherently. Here, we will introduce recent works on probing linguistic information, factual knowledge, and symbolic reasoning ability from pre-trained models respectively. In terms of linguistic information, Hewitt and Manning (2019) learn a linear transformation to predict the depth of each word on a syntax tree based on their representation, which indicates that the syntax information is implicitly embedded in the BERT model. However, Yaushian et al. (2019) find that the attention scores calculated by pre-trained models seem to be inconsistent with human intuitions of hierarchical structures, and indicate that certain complex syntax information may not be naturally embedded in BERT. In terms of probing factual knowledge, Petroni et al. (2019) find that pretrained models are able to answer fact-filling cloze tests, which indicates that the pre-trained models have memorized factual knowledge. However, Poerner et al. (2019) argue that BERT's outstanding performance of answering fact-filling cloze tests is partly due to the reasoning of the surface form of name patterns. In terms of symbolic reasoning, Talmor et al. (2020) test the pre-trained models on eight reasoning tasks and find that the models completely fail on half of the tasks. Although probing knowledge from pre-trained model is a worthwhile area, it runs perpendicular to infusing knowledge into pre-trained models.",
                "cite_spans": [
                    {
                        "start": 44,
                        "end": 65,
                        "text": "(Devlin et al., 2019;",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 66,
                        "end": 87,
                        "text": "Radford et al., 2018)",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 613,
                        "end": 635,
                        "text": "Yaushian et al. (2019)",
                        "ref_id": "BIBREF31"
                    },
                    {
                        "start": 905,
                        "end": 926,
                        "text": "Petroni et al. (2019)",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 1087,
                        "end": 1108,
                        "text": "Poerner et al. (2019)",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 1290,
                        "end": 1310,
                        "text": "Talmor et al. (2020)",
                        "ref_id": "BIBREF22"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Probing Pre-trained Models",
                "sec_num": "2.1"
            },
            {
                "text": "Recently, there has been growing interest in enhancing pre-trained models with syntax of text. Existing methods attempt to inject syntax information in the fine-tuning stage or only in the pre-training stage.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Integrating Syntax into Pre-trained Models",
                "sec_num": "2.2"
            },
            {
                "text": "We first introduce related works that inject syntax in the fine-tuning stage. Nguyen et al. (2020) on the local relationship between two related tokens, which prevents each token from being able to perceive the information of the entire tree. Despite the success of utilizing syntax information, existing methods only consider the syntactic information of text in the pre-training or the fine-tuning stage so that they suffer from discrepancy between the pre-training and the fine-tuning stage. To bridge this gap, we conduct a large-scale study on injecting automatically produced syntax information in both the two stages. Compared with the head prediction task (Wang et al., 2020) that captures the local relationship, we introduce the dependency distance prediction task that leverages the global relationship to predict the distance of two given tokens.",
                "cite_spans": [
                    {
                        "start": 78,
                        "end": 98,
                        "text": "Nguyen et al. (2020)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 664,
                        "end": 683,
                        "text": "(Wang et al., 2020)",
                        "ref_id": "BIBREF24"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Integrating Syntax into Pre-trained Models",
                "sec_num": "2.2"
            },
            {
                "text": "In this paper, we adopt the dependency tree to express the syntax information. Such a tree structure is concise and only expresses necessary information for the parse (Jurafsky, 2000) . Meanwhile, its head-dependent relation can be viewed as an approximation to the semantic relationship between tokens, which is directly useful for capturing semantic information. The above advantages help our model make more effective use of syntax information. Another available type of syntax information is the constituency tree, which is used in Nguyen et al. (2020) . However, as pointed out in Jurafsky (2000) , the relationships between the tokens in dependency tree can directly reflect important syntax information, which is often buried in the more complex constituency trees. This property requires extra techniques to extracting relation among the words from a constituency tree (Jurafsky, 2000) 3 .",
                "cite_spans": [
                    {
                        "start": 167,
                        "end": 183,
                        "text": "(Jurafsky, 2000)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 536,
                        "end": 556,
                        "text": "Nguyen et al. (2020)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 586,
                        "end": 601,
                        "text": "Jurafsky (2000)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 877,
                        "end": 893,
                        "text": "(Jurafsky, 2000)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data Construction",
                "sec_num": "3"
            },
            {
                "text": "The dependency tree takes linguistic words as one of its basic units. However, most pre-trained models take subwords (also known as the word pieces) instead of the entire linguistic words as the input unit, and this necessitates us to extend the definition of the dependency tree to include subwords. Following Wang et al. (2020) , we will add edges 3 https://web.stanford.edu/\u02dcjurafsky/slp3/ from the first subword of v to all subwords of u, if there exists a relationship between linguistic word v and word u.",
                "cite_spans": [
                    {
                        "start": 311,
                        "end": 329,
                        "text": "Wang et al. (2020)",
                        "ref_id": "BIBREF24"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data Construction",
                "sec_num": "3"
            },
            {
                "text": "Based on the above extended definition, we build a pre-training dataset from open-domain sources. Specifically, we randomly collect 1B sentences from publicly released common crawl news datasets (Zellers et al., 2019) that contain English news articles crawled between December 2016 and March 2019. Considering its effectiveness and ability to expand to multiple languages, we adopt offthe-shelf Stanza4 to automatically generate the syntax information for each sentence. The average token length of each sentence is 25.34, and the average depth of syntax trees is 5.15.",
                "cite_spans": [
                    {
                        "start": 195,
                        "end": 217,
                        "text": "(Zellers et al., 2019)",
                        "ref_id": "BIBREF32"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data Construction",
                "sec_num": "3"
            },
            {
                "text": "In this section, we present the proposed Syntax-Enhanced PRE-trained Model (SEPREM). We first define the syntax distance between two tokens. Based on the syntax distance, we then introduce a syntax-aware attention layer to learn syntax-aware representation and a pre-training task to enable model to capture global syntactic relations among tokens.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Methodology",
                "sec_num": "4"
            },
            {
                "text": "Intuitively, the distance between two tokens on the syntactic tree may reflect the strength of their linguistic correlation. If two tokens are far away from each other on the syntactic tree, the strength of their linguistic correlation is likely weak. Thus, we define the distance of two tokens over the dependency tree as their syntactic distance. Specifically, we define the distance between the token v and token u as 1, i.e. d(v, u) = 1, if v is the head of u. If two tokens are not directly connected in the dependency graph, their distance is the summation of the distances between adjacent nodes on the path. If two tokens are separated in the graph, their distance is set to infinite. Taking the sentence \"My dog is playing frisbee outside the room.\" in Fig 1 as 5415 an example, d(playing, frisbee) equals 1 since the token \"playing\" is the head of the token \"frisbee\".",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 768,
                        "end": 770,
                        "text": "as",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Syntax Distance over Syntactic Tree",
                "sec_num": "4.1"
            },
            {
                "text": "We follow BERT (Devlin et al., 2019) and use the multi-layer bidirectional Transformer (Vaswani et al., 2017) as the model backbone. The model takes a sequence X as the input and applies N transformer layers to produce contextual representation:",
                "cite_spans": [
                    {
                        "start": 15,
                        "end": 36,
                        "text": "(Devlin et al., 2019)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 87,
                        "end": 109,
                        "text": "(Vaswani et al., 2017)",
                        "ref_id": "BIBREF23"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Syntax-Aware Transformer",
                "sec_num": "4.2"
            },
            {
                "text": "H n = transf ormer n ((1 -\u03b1)H n-1 + \u03b1 \u0124n-1 )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Syntax-Aware Transformer",
                "sec_num": "4.2"
            },
            {
                "text": "(1) where n \u2208 [1, N ] denotes the n-th layer of the model, \u0124 is the syntax-aware representation which will be described in Section 4.3, H 0 is embeddings of the sequence input X, and \u03b1 is a learnable variable.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Syntax-Aware Transformer",
                "sec_num": "4.2"
            },
            {
                "text": "However, the introduction of syntax-aware representation \u0124 in the Equation 1 changes the architecture of Transformer, invalidating the original weights from pre-trained model, such as BERT and RoBERTa. Instead, we introduce a learnable importance score \u03b1 that controls the proportion of integration between contextual and syntax-aware representation. When \u03b1 is equal to zero, the syntaxaware representation is totally excluded and the model is architectural identical to vanilla Transformer. Therefore, we initialize the parameter \u03b1 as the small but not zero value, which can help better fuse syntactic information into existing pre-trained models. We will discuss importance score \u03b1 in detailed in Section 5.6.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Syntax-Aware Transformer",
                "sec_num": "4.2"
            },
            {
                "text": "Each transformer layer transf ormer n contains an architecturally identical transformer block, which is composed of a multi-headed self-attention M ultiAttn (Vaswani et al., 2017) and a followed feed forward layer F F N . Formally, the output H n of the transformer block transf ormer n (H n-1 ) is computed as:",
                "cite_spans": [
                    {
                        "start": 157,
                        "end": 179,
                        "text": "(Vaswani et al., 2017)",
                        "ref_id": "BIBREF23"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Syntax-Aware Transformer",
                "sec_num": "4.2"
            },
            {
                "text": "G n = LN (M ultiAttn(H n-1 ) + H n-1 ) H n = LN (F F N (G n ) + G n ) (2)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Syntax-Aware Transformer",
                "sec_num": "4.2"
            },
            {
                "text": "where the input",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Syntax-Aware Transformer",
                "sec_num": "4.2"
            },
            {
                "text": "H n-1 is (1 -\u03b1)H n-1 + \u03b1 \u0124n-1",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Syntax-Aware Transformer",
                "sec_num": "4.2"
            },
            {
                "text": "and LN represents a layer normalization operation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Syntax-Aware Transformer",
                "sec_num": "4.2"
            },
            {
                "text": "In this section, we will introduce how to obtain the syntax-aware representation \u0124 used in syntaxaware transformer.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Syntax-aware Attention Layer",
                "sec_num": "4.3"
            },
            {
                "text": "Tree Structure Encoding We adopt a distance matrix D to encode the tree structure. The advantages of distance matrix D are that it can well preserve the hierarchical syntactic structure of text and can directly reflect the distance of two given tokens. Meanwhile, its uniqueness property guarantees the one-to-one mapping of the tree structure. Given a dependency tree, the element D i,j of distance matrix D in i-th row and j-th column is defined as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Syntax-aware Attention Layer",
                "sec_num": "4.3"
            },
            {
                "text": "D i,j = d(i, j), if exists a path from v i to v j , 0, if i = j and otherwise.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Syntax-aware Attention Layer",
                "sec_num": "4.3"
            },
            {
                "text": "(3) where v i and v j are tokens on the dependency tree. Based on the concept that distance is inversely proportional to importance, we normalize the matrix D and obtain the normalized correlation strength matrix D as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Syntax-aware Attention Layer",
                "sec_num": "4.3"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "Di,j = 1/D i,j z\u2208{y|D i,y =0} (1/D i,z ) , if D i,j = 0, 0, otherwise.",
                        "eq_num": "(4)"
                    }
                ],
                "section": "Syntax-aware Attention Layer",
                "sec_num": "4.3"
            },
            {
                "text": "Syntax-aware Representation Given the tree structure representation D and the contextual representation H n , we fuse the tree structure into the contextual representation as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Syntax-aware Attention Layer",
                "sec_num": "4.3"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "\u0124n = \u03c3(W 1 n H n + W 2 n DH n ) (",
                        "eq_num": "5"
                    }
                ],
                "section": "Syntax-aware Attention Layer",
                "sec_num": "4.3"
            },
            {
                "text": ")",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Syntax-aware Attention Layer",
                "sec_num": "4.3"
            },
            {
                "text": "where \u03c3 is the activation function, W 1 n and W 2 n \u2208 R d h \u00d7d h are model parameters. We can see that DH n allows one to aggregate information from others along the tree structure. The closer they are on the dependency tree, the larger the attention weight, and thus more information will be propagated to each other, and vice verse.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Syntax-aware Attention Layer",
                "sec_num": "4.3"
            },
            {
                "text": "To better understand the sentences, it is beneficial for model to be aware of the underlying syntax. To this end, a new pre-training task, named dependency distance prediction task (DP), is designed to enhance the model's ability of capturing global syntactic relations among tokens. Specifically, we first randomly mask some elements in the distance matrix D, e.g., supposed D i,j . Afterwards, the representations of tokens i and j from SEPREM are concatenated and fed into a linear classifier, which outputs the probabilities over difference distances. In all of our experiments, 15% of distance are masked at random. Similar to BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) , we conduct the following operations to boost the robustness. The distance in matrix D will be masked at 80% probability or replaced by a random integer with a probability of 10%. For the rest 10% probability, the distance will be maintained.",
                "cite_spans": [
                    {
                        "start": 637,
                        "end": 658,
                        "text": "(Devlin et al., 2019)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 671,
                        "end": 689,
                        "text": "(Liu et al., 2019)",
                        "ref_id": "BIBREF12"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Syntax-aware Pre-training Task",
                "sec_num": "4.4"
            },
            {
                "text": "During pre-training, in addition to the DP pretraining task, we also use the dependency head prediction (HP) task, which is used in Wang et al. (2020) to capture the local head relation among words, and the dynamic masked language model (MLM), which is used in Liu et al. (2019) to capture contextual information. The final loss for the pretraining is the summation of the training loss of DP, HP and MLM tasks.",
                "cite_spans": [
                    {
                        "start": 132,
                        "end": 150,
                        "text": "Wang et al. (2020)",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 261,
                        "end": 278,
                        "text": "Liu et al. (2019)",
                        "ref_id": "BIBREF12"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Syntax-aware Pre-training Task",
                "sec_num": "4.4"
            },
            {
                "text": "The implementation of SEPREM is based on Hug-gingFace's Transformer (Wolf et al., 2019) . To accelerate the training process, we initialize parameters from RoBERTa model released by Hugging-Face 5 , which contains 24 layers, with 1024 hidden states in each layer. The number of parameters of our model is 464M. We pre-train our model with 16 32G NVIDIA V100 GPUs for approximately two weeks. The batch size is set to 2048, and the total steps are 500000, of which 30000 is the warm up steps.",
                "cite_spans": [
                    {
                        "start": 68,
                        "end": 87,
                        "text": "(Wolf et al., 2019)",
                        "ref_id": "BIBREF29"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementation Details",
                "sec_num": "4.5"
            },
            {
                "text": "In both pre-training and fine-tuning stages, our model takes the syntax of the text as the additional input, which is pre-processed in advance. Specially, we obtain the dependency tree of each sentence via Stanza and then generate the normalized distance matrix.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementation Details",
                "sec_num": "4.5"
            },
            {
                "text": "In this section, we evaluate the proposed SEPREM on six benchmark datasets over three downstream tasks, i.e., entity typing, question answering and relation classification.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "5"
            },
            {
                "text": "The entity typing task requires the model to predict the type of a given entity based on its context. Two fine-grained public datasets, Open Entity (Choi et al., 2018) and FIGER (Ling et al., 2015) , are employed to evaluate our model. The statistics of the aforementioned datasets are shown in Table 1 . Following Wang et al. (2020) \"@\" is added before and after a certain entity, then the representation of the first special token \"@\" is adopted to predict the type of the given entity. To keep the evaluation criteria consistent with previous works (Shimaoka et al., 2016; Zhang et al., 2019; Peters et al., 2019; Wang et al., 2019; Xiong et al., 2020) , we adopt loose micro precision, recall, and F1 to evaluate model performance on Open Entity datasets. As for FIGER datasets, we utilize strict accuracy, loose macro-F1, and loose micro-F1 as evaluation metrics.",
                "cite_spans": [
                    {
                        "start": 148,
                        "end": 167,
                        "text": "(Choi et al., 2018)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 178,
                        "end": 197,
                        "text": "(Ling et al., 2015)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 315,
                        "end": 333,
                        "text": "Wang et al. (2020)",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 552,
                        "end": 575,
                        "text": "(Shimaoka et al., 2016;",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 576,
                        "end": 595,
                        "text": "Zhang et al., 2019;",
                        "ref_id": "BIBREF35"
                    },
                    {
                        "start": 596,
                        "end": 616,
                        "text": "Peters et al., 2019;",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 617,
                        "end": 635,
                        "text": "Wang et al., 2019;",
                        "ref_id": "BIBREF31"
                    },
                    {
                        "start": 636,
                        "end": 655,
                        "text": "Xiong et al., 2020)",
                        "ref_id": "BIBREF30"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 301,
                        "end": 302,
                        "text": "1",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Entity Typing",
                "sec_num": "5.1"
            },
            {
                "text": "Baselines NFGEC (Shimaoka et al., 2016) recursively composes representation of entity context and further incorporates an attention mechanism to capture fine-grained category memberships of an entity. KEPLER (Wang et al., 2019) infuses knowledge into the pre-trained models and jointly learns the knowledge embeddings and language representation. RoBERTa-large (continue training) learns on the proposed pre-training dataset under the same settings with SEPREM but only with dynamic MLM task. In addition, we also report the results of BERT-base (Devlin et al., 2019) , ERNIE (Zhang et al., 2019) , KnowBERT (Peters et al., 2019) , WKLM (Xiong et al., 2020) , RoBERTalarge, and K-adapter (Wang et al., 2020) for a full comparison.",
                "cite_spans": [
                    {
                        "start": 16,
                        "end": 39,
                        "text": "(Shimaoka et al., 2016)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 208,
                        "end": 227,
                        "text": "(Wang et al., 2019)",
                        "ref_id": "BIBREF31"
                    },
                    {
                        "start": 546,
                        "end": 567,
                        "text": "(Devlin et al., 2019)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 576,
                        "end": 596,
                        "text": "(Zhang et al., 2019)",
                        "ref_id": "BIBREF35"
                    },
                    {
                        "start": 599,
                        "end": 629,
                        "text": "KnowBERT (Peters et al., 2019)",
                        "ref_id": null
                    },
                    {
                        "start": 637,
                        "end": 657,
                        "text": "(Xiong et al., 2020)",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 688,
                        "end": 707,
                        "text": "(Wang et al., 2020)",
                        "ref_id": "BIBREF24"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Entity Typing",
                "sec_num": "5.1"
            },
            {
                "text": "As we can see in This improvement indicates that injecting syntactic information in both the pre-training and fine-tuning stages can make full use of the syntax of the text, thereby benefiting downstream tasks.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Results",
                "sec_num": null
            },
            {
                "text": "We use open-domain question answering (QA) task and commonsense QA task to evaluate the proposed model. Open-domain QA requires models to answer open-domain questions with the help of external resources such as materials of collected documents and webpages. We use SearchQA (Dunn et al., 2017) and QuasarT (Dhingra et al., 2017) for this task, and adopt ExactMatch (EM) and loose F1 scores as evaluation metrics. In this task, we first retrieve related paragraphs according to the question from external materials via the information retrieval system, and then a reading comprehension technique is adopted to extract possible answers from the above retrieved paragraphs. Following previous work (Lin et al., 2018) , we use the retrieved paragraphs provided by Wang et al. (2017b) for the two datasets. For fair comparison, we follow Wang et al. (2020) to use [<sep>, quesiton,</sep>, paragraph,</sep>] as the input, where <sep> is a special token in front of two segmants and </sep> is a special symbol to split two kinds of data types. We take the task as a multi-classification to fine-tune the model and use two linear layers over the last hidden features from models to predict the start and end positions of the answer span.",
                "cite_spans": [
                    {
                        "start": 274,
                        "end": 293,
                        "text": "(Dunn et al., 2017)",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 306,
                        "end": 328,
                        "text": "(Dhingra et al., 2017)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 695,
                        "end": 713,
                        "text": "(Lin et al., 2018)",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 760,
                        "end": 779,
                        "text": "Wang et al. (2017b)",
                        "ref_id": null
                    },
                    {
                        "start": 833,
                        "end": 851,
                        "text": "Wang et al. (2020)",
                        "ref_id": "BIBREF24"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Question Answering",
                "sec_num": "5.2"
            },
            {
                "text": "Commonsense QA aims to answer questions which require commonsense knowledge that is not explicitly expressed in the question. We use the public CosmosQA dataset (Huang et al., 2019) for this task, and the accuracy scores are used as evaluation metrics. The data statistics of the above three datasets are shown in Table 3 . In CosmosQA, each question has 4 candidate answers, and we concatenate the question together with each answer separately as [<sep>, context,</sep>, paragraph,</sep>] for input. The representation of the first token is adopted to calculate a score for this answer, and the answer with the highest score is regarded as the prediction answer for this question.",
                "cite_spans": [
                    {
                        "start": 161,
                        "end": 181,
                        "text": "(Huang et al., 2019)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 320,
                        "end": 321,
                        "text": "3",
                        "ref_id": "TABREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Question Answering",
                "sec_num": "5.2"
            },
            {
                "text": "Baselines BiDAF (Seo et al., 2017) is a bidirectional attention network to obtain query-aware context representation. AQA (Buck et al., 2018) adopts a reinforce-guide questions re-write system and generates answers according to the re-written questions. R\u02c63 (Wang et al., 2017a) selects the most confident paragraph with a designed reinforcement ranker. DSQA (Lin et al., 2018) employs a paragraph selector to remove paragraphs with noise and a paragraph reader to extract the correct answer from denoised paragraphs. Evidence Agg. (Wang et al., 2018) makes use of multiple passages to generate answers. BERT-FT RACE+SW AG (Huang et al., 2019) sequentially fine-tunes the BERT model on the RACE and SWAG datasets for knowledge transfer. Besides the aforementioned models, we also report the results of BERT (Xiong et al., 2020) , WKLM (Xiong et al., 2020) , WKLM + Ranking (Xiong et al., 2020) , RoBERTa-large, RoBERTa-large (continue training), and K-Adapter (Wang et al., 2020) for a detailed comparison.",
                "cite_spans": [
                    {
                        "start": 122,
                        "end": 141,
                        "text": "(Buck et al., 2018)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 258,
                        "end": 278,
                        "text": "(Wang et al., 2017a)",
                        "ref_id": null
                    },
                    {
                        "start": 359,
                        "end": 377,
                        "text": "(Lin et al., 2018)",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 532,
                        "end": 551,
                        "text": "(Wang et al., 2018)",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 623,
                        "end": 643,
                        "text": "(Huang et al., 2019)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 807,
                        "end": 827,
                        "text": "(Xiong et al., 2020)",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 835,
                        "end": 855,
                        "text": "(Xiong et al., 2020)",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 873,
                        "end": 893,
                        "text": "(Xiong et al., 2020)",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 960,
                        "end": 979,
                        "text": "(Wang et al., 2020)",
                        "ref_id": "BIBREF24"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Question Answering",
                "sec_num": "5.2"
            },
            {
                "text": "The results of the opendomain QA task are shown in Table 4 . We can see that the proposed SEPREM model brings significant gains of 3.1% and 8.4% in F1 scores, compared with RoBERTa-large (continue training) model. This may be partially attributed to the fact that, QA task requires a model to have reading comprehension ability (Wang et al., 2020) , and the introduced syntax information can guide the model to avoid concentrating on certain dispensable words and improve its reading comprehension capacity (Zhang et al., 2020) . Meanwhile, SEPREM achieves state-of-the-art results on the CosmosQA dataset, which demonstrates the effectiveness of the proposed SEPREM model. It can be also seen that the performance gains observed in CosmosQA are not as substantial as those in the open-domain QA tasks. We speculate that Cos-mosQA requires capacity for contextual commonsense reasoning and the lack of explicitly injection of commonsense knowledge into SEPREM model limits its improvement.",
                "cite_spans": [
                    {
                        "start": 328,
                        "end": 347,
                        "text": "(Wang et al., 2020)",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 507,
                        "end": 527,
                        "text": "(Zhang et al., 2020)",
                        "ref_id": "BIBREF36"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 57,
                        "end": 58,
                        "text": "4",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Experimental Results",
                "sec_num": null
            },
            {
                "text": "A relation classification task aims to predict the relation between two given entities in a sentence. We use a large-scale relation classification dataset TA-CRED (Zhang et al., 2017) for this task, and adopt Micro-precision, recall, and F1 scores as evaluation metrics. The statistics of the TACRED datasets are shown in Table 1 . Following Wang et al. (2020) , we add special tokens \"@\" and \"#\" before and after the first and second entity respectively. Then, the representations of the former token \"@\" and \"#\" are concatenated to perform relation classification.",
                "cite_spans": [
                    {
                        "start": 163,
                        "end": 183,
                        "text": "(Zhang et al., 2017)",
                        "ref_id": "BIBREF34"
                    },
                    {
                        "start": 342,
                        "end": 360,
                        "text": "Wang et al. (2020)",
                        "ref_id": "BIBREF24"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 328,
                        "end": 329,
                        "text": "1",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Relation Classification",
                "sec_num": "5.3"
            },
            {
                "text": "Baselines C-GCN (Zhang et al., 2018) encodes the dependency tree via graph convolutional networks for relation classification. BERT+MTB (Baldini Soares et al., 2019) trains relation representation by matching the blanks. We also include the baseline models of BERT-base (Zhang et al., 2019) , ERNIE (Zhang et al., 2019) , BERT-large (Baldini Soares et al., 2019) , KnowBERT (Peters et al., 2019) , KEPLER (Wang et al., 2019) , RoBERTa- large, RoBERTa-large (continue training), and K-Adapter (Wang et al., 2020) for a comprehensive comparison.",
                "cite_spans": [
                    {
                        "start": 16,
                        "end": 36,
                        "text": "(Zhang et al., 2018)",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 127,
                        "end": 165,
                        "text": "BERT+MTB (Baldini Soares et al., 2019)",
                        "ref_id": null
                    },
                    {
                        "start": 270,
                        "end": 290,
                        "text": "(Zhang et al., 2019)",
                        "ref_id": "BIBREF35"
                    },
                    {
                        "start": 299,
                        "end": 319,
                        "text": "(Zhang et al., 2019)",
                        "ref_id": "BIBREF35"
                    },
                    {
                        "start": 322,
                        "end": 362,
                        "text": "BERT-large (Baldini Soares et al., 2019)",
                        "ref_id": null
                    },
                    {
                        "start": 365,
                        "end": 395,
                        "text": "KnowBERT (Peters et al., 2019)",
                        "ref_id": null
                    },
                    {
                        "start": 405,
                        "end": 424,
                        "text": "(Wang et al., 2019)",
                        "ref_id": "BIBREF31"
                    },
                    {
                        "start": 492,
                        "end": 511,
                        "text": "(Wang et al., 2020)",
                        "ref_id": "BIBREF24"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Relation Classification",
                "sec_num": "5.3"
            },
            {
                "text": "Table 5 shows the performances of baseline models and the proposed SEPREM on TACRED. As we can see that the proposed syntax-aware pre-training tasks and syntaxaware attention mechanism can continuously bring gains in relation classification task and SEPREM outperforms baseline models overall. This further confirms the outstanding generalization capacity of our proposed model. It can be also seen that compared with K-Adapter model, the performance gains of SEPREM model observed in the TACRED dataset are not as substantial as that in Open Entity dataset. This may be partially due to the fact that K-Adapter also injects factual knowledge into the model, which may help in identifying relationships.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "5",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Experimental Results",
                "sec_num": null
            },
            {
                "text": "To investigate the impacts of various components in SEPREM, experiments are conducted for en-tity typing, question answering and relation classification tasks under the different corresponding benchmarks, i .e., Open Entity, CosmosQA, and TACRED, respectively. Note that due to the timeconsuming issue of training the models on entire data, we randomly sample 10 million sentences from the whole data to build a small dataset in this ablation study.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Ablation Study",
                "sec_num": "5.4"
            },
            {
                "text": "The results are illustrated in Figure 2 , in which we eliminate two syntax-aware pre-training tasks (i.e., HP and DP) and syntax-aware attention layer to evaluate their effectiveness. It can be seen that without using the syntax-aware attention layer, immediate performance degradation is observed, indicating that leveraging syntax-aware attention layer to learn syntax-aware representation could benefit the SEPREM. Another observation is that for all three experiments, eliminating DP pre-training task leads to worse empirical results. In other words, compared with existing method (i.e., head prediction task), the proposed dependency distance prediction task is more advantageous to various downstream tasks. This observation may be attributed to the fact that leveraging global syntactic correlation is more beneficial than considering local correlation. Moreover, significant performance gains can be obtained by simultaneously exploiting the two pre-training tasks and syntax-aware attention layer, which further confirms superiority of our pre-training architecture.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 38,
                        "end": 39,
                        "text": "2",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Ablation Study",
                "sec_num": "5.4"
            },
            {
                "text": "We conduct a case study to empirically explore the effectiveness of utilizing syntax information. In the case of relation classification task, we need to predict the relationship of two tokens in a sentence. As the three examples shown in Figure 3 , SEPREM can capture the syntax information by the dependency tree and make correct predictions. However, without utilizing syntax information, RoBERTa fails to recognize the correct relationship. To give further insight of how syntax information affects prediction, we also take case 1 for detailed analysis. The extracted dependency tree captures the close correlation of \"grew\" and \"Jersey\", which indicates that \"New Jersey\" is more likely to be a residence place. These results reflects that our model can better understand the global syntax relations among tokens by utilizing dependency tree.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 246,
                        "end": 247,
                        "text": "3",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Case Study",
                "sec_num": "5.5"
            },
            {
                "text": "Under the syntax-enhanced pre-trained framework introduced here, the contextual representation (H n ) and syntax-aware representation ( \u0124n ) are jointly optimized to abstract semantic information from sentences. An interesting question concerns how much syntactic information should be leveraged for our pre-trained model. In this regard, we further investigate the effect of the importance score \u03b1 on the aforementioned six downstream tasks, and the learned weights \u03b1 after fine-tuning SEPREM model are shown in Table 6 . We observe that the values of \u03b1 are in the range of 13% and 15% on six downstream datasets, which indicates that those downstream tasks require syntactic information to obtain the best performance and once again confirms the effectiveness of utilizing syntax information.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 519,
                        "end": 520,
                        "text": "6",
                        "ref_id": "TABREF6"
                    }
                ],
                "eq_spans": [],
                "section": "Analysis of Importance Score \u03b1",
                "sec_num": "5.6"
            },
            {
                "text": "To have a further insight of the effect brought by importance score \u03b1, we conduct experiments on SEPREM w/o \u03b1, which eliminates the \u03b1 in Equation 1 and equally integrates the syntaxaware and contextual representation, i.e., H n = transf ormer n (H n-1 + \u0124n-1 ). with the proposed SEPREM model. It can be seen in Table 6 that, the performances drop 1%\u223c3% on the six datasets when excluding the \u03b1. This observation indicates the necessity of introducing the \u03b1 to better integrate the syntax-aware and contextual representation.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 318,
                        "end": 319,
                        "text": "6",
                        "ref_id": "TABREF6"
                    }
                ],
                "eq_spans": [],
                "section": "Analysis of Importance Score \u03b1",
                "sec_num": "5.6"
            },
            {
                "text": "In this paper, we present SEPREM that leverage syntax information to enhance pre-trained models. To inject syntactic information, we introduce a syntax-aware attention layer and a newly designed pre-training task are proposed. Experimental results show that our method achieves state-of-theart performance over six datasets. Further analysis shows that the proposed dependency distance prediction task performs better than dependency head prediction task.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "6"
            },
            {
                "text": "Such tokens can be words or word pieces. We use token for clarity.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://github.com/stanfordnlp/stanza",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "We are grateful to Yeyun Gong, Ruize Wang and Junjie Huang for fruitful comments. We are obliged to Zijing Ou and Wenxuan Li for perfecting this article. We appreciate Genifer Zhao for beautifying the figures of this article. Zenan Xu and Qinliang Su are supported by the National Natural Science Foundation of China (No. 61806223, 61906217, U1811264), Key R&D Program of Guangdong Province (No. 2018B010107005), National Natural Science Foundation of Guangdong Province (No. 2021A1515012299). Zenan Xu and Qinliang Su are also supported by Huawei MindSpore.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgments",
                "sec_num": null
            },
            {
                "text": "SearchQA Quasar-T CosmosQA EM F1 EM F1 AccuracyBiDAF (Seo et al., 2017) 28.60 34.60 25.90 28.50 -AQA (Buck et al., 2018) 40.50 47.40 ---R\u02c63 (Wang et al., 2017a) 49.00 55.30 35.30 41.70 -DSQA (Lin et al., 2018) 49.00 55.30 42.30 49.30 -Evidence Agg. (Wang et al., 2018) 57.00 63.20 42.30 49.60 -BERT (Xiong et al., 2020) 57.10 61.90 40.40 46.10 -WKLM (Xiong et al., 2020) 58.70 63.30 43.70 49.90 -WKLM + Ranking (Xiong et al., 2020) 61.70 66.70 45.80 52.20 -BERT-FTRACE+SW AG (Huang et al., 2019) ----68.70 K-ADAPTER (Wang et al., 2020) 61 ",
                "cite_spans": [
                    {
                        "start": 53,
                        "end": 71,
                        "text": "(Seo et al., 2017)",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 101,
                        "end": 120,
                        "text": "(Buck et al., 2018)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 140,
                        "end": 160,
                        "text": "(Wang et al., 2017a)",
                        "ref_id": null
                    },
                    {
                        "start": 191,
                        "end": 209,
                        "text": "(Lin et al., 2018)",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 249,
                        "end": 268,
                        "text": "(Wang et al., 2018)",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 299,
                        "end": 319,
                        "text": "(Xiong et al., 2020)",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 350,
                        "end": 370,
                        "text": "(Xiong et al., 2020)",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 411,
                        "end": 431,
                        "text": "(Xiong et al., 2020)",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 475,
                        "end": 495,
                        "text": "(Huang et al., 2019)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 516,
                        "end": 535,
                        "text": "(Wang et al., 2020)",
                        "ref_id": "BIBREF24"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Matching the Blanks: Distributional Similarity for Relation Learning",
                "authors": [
                    {
                        "first": "Baldini",
                        "middle": [],
                        "last": "Livio",
                        "suffix": ""
                    },
                    {
                        "first": "Nicholas",
                        "middle": [],
                        "last": "Soares",
                        "suffix": ""
                    },
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Fitzgerald",
                        "suffix": ""
                    },
                    {
                        "first": "Tom",
                        "middle": [],
                        "last": "Ling",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Kwiatkowski",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "ACL",
                "volume": "",
                "issue": "",
                "pages": "2895--2905",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Livio Baldini Soares, Nicholas FitzGerald, Jeffrey Ling, and Tom Kwiatkowski. 2019. Matching the Blanks: Distributional Similarity for Relation Learn- ing. In ACL, pages 2895-2905.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Ask the Right Questions: Active Question Reformulation with Reinforcement Learning",
                "authors": [
                    {
                        "first": "Christian",
                        "middle": [],
                        "last": "Buck",
                        "suffix": ""
                    },
                    {
                        "first": "Jannis",
                        "middle": [],
                        "last": "Bulian",
                        "suffix": ""
                    },
                    {
                        "first": "Massimiliano",
                        "middle": [],
                        "last": "Ciaramita",
                        "suffix": ""
                    },
                    {
                        "first": "Andrea",
                        "middle": [],
                        "last": "Gesmundo",
                        "suffix": ""
                    },
                    {
                        "first": "Neil",
                        "middle": [],
                        "last": "Houlsby",
                        "suffix": ""
                    },
                    {
                        "first": "Wojciech",
                        "middle": [],
                        "last": "Gajewski",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "ICLR",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Christian Buck, Jannis Bulian, Massimiliano Cia- ramita, Andrea Gesmundo, Neil Houlsby, Wojciech Gajewski, and Wei Wang. 2018. Ask the Right Questions: Active Question Reformulation with Re- inforcement Learning. In ICLR.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Ultra-fine entity typing",
                "authors": [
                    {
                        "first": "Eunsol",
                        "middle": [],
                        "last": "Choi",
                        "suffix": ""
                    },
                    {
                        "first": "Omer",
                        "middle": [],
                        "last": "Levy",
                        "suffix": ""
                    },
                    {
                        "first": "Yejin",
                        "middle": [],
                        "last": "Choi",
                        "suffix": ""
                    },
                    {
                        "first": "Luke",
                        "middle": [],
                        "last": "Zettlemoyer",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "ACL",
                "volume": "",
                "issue": "",
                "pages": "87--96",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Eunsol Choi, Omer Levy, Yejin Choi, and Luke Zettle- moyer. 2018. Ultra-fine entity typing. In ACL, pages 87-96.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
                "authors": [
                    {
                        "first": "Jacob",
                        "middle": [],
                        "last": "Devlin",
                        "suffix": ""
                    },
                    {
                        "first": "Ming-Wei",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "Kenton",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Kristina",
                        "middle": [],
                        "last": "Toutanova",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "volume": "1",
                "issue": "",
                "pages": "4171--4186",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/N19-1423"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 1 (Long and Short Papers), pages 4171-4186.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Quasar: Datasets for question answering by search and reading",
                "authors": [
                    {
                        "first": "Bhuwan",
                        "middle": [],
                        "last": "Dhingra",
                        "suffix": ""
                    },
                    {
                        "first": "Kathryn",
                        "middle": [],
                        "last": "Mazaitis",
                        "suffix": ""
                    },
                    {
                        "first": "William",
                        "middle": [
                            "W"
                        ],
                        "last": "Cohen",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1707.03904"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Bhuwan Dhingra, Kathryn Mazaitis, and William W Cohen. 2017. Quasar: Datasets for question an- swering by search and reading. In arXiv preprint arXiv:1707.03904.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine",
                "authors": [
                    {
                        "first": "Matthew",
                        "middle": [],
                        "last": "Dunn",
                        "suffix": ""
                    },
                    {
                        "first": "Levent",
                        "middle": [],
                        "last": "Sagun",
                        "suffix": ""
                    },
                    {
                        "first": "Mike",
                        "middle": [],
                        "last": "Higgins",
                        "suffix": ""
                    },
                    {
                        "first": "V",
                        "middle": [
                            "Ugur"
                        ],
                        "last": "G\u00fcney",
                        "suffix": ""
                    },
                    {
                        "first": "Volkan",
                        "middle": [],
                        "last": "Cirik",
                        "suffix": ""
                    },
                    {
                        "first": "Kyunghyun",
                        "middle": [],
                        "last": "Cho",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1704.05179"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Matthew Dunn, Levent Sagun, Mike Higgins, V. Ugur G\u00fcney, Volkan Cirik, and Kyunghyun Cho. 2017. SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine. In ArXiv preprint arXiv:1704.05179.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Realm: Retrievalaugmented language model pre-training",
                "authors": [
                    {
                        "first": "Kelvin",
                        "middle": [],
                        "last": "Guu",
                        "suffix": ""
                    },
                    {
                        "first": "Kenton",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Zora",
                        "middle": [],
                        "last": "Tung",
                        "suffix": ""
                    },
                    {
                        "first": "Panupong",
                        "middle": [],
                        "last": "Pasupat",
                        "suffix": ""
                    },
                    {
                        "first": "Ming-Wei",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2002.08909"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu- pat, and Ming-Wei Chang. 2020. Realm: Retrieval- augmented language model pre-training. arXiv preprint arXiv:2002.08909.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "A structural probe for finding syntax in word representations",
                "authors": [
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Hewitt",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "volume": "1",
                "issue": "",
                "pages": "4129--4138",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "John Hewitt and Christopher D Manning. 2019. A structural probe for finding syntax in word represen- tations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 1 (Long and Short Papers), pages 4129-4138.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Cosmos QA: Machine reading comprehension with contextual commonsense reasoning",
                "authors": [
                    {
                        "first": "Lifu",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Le",
                        "middle": [],
                        "last": "Ronan",
                        "suffix": ""
                    },
                    {
                        "first": "Chandra",
                        "middle": [],
                        "last": "Bras",
                        "suffix": ""
                    },
                    {
                        "first": "Yejin",
                        "middle": [],
                        "last": "Bhagavatula",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Choi",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "EMNLP",
                "volume": "",
                "issue": "",
                "pages": "2391--2401",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Lifu Huang, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2019. Cosmos QA: Machine reading comprehension with contextual commonsense rea- soning. In EMNLP, pages 2391-2401.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Speech & language processing",
                "authors": [
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Jurafsky",
                        "suffix": ""
                    }
                ],
                "year": 2000,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dan Jurafsky. 2000. Speech & language processing. Pearson Education India.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Denoising distantly supervised open-domain question answering",
                "authors": [
                    {
                        "first": "Yankai",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Haozhe",
                        "middle": [],
                        "last": "Ji",
                        "suffix": ""
                    },
                    {
                        "first": "Zhiyuan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Maosong",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "ACL",
                "volume": "",
                "issue": "",
                "pages": "1736--1745",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yankai Lin, Haozhe Ji, Zhiyuan Liu, and Maosong Sun. 2018. Denoising distantly supervised open-domain question answering. In ACL, pages 1736-1745.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Design challenges for entity linking",
                "authors": [
                    {
                        "first": "Xiao",
                        "middle": [],
                        "last": "Ling",
                        "suffix": ""
                    },
                    {
                        "first": "Sameer",
                        "middle": [],
                        "last": "Singh",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [
                            "S"
                        ],
                        "last": "Weld",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "TACL",
                "volume": "3",
                "issue": "",
                "pages": "315--328",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xiao Ling, Sameer Singh, and Daniel S. Weld. 2015. Design challenges for entity linking. TACL, 3:315- 328.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Roberta: A robustly optimized bert pretraining approach",
                "authors": [
                    {
                        "first": "Yinhan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Myle",
                        "middle": [],
                        "last": "Ott",
                        "suffix": ""
                    },
                    {
                        "first": "Naman",
                        "middle": [],
                        "last": "Goyal",
                        "suffix": ""
                    },
                    {
                        "first": "Jingfei",
                        "middle": [],
                        "last": "Du",
                        "suffix": ""
                    },
                    {
                        "first": "Mandar",
                        "middle": [],
                        "last": "Joshi",
                        "suffix": ""
                    },
                    {
                        "first": "Danqi",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Omer",
                        "middle": [],
                        "last": "Levy",
                        "suffix": ""
                    },
                    {
                        "first": "Mike",
                        "middle": [],
                        "last": "Lewis",
                        "suffix": ""
                    },
                    {
                        "first": "Luke",
                        "middle": [],
                        "last": "Zettlemoyer",
                        "suffix": ""
                    },
                    {
                        "first": "Veselin",
                        "middle": [],
                        "last": "Stoyanov",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1907.11692"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man- dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining ap- proach. arXiv preprint arXiv:1907.11692.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Tree-structured attention with hierarchical accumulation",
                "authors": [
                    {
                        "first": "Xuan-Phi",
                        "middle": [],
                        "last": "Nguyen",
                        "suffix": ""
                    },
                    {
                        "first": "Shafiq",
                        "middle": [],
                        "last": "Joty",
                        "suffix": ""
                    },
                    {
                        "first": "Steven",
                        "middle": [],
                        "last": "Hoi",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "International Conference on Learning Representations",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xuan-Phi Nguyen, Shafiq Joty, Steven Hoi, and Richard Socher. 2020. Tree-structured attention with hierarchical accumulation. In International Conference on Learning Representations.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Knowledge enhanced contextual word representations",
                "authors": [
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Matthew E Peters",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Neumann",
                        "suffix": ""
                    },
                    {
                        "first": "L",
                        "middle": [],
                        "last": "Logan",
                        "suffix": ""
                    },
                    {
                        "first": "Roy",
                        "middle": [],
                        "last": "Robert",
                        "suffix": ""
                    },
                    {
                        "first": "Vidur",
                        "middle": [],
                        "last": "Schwartz",
                        "suffix": ""
                    },
                    {
                        "first": "Sameer",
                        "middle": [],
                        "last": "Joshi",
                        "suffix": ""
                    },
                    {
                        "first": "Noah",
                        "middle": [
                            "A"
                        ],
                        "last": "Singh",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Smith",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "EMNLP",
                "volume": "",
                "issue": "",
                "pages": "43--54",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Matthew E Peters, Mark Neumann, IV Logan, L Robert, Roy Schwartz, Vidur Joshi, Sameer Singh, and Noah A Smith. 2019. Knowledge enhanced con- textual word representations. In EMNLP, pages 43- 54.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Language models as knowledge bases?",
                "authors": [
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Petroni",
                        "suffix": ""
                    },
                    {
                        "first": "Tim",
                        "middle": [],
                        "last": "Rockt\u00e4schel",
                        "suffix": ""
                    },
                    {
                        "first": "Patrick",
                        "middle": [],
                        "last": "Lewis",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Bakhtin",
                        "suffix": ""
                    },
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Alexander",
                        "middle": [
                            "H"
                        ],
                        "last": "Miller",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Riedel",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "F. Petroni, Tim Rockt\u00e4schel, Patrick Lewis, A. Bakhtin, Y. Wu, Alexander H. Miller, and S. Riedel. 2019. Language models as knowledge bases? ArXiv, abs/1909.01066.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Bert is not a knowledge base (yet): Factual knowledge vs. name-based reasoning in unsupervised qa",
                "authors": [
                    {
                        "first": "Nina",
                        "middle": [],
                        "last": "Poerner",
                        "suffix": ""
                    },
                    {
                        "first": "Ulli",
                        "middle": [],
                        "last": "Waltinger",
                        "suffix": ""
                    },
                    {
                        "first": "Hinrich",
                        "middle": [],
                        "last": "Sch\u00fctze",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Nina Poerner, Ulli Waltinger, and Hinrich Sch\u00fctze. 2019. Bert is not a knowledge base (yet): Fac- tual knowledge vs. name-based reasoning in unsu- pervised qa. ArXiv, abs/1911.03681.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Stanza: A Python natural language processing toolkit for many human languages",
                "authors": [
                    {
                        "first": "Peng",
                        "middle": [],
                        "last": "Qi",
                        "suffix": ""
                    },
                    {
                        "first": "Yuhao",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Yuhui",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Bolton",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Peng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, and Christopher D. Manning. 2020. Stanza: A Python natural language processing toolkit for many human languages. In Proceedings of the 58th An- nual Meeting of the Association for Computational Linguistics: System Demonstrations.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Improving language understanding by generative pre-training",
                "authors": [
                    {
                        "first": "Alec",
                        "middle": [],
                        "last": "Radford",
                        "suffix": ""
                    },
                    {
                        "first": "Karthik",
                        "middle": [],
                        "last": "Narasimhan",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language under- standing by generative pre-training.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Do syntax trees help pretrained transformers extract information? arXiv preprint",
                "authors": [
                    {
                        "first": "Devendra",
                        "middle": [],
                        "last": "Singh Sachan",
                        "suffix": ""
                    },
                    {
                        "first": "Yuhao",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Peng",
                        "middle": [],
                        "last": "Qi",
                        "suffix": ""
                    },
                    {
                        "first": "William",
                        "middle": [],
                        "last": "Hamilton",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2008.09084"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Devendra Singh Sachan, Yuhao Zhang, Peng Qi, and William Hamilton. 2020. Do syntax trees help pre- trained transformers extract information? arXiv preprint arXiv:2008.09084.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Bidirectional attention flow for machine comprehension",
                "authors": [
                    {
                        "first": "Min",
                        "middle": [],
                        "last": "Joon",
                        "suffix": ""
                    },
                    {
                        "first": "Seo",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Aniruddha",
                        "middle": [],
                        "last": "Kembhavi",
                        "suffix": ""
                    },
                    {
                        "first": "Ali",
                        "middle": [],
                        "last": "Farhadi",
                        "suffix": ""
                    },
                    {
                        "first": "Hannaneh",
                        "middle": [],
                        "last": "Hajishirzi",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "5th International Conference on Learning Representations",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Min Joon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2017. Bidirectional attention flow for machine comprehension. In 5th Inter- national Conference on Learning Representations, ICLR 2017.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "An attentive neural architecture for fine-grained entity type classification",
                "authors": [
                    {
                        "first": "Sonse",
                        "middle": [],
                        "last": "Shimaoka",
                        "suffix": ""
                    },
                    {
                        "first": "Pontus",
                        "middle": [],
                        "last": "Stenetorp",
                        "suffix": ""
                    },
                    {
                        "first": "Kentaro",
                        "middle": [],
                        "last": "Inui",
                        "suffix": ""
                    },
                    {
                        "first": "Sebastian",
                        "middle": [],
                        "last": "Riedel",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the 5th Workshop on Automated Knowledge Base Construction(AKBC)",
                "volume": "",
                "issue": "",
                "pages": "69--74",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sonse Shimaoka, Pontus Stenetorp, Kentaro Inui, and Sebastian Riedel. 2016. An attentive neural ar- chitecture for fine-grained entity type classification. In Proceedings of the 5th Workshop on Automated Knowledge Base Construction(AKBC), pages 69- 74.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "olmpics-on what language model pre-training captures",
                "authors": [
                    {
                        "first": "Alon",
                        "middle": [],
                        "last": "Talmor",
                        "suffix": ""
                    },
                    {
                        "first": "Yanai",
                        "middle": [],
                        "last": "Elazar",
                        "suffix": ""
                    },
                    {
                        "first": "Yoav",
                        "middle": [],
                        "last": "Goldberg",
                        "suffix": ""
                    },
                    {
                        "first": "Jonathan",
                        "middle": [],
                        "last": "Berant",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Transactions of the Association for Computational Linguistics",
                "volume": "8",
                "issue": "",
                "pages": "743--758",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alon Talmor, Yanai Elazar, Yoav Goldberg, and Jonathan Berant. 2020. olmpics-on what language model pre-training captures. Transactions of the As- sociation for Computational Linguistics, 8:743-758.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Attention is all you need",
                "authors": [
                    {
                        "first": "Ashish",
                        "middle": [],
                        "last": "Vaswani",
                        "suffix": ""
                    },
                    {
                        "first": "Noam",
                        "middle": [],
                        "last": "Shazeer",
                        "suffix": ""
                    },
                    {
                        "first": "Niki",
                        "middle": [],
                        "last": "Parmar",
                        "suffix": ""
                    },
                    {
                        "first": "Jakob",
                        "middle": [],
                        "last": "Uszkoreit",
                        "suffix": ""
                    },
                    {
                        "first": "Llion",
                        "middle": [],
                        "last": "Jones",
                        "suffix": ""
                    },
                    {
                        "first": "Aidan",
                        "middle": [
                            "N"
                        ],
                        "last": "Gomez",
                        "suffix": ""
                    },
                    {
                        "first": "\u0141ukasz",
                        "middle": [],
                        "last": "Kaiser",
                        "suffix": ""
                    },
                    {
                        "first": "Illia",
                        "middle": [],
                        "last": "Polosukhin",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Advances in neural information processing systems",
                "volume": "",
                "issue": "",
                "pages": "5998--6008",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information pro- cessing systems, pages 5998-6008.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "K-adapter: Infusing knowledge into pre-trained models with adapters",
                "authors": [
                    {
                        "first": "Ruize",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Duyu",
                        "middle": [],
                        "last": "Tang",
                        "suffix": ""
                    },
                    {
                        "first": "Nan",
                        "middle": [],
                        "last": "Duan",
                        "suffix": ""
                    },
                    {
                        "first": "Zhongyu",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Xuanjing",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Cuihong",
                        "middle": [],
                        "last": "Cao",
                        "suffix": ""
                    },
                    {
                        "first": "Daxin",
                        "middle": [],
                        "last": "Jiang",
                        "suffix": ""
                    },
                    {
                        "first": "Ming",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2002.01808"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing Huang, Cuihong Cao, Daxin Jiang, Ming Zhou, et al. 2020. K-adapter: Infusing knowl- edge into pre-trained models with adapters. arXiv preprint arXiv:2002.01808.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Reinforced reader-ranker for open-domain question answering",
                "authors": [
                    {
                        "first": "Shuohang",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Mo",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaoxiao",
                        "middle": [],
                        "last": "Guo",
                        "suffix": ""
                    },
                    {
                        "first": "Zhiguo",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Tim",
                        "middle": [],
                        "last": "Klinger",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Shiyu",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "Gerald",
                        "middle": [],
                        "last": "Tesauro",
                        "suffix": ""
                    },
                    {
                        "first": "Bowen",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Jing",
                        "middle": [],
                        "last": "Jiang",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1709.00023"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang, Gerald Tesauro, Bowen Zhou, and Jing Jiang. 2017a. Re- inforced reader-ranker for open-domain question an- swering. arXiv preprint arXiv:1709.00023.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Evidence aggregation for answer re-ranking in opendomain question answering",
                "authors": [
                    {
                        "first": "Shuohang",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Mo",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Jing",
                        "middle": [],
                        "last": "Jiang",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaoxiao",
                        "middle": [],
                        "last": "Guo",
                        "suffix": ""
                    },
                    {
                        "first": "Shiyu",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "Zhiguo",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Tim",
                        "middle": [],
                        "last": "Klinger",
                        "suffix": ""
                    },
                    {
                        "first": "Gerald",
                        "middle": [],
                        "last": "Tesauro",
                        "suffix": ""
                    },
                    {
                        "first": "Murray",
                        "middle": [],
                        "last": "Campbell",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "ICLR",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Shuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaox- iao Guo, Shiyu Chang, Zhiguo Wang, Tim Klinger, Gerald Tesauro, and Murray Campbell. 2018. Ev- idence aggregation for answer re-ranking in open- domain question answering. In ICLR.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Gated self-matching networks for reading comprehension and question answering",
                "authors": [
                    {
                        "first": "Wenhui",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Nan",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Furu",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Baobao",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "Ming",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "ACL",
                "volume": "",
                "issue": "",
                "pages": "189--198",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Wenhui Wang, Nan Yang, Furu Wei, Baobao Chang, and Ming Zhou. 2017b. Gated self-matching net- works for reading comprehension and question an- swering. In ACL, pages 189-198.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Kepler: A unified model for knowledge embedding and pretrained language representation",
                "authors": [
                    {
                        "first": "Xiaozhi",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Tianyu",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Zhaocheng",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    },
                    {
                        "first": "Zhiyuan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Juanzi",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Jian",
                        "middle": [],
                        "last": "Tang",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1911.06136"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Xiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhiyuan Liu, Juanzi Li, and Jian Tang. 2019. Kepler: A unified model for knowledge embedding and pre- trained language representation. arXiv preprint arXiv:1911.06136.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Huggingface's transformers: Stateof-the-art natural language processing",
                "authors": [
                    {
                        "first": "Thomas",
                        "middle": [],
                        "last": "Wolf",
                        "suffix": ""
                    },
                    {
                        "first": "Lysandre",
                        "middle": [],
                        "last": "Debut",
                        "suffix": ""
                    },
                    {
                        "first": "Victor",
                        "middle": [],
                        "last": "Sanh",
                        "suffix": ""
                    },
                    {
                        "first": "Julien",
                        "middle": [],
                        "last": "Chaumond",
                        "suffix": ""
                    },
                    {
                        "first": "Clement",
                        "middle": [],
                        "last": "Delangue",
                        "suffix": ""
                    },
                    {
                        "first": "Anthony",
                        "middle": [],
                        "last": "Moi",
                        "suffix": ""
                    },
                    {
                        "first": "Pierric",
                        "middle": [],
                        "last": "Cistac",
                        "suffix": ""
                    },
                    {
                        "first": "Tim",
                        "middle": [],
                        "last": "Rault",
                        "suffix": ""
                    },
                    {
                        "first": "R\u00e9mi",
                        "middle": [],
                        "last": "Louf",
                        "suffix": ""
                    },
                    {
                        "first": "Morgan",
                        "middle": [],
                        "last": "Funtowicz",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "ArXiv",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier- ric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtow- icz, et al. 2019. Huggingface's transformers: State- of-the-art natural language processing. ArXiv, pages arXiv-1910.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model",
                "authors": [
                    {
                        "first": "Wenhan",
                        "middle": [],
                        "last": "Xiong",
                        "suffix": ""
                    },
                    {
                        "first": "Jingfei",
                        "middle": [],
                        "last": "Du",
                        "suffix": ""
                    },
                    {
                        "first": "William",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Wang",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Veselin",
                        "middle": [],
                        "last": "Stoyanov",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "ICLR",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Wenhan Xiong, Jingfei Du, William Yang Wang, and Veselin Stoyanov. 2020. Pretrained Encyclope- dia: Weakly Supervised Knowledge-Pretrained Lan- guage Model. In ICLR.",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "Multitree transformer: Integrating tree structures into self-attention",
                "authors": [
                    {
                        "first": "Wang",
                        "middle": [],
                        "last": "Yaushian",
                        "suffix": ""
                    },
                    {
                        "first": "Lee",
                        "middle": [],
                        "last": "Hung-Yi",
                        "suffix": ""
                    },
                    {
                        "first": "Chen",
                        "middle": [],
                        "last": "Yun-Nung",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Wang Yaushian, Lee Hung-Yi, and Chen Yun-Nung. 2019. Multitree transformer: Integrating tree struc- tures into self-attention. In Proceedings of the 2019 Conference on Empirical Methods in Natural Lan- guage Processing.",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "Defending against neural fake news",
                "authors": [
                    {
                        "first": "Rowan",
                        "middle": [],
                        "last": "Zellers",
                        "suffix": ""
                    },
                    {
                        "first": "Ari",
                        "middle": [],
                        "last": "Holtzman",
                        "suffix": ""
                    },
                    {
                        "first": "Hannah",
                        "middle": [],
                        "last": "Rashkin",
                        "suffix": ""
                    },
                    {
                        "first": "Yonatan",
                        "middle": [],
                        "last": "Bisk",
                        "suffix": ""
                    },
                    {
                        "first": "Ali",
                        "middle": [],
                        "last": "Farhadi",
                        "suffix": ""
                    },
                    {
                        "first": "Franziska",
                        "middle": [],
                        "last": "Roesner",
                        "suffix": ""
                    },
                    {
                        "first": "Yejin",
                        "middle": [],
                        "last": "Choi",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "32",
                "issue": "",
                "pages": "9054--9065",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. 2019. Defending against neural fake news. In H. Wallach, H. Larochelle, A. Beygelz- imer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 9054-9065. Curran Associates, Inc.",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "Graph Convolution over Pruned Dependency Trees Improves Relation Extraction",
                "authors": [
                    {
                        "first": "Yuhao",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Peng",
                        "middle": [],
                        "last": "Qi",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "EMNLP",
                "volume": "",
                "issue": "",
                "pages": "2205--2215",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yuhao Zhang, Peng Qi, and Christopher D. Manning. 2018. Graph Convolution over Pruned Dependency Trees Improves Relation Extraction. In EMNLP, pages 2205-2215.",
                "links": null
            },
            "BIBREF34": {
                "ref_id": "b34",
                "title": "Positionaware Attention and Supervised Data Improve Slot Filling",
                "authors": [
                    {
                        "first": "Yuhao",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Victor",
                        "middle": [],
                        "last": "Zhong",
                        "suffix": ""
                    },
                    {
                        "first": "Danqi",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Gabor",
                        "middle": [],
                        "last": "Angeli",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "EMNLP",
                "volume": "",
                "issue": "",
                "pages": "35--45",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yuhao Zhang, Victor Zhong, Danqi Chen, Gabor An- geli, and Christopher D. Manning. 2017. Position- aware Attention and Supervised Data Improve Slot Filling. In EMNLP, pages 35-45.",
                "links": null
            },
            "BIBREF35": {
                "ref_id": "b35",
                "title": "ERNIE: Enhanced Language Representation with Informative Entities",
                "authors": [
                    {
                        "first": "Zhengyan",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Xu",
                        "middle": [],
                        "last": "Han",
                        "suffix": ""
                    },
                    {
                        "first": "Zhiyuan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Xin",
                        "middle": [],
                        "last": "Jiang",
                        "suffix": ""
                    },
                    {
                        "first": "Maosong",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Qun",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "ACL",
                "volume": "",
                "issue": "",
                "pages": "1441--1451",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. 2019. ERNIE: En- hanced Language Representation with Informative Entities. In ACL, pages 1441-1451.",
                "links": null
            },
            "BIBREF36": {
                "ref_id": "b36",
                "title": "Sg-net: Syntax-guided machine reading comprehension",
                "authors": [
                    {
                        "first": "Zhuosheng",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Yuwei",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Junru",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Sufeng",
                        "middle": [],
                        "last": "Duan",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI-2020",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zhuosheng Zhang, Yuwei Wu, Junru Zhou, and Sufeng Duan. 2020. Sg-net: Syntax-guided machine read- ing comprehension. In Thirty-Fourth AAAI Confer- ence on Artificial Intelligence (AAAI-2020).",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 2: Ablation study of the SEPREM model on three different datasets over entity typing, question answering, and relation classification tasks. All the evaluation models are pre-trained on 10 million sentences.",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Figure 3: Case study results on the TACRED dataset of relation classification tasks. Models are required to predict the relation between tokens in orange and blue colors. Predictions with mark are the same with true labels.",
                "uris": null,
                "fig_num": "3",
                "type_str": "figure"
            },
            "TABREF1": {
                "content": "<table><tr><td>, special token</td></tr><tr><td>5 https://huggingface.co/transformers/</td></tr></table>",
                "type_str": "table",
                "text": "The statistics of the entity typing datasets, i.e., Open Entity and FIGER, and relation classification dataset TACRED. Label refers to type of a given entity or relation between two entities.",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table><tr><td>, our SEPREM outperforms all other baselines</td></tr><tr><td>on both entity typing datasets. In the Open En-</td></tr><tr><td>tity dataset, with the utility of the syntax of text,</td></tr><tr><td>SEPREM achieves an improvement of 3.6% in</td></tr><tr><td>micro-F1 score comparing with RoBERTa-large</td></tr><tr><td>(continue training) model. The result demonstrates</td></tr><tr><td>that the proposed syntax-aware pre-training tasks</td></tr><tr><td>and syntax-aware attention layer help to capture</td></tr><tr><td>the syntax of text, which is beneficial to predict the</td></tr><tr><td>types more accurately. As for the FIGER dataset,</td></tr><tr><td>which contains more labels about the type of entity,</td></tr><tr><td>SEPREM still brings an improvement in strict accu-</td></tr><tr><td>racy, macro-F1, and micro-F1. This demonstrates</td></tr></table>",
                "type_str": "table",
                "text": "",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table><tr><td>Dataset</td><td>Train</td><td>Dev</td><td>Test</td></tr><tr><td>SearchQA</td><td colspan=\"3\">99,811 13,893 27,247</td></tr><tr><td>Quasar-T</td><td colspan=\"2\">28,496 3,000</td><td>3,000</td></tr><tr><td colspan=\"3\">CosmosQA 25,588 3,000</td><td>7,000</td></tr></table>",
                "type_str": "table",
                "text": "Results for entity typing task on the OpenEntity and FIGER datasets.",
                "html": null,
                "num": null
            },
            "TABREF4": {
                "content": "<table><tr><td>the effectiveness of leveraging syntactic informa-</td></tr><tr><td>tion in tasks with more fine-grained information.</td></tr><tr><td>Specifically, compared with the K-adapter model,</td></tr><tr><td>our SEPREM model brings an improvement of</td></tr><tr><td>2.6% F1 score on Open Entity dataset. It is worth</td></tr><tr><td>noting that SEPREM model is complementary to</td></tr><tr><td>the K-adapter model, both of which inject syntactic</td></tr><tr><td>information into model during pre-training stage.</td></tr></table>",
                "type_str": "table",
                "text": "The statistics of the question answering datasets: SearchQA, Quasar-T and CosmosQA.",
                "html": null,
                "num": null
            },
            "TABREF6": {
                "content": "<table><tr><td>Datasets</td><td>Model</td><td colspan=\"2\">Performance Values of \u03b1</td></tr><tr><td>Open Entity</td><td>SEPREM</td><td>79.06</td><td>0.1334</td></tr><tr><td/><td>SEPREM w/o \u03b1</td><td>77.13</td><td>-</td></tr><tr><td>FIGER</td><td>SEPREM</td><td>82.05</td><td>0.1428</td></tr><tr><td/><td>SEPREM w/o \u03b1</td><td>79.54</td><td>-</td></tr><tr><td>SearchQA</td><td>SEPREM</td><td>67.74</td><td>0.1385</td></tr><tr><td/><td>SEPREM w/o \u03b1</td><td>66.31</td><td>-</td></tr><tr><td>Quasar-T</td><td>SEPREM</td><td>53.18</td><td>0.1407</td></tr><tr><td/><td>SEPREM w/o \u03b1</td><td>51.84</td><td>-</td></tr><tr><td>CosmosQA</td><td>SEPREM</td><td>82.37</td><td>0.1357</td></tr><tr><td/><td>SEPREM w/o \u03b1</td><td>81.06</td><td>-</td></tr><tr><td>TACRED</td><td>SEPREM</td><td>72.42</td><td>0.1407</td></tr><tr><td/><td>SEPREM w/o \u03b1</td><td>71.82</td><td>-</td></tr></table>",
                "type_str": "table",
                "text": "The pre-training settings of the SEPREM w/o \u03b1 model are the same The model's performance and the corresponding values of importance score \u03b1 after fine-tuning on six public benchmark datasets. Performance is under the evaluate metrics of either Mi-F1 or accuracy scores.",
                "html": null,
                "num": null
            }
        }
    }
}