{
    "paper_id": "E17-1004",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:00:56.382018Z"
    },
    "title": "Classifying Illegal Activities on Tor Network Based on Web Textual Contents",
    "authors": [
        {
            "first": "Mhd",
            "middle": [],
            "last": "Wesam",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Al",
            "middle": [],
            "last": "Nabki",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Le\u00f3n",
                "location": {
                    "country": "Spain"
                }
            },
            "email": ""
        },
        {
            "first": "Eduardo",
            "middle": [],
            "last": "Fidalgo",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Le\u00f3n",
                "location": {
                    "country": "Spain"
                }
            },
            "email": "eduardo.fidalgo@unileon.es"
        },
        {
            "first": "Enrique",
            "middle": [],
            "last": "Alegre",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Le\u00f3n",
                "location": {
                    "country": "Spain"
                }
            },
            "email": ""
        },
        {
            "first": "Ivan",
            "middle": [],
            "last": "De Paz",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Le\u00f3n",
                "location": {
                    "country": "Spain"
                }
            },
            "email": ""
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "The freedom of the Deep Web offers a safe place where people can express themselves anonymously but they also can conduct illegal activities. In this paper, we present and make publicly available 1 a new dataset for Darknet active domains, which we call it \"Darknet Usage Text Addresses\" (DUTA). We built DUTA by sampling the Tor network during two months and manually labeled each address into 26 classes. Using DUTA, we conducted a comparison between two well-known text representation techniques crossed by three different supervised classifiers to categorize the Tor hidden services. We also fixed the pipeline elements and identified the aspects that have a critical influence on the classification results. We found that the combination of TF-IDF words representation with Logistic Regression classifier achieves 96.6% of 10 folds cross-validation accuracy and a macro F1 score of 93.7% when classifying a subset of illegal activities from DUTA. The good performance of the classifier might support potential tools to help the authorities in the detection of these activities.",
    "pdf_parse": {
        "paper_id": "E17-1004",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "The freedom of the Deep Web offers a safe place where people can express themselves anonymously but they also can conduct illegal activities. In this paper, we present and make publicly available 1 a new dataset for Darknet active domains, which we call it \"Darknet Usage Text Addresses\" (DUTA). We built DUTA by sampling the Tor network during two months and manually labeled each address into 26 classes. Using DUTA, we conducted a comparison between two well-known text representation techniques crossed by three different supervised classifiers to categorize the Tor hidden services. We also fixed the pipeline elements and identified the aspects that have a critical influence on the classification results. We found that the combination of TF-IDF words representation with Logistic Regression classifier achieves 96.6% of 10 folds cross-validation accuracy and a macro F1 score of 93.7% when classifying a subset of illegal activities from DUTA. The good performance of the classifier might support potential tools to help the authorities in the detection of these activities.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "If we think about the web as an ocean of data, the Surface Web is no more than the slight waves that float on the top. While in the depth, there is a lot of sunken information that is not reached by the traditional search engines. The web can be divided into Surface Web and Deep Web. The Surface Web is the portion of the web that can be crawled and 1 The dataset is available upon request to the first author (email) .",
                "cite_spans": [
                    {
                        "start": 411,
                        "end": 418,
                        "text": "(email)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "indexed by the standard search engines, such as Google or Bing. However, despite their existence, there is still an enormous part of the web remained without indexing due to its vast size and the lack of hyperlinks, i.e. not referenced by the other web pages. This part, that can not be found using a search engine, is known as Deep Web (Noor et al., 2011; Boswell, 2016) . Additionally, the content might be locked and requires human interaction to access e.g. to solve a CAPTCHA or to enter a log-in credential to access. This type of web pages is referred to as \"database-driven\" websites. Moreover, the traditional search engines do not examine the underneath layers of the web, and consequently, do not reach the Deep Web. The Darknet, which is also known as Dark Web, is a subset of the Deep Web. It is not only not indexed and isolated, but also requires a specific software or a dedicated proxy server to access it. The Darknet works over a virtual sub-network of the World Wide Web (WWW) that provides an additional layer of anonymity for the network users. The most popular ones are \"The Onion Router\"2 also known as Tor network, \"Invisible Internet Project\" I2P3 , and Freenet4 . The community of Tor refers to Darknet websites as \"Hidden Services\" (HS) which can be accessed via a special browser called Tor Browser5 .",
                "cite_spans": [
                    {
                        "start": 337,
                        "end": 356,
                        "text": "(Noor et al., 2011;",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 357,
                        "end": 371,
                        "text": "Boswell, 2016)",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "A study by Bergman et al. (2001) has stated astonishing statistics about the Deep Web. For example, only on Deep Web there are more than 550 billion individual documents comparing to only 1 billion on Surface Web. Furthermore, in the study of Rudesill et al. (2015) they emphasized on the immensity of the Deep Web which was estimated to be 400 to 500 times wider than the Surface Web.",
                "cite_spans": [
                    {
                        "start": 11,
                        "end": 32,
                        "text": "Bergman et al. (2001)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 243,
                        "end": 265,
                        "text": "Rudesill et al. (2015)",
                        "ref_id": "BIBREF24"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The concepts of Darknet and Deep Net have ex-isted since the establishment of World Wide Web (WWW), but what make it very popular in the recent years is when the FBI had arrested Dread Pirate Roberts, the owner of Silk Road black market, in October 2013. The FBI has estimated the sales on Silk Road to be 1.2 Billion dollars by July 2013. The trading network covered among 150,000 anonymous customers and approximately 4,000 vendors (Rudesill et al., 2015) . The cryptocurrency (Nakamoto, 2008 ) is a hot topic in the field of Darknet since it anonymizes the financial transactions and hides the trading parties identities (Ron and Shamir, 2014) .",
                "cite_spans": [
                    {
                        "start": 434,
                        "end": 457,
                        "text": "(Rudesill et al., 2015)",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 479,
                        "end": 494,
                        "text": "(Nakamoto, 2008",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 624,
                        "end": 646,
                        "text": "(Ron and Shamir, 2014)",
                        "ref_id": "BIBREF23"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The Darknet is often associated with illegal activities. In a study carried out by Intelliagg group (2015) over 1K samples of hidden services, they claimed that 68% of Darknet contents would be illegal. Moore et at. (2016) showed, after analyzing 5K onion domains, that the most common usages for Tor HS are criminal and illegal activities, such as drugs, weapons and all kind of pornography.",
                "cite_spans": [
                    {
                        "start": 203,
                        "end": 222,
                        "text": "Moore et at. (2016)",
                        "ref_id": "BIBREF18"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "It is worth to mention about dramatic increase in the proliferation of Darknet domains which doubled their size from 30K to 60K between August 2015 and 2016 (Figure 1 ). However, the publicly reachable domains are no more than 6K to 7K due to the ambiguity nature of the Darknet (Ciancaglini et al., 2016) . Motivated by the critical buried contents on the Darknet and its high abuse, we focused our research in designing and building a system that classifies the illegitimate practices on Darknet. In this paper, we present the first publicly available dataset called \"Darknet Usage Text Addresses\" (DUTA) that is extracted from the Tor HS Darknet.",
                "cite_spans": [
                    {
                        "start": 279,
                        "end": 305,
                        "text": "(Ciancaglini et al., 2016)",
                        "ref_id": "BIBREF5"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 165,
                        "end": 166,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "DUTA contains 26 categories that cover all the legal and the illegal activities monitored on Darknet during our sampling period. Our objective is to create a precise categorization of the Darknet via classifying the textual content of the HS. In order to achieve our target, we designed and compared different combinations of some of the most wellknown text classification techniques by identifying the key stages that have a high influence on the method performance. We set a baseline methodology by fixing the elements of text classification pipeline which allows the scientific community to compare their future research with this baseline under the defined pipeline. The fixed methodology we propose might represent a significant contribution into a tool for the authorities who monitor the Darknet abuse.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The rest of the paper is organized as follows: Section 2 presents the related work. Next, Section 3 explains the proposed dataset DUTA and its characteristics. After that, Section 4 describes the set of the designed classification pipelines. Then, in Section 5 we discuss the experiments performed and the results. In Section 6 we describe the technical implementation details and how we employed the successful classifier in an application. Finally, in Section 7 we present our conclusions with a pointing to our future work.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In the recent years, many researchers have investigated the classification of the Surface Web (Dumais and Chen, 2000; Sun et al., 2002; Kan, 2004; Kan and Thi, 2005; Kaur, 2014) , and the Deep Web (Su et al., 2006; Xu et al., 2007; Barbosa et al., 2007; Lin et al., 2008; Zhao et al., 2008; Xian et al., 2009; Khelghati, 2016) . However, the Darknet classification literature is still in its early stages and specifically the classification of the illegal activities (Graczyk and Kinningham, 2015; Moore and Rid, 2016) . Kaur (2014) introduced an interesting survey covering several algorithms to classify web content, paying attention to its importance in the field of data mining. Furthermore, the survey included the pre-processing techniques that might help in features selection, like eliminating the HTML tags, punctuation marks and stemming. Kan et al. explored the use of Uniform Resource Locators (URL) in web classification by extracting the features through parsing and segmenting it (Kan, 2004; Kan and Thi, 2005) . These techniques can not be applied to Tor HS since the onion addresses are constructed with 16 random characters. However, tools like Scallion6 and Shallot7 allow Tor users to create customized .onion addresses based on the brute-force technique e.g. Shallot needs 2.5 years to build only 9 customized characters out of 16. Sun et at. (2002) employed Support Vector Machine (SVM) to classify the web content by taking the advantage of the context features e.g. HTML tags and hyperlinks in addition to the textual features to build the feature set.",
                "cite_spans": [
                    {
                        "start": 94,
                        "end": 117,
                        "text": "(Dumais and Chen, 2000;",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 118,
                        "end": 135,
                        "text": "Sun et al., 2002;",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 136,
                        "end": 146,
                        "text": "Kan, 2004;",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 147,
                        "end": 165,
                        "text": "Kan and Thi, 2005;",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 166,
                        "end": 177,
                        "text": "Kaur, 2014)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 197,
                        "end": 214,
                        "text": "(Su et al., 2006;",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 215,
                        "end": 231,
                        "text": "Xu et al., 2007;",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 232,
                        "end": 253,
                        "text": "Barbosa et al., 2007;",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 254,
                        "end": 271,
                        "text": "Lin et al., 2008;",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 272,
                        "end": 290,
                        "text": "Zhao et al., 2008;",
                        "ref_id": "BIBREF31"
                    },
                    {
                        "start": 291,
                        "end": 309,
                        "text": "Xian et al., 2009;",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 310,
                        "end": 326,
                        "text": "Khelghati, 2016)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 467,
                        "end": 497,
                        "text": "(Graczyk and Kinningham, 2015;",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 498,
                        "end": 518,
                        "text": "Moore and Rid, 2016)",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 521,
                        "end": 532,
                        "text": "Kaur (2014)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 849,
                        "end": 859,
                        "text": "Kan et al.",
                        "ref_id": null
                    },
                    {
                        "start": 995,
                        "end": 1006,
                        "text": "(Kan, 2004;",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 1007,
                        "end": 1025,
                        "text": "Kan and Thi, 2005)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 1353,
                        "end": 1370,
                        "text": "Sun et at. (2002)",
                        "ref_id": "BIBREF27"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "Regarding the Deep Web classification, Noor et al. (2011) discussed the common techniques that are used for the content extraction from the Deep Web data sources called \"Query Probing\", which is commonly used for supervised learning algorithms, and \"Visible Form Features\" (Xian et al., 2009) . Su et al. (2006) have proposed a combination between SVM with query probing to classify the structured Deep Web hierarchically. Barbosa et al. (2007) proposed an unsupervised machine learning clustering pipeline, in which Term Frequency Inverse Document Frequency (TF-IDF) was used for the text representation, and the cosine similarity for distance measurement for the kmeans.",
                "cite_spans": [
                    {
                        "start": 39,
                        "end": 57,
                        "text": "Noor et al. (2011)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 273,
                        "end": 292,
                        "text": "(Xian et al., 2009)",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 295,
                        "end": 311,
                        "text": "Su et al. (2006)",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 423,
                        "end": 444,
                        "text": "Barbosa et al. (2007)",
                        "ref_id": "BIBREF1"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "With respect to the Darknet, Moore et. al. in (2016) have presented a new study based on Tor hidden services to analyze and classify the Darknet. Initially, they collected 5K samples of Tor onion pages and classified them into 12 classes using SVM classifier. Graczyk et al. (2015) proposed a pipeline to classify the products of a famous black market on Darknet, called Agora, into 12 classes with 79% of accuracy. Their pipeline architecture uses the TF-IDF for text features extraction, the PCA for features selection, and SVM for features classification.",
                "cite_spans": [
                    {
                        "start": 29,
                        "end": 52,
                        "text": "Moore et. al. in (2016)",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 260,
                        "end": 281,
                        "text": "Graczyk et al. (2015)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "Several attempts in literature have been proposed to detect illegal activities whether on the World Wide Web (WWW) network (Biryukov et al., 2014; Graczyk and Kinningham, 2015; Moore and Rid, 2016) , peer-to-peer networks (P2P) (Latapy et al., 2013; Peersman et al., 2014) and in chatting messaging systems (Morris and Hirst, 2012) . Latapy el at. (2013) investigated P2P systems, e.g. eDonkey, to quantify the paedophile activity by building a tool to detect child-pornography queries by performing a series of lexical text processing. They found that 0.25% of entered queries are related to pedophilia context, which means that 0.2% of eDonkey network users are entering such queries. However, this method is based on a predefined list of keywords which can not detect new or previously unknown words.",
                "cite_spans": [
                    {
                        "start": 123,
                        "end": 146,
                        "text": "(Biryukov et al., 2014;",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 147,
                        "end": 176,
                        "text": "Graczyk and Kinningham, 2015;",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 177,
                        "end": 197,
                        "text": "Moore and Rid, 2016)",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 228,
                        "end": 249,
                        "text": "(Latapy et al., 2013;",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 250,
                        "end": 272,
                        "text": "Peersman et al., 2014)",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 307,
                        "end": 331,
                        "text": "(Morris and Hirst, 2012)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "3 The Dataset",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "To best of our knowledge, there is no labeled dataset that encompasses the activities on the Darknet web pages. Therefore, we have created the first publicly available Darknet dataset and we called it Darknet Usage Text Addresses (DUTA) dataset. Currently, DUTA contains only Tor hidden services (HS). We built a customized crawler that utilizes Tor socket to fetch onion web pages through port 80 only i.e. the HTTP protocol. The crawler has 70 worker threads in parallel to download the HTML code behind the HS. Each thread dives into the second level in depth for each HS in order to gather as much text as possible rather than just the index page as in others work (Biryukov et al., 2014) . It searches for the HS links on several famous Darknet resources like onion.city8 and ahmia.fi9 . We reached more than 250K HS addresses, but only 7K were alive, and the others were down or not responding. After that, we concatenated the HTML pages of every HS into a single HTML file resulting a single HTML file for each single HS domain. We collected 7,931 hidden services by running the crawler for two months between May and July 2016. For the time being, we labeled 6,831 samples.",
                "cite_spans": [
                    {
                        "start": 669,
                        "end": 692,
                        "text": "(Biryukov et al., 2014)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dataset Building Procedure",
                "sec_num": "3.1"
            },
            {
                "text": "Darknet researchers have analyzed the HS contents and categorized them into a different number of categories. Biryukov et al. (2014) Based on our objective to build a multipurpose dataset and for the sake of completeness, we classified DUTA manually into 26 classes. To the best of our knowledge, this classification is the most extent and complete up to date. The collected samples were divided among the four authors and each one labeled their designated part; if an author hesitated, it was openly discussed with the rest of the authors. Finally, to check the consistency of the manual labeling, the first author reviewed the final labeling by analyzing random samples of the categorization made by the others.",
                "cite_spans": [
                    {
                        "start": 110,
                        "end": 132,
                        "text": "Biryukov et al. (2014)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dataset Characteristics",
                "sec_num": "3.2"
            },
            {
                "text": "In addition to labeling the main classes, we dived into labeling the sub-classes of the HS. For example, the class Counterfeit Personal Identification has three sub-classes: Identity Card, Driving License, and Passport. Counterfeit is a wide class so we split it into three main classes 1) Counterfeit Personal Identification which is related to government documents forging. 2) Counterfeit Money includes currencies forging and 3) Counterfeit Credit Cards covers cloning credit cards, hacked PayPal accounts and fake markets cards like Amazon and eBay. The class Services contains the legal services that are provided by individuals or organizations. The class Down contains the errors that were returned by the down web pages while crawling them e.g. an SQL error in a website database or a javascript error.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dataset Characteristics",
                "sec_num": "3.2"
            },
            {
                "text": "We assign class Empty to a web page when:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dataset Characteristics",
                "sec_num": "3.2"
            },
            {
                "text": "1) The text is very short i.e. less than 5 words,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dataset Characteristics",
                "sec_num": "3.2"
            },
            {
                "text": "2) It has only images with no text, 3) It contains unreadable text like special characters, numbers, or unreadable words, 4) The empty Cryptolockers pages (ransomware) (Ciancaglini et al., 2016) . The class Locked contains the HS that require solving a CAPTCHA or a log-in credential.",
                "cite_spans": [
                    {
                        "start": 168,
                        "end": 194,
                        "text": "(Ciancaglini et al., 2016)",
                        "ref_id": "BIBREF5"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dataset Characteristics",
                "sec_num": "3.2"
            },
            {
                "text": "We noticed that some people love to present their works, projects, or even their personal information through an HS page so we labeled them into class Personal. The pages that fell under more than one category were labeled based on its main content.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dataset Characteristics",
                "sec_num": "3.2"
            },
            {
                "text": "For example, we assign Forum label to the multitopic forums unless the whole forum is related to a single topic. e.g. a hacking forum was assigned to Hacking class instead of Forum. The class Marketplace was divided into Black when it contained a group of illegal services like Drugs, Weapons, and Counterfeit services and White when the marketplace offered legal shops like mobile phones or clothes.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dataset Characteristics",
                "sec_num": "3.2"
            },
            {
                "text": "As we have labeled DUTA manually, we realized that some forums on HS contain numerous web pages and all of them are related to a single class i.e. we found a forum about childpornography that has more than 800 pages of textual content, so we split it up into single samples representing one single forum page, and we added them to the dataset.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dataset Characteristics",
                "sec_num": "3.2"
            },
            {
                "text": "Each classification pipeline is comprised of three main stages. First, text pre-processing, then, features extraction, and finally, classification. We used two famous text representation techniques across three different supervised classifiers resulting six different classification pipelines, and we examined every pipeline to figure out the best combination with the best parameters that can achieve high performance.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Methodology",
                "sec_num": "4"
            },
            {
                "text": "Initially, we eliminated all the HTML tags, and when we detected an image tag, we preserved the image name and removed the extension. Furthermore, we filtered the training set for the non-English samples using Langdetect 11 python library and stemmed the text using Porter library from NLTK package 12 . Additionally, we re-moved special characters and stop words thanks to SMART stop list13 (Salton, 1971) . At this stage, we modified the stop words list by adding 100 words more in order to make it compatible with the work domain. Moreover, we mapped all emails, URLs, and currencies into a single common token for each.",
                "cite_spans": [
                    {
                        "start": 392,
                        "end": 406,
                        "text": "(Salton, 1971)",
                        "ref_id": "BIBREF25"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Text Pre-processing",
                "sec_num": "4.1"
            },
            {
                "text": "After pre-processing the text, we used two famous text representation techniques. A) Bag-of-Words (BOW) is a well-known model for text representation that extracts the features from the text corpus by counting the words frequency. Consequently, every document is represented as a sparse feature vector where every feature corresponds to a single word in the training corpus. B) Term Frequency Inverse Document Frequency model (TD-IDF) (Aizawa, 2003) is a statistical model that assign weights for the vocabularies where it emphasizes the words that occur frequently in a given document, while at the same time de-emphasizes words that occur frequently in many documents. However, even though the BOW and TF-IDF do not take into considerations the words order, they are simple, computationally efficient and compatible with medium dataset sizes.",
                "cite_spans": [
                    {
                        "start": 435,
                        "end": 449,
                        "text": "(Aizawa, 2003)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Features Extraction",
                "sec_num": "4.2"
            },
            {
                "text": "For each features representation method, we examined three different supervised machine learning algorithms which are Support Vector Machine (SVM) (Suykens and Vandewalle, 1999) , Logistic Regression (LR) (Hosmer Jr and Lemeshow, 2004) , and Naive Bayes (NB) (McCallum et al., 1998) .",
                "cite_spans": [
                    {
                        "start": 147,
                        "end": 177,
                        "text": "(Suykens and Vandewalle, 1999)",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 205,
                        "end": 235,
                        "text": "(Hosmer Jr and Lemeshow, 2004)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 259,
                        "end": 282,
                        "text": "(McCallum et al., 1998)",
                        "ref_id": "BIBREF17"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Classifier Selection",
                "sec_num": "4.3"
            },
            {
                "text": "Due to the purpose of this paper to classify the Darknet illegal activities, we selected a subset of our DUTA dataset by creating eight categories trying to cover the most representative illegal activities on the Darknet. Another condition that we imposed was that each class in the selected subset should be monotopic (i.e. related to a single category) and contain a sufficient amount of samples (i.e. 40 samples minimum). The rest of the classes are assigned to a 9th category which we called Others. Since we are working on classifying the illegal activities, we did not consider the class Black-Market in the training set because its contents are related to more than one class at a single time, and we wanted the classifier to learn from pure patterns. Moreover, when a sample contains relevant images but an irrelevant text or without any textual information, we excluded it from the dataset. Therefore, we had 5,635 samples distributed over nine classes i.e. the eight classes plus the Others one ( Table 2 ). After the text pre-processing, we got 5,002 sample split it into a training set that contains 3,501 samples and a testing set of 1,501 samples. The dataset is highly unbalanced since the largest class has 3,513 samples while the smallest one has only 40 samples. We solved the skew in the dataset thanks to the class-weight parameter in Scikit-Learn library14 which assigns a weight for each class proportional to the number of samples it has (Hauck, 2014) . In addition to adjusting the weights of classes, we split up forums by the discussion page (See Section 3.2).",
                "cite_spans": [
                    {
                        "start": 1461,
                        "end": 1474,
                        "text": "(Hauck, 2014)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 1013,
                        "end": 1014,
                        "text": "2",
                        "ref_id": "TABREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Experimental Setting",
                "sec_num": "5.1"
            },
            {
                "text": "For the models tuning, we applied a grid search over different combinations of parameters with a cross-validation of 10 folds. The successful combination, which corresponds to the selected classification pipeline, is the one that can achieve the highest value of an averaged F1 score metric and an accuracy of 10 folds cross-validation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Setting",
                "sec_num": "5.1"
            },
            {
                "text": "We used Python3 with Scikit-Learn machine learning library for the pipelines implementation. We modified the parameters that have a critical influence on the performance of the models. For the BOW dictionary, we set it to 30,000 words with a minimum word frequency of 3, and we left the rest of the parameters to default. Regarding the TF-IDF, we set the maximum feature vectors length to 10,000 and the minimum to 3. With respect to the classifiers parameters, we kept the default setting for the NB. In contrast, for the LR, we modified only the value of the regularization parameter \"C\" by setting it to 10 with the balanced class-weight flag activated. For the SVM classifier, we set the decision function parameter to one-vs-rest \"ovr\", kernel to \"RBF\", \"C\" parameter to 10e5, balanced classes weights, and the rest were left to default.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Setting",
                "sec_num": "5.1"
            },
            {
                "text": "Since we are working on an unbalanced multiclass problem, every class has a precision, a recall, and an F1 score. To combine these three values into a single value, we calculated the macro, micro and weighted average for each class as Table 3 shows. We can see that the pipeline of TF-IDF with LR achieves the highest value with a macro F1 score of 93.7% and the highest cross-validation accuracy of 96.6%. The state-of-the-art paper has achieved 94% accuracy on a different dataset that contains 1K samples (Intelliagg, 2015) . Additionally, we plot the macro average precision-recall curve for four classifiers (Figure 2 ). The plot indicates that the pipeline of TF-IDF with LR achieves the highest precision-recall. Credit Cards and Hacking have a low F1 score over all the pipelines, which is due to several reasons: firstly, the words interference between the classes. For example, the websites which offer counterfeiting credit cards services are most probably \"Hack\" the credit card system or \"Attack\" the PayPal accounts, the use sentences like \"We hack credit card\" or \"Hacked Paypal account for sale\". Moreover, those classes intersect with Counterfeit Personal Identification class due to their similarity from the perspective of forgery. Secondly, the number of samples that were used for training plays an important role during the learning phase, e.g. class Violence has 60 samples only. Nevertheless, the learning curve for the TF-IDF LR pipeline in Figure 4 proves that the algorithm is learning correctly where the validation accuracy curve is raising up and classification accuracy is improving by increasing the number of the samples while the training accuracy curve is starting to decrease slightly. This high accuracy archived will help to build a solid model that will be able to detect illegal activity on Darknet.",
                "cite_spans": [
                    {
                        "start": 508,
                        "end": 526,
                        "text": "(Intelliagg, 2015)",
                        "ref_id": "BIBREF10"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 241,
                        "end": 242,
                        "text": "3",
                        "ref_id": "TABREF4"
                    },
                    {
                        "start": 621,
                        "end": 622,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    },
                    {
                        "start": 1473,
                        "end": 1474,
                        "text": "4",
                        "ref_id": "FIGREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Results and Discussion",
                "sec_num": "5.2"
            },
            {
                "text": "The work presented in the previous sections has been included into an application that can be accessed and tested through a web browser. The implementation of the methods was developed in Python3 using Nltk library to stem the document text, Langdetect library to detect the language of The Docker image is not publicly available, neither the applications, but under email request, we will grant a temporal access to the web interface.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Application and Implementation",
                "sec_num": "6"
            },
            {
                "text": "In this paper, we have categorized illegal activities of Tor HS by using two text representation methods, TF-IDF and BOW, combined with three classifiers, SVM, LR, and NB. To support the classification pipelines, we built the dataset DUTA, We found that the combination of the TF-IDF text representation with the Logistic Regression classifier can achieve 96.6% accuracy over 10 folds of cross-validation and 93.7% macro F1 score. We noticed that our classifier suffers from overfitting due to the difficulty of reaching more samples of onion hidden services for some classes like counterfeiting personal identification or illegal drugs. However, our results are encouraging, and yet there is still a wide margin for future improvements. We are looking forward to enlarging the dataset by digging deeper into the Darknet by adding more HS sources, even from I2P and Freenet, and exploring ports other than the HTTP port. Moreover, we plan to get the benefit of the HTML tags and the hyperlinks by weighting some tags or parsing the hyperlinks text. Also, during the manual labeling of the dataset, we realized that a wide portion of the hidden services advertise their illegal products graphically, i.e. the service owner uses the images instead of the text. Therefore, our aim is to build an image classifier to work in parallel with the text classification. The high accuracy we have obtained in this work might represent an opportunity to insert our research into a tool that supports the authorities in monitoring the Darknet.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusions and Future Work",
                "sec_num": "7"
            },
            {
                "text": "This research was funded by the frame agreement between the University of Len and INCIBE (Spanish National Cybersecurity Institute) under addendum 22. We also want to thanks Francisco J. Rodrguez (INCIBE) for providing us the .onion web pages used to create the dataset.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "ACKNOWLEDGEMENT",
                "sec_num": "8"
            },
            {
                "text": "www.torproject.org",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "www.geti2p.net",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "www.freenetproject.org",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "www.torproject.org/projects/ torbrowser.html.en",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "www.github.com/lachesis/scallion",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "www.github.com/katmagic/Shallot",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "www.onion.city",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "www.ahmia.fi",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "This class includes 57 unique sample plus 857 samples that are extracted from a single forum (See Section 3.2)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://pypi.python.org/pypi/langdetect",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://tartarus.org/martin/PorterStemmer/",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "http://jmlr.csail.mit.edu/papers/ volume5/lewis04a/a11smart-stop-list/",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "http://scikit-learn.org/",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "An information-theoretic perspective of tf-idf measures",
                "authors": [
                    {
                        "first": "Akiko",
                        "middle": [],
                        "last": "Aizawa",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Information Processing & Management",
                "volume": "39",
                "issue": "1",
                "pages": "45--65",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Akiko Aizawa. 2003. An information-theoretic per- spective of tf-idf measures. Information Processing & Management, 39(1):45-65.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Organizing hidden-web databases by clustering visible web documents",
                "authors": [
                    {
                        "first": "Luciano",
                        "middle": [],
                        "last": "Barbosa",
                        "suffix": ""
                    },
                    {
                        "first": "Juliana",
                        "middle": [],
                        "last": "Freire",
                        "suffix": ""
                    },
                    {
                        "first": "Altigran",
                        "middle": [],
                        "last": "Silva",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "2007 IEEE 23rd International Conference on Data Engineering",
                "volume": "",
                "issue": "",
                "pages": "326--335",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Luciano Barbosa, Juliana Freire, and Altigran Silva. 2007. Organizing hidden-web databases by cluster- ing visible web documents. In 2007 IEEE 23rd In- ternational Conference on Data Engineering, pages 326-335. IEEE.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "White paper: the deep web: surfacing hidden value",
                "authors": [
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Michael",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Bergman",
                        "suffix": ""
                    }
                ],
                "year": 2001,
                "venue": "Journal of electronic publishing",
                "volume": "7",
                "issue": "1",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Michael K. Bergman. 2001. White paper: the deep web: surfacing hidden value. Journal of electronic publishing, 7(1).",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Content and popularity analysis of tor hidden services",
                "authors": [
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Biryukov",
                        "suffix": ""
                    },
                    {
                        "first": "Ivan",
                        "middle": [],
                        "last": "Pustogarov",
                        "suffix": ""
                    },
                    {
                        "first": "Fabrice",
                        "middle": [],
                        "last": "Thill",
                        "suffix": ""
                    },
                    {
                        "first": "Ralf-Philipp",
                        "middle": [],
                        "last": "Weinmann",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "2014 IEEE 34th International Conference on Distributed Computing Systems Workshops (ICDCSW)",
                "volume": "",
                "issue": "",
                "pages": "188--193",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alex Biryukov, Ivan Pustogarov, Fabrice Thill, and Ralf-Philipp Weinmann. 2014. Content and popu- larity analysis of tor hidden services. In 2014 IEEE 34th International Conference on Distributed Com- puting Systems Workshops (ICDCSW), pages 188- 193. IEEE.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "How to mine the invisible web: The ultimate guide",
                "authors": [
                    {
                        "first": "Wendy",
                        "middle": [],
                        "last": "Boswell",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Wendy Boswell. 2016. How to mine the invisible web: The ultimate guide.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Below the surface: Exploring the deep web",
                "authors": [
                    {
                        "first": "V",
                        "middle": [],
                        "last": "Ciancaglini",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Balduzzi",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Mcardle",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "R\u00f6sler",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Trend Micro Incorporated. As",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "V. Ciancaglini, M. Balduzzi, R. McArdle, and M. R\u00f6sler. 2016. Below the surface: Exploring the deep web. Trend Micro Incorporated. As of, 12.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Hierarchical classification of web content",
                "authors": [
                    {
                        "first": "Susan",
                        "middle": [],
                        "last": "Dumais",
                        "suffix": ""
                    },
                    {
                        "first": "Hao",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    }
                ],
                "year": 2000,
                "venue": "Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval",
                "volume": "",
                "issue": "",
                "pages": "256--263",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Susan Dumais and Hao Chen. 2000. Hierarchical classification of web content. In Proceedings of the 23rd annual international ACM SIGIR confer- ence on Research and development in information retrieval, pages 256-263. ACM.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Automatic product categorization for anonymous marketplaces",
                "authors": [
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Graczyk",
                        "suffix": ""
                    },
                    {
                        "first": "Kevin",
                        "middle": [],
                        "last": "Kinningham",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Michael Graczyk and Kevin Kinningham. 2015. Au- tomatic product categorization for anonymous mar- ketplaces.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "scikit-learn Cookbook",
                "authors": [
                    {
                        "first": "Trent",
                        "middle": [],
                        "last": "Hauck",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Trent Hauck. 2014. scikit-learn Cookbook. Packt Pub- lishing Ltd.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Applied logistic regression",
                "authors": [
                    {
                        "first": "David",
                        "middle": [
                            "W"
                        ],
                        "last": "Hosmer",
                        "suffix": ""
                    },
                    {
                        "first": "Stanley",
                        "middle": [],
                        "last": "Lemeshow",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "David W. Hosmer Jr. and Stanley Lemeshow. 2004. Applied logistic regression. John Wiley & Sons.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Deep light shining a light on the dark web",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Intelliagg",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Intelliagg. 2015. Deep light shining a light on the dark web. Magazine.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Fast webpage classification using url features",
                "authors": [
                    {
                        "first": "Min-Yen",
                        "middle": [],
                        "last": "Kan",
                        "suffix": ""
                    },
                    {
                        "first": "Hoang",
                        "middle": [],
                        "last": "Oanh",
                        "suffix": ""
                    },
                    {
                        "first": "Nguyen",
                        "middle": [],
                        "last": "Thi",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of the 14th ACM international conference on Information and knowledge management",
                "volume": "",
                "issue": "",
                "pages": "325--326",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Min-Yen Kan and Hoang Oanh Nguyen Thi. 2005. Fast webpage classification using url features. In Proceedings of the 14th ACM international confer- ence on Information and knowledge management, pages 325-326. ACM.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Web page classification without the web page",
                "authors": [
                    {
                        "first": "Min-Yen",
                        "middle": [],
                        "last": "Kan",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proceedings of the 13th international World Wide Web conference on Alternate track papers & posters",
                "volume": "",
                "issue": "",
                "pages": "262--263",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Min-Yen Kan. 2004. Web page classification with- out the web page. In Proceedings of the 13th inter- national World Wide Web conference on Alternate track papers & posters, pages 262-263. ACM.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Web content classification: A survey",
                "authors": [
                    {
                        "first": "Prabhjot",
                        "middle": [],
                        "last": "Kaur",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1405.0580"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Prabhjot Kaur. 2014. Web content classification: A survey. arXiv preprint arXiv:1405.0580.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Deep Web Content Monitoring",
                "authors": [
                    {
                        "first": "Khelghati",
                        "middle": [],
                        "last": "Mohammadreza",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "S Mohammadreza Khelghati. 2016. Deep Web Con- tent Monitoring. Ph.D. thesis.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Quantifying paedophile activity in a large p2p system",
                "authors": [
                    {
                        "first": "Matthieu",
                        "middle": [],
                        "last": "Latapy",
                        "suffix": ""
                    },
                    {
                        "first": "Clemence",
                        "middle": [],
                        "last": "Magnien",
                        "suffix": ""
                    },
                    {
                        "first": "Raphael",
                        "middle": [],
                        "last": "Fournier",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Information Processing & Management",
                "volume": "49",
                "issue": "",
                "pages": "248--263",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Matthieu Latapy, Clemence Magnien, and Raphael Fournier. 2013. Quantifying paedophile activity in a large p2p system. Information Processing & Man- agement, 49(1):248-263.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Research on automatic classification for deep web query interfaces",
                "authors": [
                    {
                        "first": "Peiguang",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Yibing",
                        "middle": [],
                        "last": "Du",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaohua",
                        "middle": [],
                        "last": "Tan",
                        "suffix": ""
                    },
                    {
                        "first": "Chao",
                        "middle": [],
                        "last": "Lv",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Information Processing (ISIP), 2008 International Symposiums on",
                "volume": "",
                "issue": "",
                "pages": "313--317",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Peiguang Lin, Yibing Du, Xiaohua Tan, and Chao Lv. 2008. Research on automatic classification for deep web query interfaces. In Information Processing (ISIP), 2008 International Symposiums on, pages 313-317. IEEE.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "A comparison of event models for naive bayes text classification",
                "authors": [
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Mccallum",
                        "suffix": ""
                    },
                    {
                        "first": "Kamal",
                        "middle": [],
                        "last": "Nigam",
                        "suffix": ""
                    }
                ],
                "year": 1998,
                "venue": "AAAI-98 workshop on learning for text categorization",
                "volume": "752",
                "issue": "",
                "pages": "41--48",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Andrew McCallum, Kamal Nigam, et al. 1998. A comparison of event models for naive bayes text classification. In AAAI-98 workshop on learning for text categorization, volume 752, pages 41-48. Cite- seer.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Cryptopolitik and the darknet",
                "authors": [
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Moore",
                        "suffix": ""
                    },
                    {
                        "first": "Thomas",
                        "middle": [],
                        "last": "Rid",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "58",
                "issue": "",
                "pages": "7--38",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Daniel Moore and Thomas Rid. 2016. Cryptopolitik and the darknet. Survival, 58(1):7-38.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Identifying sexual predators by svm classification with lexical and behavioral features",
                "authors": [
                    {
                        "first": "Colin",
                        "middle": [],
                        "last": "Morris",
                        "suffix": ""
                    },
                    {
                        "first": "Graeme",
                        "middle": [],
                        "last": "Hirst",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "CLEF",
                "volume": "12",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Colin Morris and Graeme Hirst. 2012. Identifying sexual predators by svm classification with lexical and behavioral features. In CLEF (Online Working Notes/Labs/Workshop), volume 12, page 29.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Bitcoin: A peer-to-peer electronic cash system",
                "authors": [
                    {
                        "first": "Satoshi",
                        "middle": [],
                        "last": "Nakamoto",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Satoshi Nakamoto. 2008. Bitcoin: A peer-to-peer electronic cash system.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "A survey of automatic deep web classification techniques",
                "authors": [
                    {
                        "first": "Umara",
                        "middle": [],
                        "last": "Noor",
                        "suffix": ""
                    },
                    {
                        "first": "Zahid",
                        "middle": [],
                        "last": "Rashid",
                        "suffix": ""
                    },
                    {
                        "first": "Azhar",
                        "middle": [],
                        "last": "Rauf",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "International Journal of Computer Applications",
                "volume": "19",
                "issue": "6",
                "pages": "43--50",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Umara Noor, Zahid Rashid, and Azhar Rauf. 2011. A survey of automatic deep web classification tech- niques. International Journal of Computer Applica- tions, 19(6):43-50.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "icop: Automatically identifying new child abuse media in p2p networks",
                "authors": [
                    {
                        "first": "Claudia",
                        "middle": [],
                        "last": "Peersman",
                        "suffix": ""
                    },
                    {
                        "first": "Christian",
                        "middle": [],
                        "last": "Schulze",
                        "suffix": ""
                    },
                    {
                        "first": "Awais",
                        "middle": [],
                        "last": "Rashid",
                        "suffix": ""
                    },
                    {
                        "first": "Margaret",
                        "middle": [],
                        "last": "Brennan",
                        "suffix": ""
                    },
                    {
                        "first": "Carl",
                        "middle": [],
                        "last": "Fischer",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "124--131",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Claudia Peersman, Christian Schulze, Awais Rashid, Margaret Brennan, and Carl Fischer. 2014. icop: Automatically identifying new child abuse media in p2p networks. In Security and Privacy Workshops (SPW), 2014 IEEE, pages 124-131. IEEE.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "How did dread pirate roberts acquire and protect his bitcoin wealth?",
                "authors": [
                    {
                        "first": "Dorit",
                        "middle": [],
                        "last": "Ron",
                        "suffix": ""
                    },
                    {
                        "first": "Adi",
                        "middle": [],
                        "last": "Shamir",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "International Conference on Financial Cryptography and Data Security",
                "volume": "",
                "issue": "",
                "pages": "3--15",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dorit Ron and Adi Shamir. 2014. How did dread pi- rate roberts acquire and protect his bitcoin wealth? In International Conference on Financial Cryptog- raphy and Data Security, pages 3-15. Springer.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "The deep web and the darknet: A look inside the internet's massive black box",
                "authors": [
                    {
                        "first": "James",
                        "middle": [],
                        "last": "Dakota S Rudesill",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Caverlee",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Sui",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dakota S Rudesill, James Caverlee, and Daniel Sui. 2015. The deep web and the darknet: A look inside the internet's massive black box. Woodrow Wilson International Center for Scholars, STIP, 3.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "The smart retrieval systemexperiments in automatic document processing",
                "authors": [
                    {
                        "first": "Gerard",
                        "middle": [],
                        "last": "Salton",
                        "suffix": ""
                    }
                ],
                "year": 1971,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Gerard Salton. 1971. The smart retrieval systemexper- iments in automatic document processing.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Automatic hierarchical classification of structured deep web databases",
                "authors": [
                    {
                        "first": "Weifeng",
                        "middle": [],
                        "last": "Su",
                        "suffix": ""
                    },
                    {
                        "first": "Jiying",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Frederick",
                        "middle": [],
                        "last": "Lochovsky",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "International Conference on Web Information Systems Engineering",
                "volume": "",
                "issue": "",
                "pages": "210--221",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Weifeng Su, Jiying Wang, and Frederick Lochovsky. 2006. Automatic hierarchical classification of struc- tured deep web databases. In International Con- ference on Web Information Systems Engineering, pages 210-221. Springer.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Web classification using support vector machine",
                "authors": [
                    {
                        "first": "Aixin",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Ee-Peng",
                        "middle": [],
                        "last": "Lim",
                        "suffix": ""
                    },
                    {
                        "first": "Wee-Keong",
                        "middle": [],
                        "last": "Ng",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Proceedings of the 4th international workshop on Web information and data management",
                "volume": "",
                "issue": "",
                "pages": "96--99",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Aixin Sun, Ee-Peng Lim, and Wee-Keong Ng. 2002. Web classification using support vector machine. In Proceedings of the 4th international workshop on Web information and data management, pages 96- 99. ACM.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Least squares support vector machine classifiers",
                "authors": [
                    {
                        "first": "Johan",
                        "middle": [
                            "A K"
                        ],
                        "last": "Suykens",
                        "suffix": ""
                    },
                    {
                        "first": "Joos",
                        "middle": [],
                        "last": "Vandewalle",
                        "suffix": ""
                    }
                ],
                "year": 1999,
                "venue": "Neural processing letters",
                "volume": "9",
                "issue": "3",
                "pages": "293--300",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Johan A.K. Suykens and Joos Vandewalle. 1999. Least squares support vector machine classifiers. Neural processing letters, 9(3):293-300.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Automatic classification of deep web databases with simple query interface",
                "authors": [
                    {
                        "first": "Xuefeng",
                        "middle": [],
                        "last": "Xian",
                        "suffix": ""
                    },
                    {
                        "first": "Pengpeng",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Fang",
                        "suffix": ""
                    },
                    {
                        "first": "Jie",
                        "middle": [],
                        "last": "Xin",
                        "suffix": ""
                    },
                    {
                        "first": "Zhiming",
                        "middle": [],
                        "last": "Cui",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "ICIMA 2009. International Conference on",
                "volume": "",
                "issue": "",
                "pages": "85--88",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xuefeng Xian, Pengpeng Zhao, Wei Fang, Jie Xin, and Zhiming Cui. 2009. Automatic classification of deep web databases with simple query interface. In Industrial Mechatronics and Automation, 2009. ICIMA 2009. International Conference on, pages 85-88. IEEE.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "A method of deep web classification",
                "authors": [
                    {
                        "first": "He-Xiang",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Xiu-Lan",
                        "middle": [],
                        "last": "Hao",
                        "suffix": ""
                    },
                    {
                        "first": "Shu-Yun",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Yun-Fa",
                        "middle": [],
                        "last": "Hu",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "2007 International Conference on Machine Learning and Cybernetics",
                "volume": "7",
                "issue": "",
                "pages": "4009--4014",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "He-Xiang Xu, Xiu-Lan Hao, Shu-Yun Wang, and Yun- Fa Hu. 2007. A method of deep web classifica- tion. In 2007 International Conference on Machine Learning and Cybernetics, volume 7, pages 4009- 4014. IEEE.",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "Organizing structured deep web by clustering query interfaces link graph",
                "authors": [
                    {
                        "first": "Pengpeng",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Li",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Fang",
                        "suffix": ""
                    },
                    {
                        "first": "Zhiming",
                        "middle": [],
                        "last": "Cui",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "International Conference on Advanced Data Mining and Applications",
                "volume": "",
                "issue": "",
                "pages": "683--690",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Pengpeng Zhao, Li Huang, Wei Fang, and Zhiming Cui. 2008. Organizing structured deep web by clus- tering query interfaces link graph. In International Conference on Advanced Data Mining and Applica- tions, pages 683-690. Springer.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 1: The number of unique *.onion addresses in Tor network between August 2015 to August 2016",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Figure 2: Macro averaging Precision-Recall curve over 4 pipelines, where the area value corresponds to the macro-average Precision-recall curve",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF3": {
                "num": null,
                "text": "Figure 3: F1 score comparison for each class for 6 classification pipelines. When a bar is not shown, it means that its value is zero.",
                "uris": null,
                "fig_num": "3",
                "type_str": "figure"
            },
            "FIGREF4": {
                "num": null,
                "text": "Figure 4: Learning Curve for TF-IDF with LR classifier",
                "uris": null,
                "fig_num": "4",
                "type_str": "figure"
            },
            "FIGREF5": {
                "num": null,
                "text": "Figure 5: The application has three interfaces. (a) Pipeline selection. (b)The HS content preview. (c) The classification result.",
                "uris": null,
                "fig_num": "5",
                "type_str": "figure"
            },
            "TABREF1": {
                "content": "<table><tr><td>Main Class</td><td>Sub-Class</td><td>Count</td><td>Main Class</td><td>Count</td></tr><tr><td/><td>Hate</td><td>4</td><td>Art/ Music</td><td>8</td></tr><tr><td>Violence</td><td>Hitman</td><td>11</td><td>Casino/ Gambling</td><td>26</td></tr><tr><td/><td>Weapons</td><td>47</td><td>Services</td><td>285</td></tr><tr><td>Counterfeit Personal Identification</td><td>Driving-Licence ID Passport</td><td>4 7 37</td><td>Cryptocurrency Down Empty</td><td>586 608 1649</td></tr><tr><td/><td>File-Sharing</td><td>111</td><td>Forum</td><td>104</td></tr><tr><td>Hosting and Software</td><td>Folders Search-Engine Server</td><td>63 38 95</td><td>Hacking Wiki Leaked-Data</td><td>90 29 12</td></tr><tr><td/><td>Software</td><td>121</td><td>Locked</td><td>435</td></tr><tr><td/><td>Directory</td><td>142</td><td>Personal</td><td>405</td></tr><tr><td>Drugs</td><td>Illegal Legal</td><td>230 9</td><td>Politics Religion</td><td>8 6</td></tr><tr><td>Marketplace</td><td>Black White</td><td>63 67</td><td>Library/Books Fraud</td><td>27 4</td></tr><tr><td>Pornography</td><td>Child-pornography</td><td>914( 10 )</td><td>Counterfeit Money</td><td>55</td></tr><tr><td/><td>General-pornography</td><td>83</td><td>Counterfeit Credit Cards</td><td>240</td></tr><tr><td>Social-Network</td><td>Blog Chat Email</td><td>71 47 56</td><td>Human-Trafficking</td><td>2</td></tr><tr><td/><td>News</td><td>32</td><td>The total count</td><td>6831</td></tr></table>",
                "type_str": "table",
                "text": "Table 1 enumerates DUTA classes. DUTA dataset classes",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table><tr><td>Experiment Main Class</td><td>Count</td></tr><tr><td>Pornography</td><td>963</td></tr><tr><td>Cryptocurrency</td><td>578</td></tr><tr><td>Counterfeit Credit Cards</td><td>209</td></tr><tr><td>Drugs</td><td>169</td></tr><tr><td>Violence</td><td>60</td></tr><tr><td>Hacking</td><td>57</td></tr><tr><td>Counterfeit Money</td><td>46</td></tr><tr><td>Counterfeit Personal Identification (Driving-License, ID, Passport)</td><td>40</td></tr><tr><td>Others</td><td>3513</td></tr></table>",
                "type_str": "table",
                "text": "Illegal activities dataset classes (A portion of DUTA dataset)",
                "html": null,
                "num": null
            },
            "TABREF4": {
                "content": "<table><tr><td>: A comparison between the classification</td></tr><tr><td>pipelines with respect to 10 folds cross-validation</td></tr><tr><td>accuracy (CV), precision (P), recall (R) and F1</td></tr><tr><td>score metrics for micro, macro and weighted av-</td></tr><tr><td>eraging.</td></tr></table>",
                "type_str": "table",
                "text": "",
                "html": null,
                "num": null
            }
        }
    }
}