{
    "paper_id": "2020",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:15:29.948615Z"
    },
    "title": "Point to the Expression: Solving Algebraic Word Problems using the Expression-Pointer Transformer Model",
    "authors": [
        {
            "first": "Bugeun",
            "middle": [],
            "last": "Kim",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Seoul National University",
                "location": {
                    "settlement": "Seoul",
                    "country": "Republic of Korea"
                }
            },
            "email": ""
        },
        {
            "first": "Kyung",
            "middle": [],
            "last": "Seo",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Seoul National University",
                "location": {
                    "settlement": "Seoul",
                    "country": "Republic of Korea"
                }
            },
            "email": ""
        },
        {
            "first": "Ki",
            "middle": [
                "Donggeon"
            ],
            "last": "Lee",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Seoul National University",
                "location": {
                    "settlement": "Seoul",
                    "country": "Republic of Korea"
                }
            },
            "email": ""
        },
        {
            "first": "Gahgene",
            "middle": [],
            "last": "Gweon",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Seoul National University",
                "location": {
                    "settlement": "Seoul",
                    "country": "Republic of Korea"
                }
            },
            "email": "ggweon@snu.ac.kr"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Solving algebraic word problems has recently emerged as an important natural language processing task. To solve algebraic word problems, recent studies suggested neural models that generate solution equations by using 'Op (operator/operand)' tokens as a unit of input/output. However, such a neural model suffered two issues: expression fragmentation and operand-context separation. To address each of these two issues, we propose a pure neural model, Expression-Pointer Transformer (EPT), which uses (1) 'Expression' token and (2) operand-context pointers when generating solution equations. The performance of the EPT model is tested on three datasets: ALG514, DRAW-1K, and MAWPS. Compared to the state-of-the-art (SoTA) models, the EPT model achieved a comparable performance accuracy in each of the three datasets; 81.3% on ALG514, 59.5% on DRAW-1K, and 84.5% on MAWPS. The contribution of this paper is two-fold; (1) We propose a pure neural model, EPT, which can address the expression fragmentation and the operandcontext separation. (2) The fully automatic EPT model, which does not use hand-crafted features, yields comparable performance to existing models using hand-crafted features, and achieves better performance than existing pure neural models by at most 40%.",
    "pdf_parse": {
        "paper_id": "2020",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Solving algebraic word problems has recently emerged as an important natural language processing task. To solve algebraic word problems, recent studies suggested neural models that generate solution equations by using 'Op (operator/operand)' tokens as a unit of input/output. However, such a neural model suffered two issues: expression fragmentation and operand-context separation. To address each of these two issues, we propose a pure neural model, Expression-Pointer Transformer (EPT), which uses (1) 'Expression' token and (2) operand-context pointers when generating solution equations. The performance of the EPT model is tested on three datasets: ALG514, DRAW-1K, and MAWPS. Compared to the state-of-the-art (SoTA) models, the EPT model achieved a comparable performance accuracy in each of the three datasets; 81.3% on ALG514, 59.5% on DRAW-1K, and 84.5% on MAWPS. The contribution of this paper is two-fold; (1) We propose a pure neural model, EPT, which can address the expression fragmentation and the operandcontext separation. (2) The fully automatic EPT model, which does not use hand-crafted features, yields comparable performance to existing models using hand-crafted features, and achieves better performance than existing pure neural models by at most 40%.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Solving algebraic word problems has recently become an important research task in that automatically generating solution equations requires understanding natural language. Table 1 shows a sample algebraic word problem, along with corresponding solution equations that are used to generate answers for the problem. To solve such problems with deep learning technology, researchers recently suggested neural models that generate solution equations automatically (Huang Problem One number is eight more than twice another and their sum is 20.",
                "cite_spans": [
                    {
                        "start": 460,
                        "end": 474,
                        "text": "(Huang Problem",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 178,
                        "end": 179,
                        "text": "1",
                        "ref_id": "TABREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "What are their numbers? Numbers 1('one'), 8('eight'), 2('twice'), 20. Equations x 0 -2x 1 = 8, x 0 + x 1 = 20 Answers (16, 4) et al., 2018; Amini et al., 2019; Chiang and Chen, 2019; Wang et al., 2019) . However, suggested neural models showed a fairly large performance gap compared to existing state-of-the-art models based on hand-crafted features in popular algebraic word problem datasets, such as ALG514 (44.5% for pure neural model vs. 83.0% for using hand-crafted features) (Huang et al., 2018; Upadhyay and Chang, 2016) . To address the large performance gap in this study, we propose a larger unit of input/output (I/O) token called \"Expressions\" for a pure neural model. Figure 1 illustrates conventionally used \"Op (operator/operands)\" versus our newly proposed \"Expression\" token.",
                "cite_spans": [
                    {
                        "start": 126,
                        "end": 139,
                        "text": "et al., 2018;",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 140,
                        "end": 159,
                        "text": "Amini et al., 2019;",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 160,
                        "end": 182,
                        "text": "Chiang and Chen, 2019;",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 183,
                        "end": 201,
                        "text": "Wang et al., 2019)",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 482,
                        "end": 502,
                        "text": "(Huang et al., 2018;",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 503,
                        "end": 528,
                        "text": "Upadhyay and Chang, 2016)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 689,
                        "end": 690,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "To improve the performance of pure neural models that can solve algebraic word problems, we identified two issues that can be addressed using Expression tokens, which are shown in Figure 1: (1) expression fragmentation and (2) operandcontext separation. First, the expression fragmentation issue is a segmentation of an expression tree, which represents a computational structure of equations that are used to generate a solution. This issue arises when Op, rather than the whole expression tree, is used as an input/output unit of a problem-solving model. For example, as shown in Figure 1 (a), using Op tokens as an input to a problem-solving model disassembles a tree structure into operators (\"\u00d7\") and operands (\"x 1 \" and \"2\"). Meanwhile, we propose using the Figure 1 : Illustration using the word problem in Table 1 for the (a) expression fragmentation issue, (b) operandcontext separation issue, and (c) our solution for these two issues.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 187,
                        "end": 189,
                        "text": "1:",
                        "ref_id": null
                    },
                    {
                        "start": 589,
                        "end": 590,
                        "text": "1",
                        "ref_id": null
                    },
                    {
                        "start": 772,
                        "end": 773,
                        "text": "1",
                        "ref_id": null
                    },
                    {
                        "start": 821,
                        "end": 822,
                        "text": "1",
                        "ref_id": "TABREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\"Expression\" (\u00d7(x 1 , 2)) token, which can explicitly capture a tree structure as a whole, as shown in Figure 1 (c) .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 110,
                        "end": 115,
                        "text": "1 (c)",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The second issue of operand-context separation is the disconnection between an operand and a number that is associated with the operand. This issue arises when a problem-solving model substitutes a number stated in an algebraic word problem into an abstract symbol for generalization. As shown in Figure 1 (b), when using an Op token, the number 8 is changed into an abstract symbol 'N 1 '. Meanwhile, when using an Expression token, the number 8 is not transformed into a symbol. Rather a pointer is made to the location where the number 8 occurred in an algebraic word problem. Therefore, using such an \"operand-context pointer\" enables a model to access contextual information about the number directly, as shown in Figure 1 (c) ; thus, the operand-context separation issue can be addressed.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 304,
                        "end": 305,
                        "text": "1",
                        "ref_id": null
                    },
                    {
                        "start": 726,
                        "end": 731,
                        "text": "1 (c)",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this paper, we propose a pure neural model called Expression-Pointer Transformer (EPT) to address the two issues above. The contribution of this paper is two-fold;",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "1. We propose a pure neural model, Expression-Pointer Transformer (EPT), which can address the expression fragmentation and operandcontext separation issues.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "2. The EPT model is the first pure neural model that showed comparable accuracy to the existing state-of-the-art models, which used handcrafted features. Compared to the state-ofthe-art pure neural models, the EPT achieves better performance by about 40%.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In the rest of the paper, we introduce existing approaches to solve algebraic word problems in Section 2. Next, Section 3 introduces our proposed model, EPT, and Section 4 reports the experimental settings. Then in Section 5, results of two studies are presented. Section 5.1 presents a performance comparison between EPT and existing SoTA models. Section 5.2 presents an ablation study examining the effects of Expression tokens and applying operand-context pointers. Finally, in Section 6, a conclusion is presented with possible future directions for our work.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Our goal is to design a pure neural model that generates equations using 'Expression' tokens to solve algebraic word problems. Early attempts for solving algebraic word problems noted the importance of Expressions in building models with hand-crafted features (Kushman et al., 2014; Roy et al., 2015; Roy and Roth, 2015; Zhou et al., 2015; Upadhyay et al., 2016) . However, recent neural models have only utilized 'Op (operator/operand)' tokens (Wang et al., 2017; Amini et al., 2019; Chiang and Chen, 2019; Huang et al., 2018; Wang et al., 2019) , resulting in two issues: (1) the expression fragmentation issue and (2) the operandcontext separation issue. In the remaining section, we present existing methods for tackling each of these two issues.",
                "cite_spans": [
                    {
                        "start": 260,
                        "end": 282,
                        "text": "(Kushman et al., 2014;",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 283,
                        "end": 300,
                        "text": "Roy et al., 2015;",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 301,
                        "end": 320,
                        "text": "Roy and Roth, 2015;",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 321,
                        "end": 339,
                        "text": "Zhou et al., 2015;",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 340,
                        "end": 362,
                        "text": "Upadhyay et al., 2016)",
                        "ref_id": null
                    },
                    {
                        "start": 445,
                        "end": 464,
                        "text": "(Wang et al., 2017;",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 465,
                        "end": 484,
                        "text": "Amini et al., 2019;",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 485,
                        "end": 507,
                        "text": "Chiang and Chen, 2019;",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 508,
                        "end": 527,
                        "text": "Huang et al., 2018;",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 528,
                        "end": 546,
                        "text": "Wang et al., 2019)",
                        "ref_id": "BIBREF20"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related work",
                "sec_num": "2"
            },
            {
                "text": "To address the expression fragmentation issue, researchers tried to reflect relational information between operators and operands either by using a two-step procedure or a single step with sequenceto-sequence models. Earlier attempts predicted operators and their operands by using a two-step procedure. Such early models selected operators first by classifying a predefined template (Kushman et al., 2014; Zhou et al., 2015; Upadhyay et al., 2016) , then in the second step, operands were applied to the template selected in the first step. Other models selected operands first before constructing expression trees with operators in the second step (Roy et al., 2015; Roy and Roth, 2015) . However, such two-step procedures in these early attempts",
                "cite_spans": [
                    {
                        "start": 384,
                        "end": 406,
                        "text": "(Kushman et al., 2014;",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 407,
                        "end": 425,
                        "text": "Zhou et al., 2015;",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 426,
                        "end": 448,
                        "text": "Upadhyay et al., 2016)",
                        "ref_id": null
                    },
                    {
                        "start": 650,
                        "end": 668,
                        "text": "(Roy et al., 2015;",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 669,
                        "end": 688,
                        "text": "Roy and Roth, 2015)",
                        "ref_id": "BIBREF13"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related work",
                "sec_num": "2"
            },
            {
                "text": "Input Output Expression token Meaning position index Operator (f i ) Operand 0 (a i0 ) Operand 1 (a i1 ) 0 - BEGIN (Start an equation) 1 R 0 VAR (Generate variable x 0 ) 2 R 1 VAR (Generate variable x 1 ) 3 R 2 \u00d7 2 R 1 2x 1 4 R 3 - R 0 R 2 x 0 -2x 1 5 R 4 = R 3 8 x 0 -2x 1 = 8 6 R 5 + R 0 R 1 x 0 + x 1 7 R 6 = R 5 20 x 0 + x 1 = 20 - R 7 END (Gather all equations)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related work",
                "sec_num": "2"
            },
            {
                "text": "Table 2 : The Expression token sequence for x 0 -2x 1 = 8 and x 0 + x 1 = 20 can be performed via a single-step procedure with neural models. Specifically, recent attempts have utilized sequence-to-sequence (seq2seq) models as a single-step procedure to learn the implicit relationship between operators and operands (Amini et al., 2019; Chiang and Chen, 2019; Wang et al., 2019) . For example, to capture the operatoroperand relationship, Chiang and Chen ( 2019) constructed a seq2seq model that used push/pop actions on a stack for generating operator/operand tokens. Similarly, Amini et al. ( 2019) built a seq2seq model to generate an operator token right after producing required operand tokens. However, although these seq2seq approaches consider relational information of operands when generating operators, the approach still does not address the problem of lacking relation information of operators when generating operands. On the other hand, by using Expression token, our model can consider relational information when generating both operator and operands.",
                "cite_spans": [
                    {
                        "start": 317,
                        "end": 337,
                        "text": "(Amini et al., 2019;",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 338,
                        "end": 360,
                        "text": "Chiang and Chen, 2019;",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 361,
                        "end": 379,
                        "text": "Wang et al., 2019)",
                        "ref_id": "BIBREF20"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Related work",
                "sec_num": "2"
            },
            {
                "text": "Secondly, there were efforts to address the operand-context separation issue. To utilize contextual information of an operand token, researchers built hand-crafted features that capture the semantic content of a word, such as the unit of a given number (Roy and Roth, 2015; Koncel-Kedziorski et al., 2015; Zhou et al., 2015; Upadhyay et al., 2016; Roy and Roth, 2017) or dependency relationship between numbers (Kushman et al., 2014; Zhou et al., 2015; Upadhyay et al., 2016) . However, devising hand-crafted input features was timeconsuming and required domain expertise. Therefore, recent approaches have employed distributed representations and neural models to learn numeric context of operands automatically (Wang et al., 2017; Huang et al., 2018; Chiang and Chen, 2019; Amini et al., 2019) . For example, Huang et al. (2018) used a pointer-generator network that can point to the context of a number in a given math problem. Although Huang's model can address the operand-context separation issue using pointers, their pure neural model did not yield a comparable performance to the state-of-the-art model using hand-crafted features (44.5% vs. 83.0%). In this paper, we propose that by including additional pointers that utilize the contextual information of operands and neighboring Expression tokens, performance of pure neural models can improve.",
                "cite_spans": [
                    {
                        "start": 253,
                        "end": 273,
                        "text": "(Roy and Roth, 2015;",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 274,
                        "end": 305,
                        "text": "Koncel-Kedziorski et al., 2015;",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 306,
                        "end": 324,
                        "text": "Zhou et al., 2015;",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 325,
                        "end": 347,
                        "text": "Upadhyay et al., 2016;",
                        "ref_id": null
                    },
                    {
                        "start": 348,
                        "end": 367,
                        "text": "Roy and Roth, 2017)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 411,
                        "end": 433,
                        "text": "(Kushman et al., 2014;",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 434,
                        "end": 452,
                        "text": "Zhou et al., 2015;",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 453,
                        "end": 475,
                        "text": "Upadhyay et al., 2016)",
                        "ref_id": null
                    },
                    {
                        "start": 713,
                        "end": 732,
                        "text": "(Wang et al., 2017;",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 733,
                        "end": 752,
                        "text": "Huang et al., 2018;",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 753,
                        "end": 775,
                        "text": "Chiang and Chen, 2019;",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 776,
                        "end": 795,
                        "text": "Amini et al., 2019)",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 811,
                        "end": 830,
                        "text": "Huang et al. (2018)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related work",
                "sec_num": "2"
            },
            {
                "text": "Figure 2 shows the proposed Expression-Pointer Transformer (EPT)1 model, which adopts the encoder-decoder architecture of a Transformer model (Vaswani et al., 2017) . The EPT utilizes the ALBERT model (Lan et al., 2019) , a pretrained language model, as the encoder. The encoder input is tokenized words of the given word problem, and encoder output is the encoder's hidden-state vectors that denote numeric contexts of the given problem.",
                "cite_spans": [
                    {
                        "start": 142,
                        "end": 164,
                        "text": "(Vaswani et al., 2017)",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 201,
                        "end": 219,
                        "text": "(Lan et al., 2019)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "2",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "EPT: Expression-Pointer Transformer",
                "sec_num": "3"
            },
            {
                "text": "After obtaining the encoder's hidden-state vectors from the ALBERT encoder, the transformer decoder generates 'Expression' tokens. The two decoder inputs are Expression tokens and the ALBERT encoder's hidden-state vectors, which are used as memories. For the given example problem, the input is a list of 8 Expression tokens shown in Table 2 . We included three special commands in the list: VAR (generate a variable), BEGIN (start an equation), and END (gather all equations). Following the order specified in the list of Table 2 , the EPT receives one input Expression at a time. For the ith Expression input, the model computes an input vector v i . The EPT's decoder then transforms this input vector to a decoder's hidden-state vector d i . Finally, the EPT predicts the next Expression token by generating the next operator and operands simultaneously.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 340,
                        "end": 341,
                        "text": "2",
                        "ref_id": null
                    },
                    {
                        "start": 529,
                        "end": 530,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "EPT: Expression-Pointer Transformer",
                "sec_num": "3"
            },
            {
                "text": "To produce 'Expression' tokens, two components are modified from the vanilla Transformer: input vector and output layer. In the following subsections, we explain the two components.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "EPT: Expression-Pointer Transformer",
                "sec_num": "3"
            },
            {
                "text": "The input vector v i of ith Expression token is obtained by combining operator embedding f i and operand embedding a ij as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Input vector of EPT's decoder",
                "sec_num": "3.1"
            },
            {
                "text": "v i = FF in (Concat (f i , a i1 , a i2 , \u2022 \u2022 \u2022 , a ip )) , (1)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Input vector of EPT's decoder",
                "sec_num": "3.1"
            },
            {
                "text": "where FF * indicates a feed-forward linear layer, and Concat(\u2022) means concatenation of all vectors inside the parentheses. All the vectors, including v i , f i , and a ij , have the same dimension D. Formulae for computing the two types of embedding vectors, f i and a ij are stated in the next paragraph.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Input vector of EPT's decoder",
                "sec_num": "3.1"
            },
            {
                "text": "For the operator token f i of ith Expression, the EPT computes the operator embedding vector f i as in Vaswani et al. (2017) 's setting:",
                "cite_spans": [
                    {
                        "start": 103,
                        "end": 124,
                        "text": "Vaswani et al. (2017)",
                        "ref_id": "BIBREF18"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Input vector of EPT's decoder",
                "sec_num": "3.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "f i = LN f (c f E f (f i ) + PE(i)) ,",
                        "eq_num": "(2)"
                    }
                ],
                "section": "Input vector of EPT's decoder",
                "sec_num": "3.1"
            },
            {
                "text": "where E * (\u2022) indicates a look-up table for embedding vectors, c * denotes a scalar parameter, and LN * (\u2022) and PE(\u2022) represent layer normalization (Ba et al., 2016) and positional encoding (Vaswani et al., 2017) , respectively.",
                "cite_spans": [
                    {
                        "start": 148,
                        "end": 165,
                        "text": "(Ba et al., 2016)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 190,
                        "end": 212,
                        "text": "(Vaswani et al., 2017)",
                        "ref_id": "BIBREF18"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Input vector of EPT's decoder",
                "sec_num": "3.1"
            },
            {
                "text": "The embedding vector a ij , which represents the jth operand of ith Expression, is calculated differently according to the operand a ij 's source. To reflect contextual information of operands, three possible sources are utilized: problem-dependent numbers, problem-independent constants, and the result of prior Expression tokens. First, problemdependent numbers are numbers provided in an algebraic problem (e.g., '20' in Table 1 ). To compute a ij of a number, we reuse the encoder's hidden-state vectors corresponding to such number tokens as follows:",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 430,
                        "end": 431,
                        "text": "1",
                        "ref_id": "TABREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Input vector of EPT's decoder",
                "sec_num": "3.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "a ij = LN a c a u num + e a ij ,",
                        "eq_num": "(3)"
                    }
                ],
                "section": "Input vector of EPT's decoder",
                "sec_num": "3.1"
            },
            {
                "text": "where u * denotes a vector representing the source, and e a ij is the encoder's hidden-state vector corresponding to the number a ij .2 Second, problemindependent constants are predefined numbers that are not stated in the problem (e.g., 100 is often used for percentiles). To compute a ij of a constant, we use a look-up table E c as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Input vector of EPT's decoder",
                "sec_num": "3.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "a ij = LN a (c a u const + E c (a ij )) .",
                        "eq_num": "(4)"
                    }
                ],
                "section": "Input vector of EPT's decoder",
                "sec_num": "3.1"
            },
            {
                "text": "Note that LN a , c a are shared across different sources. Third, the result of the prior Expression token is an Expression generated before the ith Expression (e.g., R 0 ). To compute a ij of a result, we utilize the positional encoding as follows3 :",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Input vector of EPT's decoder",
                "sec_num": "3.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "a ij = LN a (c a u expr + PE(k)) , (",
                        "eq_num": "5"
                    }
                ],
                "section": "Input vector of EPT's decoder",
                "sec_num": "3.1"
            },
            {
                "text": ")",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Input vector of EPT's decoder",
                "sec_num": "3.1"
            },
            {
                "text": "where k is the index where the prior Expression a ij generated.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Input vector of EPT's decoder",
                "sec_num": "3.1"
            },
            {
                "text": "The output layer of the EPT's decoder predicts the next operator f i+1 and operands a i+1,j simultaneously when the ith Expression token is provided. First, the next operator, f i+1 , is predicted as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Output layer of EPT's decoder",
                "sec_num": "3.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "f i+1 = arg max f \u03c3(f |FF out (d i )),",
                        "eq_num": "(6)"
                    }
                ],
                "section": "Output layer of EPT's decoder",
                "sec_num": "3.2"
            },
            {
                "text": "where \u03c3(k|x) is the probability of selecting an item k under a distribution following the output of softmax function, \u03c3(x).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Output layer of EPT's decoder",
                "sec_num": "3.2"
            },
            {
                "text": "Second, to utilize the context of operands when predicting an operand, the output layer applies 'operand-context pointers,' inspired by the pointer networks (Vinyals et al., 2015) . In the pointer networks, the output layer predicts the next token using attention over candidate vectors. The EPT collects candidate vectors for the next (i + 1)th Expression in three different ways depending on the source of operands:",
                "cite_spans": [
                    {
                        "start": 157,
                        "end": 179,
                        "text": "(Vinyals et al., 2015)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Output layer of EPT's decoder",
                "sec_num": "3.2"
            },
            {
                "text": "e k",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Output layer of EPT's decoder",
                "sec_num": "3.2"
            },
            {
                "text": "for the kth number in the problem, d k for the kth Expression output, E c (x) for a constant x (7) Then the EPT predicts the next jth operand a i+1,j , as follows. Let A ij be a matrix whose row vectors are such candidates. Then, the EPT predicts a i+1,j by computing attention of a query vector Q ij on a key matrix K ij , as follows.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Output layer of EPT's decoder",
                "sec_num": "3.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "Q ij = FF query,j (d i ), (8) K ij = FF key,j (A ij ),",
                        "eq_num": "(9)"
                    }
                ],
                "section": "Output layer of EPT's decoder",
                "sec_num": "3.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "a i+1,j = arg max a \u03c3 a Q ij K ij \u221a D .",
                        "eq_num": "(10)"
                    }
                ],
                "section": "Output layer of EPT's decoder",
                "sec_num": "3.2"
            },
            {
                "text": "As the output layer is modified to predict an operator and its operands simultaneously, we also modified the loss function. We compute the loss of an Expression by summing up the loss of an operator and the loss of required arguments. All loss functions are computed using cross-entropy with the label smoothing approach (Szegedy et al., 2016) .",
                "cite_spans": [
                    {
                        "start": 321,
                        "end": 343,
                        "text": "(Szegedy et al., 2016)",
                        "ref_id": "BIBREF14"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Output layer of EPT's decoder",
                "sec_num": "3.2"
            },
            {
                "text": "The metric for measuring the EPT model's performance is answer accuracy, which is the proportion of correctly answered problems over the entire set of problems. We regard a problem is correctly answered if a solution to the generated equations matches the correct answer without considering the order of answer-tuple, as in Kushman et al. (2014) .",
                "cite_spans": [
                    {
                        "start": 324,
                        "end": 345,
                        "text": "Kushman et al. (2014)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Metric and Datasets",
                "sec_num": "4.1"
            },
            {
                "text": "To obtain a solution to the generated equations, we use SymPy (Meurer et al., 2017) at the end of the training phase.",
                "cite_spans": [
                    {
                        "start": 62,
                        "end": 83,
                        "text": "(Meurer et al., 2017)",
                        "ref_id": "BIBREF10"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Metric and Datasets",
                "sec_num": "4.1"
            },
            {
                "text": "For the datasets, we use three publicly available English algebraic word problem datasets4 : ALG514 (Kushman et al., 2014) 5 , DRAW-1K (Upadhyay and Chang, 2016)6 , and MAWPS (Koncel-Kedziorski et al., 2016)7 . The three datasets differ in terms of size and complexity, as shown in Table 3 . The high-complexity datasets, ALG514 and DRAW-1K, require more expressions and unknowns when solving the algebraic problems than the low-complexity dataset, MAWPS. For DRAW-1K, we report the accuracy of a model on the development and test set since training and development sets are provided. For the other two datasets -MAWPS and ALG514, -we report the average accuracy and standard error using 5-fold cross-validation.",
                "cite_spans": [
                    {
                        "start": 100,
                        "end": 122,
                        "text": "(Kushman et al., 2014)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 288,
                        "end": 289,
                        "text": "3",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Metric and Datasets",
                "sec_num": "4.1"
            },
            {
                "text": "We examine the performance of EPT against five existing state-of-the-art (SoTA) models. The five models are categorized into three types; model using hand-crafted features, pure neural models, and a hybrid of these two types.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baseline and ablated models",
                "sec_num": "4.2"
            },
            {
                "text": "\u2022 Models using hand-crafted features use expertdefined input features without using a neural model: MixedSP (Upadhyay et al., 2016) . Upadhyay et al. (2016) designed a model using a set of hand-crafted features similar to those used by Zhou et al. (2015) . Using a data augmentation technique, they achieved the SoTA on ALG514 (83.0%) and DRAW-1K (59.5%).",
                "cite_spans": [
                    {
                        "start": 108,
                        "end": 131,
                        "text": "(Upadhyay et al., 2016)",
                        "ref_id": null
                    },
                    {
                        "start": 134,
                        "end": 156,
                        "text": "Upadhyay et al. (2016)",
                        "ref_id": null
                    },
                    {
                        "start": 236,
                        "end": 254,
                        "text": "Zhou et al. (2015)",
                        "ref_id": "BIBREF25"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baseline and ablated models",
                "sec_num": "4.2"
            },
            {
                "text": "\u2022 Pure neural models take algebraic word problems as the raw input to a neural model and do not require the use of a rule-based model: CASS-RL (Huang et al., 2018) and T-MTDNN (Lee and Gweon, 2020) \u2022 Hybrid models are models that are neither purely hand-crafted nor pure neural models: CASS-hybrid (Huang et al., 2018) and DNS (Wang et al., 2019) . The CASS-hybrid is the best-performing hybrid model of the CASS-RL and Huang et al. ( 2017)'s model using hand-crafted features. The DNS is a hybrid model of a sequence-to-sequence model and a model using hand-crafted features. We copied the accuracy of DNS on DRAW-1K from Zhang et al. (2019) .",
                "cite_spans": [
                    {
                        "start": 143,
                        "end": 163,
                        "text": "(Huang et al., 2018)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 176,
                        "end": 197,
                        "text": "(Lee and Gweon, 2020)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 298,
                        "end": 318,
                        "text": "(Huang et al., 2018)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 327,
                        "end": 346,
                        "text": "(Wang et al., 2019)",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 623,
                        "end": 642,
                        "text": "Zhang et al. (2019)",
                        "ref_id": "BIBREF24"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baseline and ablated models",
                "sec_num": "4.2"
            },
            {
                "text": "After examining the EPT model performance, we conducted an ablation study to analyze the effect of using two main components of EPT; Expression tokens and operand-context pointers. We compared three types of models to test each of the components: (1) the vanilla Transformer model, (2) the Transformer with Expression token model, which investigates the effect of using Expression tokens, and (3) the EPT, which investigates the effect of using pointers in addition to Expression tokens. Additional details on the input/output of the vanilla Transformer and the Transformer with Expression token models are provided in Appendix A.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baseline and ablated models",
                "sec_num": "4.2"
            },
            {
                "text": "The implementation details of EPT and its ablated models are as follows. To build encoder-decoder models, we used PyTorch 1.5 (Paszke et al., 2019) . For the encoder, three different sizes of ALBERT models in the transformers library (Wolf et al., 2019) are used: albert-base-v2, albert-large-v2, and albert-xlarge-v2.",
                "cite_spans": [
                    {
                        "start": 126,
                        "end": 147,
                        "text": "(Paszke et al., 2019)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 234,
                        "end": 253,
                        "text": "(Wolf et al., 2019)",
                        "ref_id": "BIBREF22"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementation details",
                "sec_num": "4.3"
            },
            {
                "text": "We fixed the encoder's embedding matrix during the training since such fixation preserves the world knowledge embedded in the matrix and stabilizes the entire learning process. For the decoder, we stacked six decoder layers and shared the parameters across different layers to reduce memory usage. We set the dimension of input vector D as the same dimension of encoder hidden-state vectors. To train and evaluate the entire model, we used teacher forcing in the training phase and beam search with 3 beams in the evaluation phase.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementation details",
                "sec_num": "4.3"
            },
            {
                "text": "For the hyperparameters of the EPT, parameters follow the ALBERT model's parameters except for training epoch, batch size, warm-up epoch, and learning rate. First, for the training epoch T , a model is trained in 500, 500, and 100 epochs on ALG514, DRAW-1K, and MAWPS, respectively. For batch sizes, we used 2,048 (albert-base-v2 and albert-large-v2) and 1,024 (albert-xlarge-v2) in terms of Op or Expression tokens. To acquire a similar effect of using 4,096 tokens as a batch, we also employed gradient accumulation technique on two types of consecutive mini-batches; two (base and large) and four (xlarge). Then, for the warm-up epoch and learning rate, we conduct the grid-search algorithm for each pair of a dataset and the size of the ALBERT model. For the grid search, we set the sampling space as follows: {0.00125, 0.00176, 0.0025} for the learning rates and {0, 0.005T, 0.01T, 0.015T, 0.02T, 0.025T } for the warm-up. The resulting parameters are listed in Appendix B. During each grid search, we only use the following training/validation sets and keep other sets unseen: the fold-0 training/test split for ALG514 and MAWPS and the training/development set for DRAW-1K. For the unstated hyperparameters, the parameters follow those of the ALBERT. These parameters include the optimizer and warm-up scheduler; we used LAMB (You et al., 2019) optimizer with \u03b2 1 = 0.9, \u03b2 2 = 0.999, and = 10 -12 ; and we Model ALG DRAW-1K MAWPS 514 (Dev.) (Test) State of the art (SOTA) Hand-crafted 83.0 [M] 59.5 [M] -Pure neural 44.5 [C] -78.9 [T] Ensembles 82.5 [H] 31.0 Note: [M] MixedSP, [C] CASS-RL, [T] T-MTDNN, [H] CASS-hybrid, [D] DNS. * Overfitted on some folds. employed linear decay with warm-up scheduling. All the experiment, including hyperparameter search, was conducted on a local computer with 64GB RAM and two GTX1080 Ti GPUs.",
                "cite_spans": [
                    {
                        "start": 1333,
                        "end": 1351,
                        "text": "(You et al., 2019)",
                        "ref_id": "BIBREF23"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementation details",
                "sec_num": "4.3"
            },
            {
                "text": "In section 5.1, we first present a comparison study, which examines the EPT's performance. Next, in section 5.2, we present an ablation study, which analyzes the two main components of EPT; Expression tokens and operand-context pointers.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Result and Discussion",
                "sec_num": "5"
            },
            {
                "text": "As shown in Table 4 , the performance of EPT is comparable or better in terms of performance accuracy compared to existing state-of-the-art (SoTA) models when tested on the three datasets of ALG514, DRAW-1K, and MAWPS. The fully automatic EPT model, which does not use handcrafted features, yields comparable performance to existing models using hand-crafted features. Specifically, on the ALG514 dataset, the EPT outperforms the best-performing pure neural model by about 40% and shows comparable performance accuracy to the SoTA model that uses hand-crafted features. On the DRAW-1K dataset, which is harder than ALG514 dataset, a similar performance trend to ALG514 is found. The EPT model outperforms the hybrid model by about 30% and achieved comparable accuracy to the SoTA model that uses hand-crafted features. On the MAWPS dataset, which is only tested on pure neural models in existing studies, the EPT achieves SoTA accuracy. One possible explanation for EPT's outstanding performance over the existing pure neural model is the use of operand's contextual information. Existing neural models solve algebraic word problems by using symbols to provide an abstraction of problem-dependent numbers or unknowns. For example, Figure 1 shows that existing methods used Op tokens, such as x 0 and N 1 . However, treating operands as symbols only reflects 2 out of 4 means in which symbols are used in humans' mathematical problem-solving procedures (Usiskin, 1999) . The 4 means of symbol usage are; (1) generalizing common patterns, (2) representing unknowns in an equation, (3) indicating an argument of a function, and (4) replacing arbitrary marks. By applying template classification or machine learning techniques, (1) and (2) were successfully utilized in existing neural models. However, the existing neural models could not consider (3) and (4). Therefore, in our suggested EPT model, we dealt with (3) by using Expression tokens and (4) by using operand-context pointers. We suspect that the EPT's performance, which is comparable to existing models using hand-crafted features, comes from dealing with (3) and ( 4) explicitly when solving algebraic word problems.",
                "cite_spans": [
                    {
                        "start": 1452,
                        "end": 1467,
                        "text": "(Usiskin, 1999)",
                        "ref_id": "BIBREF17"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 18,
                        "end": 19,
                        "text": "4",
                        "ref_id": "TABREF4"
                    },
                    {
                        "start": 1238,
                        "end": 1239,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Comparison study",
                "sec_num": "5.1"
            },
            {
                "text": "From the ablation study, our data showed that the two components of generating 'Expression' token and applying operand-context pointer, each improved the accuracy of the EPT model in different ways. Specifically, as seen in Table 5 , adding Expression token to the vanilla Transformer improved the performance accuracy by about 15% in ALG514 and DRAW-1K and about 1% in MAWPS. In addition, applying operand-context pointer to the Transformer with Expression token Case 1.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 230,
                        "end": 231,
                        "text": "5",
                        "ref_id": "TABREF6"
                    }
                ],
                "eq_spans": [],
                "section": "Ablation study",
                "sec_num": "5.2"
            },
            {
                "text": "The sum of two numbers is 90. Three times the smaller is 10 more than the larger. Find the larger number. Expected 3x 0 -x 1 = 10,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Problem",
                "sec_num": null
            },
            {
                "text": "x 0 + x 1 = 90 Vanilla Transformer x 0 + x 1 = 3, x 0 -x 1 = 10 (Incorrect) + Expression 3x 0 -x 1 = 10, x 0 + x 1 = 90 (Correct) + Pointer (EPT) 3x 0 -x 1 = 10, x 0 + x 1 = 90 (Correct)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Problem",
                "sec_num": null
            },
            {
                "text": "Case 2.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Problem",
                "sec_num": null
            },
            {
                "text": "Problem A minor league baseball team plays 130 games in a season. If the team won 14 more than three times as many games as they lost, how many wins and losses did the team have? Expected",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Effect of using pointers",
                "sec_num": null
            },
            {
                "text": "x 0 -3x 1 = 14,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Effect of using pointers",
                "sec_num": null
            },
            {
                "text": "x 0 + x 1 =130 Vanilla Transformer 14x 0 -3x 1 = 0, x 0 + x 1 =130 (Incorrect) + Expression x 0 -3x 1 = 14, 130x 0 -x 1 = 0 (Incorrect) + Pointer (EPT) x 0 -3x 1 = 14, x 0 + x 1 =130 (Correct)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Effect of using pointers",
                "sec_num": null
            },
            {
                "text": "Case 3.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Effect of using pointers",
                "sec_num": null
            },
            {
                "text": "One number is 6 more than another. If the sum of the smaller number and 3 times the larger number is 34, find the two numbers. Expected",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Problem",
                "sec_num": null
            },
            {
                "text": "x 0 + 3x 1 = 34, x 1 -x 0 = 6 Vanilla Transformer x 0 + 3x 1 = 34, x 1 -x 0 = 6 (Correct) + Expression 3x 0 + 34x 1 = 2, x 1 -x 0 = 6 (Incorrect) + Pointer (EPT) 3x 0 + x 1 = 34, x 1 -x 0 = 6 (Incorrect)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Problem",
                "sec_num": null
            },
            {
                "text": "Case 4.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Problem",
                "sec_num": null
            },
            {
                "text": "The denominator of a fraction exceeds the numerator by 7. if the numerator is increased by three and the denominator increased by 5, the resulting fraction is equal to half. Find the original fraction. Expected model enhanced the performance by about 30% in ALG514 and DRAW-1K and about 3% in MAWPS.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Problem",
                "sec_num": null
            },
            {
                "text": "x 0 -1 2 x 1 = 1 2 \u2022 5 -3, x 0 -x 1 = 7 Vanilla Transformer 3x 0 + 5x 1 = 1 2 N 4 , (Incorrect) + Expression 3x 0 -5x 1 = 0, x 0 + x 1 = 7 (Incorrect) + Pointer (EPT) 5x 0 -3x 1 = 1 2 , x 1 -x 0 = 7 (Incorrect)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Problem",
                "sec_num": null
            },
            {
                "text": "Table 6 shows the result of an error analysis. The cases 1 and 2 show how the EPT model's two components contributed to performance improvement. In case 1, the vanilla Transformer yields an incorrect solution equation by incorrectly associating x 0 + x 1 and 3. However, using an Expression token, the explicit relationship between operator and operands is maintained, enabling the distinction between x 0 +x 1 and 3x 0 -x 1 . The case 2 example shows how adding an operand-context pointer can help distinguish between different expressions, in our example, x 0 , 130x 0 , and 14x 0 . As the operand-context pointer directly points to the contextual information of an operand, the EPT could utilize the relationship between unknown (x 0 ) and its multiples (130x 0 or 14x 0 ) without confusion.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "6",
                        "ref_id": "TABREF7"
                    }
                ],
                "eq_spans": [],
                "section": "Problem",
                "sec_num": null
            },
            {
                "text": "We observed that the existing pure neural model's performance on low-complexity dataset of MAWPS was relatively high at 78.9%, compared to that of high-complexity dataset of ALG514 (44.5%). Therefore, using Expression tokens and operand-context pointers contributed to higher performance when applied to high-complexity datasets of ALG514 and DRAW-1K, as shown in Table 5 . We suspect two possible explanations for such a performance enhancement.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 370,
                        "end": 371,
                        "text": "5",
                        "ref_id": "TABREF6"
                    }
                ],
                "eq_spans": [],
                "section": "Problem",
                "sec_num": null
            },
            {
                "text": "First, using Expression tokens in highcomplexity datasets address the expression fragmentation issue when generating solution equations, which is more complex in ALG514 and DRAW-1K than MAWPS. Specifically, Table 3 shows that on average the number of unknowns in ALG514 and DRAW-1K is almost twice (1.82 and 1.75, respectively) than MAWPS (1.0). Similarly, the number of Op tokens is also twice in ALG514 and respectively) than that of MAWPS (6.20) . As the expression fragmentation issue can arise for each token, probability of fragmentation issues' occurrence increases exponentially as the number of unknowns/Op tokens in a problem increases.",
                "cite_spans": [
                    {
                        "start": 409,
                        "end": 422,
                        "text": "respectively)",
                        "ref_id": null
                    },
                    {
                        "start": 436,
                        "end": 448,
                        "text": "MAWPS (6.20)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 213,
                        "end": 214,
                        "text": "3",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Problem",
                "sec_num": null
            },
            {
                "text": "Therefore, the vanilla Transformer model, which could not handle the fragmentation issue, yields low accuracy on high-complexity datasets.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Problem",
                "sec_num": null
            },
            {
                "text": "Second, using operand-context pointers in highcomplexity datasets addresses the operand-context separation issue when selecting an operand, which is more complex in ALG514 and DRAW-1K than MAWPS. Specifically, Table 3 shows that on average the amount of Expression tokens is also twice in ALG514 and respectively) than that of MAWPS (3.60). As numbers and Expression tokens are candidates for selecting an operand, probability of separation issues' occurrence increases linearly as the amount of numbers/Expressions in an equation increases. Since a Transformer with Expression token could not handle the separation issue, the model showed lower accuracy on high-complexity datasets.",
                "cite_spans": [
                    {
                        "start": 300,
                        "end": 313,
                        "text": "respectively)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 216,
                        "end": 217,
                        "text": "3",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Problem",
                "sec_num": null
            },
            {
                "text": "In addition to the correctly solved problem examples, Table 6 also shows cases 3 and 4, which were incorrectly answered by the EPT model. The erroneous examples can be categorized into two groups; 'Comparative' error and 'Temporal order' error. 'Comparative' occurs when an algebraic problem contains comparative phrases, such as '6 more than,' as in case 3. 49.3% of incorrectly solved problems contained comparatives. When generating solution equations for the comparative phrases, the order of arguments is a matter for an equation that contains non-commutative operators, such as subtractions or divisions. Therefore, errors occurred when the order of arguments for comparative phrases with non-commutative operators was mixed up. Another group of error is 'Temporal order' error that occurs when a problem contains phrases with temporal orders, such as 'the numerator is increased by three,' as in case 4. 44.5% of incorrectly solved problems contained temporal orders. We suspect that these problems occur when co-referencing is not handled correctly. In a word problem with temporal ordering, a same entity may have two or more numeric values that change over time. For example, in case 4, the denominator has two different values of x 1 and x 1 + 7. The EPT model failed to assign a same variable for the denominators. The model assigned x 0 in the former expression and x 1 in the latter.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Problem",
                "sec_num": null
            },
            {
                "text": "In this study, we proposed a neural algebraic word problem solver, Expression-Pointer Transformer (EPT), and examined its characteristics. We designed EPT to address two issues: expression fragmentation and operand-context separation. The EPT resolves the expression fragmentation issue by generating 'Expression' tokens, which simultaneously generate an operator and required operands. In addition, the EPT resolves the operand-context separation issue by applying operand-context pointers. Our work is meaningful in that we demonstrated a possibility for alleviating the costly procedure of devising hand-crafted features in the domain of solving algebraic word problems. As future work, we plan to generalize the EPT to other datasets, including non-English word problems or non-algebraic domains in math, to extend our model.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "6"
            },
            {
                "text": "In this section, we describe how we compute the input and output of the two ablation models: (1) a vanilla Transformer and (2) a vanilla Transformer with 'Expression' tokens. Figure 3 shows the two models.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 182,
                        "end": 183,
                        "text": "3",
                        "ref_id": "FIGREF2"
                    }
                ],
                "eq_spans": [],
                "section": "A Input/output of ablation models",
                "sec_num": null
            },
            {
                "text": "The first ablation model is a vanilla Transformer. The model generates an 'Op' token sequence and does not use operand-context pointers. The model manages an 'Op' token vocabulary that contains operators, constants, variables, and number placeholders (e.g., N 0 ). So the input of this model's decoder only utilizes a look-up table for embedding vectors. For the decoder's output, the vanilla Transformer uses a feed-forward softmax layer to output the probability of selecting an Op token. In summary, the input vector v i of a token t i and the output t i+1 can be computed as follows.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Input/output of ablation models",
                "sec_num": null
            },
            {
                "text": "v i = LN in (c in E in (t i ) + PE(i)) , (11) t i+1 = arg max t \u03c3 (FF out (d i )) t . (12)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Input/output of ablation models",
                "sec_num": null
            },
            {
                "text": "The second ablation model is a vanilla Transformer model that uses 'Expression' tokens as a unit of input/output. This model generates an 'Expression' token sequence but does not apply operand-context pointers. Instead of using operandcontext pointers, this model uses an operand vocabulary that contains constants, placeholders for numbers, and placeholders of previous Expression token results (e.g., R 0 ). The input of this model's decoder is similar to that of EPT's decoder, but we replaced the equations 3 and 5 with the following formulae.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Input/output of ablation models",
                "sec_num": null
            },
            {
                "text": "a ij = LN a (c a u num + E c (a ij )) , (13) a ij = LN a (c a u expr + E c (a ij )) . ( 14)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Input/output of ablation models",
                "sec_num": null
            },
            {
                "text": "For the output of this model's decoder, we used a feed-forward softmax layer to output the probability of selecting an operand. Since the softmax output can select the unavailable operand, we set the probability of such unavailable tokens as zeros to mask them. So, we replace equation 10 with the following formula. where M is a masking function to set zero probability on unavailable tokens when generating ith Op token. The other unstated equations 1, 2, 4, and 6 remain the same.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Input/output of ablation models",
                "sec_num": null
            },
            {
                "text": "Table 7 shows the best parameters and performances on the development set, which are found using grid search. ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "7",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "B Hyperparameters used for this study",
                "sec_num": null
            },
            {
                "text": "The code is available on https://github.com/ snucclab/ept.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "When two or more tokens form a number in the problem, we averaged all related hidden-state vectors.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Since we want to sustain simultaneous decoding, which is one of the strengths in the Transformer, we use PE(k) for the kth prior Expression, although it is possible to use decoder hidden state d k .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "We provide a preprocessed version of these datasets on https://github.com/snucclab/ept/tree/ master/dataset.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "http://groups.csail.mit.edu/rbg/code/ wordprobs/",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://www.microsoft.com/en-us/ download/details.aspx?id=52628",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "http://lang.ee.washington.edu/MAWPS",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. 2020R1C1C1010162).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgments",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "MathQA: Towards interpretable math word problem solving with operation-based formalisms",
                "authors": [
                    {
                        "first": "Aida",
                        "middle": [],
                        "last": "Amini",
                        "suffix": ""
                    },
                    {
                        "first": "Saadia",
                        "middle": [],
                        "last": "Gabriel",
                        "suffix": ""
                    },
                    {
                        "first": "Shanchuan",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Rik",
                        "middle": [],
                        "last": "Koncel-Kedziorski",
                        "suffix": ""
                    },
                    {
                        "first": "Yejin",
                        "middle": [],
                        "last": "Choi",
                        "suffix": ""
                    },
                    {
                        "first": "Hannaneh",
                        "middle": [],
                        "last": "Hajishirzi",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 2019 Conference of the North American Chapter",
                "volume": "1",
                "issue": "",
                "pages": "2357--2367",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/N19-1245"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Ha- jishirzi. 2019. MathQA: Towards interpretable math word problem solving with operation-based formalisms. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2357-2367, Minneapolis, Minnesota. Associ- ation for Computational Linguistics.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Layer normalization",
                "authors": [
                    {
                        "first": "Jimmy",
                        "middle": [],
                        "last": "Lei Ba",
                        "suffix": ""
                    },
                    {
                        "first": "Jamie",
                        "middle": [],
                        "last": "Ryan Kiros",
                        "suffix": ""
                    },
                    {
                        "first": "Geoffrey",
                        "middle": [
                            "E"
                        ],
                        "last": "Hinton",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1607.06450"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. arXiv preprint arXiv:1607.06450.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Semantically-aligned equation generation for solving and reasoning math word problems",
                "authors": [
                    {
                        "first": "Ting-Rui",
                        "middle": [],
                        "last": "Chiang",
                        "suffix": ""
                    },
                    {
                        "first": "Yun-Nung",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "volume": "1",
                "issue": "",
                "pages": "2656--2668",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/N19-1272"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ting-Rui Chiang and Yun-Nung Chen. 2019. Semantically-aligned equation generation for solving and reasoning math word problems. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2656-2668, Minneapolis, Minnesota. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Neural math word problem solver with reinforcement learning",
                "authors": [
                    {
                        "first": "Danqing",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Jing",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Chin-Yew",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Jian",
                        "middle": [],
                        "last": "Yin",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 27th International Conference on Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "213--223",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Danqing Huang, Jing Liu, Chin-Yew Lin, and Jian Yin. 2018. Neural math word problem solver with reinforcement learning. In Proceedings of the 27th International Conference on Computational Linguis- tics, pages 213-223, Santa Fe, New Mexico, USA. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Learning fine-grained expressions to solve math word problems",
                "authors": [
                    {
                        "first": "Danqing",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Shuming",
                        "middle": [],
                        "last": "Shi",
                        "suffix": ""
                    },
                    {
                        "first": "Chin-Yew",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Jian",
                        "middle": [],
                        "last": "Yin",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "805--814",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/D17-1084"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Danqing Huang, Shuming Shi, Chin-Yew Lin, and Jian Yin. 2017. Learning fine-grained expressions to solve math word problems. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 805-814, Copenhagen, Denmark. Association for Computational Linguis- tics.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Parsing algebraic word problems into equations",
                "authors": [
                    {
                        "first": "Rik",
                        "middle": [],
                        "last": "Koncel-Kedziorski",
                        "suffix": ""
                    },
                    {
                        "first": "Hannaneh",
                        "middle": [],
                        "last": "Hajishirzi",
                        "suffix": ""
                    },
                    {
                        "first": "Ashish",
                        "middle": [],
                        "last": "Sabharwal",
                        "suffix": ""
                    },
                    {
                        "first": "Oren",
                        "middle": [],
                        "last": "Etzioni",
                        "suffix": ""
                    },
                    {
                        "first": "Siena",
                        "middle": [],
                        "last": "Dumas",
                        "suffix": ""
                    },
                    {
                        "first": "Ang",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Transactions of the Association for Computational Linguistics",
                "volume": "3",
                "issue": "",
                "pages": "585--597",
                "other_ids": {
                    "DOI": [
                        "10.1162/tacl_a_00160"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, and Siena Dumas Ang. 2015. Parsing algebraic word problems into equa- tions. Transactions of the Association for Computa- tional Linguistics, 3:585-597.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "MAWPS: A math word problem repository",
                "authors": [
                    {
                        "first": "Rik",
                        "middle": [],
                        "last": "Koncel-Kedziorski",
                        "suffix": ""
                    },
                    {
                        "first": "Subhro",
                        "middle": [],
                        "last": "Roy",
                        "suffix": ""
                    },
                    {
                        "first": "Aida",
                        "middle": [],
                        "last": "Amini",
                        "suffix": ""
                    },
                    {
                        "first": "Nate",
                        "middle": [],
                        "last": "Kushman",
                        "suffix": ""
                    },
                    {
                        "first": "Hannaneh",
                        "middle": [],
                        "last": "Hajishirzi",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "volume": "",
                "issue": "",
                "pages": "1152--1157",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/N16-1136"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. 2016. MAWPS: A math word problem repository. In Pro- ceedings of the 2016 Conference of the North Amer- ican Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1152-1157, San Diego, California. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Learning to automatically solve algebra word problems",
                "authors": [
                    {
                        "first": "Nate",
                        "middle": [],
                        "last": "Kushman",
                        "suffix": ""
                    },
                    {
                        "first": "Yoav",
                        "middle": [],
                        "last": "Artzi",
                        "suffix": ""
                    },
                    {
                        "first": "Luke",
                        "middle": [],
                        "last": "Zettlemoyer",
                        "suffix": ""
                    },
                    {
                        "first": "Regina",
                        "middle": [],
                        "last": "Barzilay",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics",
                "volume": "1",
                "issue": "",
                "pages": "271--281",
                "other_ids": {
                    "DOI": [
                        "10.3115/v1/P14-1026"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Nate Kushman, Yoav Artzi, Luke Zettlemoyer, and Regina Barzilay. 2014. Learning to automatically solve algebra word problems. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 271-281, Baltimore, Maryland. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Albert: A lite bert for self-supervised learning of language representations",
                "authors": [
                    {
                        "first": "Zhenzhong",
                        "middle": [],
                        "last": "Lan",
                        "suffix": ""
                    },
                    {
                        "first": "Mingda",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Sebastian",
                        "middle": [],
                        "last": "Goodman",
                        "suffix": ""
                    },
                    {
                        "first": "Kevin",
                        "middle": [],
                        "last": "Gimpel",
                        "suffix": ""
                    },
                    {
                        "first": "Piyush",
                        "middle": [],
                        "last": "Sharma",
                        "suffix": ""
                    },
                    {
                        "first": "Radu",
                        "middle": [],
                        "last": "Soricut",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019. Albert: A lite bert for self-supervised learning of language representations.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Solving arithmetic word problems with a templatebased multi-task deep neural network (t-mtdnn)",
                "authors": [
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "G",
                        "middle": [],
                        "last": "Gweon",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "2020 IEEE International Conference on Big Data and Smart Computing (Big-Comp)",
                "volume": "",
                "issue": "",
                "pages": "271--274",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "D. Lee and G. Gweon. 2020. Solving arithmetic word problems with a templatebased multi-task deep neu- ral network (t-mtdnn). In 2020 IEEE International Conference on Big Data and Smart Computing (Big- Comp), pages 271-274.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Sympy: symbolic computing in python",
                "authors": [
                    {
                        "first": "Aaron",
                        "middle": [],
                        "last": "Meurer",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "P"
                        ],
                        "last": "Smith",
                        "suffix": ""
                    },
                    {
                        "first": "Mateusz",
                        "middle": [],
                        "last": "Paprocki",
                        "suffix": ""
                    },
                    {
                        "first": "Ond\u0159ej",
                        "middle": [],
                        "last": "\u010cert\u00edk",
                        "suffix": ""
                    },
                    {
                        "first": "B",
                        "middle": [],
                        "last": "Sergey",
                        "suffix": ""
                    },
                    {
                        "first": "Matthew",
                        "middle": [],
                        "last": "Kirpichev",
                        "suffix": ""
                    },
                    {
                        "first": "Amit",
                        "middle": [],
                        "last": "Rocklin",
                        "suffix": ""
                    },
                    {
                        "first": "Sergiu",
                        "middle": [],
                        "last": "Kumar",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [
                            "K"
                        ],
                        "last": "Ivanov",
                        "suffix": ""
                    },
                    {
                        "first": "Sartaj",
                        "middle": [],
                        "last": "Moore",
                        "suffix": ""
                    },
                    {
                        "first": "Thilina",
                        "middle": [],
                        "last": "Singh",
                        "suffix": ""
                    },
                    {
                        "first": "Sean",
                        "middle": [],
                        "last": "Rathnayake",
                        "suffix": ""
                    },
                    {
                        "first": "Brian",
                        "middle": [
                            "E"
                        ],
                        "last": "Vig",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [
                            "P"
                        ],
                        "last": "Granger",
                        "suffix": ""
                    },
                    {
                        "first": "Francesco",
                        "middle": [],
                        "last": "Muller",
                        "suffix": ""
                    },
                    {
                        "first": "Harsh",
                        "middle": [],
                        "last": "Bonazzi",
                        "suffix": ""
                    },
                    {
                        "first": "Shivam",
                        "middle": [],
                        "last": "Gupta",
                        "suffix": ""
                    },
                    {
                        "first": "Fredrik",
                        "middle": [],
                        "last": "Vats",
                        "suffix": ""
                    },
                    {
                        "first": "Fabian",
                        "middle": [],
                        "last": "Johansson",
                        "suffix": ""
                    },
                    {
                        "first": "Matthew",
                        "middle": [
                            "J"
                        ],
                        "last": "Pedregosa",
                        "suffix": ""
                    },
                    {
                        "first": "Andy",
                        "middle": [
                            "R"
                        ],
                        "last": "Curry",
                        "suffix": ""
                    },
                    {
                        "first": "\u0160t\u011bp\u00e1n",
                        "middle": [],
                        "last": "Terrel",
                        "suffix": ""
                    },
                    {
                        "first": "Ashutosh",
                        "middle": [],
                        "last": "Rou\u010dka",
                        "suffix": ""
                    },
                    {
                        "first": "Isuru",
                        "middle": [],
                        "last": "Saboo",
                        "suffix": ""
                    },
                    {
                        "first": "Sumith",
                        "middle": [],
                        "last": "Fernando",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Kulal",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "PeerJ Computer Science",
                "volume": "3",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.7717/peerj-cs.103"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Aaron Meurer, Christopher P. Smith, Mateusz Pa- procki, Ond\u0159ej \u010cert\u00edk, Sergey B. Kirpichev, Matthew Rocklin, AMiT Kumar, Sergiu Ivanov, Jason K. Moore, Sartaj Singh, Thilina Rathnayake, Sean Vig, Brian E. Granger, Richard P. Muller, Francesco Bonazzi, Harsh Gupta, Shivam Vats, Fredrik Johansson, Fabian Pedregosa, Matthew J. Curry, Andy R. Terrel, \u0160t\u011bp\u00e1n Rou\u010dka, Ashutosh Saboo, Isuru Fernando, Sumith Kulal, Robert Cimr- man, and Anthony Scopatz. 2017. Sympy: symbolic computing in python. PeerJ Computer Science, 3:e103.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Pytorch: An imperative style, high-performance deep learning library",
                "authors": [
                    {
                        "first": "Adam",
                        "middle": [],
                        "last": "Paszke",
                        "suffix": ""
                    },
                    {
                        "first": "Sam",
                        "middle": [],
                        "last": "Gross",
                        "suffix": ""
                    },
                    {
                        "first": "Francisco",
                        "middle": [],
                        "last": "Massa",
                        "suffix": ""
                    },
                    {
                        "first": "Adam",
                        "middle": [],
                        "last": "Lerer",
                        "suffix": ""
                    },
                    {
                        "first": "James",
                        "middle": [],
                        "last": "Bradbury",
                        "suffix": ""
                    },
                    {
                        "first": "Gregory",
                        "middle": [],
                        "last": "Chanan",
                        "suffix": ""
                    },
                    {
                        "first": "Trevor",
                        "middle": [],
                        "last": "Killeen",
                        "suffix": ""
                    },
                    {
                        "first": "Zeming",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Natalia",
                        "middle": [],
                        "last": "Gimelshein",
                        "suffix": ""
                    },
                    {
                        "first": "Luca",
                        "middle": [],
                        "last": "Antiga",
                        "suffix": ""
                    },
                    {
                        "first": "Alban",
                        "middle": [],
                        "last": "Desmaison",
                        "suffix": ""
                    },
                    {
                        "first": "Andreas",
                        "middle": [],
                        "last": "Kopf",
                        "suffix": ""
                    },
                    {
                        "first": "Edward",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Zachary",
                        "middle": [],
                        "last": "Devito",
                        "suffix": ""
                    },
                    {
                        "first": "Martin",
                        "middle": [],
                        "last": "Raison",
                        "suffix": ""
                    },
                    {
                        "first": "Alykhan",
                        "middle": [],
                        "last": "Tejani",
                        "suffix": ""
                    },
                    {
                        "first": "Sasank",
                        "middle": [],
                        "last": "Chilamkurthy",
                        "suffix": ""
                    },
                    {
                        "first": "Benoit",
                        "middle": [],
                        "last": "Steiner",
                        "suffix": ""
                    },
                    {
                        "first": "Lu",
                        "middle": [],
                        "last": "Fang",
                        "suffix": ""
                    },
                    {
                        "first": "Junjie",
                        "middle": [],
                        "last": "Bai",
                        "suffix": ""
                    },
                    {
                        "first": "Soumith",
                        "middle": [],
                        "last": "Chintala",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
                "volume": "32",
                "issue": "",
                "pages": "1743--1752",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/D15-1202"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Te- jani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019. Py- torch: An imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d' Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 8024-8035. Curran Associates, Inc. Subhro Roy and Dan Roth. 2015. Solving general arithmetic word problems. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1743-1752, Lisbon, Portugal. Association for Computational Linguis- tics.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Unit dependency graph and its application to arithmetic word problem solving",
                "authors": [
                    {
                        "first": "Subhro",
                        "middle": [],
                        "last": "Roy",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Roth",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, AAAI'17",
                "volume": "",
                "issue": "",
                "pages": "3082--3088",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Subhro Roy and Dan Roth. 2017. Unit dependency graph and its application to arithmetic word problem solving. In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, AAAI'17, page 3082-3088. AAAI Press.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Reasoning about quantities in natural language. Transactions of the Association for Computational Linguistics",
                "authors": [
                    {
                        "first": "Subhro",
                        "middle": [],
                        "last": "Roy",
                        "suffix": ""
                    },
                    {
                        "first": "Tim",
                        "middle": [],
                        "last": "Vieira",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Roth",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "",
                "volume": "3",
                "issue": "",
                "pages": "1--13",
                "other_ids": {
                    "DOI": [
                        "10.1162/tacl_a_00118"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Subhro Roy, Tim Vieira, and Dan Roth. 2015. Reason- ing about quantities in natural language. Transac- tions of the Association for Computational Linguis- tics, 3:1-13.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Rethinking the inception architecture for computer vision",
                "authors": [
                    {
                        "first": "Christian",
                        "middle": [],
                        "last": "Szegedy",
                        "suffix": ""
                    },
                    {
                        "first": "Vincent",
                        "middle": [],
                        "last": "Vanhoucke",
                        "suffix": ""
                    },
                    {
                        "first": "Sergey",
                        "middle": [],
                        "last": "Ioffe",
                        "suffix": ""
                    },
                    {
                        "first": "Jon",
                        "middle": [],
                        "last": "Shlens",
                        "suffix": ""
                    },
                    {
                        "first": "Zbigniew",
                        "middle": [],
                        "last": "Wojna",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. 2016. Rethinking the inception architecture for computer vision. In The IEEE Conference on Computer Vision and Pat- tern Recognition (CVPR).",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Annotating derivations: A new evaluation strategy and dataset for algebra word problems",
                "authors": [
                    {
                        "first": "Shyam",
                        "middle": [],
                        "last": "Upadhyay",
                        "suffix": ""
                    },
                    {
                        "first": "Ming-Wei",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Shyam Upadhyay and Ming-Wei Chang. 2016. An- notating derivations: A new evaluation strategy and dataset for algebra word problems. CoRR, abs/1609.07197.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Learning from explicit and implicit supervision jointly for algebra word problems",
                "authors": [
                    {
                        "first": "Shyam",
                        "middle": [],
                        "last": "Upadhyay",
                        "suffix": ""
                    },
                    {
                        "first": "Ming-Wei",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "Kai-Wei",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "Wen-Tau",
                        "middle": [],
                        "last": "Yih",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "297--306",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/D16-1029"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Shyam Upadhyay, Ming-Wei Chang, Kai-Wei Chang, and Wen-tau Yih. 2016. Learning from explicit and implicit supervision jointly for algebra word problems. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Process- ing, pages 297-306, Austin, Texas. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Algebraic Thinking, Grades K-12: Readings from NCTM's School-Based Journals and Other Publications, chapter Conceptions of School Algebra and Uses of Variables",
                "authors": [
                    {
                        "first": "Zalman",
                        "middle": [],
                        "last": "Usiskin",
                        "suffix": ""
                    }
                ],
                "year": 1999,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zalman Usiskin. 1999. Algebraic Thinking, Grades K-12: Readings from NCTM's School-Based Jour- nals and Other Publications, chapter Conceptions of School Algebra and Uses of Variables. National Council of Teachers of Mathematics.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Attention is all you need",
                "authors": [
                    {
                        "first": "Ashish",
                        "middle": [],
                        "last": "Vaswani",
                        "suffix": ""
                    },
                    {
                        "first": "Noam",
                        "middle": [],
                        "last": "Shazeer",
                        "suffix": ""
                    },
                    {
                        "first": "Niki",
                        "middle": [],
                        "last": "Parmar",
                        "suffix": ""
                    },
                    {
                        "first": "Jakob",
                        "middle": [],
                        "last": "Uszkoreit",
                        "suffix": ""
                    },
                    {
                        "first": "Llion",
                        "middle": [],
                        "last": "Jones",
                        "suffix": ""
                    },
                    {
                        "first": "Aidan",
                        "middle": [
                            "N"
                        ],
                        "last": "Gomez",
                        "suffix": ""
                    },
                    {
                        "first": "\u0141ukasz",
                        "middle": [],
                        "last": "Kaiser",
                        "suffix": ""
                    },
                    {
                        "first": "Illia",
                        "middle": [],
                        "last": "Polosukhin",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Advances in neural information processing systems",
                "volume": "",
                "issue": "",
                "pages": "5998--6008",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems, pages 5998-6008.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Pointer networks",
                "authors": [
                    {
                        "first": "Oriol",
                        "middle": [],
                        "last": "Vinyals",
                        "suffix": ""
                    },
                    {
                        "first": "Meire",
                        "middle": [],
                        "last": "Fortunato",
                        "suffix": ""
                    },
                    {
                        "first": "Navdeep",
                        "middle": [],
                        "last": "Jaitly",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "28",
                "issue": "",
                "pages": "2692--2700",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. 2015. Pointer networks. In Advances in Neural Information Processing Systems 28, pages 2692- 2700. Curran Associates, Inc.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Template-based math word problem solvers with recursive neural networks",
                "authors": [
                    {
                        "first": "Lei",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Dongxiang",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Jipeng",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Xing",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Lianli",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Bing",
                        "middle": [
                            "Tian"
                        ],
                        "last": "Dai",
                        "suffix": ""
                    },
                    {
                        "first": "Heng Tao",
                        "middle": [],
                        "last": "Shen",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "volume": "33",
                "issue": "",
                "pages": "7144--7151",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Lei Wang, Dongxiang Zhang, Jipeng Zhang, Xing Xu, Lianli Gao, Bing Tian Dai, and Heng Tao Shen. 2019. Template-based math word problem solvers with recursive neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 7144-7151.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Deep neural solver for math word problems",
                "authors": [
                    {
                        "first": "Yan",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaojiang",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Shuming",
                        "middle": [],
                        "last": "Shi",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "845--854",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/D17-1088"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yan Wang, Xiaojiang Liu, and Shuming Shi. 2017. Deep neural solver for math word problems. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 845-854, Copenhagen, Denmark. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Huggingface's transformers: State-of-the-art natural language processing",
                "authors": [
                    {
                        "first": "Thomas",
                        "middle": [],
                        "last": "Wolf",
                        "suffix": ""
                    },
                    {
                        "first": "Lysandre",
                        "middle": [],
                        "last": "Debut",
                        "suffix": ""
                    },
                    {
                        "first": "Victor",
                        "middle": [],
                        "last": "Sanh",
                        "suffix": ""
                    },
                    {
                        "first": "Julien",
                        "middle": [],
                        "last": "Chaumond",
                        "suffix": ""
                    },
                    {
                        "first": "Clement",
                        "middle": [],
                        "last": "Delangue",
                        "suffix": ""
                    },
                    {
                        "first": "Anthony",
                        "middle": [],
                        "last": "Moi",
                        "suffix": ""
                    },
                    {
                        "first": "Pierric",
                        "middle": [],
                        "last": "Cistac",
                        "suffix": ""
                    },
                    {
                        "first": "Tim",
                        "middle": [],
                        "last": "Rault",
                        "suffix": ""
                    },
                    {
                        "first": "R'emi",
                        "middle": [],
                        "last": "Louf",
                        "suffix": ""
                    },
                    {
                        "first": "Morgan",
                        "middle": [],
                        "last": "Funtowicz",
                        "suffix": ""
                    },
                    {
                        "first": "Jamie",
                        "middle": [],
                        "last": "Brew",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier- ric Cistac, Tim Rault, R'emi Louf, Morgan Fun- towicz, and Jamie Brew. 2019. Huggingface's transformers: State-of-the-art natural language pro- cessing. ArXiv, abs/1910.03771.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Reducing BERT pre-training time from 3 days to 76 minutes",
                "authors": [
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "You",
                        "suffix": ""
                    },
                    {
                        "first": "Jing",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Jonathan",
                        "middle": [],
                        "last": "Hseu",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaodan",
                        "middle": [],
                        "last": "Song",
                        "suffix": ""
                    },
                    {
                        "first": "James",
                        "middle": [],
                        "last": "Demmel",
                        "suffix": ""
                    },
                    {
                        "first": "Cho-Jui",
                        "middle": [],
                        "last": "Hsieh",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yang You, Jing Li, Jonathan Hseu, Xiaodan Song, James Demmel, and Cho-Jui Hsieh. 2019. Reducing BERT pre-training time from 3 days to 76 minutes. CoRR, abs/1904.00962.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "The gap of semantic parsing: A survey on automatic math word problem solvers",
                "authors": [
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "L",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "L",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "B",
                        "middle": [
                            "T"
                        ],
                        "last": "Dai",
                        "suffix": ""
                    },
                    {
                        "first": "H",
                        "middle": [
                            "T"
                        ],
                        "last": "Shen",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "volume": "",
                "issue": "",
                "pages": "1--1",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "D. Zhang, L. Wang, L. Zhang, B. T. Dai, and H. T. Shen. 2019. The gap of semantic parsing: A survey on automatic math word problem solvers. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 1-1.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Learn to solve algebra word problems using quadratic programming",
                "authors": [
                    {
                        "first": "Lipu",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Shuaixiang",
                        "middle": [],
                        "last": "Dai",
                        "suffix": ""
                    },
                    {
                        "first": "Liwei",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "817--822",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/D15-1096"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Lipu Zhou, Shuaixiang Dai, and Liwei Chen. 2015. Learn to solve algebra word problems using quadratic programming. In Proceedings of the 2015 Conference on Empirical Methods in Natural Lan- guage Processing, pages 817-822, Lisbon, Portugal. Association for Computational Linguistics.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 2: The architecture of Expression-Pointer Transformer (EPT) where two ideas applied: (1) Expression token and (2) operand-context pointer.",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "i+1,j = arg max a \u03c3 (a |M (FF j (d i )) ) , (15)",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "Figure 3: The architecture of two ablated model of EPT: a vanilla Transformer and a Transformer using Expression tokens",
                "uris": null,
                "fig_num": "3",
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table/>",
                "type_str": "table",
                "text": "A sample algebraic word problem",
                "html": null,
                "num": null
            },
            "TABREF1": {
                "content": "<table><tr><td colspan=\"3\">ALG514 DRAW-1K</td><td>MAWPS</td></tr><tr><td>Dataset size</td><td/><td/><td/></tr><tr><td>Problems</td><td>514</td><td>1,000</td><td>2,373</td></tr><tr><td>Splits</td><td>5-fold</td><td>Train 600</td><td>5-fold</td></tr><tr><td/><td colspan=\"2\">Dev., Test 200</td><td/></tr><tr><td colspan=\"4\">Complexity of generating equations (per problem)</td></tr><tr><td>Unknown</td><td>1.82</td><td>1.75</td><td>1.00</td></tr><tr><td>Op tokens</td><td>13.08</td><td>14.16</td><td>6.20</td></tr><tr><td colspan=\"4\">Complexity of selecting an operand (per problem)</td></tr><tr><td>Numbers</td><td>4.26</td><td>3.88</td><td>2.72</td></tr><tr><td>Expressions</td><td>7.45</td><td>7.95</td><td>3.60</td></tr></table>",
                "type_str": "table",
                "text": "Characteristics of datasets used in the experiment",
                "html": null,
                "num": null
            },
            "TABREF4": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Accuracy(%) of the EPT and existing models. (B), (L), and (XL) indicate albert-base-v2, albert-large-v2, and albert-xlarge-v2.",
                "html": null,
                "num": null
            },
            "TABREF6": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Accuracy(%) of the EPT and its ablated models (albert-base-v2).",
                "html": null,
                "num": null
            },
            "TABREF7": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Sample incorrect problems (albert-base-v2) from the DRAW-1K development dataset.",
                "html": null,
                "num": null
            }
        }
    }
}