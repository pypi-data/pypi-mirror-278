{
    "paper_id": "D14-1035",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:16:27.052853Z"
    },
    "title": "Parsing low-resource languages using Gibbs sampling for PCFGs with latent annotations",
    "authors": [
        {
            "first": "Liang",
            "middle": [],
            "last": "Sun",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Jason",
            "middle": [],
            "last": "Mielens",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "The University of Texas",
                "location": {
                    "settlement": "Austin"
                }
            },
            "email": "jmielens@utexas.edu"
        },
        {
            "first": "Jason",
            "middle": [],
            "last": "Baldridge",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "The University of Texas",
                "location": {
                    "settlement": "Austin"
                }
            },
            "email": "jbaldrid@utexas.edu"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "PCFGs with latent annotations have been shown to be a very effective model for phrase structure parsing. We present a Bayesian model and algorithms based on a Gibbs sampler for parsing with a grammar with latent annotations. For PCFG-LA, we present an additional Gibbs sampler algorithm to learn annotations from training data, which are parse trees with coarse (unannotated) symbols. We show that a Gibbs sampling technique is capable of parsing sentences in a wide variety of languages and producing results that are on-par with or surpass previous approaches. Our results for Kinyarwanda and Malagasy in particular demonstrate that low-resource language parsing can benefit substantially from a Bayesian approach.",
    "pdf_parse": {
        "paper_id": "D14-1035",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "PCFGs with latent annotations have been shown to be a very effective model for phrase structure parsing. We present a Bayesian model and algorithms based on a Gibbs sampler for parsing with a grammar with latent annotations. For PCFG-LA, we present an additional Gibbs sampler algorithm to learn annotations from training data, which are parse trees with coarse (unannotated) symbols. We show that a Gibbs sampling technique is capable of parsing sentences in a wide variety of languages and producing results that are on-par with or surpass previous approaches. Our results for Kinyarwanda and Malagasy in particular demonstrate that low-resource language parsing can benefit substantially from a Bayesian approach.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Despite great progress over the past two decades on parsing, relatively little work has considered the problem of creating accurate parsers for low-resource languages. Existing work in this area focuses primarily on approaches that use some form of cross-lingual bootstrapping to improve performance. For instance, Hwa et al. (2005) use a parallel Chinese/English corpus and an English dependency grammar to induce an annotated Chinese corpus in order to train a Chinese dependency grammar. Kuhn (2004b) also considers the benefits of using multiple languages to induce a monolingual grammar, making use of a measure for data reliability in order to weight training data based on confidence of annotation. Bootstrapping approaches such as these achieve markedly improved results, but they are dependent on the existence of a parallel bilingual corpus. Very few such corpora are readily available, particularly for low-resource languages, and creating such corpora obviously presents a challenge for many practical applications. Kuhn (2004a) shows some of the difficulty in handling low-resource languages by examining various tasks using Q'anjob'al as an example. Another approach is that of Bender et al. (2002) , who take a more linguistically-motivated approach by making use of linguistic universals to seed newly developed grammars. This substantially reduces the effort by making it unnecessary to learn the basic parameters of a language, but it lacks the robustness of grammars learned from data.",
                "cite_spans": [
                    {
                        "start": 315,
                        "end": 332,
                        "text": "Hwa et al. (2005)",
                        "ref_id": null
                    },
                    {
                        "start": 491,
                        "end": 503,
                        "text": "Kuhn (2004b)",
                        "ref_id": null
                    },
                    {
                        "start": 1028,
                        "end": 1040,
                        "text": "Kuhn (2004a)",
                        "ref_id": null
                    },
                    {
                        "start": 1192,
                        "end": 1212,
                        "text": "Bender et al. (2002)",
                        "ref_id": "BIBREF1"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Recent work on Probabilistic Context-Free Grammars with latent annotations (PCFG-LA) (Matsuzaki et al., 2005; Petrov et al., 2006) have shown them to be effective models for syntactic parsing, especially when less training material is available (Liang et al., 2009; Shindo et al., 2012) . The coarse nonterminal symbols found in vanilla PCFGs are refined by latent variables; these latent annotations can model subtypes of grammar symbols that result in better grammars and enable better estimates of grammar productions. In this paper, we provide a Gibbs sampler for learning PCFG-LA models and show its effectiveness for parsing lowresource languages such as Malagasy and Kinyawanda.",
                "cite_spans": [
                    {
                        "start": 85,
                        "end": 109,
                        "text": "(Matsuzaki et al., 2005;",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 110,
                        "end": 130,
                        "text": "Petrov et al., 2006)",
                        "ref_id": "BIBREF36"
                    },
                    {
                        "start": 245,
                        "end": 265,
                        "text": "(Liang et al., 2009;",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 266,
                        "end": 286,
                        "text": "Shindo et al., 2012)",
                        "ref_id": "BIBREF39"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Previous PCFG-LA work focuses on the problem of parameter estimation, including expectationmaximization (EM) (Matsuzaki et al., 2005; Petrov et al., 2006) , spectral learning (Cohen et al., 2012; Cohen et al., 2013) , and variational inference (Liang et al., 2009; Wang and Blunsom, 2013) . Regardless of inference method, previous work has used the same method to parse new sentences: a Viterbi parse under a new sentence-specific PCFG obtained from an approximation of the original grammar (Matsuzaki et al., 2005) . Here, we provide an alternative approach to parsing new sentences: an extension of the Gibbs sampling algorithm of Johnson et al. (2007) , which learns rule probabilities in an unsupervised PCFG.",
                "cite_spans": [
                    {
                        "start": 109,
                        "end": 133,
                        "text": "(Matsuzaki et al., 2005;",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 134,
                        "end": 154,
                        "text": "Petrov et al., 2006)",
                        "ref_id": "BIBREF36"
                    },
                    {
                        "start": 175,
                        "end": 195,
                        "text": "(Cohen et al., 2012;",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 196,
                        "end": 215,
                        "text": "Cohen et al., 2013)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 244,
                        "end": 264,
                        "text": "(Liang et al., 2009;",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 265,
                        "end": 288,
                        "text": "Wang and Blunsom, 2013)",
                        "ref_id": "BIBREF41"
                    },
                    {
                        "start": 492,
                        "end": 516,
                        "text": "(Matsuzaki et al., 2005)",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 634,
                        "end": 655,
                        "text": "Johnson et al. (2007)",
                        "ref_id": "BIBREF23"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "We use a Gibbs sampler to collect sampled trees theoretically distributed from the true posterior distribution in order to parse. Priors in a Bayesian model can control the sparsity of grammars (which the insideoutside algorithm fails to do), while naturally incorporating smoothing into the model (Johnson et al., 2007; Liang et al., 2009) . We also build a Bayesian model for parsing with a treebank, and incorporate information from training data as a prior. Moreover, we extend the Gibbs sampler to learn and parse PCFGs with latent annotations. Learning the latent annotations is a compute-intensive process. We show how a small amount of training data can be used to bootstrap: after running a large number of sampling iterations on a small set, the resulting parameters are used to seed a smaller number of iterations on the full training data. This allows us to employ more latent annotations while maintaining reasonable training times and still making full use of the available training data.",
                "cite_spans": [
                    {
                        "start": 298,
                        "end": 320,
                        "text": "(Johnson et al., 2007;",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 321,
                        "end": 340,
                        "text": "Liang et al., 2009)",
                        "ref_id": "BIBREF29"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "To determine the cross-linguistic applicability of these methods, we evaluate on a wide variety of languages with varying amounts of available training data. We use English and Chinese as examples of languages with high data availability, while Italian, Malagasy, and Kinyarwanda provide examples of languages with little available data.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "We find that our technique comes near state of the art results on large datasets, such as those for Chinese and English, and it provides excellent results on limited datasets -both artificially limited in the case of English, and naturally limited in the case of Italian, Malagasy, and Kinyarwanda. This, combined with its ability to run off-the-shelf on new languages without any supporting materials such as parallel corpora, make it a valuable technique for the parsing of low-resource languages.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Our starting point is a Gibbs Sampling algorithm for vanilla PCFGs introduced by Johnson et al. (2007) for estimating rule probabilities in an unsupervised PCFG. We focus instead on using this algorithm for parsing new sentences and then extending it to learn PCFGs with latent annotations. We begin by summarizing the Bayesian PCFG and Gibbs sampler defined by Johnson et al. (2007) .",
                "cite_spans": [
                    {
                        "start": 81,
                        "end": 102,
                        "text": "Johnson et al. (2007)",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 362,
                        "end": 383,
                        "text": "Johnson et al. (2007)",
                        "ref_id": "BIBREF23"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Gibbs sampling for PCFGs",
                "sec_num": "2"
            },
            {
                "text": "Bayesian PCFG For a grammar G, each rule r in the set of rules R has an associated probability \u03b8 r . The probabilities for all the rules that expand the same nonterminal A must sum to one: A\u2192\u03b2\u2208R \u03b8 A\u2192\u03b2 = 1.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Gibbs sampling for PCFGs",
                "sec_num": "2"
            },
            {
                "text": "Given an input corpus w=(w (1) , \u2022 \u2022 \u2022 , w (n) ), we introduce a latent variable t=(t (1) , \u2022 \u2022 \u2022 , t (n) ) for trees generated by G for each sentence. The joint posterior distribution of t and \u03b8 conditioned on w is:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Gibbs sampling for PCFGs",
                "sec_num": "2"
            },
            {
                "text": "p(t, \u03b8 | w) \u221d p(\u03b8)p(w | t)p(t | \u03b8) = p(\u03b8)( n i=1 p(w (i) | t (i) )p(t (i) | \u03b8)) = p(\u03b8)( n i=1 p(w (i) | t (i) ) r\u2208R \u03b8 fr(t (i) r )) (1)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Gibbs sampling for PCFGs",
                "sec_num": "2"
            },
            {
                "text": "Here f r (t) is the number of occurrences of rule r in the derivation of t; p(w (i) | t (i) ) = 1 if the yield of t (i) is the sequence w (i) , and 0 otherwise.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Gibbs sampling for PCFGs",
                "sec_num": "2"
            },
            {
                "text": "We use a Dirichlet distribution parametrized by \u03b1 A : Dir(\u03b1 A ) as the prior of the probability distribution for all rules expanding non-terminal A (p(\u03b8 A )). The prior for all \u03b8, p(\u03b8), is the product of all Dirichlet distributions over all non-terminals A \u2208 N :",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Gibbs sampling for PCFGs",
                "sec_num": "2"
            },
            {
                "text": "p(\u03b8 | \u03b1) = A\u2208N p(\u03b8 A | \u03b1 A ).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Gibbs sampling for PCFGs",
                "sec_num": "2"
            },
            {
                "text": "Since the Dirichlet distribution is conjugate to the Multinomial distribution, which we use to model the likelihood of trees, the conditional posterior of \u03b8 A can be updated as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Gibbs sampling for PCFGs",
                "sec_num": "2"
            },
            {
                "text": "p G (\u03b8 | t, \u03b1) \u221d p G (t | \u03b8)p(\u03b8 | \u03b1) \u221d ( r\u2208R \u03b8 fr(t) r )( r\u2208R \u03b8 \u03b1r-1 r ) = r\u2208R \u03b8 fr(t)+\u03b1r-1 r (2)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Gibbs sampling for PCFGs",
                "sec_num": "2"
            },
            {
                "text": "which is still a Dirichlet distribution with updated parameter f r (t) + \u03b1 r for each rule r \u2208 R.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Gibbs sampling for PCFGs",
                "sec_num": "2"
            },
            {
                "text": "The parameters of the PCFG model can be learned from an annotated corpus by simply counting rules. However, parsing cannot be done directly with standard CKY as with standard PCFGs, so we use the Gibbs sampling algorithm presented in Johnson et al. (2007) . An additional motivation for using this algorithm is that Johnson et al. use it for learning without annotated structures, and in future work we seek to learn from fewer, and at times partial, annotations.",
                "cite_spans": [
                    {
                        "start": 234,
                        "end": 255,
                        "text": "Johnson et al. (2007)",
                        "ref_id": "BIBREF23"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Gibbs sampler",
                "sec_num": null
            },
            {
                "text": "An advantage of using Gibbs sampling for Bayesian inference, as opposed to other approximation algorithms such as Variational Bayesian inference (VB) and Collapsed Variational Bayesian inference (CVB), is that Markov Chain Monte Carlo (MCMC) algorithms are guaranteed to converge to a sample from the true posterior under appropriate conditions (Taddy, 2011) . Both VB and CVB converge to inaccurate and locally optimal solutions, like EM. In some models, CVB can achieve more accurate results due to weaker assumptions (Wang and Blunsom, 2013) . Another advantage of Gibbs sampling is that the sampler allows for parallel computation by allowing each sentence to be sampled entirely independently of the others. After each parallel sampling stage, all model parameters are updated in a single step, and the process then repeats (see \u00a72).",
                "cite_spans": [
                    {
                        "start": 345,
                        "end": 358,
                        "text": "(Taddy, 2011)",
                        "ref_id": "BIBREF40"
                    },
                    {
                        "start": 520,
                        "end": 544,
                        "text": "(Wang and Blunsom, 2013)",
                        "ref_id": "BIBREF41"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Gibbs sampler",
                "sec_num": null
            },
            {
                "text": "To sample the joint posterior p(t, \u03b8 | w), we sample production probabilities \u03b8 and then trees t from these conditional distributions:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Gibbs sampler",
                "sec_num": null
            },
            {
                "text": "p(t | \u03b8, w, \u03b1) = n i=1 p(t i | w i , \u03b8) (3) p(\u03b8 | t, w, \u03b1) = A\u2208N Dir(\u03b8 A | f A (t) + \u03b1 A ) (4)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Gibbs sampler",
                "sec_num": null
            },
            {
                "text": "Step 1: Sample Rule Probabilities. Given trees t and prior \u03b1, the production probabilities \u03b8 A for each nonterminal A\u2208N are sampled from a Dirichlet distribution with parameters f A (t) + \u03b1 A . f A (t) is a vector, and each component of f A (t), is the number of occurrences of one rule expanding nonterminal A.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Gibbs sampler",
                "sec_num": null
            },
            {
                "text": "Step 2: Sample Tree Structures. To sample trees from p(t i | w i , \u03b8), we use the efficient sampling scheme used in previous work (Goodman, 1998; Finkel et al., 2006; Johnson et al., 2007) . There are two parts to this algorithm. The first constructs an inside table as in the Inside-Outside algorithm for PCFGs (Lary and Young, 1990) . The second selects the tree by recursively sampling productions from top to bottom. ",
                "cite_spans": [
                    {
                        "start": 130,
                        "end": 145,
                        "text": "(Goodman, 1998;",
                        "ref_id": null
                    },
                    {
                        "start": 146,
                        "end": 166,
                        "text": "Finkel et al., 2006;",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 167,
                        "end": 188,
                        "text": "Johnson et al., 2007)",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 312,
                        "end": 334,
                        "text": "(Lary and Young, 1990)",
                        "ref_id": "BIBREF28"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Gibbs sampler",
                "sec_num": null
            },
            {
                "text": "i,k = (w i+1 , \u2022 \u2022 \u2022 , w k ).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Gibbs sampler",
                "sec_num": null
            },
            {
                "text": "Given \u03b8, we construct the inside table with entries p A,i,k for each nonterminal and each word span w i,k : 0 \u2264 i < k \u2264 l, where p A,i,k = P G A (w i,k |\u03b8) is the probability that words i through k were produced by the non-terminal A. The table is computed recursively by",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Gibbs sampler",
                "sec_num": null
            },
            {
                "text": "p A,k-1,k = \u03b8 A\u2192w k (5) p A,i,k = A\u2192BC\u2208R i<j<k \u03b8 A\u2192BC \u2022 p B,i,j \u2022 p C,j,k (6) for all A, B, C \u2208 N and 0 \u2264 i < j < k \u2264 l.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Gibbs sampler",
                "sec_num": null
            },
            {
                "text": "The resulting inside probabilities are then used to generate trees from the distribution of all valid trees of the sentence. The tree is generated from top to bottom recursively with the function T reeSampler defined in Algorithm 1.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Gibbs sampler",
                "sec_num": null
            },
            {
                "text": "In unsupervised PCFG learning, the rule probabilities can be resampled using the sampled trees, then used to reparse the corpus, and so on. We use this property to refine latent annotations for the PCFG-LA model described in the next section.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Gibbs sampler",
                "sec_num": null
            },
            {
                "text": "When labeled trees are available, rule frequencies can be directly extracted and used as priors for a PCFG. However, when learning PCFG-LAs, we must learn the fine-grained rules from the coarse trees, so we extend the Gibbs sampler to assign latent annotations to unannotated trees. The resulting learned PCFG-LA parser outputs samples of annotated trees so that we can obtain unannotated trees after marginalizing.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "PCFG with latent annotations",
                "sec_num": "3"
            },
            {
                "text": "With the PCFG-LA model (Matsuzaki et al., 2005; Petrov et al., 2006) fine-grained CFG rules are automatically induced from training, effectively providing a form of feature engineering without human intervention. Given H = {1, \u2022 \u2022 \u2022 , K}, a set of latent annotation symbols, and x \u2208 H:",
                "cite_spans": [
                    {
                        "start": 23,
                        "end": 47,
                        "text": "(Matsuzaki et al., 2005;",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 48,
                        "end": 68,
                        "text": "Petrov et al., 2006)",
                        "ref_id": "BIBREF36"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model",
                "sec_num": "3.1"
            },
            {
                "text": "\u2022 \u03b8 A[x]\u2192U is the probability of rule A[x] \u2192 U ,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model",
                "sec_num": "3.1"
            },
            {
                "text": "where U \u2208 N \u00d7 N \u222a T . The probabilities for all rules that expand the same annotated non-terminal must sum to one.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model",
                "sec_num": "3.1"
            },
            {
                "text": "\u2022 \u03b2 A[x],B,C\u2192y,z is the probability of assigning la- tent annotation y, z to child nodes B, C of A[x].",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model",
                "sec_num": "3.1"
            },
            {
                "text": "y,z\u2208H\u00d7H \u03b2 A[x],B,C\u2192y,z = 1. The inputs to the PCFG-LA are a CFG G with finite number of latent annotations for each non-terminal, an initial guess of probabilities of grammar rule \u03b8 0 , and a prior \u03b1 \u03b8 is learned from training.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model",
                "sec_num": "3.1"
            },
            {
                "text": "The joint posterior distribution of t and \u03b8, \u03b2 conditioned on w is:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model",
                "sec_num": "3.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "p(t, \u03b8, \u03b2 | w) \u221d p(\u03b8, \u03b2)p(w | t)p(t | \u03b8, \u03b2) = p(\u03b8)p(\u03b2)( n i=1 p(w i | t i )p(t i | \u03b8, \u03b2))",
                        "eq_num": "(7)"
                    }
                ],
                "section": "Model",
                "sec_num": "3.1"
            },
            {
                "text": "We assume that \u03b8 and \u03b2 are independent to get P (\u03b8, \u03b2) = P (\u03b8)P (\u03b2).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model",
                "sec_num": "3.1"
            },
            {
                "text": "To learn parameters \u03b8, \u03b2, we use a Dirichlet distribution as a prior for both \u03b8 and \u03b2. The distribution for all rules expanding A[x] is:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model",
                "sec_num": "3.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "P (\u03b8 | \u03b1 \u03b8 ) = A\u2208N,x\u2208H P (\u03b8 A[x] | \u03b1 \u03b8 A[x] )",
                        "eq_num": "(8)"
                    }
                ],
                "section": "Model",
                "sec_num": "3.1"
            },
            {
                "text": "The distribution for latent annotations associated with child nodes of A[x] \u2192 BC is:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model",
                "sec_num": "3.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "P (\u03b2 | \u03b1 \u03b2 ) = y,z\u2208H\u00d7H P (\u03b2 A[x],B,C | \u03b1 \u03b2 A[x],B,C ).",
                        "eq_num": "(9)"
                    }
                ],
                "section": "Model",
                "sec_num": "3.1"
            },
            {
                "text": "With this setting, the conditional posterior of \u03b8 A[x] and \u03b2 A[x],B,C can be updated, as in \u00a72. For all unary and binary rules r expanding A[x]:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model",
                "sec_num": "3.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "\u03b8 A[x] | t, \u03b1 \u03b8 \u223c Dir(f r (t) + \u03b1 \u03b8 r )",
                        "eq_num": "(10)"
                    }
                ],
                "section": "Model",
                "sec_num": "3.1"
            },
            {
                "text": "Here, f r (t) is the number of occurrence of annotated rule r in t. Also, for combination of latent annotations",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model",
                "sec_num": "3.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "y, z \u2208 H \u00d7 H assigned to B, C in rule A[x] \u2192 B, C: \u03b2 A[x],B,C | t, \u03b1 \u03b2 \u223c Dir(f d (t) + \u03b1 \u03b2 d )",
                        "eq_num": "(11)"
                    }
                ],
                "section": "Model",
                "sec_num": "3.1"
            },
            {
                "text": "Here, f d (t) is the number of occurrences of combination d in t.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model",
                "sec_num": "3.1"
            },
            {
                "text": "To learn from raw text, we extend the sampler in \u00a72 to PCFG-LA. Given priors \u03b1 \u03b8 , \u03b1 \u03b2 and raw text, the algorithm alternates between two steps. The first samples trees for the entire corpus; the second samples \u03b8 and \u03b2 from Dirichlet distributions with updated parameters, combining priors and counts from sampled trees. The algorithm then alternates between these steps until convergence. The outputs are samples of \u03b8, \u03b2 and annotated trees. The parsing process is specified in Algorithm 2. The first step assigns a tree to a sentence, say w 0,l . We first ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning PCFG-LAs from raw text",
                "sec_num": "3.2"
            },
            {
                "text": "Require: w 1 , \u2022 \u2022 \u2022 ,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning PCFG-LAs from raw text",
                "sec_num": "3.2"
            },
            {
                "text": "s , \u2022 \u2022 \u2022 , T (M ) s Find the mode of T (1) s , \u2022 \u2022 \u2022 , T (M ) s : T s end for return T 1 , \u2022 \u2022 \u2022 , T n end function",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning PCFG-LAs from raw text",
                "sec_num": "3.2"
            },
            {
                "text": "Algorithm 2: Parsing new sentences construct an inside table (see \u00a72). Each entry in the table stores the probability that a word span is produced by a given annotated nonterminal. For root node S, with \u03b8, \u03b2 and inside table p A[x],i,k , we sample one annotation based on all p S[x],0,l , x \u2208 H. Assume that we sampled x for S, we further sample a rule to expand S[x] and possible splits of the span w 0,l jointly. Assume that we sampled nonterminals B, C to expand S[x], where B is responsible for w 0,j and C is responsible for w j,l . We further sample annotations for B, C together, say y, z. Then we sample rules and split positions to expand B[y] and C[z], and continue until reaching the terminals.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning PCFG-LAs from raw text",
                "sec_num": "3.2"
            },
            {
                "text": "This algorithm alone could be used for unsupervised learning of PCFG-LA if we input a non-informed or weakly-informed prior \u03b1 \u03b8 and \u03b1 \u03b2 . With access to unannotated trees for training, we only need to assign latent annotations to them and then use the frequencies of these annotated rules as the prior when parsing. The details of training when trees are available are illustrated in \u00a73.3.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning PCFG-LAs from raw text",
                "sec_num": "3.2"
            },
            {
                "text": "Once we have trees (with latent annotations), the step of sampling \u03b8 and \u03b2 from a Dirichlet distribution is direct. We need to count the number of occurrences f r (t) for each rule r like A",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning PCFG-LAs from raw text",
                "sec_num": "3.2"
            },
            {
                "text": "[x] \u2192 U, U \u2208 N \u00d7 N \u222a T in updated annotated trees t, and draw \u03b8 A[x] from the updated Dirichlet distribution Dir(f A[x] (t) + \u03b1 \u03b8 A[x]",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning PCFG-LAs from raw text",
                "sec_num": "3.2"
            },
            {
                "text": "). We also need to count the number of occurrences of ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning PCFG-LAs from raw text",
                "sec_num": "3.2"
            },
            {
                "text": "f d (t) for each combination of yz \u2208 H \u00d7 H assigned to B, C given A[x] \u2192 B, C in t,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning PCFG-LAs from raw text",
                "sec_num": "3.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "p A[x],k-1,k = \u03b8 A[x]\u2192w k (12) p A[x],i,k = A[x]\u2192BC:BC\u2208N \u00d7N j:i<j<k yz\u2208H\u00d7H \u03b8 A[x]\u2192BC \u03b2 A[x]BC\u2192yz p B[y],i,j p C[z],j,k",
                        "eq_num": "(13)"
                    }
                ],
                "section": "Learning PCFG-LAs from raw text",
                "sec_num": "3.2"
            },
            {
                "text": "Sample the tree, top to bottom. First, from start symbol S, sample latent annotation from multinomial with probability \u03c0 S[x] p S[x],0,l for each x \u2208 H. Next, given annotated non-terminal A[x] and i, k, sample possible child nodes and split positions from multinomial with probability:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning PCFG-LAs from raw text",
                "sec_num": "3.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "p(B, C, j) = 1 p A[x],i,k \u2022 y,z\u2208H \u03b8 A[x]\u2192BC \u03b2 A[x]BC\u2192yz p B[y],i,j p C[z],j,k",
                        "eq_num": "(14)"
                    }
                ],
                "section": "Learning PCFG-LAs from raw text",
                "sec_num": "3.2"
            },
            {
                "text": "Here the probability is calculated by marginalizing all possible latent annotations for B, C, and",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning PCFG-LAs from raw text",
                "sec_num": "3.2"
            },
            {
                "text": "\u03b8 A[x]\u2192BC \u03b2 A[x]BC\u2192yz is the probability of choosing B[y], C[z] to expand A[x]",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning PCFG-LAs from raw text",
                "sec_num": "3.2"
            },
            {
                "text": ", and p B[y],i,j p C[z],j,k are the probabilities for B[y] and C[z] to be responsible for word span w i,j and w j,k respectively. And p A[x],i,k is the normalizing term. Third, given A[x], B, C, i, j, k, sample annotations for B, C from multinomial with probability:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning PCFG-LAs from raw text",
                "sec_num": "3.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "p(y, z) = \u03b2 A[x]BC\u2192yz p B[y],i,j p C[z],j,k y,z \u03b2 A[x]BC\u2192yz p B[y],i,j p C[z],j,k",
                        "eq_num": "(15)"
                    }
                ],
                "section": "Learning PCFG-LAs from raw text",
                "sec_num": "3.2"
            },
            {
                "text": "A crucial aspect of this procedure is that all trees can be sampled independently. This parallel process produces a substantial speed gain that is important particularly when using more latent annotations. After all trees have been sampled (independently), the counts from each individual tree are combined prior to the next sampling iteration.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning PCFG-LAs from raw text",
                "sec_num": "3.2"
            },
            {
                "text": "In training, we need to learn the probabilities of finegrained rules given coarsely-labeled trees. We perform Gibbs sampling on the training data by first iteratively sampling probabilities and then assigning annotations to tree nodes. We use the average counts of annotated production rules from sampled trees to produce the prior \u03b1 \u03b8 and \u03b1 \u03b2 incorporated into parsing raw sentences.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning from coarse training trees",
                "sec_num": "3.3"
            },
            {
                "text": "We first index the non-terminal nodes of each tree T by 1, 2, \u2022 \u2022 \u2022 from top to bottom, and left to right. Then the sampler iterates between two steps. The first samples \u03b8, \u03b2 given annotated trees (as in \u00a73.2). The second samples latent annotations for nonterminal nodes Require: T 1 , \u2022 \u2022 \u2022 , T n are fully parsed trees; \u03b8 0 , \u03b2 0 are initial values; \u03b1 \u03b80 , \u03b1 \u03b20 are priors; M is the number of iterations function ANNO(T 1 , \u2022 \u2022 \u2022 , T n , \u03b8 0 , \u03b2 0 , \u03b1 \u03b80 , \u03b1 \u03b20 , M ) for iteration i = 1 to M do for sentence s = 1 to n do Calculate inside probability Sample latent annotations for each node in the tree, get tree with latent annotations t (i) s end for Sample \u03b8 (i) , \u03b2 (i) end for return Mean of number of occurrences of production rules and associated latent annotations from all sampled annotated trees end function Algorithm 3: Learning prior from training in parsed trees, which also takes two steps. The first step is to, for each node in the tree, calculate and store the probability that the node is annotated by x. The second step is to jointly sample latent annotations for child nodes of root nodes, and then continue this process from top to bottom until reaching the pre-terminal nodes.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning from coarse training trees",
                "sec_num": "3.3"
            },
            {
                "text": "Step one: inside probabilities. Given tree T , compute b i T [x] for each non-terminal i recursively: 1. If node N i is a pre-terminal node above terminal symbol w, then for x\u2208H b",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning from coarse training trees",
                "sec_num": "3.3"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "i T [x] = \u03b8 Ni[x]\u2192w",
                        "eq_num": "(16)"
                    }
                ],
                "section": "Learning from coarse training trees",
                "sec_num": "3.3"
            },
            {
                "text": "2. Otherwise, let j, k be two child nodes of i, then for",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning from coarse training trees",
                "sec_num": "3.3"
            },
            {
                "text": "x \u2208 H b i T [x] = y,z\u2208H \u03b8 Ni[x]\u2192Nj N k \u03b2 Ni[x]Nj N k \u2192y,z b j T [y]b k T [z] (17)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning from coarse training trees",
                "sec_num": "3.3"
            },
            {
                "text": "Step two: outside sampling. Given inside probability b i T [x] for every non-terminal i and all latent annotations x\u2208H, we sample the latent annotations from top to bottom:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning from coarse training trees",
                "sec_num": "3.3"
            },
            {
                "text": "1. If node i is the root node (i = 1), then sample x \u2208 H from a multinomial distribution with",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning from coarse training trees",
                "sec_num": "3.3"
            },
            {
                "text": "f i T [x] = \u03c0(N i [x]).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning from coarse training trees",
                "sec_num": "3.3"
            },
            {
                "text": "N i [x] with children N j , N k , sample latent annotations for these two nodes from a multinomial distribution with",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "For a parent node with sampled latent annotation",
                "sec_num": "2."
            },
            {
                "text": "f i T [y, z] = 1 b i T [x] \u2022 \u03b8 Ni[x]\u2192Nj N k \u03b2 Ni[x]Nj N k \u2192y,z b j T [y]b k T [z] (18)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "For a parent node with sampled latent annotation",
                "sec_num": "2."
            },
            {
                "text": "After training, we take the average counts of sampled annotated rules and combinations of latent annotations as priors to parse raw sentences.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "For a parent node with sampled latent annotation",
                "sec_num": "2."
            },
            {
                "text": "Our goal is to understand parsing efficacy using sampling and latent annotations for low-resource languages, so we perform experiments on five languages with varying amount of training data. We compare our results to a number of previously established baselines. First, for all languages, we use both a standard unsmoothed PCFG and the Bikel parser, trained on the training corpus. Additionally, we compare to state-of-the-art results for both English and Chinese, which have an existing body of work in PCFGs using a Bayesian framework. For Chinese, we compare to Huang & Harper (2009) , using their results that only use the Chinese Treebank (CTB). For English, we compare to Liang et al. (2009) . Prior results for parsing the constituency version of the Italian data are available from Alicante et al. (2012) , but as they make use of a different version of the treebank including extra sentences, and additionally use the extensive functional tags present in the corpus, we do not directly compare our results to theirs.2 ",
                "cite_spans": [
                    {
                        "start": 565,
                        "end": 586,
                        "text": "Huang & Harper (2009)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 678,
                        "end": 697,
                        "text": "Liang et al. (2009)",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 790,
                        "end": 812,
                        "text": "Alicante et al. (2012)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments 1",
                "sec_num": "4"
            },
            {
                "text": "English (ENG) and Chinese (CHI) are the two main languages used for this work; they are commonly used in parser evaluation and have previous examples of statistical parsers using a Bayesian framework. And since we primarily are interested in parsing low-resource languages, we include results for Kinyarwanda (KIN) and Malagasy (MLG) as examples of languages without substantial existing treebanks. Finally, as a middleground language, we use Italian (ITL).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data",
                "sec_num": "4.1"
            },
            {
                "text": "For English, we use the Wall-Street Journal section of the Penn Treebank (WSJ) (Marcus et al., 1993) . The data split is sections 02-21 for training, section 22 for development, and section 23 for testing. For Chinese, the Chinese Treebank (CTB5) (Xue et al., 2005) was used. The data split is files 81-899 for training, files 41-80 for development, and files 1-40/900-931 for testing.",
                "cite_spans": [
                    {
                        "start": 79,
                        "end": 100,
                        "text": "(Marcus et al., 1993)",
                        "ref_id": "BIBREF31"
                    },
                    {
                        "start": 247,
                        "end": 265,
                        "text": "(Xue et al., 2005)",
                        "ref_id": "BIBREF42"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data",
                "sec_num": "4.1"
            },
            {
                "text": "The ITL data is from the Turin University Treebank (TUT) (Bosco et al., 2000) and consists of 2,860 Italian sentences from a variety of domains. It was split into training, development, and test sets with a 70/15/15 percentage split.",
                "cite_spans": [
                    {
                        "start": 57,
                        "end": 77,
                        "text": "(Bosco et al., 2000)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data",
                "sec_num": "4.1"
            },
            {
                "text": "The KIN texts are transcripts of testimonies by survivors of the Rwandan genocide provided by the Kigali Genocide Memorial Center, along with a few BBC news articles. The MLG texts are articles from the websites Lakroa and La Gazette and Malagasy Global Voices. Both datasets are described in Garrette and Baldridge (2013) . The KIN and MLG data is very small compared to ENG and CHI: the KIN dataset con-tains 677 sentences, while the MLG dataset has only 113. Also, we simulated a small training set for ENG data by using only section 02 of the WSJ for training.",
                "cite_spans": [
                    {
                        "start": 293,
                        "end": 322,
                        "text": "Garrette and Baldridge (2013)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data",
                "sec_num": "4.1"
            },
            {
                "text": "As a preprocessing step, all trees are converted into Chomsky Normal-Form such that all non-terminal productions are binary and all unary chains are removed.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Setup",
                "sec_num": "4.2"
            },
            {
                "text": "Additional standard normalization is performed. Functional tags (e.g. the SBJ part of NP-SBJ), empty nodes (traces), and indices are removed. Our binarization is simple: given a parent, select the rightmost child as the head and add a stand-in node that contains the remainder of the original children; the process then recurses. This simple technique uses no explicit headfinding rules, which eases cross-linguistic applicability.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Setup",
                "sec_num": "4.2"
            },
            {
                "text": "From this normalized data, we train latent PCFGs with K=1,2,4,8,16,32 (where K=1 is equivalent to the plain PCFG described in section 2).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Setup",
                "sec_num": "4.2"
            },
            {
                "text": "Unknown word handling. We use a similar unknown word handling procedure to Liang et al. (2009) . From our raw corpus we extract features associated with every word, these features include surrounding context words as well as substring suffix/prefix features. Using these features we produce fifty clusters using k-means. Then, as a pre-parsing step, we replace all words occurring less than five times with their cluster labelthis simulates unknown words for training. Finally, during evaluation, any word not seen in training was also replaced with its corresponding cluster label. This final step is simple because there are no 'unknown unknowns' in our corpus, as the clustering has been performed over the entire corpus prior to training. This approach is similar to methods for unsupervised POStag induction that also utilize clusters in this manner (Dasgupta & Ng, 2007) .",
                "cite_spans": [
                    {
                        "start": 75,
                        "end": 94,
                        "text": "Liang et al. (2009)",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 855,
                        "end": 876,
                        "text": "(Dasgupta & Ng, 2007)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Practical refinements",
                "sec_num": "4.3"
            },
            {
                "text": "We compare this unknown word handling method to one in which the clustering and a classifier is trained not on the corpus under consideration, but rather on a separate corpus of unrelated data. This comparison was made to understand the effects of including the evaluation set in the training data (without labels) versus training on out-of-domain texts. This is a more realistic measurement of out-of-the-box performance of a trained model.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Practical refinements",
                "sec_num": "4.3"
            },
            {
                "text": "Jump-starting sampling. In the basic setup, training high K-value models takes a prohibitively long time, so we also consider a jump-start technique that allows larger annotation values (such as K=16) to be run in less time. We train these high-K value models first on a highly reduced training set (5% of the full training set) for a large number of iterations, and then use the found \u03b8 values as the starting point for training on the full training set for a small number of iterations. Although many of the estimated parameters on the reduced set will be zero, the prior allows us to eventually recover this information in the full set. This allows us to train on the full training set, which is desirable relative to training on a reduced set, while still allowing the model sufficient iterations to burn in. The fact that we are likely starting in a fairly good position within the search space (due to estimating \u03b8 from the corpus) also likely helps enable these lower iteration counts.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Practical refinements",
                "sec_num": "4.3"
            },
            {
                "text": "We start with Tables 1 and 2 , which show performance when training on section 02 of the WSJ (pretending English is a \"low-resource\" language). The results show that the basic Gibbs PCFG (where K=1), with an Fscore of 61.0, substantially outperforms not only an unsmoothed PCFG (the simplest baseline), but also the Bikel parser (Bikel, 2004b) trained on the same amount of data. Table 1 also shows further large gains are obtained from using latent annotations-from 60.5 for K=1 to 78.7 for K=8. The Gibbs PCFG also compares quite favorably to the PCFG-LA of Liang et al. (2009) -slightly better for K=1 and K=2 and slightly worse for K=4 and K=8. Table 2 shows that the Gibbs PCFG is able to produce results with a smaller amount of variance relative to the Berkeley Parser, even at low training sizes. This trend is repeated in Table 3 , which shows that the Gibbs PCFG also produces less variance when training on different single sections of the WSJ relative to the Berkeley Parser, although it again produces slightly lower F1 scores.",
                "cite_spans": [
                    {
                        "start": 329,
                        "end": 343,
                        "text": "(Bikel, 2004b)",
                        "ref_id": null
                    },
                    {
                        "start": 560,
                        "end": 579,
                        "text": "Liang et al. (2009)",
                        "ref_id": "BIBREF29"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 21,
                        "end": 22,
                        "text": "1",
                        "ref_id": "TABREF3"
                    },
                    {
                        "start": 27,
                        "end": 28,
                        "text": "2",
                        "ref_id": "TABREF5"
                    },
                    {
                        "start": 386,
                        "end": 387,
                        "text": "1",
                        "ref_id": "TABREF3"
                    },
                    {
                        "start": 655,
                        "end": 656,
                        "text": "2",
                        "ref_id": "TABREF5"
                    },
                    {
                        "start": 837,
                        "end": 838,
                        "text": "3",
                        "ref_id": "TABREF6"
                    }
                ],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "5"
            },
            {
                "text": "We also use the small English corpus to determine the effects of weighting the prior when sampling annotations, varying \u03b1 between 0.1 and 10.0. Though performance is not sensitive to varying \u03b1 for larger corpora, Figure 1 shows it can make a substantial difference for smaller corpora (with an optimal value was obtained with an \u03b1 value of 5 for this small training set). This seems to indicate that the lower counts associated with the smaller training sets should be compensated for by weighting those counts more heavily when processing the evaluation set, as we had anticipated. 2 . The full ENG K=4 F-score is 86.2, so these results represent a slight step back. Nonetheless, the technique is still valuable in that it allows for inferring latent annotations for higher K-values than would typically be available to us in a reasonable timeframe.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 220,
                        "end": 221,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    },
                    {
                        "start": 583,
                        "end": 584,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "5"
            },
            {
                "text": "Table 4 shows the results for the main experiments. Sampling a vanilla PCFG (K=1) produces results that are not state-of-the-art, but still good overall and always better than an unsmoothed PCFG. The benefits of the latent annotations are further shown in the increase Wang and Blunsom (2013) obtain an ENG Fscore of 77.9% using collapsed VB for K=2. Though they do not give exact numbers, their Fig. 7 indicates an F-score of about 87% for K=16. of F1 score in all languages, as compared to the vanilla PCFG. Experiments were run up to K=32 primarily due to time constraint. Although previous literature results report increases up to the equivalent of K=64, it may be the case that higher K values with no merge step more easily lead to overfitting in our model -reducing the effectiveness of those high values, as shown by the overall poorer performance on several languages at K=32 when compared to K=16 as well as the general levelling-off seen at the high K values.",
                "cite_spans": [
                    {
                        "start": 269,
                        "end": 292,
                        "text": "Wang and Blunsom (2013)",
                        "ref_id": "BIBREF41"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "4",
                        "ref_id": "TABREF7"
                    },
                    {
                        "start": 401,
                        "end": 402,
                        "text": "7",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "5"
            },
            {
                "text": "For English and Chinese, the previous Bayesian framework parsers outperform our own, but only by around two points. Additionally, our parsing of Chinese improves on the Bikel parser (trained on our training data) despite the fact that the Bikel parser makes use of language specific optimizations. Our parser needs no changes to switch languages.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "5"
            },
            {
                "text": "The Gibbs PCFG with K=16 is superior to the strong Bikel and Berkeley Parser benchmarks for both KIN and MLG, a promising result for future work on parsing low-resource languages in general. Note also that our parser exhibits less variance than Berkeley Parser especially for KIN and MLG, which supports the fact that the variance of Berkeley Parser is higher for models with few subcategories (Petrov et al., 2006) .",
                "cite_spans": [
                    {
                        "start": 394,
                        "end": 415,
                        "text": "(Petrov et al., 2006)",
                        "ref_id": "BIBREF36"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "5"
            },
            {
                "text": "Examples of the improvement across latent annotations for a given tree are shown in Figure 3 . The details of the noun phrase 'no major bond offerings' were the same for each tree, and are thus abstracted here. The low K-value tree (K=2) is shown in 3a, and primarily suffers from issues related to the prepositional phrase, 'in Europe friday'. In particular, the low K-value tree incorrectly groups 'Europe friday' as a noun phrase object of 'in'.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 91,
                        "end": 92,
                        "text": "3",
                        "ref_id": "FIGREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "5"
            },
            {
                "text": "The higher K-value tree (K=8) is shown in 3b. This tree manages to correctly analyze the prepositional phrase, accurately separately the temporal locative 'Friday' from the actual prepositional phrase of 'in Europe'. However, the high K-value tree makes a different mistake that the low K-value tree did not; it groups 'no major bond offerings in Europe Friday' as a noun phrase, when it should be three separate phrases (two noun phrases and a prepositional phrase). This error may be related to the additional latent annotations. With more available noun phrase subtypes, it may be the case that a more unusual noun phrase could be permitted that would have been too low probability with only a few subtypes.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "5"
            },
            {
                "text": "To determine whether the substantial range in F1 scores across languages are primarily the result of the much larger training corpora available for certain languages, two extreme training set reduction experiments were conducted. The training sets for all languages were reduced to a total of either 100 or 500 sentences. This process was repeated 10 times in a crossvalidation setup, where 10 separate sets of sentences were selected for each language. The results of these experiments are shown in Table 5 .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 506,
                        "end": 507,
                        "text": "5",
                        "ref_id": "TABREF9"
                    }
                ],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "5"
            },
            {
                "text": "We conclude that while data availability is a major factor in the higher performance of English and Chinese in our original experiments, it is not the only issue. Clearly, either the linguistic facts of particular languages or perhaps choices of formalism and annotation conventions in the corpora make some of the languages more difficult to parse than others. The primary questions is why Gibbs-PCFG is able to achieve higher relative performance on the KIN/MLG datasets when compared to the other parsers, and why this advantage does not necessarily transfer to the extreme small-scale versions of the ENG/CHI/ITL data. Preliminary investigation into the properties of the corpora have revealed a number of potential answers. For instance, the POS tagsets for KIN/MLG are substantially reduced compared to the other corpora, and there are differences in the branching factor of the native versions of the corpora as well: a typical maximum branching factor for a tree in ENG/CHI/ITL is around 4-5, while for KIN/MLG it is almost always 2 (natively binary). Branching factors above 5 essentially never occur in KIN/MLG, while they are not rare in ENG/CHI/ITL. The question of exactly why the Gibbs-PCFG seems to perform well on these corpora remains an open question, but these differences could provide a starting point for future analysis.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "5"
            },
            {
                "text": "In addition to the actual F1 scores, the relative uniformity of the standard deviation results indicates that the individual parsers are not that much different in terms of their ability to provide consistent results at these small data extremes, as opposed to the slightly higher training levels where the Gibbs-PCFG generated smaller variances.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "5"
            },
            {
                "text": "Considering the effects of unknown word handling, Table 6 shows that using the evaluation set when creating the unknown word classifier does improve overall parsing accuracy when compared to an unknown word handler that is trained on out-of-domain texts. This shows that results reported in previous work somewhat overstate the accuracy of these parsers when used in the wild-which matters greatly in the low-resource setting.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 56,
                        "end": 57,
                        "text": "6",
                        "ref_id": "TABREF8"
                    }
                ],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "5"
            },
            {
                "text": "Our experiments demonstrate that sampling vanilla PCFGs, as well as PCFGs with latent annotations, is feasible with the use of a Gibbs sampler technique and produces results that are in line with previous parsers on controlled test sets. Our results also show that our methods are effective on a wide variety of languages-including two low-resource languageswith no language-specific model modifications needed.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "6"
            },
            {
                "text": "Additionally, although not a uniform winner, the Gibbs-PCFG shows a propensity for performing well on naturally small corpora (here, KIN/MLG). The exact reason for this remains slightly unclear, but the fact that a similar advantage is not found for extremely small versions of large corpora indicates that our approach may be particularly well-suited for application in real low-resource environments as opposed to a sim- Having established this procedure and its relative tolerance for low amounts of data, we would like to extend the model to make use of partial bracketing information instead of complete trees, perhaps in the form of Fragmentary Unlabeled Dependency Grammar annotations (Schneider et al., 2013) . This would allow the sampling procedure to potentially operate using corpora with lighter annotations than full trees, making initial annotation effort not quite as heavy and potentially increasing the amount of available data for low-resource languages. Additionally, using the expert partial annotations to help restrict the sample space could provide good gains in terms of training time.",
                "cite_spans": [
                    {
                        "start": 692,
                        "end": 716,
                        "text": "(Schneider et al., 2013)",
                        "ref_id": "BIBREF38"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "6"
            },
            {
                "text": "Code available at github.com/jmielens/gibbs-pcfg-2014, along with instructions for replicating experiments when possible",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "2 As part of a standardized pre-processing step, we strip functional tags, which makes a direct comparison to their results inappropriate.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "Supported by the U.S. Army Research Office under grant number W911NF-10-1-0533. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the view of the U.S. Army Research Office.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgments",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "A treebank-based study on the influence of Italian word order on parsing performance",
                "authors": [
                    {
                        "first": "Anita",
                        "middle": [],
                        "last": "Alicante",
                        "suffix": ""
                    },
                    {
                        "first": "Cristina",
                        "middle": [],
                        "last": "Bosco",
                        "suffix": ""
                    },
                    {
                        "first": "Anna",
                        "middle": [],
                        "last": "Corazza",
                        "suffix": ""
                    },
                    {
                        "first": "Alberto",
                        "middle": [],
                        "last": "Lavelli",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Proceedings of LREC'12",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Anita Alicante, Cristina Bosco, Anna Corazza, and Alberto Lavelli. 2012. A treebank-based study on the influence of Italian word order on pars- ing performance. In Nicoletta Calzolari (Con- ference Chair), Khalid Choukri, Thierry Declerck, Mehmet Uur Doan, Bente Maegaard, Joseph Mar- iani, Jan Odijk, and Stelios Piperidis, editors, Pro- ceedings of LREC'12, Istanbul, Turkey. European Language Resources Association (ELRA).",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "The Grammar Matrix: An Open-Source Starter-Kit for the Rapid Development of Cross-Linguistically Consistent Broad-Coverage Precision Grammars",
                "authors": [
                    {
                        "first": "Emily",
                        "middle": [
                            "M"
                        ],
                        "last": "Bender",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Flickinger",
                        "suffix": ""
                    },
                    {
                        "first": "Stephan",
                        "middle": [],
                        "last": "Oepen",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Proceedings of the Workshop on Grammar Engineering and Evaluation at the 19th International Conference on Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "8--14",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Emily M. Bender, Dan Flickinger, and Stephan Oepen. 2002. The Grammar Matrix: An Open-Source Starter-Kit for the Rapid Development of Cross- Linguistically Consistent Broad-Coverage Precision Grammars. In John Carroll, Nelleke Oostdijk, and Richard Sutcliffe, editors, Proceedings of the Work- shop on Grammar Engineering and Evaluation at the 19th International Conference on Computational Linguistics, pages 8-14, Taipei, Taiwan.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "On The Parameter Space of Generative Lexicalized Statistical Parsing Models",
                "authors": [
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Bikel",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dan Bikel. 2004a. On The Parameter Space of Gener- ative Lexicalized Statistical Parsing Models. Ph.D. thesis, University of Pennsylvania.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Intricacies of Collins' parsing model",
                "authors": [
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Daniel",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Bikel",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Computational Linguistics",
                "volume": "30",
                "issue": "4",
                "pages": "479--511",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Daniel M Bikel. 2004b. Intricacies of Collins' parsing model. Computational Linguistics, 30(4):479-511.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Towards history-based grammars: Using richer models for probabilistic parsing",
                "authors": [
                    {
                        "first": "Ezra",
                        "middle": [],
                        "last": "Black",
                        "suffix": ""
                    },
                    {
                        "first": "Fred",
                        "middle": [],
                        "last": "Jelinek",
                        "suffix": ""
                    },
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Lafferty",
                        "suffix": ""
                    },
                    {
                        "first": "Robert",
                        "middle": [],
                        "last": "David M Magerman",
                        "suffix": ""
                    },
                    {
                        "first": "Salim",
                        "middle": [],
                        "last": "Mercer",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Roukos",
                        "suffix": ""
                    }
                ],
                "year": 1992,
                "venue": "Proceedings of the workshop on Speech and Natural Language",
                "volume": "",
                "issue": "",
                "pages": "134--139",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ezra Black, Fred Jelinek, John Lafferty, David M Magerman, Robert Mercer, and Salim Roukos. 1992. Towards history-based grammars: Using richer models for probabilistic parsing. In Proceed- ings of the workshop on Speech and Natural Lan- guage, pages 134-139. Association for Computa- tional Linguistics.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Applying probability measures to abstract languages. Computers",
                "authors": [
                    {
                        "first": "L",
                        "middle": [],
                        "last": "Taylor",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [
                            "A"
                        ],
                        "last": "Booth",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Thompson",
                        "suffix": ""
                    }
                ],
                "year": 1973,
                "venue": "IEEE Transactions on",
                "volume": "100",
                "issue": "5",
                "pages": "442--450",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Taylor L Booth and Richard A Thompson. 1973. Ap- plying probability measures to abstract languages. Computers, IEEE Transactions on, 100(5):442-450.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Building a Treebank for Italian: a Data-driven Annotation Schema",
                "authors": [
                    {
                        "first": "Cristina",
                        "middle": [],
                        "last": "Bosco",
                        "suffix": ""
                    },
                    {
                        "first": "Vincenzo",
                        "middle": [],
                        "last": "Lombardo",
                        "suffix": ""
                    },
                    {
                        "first": "Daniela",
                        "middle": [],
                        "last": "Vassallo",
                        "suffix": ""
                    },
                    {
                        "first": "Leonardo",
                        "middle": [],
                        "last": "Lesmo",
                        "suffix": ""
                    }
                ],
                "year": 2000,
                "venue": "Proceedings of the Second International Conference on Language Resources and Evaluation LREC-2000",
                "volume": "",
                "issue": "",
                "pages": "99--105",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Cristina Bosco, Vincenzo Lombardo, Daniela Vassallo, and Leonardo Lesmo. 2000. Building a Treebank for Italian: a Data-driven Annotation Schema. In In Proceedings of the Second International Conference on Language Resources and Evaluation LREC-2000 (pp. 99, pages 99-105.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Two experiments on learning probabilistic dependency grammars from corpora",
                "authors": [
                    {
                        "first": "Glenn",
                        "middle": [],
                        "last": "Carroll",
                        "suffix": ""
                    },
                    {
                        "first": "Eugene",
                        "middle": [],
                        "last": "Charniak",
                        "suffix": ""
                    }
                ],
                "year": 1992,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Glenn Carroll and Eugene Charniak. 1992. Two exper- iments on learning probabilistic dependency gram- mars from corpora. Department of Computer Sci- ence, Univ.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Tree-bank grammars",
                "authors": [
                    {
                        "first": "Eugene",
                        "middle": [],
                        "last": "Charniak",
                        "suffix": ""
                    }
                ],
                "year": 1996,
                "venue": "Proceedings of the National Conference on Artificial Intelligence",
                "volume": "",
                "issue": "",
                "pages": "1031--1036",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Eugene Charniak. 1996. Tree-bank grammars. In Pro- ceedings of the National Conference on Artificial In- telligence, pages 1031-1036.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "A maximum-entropyinspired parser",
                "authors": [
                    {
                        "first": "Eugene",
                        "middle": [],
                        "last": "Charniak",
                        "suffix": ""
                    }
                ],
                "year": 2000,
                "venue": "Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference",
                "volume": "",
                "issue": "",
                "pages": "132--139",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Eugene Charniak. 2000. A maximum-entropy- inspired parser. In Proceedings of the 1st North American chapter of the Association for Computa- tional Linguistics conference, pages 132-139. Asso- ciation for Computational Linguistics.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Three models for the description of language",
                "authors": [
                    {
                        "first": "Noam",
                        "middle": [],
                        "last": "Chomsky",
                        "suffix": ""
                    }
                ],
                "year": 1956,
                "venue": "Information Theory, IRE Transactions on",
                "volume": "2",
                "issue": "3",
                "pages": "113--124",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Noam Chomsky. 1956. Three models for the descrip- tion of language. Information Theory, IRE Transac- tions on, 2(3):113-124.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Spectral learning of latent-variable PCFGs",
                "authors": [
                    {
                        "first": "Karl",
                        "middle": [],
                        "last": "Shay B Cohen",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Stratos",
                        "suffix": ""
                    },
                    {
                        "first": "Dean",
                        "middle": [
                            "P"
                        ],
                        "last": "Collins",
                        "suffix": ""
                    },
                    {
                        "first": "Lyle",
                        "middle": [],
                        "last": "Foster",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Ungar",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers",
                "volume": "1",
                "issue": "",
                "pages": "223--231",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Shay B Cohen, Karl Stratos, Michael Collins, Dean P Foster, and Lyle Ungar. 2012. Spectral learning of latent-variable PCFGs. In Proceedings of the 50th Annual Meeting of the Association for Compu- tational Linguistics: Long Papers-Volume 1, pages 223-231. Association for Computational Linguis- tics.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Experiments with spectral learning of latent-variable PCFGs",
                "authors": [
                    {
                        "first": "Karl",
                        "middle": [],
                        "last": "Shay B Cohen",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Stratos",
                        "suffix": ""
                    },
                    {
                        "first": "Dean",
                        "middle": [
                            "P"
                        ],
                        "last": "Collins",
                        "suffix": ""
                    },
                    {
                        "first": "Lyle",
                        "middle": [],
                        "last": "Foster",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Ungar",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of NAACL-HLT",
                "volume": "",
                "issue": "",
                "pages": "148--157",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Shay B Cohen, Karl Stratos, Michael Collins, Dean P Foster, and Lyle Ungar. 2013. Experiments with spectral learning of latent-variable PCFGs. In Pro- ceedings of NAACL-HLT, pages 148-157.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "A new statistical parser based on bigram lexical dependencies",
                "authors": [
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "John",
                        "suffix": ""
                    },
                    {
                        "first": "Collins",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 1996,
                "venue": "Proceedings of the 34th annual meeting on Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "184--191",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Michael John Collins. 1996. A new statistical parser based on bigram lexical dependencies. In Proceed- ings of the 34th annual meeting on Association for Computational Linguistics, pages 184-191. Associ- ation for Computational Linguistics.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Three generative, lexicalised models for statistical parsing",
                "authors": [
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Collins",
                        "suffix": ""
                    }
                ],
                "year": 1997,
                "venue": "Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "16--23",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Michael Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings of the 35th Annual Meeting of the Association for Com- putational Linguistics and Eighth Conference of the European Chapter of the Association for Compu- tational Linguistics, pages 16-23. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Head-driven statistical models for natural language parsing",
                "authors": [
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Collins",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Computational linguistics",
                "volume": "29",
                "issue": "4",
                "pages": "589--637",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Michael Collins. 2003. Head-driven statistical mod- els for natural language parsing. Computational lin- guistics, 29(4):589-637.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Solving the problem of cascading errors: Approximate Bayesian inference for linguistic annotation pipelines",
                "authors": [
                    {
                        "first": "Jenny",
                        "middle": [
                            "Rose"
                        ],
                        "last": "Finkel",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [
                            "Y"
                        ],
                        "last": "Ng",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "618--626",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jenny Rose Finkel, Christopher D Manning, and An- drew Y Ng. 2006. Solving the problem of cascading errors: Approximate Bayesian inference for linguis- tic annotation pipelines. In Proceedings of the 2006 Conference on Empirical Methods in Natural Lan- guage Processing, pages 618-626. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Learning a Part-of-Speech Tagger from Two Hours of Annotation",
                "authors": [
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Garrette",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Baldridge",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of NAACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dan Garrette and Jason Baldridge. 2013. Learning a Part-of-Speech Tagger from Two Hours of Annota- tion. In Proceedings of NAACL, Atlanta, Georgia.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Real-World Semi-Supervised Learning of POS-Taggers for Low-Resource Languages",
                "authors": [
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Garrette",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Mielens",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Baldridge",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of the 51th annual meeting on Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dan Garrette, Jason Mielens, and Jason Baldridge. 2013. Real-World Semi-Supervised Learning of POS-Taggers for Low-Resource Languages. In Pro- ceedings of the 51th annual meeting on Associa- tion for Computational Linguistics. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. Pattern Analysis and Machine Intelligence",
                "authors": [
                    {
                        "first": "Stuart",
                        "middle": [],
                        "last": "Geman",
                        "suffix": ""
                    },
                    {
                        "first": "Donald",
                        "middle": [],
                        "last": "Geman",
                        "suffix": ""
                    }
                ],
                "year": 1984,
                "venue": "IEEE Transactions on",
                "volume": "",
                "issue": "6",
                "pages": "721--741",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Stuart Geman and Donald Geman. 1984. Stochas- tic relaxation, Gibbs distributions, and the Bayesian restoration of images. Pattern Analysis and Machine Intelligence, IEEE Transactions on, (6):721-741.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Self-Training PCFG grammars with latent annotations across languages",
                "authors": [
                    {
                        "first": "Zhongqiang",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Mary",
                        "middle": [],
                        "last": "Harper",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing",
                "volume": "2",
                "issue": "",
                "pages": "832--841",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zhongqiang Huang and Mary Harper. 2009. Self- Training PCFG grammars with latent annotations across languages. In Proceedings of the 2009 Con- ference on Empirical Methods in Natural Language Processing: Volume 2-Volume 2, pages 832-841. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Breaking the Resource Bottleneck for Multilingual Parsing",
                "authors": [
                    {
                        "first": "Rebecca",
                        "middle": [],
                        "last": "Hwa",
                        "suffix": ""
                    },
                    {
                        "first": "Philip",
                        "middle": [],
                        "last": "Resnik",
                        "suffix": ""
                    },
                    {
                        "first": "Amy",
                        "middle": [],
                        "last": "Weinberg",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "The Proceedings of the Workshop on Linguistic Knowledge Acquisition and Representation: Bootstrapping Annotated Language Data. Conference on Language Resources and Evaluation",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Rebecca Hwa, Philip Resnik, and Amy Weinberg. Breaking the Resource Bottleneck for Multilingual Parsing. In The Proceedings of the Workshop on Lin- guistic Knowledge Acquisition and Representation: Bootstrapping Annotated Language Data. Confer- ence on Language Resources and Evaluation.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Bayesian inference for PCFGs via Markov Chain Monte Carlo",
                "authors": [
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Johnson",
                        "suffix": ""
                    },
                    {
                        "first": "Thomas",
                        "middle": [],
                        "last": "Griffiths",
                        "suffix": ""
                    },
                    {
                        "first": "Sharon",
                        "middle": [],
                        "last": "Goldwater",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference",
                "volume": "",
                "issue": "",
                "pages": "139--146",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mark Johnson, Thomas Griffiths, and Sharon Gold- water. 2007. Bayesian inference for PCFGs via Markov Chain Monte Carlo. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computa- tional Linguistics; Proceedings of the Main Confer- ence, pages 139-146.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "PCFG models of linguistic tree representations",
                "authors": [
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Johnson",
                        "suffix": ""
                    }
                ],
                "year": 1998,
                "venue": "Computational Linguistics",
                "volume": "24",
                "issue": "4",
                "pages": "613--632",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mark Johnson. 1998. PCFG models of linguis- tic tree representations. Computational Linguistics, 24(4):613-632.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Accurate unlexicalized parsing",
                "authors": [
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Klein",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Proceedings of the 41st Annual Meeting on Association for Computational Linguistics",
                "volume": "1",
                "issue": "",
                "pages": "423--430",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dan Klein and Christopher D Manning. 2003. Ac- curate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computa- tional Linguistics-Volume 1, pages 423-430. Asso- ciation for Computational Linguistics.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Applying computational linguistic techniques in a documentary project for Qanjobal (Mayan, Guatemala)",
                "authors": [
                    {
                        "first": "Jonas",
                        "middle": [],
                        "last": "Kuhn",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proceedings of LREC 2004",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jonas Kuhn. 2004a. Applying computational linguis- tic techniques in a documentary project for Qanjobal (Mayan, Guatemala). In In Proceedings of LREC 2004. Citeseer.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Experiments in parallel-text based grammar induction",
                "authors": [
                    {
                        "first": "Jonas",
                        "middle": [],
                        "last": "Kuhn",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics",
                "volume": "470",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jonas Kuhn. 2004b. Experiments in parallel-text based grammar induction. In Proceedings of the 42nd An- nual Meeting on Association for Computational Lin- guistics, page 470. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "The estimation of stochastic context-free grammars using the insideoutside algrithm",
                "authors": [
                    {
                        "first": "Karim",
                        "middle": [],
                        "last": "Lary",
                        "suffix": ""
                    },
                    {
                        "first": "Steve",
                        "middle": [
                            "J"
                        ],
                        "last": "Young",
                        "suffix": ""
                    }
                ],
                "year": 1990,
                "venue": "Computer, Speech and Language",
                "volume": "4",
                "issue": "",
                "pages": "35--56",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Karim Lary and Steve J Young. 1990. The estimation of stochastic context-free grammars using the inside- outside algrithm. Computer, Speech and Language, 4:35-56.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Probabilistic grammars and hierarchical Dirichlet processes. The handbook of applied Bayesian analysis",
                "authors": [
                    {
                        "first": "Percy",
                        "middle": [],
                        "last": "Liang",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [
                            "I"
                        ],
                        "last": "Jordan",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Klein",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Percy Liang, Michael I Jordan, and Dan Klein. 2009. Probabilistic grammars and hierarchical Dirichlet processes. The handbook of applied Bayesian anal- ysis.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "Statistical decision-tree models for parsing",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "David M Magerman",
                        "suffix": ""
                    }
                ],
                "year": 1995,
                "venue": "Proceedings of the 33rd annual meeting on Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "276--283",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "David M Magerman. 1995. Statistical decision-tree models for parsing. In Proceedings of the 33rd an- nual meeting on Association for Computational Lin- guistics, pages 276-283. Association for Computa- tional Linguistics.",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "Building a large annotated corpus of english: The penn treebank",
                "authors": [
                    {
                        "first": "Mitchell",
                        "middle": [
                            "P"
                        ],
                        "last": "Marcus",
                        "suffix": ""
                    },
                    {
                        "first": "Beatrice",
                        "middle": [],
                        "last": "Santorini",
                        "suffix": ""
                    },
                    {
                        "first": "Mary",
                        "middle": [
                            "Ann"
                        ],
                        "last": "Marcinkiewicz",
                        "suffix": ""
                    }
                ],
                "year": 1993,
                "venue": "COMPUTA-TIONAL LINGUISTICS",
                "volume": "19",
                "issue": "2",
                "pages": "313--330",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of english: The penn treebank. COMPUTA- TIONAL LINGUISTICS, 19(2):313-330.",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "Probabilistic CFG with latent annotations",
                "authors": [
                    {
                        "first": "Takuya",
                        "middle": [],
                        "last": "Matsuzaki",
                        "suffix": ""
                    },
                    {
                        "first": "Yusuke",
                        "middle": [],
                        "last": "Miyao",
                        "suffix": ""
                    },
                    {
                        "first": "Jun'ichi",
                        "middle": [],
                        "last": "Tsujii",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "75--82",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Takuya Matsuzaki, Yusuke Miyao, and Jun'ichi Tsu- jii. 2005. Probabilistic CFG with latent annotations. In Proceedings of the 43rd Annual Meeting on Asso- ciation for Computational Linguistics, pages 75-82. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "Insideoutside reestimation from partially bracketed corpora",
                "authors": [
                    {
                        "first": "Fernando",
                        "middle": [],
                        "last": "Pereira",
                        "suffix": ""
                    },
                    {
                        "first": "Yves",
                        "middle": [],
                        "last": "Schabes",
                        "suffix": ""
                    }
                ],
                "year": 1992,
                "venue": "Proceedings of the 30th annual meeting on Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "128--135",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Fernando Pereira and Yves Schabes. 1992. Inside- outside reestimation from partially bracketed cor- pora. In Proceedings of the 30th annual meeting on Association for Computational Linguistics, pages 128-135. Association for Computational Linguis- tics.",
                "links": null
            },
            "BIBREF34": {
                "ref_id": "b34",
                "title": "Improved Inference for Unlexicalized Parsing",
                "authors": [
                    {
                        "first": "Slav",
                        "middle": [],
                        "last": "Petrov",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Klein",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "HLT-NAACL",
                "volume": "",
                "issue": "",
                "pages": "404--411",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Slav Petrov and Dan Klein. 2007. Improved Inference for Unlexicalized Parsing. In HLT-NAACL, pages 404-411.",
                "links": null
            },
            "BIBREF35": {
                "ref_id": "b35",
                "title": "Sparse multi-scale grammars for discriminative latent variable parsing",
                "authors": [
                    {
                        "first": "Slav",
                        "middle": [],
                        "last": "Petrov",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Klein",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "867--876",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Slav Petrov and Dan Klein. 2008. Sparse multi-scale grammars for discriminative latent variable pars- ing. In Proceedings of the Conference on Empiri- cal Methods in Natural Language Processing, pages 867-876. Association for Computational Linguis- tics.",
                "links": null
            },
            "BIBREF36": {
                "ref_id": "b36",
                "title": "Learning accurate, compact, and interpretable tree annotation",
                "authors": [
                    {
                        "first": "Slav",
                        "middle": [],
                        "last": "Petrov",
                        "suffix": ""
                    },
                    {
                        "first": "Leon",
                        "middle": [],
                        "last": "Barrett",
                        "suffix": ""
                    },
                    {
                        "first": "Romain",
                        "middle": [],
                        "last": "Thibaux",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Klein",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "433--440",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the As- sociation for Computational Linguistics, pages 433- 440. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF37": {
                "ref_id": "b37",
                "title": "Simple Unsupervised Grammar Induction from Raw Text with Cascaded Finite State Models",
                "authors": [
                    {
                        "first": "Elias",
                        "middle": [],
                        "last": "Ponvert",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Baldridge",
                        "suffix": ""
                    },
                    {
                        "first": "Katrin",
                        "middle": [],
                        "last": "Erk",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "ACL",
                "volume": "",
                "issue": "",
                "pages": "1077--1086",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Elias Ponvert, Jason Baldridge, and Katrin Erk. 2011. Simple Unsupervised Grammar Induction from Raw Text with Cascaded Finite State Models. In ACL, pages 1077-1086.",
                "links": null
            },
            "BIBREF38": {
                "ref_id": "b38",
                "title": "A framework for (under) specifying dependency syntax without overloading annotators",
                "authors": [
                    {
                        "first": "Nathan",
                        "middle": [],
                        "last": "Schneider",
                        "suffix": ""
                    },
                    {
                        "first": "O'",
                        "middle": [],
                        "last": "Brendan",
                        "suffix": ""
                    },
                    {
                        "first": "Naomi",
                        "middle": [],
                        "last": "Connor",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Saphra",
                        "suffix": ""
                    },
                    {
                        "first": "Manaal",
                        "middle": [],
                        "last": "Bamman",
                        "suffix": ""
                    },
                    {
                        "first": "Noah",
                        "middle": [
                            "A"
                        ],
                        "last": "Faruqui",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Smith",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Dyer",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Baldridge",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1306.2091"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Nathan Schneider, Brendan O'Connor, Naomi Saphra, David Bamman, Manaal Faruqui, Noah A Smith, Chris Dyer, and Jason Baldridge. 2013. A framework for (under) specifying dependency syn- tax without overloading annotators. arXiv preprint arXiv:1306.2091.",
                "links": null
            },
            "BIBREF39": {
                "ref_id": "b39",
                "title": "Bayesian symbol-refined tree substitution grammars for syntactic parsing",
                "authors": [
                    {
                        "first": "Hiroyuki",
                        "middle": [],
                        "last": "Shindo",
                        "suffix": ""
                    },
                    {
                        "first": "Yusuke",
                        "middle": [],
                        "last": "Miyao",
                        "suffix": ""
                    },
                    {
                        "first": "Akinori",
                        "middle": [],
                        "last": "Fujino",
                        "suffix": ""
                    },
                    {
                        "first": "Masaaki",
                        "middle": [],
                        "last": "Nagata",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers",
                "volume": "1",
                "issue": "",
                "pages": "440--448",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Hiroyuki Shindo, Yusuke Miyao, Akinori Fujino, and Masaaki Nagata. 2012. Bayesian symbol-refined tree substitution grammars for syntactic parsing. In Proceedings of the 50th Annual Meeting of the Asso- ciation for Computational Linguistics: Long Papers- Volume 1, pages 440-448. Association for Computa- tional Linguistics.",
                "links": null
            },
            "BIBREF40": {
                "ref_id": "b40",
                "title": "On estimation and selection for topic models",
                "authors": [
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Matthew",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Taddy",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1109.4518"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Matthew A Taddy. 2011. On estimation and selection for topic models. arXiv preprint arXiv:1109.4518.",
                "links": null
            },
            "BIBREF41": {
                "ref_id": "b41",
                "title": "Collapsed Variational Bayesian Inference for PCFGs",
                "authors": [
                    {
                        "first": "Pengyu",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Phil",
                        "middle": [],
                        "last": "Blunsom",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of the Seventeenth Conference on Computational Natural Language Learning",
                "volume": "",
                "issue": "",
                "pages": "173--182",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Pengyu Wang and Phil Blunsom. 2013. Collapsed Variational Bayesian Inference for PCFGs. In Pro- ceedings of the Seventeenth Conference on Com- putational Natural Language Learning, pages 173- 182, Sofia, Bulgaria, August. Association for Com- putational Linguistics.",
                "links": null
            },
            "BIBREF42": {
                "ref_id": "b42",
                "title": "The Penn Chinese TreeBank: Phrase structure annotation of a large corpus",
                "authors": [
                    {
                        "first": "Naiwen",
                        "middle": [],
                        "last": "Xue",
                        "suffix": ""
                    },
                    {
                        "first": "Fei",
                        "middle": [],
                        "last": "Xia",
                        "suffix": ""
                    },
                    {
                        "first": "Fu-Dong",
                        "middle": [],
                        "last": "Chiou",
                        "suffix": ""
                    },
                    {
                        "first": "Marta",
                        "middle": [],
                        "last": "Palmer",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Nat. Lang. Eng",
                "volume": "11",
                "issue": "2",
                "pages": "207--238",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Naiwen Xue, Fei Xia, Fu-dong Chiou, and Marta Palmer. 2005. The Penn Chinese TreeBank: Phrase structure annotation of a large corpus. Nat. Lang. Eng., 11(2):207-238, June.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 1: Accuracy by varying \u03b1 levels for small English data.",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Figure 2: F-Score for K=4, varying full-set training iterations (with and without 100x jump start).",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "Figure 3: Examples of tree progression in the Gibbs PCFG with a) K=2, b) K=8, and c) gold tree.",
                "uris": null,
                "fig_num": "3",
                "type_str": "figure"
            },
            "TABREF1": {
                "content": "<table><tr><td/><td>(i) s</td></tr><tr><td>end for</td><td/></tr><tr><td colspan=\"2\">Sample \u03b8 (i) , \u03b2 (i)</td></tr><tr><td>end for</td><td/></tr><tr><td colspan=\"2\">for sentence s = 1 to n do</td></tr><tr><td colspan=\"2\">Marginalize the latent annotations to get</td></tr><tr><td>unannotated trees T</td><td>(1)</td></tr></table>",
                "type_str": "table",
                "text": "w n are raw sentences; \u03b8 0 , \u03b2 0 are initial values; \u03b1 \u03b8 , \u03b1 \u03b2 are priors; M is the number of iterations function PARSE(w 1 , .., w n , \u03b8 0 , \u03b2 0 , \u03b1 \u03b8 , \u03b1 \u03b2 , M )for iteration i = 1 to M do for sentence s = 1 to n do Calculate Inside TableSampletree nodes and associated latent annotations, get tree structure t",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table/>",
                "type_str": "table",
                "text": "and draw \u03b2 A[x],B,C from the updated Dirichlet distribution Dir(f , we construct a table with entries p A[x],i,k for each A\u2208N , x \u2208 H and 0 \u2264 i < k \u2264 l, where p A[x],i,k = P G A[x] (w i,k |\u03b8, \u03b2) is the probability that words i through k were produced by the annotated nonterminal A[x]. The table can be computed recursively, for all A \u2208 N , x \u2208 H, by",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table><tr><td>System</td><td colspan=\"5\">K=1 K=2 K=4 K=8 K=16</td></tr><tr><td colspan=\"2\">Unsmoothed PCFG 40.2</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Bikel Parser</td><td>57.9</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Liang et al. 07</td><td colspan=\"4\">60.5 71.1 77.2 79.2</td><td>78.2</td></tr><tr><td>Berkeley Parser</td><td colspan=\"4\">60.8 74.4 78.4 79.1</td><td>78.7</td></tr><tr><td>Gibbs PCFG</td><td colspan=\"4\">61.0 71.3 76.6 78.7</td><td>78.0</td></tr></table>",
                "type_str": "table",
                "text": "F1 scores for small English training data experiments. 'K' is the number of latent annotations -K=1 represents a vanilla, unannotated PCFG.",
                "html": null,
                "num": null
            },
            "TABREF5": {
                "content": "<table><tr><td>System</td><td>F1 / StDev</td></tr><tr><td colspan=\"2\">Berkeley Parser 77.5 \u00b1 2.1</td></tr><tr><td>Gibbs PCFG</td><td>77.0 \u00b1 1.4</td></tr></table>",
                "type_str": "table",
                "text": "F1 scores with standard deviation over ten runs of small training data, K=4.",
                "html": null,
                "num": null
            },
            "TABREF6": {
                "content": "<table/>",
                "type_str": "table",
                "text": "F1 scores with standard deviations over twenty runs, training on individual WSJ sections (02-21).",
                "html": null,
                "num": null
            },
            "TABREF7": {
                "content": "<table><tr><td>Condition</td><td colspan=\"5\">ENG CHI ITA KIN MLG</td></tr><tr><td>Unsmoothed PCFG</td><td colspan=\"4\">69.9 66.8 62.1 45.9</td><td>49.2</td></tr><tr><td>Liang et al. 07</td><td>87.1</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Huang &amp; Harper09</td><td>-</td><td>84.1</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Bikel Parser</td><td colspan=\"4\">86.9 81.1 74.5 55.7</td><td>49.5</td></tr><tr><td>Berkeley Parser</td><td colspan=\"4\">90.1 83.4 71.6 61.4</td><td>51.8</td></tr><tr><td>Gibbs PCFG,K=1</td><td colspan=\"4\">79.3 75.4 66.3 58.5</td><td>55.1</td></tr><tr><td>Gibbs PCFG,K=2</td><td colspan=\"4\">82.6 79.8 69.3 65.0</td><td>57.0</td></tr><tr><td>Gibbs PCFG,K=4</td><td colspan=\"4\">86.0 82.3 71.9 67.2</td><td>57.8</td></tr><tr><td>Gibbs PCFG,K=16</td><td colspan=\"4\">87.2 83.2 72.4 68.1</td><td>58.2</td></tr><tr><td>Gibbs PCFG,K=32</td><td colspan=\"4\">87.4 83.4 71.0 66.8</td><td>55.3</td></tr></table>",
                "type_str": "table",
                "text": "F1 scores for experiments on sampled PCFGs. Note that",
                "html": null,
                "num": null
            },
            "TABREF8": {
                "content": "<table><tr><td>Condition</td><td colspan=\"2\">In-Domain Out-of-Domain</td></tr><tr><td>Full English (K=4)</td><td>86.0</td><td>83.3</td></tr><tr><td>Small English (K=4)</td><td>76.6</td><td>75.7</td></tr><tr><td>Kinyarwanda (K=4)</td><td>67.2</td><td>65.1</td></tr><tr><td>Malagasy (K=4)</td><td>57.8</td><td>55.4</td></tr></table>",
                "type_str": "table",
                "text": "Effect of differing regimes for handling unknown words.",
                "html": null,
                "num": null
            },
            "TABREF9": {
                "content": "<table><tr><td>Parser</td><td>Size</td><td>ENG</td><td>CHI</td><td>ITL</td><td>KIN</td><td>MLG</td></tr><tr><td>Bikel</td><td colspan=\"3\">100 54.7 \u00b1 2.2 51.4 \u00b1 3.0</td><td>51 \u00b1 2.4</td><td colspan=\"2\">47.1 \u00b1 2.3 44.4 \u00b1 2.0</td></tr><tr><td colspan=\"6\">Berkeley 47.8 ulated environment. 100 55.2 \u00b1 2.6 53.9 \u00b1 2.9 50 \u00b1 2.8</td><td/></tr></table>",
                "type_str": "table",
                "text": "\u00b1 2.1 44.5 \u00b1 2.3 Gibbs-PCFG 100 54.5 \u00b1 2.0 51.7 \u00b1 2.4 49.5 \u00b1 3.6 50.3 \u00b1 2.3 45.8 \u00b1 1.8 Bikel 500 56.2 \u00b1 2.0 54.1 \u00b1 2.7 54.2 \u00b1 2.4 --Berkeley 500 58.9 \u00b1 2.2 56.4 \u00b1 2.7 52.5 \u00b1 2.7 --Gibbs-PCFG 500 58.1 \u00b1 2.0 55.7 \u00b1 2.3 51.1 \u00b1 3.2 --100/500 sentence training set results, including st.dev over 10 runs. KIN/MLG did not have enough data to run the 500 sentence version.",
                "html": null,
                "num": null
            }
        }
    }
}