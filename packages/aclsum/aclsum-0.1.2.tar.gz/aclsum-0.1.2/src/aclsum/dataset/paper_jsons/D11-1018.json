{
    "paper_id": "D11-1018",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:00:00.950131Z"
    },
    "title": "Inducing Sentence Structure from Parallel Corpora for Reordering",
    "authors": [
        {
            "first": "John",
            "middle": [],
            "last": "Denero",
            "suffix": "",
            "affiliation": {},
            "email": "denero@google.com"
        },
        {
            "first": "Jakob",
            "middle": [],
            "last": "Uszkoreit",
            "suffix": "",
            "affiliation": {},
            "email": "uszkoreit@google.com"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "When translating among languages that differ substantially in word order, machine translation (MT) systems benefit from syntactic preordering-an approach that uses features from a syntactic parse to permute source words into a target-language-like order. This paper presents a method for inducing parse trees automatically from a parallel corpus, instead of using a supervised parser trained on a treebank. These induced parses are used to preorder source sentences. We demonstrate that our induced parser is effective: it not only improves a state-of-the-art phrase-based system with integrated reordering, but also approaches the performance of a recent preordering method based on a supervised parser. These results show that the syntactic structure which is relevant to MT pre-ordering can be learned automatically from parallel text, thus establishing a new application for unsupervised grammar induction.",
    "pdf_parse": {
        "paper_id": "D11-1018",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "When translating among languages that differ substantially in word order, machine translation (MT) systems benefit from syntactic preordering-an approach that uses features from a syntactic parse to permute source words into a target-language-like order. This paper presents a method for inducing parse trees automatically from a parallel corpus, instead of using a supervised parser trained on a treebank. These induced parses are used to preorder source sentences. We demonstrate that our induced parser is effective: it not only improves a state-of-the-art phrase-based system with integrated reordering, but also approaches the performance of a recent preordering method based on a supervised parser. These results show that the syntactic structure which is relevant to MT pre-ordering can be learned automatically from parallel text, thus establishing a new application for unsupervised grammar induction.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Recent work in statistical machine translation (MT) has demonstrated the effectiveness of syntactic preordering: an approach that permutes source sentences into a target-like order as a pre-processing step, using features of a source-side syntactic parse (Collins et al., 2005; Xu et al., 2009) . Syntactic pre-ordering is particularly effective at applying structural transformations, such as the ordering change from a subject-verb-object (SVO) language like English to a subject-object-verb (SOV) language like Japanese. However, state-of-the-art pre-ordering methods require a supervised syntactic parser to provide structural information about each sentence. We propose a method that learns both a parsing model and a reordering model directly from a word-aligned parallel corpus. Our approach, which we call Structure Induction for Reordering (STIR), requires no syntactic annotations to train, but approaches the performance of a recent syntactic pre-ordering method in a large-scale English-Japanese MT system.",
                "cite_spans": [
                    {
                        "start": 255,
                        "end": 277,
                        "text": "(Collins et al., 2005;",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 278,
                        "end": 294,
                        "text": "Xu et al., 2009)",
                        "ref_id": "BIBREF32"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "STIR predicts a pre-ordering via two pipelined models: (1) parsing and (2) tree reordering. The first model induces a binary parse, which defines the space of possible reorderings. In particular, only trees that properly separate verbs from their object noun phrases will license an SVO to SOV transformation. The second model locally permutes this tree. Our approach resembles work with binary synchronous grammars (Wu, 1997) , but is distinct in its emphasis on monolingual parsing as a first phase, and in selecting reorderings without the aid of a target-side language model.",
                "cite_spans": [
                    {
                        "start": 416,
                        "end": 426,
                        "text": "(Wu, 1997)",
                        "ref_id": "BIBREF30"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The parsing model is trained to maximize the conditional likelihood of trees that license the reorderings implied by observed word alignments in a parallel corpus. This objective differs from those of previous grammar induction models, which typically focus on succinctly explaining the observed source language corpus via latent hierarchical structure (Pereira and Schabes, 1992; Klein and Manning, 2002) . Our convex objective allows us to train a feature-rich log-linear parsing model, even without supervised treebank data.",
                "cite_spans": [
                    {
                        "start": 353,
                        "end": 380,
                        "text": "(Pereira and Schabes, 1992;",
                        "ref_id": null
                    },
                    {
                        "start": 381,
                        "end": 405,
                        "text": "Klein and Manning, 2002)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Focusing on pre-ordering for MT leads to a new perspective on the canonical NLP task of grammar induction-one which marries the wide-spread scientific interest in unsupervised parsing models with a clear application and extrinsic evaluation methodology. To support this perspective, we highlight several avenues of future research throughout the paper. We evaluate STIR in a large-scale English-Japanese machine translation system. We measure how closely our predicted reorderings match those implied by hand-annotated word alignments. STIR approaches the performance of the state-of-the-art pre-ordering method described in Genzel (2010) , which learns reordering rules for supervised treebank parses. STIR gives a translation improvement of 3.84 BLEU over a standard phrase-based system with an integrated reordering model.",
                "cite_spans": [
                    {
                        "start": 625,
                        "end": 638,
                        "text": "Genzel (2010)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "STIR consists of two pipelined log-linear models for parsing and reordering, as well as a third model for inducing trees from parallel corpora, trees that serve to train the first two models. This section describes the domain and structure of each model, while Section 3 describes features and learning objectives.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Parsing and Reordering Models",
                "sec_num": "2"
            },
            {
                "text": "Figure 1 depicts the relationship between the three models. For each aligned sentence pair in a parallel corpus, the parallel parsing model selects a binary tree t over the source sentence, such that t licenses the reordering pattern implied by the word alignment (Section 2.2). The monolingual parsing model is trained to generate t without inspecting the alignments or target sentences (Section 2.3). The tree reordering model is trained to locally permute t to produce the target order (Section 2.4). In the context of an MT system, the monolingual parser and tree reorderer are applied in sequence to pre-order source sentences.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Parsing and Reordering Models",
                "sec_num": "2"
            },
            {
                "text": "Unlabeled binary trees are central to the STIR pipeline. We represent trees via their constituent spans. Let [k, ) denote a span of indices of a 0indexed word sequence e, where i \u2208 \u2022 The root span [0, n) \u2208 T \u222a N .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Unlabeled Binary Trees",
                "sec_num": "2.1"
            },
            {
                "text": "[k, ) if k \u2264 i < . [0, n) denotes",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Unlabeled Binary Trees",
                "sec_num": "2.1"
            },
            {
                "text": "\u2022 For each [k, ) \u2208 N , there exists exactly one m such that {[k, m), [m, )} \u2282 T \u222a N .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Unlabeled Binary Trees",
                "sec_num": "2.1"
            },
            {
                "text": "\u2022 Terminal spans T are disjoint, but cover [0, n).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Unlabeled Binary Trees",
                "sec_num": "2.1"
            },
            {
                "text": "These trees include multi-word terminal spans. It is often convenient to refer to a split non-terminal triple (k, m, ) that include a non-terminal span [k, ) and its split point m. We denote the set of these triples as",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Unlabeled Binary Trees",
                "sec_num": "2.1"
            },
            {
                "text": "N + = {(k, m, ) : {[k, ), [k, m), [m, )} \u2208 T \u222a N } .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Unlabeled Binary Trees",
                "sec_num": "2.1"
            },
            {
                "text": "The first step in the STIR pipeline is to select a binary parse of each source sentence in a parallel corpus, one which licenses the reordering implied by a word alignment. Let the triple (e, f , A) be an aligned sentence pair, where e and f are word sequences and A is a set of links (i, j) indicating that e i aligns to f j .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Parallel Parsing Model",
                "sec_num": "2.2"
            },
            {
                "text": "The set A provides ordering information over e. To simplify definitions below, we first adjust A to 194 ignore all unaligned words in f . A = {(i, c(j)) : (i, j) \u2208 A} c(j) = |{j : j < j \u2227 \u2203i such that (i, j ) \u2208 A}| . c(j) is the number of aligned words in f prior to position j. Next, we define a projection function:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Parallel Parsing Model",
                "sec_num": "2.2"
            },
            {
                "text": "\u03c8(i) = min j\u2208J i j, max j\u2208J i j + 1 J i = {j : (i, j) \u2208 A } ,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Parallel Parsing Model",
                "sec_num": "2.2"
            },
            {
                "text": "and let \u03c8(i) = \u2205 if e i is unaligned. We can extend this projection function to spans [k, ) of e via union:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Parallel Parsing Model",
                "sec_num": "2.2"
            },
            {
                "text": "\u03c8(k, ) = k\u2264i< \u03c8(i) .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Parallel Parsing Model",
                "sec_num": "2.2"
            },
            {
                "text": "We say that a span [k, ) aligns contiguously if",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Parallel Parsing Model",
                "sec_num": "2.2"
            },
            {
                "text": "\u2200(i, j) \u2208 A , j \u2208 \u03c8(k, ) implies i \u2208 [k, ) ,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Parallel Parsing Model",
                "sec_num": "2.2"
            },
            {
                "text": "which corresponds to the familiar definition that [k, ) is one side of an extractable phrase pair. Unaligned spans do not align contiguously.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Parallel Parsing Model",
                "sec_num": "2.2"
            },
            {
                "text": "Given this notion of projection, we can relate trees to alignments. A tree (T , N ) over e respects an alignment A if all [k, ) \u2208 T \u222a N align contiguously, and for every (k, m, ), the projections \u03c8(k, m) and \u03c8(m, ) are adjacent. Projections are adjacent if the left bound of one is the right bound of the other, or if either is empty.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Parallel Parsing Model",
                "sec_num": "2.2"
            },
            {
                "text": "The parallel parsing model is a linear model over trees that respect A , which factors over spans.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Parallel Parsing Model",
                "sec_num": "2.2"
            },
            {
                "text": "s(t) = [k, )\u2208T w T \u03c6 T (k, ) + (k,m, )\u2208N + w N \u03c6 N (k, m, )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Parallel Parsing Model",
                "sec_num": "2.2"
            },
            {
                "text": "where the weight vector w = (w T w N ) scores features \u03c6 T on terminal spans and \u03c6 N on non-terminal spans and their split points.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Parallel Parsing Model",
                "sec_num": "2.2"
            },
            {
                "text": "Exact inference under this model can be performed via a dynamic program that exploits the following recurrence. Let s(k, ) be the score of the highest scoring binary tree over the span [k, ) that respects A . Then,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Parallel Parsing Model",
                "sec_num": "2.2"
            },
            {
                "text": "s T (k, ) = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 w T \u03c6 T (k, ) if [k, ) aligns contiguously -\u221e otherwise f (k, m, ) = s(k, m) + s(m, ) + w N \u03c6 N (k, m, ) s N (k, ) = max m:k<m< \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 f (k, m, ) if \u03c8(k, m) is adjacent to \u03c8(m, ) -\u221e otherwise s(k, ) = max [s T (k, ), s N (k, )]",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Parallel Parsing Model",
                "sec_num": "2.2"
            },
            {
                "text": "Above, s T scores terminal spans while filtering out those which are not contiguous. The function f scores non-terminal spans by the sum of their child scores and additional features \u03c6 N of the parent span. The recursive function s N maximizes over split points while filtering out non-adjacent children.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Parallel Parsing Model",
                "sec_num": "2.2"
            },
            {
                "text": "The recurrence will assign a score of -\u221e to any tree that does not respect A . Section 3 describes the features of this model. s(k, ) can be computed efficiently using the CKY algorithm.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Parallel Parsing Model",
                "sec_num": "2.2"
            },
            {
                "text": "The monolingual parsing model is trained to select the same trees as the parallel model, but without any features or constraints that reference word alignments. Hence, it can be applied to a source sentence before its translation is known. This model also scores untyped binary trees according to a linear model parameterized by some w = (w T w N ) that weights features on terminal and non-terminal spans, respectively. We impose a maximum terminal length of L, but otherwise allow any binary tree. The score s(k, ) of the maximal tree over a span [k, ) satisfies the familiar recurrence:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Monolingual Parsing Model",
                "sec_num": "2.3"
            },
            {
                "text": "s M (k, ) = w T \u03c6 T (k, ) if -k \u2264 L -\u221e otherwise s(k, ) = max s L (k, ), max m:k<m< f (k, m, )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Monolingual Parsing Model",
                "sec_num": "2.3"
            },
            {
                "text": "Inference under this recurrence can also be performed using the CKY algorithm. Section 3 describes the feature functions and training method.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Monolingual Parsing Model",
                "sec_num": "2.3"
            },
            {
                "text": "Given a binary tree (T , N ) over a sentence e, we can reorder e by (a) permuting the children of nonterminals and (b) permuting the words of terminal spans. Formally, a reordering r assigns each terminal [k, ) \u2208 T a permutation \u03c3(k, ) of its words and each split non-terminal (k, m, ) a permutation b(k, m, ) of its subspans, which can be either monotone or inverted, in the case of a binary tree. The permutation \u03c3(k, ) of a non-terminal span [k, ) / \u2208 T is defined recursively as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Tree Reordering Model",
                "sec_num": "2.4"
            },
            {
                "text": "\u03c3(k, m) \u03c3(m, ) if b(k, m, ) is monotone \u03c3(m, ) \u03c3(k, m) if b(k, m, ) is inverted",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Tree Reordering Model",
                "sec_num": "2.4"
            },
            {
                "text": "In this paper, we use a reordering model that selects each terminal \u03c3(k, ) and each split nonterminal b(k, m, ) independently, conditioned on the sentence e. While the sub-problems of choosing \u03c3(k, ) and b(k, m, ) are formally similar, we consider and evaluate them separately because the former deals only with local reordering, while the latter involves long-distance structural reordering.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Tree Reordering Model",
                "sec_num": "2.4"
            },
            {
                "text": "Because our trees are binary, selecting b(k, m, ) is a binary classification problem. Selecting \u03c3(k, ) for a terminal is a multiclass prediction problem that chooses among the (k)! permutations of terminal [k, ). Development experiments in English-Japanese yielded the best results with a maximum terminal span length L = 2. Hence, in experiments, terminal reordering is also binary classification.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Tree Reordering Model",
                "sec_num": "2.4"
            },
            {
                "text": "Because each permutation is independent of all the others, reordering inference via a single pass through the tree is optimal. However, a more complex search procedure would be necessary to maintain optimality if the decision of b(k, m, ) referenced other permutations, such as \u03c3([k, m)) or \u03c3([m, )). Coupling together inference in this way represents a possible area of future study.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Tree Reordering Model",
                "sec_num": "2.4"
            },
            {
                "text": "Each of these linear models factors over features on either terminal spans [k, ) or split non-terminals (k, m, ). Features vary in concert with the learning objectives and search spaces of each model. Figure 2 shows an example sentence from our development corpus, including the target (Japanese) On the other hand, the induced parse may only condition on the source sentence. The induced order is restricted by the induced parse. In this example, the induced order is incorrect because the subject and verb form a constituent in the induced parse that cannot be separated correctly by the reordering model. This example demonstrates the important role of the induced parser in the STIR pipeline.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 208,
                        "end": 209,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Features and Training Objectives",
                "sec_num": "3"
            },
            {
                "text": "sentence, alignment, projections, parallel parser prediction, monolingual parser prediction, and predicted permutation. The feature descriptions below reference this example.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Features and Training Objectives",
                "sec_num": "3"
            },
            {
                "text": "The tree reordering model consists of two local classifiers: the first can invert the two children of a non-terminal span, while the second can permute the words of a terminal span. The non-terminal classifier is trained on the trees that are selected by the parallel parsing model; the weights are chosen to minimize log loss of the correct permutation of each span (i.e., a maximum entropy model).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Tree Reordering Features",
                "sec_num": "3.1"
            },
            {
                "text": "The terminal model is a multi-class maximum entropy model over the n! possible permutations of the words in a terminal span. To make reordering more robust to monolingual parsing errors, the terminal 196 model is trained on all contiguous spans of each sentence up to length L, not just the terminal spans included in the parallel parsing tree.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Tree Reordering Features",
                "sec_num": "3.1"
            },
            {
                "text": "The feature templates we apply to each span can be divided into the following five categories. Most features are shared across the two models.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Tree Reordering Features",
                "sec_num": "3.1"
            },
            {
                "text": "Statistics. From a large aligned parallel corpus, we compute two statistics.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Tree Reordering Features",
                "sec_num": "3.1"
            },
            {
                "text": "\u2022 P C (e) = count(e aligns contiguously) count(e)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Tree Reordering Features",
                "sec_num": "3.1"
            },
            {
                "text": "is the fraction of the time that a phrase e aligns contiguously to some target phrase, for all phrases up to length 4. (Petrov et al., 2011) . Features based on these tags are computed identically to the features based on word classes.",
                "cite_spans": [
                    {
                        "start": 119,
                        "end": 140,
                        "text": "(Petrov et al., 2011)",
                        "ref_id": "BIBREF23"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Tree Reordering Features",
                "sec_num": "3.1"
            },
            {
                "text": "Lexical. For a list of very common words in the source language, we include lexical indicator features for the boundary words e k and e -1 . For instance, the word \"to\" triggers a reordering, as do prepositions in general.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Tree Reordering Features",
                "sec_num": "3.1"
            },
            {
                "text": "Length. Length computed ask, length as a fraction of sentence length, and quantized length features all contribute structural information.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Tree Reordering Features",
                "sec_num": "3.1"
            },
            {
                "text": "All features except POS are computed directly from aligned parallel corpora. The Cluster and POS features play a similar role of expressing reordering patterns over collections of similar words. The ablation study in Section 5 compares these two feature sets directly.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Tree Reordering Features",
                "sec_num": "3.1"
            },
            {
                "text": "The monolingual parsing model is also trained discriminatively, but involves structured prediction, as in a conditional random field (Lafferty et al., 2001) . Conditional likelihood objectives have proven effective for supervised parsers (Finkel et al., 2008; Petrov and Klein, 2008) . Recall that the score of a tree t = (T , N ) factors over spans.",
                "cite_spans": [
                    {
                        "start": 133,
                        "end": 156,
                        "text": "(Lafferty et al., 2001)",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 238,
                        "end": 259,
                        "text": "(Finkel et al., 2008;",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 260,
                        "end": 283,
                        "text": "Petrov and Klein, 2008)",
                        "ref_id": "BIBREF22"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Monolingual Parsing Features",
                "sec_num": "3.2"
            },
            {
                "text": "s(t) = [k, )\u2208T w T \u03c6 T (k, ) + [k, )\u2208N w N \u03c6 N (k, m, ) P(t|e) = exp [s(t)] (t )\u2208B(e) exp [s(t )]",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Monolingual Parsing Features",
                "sec_num": "3.2"
            },
            {
                "text": "where B(e) is the set of well-formed trees over e.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Monolingual Parsing Features",
                "sec_num": "3.2"
            },
            {
                "text": "The parallel parsing model (Section 2.2) produces a tree over the source sentence of each aligned sentence pair; these trees serve as our training examples. We can maximize their conditional likelihood according to this model via gradient methods. Each tree t over sentence e has a cumulative feature vector of dimension |w| = |w T |+|w N |, formed by stacking the terminal and non-terminal vectors:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Monolingual Parsing Features",
                "sec_num": "3.2"
            },
            {
                "text": "\u03c6(t, e) = \uf8eb \uf8ed [k, )\u2208T \u03c6 T (k, ) [k, )\u2208N \u03c6 N (k, m, ) \uf8f6 \uf8f8",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Monolingual Parsing Features",
                "sec_num": "3.2"
            },
            {
                "text": "The contribution to the gradient objective from a tree t for a sentence e is the difference between observed and expected feature vectors.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Monolingual Parsing Features",
                "sec_num": "3.2"
            },
            {
                "text": "(t,e) log P(t|e)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "L(w) =",
                "sec_num": null
            },
            {
                "text": "\u2207L(w) = (t,e) \uf8ee \uf8f0 \u03c6(t, e) - t \u2208B(e) P(t |e) \u2022 \u03c6(t , e) \uf8f9 \uf8fb",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "L(w) =",
                "sec_num": null
            },
            {
                "text": "The second term in the gradient-the expected feature vector-can be computed efficiently because the feature vector \u03c6(t ) decomposes over the spans of t . In particular, the inside-outside algorithm provides the quantities needed to compute the posterior probability of each terminal span [k, ) and each split non-terminal (k, m, ). Let, \u03b1(k, ) and \u03b2(k, ) be the outside and inside scores of a span, respectively, computed using a log-sum semiring. Then, the log probablility that a terminal span [k, ) appears in the tree for e under the posterior distribution P(t|e) is \u03b1(k, ) + w T \u03c6 T (k, ) . Note that this terminal posterior does not include the inside score of the span.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "L(w) =",
                "sec_num": null
            },
            {
                "text": "The log probability that a non-terminal span [k, ) appears with split point m is",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "L(w) =",
                "sec_num": null
            },
            {
                "text": "\u03b1(k, ) + \u03b2(k, m) + \u03b2(m, ) + w N \u03c6 N (k, m, )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "L(w) =",
                "sec_num": null
            },
            {
                "text": "By the linearity of expectations, the expected feature vector for e can be computed by averaging the feature vectors of each terminal and split non-terminal span, weighted by their posterior probabilities.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "L(w) =",
                "sec_num": null
            },
            {
                "text": "In future work, one may consider training this model to maximize the likelihood of an entire forest of trees, in order to maintain uncertainty over which tree licensed a particular alignment.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "L(w) =",
                "sec_num": null
            },
            {
                "text": "We are currently using l-BFGS to optimize this objective over a relatively small training corpus, for 35 iterations. For this reason, we only include lexical features for very common words. Distributed or online training algorithms would perhaps allow for more training data (and therefore more lexicalized features) to be used in the future.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "L(w) =",
                "sec_num": null
            },
            {
                "text": "The features of this parsing model share the same types as the tree reordering models, but vary in their definition. The differences stem primarily from the different purpose of the model: here, features are not meant to decide how to reorder the sentence, but instead how to bracket the sentence hierarchically so that it can be reordered.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "L(w) =",
                "sec_num": null
            },
            {
                "text": "In particular, terminal spans have features on the sequence of POS tags and word clusters they contain, while a split non-terminal (k, m, ) is scored based on the tags/clusters of the following words and word pairs: e k , e m-1 , e m , e -1 , (e k , e m ), (e k , e -1 ), and (e m-1 , e m ). The head word of a constituent often appears at one of its boundary positions, and so these features provide a proxy for explicitly tracking constituent heads in a parser.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "L(w) =",
                "sec_num": null
            },
            {
                "text": "Context features also appear, inspired by the constituent-context model of Klein and Manning (2001) . For a span [k, ), we add indicator features on the POS tags and word clusters of the words (e k-1 , e ) which directly surround the constituent.",
                "cite_spans": [
                    {
                        "start": 75,
                        "end": 99,
                        "text": "Klein and Manning (2001)",
                        "ref_id": "BIBREF10"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "L(w) =",
                "sec_num": null
            },
            {
                "text": "Features based on the statistic P C (e) are also scored in the parsing model on all spans of length up to 4.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "L(w) =",
                "sec_num": null
            },
            {
                "text": "Length features score various structural aspects of each non-terminal (k, m, ), such as m-k -k , m-k k-m , etc. One particularly interesting direction for future work is to train a single parsing model that licenses the reordering for several different languages. We might expect that a reasonable syntactic bracketing of English would simultaneously license the head-final transformations necessary to produce a Japanese or Korean ordering, and also the verbsubject-object ordering of formal Arabic.1 ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "L(w) =",
                "sec_num": null
            },
            {
                "text": "The parallel parsing model does not run at translation time, but instead provides training examples to the other two models. Hence, defining an appropriate learning objective for this model is more challenging.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Parallel Parsing Features",
                "sec_num": "3.3"
            },
            {
                "text": "In the end, we are interested in selecting trees that we can learn to reproduce without an alignment (via the monolingual parsing model) and which can be reordered reliably (via the tree reordering model). Note that by construction, any tree selected by the parallel parsing model can be reordered perfectly. However, some of those trees will be easier to reproduce and reorder than others.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Parallel Parsing Features",
                "sec_num": "3.3"
            },
            {
                "text": "In order to measure the effectiveness of a reordering pipeline, we would like a metric over permutations. Fortunately, permutation loss for machine translation is already an established component of the METEOR metric, called a fragmentation penalty (Lavie and Agarwal, 2007) . We define a slight variant of METEOR's fragmentation penalty that ranges from 0 to 1.",
                "cite_spans": [
                    {
                        "start": 249,
                        "end": 274,
                        "text": "(Lavie and Agarwal, 2007)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Reordering Loss Function",
                "sec_num": "3.3.1"
            },
            {
                "text": "Given a sentence e, a reference permutation \u03c3 * of (0, \u2022 \u2022 \u2022 , |e| -1), and a hypothesized permutation \u03c3, let chunks(\u03c3, \u03c3 * ) be the minimum number of \"chunks\" in \u03c3: the number of elements in a partition of \u03c3 such that each contiguous subsequence is also contiguous in \u03c3 * .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Reordering Loss Function",
                "sec_num": "3.3.1"
            },
            {
                "text": "We can define the reordering score between two permutations in terms of chunks.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Reordering Loss Function",
                "sec_num": "3.3.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "R(\u03c3, \u03c3 * ) = |\u03c3 * | -chunks(\u03c3, \u03c3 * ) |\u03c3 * | -1",
                        "eq_num": "(1)"
                    }
                ],
                "section": "Reordering Loss Function",
                "sec_num": "3.3.1"
            },
            {
                "text": "If \u03c3 = \u03c3 * , then chunks(\u03c3, \u03c3 * ) = 1. If no two adjacent elements of \u03c3 are adjacent in \u03c3 * , then chunks(\u03c3, \u03c3 * ) = |\u03c3|. Hence, the metric defined by Equation 1 ranges from 0 to 1.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Reordering Loss Function",
                "sec_num": "3.3.1"
            },
            {
                "text": "The reference permutation \u03c3 * of a source sentence e can be defined from an aligned sentence pair (e, f , A) by sorting the words e i of e by the left bound of their projection \u03c8(i). Null-aligned words are placed to the left of the next aligned word to their right in the original order.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Reordering Loss Function",
                "sec_num": "3.3.1"
            },
            {
                "text": "The reordering-specific loss functions defined in Equation 1 has been shown to correlate with human judgements of translation quality, especially for language pairs with substantial reordering like English-Japanese (Talbot et al., 2011) . Other reorderingspecific loss functions also correlate with human judgements (Birch et al., 2010) . Future research could experiment with alternative reordering-based loss functions, such as Kendall's Tau, as suggested by Birch and Osborne (2011) .",
                "cite_spans": [
                    {
                        "start": 215,
                        "end": 236,
                        "text": "(Talbot et al., 2011)",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 316,
                        "end": 336,
                        "text": "(Birch et al., 2010)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 461,
                        "end": 485,
                        "text": "Birch and Osborne (2011)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Reordering Loss Function",
                "sec_num": "3.3.1"
            },
            {
                "text": "We can train our reordering pipeline by dividing an aligned parallel corpus into two halves, A and B, where the monolingual parsing and tree reordering models are trained on A, and their effectiveness is evaluated on held-out set B. Then, the effectiveness of the parallel parsing model is best measured on B, given fully trained parsing and reordering models.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Parallel Parsing Objective",
                "sec_num": "3.3.2"
            },
            {
                "text": "(e,\u03c3 * )\u2208B R \u03c3 arg max t\u2208B(e) [w \u2022 \u03c6(t)] , \u03c3 * (2)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Parallel Parsing Objective",
                "sec_num": "3.3.2"
            },
            {
                "text": "Evaluating this objective involves training the other two models. Therefore, we can only hope to optimize this objective directly over a small dimensional space, for instance using a grid search. For this reason, we currently only include 4 features in the parallel parsing model for a tree t:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Parallel Parsing Objective",
                "sec_num": "3.3.2"
            },
            {
                "text": "1. The sum of log P C (e) for all terminals e in t with length greater than 1.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Parallel Parsing Objective",
                "sec_num": "3.3.2"
            },
            {
                "text": "2. The count of length-1 terminal spans in t.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Parallel Parsing Objective",
                "sec_num": "3.3.2"
            },
            {
                "text": "3. The count of terminals of length greater than k.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Parallel Parsing Objective",
                "sec_num": "3.3.2"
            },
            {
                "text": "4. An indicator feature of whether parentheses and brackets are balanced in each span.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Parallel Parsing Objective",
                "sec_num": "3.3.2"
            },
            {
                "text": "The model weights of features 3 and 4 above are fixed to large negative constants to prefer terminal spans of length up to k and spans with balanced punctuation. The weight of feature 1 is fixed to 1, and weight 2 was set via line search to 0.3. Ties among trees were broken randomly.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Parallel Parsing Objective",
                "sec_num": "3.3.2"
            },
            {
                "text": "Of course, the problem of selecting training trees need not be directly tied to the end task of reordering, as in Equation 2. Instead, we might consider selecting trees according to a likelihood objective on the source side of a parallel corpus, similar to how monolingual grammar induction models often optimize corpus likelihood. In such a case, we could imagine training models with far more parameters, but we leave this research direction to future work.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Parallel Parsing Objective",
                "sec_num": "3.3.2"
            },
            {
                "text": "Our approach to inducing hierarchical structure for pre-ordering relates to several areas of previous work, including other pre-ordering methods, reordering models more generally, and models for the unsupervised induction of syntactic structure.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "4"
            },
            {
                "text": "Our reordering pipeline is intentionally similar to approaches that use a treebank-trained supervised parser to reorder source sentences at training and translation time (Xia and McCord, 2004; Collins et al., 2005; Lee et al., 2010) . Given a supervised parser, a rule-based pre-ordering procedure can either be specified by hand (Xu et al., 2009) or learned automatically (Genzel, 2010) . We consider our approach to be a direct extension of these approaches, but one which induces structure from parallel corpora rather than relying on a treebank. Tromble (2009) show that some pre-ordering benefits can be realized without a parsing step at all, by instead casting pre-ordering as a permutation modeling problem. While not splitting the task of preordering into parsing and tree rordering, that work shows that pre-ordering models can be learned directly from parallel corpora.",
                "cite_spans": [
                    {
                        "start": 170,
                        "end": 192,
                        "text": "(Xia and McCord, 2004;",
                        "ref_id": "BIBREF31"
                    },
                    {
                        "start": 193,
                        "end": 214,
                        "text": "Collins et al., 2005;",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 215,
                        "end": 232,
                        "text": "Lee et al., 2010)",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 330,
                        "end": 347,
                        "text": "(Xu et al., 2009)",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 373,
                        "end": 387,
                        "text": "(Genzel, 2010)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 550,
                        "end": 564,
                        "text": "Tromble (2009)",
                        "ref_id": "BIBREF26"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Pre-Ordering Models",
                "sec_num": "4.1"
            },
            {
                "text": "Distortion models have been primary components in machine translation models since the advent of statistical MT (Brown et al., 1993) . In modern systems, reordering models are integrated into decoders as additional features in a discriminative loglinear model, which also includes a language model, translation features, etc. In these cases, reordering models interact with the strong signal of a targetside language model. Because ordering prediciton is conflated with target-side generation, evaluations are conducted on the entire generated output, which cannot isolate reordering errors from other sorts of errors, like lexical selection. Despite these differences, certain integrated reordering models are similar in character to syntactic pre-ordering models. In particular, the tree rotation model of Yamada and Knight (2001) posited that reordering decisions involve rotations of a source-side syntax tree. The parameters of such a model can be trained by treating tree rotations as latent variables in a factored translation model, which parameterizes reordering and transfer separately but performs joint inference (Dyer and Resnik, 2010) . Syntactic reordering and transfer can also be modeled jointly, for instance in a tree-to-string translation system parameterized by a transducer grammar.",
                "cite_spans": [
                    {
                        "start": 109,
                        "end": 132,
                        "text": "MT (Brown et al., 1993)",
                        "ref_id": null
                    },
                    {
                        "start": 808,
                        "end": 832,
                        "text": "Yamada and Knight (2001)",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 1125,
                        "end": 1148,
                        "text": "(Dyer and Resnik, 2010)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Integrated Reordering Models",
                "sec_num": "4.2"
            },
            {
                "text": "While the success of integrated reordering models certainly highlights the importance of reordering in machine translation systems, we see several advantages to a pipelined, pre-ordering approach. First, the pre-ordering model can be trained and evaluated directly. Second, pre-ordering models need not factor according to the same dynamic program as the translation model. Third, the same reordering can be applied during training (for word alignment and rule extraction) and translation time without adding complexity to the extraction and decoding algorithms. Of course, integrating our model into translation inference represents a potentially fruitful avenue of future research.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Integrated Reordering Models",
                "sec_num": "4.2"
            },
            {
                "text": "The language processing community actively works on the problem of automatically inducing grammatical structure from a corpus of text (Pereira and Schabes, 1992) . Some success in this area has been demonstrated via generative models (Klein and Manning, 2002) , which often benefit from wellchosen priors (Cohen and Smith, 2009) or posterior constraints (Ganchev et al., 2009) . In principle, these models must discover the syntactic patterns that govern a language from the sequences of word tokens alone. These models are often evaluated relative to reference treebank annotations.",
                "cite_spans": [
                    {
                        "start": 134,
                        "end": 161,
                        "text": "(Pereira and Schabes, 1992)",
                        "ref_id": null
                    },
                    {
                        "start": 234,
                        "end": 259,
                        "text": "(Klein and Manning, 2002)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 305,
                        "end": 328,
                        "text": "(Cohen and Smith, 2009)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 354,
                        "end": 376,
                        "text": "(Ganchev et al., 2009)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Grammar Induction",
                "sec_num": "4.3"
            },
            {
                "text": "Grammar induction in the context of machine translation reordering offers different properties. The alignment patterns in a parallel corpus provide an additional signal to models that is strongly tied to syntactic properties of the aligned languages. Also, the evaluation is straightforward-any syntactic structure that supports the prediction of reordering is rewarded. Kuhn (2004) applied alignment-based constraints to the problem of inducing probabilistic context-free grammars, and showed an improvement with respect to Penn Treebank annotations over monolingual induction. Their work is distinct from ours because it focused on projecting distituents across languages, but mirrors ours in demonstrating that there is a role for aligned parallel corpora in grammar induction. Snyder et al. (2009) ",
                "cite_spans": [
                    {
                        "start": 371,
                        "end": 382,
                        "text": "Kuhn (2004)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 781,
                        "end": 801,
                        "text": "Snyder et al. (2009)",
                        "ref_id": "BIBREF24"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Grammar Induction",
                "sec_num": "4.3"
            },
            {
                "text": "Also related to STIR is previous work on bilingual grammar induction from parallel corpora using ITG (Blunsom et al., 2009) . These models have focused on learning phrasal translations -which are the terminal productions of a synchronous ITG -rather than reordering patterns that occur higher in the tree. Hence, while this paper shares formal machinery and data sources with that line of work, the models themselves target orthogonal aspects of the translation problem.",
                "cite_spans": [
                    {
                        "start": 101,
                        "end": 123,
                        "text": "(Blunsom et al., 2009)",
                        "ref_id": "BIBREF2"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Bilingual Grammar Induction",
                "sec_num": "4.4"
            },
            {
                "text": "As training data for our models we used 14,000 English sentences that were sampled from the web, translated into Japanese, and manually annotated with word alignments. The annotation was carried out by the original translators to promote consistency of analysis. Talbot et al. (2011) describes this corpus in further detail. A held-out test set of 396 manually aligned sentence pairs was used to evaluate reordering accuracy. Statistics used for features were computed from the full, unreordered, automatically word aligned, parallel training corpus used for the translation experiments described below.",
                "cite_spans": [
                    {
                        "start": 263,
                        "end": 283,
                        "text": "Talbot et al. (2011)",
                        "ref_id": "BIBREF25"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Results",
                "sec_num": "5"
            },
            {
                "text": "We evaluate the accuracy of the monolingual parsing models by their span F1, relative to the trees induced by the parallel parsing model on the held-out set.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Individual Model Accuracy",
                "sec_num": "5.1"
            },
            {
                "text": "The first row of Table 1 shows that the model was able to reliably replicate the parses induced from alignments, at 84.8% F1. The following three lines show that removing either POS or cluster features degrades performance by only 0.4% F1, indicating that POS features are largely redundant in the presence of automatically induced word class features. Hence, no syntactic annotations are necessary at all to train the model. We report two accuracy measures for the tree reordering model, one for non-terminal spans (acc N ) and one for terminal spans (acc T ). The following column, labeled R O , is the reordering score of the tree reordering model applied to the oracle parallel parser tree. This score is independent of the monolingual parsing model.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 23,
                        "end": 24,
                        "text": "1",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Individual Model Accuracy",
                "sec_num": "5.1"
            },
            {
                "text": "The fifth line, labeled learned alignments, shows the impact of replacing manual alignment annotations with learned Model 1 alignments, trained in both directions and combined with the refined heuristic (Brown et al., 1993; Och et al., 1999) .",
                "cite_spans": [
                    {
                        "start": 203,
                        "end": 223,
                        "text": "(Brown et al., 1993;",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 224,
                        "end": 241,
                        "text": "Och et al., 1999)",
                        "ref_id": "BIBREF20"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Individual Model Accuracy",
                "sec_num": "5.1"
            },
            {
                "text": "The pipeline column shows the reordering score of the full STIR pipeline compared to two simple baselines: Monotone applies no reordering, while inverted simply inverts the word order. STIR outperforms all three other systems.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Individual Model Accuracy",
                "sec_num": "5.1"
            },
            {
                "text": "In the final line, we compare to the syntax-based pre-ordering system described in Genzel (2010) . This approach first parses source sentences with a supervised parser, then learns reordering rules that permute those trees.",
                "cite_spans": [
                    {
                        "start": 83,
                        "end": 96,
                        "text": "Genzel (2010)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Individual Model Accuracy",
                "sec_num": "5.1"
            },
            {
                "text": "We apply STIR as a pre-ordering step in a stateof-the-art phrase-based translation system from English to Japanese (Koehn et al., 2003) . At training time, pre-ordering is applied to the source side of every sentence pair in the training corpus before word alignment and phrase extraction. Likewise, every input sentence is pre-ordered at translation time.",
                "cite_spans": [
                    {
                        "start": 115,
                        "end": 135,
                        "text": "(Koehn et al., 2003)",
                        "ref_id": "BIBREF13"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Translation Quality",
                "sec_num": "5.2"
            },
            {
                "text": "Our baseline is the same system, but without preordering. Our implementation's integrated distortion model is expressed as a negative exponential function of the distance between the current and previous source phrase, with a maximum jump width of four words. Our in-house decoder is based on the alignment template approach to translation and uses a small set of standard feature functions during decoding (Och and Ney, 2004) .",
                "cite_spans": [
                    {
                        "start": 398,
                        "end": 426,
                        "text": "decoding (Och and Ney, 2004)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Translation Quality",
                "sec_num": "5.2"
            },
            {
                "text": "We compare to using an integrated lexicalized reordering model (Koehn and Monz, 2005) , a forestto-string translation model (Zhang et al., 2011) and finally the syntactic pre-ordering technique of Genzel (2010) applied to the phrase-based baseline. We evaluate the impact of the proposed approach on translation quality as measured by the BLEU score on the token level (Papineni et al., 2002) .",
                "cite_spans": [
                    {
                        "start": 63,
                        "end": 85,
                        "text": "(Koehn and Monz, 2005)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 124,
                        "end": 144,
                        "text": "(Zhang et al., 2011)",
                        "ref_id": "BIBREF34"
                    },
                    {
                        "start": 369,
                        "end": 392,
                        "text": "(Papineni et al., 2002)",
                        "ref_id": "BIBREF21"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Translation Quality",
                "sec_num": "5.2"
            },
            {
                "text": "The translation model is trained on 700 million tokens of parallel text, primarily extracted from the web using automated parallel document identification (Uszkoreit et al., 2010) . Alignments were learned using two iterations of Model 1 and two iterations of the HMM alignment model (Vogel et al., 1996) . Our dev and test data sets consist of 3100 and 1000 English sentences, respectively, that were randomly sampled from the web and translated into Japanese. The eval set is a larger, heterogenous set containing 12,784 sentences. In all cases, the final log-linear models were optimized on the dev set using lattice-based Minimum Error Rate Training (Macherey et al., 2008) .",
                "cite_spans": [
                    {
                        "start": 155,
                        "end": 179,
                        "text": "(Uszkoreit et al., 2010)",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 284,
                        "end": 304,
                        "text": "(Vogel et al., 1996)",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 654,
                        "end": 677,
                        "text": "(Macherey et al., 2008)",
                        "ref_id": "BIBREF18"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Translation Quality",
                "sec_num": "5.2"
            },
            {
                "text": "Table 2 shows that STIR improves over the baseline system by a large margin of 3.84% BLEU (test). These gains are comparable in magnitude to those reported in Genzel (2010) . Our induced parses are competitive with both systems that use syntactic parsers and substantially outperform lexicalized reordering.",
                "cite_spans": [
                    {
                        "start": 159,
                        "end": 172,
                        "text": "Genzel (2010)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "2",
                        "ref_id": "TABREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Translation Quality",
                "sec_num": "5.2"
            },
            {
                "text": "We have demonstrated that induced parses suffice for pre-ordering. We hope that future work in grammar induction will also consider pre-ordering as an extrinsic evaluation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "6"
            },
            {
                "text": "An astute reviewer pointed out that no binary tree over an S-V-O sentence can license both S-O-V and V-S-O orderings. Hence, parse trees that are induced for multilingual reordering will need n-ary branches.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Reordering metrics for MT",
                "authors": [
                    {
                        "first": "Alexandra",
                        "middle": [],
                        "last": "Birch",
                        "suffix": ""
                    },
                    {
                        "first": "Miles",
                        "middle": [],
                        "last": "Osborne",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Proceedings of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alexandra Birch and Miles Osborne. 2011. Reordering metrics for MT. In Proceedings of the Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Metrics for MT evaluation: Evaluating reordering",
                "authors": [
                    {
                        "first": "Alexandra",
                        "middle": [],
                        "last": "Birch",
                        "suffix": ""
                    },
                    {
                        "first": "Phil",
                        "middle": [],
                        "last": "Blunsom",
                        "suffix": ""
                    },
                    {
                        "first": "Miles",
                        "middle": [],
                        "last": "Osborne",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alexandra Birch, Phil Blunsom, and Miles Osborne. 2010. Metrics for MT evaluation: Evaluating reorder- ing. Machine Translation.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "A Gibbs sampler for phrasal synchronous grammar induction",
                "authors": [
                    {
                        "first": "Phil",
                        "middle": [],
                        "last": "Blunsom",
                        "suffix": ""
                    },
                    {
                        "first": "Trevor",
                        "middle": [],
                        "last": "Cohn",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Dyer",
                        "suffix": ""
                    },
                    {
                        "first": "Miles",
                        "middle": [],
                        "last": "Osborne",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of the",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os- borne. 2009. A Gibbs sampler for phrasal syn- chronous grammar induction. In Proceedings of the Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "The mathematics of statistical machine translation: Parameter estimation",
                "authors": [
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Peter",
                        "suffix": ""
                    },
                    {
                        "first": "Stephen",
                        "middle": [
                            "A"
                        ],
                        "last": "Brown",
                        "suffix": ""
                    },
                    {
                        "first": "Vincent",
                        "middle": [
                            "J"
                        ],
                        "last": "Della Pietra",
                        "suffix": ""
                    },
                    {
                        "first": "Robert",
                        "middle": [
                            "L"
                        ],
                        "last": "Della Pietra",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Mercer",
                        "suffix": ""
                    }
                ],
                "year": 1993,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estima- tion. Computational Linguistics.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction",
                "authors": [
                    {
                        "first": "Shay",
                        "middle": [],
                        "last": "Cohen",
                        "suffix": ""
                    },
                    {
                        "first": "Noah",
                        "middle": [],
                        "last": "Smith",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of the North American Chapter",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Shay Cohen and Noah Smith. 2009. Shared logistic normal distributions for soft parameter tying in unsu- pervised grammar induction. In Proceedings of the North American Chapter of the Association for Com- putational Linguistics.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Clause restructuring for statistical machine translation",
                "authors": [
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Collins",
                        "suffix": ""
                    },
                    {
                        "first": "Philipp",
                        "middle": [],
                        "last": "Koehn",
                        "suffix": ""
                    },
                    {
                        "first": "Ivona",
                        "middle": [],
                        "last": "Kucerova",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of the",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Michael Collins, Philipp Koehn, and Ivona Kucerova. 2005. Clause restructuring for statistical machine translation. In Proceedings of the Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Context-free reordering, finite-state translation",
                "authors": [
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Dyer",
                        "suffix": ""
                    },
                    {
                        "first": "Philip",
                        "middle": [],
                        "last": "Resnik",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the North American Chapter",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chris Dyer and Philip Resnik. 2010. Context-free re- ordering, finite-state translation. In Proceedings of the North American Chapter of the Association for Com- putational Linguistics.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Efficient, feature-based, conditional random field parsing",
                "authors": [
                    {
                        "first": "Jenny",
                        "middle": [
                            "Rose"
                        ],
                        "last": "Finkel",
                        "suffix": ""
                    },
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Kleeman",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Proceedings of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jenny Rose Finkel, Alex Kleeman, and Christopher D. Manning. 2008. Efficient, feature-based, conditional random field parsing. In Proceedings of the Associa- tion for Computational Linguistics.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Dependency grammar induction via bitext projection constraints",
                "authors": [
                    {
                        "first": "Kuzman",
                        "middle": [],
                        "last": "Ganchev",
                        "suffix": ""
                    },
                    {
                        "first": "Jennifer",
                        "middle": [],
                        "last": "Gillenwater",
                        "suffix": ""
                    },
                    {
                        "first": "Ben",
                        "middle": [],
                        "last": "Taskar",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of the",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kuzman Ganchev, Jennifer Gillenwater, and Ben Taskar. 2009. Dependency grammar induction via bitext pro- jection constraints. In Proceedings of the Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Automatically learning sourceside reordering rules for large scale machine translation",
                "authors": [
                    {
                        "first": "Dmitriy",
                        "middle": [],
                        "last": "Genzel",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the Conference on Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dmitriy Genzel. 2010. Automatically learning source- side reordering rules for large scale machine transla- tion. In Proceedings of the Conference on Computa- tional Linguistics.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Natural language grammar induction using a constituentcontext model",
                "authors": [
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Klein",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2001,
                "venue": "Proceedings of Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dan Klein and Christopher D. Manning. 2001. Natu- ral language grammar induction using a constituent- context model. In Proceedings of Neural Information Processing Systems.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "A generative constituent-context model for improved grammar induction",
                "authors": [
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Klein",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Proceedings of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dan Klein and Christopher D. Manning. 2002. A gener- ative constituent-context model for improved grammar induction. In Proceedings of the Association for Com- putational Linguistics.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Shared task: Statistical machine translation between european languages",
                "authors": [
                    {
                        "first": "Philipp",
                        "middle": [],
                        "last": "Koehn",
                        "suffix": ""
                    },
                    {
                        "first": "Christof",
                        "middle": [],
                        "last": "Monz",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of the International Workshop on Spoken Language Translation",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Philipp Koehn and Christof Monz. 2005. Shared task: Statistical machine translation between european lan- guages. In Proceedings of the International Workshop on Spoken Language Translation.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Statistical phrase-based translation",
                "authors": [
                    {
                        "first": "Philipp",
                        "middle": [],
                        "last": "Koehn",
                        "suffix": ""
                    },
                    {
                        "first": "Franz",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Josef",
                        "middle": [],
                        "last": "Och",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Marcu",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Proceedings of the North American Chapter",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceed- ings of the North American Chapter of the Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Experiments in parallel-text based grammar induction",
                "authors": [
                    {
                        "first": "Jonas",
                        "middle": [],
                        "last": "Kuhn",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proceedings of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jonas Kuhn. 2004. Experiments in parallel-text based grammar induction. In Proceedings of the Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
                "authors": [
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Lafferty",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Mccallum",
                        "suffix": ""
                    },
                    {
                        "first": "Fernando",
                        "middle": [],
                        "last": "Pereira",
                        "suffix": ""
                    }
                ],
                "year": 2001,
                "venue": "Proceedings of the International Conference on Machine Learning",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic mod- els for segmenting and labeling sequence data. In Pro- ceedings of the International Conference on Machine Learning.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "METEOR: An automatic metric for mt evaluation with high levels of correlation with human judgments",
                "authors": [
                    {
                        "first": "Alon",
                        "middle": [],
                        "last": "Lavie",
                        "suffix": ""
                    },
                    {
                        "first": "Abhaya",
                        "middle": [],
                        "last": "Agarwal",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Proceedings of ACL Workshop on Statistical Machine Translation",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alon Lavie and Abhaya Agarwal. 2007. METEOR: An automatic metric for mt evaluation with high levels of correlation with human judgments. In Proceedings of ACL Workshop on Statistical Machine Translation.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Constituent reordering and syntax models for Englishto-Japanese statistical machine translation",
                "authors": [
                    {
                        "first": "Young-Suk",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Bing",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaoqiang",
                        "middle": [],
                        "last": "Luo",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the Conference on Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Young-Suk Lee, Bing Zhao, and Xiaoqiang Luo. 2010. Constituent reordering and syntax models for English- to-Japanese statistical machine translation. In Pro- ceedings of the Conference on Computational Linguis- tics.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Lattice-based minimum error rate training for statistical machine translation",
                "authors": [
                    {
                        "first": "Wolfgang",
                        "middle": [],
                        "last": "Macherey",
                        "suffix": ""
                    },
                    {
                        "first": "Franz",
                        "middle": [],
                        "last": "Och",
                        "suffix": ""
                    },
                    {
                        "first": "Ignacio",
                        "middle": [],
                        "last": "Thayer",
                        "suffix": ""
                    },
                    {
                        "first": "Jakob",
                        "middle": [],
                        "last": "Uszkoreit",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Wolfgang Macherey, Franz Och, Ignacio Thayer, and Jakob Uszkoreit. 2008. Lattice-based minimum er- ror rate training for statistical machine translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "The alignment template approach to statistical machine translation",
                "authors": [
                    {
                        "first": "Josef",
                        "middle": [],
                        "last": "Franz",
                        "suffix": ""
                    },
                    {
                        "first": "Hermann",
                        "middle": [],
                        "last": "Och",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Ney",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Franz Josef Och and Hermann Ney. 2004. The align- ment template approach to statistical machine transla- tion. Computational Linguistics.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Improved alignment models for statistical machine translation",
                "authors": [
                    {
                        "first": "Josef",
                        "middle": [],
                        "last": "Franz",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [],
                        "last": "Och",
                        "suffix": ""
                    },
                    {
                        "first": "Hermann",
                        "middle": [],
                        "last": "Tillman",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Ney",
                        "suffix": ""
                    }
                ],
                "year": 1999,
                "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Franz Josef Och, Christopher Tillman, and Hermann Ney. 1999. Improved alignment models for statistical ma- chine translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "BLEU: A method for automatic evaluation of machine translation",
                "authors": [
                    {
                        "first": "Kishore",
                        "middle": [],
                        "last": "Papineni",
                        "suffix": ""
                    },
                    {
                        "first": "Salim",
                        "middle": [],
                        "last": "Roukos",
                        "suffix": ""
                    },
                    {
                        "first": "Todd",
                        "middle": [],
                        "last": "Ward",
                        "suffix": ""
                    },
                    {
                        "first": "Wei-Jing",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    }
                ],
                "year": 1992,
                "venue": "Proceedings of the Association for Computational Linguistics. Fernando Pereira and Yves Schabes",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. BLEU: A method for automatic eval- uation of machine translation. In Proceedings of the Association for Computational Linguistics. Fernando Pereira and Yves Schabes. 1992. Inside- outside reestimation from partially bracketed corpora. In Proceedings of the Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Sparse multi-scale grammars for discriminative latent variable parsing",
                "authors": [
                    {
                        "first": "Slav",
                        "middle": [],
                        "last": "Petrov",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Klein",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Slav Petrov and Dan Klein. 2008. Sparse multi-scale grammars for discriminative latent variable parsing. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "A universal part-of-speech tagset",
                "authors": [
                    {
                        "first": "Slav",
                        "middle": [],
                        "last": "Petrov",
                        "suffix": ""
                    },
                    {
                        "first": "Dipanjan",
                        "middle": [],
                        "last": "Das",
                        "suffix": ""
                    },
                    {
                        "first": "Ryan",
                        "middle": [],
                        "last": "Mcdonald",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Slav Petrov, Dipanjan Das, and Ryan McDonald. 2011. A universal part-of-speech tagset. Technical report.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Unsupervised multilingual grammar induction",
                "authors": [
                    {
                        "first": "Benjamin",
                        "middle": [],
                        "last": "Snyder",
                        "suffix": ""
                    },
                    {
                        "first": "Tahira",
                        "middle": [],
                        "last": "Naseem",
                        "suffix": ""
                    },
                    {
                        "first": "Regina",
                        "middle": [],
                        "last": "Barzilay",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of the",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Benjamin Snyder, Tahira Naseem, and Regina Barzilay. 2009. Unsupervised multilingual grammar induction. In Proceedings of the Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "A lightweight evaluation framework for machine translation reordering",
                "authors": [
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Talbot",
                        "suffix": ""
                    },
                    {
                        "first": "Hideto",
                        "middle": [],
                        "last": "Kazawa",
                        "suffix": ""
                    },
                    {
                        "first": "Hiroshi",
                        "middle": [],
                        "last": "Ichikawa",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Katz-Brown",
                        "suffix": ""
                    },
                    {
                        "first": "Masakazu",
                        "middle": [],
                        "last": "Seno",
                        "suffix": ""
                    },
                    {
                        "first": "Franz",
                        "middle": [
                            "J"
                        ],
                        "last": "Och",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Proceedings of the Sixth Workshop on Statistical Machine Translation",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "David Talbot, Hideto Kazawa, Hiroshi Ichikawa, Ja- son Katz-Brown, Masakazu Seno, and Franz J. Och. 2011. A lightweight evaluation framework for ma- chine translation reordering. In Proceedings of the Sixth Workshop on Statistical Machine Translation.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Learning linear ordering problems for better translation",
                "authors": [
                    {
                        "first": "Roy",
                        "middle": [],
                        "last": "Tromble",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Roy Tromble. 2009. Learning linear ordering problems for better translation. In Proceedings of the Confer- ence on Empirical Methods in Natural Language Pro- cessing.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Distributed word clustering for large scale class-based language modeling in machine translation",
                "authors": [
                    {
                        "first": "Jakob",
                        "middle": [],
                        "last": "Uszkoreit",
                        "suffix": ""
                    },
                    {
                        "first": "Thorsten",
                        "middle": [],
                        "last": "Brants",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Proceedings of the",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jakob Uszkoreit and Thorsten Brants. 2008. Distributed word clustering for large scale class-based language modeling in machine translation. In Proceedings of the Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Large scale parallel document mining for machine translation",
                "authors": [
                    {
                        "first": "Jakob",
                        "middle": [],
                        "last": "Uszkoreit",
                        "suffix": ""
                    },
                    {
                        "first": "Jay",
                        "middle": [],
                        "last": "Ponte",
                        "suffix": ""
                    },
                    {
                        "first": "Ashok",
                        "middle": [],
                        "last": "Popat",
                        "suffix": ""
                    },
                    {
                        "first": "Moshe",
                        "middle": [],
                        "last": "Dubiner",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the Conference on Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jakob Uszkoreit, Jay Ponte, Ashok Popat, and Moshe Du- biner. 2010. Large scale parallel document mining for machine translation. In Proceedings of the Conference on Computational Linguistics.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "HMM-based word alignment in statistical translation",
                "authors": [
                    {
                        "first": "Stephan",
                        "middle": [],
                        "last": "Vogel",
                        "suffix": ""
                    },
                    {
                        "first": "Hermann",
                        "middle": [],
                        "last": "Ney",
                        "suffix": ""
                    },
                    {
                        "first": "Christoph",
                        "middle": [],
                        "last": "Tillmann",
                        "suffix": ""
                    }
                ],
                "year": 1996,
                "venue": "Proceedings of the Conference on Computational linguistics",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996. HMM-based word alignment in statistical trans- lation. In Proceedings of the Conference on Computa- tional linguistics.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "Stochastic inversion transduction grammars and bilingual parsing of parallel corpora",
                "authors": [
                    {
                        "first": "Dekai",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    }
                ],
                "year": 1997,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics.",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "Improving a statistical mt system with automatically learned rewrite patterns",
                "authors": [
                    {
                        "first": "Fei",
                        "middle": [],
                        "last": "Xia",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Mccord",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proceedings of the Conference on Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Fei Xia and Michael McCord. 2004. Improving a sta- tistical mt system with automatically learned rewrite patterns. In Proceedings of the Conference on Com- putational Linguistics.",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "Using a dependency parser to improve smt for subject-object-verb languages",
                "authors": [
                    {
                        "first": "Peng",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Jaeho",
                        "middle": [],
                        "last": "Kang",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Ringgard",
                        "suffix": ""
                    },
                    {
                        "first": "Franz",
                        "middle": [],
                        "last": "Och",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of the North American Chapter",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Peng Xu, Jaeho Kang, Michael Ringgard, and Franz Och. 2009. Using a dependency parser to improve smt for subject-object-verb languages. In Proceedings of the North American Chapter of the Association for Com- putational Linguistics.",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "A syntax-based statistical translation model",
                "authors": [
                    {
                        "first": "Kenji",
                        "middle": [],
                        "last": "Yamada",
                        "suffix": ""
                    },
                    {
                        "first": "Kevin",
                        "middle": [],
                        "last": "Knight",
                        "suffix": ""
                    }
                ],
                "year": 2001,
                "venue": "Proceedings of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kenji Yamada and Kevin Knight. 2001. A syntax-based statistical translation model. In Proceedings of the As- sociation for Computational Linguistics.",
                "links": null
            },
            "BIBREF34": {
                "ref_id": "b34",
                "title": "Binarized forest to string translation",
                "authors": [
                    {
                        "first": "Hao",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Licheng",
                        "middle": [],
                        "last": "Fang",
                        "suffix": ""
                    },
                    {
                        "first": "Peng",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaoyun",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Proceedings of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Hao Zhang, Licheng Fang, Peng Xu, and Xiaoyun Wu. 2011. Binarized forest to string translation. In Pro- ceedings of the Association for Computational Lin- guistics.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 1: The training and reordering pipeline for STIR contains three models. The inputs and outputs of each model are indicated by solid arrows, while dashed arrows indicate the source of training examples. The parallel parsing model provides tree and reordering examples that are used to train the other models. In an MT system, the trained reordering pipeline (shaded) pre-orders a source sentence without target-side or alignment information.",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Figure2: An example from our development corpus, annotated with the information flow (left) and annotations and predictions (right). Alignments inform projections, which are spans of the target associated with each source word. The parallel parse may only include contiguous spans. On the other hand, the induced parse may only condition on the source sentence. The induced order is restricted by the induced parse. In this example, the induced order is incorrect because the subject and verb form a constituent in the induced parse that cannot be separated correctly by the reordering model. This example demonstrates the important role of the induced parser in the STIR pipeline.",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table><tr><td>POS. A supervised part-of-speech (POS) tagger provides coarse tags drawn from a 12 tag set</td></tr><tr><td>T = {Verb, Noun, Pronoun, Conjunction, Adjective, Adverb, Adposition, Determiner,</td></tr><tr><td>Number, Particle/Function word, Punctuation,</td></tr><tr><td>Other}</td></tr></table>",
                "type_str": "table",
                "text": "\u2022 P D (e i , e j ) is the fraction of the time that two co-occuring source words e i and e j align to adjacent positions in the target.-1 , as well as word sequences for spans up to length 4. Features are included for a variety of clusterings with sizes c \u2208 {2 3 , 2 4 , . . . , 2 11 }.",
                "html": null,
                "num": null
            },
            "TABREF1": {
                "content": "<table><tr><td>also demonstrated that paral-</td></tr><tr><td>lel corpora can play a role in improving the quality</td></tr><tr><td>of grammar induction models. Their work differs</td></tr><tr><td>from ours in that it focuses on multilingual lexical</td></tr><tr><td>statistics and dependency relationships, rather than</td></tr><tr><td>reordering patterns.</td></tr></table>",
                "type_str": "table",
                "text": "Accuracy of individual monolingual parsing and reordering models, as well as complete pipelines trained on annotated and learned word alignments.",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table><tr><td/><td/><td>BLEU %</td></tr><tr><td/><td>dev</td><td>test</td><td>eval</td></tr><tr><td>Baseline</td><td colspan=\"3\">18.65 19.02 13.60</td></tr><tr><td colspan=\"4\">Lexicalized Reordering 19.45 18.92 13.99</td></tr><tr><td colspan=\"4\">Forest-to-String Syntactic Pre-ordering 22.59 23.28 16.31 23.08 22.85 16.60</td></tr><tr><td>STIR: annotated</td><td colspan=\"3\">22.46 22.86 16.39</td></tr><tr><td>STIR: learned</td><td colspan=\"3\">20.28 20.66 14.64</td></tr></table>",
                "type_str": "table",
                "text": "Translation quality, measured by BLEU, for English to Japanese. STIR results use both manually annotated and learned alignments.",
                "html": null,
                "num": null
            }
        }
    }
}