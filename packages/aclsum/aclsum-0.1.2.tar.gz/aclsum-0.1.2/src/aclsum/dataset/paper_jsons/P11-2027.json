{
    "paper_id": "P11-2027",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:14:24.476636Z"
    },
    "title": "AM-FM: A Semantic Framework for Translation Quality Assessment",
    "authors": [
        {
            "first": "Rafael",
            "middle": [
                "E"
            ],
            "last": "Banchs",
            "suffix": "",
            "affiliation": {},
            "email": "rembanchs@i2r.a-star.edu.sg"
        },
        {
            "first": "Haizhou",
            "middle": [],
            "last": "Li",
            "suffix": "",
            "affiliation": {},
            "email": ""
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "This work introduces AM-FM, a semantic framework for machine translation evaluation. Based upon this framework, a new evaluation metric, which is able to operate without the need for reference translations, is implemented and evaluated. The metric is based on the concepts of adequacy and fluency, which are independently assessed by using a cross-language latent semantic indexing approach and an n-gram based language model approach, respectively. Comparative analyses with conventional evaluation metrics are conducted on two different evaluation tasks (overall quality assessment and comparative ranking) over a large collection of human evaluations involving five European languages. Finally, the main pros and cons of the proposed framework are discussed along with future research directions.",
    "pdf_parse": {
        "paper_id": "P11-2027",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "This work introduces AM-FM, a semantic framework for machine translation evaluation. Based upon this framework, a new evaluation metric, which is able to operate without the need for reference translations, is implemented and evaluated. The metric is based on the concepts of adequacy and fluency, which are independently assessed by using a cross-language latent semantic indexing approach and an n-gram based language model approach, respectively. Comparative analyses with conventional evaluation metrics are conducted on two different evaluation tasks (overall quality assessment and comparative ranking) over a large collection of human evaluations involving five European languages. Finally, the main pros and cons of the proposed framework are discussed along with future research directions.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Evaluation has always been one of the major issues in Machine Translation research, as both human and automatic evaluation methods exhibit very important limitations. On the one hand, although highly reliable, in addition to being expensive and time consuming, human evaluation suffers from inconsistency problems due to inter-and intraannotator agreement issues. On the other hand, while being consistent, fast and cheap, automatic evaluation has the major disadvantage of requiring reference translations. This makes automatic evaluation not reliable in the sense that good translations not matching the available references are evaluated as poor or bad translations.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The main objective of this work is to propose and evaluate AM-FM, a semantic framework for assessing translation quality without the need for reference translations. The proposed framework is theoretically grounded on the classical concepts of adequacy and fluency, and it is designed to account for these two components of translation quality in an independent manner. First, a cross-language latent semantic indexing model is used for assessing the adequacy component by directly comparing the output translation with the input sentence it was generated from. Second, an n-gram based language model of the target language is used for assessing the fluency component.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Both components of the metric are evaluated at the sentence level, providing the means for defining and implementing a sentence-based evaluation metric. Finally, the two components are combined into a single measure by implementing a weighted harmonic mean, for which the weighting factor can be adjusted for optimizing the metric performance.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The rest of the paper is organized as follows. Section 2, presents some background work and the specific dataset that has been used in the experimental work. Section 3, provides details on the proposed AM-FM framework and the specific metric implementation. Section 4 presents the results of the conducted comparative evaluations. Finally, section 5 presents the main conclusions and relevant issues to be dealt with in future research.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Although BLEU (Papineni et al., 2002) has become a de facto standard for machine translation evaluation, other metrics such as NIST (Doddington, 2002) and, more recently, Meteor (Banerjee and Lavie, 2005) , are commonly used too. Regarding the specific idea of evaluating machine translation without using reference translations, several works have proposed and evaluated different approaches, including round-trip translation (Somers, 2005; Rapp, 2009) , as well as other regression-and classification-based approaches (Quirk, 2004; Gamon et al., 2005; Albrecht and Hwa, 2007; Specia et al., 2009) .",
                "cite_spans": [
                    {
                        "start": 14,
                        "end": 37,
                        "text": "(Papineni et al., 2002)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 132,
                        "end": 150,
                        "text": "(Doddington, 2002)",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 178,
                        "end": 204,
                        "text": "(Banerjee and Lavie, 2005)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 427,
                        "end": 441,
                        "text": "(Somers, 2005;",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 442,
                        "end": 453,
                        "text": "Rapp, 2009)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 520,
                        "end": 533,
                        "text": "(Quirk, 2004;",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 534,
                        "end": 553,
                        "text": "Gamon et al., 2005;",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 554,
                        "end": 577,
                        "text": "Albrecht and Hwa, 2007;",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 578,
                        "end": 598,
                        "text": "Specia et al., 2009)",
                        "ref_id": "BIBREF17"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work and Dataset",
                "sec_num": "2"
            },
            {
                "text": "As part of the recent efforts on machine translation evaluation, two workshops have been organizing shared-tasks and evaluation campaigns over the last four years: the NIST Metrics for Machine Translation Challenge 1 (MetricsMATR) and the Workshop on Statistical Machine Translation 2 (WMT); which were actually held as one single event in their most recent edition in 2010.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work and Dataset",
                "sec_num": "2"
            },
            {
                "text": "The dataset used in this work corresponds to WMT-07. This dataset is used, instead of a more recent one, because no human judgments on adequacy and fluency have been conducted in WMT after year 2007, and human evaluation data is not freely available from MetricsMATR.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work and Dataset",
                "sec_num": "2"
            },
            {
                "text": "In this dataset, translation outputs are available for fourteen tasks involving five European languages: English (EN), Spanish (ES), German (DE), French (FR) and Czech (CZ); and two domains: News Commentaries (News) and European Parliament Debates (EPPS). A complete description on WMT-07 evaluation campaign and dataset is available in Callison-Burch et al. (2007) .",
                "cite_spans": [
                    {
                        "start": 337,
                        "end": 365,
                        "text": "Callison-Burch et al. (2007)",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work and Dataset",
                "sec_num": "2"
            },
            {
                "text": "System outputs for fourteen of the fifteen systems that participated in the evaluation are available. This accounts for 86 independent system outputs with a total of 172,315 individual sentence translations, from which only 10,754 were rated for both adequacy and fluency by human judges.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work and Dataset",
                "sec_num": "2"
            },
            {
                "text": "The specific vote standardization procedure described in section 5.4 of Blatz et al. (2003) was applied to all adequacy and fluency scores for removing individual voting patterns and averaging votes. Table 1 provides information on the corresponding domain, and source and target languages 1 http://www.itl.nist.gov/iad/mig/tests/metricsmatr/ 2 http://www.statmt.org/wmt10/ for each of the fourteen translation tasks, along with their corresponding number of system outputs and the amount of sentence translations for which human evaluations are available. ",
                "cite_spans": [
                    {
                        "start": 72,
                        "end": 91,
                        "text": "Blatz et al. (2003)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 206,
                        "end": 207,
                        "text": "1",
                        "ref_id": "TABREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Related Work and Dataset",
                "sec_num": "2"
            },
            {
                "text": "The framework proposed in this work (AM-FM) aims at assessing translation quality without the need for reference translations, while maintaining consistency with human quality assessments. Different from other approaches not using reference translations, we rely on a cross-language version of latent semantic indexing (Dumais et al., 1997) for creating a semantic space where translation outputs and inputs can be directly compared.",
                "cite_spans": [
                    {
                        "start": 319,
                        "end": 340,
                        "text": "(Dumais et al., 1997)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Semantic Evaluation Framework",
                "sec_num": "3"
            },
            {
                "text": "A two-component evaluation metric, based on the concepts of adequacy and fluency (White et al., 1994) is defined. While adequacy accounts for the amount of source meaning being preserved by the translation (5:all, 4:most, 3:much, 2:little, 1:none), fluency accounts for the quality of the target language in the translation (5:flawless, 4:good, 3:nonnative, 2:disfluent, 1:incomprehensible).",
                "cite_spans": [
                    {
                        "start": 81,
                        "end": 101,
                        "text": "(White et al., 1994)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Semantic Evaluation Framework",
                "sec_num": "3"
            },
            {
                "text": "For implementing the adequacy-oriented component (AM) of the metric, the cross-language latent semantic indexing approach is used (Dumais et al., 1997) , in which the source sentence originating the translation is used as evaluation reference. Accord-ing to this, the AM component can be regarded to be mainly adequacy-oriented as it is computed on a cross-language semantic space.",
                "cite_spans": [
                    {
                        "start": 130,
                        "end": 151,
                        "text": "(Dumais et al., 1997)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Metric Definition",
                "sec_num": "3.1"
            },
            {
                "text": "For implementing the fluency-oriented component (FM) of the proposed metric, an n-gram based language model approach is used (Manning and Schutze, 1999) . This component can be regarded to be mainly fluency-oriented as it is computed on the target language side in a manner that is totally independent from the source language.",
                "cite_spans": [
                    {
                        "start": 125,
                        "end": 152,
                        "text": "(Manning and Schutze, 1999)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Metric Definition",
                "sec_num": "3.1"
            },
            {
                "text": "For combining both components into a single metric, a weighted harmonic mean is proposed:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Metric Definition",
                "sec_num": "3.1"
            },
            {
                "text": "AM-FM = AM FM / (\u03b1 AM + (1-\u03b1) FM) (1)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Metric Definition",
                "sec_num": "3.1"
            },
            {
                "text": "where \u03b1 is a weighting factor ranging from \u03b1=0 (pure AM component) to \u03b1=1 (pure FM component), which can be adjusted for maximizing the correlation between the proposed metric AM-FM and human evaluation scores.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Metric Definition",
                "sec_num": "3.1"
            },
            {
                "text": "The adequacy-oriented component of the metric (AM) was implemented by following the procedure proposed by Dumais et al. (1997) , where a bilingual collection of data is used to generate a cross-language projection matrix for a vector-space representation of texts (Salton et al., 1975) by using singular value decomposition: SVD (Golub and Kahan, 1965) .",
                "cite_spans": [
                    {
                        "start": 106,
                        "end": 126,
                        "text": "Dumais et al. (1997)",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 264,
                        "end": 285,
                        "text": "(Salton et al., 1975)",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 329,
                        "end": 352,
                        "text": "(Golub and Kahan, 1965)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementation Details",
                "sec_num": "3.2"
            },
            {
                "text": "According to this formulation, a bilingual termdocument matrix X ab of dimensions M*N, where M=(M a +M b ) are vocabulary terms in languages a and b, and N are documents (sentences in our case), can be decomposed as follows: From the singular value decomposition depicted in (2), a low-dimensional representation for any sentence vector x a or x b , in language a or b, can be computed as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementation Details",
                "sec_num": "3.2"
            },
            {
                "text": "X ab = [X a ;X b ] = U ab \u03a3 ab V ab T (2)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementation Details",
                "sec_num": "3.2"
            },
            {
                "text": "y a T = [x a ;0] T U abM*L (3.a) y b T = [0; x b ] T U abM*L (3.b)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementation Details",
                "sec_num": "3.2"
            },
            {
                "text": "where y a and y b represent the L-dimensional vectors corresponding to the projections of the fulldimensional sentence vectors x a and x b , respectively; and U abM*L is a cross-language projection matrix composed of the first L column vectors of the unitary matrix U ab obtained in (2). Notice, from (3a) and (3b), how both sentence vectors x a and x b are padded with zeros at each corresponding other-language vocabulary locations for performing the cross-language projections. As similar terms in different languages would have similar occurrence patterns, theoretically, a close representation in the cross-language reduced space should be obtained for terms and sentences that are semantically related. Therefore, sentences can be compared across languages in the reduced space.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementation Details",
                "sec_num": "3.2"
            },
            {
                "text": "The AM component of the metric is finally computed in the projected space by using the cosine similarity between the source and target sentences:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementation Details",
                "sec_num": "3.2"
            },
            {
                "text": "AM = [s;0] T P ([0;t] T P) T / |[s;0] T P| / |[0;t] T P| (4)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementation Details",
                "sec_num": "3.2"
            },
            {
                "text": "where P is the projection matrix U abM*L described in (3a) and (3b), [s;0] and [0;t] are vector space representations of the source and target sentences being compared (with their target and source vocabulary elements set to zero, respectively), and | | is the L2-norm operator. In a final implementation stage, the range of AM is restricted to the interval [0,1] by truncating negative results.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementation Details",
                "sec_num": "3.2"
            },
            {
                "text": "For computing the projection matrices, random sets of 10,000 parallel sentences3 were drawn from the available training datasets. The only restriction we imposed to the extracted sentences was that each should contain at least 10 words. Seven projection matrices were constructed in total, one for each different combination of domain and language pair. TF-IDF weighting was applied to the constructed term-document matrices while maintaining all words in the vocabularies (i.e. no stopwords were removed). All computations related to SVD, sentence projections and cosine similarities were conducted with MATLAB.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementation Details",
                "sec_num": "3.2"
            },
            {
                "text": "The fluency-oriented component FM is implemented by using an n-gram language model. In order to avoid possible effects derived from differences in sentence lengths, a compensation factor is introduced in log-probability space. According to this, the FM component is computed as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementation Details",
                "sec_num": "3.2"
            },
            {
                "text": "FM = exp(\u03a3 n=1:N log(p(w n |w n-1 ,\u2026))/N) (5)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementation Details",
                "sec_num": "3.2"
            },
            {
                "text": "where p(w n |w n-1 ,\u2026) represent the target language n-gram probabilities and N is the total number of words in the target sentence being evaluated. By construction, the values of FM are also restricted to the interval [0,1]; so, both component values range within the same interval.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementation Details",
                "sec_num": "3.2"
            },
            {
                "text": "Fourteen language models were trained in total, one per task, by using the available training datasets. The models were computed with the SRILM toolbox (Stolcke, 2002) .",
                "cite_spans": [
                    {
                        "start": 152,
                        "end": 167,
                        "text": "(Stolcke, 2002)",
                        "ref_id": "BIBREF18"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementation Details",
                "sec_num": "3.2"
            },
            {
                "text": "As seen from ( 4) and ( 5), different from conventional metrics that compute matches between translation outputs and references, in the AM-FM framework, a semantic embedding is used for assessing the similarities between outputs and inputs (4) and, independently, an n-gram model is used for evaluating output language quality (5).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementation Details",
                "sec_num": "3.2"
            },
            {
                "text": "In order to evaluate the AM-FM framework, two comparative evaluations with standard metrics were conducted. More specifically, BLEU, NIST and Meteor were considered, as they are the metrics most frequently used in machine translation evaluation campaigns.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Comparative Evaluations",
                "sec_num": "4"
            },
            {
                "text": "In this first evaluation, AM-FM is compared with standard evaluation metrics in terms of their correlations with human-generated scores. Different from Callison-Burch et al. (2007) , where Spearman's correlation coefficients were used, we use here Pearson's coefficients as, instead of focusing on ranking; this first evaluation exercise focuses on evaluating the significance and noisiness of the association, if any, between the automatic metrics and human-generated scores.",
                "cite_spans": [
                    {
                        "start": 152,
                        "end": 180,
                        "text": "Callison-Burch et al. (2007)",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Correlation with Human Scores",
                "sec_num": "4.1"
            },
            {
                "text": "Three parameters should be adjusted for the AM-FM implementation described in (1): the dimensionality of the reduced space for AM, the order of n-gram model for FM, and the harmonic mean weighting parameter \u03b1. Such parameters can be adjusted for maximizing the correlation coefficient between the AM-FM metric and humangenerated scores. 4 After exploring the solution space, the following values were selected, dimensionality for AM: 1,000; order of n-gram model for FM: 3; and, weighting parameter \u03b1: 0.30",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Correlation with Human Scores",
                "sec_num": "4.1"
            },
            {
                "text": "In the comparative evaluation presented here, correlation coefficients between the automatic metrics and human-generated scores were computed at the system level (i.e. the units of analysis were system outputs), by considering all 86 available system outputs (see Table 1 ). For computing human scores and AM-FM at the system level, average values of sentence-based scores for each system output were considered.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 270,
                        "end": 271,
                        "text": "1",
                        "ref_id": "TABREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Correlation with Human Scores",
                "sec_num": "4.1"
            },
            {
                "text": "Table 2 presents the Pearson's correlation coefficients computed between the automatic metrics (BLEU, NIST, Meteor and our proposed AM-FM) and the human-generated scores (adequacy, fluency and the harmonic mean of both; i.e. 2af/(a+f)). All correlation coefficients presented in the table are statistically significant with p<0.01 (where p is the probability of getting the same correlation coefficient, with a similar number of 86 samples, by chance). As seen from the table, BLEU is the metric exhibiting the largest correlation coefficients with human-generated scores, followed by Meteor and AM-FM, while NIST exhibits the lowest correlation coefficient values. Recall that our proposed AM-FM metric is not using reference translations for assessing translation quality, while the other three metrics are.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Correlation with Human Scores",
                "sec_num": "4.1"
            },
            {
                "text": "In a similar exercise, the correlation coefficients were also computed at the sentence level (i.e. the units of analysis were sentences). These results are summarized in Table 3 . As metrics are computed at the sentence level, smoothed-bleu (Lin and Och, 2004) ",
                "cite_spans": [
                    {
                        "start": 241,
                        "end": 260,
                        "text": "(Lin and Och, 2004)",
                        "ref_id": "BIBREF10"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 176,
                        "end": 177,
                        "text": "3",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Metric",
                "sec_num": null
            },
            {
                "text": "In addition to adequacy and fluency, the WMT-07 dataset includes rankings of sentence translations. To evaluate the usefulness of AM-FM and its components in a different evaluation setting, we also conducted a comparative evaluation on their capacity for predicting human-generated rankings.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Reproducing Rankings",
                "sec_num": "4.2"
            },
            {
                "text": "As ranking evaluations allowed for ties among sentence translations, we restricted our analysis to evaluate whether automatic metrics were able to predict the best, the worst and both sentence translations for each of the 4,060 available rankings 5 . The number of items per ranking varies from 2 to 5, with an average of 4.11 items per ranking. Table 4 presents the results of the comparative evaluation on predicting rankings.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 352,
                        "end": 353,
                        "text": "4",
                        "ref_id": "TABREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Reproducing Rankings",
                "sec_num": "4.2"
            },
            {
                "text": "As seen from the table, Meteor is the automatic metric exhibiting the largest ranking prediction capability, followed by BLEU and NIST, while our proposed AM-FM metric exhibits the lowest ranking prediction capability. However, it still performs well above random chance predictions, which, for the given average of 4 items per ranking, is about 25% for best and worst ranking predictions, and about 8.33% for both. Again, recall that the AM-FM metric is not using reference translations, while the other three metrics are. Also, it is worth mentioning that human rankings were conducted 5 We discarded those rankings involving the translation system for which translation outputs were not available that, consequently, only had one translation output left. by looking at the reference translations and not the source. See Callison-Burch et al. (2007) ",
                "cite_spans": [
                    {
                        "start": 823,
                        "end": 851,
                        "text": "Callison-Burch et al. (2007)",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Reproducing Rankings",
                "sec_num": "4.2"
            },
            {
                "text": "This work presented AM-FM, a semantic framework for translation quality assessment. Two comparative evaluations with standard metrics have been conducted over a large collection of humangenerated scores involving different languages. Although the obtained performance is below standard metrics, the proposed method has the main advantage of not requiring reference translations.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusions and Future Work",
                "sec_num": "5"
            },
            {
                "text": "Notice that a monolingual version of AM-FM is also possible by using monolingual latent semantic indexing (Landauer et al., 1998) along with a set of reference translations. A detailed evaluation of a monolingual implementation of AM-FM can be found in Banchs and Li (2011) .",
                "cite_spans": [
                    {
                        "start": 106,
                        "end": 129,
                        "text": "(Landauer et al., 1998)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 253,
                        "end": 273,
                        "text": "Banchs and Li (2011)",
                        "ref_id": "BIBREF1"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusions and Future Work",
                "sec_num": "5"
            },
            {
                "text": "As future research, we plan to study the impact of different dataset sizes and vector space model parameters for improving the performance of the AM component of the metric. This will include the study of learning curves based on the amount of training data used, and the evaluation of different vector model construction strategies, such as removing stop-words and considering bigrams and word categories in addition to individual words.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusions and Future Work",
                "sec_num": "5"
            },
            {
                "text": "Finally, we also plan to study alternative uses of AM-FM within the context of statistical machine translation as, for example, a metric for MERT optimization, or using the AM component alone as an additional feature for decoding, rescoring and/or confidence estimation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusions and Future Work",
                "sec_num": "5"
            },
            {
                "text": "Although this accounts for a small proportion of the datasets (20% of News and 1% of European Parliament), it allowed for maintaining computational requirements under control while still providing a good vocabulary coverage.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "As no development dataset was available for this particular task, a subset of the same evaluation dataset had to be used.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Regression for sentence-level MT evaluation with pseudo references",
                "authors": [
                    {
                        "first": "Joshua",
                        "middle": [
                            "S"
                        ],
                        "last": "Albrecht",
                        "suffix": ""
                    },
                    {
                        "first": "Rebeca",
                        "middle": [],
                        "last": "Hwa",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Proceedings of the 45 th Annual Meeting of the Association of Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "296--303",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Joshua S. Albrecht and Rebeca Hwa. 2007. Regression for sentence-level MT evaluation with pseudo references. In Proceedings of the 45 th Annual Meeting of the Association of Computational Linguistics, 296-303.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Monolingual AM-FM: a two-dimensional machine translation evaluation method",
                "authors": [
                    {
                        "first": "Rafael",
                        "middle": [
                            "E"
                        ],
                        "last": "Banchs",
                        "suffix": ""
                    },
                    {
                        "first": "Haizhou",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Submitted to the Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Rafael E. Banchs and Haizhou Li. 2011. Monolingual AM-FM: a two-dimensional machine translation evaluation method. Submitted to the Conference on Empirical Methods in Natural Language Processing.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "METEOR: an automatic metric for MT evaluation with improved correlation with human judgments",
                "authors": [
                    {
                        "first": "Satanjeev",
                        "middle": [],
                        "last": "Banerjee",
                        "suffix": ""
                    },
                    {
                        "first": "Alon",
                        "middle": [],
                        "last": "Lavie",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization",
                "volume": "",
                "issue": "",
                "pages": "65--72",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Satanjeev Banerjee and Alon Lavie. 2005. METEOR: an automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization, 65-72.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Alex Kulesza, Alberto Sanchis and Nicola Ueffing",
                "authors": [
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Blatz",
                        "suffix": ""
                    },
                    {
                        "first": "Erin",
                        "middle": [],
                        "last": "Fitzgerald",
                        "suffix": ""
                    },
                    {
                        "first": "George",
                        "middle": [],
                        "last": "Foster",
                        "suffix": ""
                    },
                    {
                        "first": "Simona",
                        "middle": [],
                        "last": "Gandrabur",
                        "suffix": ""
                    },
                    {
                        "first": "Cyril",
                        "middle": [],
                        "last": "Goutte",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Final Report WS2003 CLSP Summer Workshop",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis and Nicola Ueffing. 2003. Confidence estimation for machine translation. Final Report WS2003 CLSP Summer Workshop, Johns Hopkins University",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "(Meta-) evaluation of machine translation",
                "authors": [
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Callison-Burch",
                        "suffix": ""
                    },
                    {
                        "first": "Cameron",
                        "middle": [],
                        "last": "Fordyce",
                        "suffix": ""
                    },
                    {
                        "first": "Philipp",
                        "middle": [],
                        "last": "Koehn",
                        "suffix": ""
                    },
                    {
                        "first": "Christof",
                        "middle": [],
                        "last": "Monz",
                        "suffix": ""
                    },
                    {
                        "first": "Josh",
                        "middle": [],
                        "last": "Schroeder",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Proceedings of Statistical Machine Translation Workshop",
                "volume": "",
                "issue": "",
                "pages": "136--158",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chris Callison-Burch, Cameron Fordyce,Philipp Koehn, Christof Monz and Josh Schroeder. 2007. (Meta-) evaluation of machine translation. In Proceedings of Statistical Machine Translation Workshop, 136-158.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Automatic evaluation of machine translation quality using n-gram cooccurrence statistics",
                "authors": [
                    {
                        "first": "George",
                        "middle": [],
                        "last": "Doddington",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Proceedings of the Human Language Technology Conference",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "George Doddington. 2002. Automatic evaluation of machine translation quality using n-gram co- occurrence statistics. In Proceedings of the Human Language Technology Conference.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Automatic cross-linguistic information retrieval using latent semantic indexing",
                "authors": [
                    {
                        "first": "Susan",
                        "middle": [],
                        "last": "Dumais",
                        "suffix": ""
                    },
                    {
                        "first": "Thomas",
                        "middle": [
                            "K"
                        ],
                        "last": "Landauer",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [
                            "L"
                        ],
                        "last": "Littman",
                        "suffix": ""
                    }
                ],
                "year": 1997,
                "venue": "Proceedings of the SIGIR Workshop on Cross-Lingual Information Retrieval",
                "volume": "",
                "issue": "",
                "pages": "16--23",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Susan Dumais, Thomas K. Landauer and Michael L. Littman. 1997. Automatic cross-linguistic information retrieval using latent semantic indexing. In Proceedings of the SIGIR Workshop on Cross- Lingual Information Retrieval, 16-23.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Sentence-level MT evaluation without reference translations: beyond language modeling",
                "authors": [
                    {
                        "first": "Anthony",
                        "middle": [],
                        "last": "Michael Gamon",
                        "suffix": ""
                    },
                    {
                        "first": "Martine",
                        "middle": [],
                        "last": "Aue",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Smets",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of the 10 th Annual Conference of the European Association for Machine Translation",
                "volume": "",
                "issue": "",
                "pages": "103--111",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Michael Gamon, Anthony Aue and Martine Smets. 2005. Sentence-level MT evaluation without reference translations: beyond language modeling. In Proceedings of the 10 th Annual Conference of the European Association for Machine Translation, 103- 111.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Calculating the singular values and pseudo-inverse of a matrix",
                "authors": [
                    {
                        "first": "G",
                        "middle": [
                            "H"
                        ],
                        "last": "Golub",
                        "suffix": ""
                    },
                    {
                        "first": "W",
                        "middle": [],
                        "last": "Kahan",
                        "suffix": ""
                    }
                ],
                "year": 1965,
                "venue": "Journal of the Society for Industrial and Applied Mathematics: Numerical Analysis",
                "volume": "2",
                "issue": "2",
                "pages": "205--224",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "G. H. Golub and W. Kahan. 1965. Calculating the singular values and pseudo-inverse of a matrix. Journal of the Society for Industrial and Applied Mathematics: Numerical Analysis, 2(2):205-224.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Introduction to Latent Semantic Analysis",
                "authors": [
                    {
                        "first": "Thomas",
                        "middle": [
                            "K"
                        ],
                        "last": "Landauer",
                        "suffix": ""
                    },
                    {
                        "first": "Peter",
                        "middle": [
                            "W"
                        ],
                        "last": "Foltz",
                        "suffix": ""
                    },
                    {
                        "first": "Darrell",
                        "middle": [],
                        "last": "Laham",
                        "suffix": ""
                    }
                ],
                "year": 1998,
                "venue": "Discourse Processes",
                "volume": "25",
                "issue": "",
                "pages": "259--284",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Thomas K. Landauer, Peter W. Foltz and Darrell Laham. 1998. Introduction to Latent Semantic Analysis. Discourse Processes, 25:259-284.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Orange: a method for evaluating automatic evaluation metrics for machine translation",
                "authors": [
                    {
                        "first": "Chin-Yew",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Franz",
                        "middle": [],
                        "last": "Josef",
                        "suffix": ""
                    },
                    {
                        "first": "Och",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proceedings of the 20th international conference on Computational Linguistics",
                "volume": "501",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chin-Yew Lin and Franz Josef Och. 2004. Orange: a method for evaluating automatic evaluation metrics for machine translation. In Proceedings of the 20th international conference on Computational Linguistics, pp 501, Morristown, NJ.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Foundations of Statistical Natural Language Processing",
                "authors": [
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    },
                    {
                        "first": "Hinrich",
                        "middle": [],
                        "last": "Schutze",
                        "suffix": ""
                    }
                ],
                "year": 1999,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Christopher D. Manning and Hinrich Schutze. 1999. Foundations of Statistical Natural Language Processing (Chapter 6). Cambridge, MA: The MIT Press.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "BLEU: a method for automatic evaluation of machine translation",
                "authors": [
                    {
                        "first": "Kishore",
                        "middle": [],
                        "last": "Papineni",
                        "suffix": ""
                    },
                    {
                        "first": "Salim",
                        "middle": [],
                        "last": "Roukos",
                        "suffix": ""
                    },
                    {
                        "first": "Todd",
                        "middle": [],
                        "last": "Ward",
                        "suffix": ""
                    },
                    {
                        "first": "Wei-Jung",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Proceedings of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "311--318",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kishore Papineni, Salim Roukos, Todd Ward and Wei- Jung Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the Association for Computational Linguistics, 311- 318.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Training a sentence-level machine translation confidence measure",
                "authors": [
                    {
                        "first": "B",
                        "middle": [],
                        "last": "Christopher",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Quirk",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proceedings of the 4 th International Conference on Language Resources and Evaluation",
                "volume": "",
                "issue": "",
                "pages": "825--828",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Christopher B. Quirk. 2004. Training a sentence-level machine translation confidence measure. In Proceedings of the 4 th International Conference on Language Resources and Evaluation, 825-828.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "The back-translation score: automatic MT evaluation at the sentences level without reference translations",
                "authors": [
                    {
                        "first": "Reinhard",
                        "middle": [],
                        "last": "Rapp",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of the ACL-IJCNLP",
                "volume": "",
                "issue": "",
                "pages": "133--136",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Reinhard Rapp. 2009. The back-translation score: automatic MT evaluation at the sentences level without reference translations. In Proceedings of the ACL-IJCNLP, 133-136.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "A vector space model for automatic indexing",
                "authors": [
                    {
                        "first": "Gerard",
                        "middle": [
                            "M"
                        ],
                        "last": "Salton",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [
                            "K"
                        ],
                        "last": "Wong",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [
                            "S"
                        ],
                        "last": "Yang",
                        "suffix": ""
                    }
                ],
                "year": 1975,
                "venue": "Communications of the ACM",
                "volume": "18",
                "issue": "11",
                "pages": "613--620",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Gerard M. Salton, Andrew K. Wong and C. S. Yang. 1975. A vector space model for automatic indexing. Communications of the ACM, 18(11):613-620.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Round-trip translation: what is it good for?",
                "authors": [
                    {
                        "first": "Harold",
                        "middle": [],
                        "last": "Somers",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "proceedings of the Australasian Language Technology Workshop",
                "volume": "",
                "issue": "",
                "pages": "127--133",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Harold Somers. 2005. Round-trip translation: what is it good for? In proceedings of the Australasian Language Technology Workshop, 127-133.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Improving the confidence of machine translation quality estimates",
                "authors": [
                    {
                        "first": "Lucia",
                        "middle": [],
                        "last": "Specia",
                        "suffix": ""
                    },
                    {
                        "first": "Craig",
                        "middle": [],
                        "last": "Saunders",
                        "suffix": ""
                    },
                    {
                        "first": "Marco",
                        "middle": [],
                        "last": "Turchi",
                        "suffix": ""
                    },
                    {
                        "first": "Zhuoran",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Shawe-Taylor",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of MT Summit XII",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Lucia Specia, Craig Saunders, Marco Turchi, Zhuoran Wang and John Shawe-Taylor. 2009. Improving the confidence of machine translation quality estimates. In Proceedings of MT Summit XII. Ottawa, Canada.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "SRILM -an extensible language modeling toolkit",
                "authors": [
                    {
                        "first": "Andreas",
                        "middle": [],
                        "last": "Stolcke",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Proceedings of the International Conference on Spoken Language Processing",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Andreas Stolcke. 2002. SRILM -an extensible language modeling toolkit. In Proceedings of the International Conference on Spoken Language Processing.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "The ARPA MT evaluation methodologies: evolution, lessons and future approaches",
                "authors": [
                    {
                        "first": "John",
                        "middle": [
                            "S"
                        ],
                        "last": "White",
                        "suffix": ""
                    },
                    {
                        "first": "Theresa",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "O'",
                        "middle": [],
                        "last": "Cornell",
                        "suffix": ""
                    },
                    {
                        "first": "Francis O'",
                        "middle": [],
                        "last": "Nava",
                        "suffix": ""
                    }
                ],
                "year": 1994,
                "venue": "Proceedings of the Association for Machine Translation in the Americas",
                "volume": "",
                "issue": "",
                "pages": "193--205",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "John S. White, Theresa O'Cornell and Francis O'Nava. 1994. The ARPA MT evaluation methodologies: evolution, lessons and future approaches. In Proceedings of the Association for Machine Translation in the Americas, 193-205.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "where [X a ;X b ] is the concatenation of the two monolingual term-document matrices X a and X b (of dimensions M a *N and M b *N) corresponding to the available parallel training collection, U ab and V ab are unitary matrices of dimensions M*M and N*N, respectively, and \u03a3 is an M*N diagonal matrix containing the singular values associated to the decomposition.",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table><tr><td colspan=\"5\">Task Domain Src. Tgt. Syst.</td><td>Sent.</td></tr><tr><td>T1</td><td>News</td><td>CZ</td><td>EN</td><td>3</td><td>727</td></tr><tr><td>T2</td><td>News</td><td>EN</td><td>CZ</td><td>2</td><td>806</td></tr><tr><td>T3</td><td>EPPS</td><td>EN</td><td>FR</td><td>7</td><td>577</td></tr><tr><td>T4</td><td>News</td><td>EN</td><td>FR</td><td>8</td><td>561</td></tr><tr><td>T5</td><td>EPPS</td><td colspan=\"2\">EN DE</td><td>6</td><td>924</td></tr><tr><td>T6</td><td>News</td><td colspan=\"2\">EN DE</td><td>6</td><td>892</td></tr><tr><td>T7</td><td>EPPS</td><td>EN</td><td>ES</td><td>6</td><td>703</td></tr><tr><td>T8</td><td>News</td><td>EN</td><td>ES</td><td>7</td><td>832</td></tr><tr><td>T9</td><td>EPPS</td><td>FR</td><td>EN</td><td>7</td><td>624</td></tr><tr><td>T10</td><td>News</td><td>FR</td><td>EN</td><td>7</td><td>740</td></tr><tr><td>T11</td><td>EPPS</td><td colspan=\"2\">DE EN</td><td>7</td><td>949</td></tr><tr><td>T12</td><td>News</td><td colspan=\"2\">DE EN</td><td>5</td><td>939</td></tr><tr><td>T13</td><td>EPPS</td><td>ES</td><td>EN</td><td>8</td><td>812</td></tr><tr><td>T14</td><td>News</td><td>ES</td><td>EN</td><td>7</td><td>668</td></tr></table>",
                "type_str": "table",
                "text": "",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table><tr><td>Metric</td><td>Adequacy</td><td>Fluency</td><td>H Mean</td></tr><tr><td>sBLEU</td><td>0.3089</td><td>0.3361</td><td>0.3486</td></tr><tr><td>NIST</td><td>0.1208</td><td>0.0834</td><td>0.1201</td></tr><tr><td>Meteor</td><td>0.3220</td><td>0.3065</td><td>0.3405</td></tr><tr><td>AM-FM</td><td>0.2142</td><td>0.2256</td><td>0.2406</td></tr><tr><td colspan=\"4\">Table 3: Pearson's correlation coefficients (com-</td></tr><tr><td colspan=\"4\">puted at the sentence level) between automatic</td></tr><tr><td colspan=\"3\">metrics and human-generated scores</td><td/></tr><tr><td colspan=\"4\">As seen from the table, in this case, BLEU and</td></tr><tr><td colspan=\"4\">Meteor are the metrics exhibiting the largest</td></tr><tr><td colspan=\"4\">correlation coefficients, followed by AM-FM and</td></tr><tr><td>NIST.</td><td/><td/><td/></tr></table>",
                "type_str": "table",
                "text": "was used in this case. Again, all correlation coefficients presented in the table are statistically significant with p<0.01.",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table><tr><td>Metric</td><td>Best</td><td>Worst</td><td>Both</td></tr><tr><td>sBLEU</td><td colspan=\"3\">51.08% 54.90% 37.86%</td></tr><tr><td>NIST</td><td colspan=\"3\">49.56% 54.98% 37.36%</td></tr><tr><td>Meteor</td><td colspan=\"3\">52.83% 58.03% 39.85%</td></tr><tr><td>AM-FM</td><td colspan=\"3\">35.25% 41.11% 25.20%</td></tr><tr><td>AM</td><td colspan=\"3\">37.19% 46.92% 28.47%</td></tr><tr><td>FM</td><td colspan=\"3\">34.01% 39.01% 24.11%</td></tr></table>",
                "type_str": "table",
                "text": "for details on the human evaluation task. Percentage of cases in which each automatic metric is able to predict the best, the worst, and both ranked sentence translations Additionally, results for the individual components, AM and FM, are also presented in the table. Notice how the AM component exhibits a better ranking capability than the FM component.",
                "html": null,
                "num": null
            }
        }
    }
}