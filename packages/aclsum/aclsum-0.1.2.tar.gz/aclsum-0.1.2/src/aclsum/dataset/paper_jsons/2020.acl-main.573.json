{
    "paper_id": "2020",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:19:05.719555Z"
    },
    "title": "Continual Relation Learning via Episodic Memory Activation and Reconsolidation",
    "authors": [
        {
            "first": "Xu",
            "middle": [],
            "last": "Han",
            "suffix": "",
            "affiliation": {
                "laboratory": "State Key Lab on Intelligent Technology and Systems",
                "institution": "",
                "location": {}
            },
            "email": ""
        },
        {
            "first": "Yi",
            "middle": [],
            "last": "Dai",
            "suffix": "",
            "affiliation": {
                "laboratory": "State Key Lab on Intelligent Technology and Systems",
                "institution": "",
                "location": {}
            },
            "email": ""
        },
        {
            "first": "Tianyu",
            "middle": [],
            "last": "Gao",
            "suffix": "",
            "affiliation": {
                "laboratory": "State Key Lab on Intelligent Technology and Systems",
                "institution": "",
                "location": {}
            },
            "email": ""
        },
        {
            "first": "Yankai",
            "middle": [],
            "last": "Lin",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Tencent Inc",
                "location": {
                    "addrLine": "WeChat AI",
                    "country": "China"
                }
            },
            "email": "yankailin@tencent.com"
        },
        {
            "first": "Zhiyuan",
            "middle": [],
            "last": "Liu",
            "suffix": "",
            "affiliation": {
                "laboratory": "State Key Lab on Intelligent Technology and Systems",
                "institution": "",
                "location": {}
            },
            "email": ""
        },
        {
            "first": "Peng",
            "middle": [],
            "last": "Li",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Tencent Inc",
                "location": {
                    "addrLine": "WeChat AI",
                    "country": "China"
                }
            },
            "email": ""
        },
        {
            "first": "Maosong",
            "middle": [],
            "last": "Sun",
            "suffix": "",
            "affiliation": {
                "laboratory": "State Key Lab on Intelligent Technology and Systems",
                "institution": "",
                "location": {}
            },
            "email": ""
        },
        {
            "first": "Jie",
            "middle": [],
            "last": "Zhou",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Tencent Inc",
                "location": {
                    "addrLine": "WeChat AI",
                    "country": "China"
                }
            },
            "email": ""
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Continual relation learning aims to continually train a model on new data to learn incessantly emerging novel relations while avoiding catastrophically forgetting old relations. Some pioneering work has proved that storing a handful of historical relation examples in episodic memory and replaying them in subsequent training is an effective solution for such a challenging problem. However, these memorybased methods usually suffer from overfitting the few memorized examples of old relations, which may gradually cause inevitable confusion among existing relations. Inspired by the mechanism in human long-term memory formation, we introduce episodic memory activation and reconsolidation (EMAR) to continual relation learning. Every time neural models are activated to learn both new and memorized data, EMAR utilizes relation prototypes for memory reconsolidation exercise to keep a stable understanding of old relations. The experimental results show that EMAR could get rid of catastrophically forgetting old relations and outperform the state-of-the-art continual learning models. The code and datasets are released on https://github.com/thunlp/ ContinualRE.",
    "pdf_parse": {
        "paper_id": "2020",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Continual relation learning aims to continually train a model on new data to learn incessantly emerging novel relations while avoiding catastrophically forgetting old relations. Some pioneering work has proved that storing a handful of historical relation examples in episodic memory and replaying them in subsequent training is an effective solution for such a challenging problem. However, these memorybased methods usually suffer from overfitting the few memorized examples of old relations, which may gradually cause inevitable confusion among existing relations. Inspired by the mechanism in human long-term memory formation, we introduce episodic memory activation and reconsolidation (EMAR) to continual relation learning. Every time neural models are activated to learn both new and memorized data, EMAR utilizes relation prototypes for memory reconsolidation exercise to keep a stable understanding of old relations. The experimental results show that EMAR could get rid of catastrophically forgetting old relations and outperform the state-of-the-art continual learning models. The code and datasets are released on https://github.com/thunlp/ ContinualRE.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Relation extraction aims at detecting relations between entities from text, e.g., extracting the relation \"the president of \" from the given sentence \"Newton served as the president of the Royal Society\", which could serve as external resource for various downstream applications (Dong et al., 2015; Xiong et al., 2017; Schlichtkrull et al., 2018) . The conventional RE methods (Riedel et al., 2013; Zeng et al., 2014; Lin et al., 2016) mostly focus on recognizing relations for a fixed pre-defined relation set, and cannot handle rapidly emerging novel relations in the real world.",
                "cite_spans": [
                    {
                        "start": 280,
                        "end": 299,
                        "text": "(Dong et al., 2015;",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 300,
                        "end": 319,
                        "text": "Xiong et al., 2017;",
                        "ref_id": "BIBREF65"
                    },
                    {
                        "start": 320,
                        "end": 347,
                        "text": "Schlichtkrull et al., 2018)",
                        "ref_id": "BIBREF55"
                    },
                    {
                        "start": 378,
                        "end": 399,
                        "text": "(Riedel et al., 2013;",
                        "ref_id": "BIBREF48"
                    },
                    {
                        "start": 400,
                        "end": 418,
                        "text": "Zeng et al., 2014;",
                        "ref_id": "BIBREF72"
                    },
                    {
                        "start": 419,
                        "end": 436,
                        "text": "Lin et al., 2016)",
                        "ref_id": "BIBREF30"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Some researchers therefore explore to detect and learn incessantly emerging relations in an open scenario. As shown in Figure 1 , their efforts can be formulated into a two-step pipeline:",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 126,
                        "end": 127,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "(1) Open Relation Learning extracts phrases and arguments to construct patterns of specific relations, and then discovers unseen relation types by clustering patterns, and finally expands sufficient examples of new relation types from large-scale textual corpora; (2) Continual Relation Learning continually uses those expanded examples of new relations to train an effective classifier. The classifier is trained on a sequence of tasks for handling both existing and novel relations, where each task has its own relation set. Although continual relation learning is vital for learning emerging relations, there are rare explorations for this field.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "A straightforward solution is to store all historical data and re-train models every time new relations and examples come in. Nevertheless, it is computationally expensive since relations are in sustainable growth. Moreover, the huge example number of each relation makes frequently mixing new and old examples become infeasible in the real world. Therefore, storing all data is not practical in continual relation learning. In view of this, the recent preliminary work (Wang et al., 2019) indicates that the main challenge of continual relation learning is the catastrophic forgetting problem, i.e., it is hard to learn new relations and meanwhile avoid forgetting old relations, considering memorizing all the data is almost impossible. Recent work (Shin et al., 2017; Kemker and Kanan, 2018; Chaudhry et al., 2019) has shown that the memory-based approaches, maintaining episodic memory to save a few training examples in old tasks and re-training memorized examples during training new tasks, are one of the most effective solutions to the catastrophic forgetting problem, especially for continual learning in NLP scenarios (Wang et al., 2019; d'Autume et al., 2019) . However, existing memory-based models still suffer from an overfitting problem: when adapting them for continual relation learning, they may frequently change feature distribution of old relations, gradually overfit a few examples in memory, and finally become confused among old relations after long-term training.",
                "cite_spans": [
                    {
                        "start": 470,
                        "end": 489,
                        "text": "(Wang et al., 2019)",
                        "ref_id": "BIBREF63"
                    },
                    {
                        "start": 751,
                        "end": 770,
                        "text": "(Shin et al., 2017;",
                        "ref_id": "BIBREF56"
                    },
                    {
                        "start": 771,
                        "end": 794,
                        "text": "Kemker and Kanan, 2018;",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 795,
                        "end": 817,
                        "text": "Chaudhry et al., 2019)",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 1128,
                        "end": 1147,
                        "text": "(Wang et al., 2019;",
                        "ref_id": "BIBREF63"
                    },
                    {
                        "start": 1148,
                        "end": 1170,
                        "text": "d'Autume et al., 2019)",
                        "ref_id": "BIBREF13"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In fact, these memory-based methods are similar to long-term memory model of mammalian memory in neuroscience (McClelland et al., 1995; Bontempi et al., 1999) . Although researchers in neuroscience are not clear about secrets inside the human brain, they reach a consensus that the formation of long-term memory relies on continually replaying and consolidating information (Tononi and Cirelli, 2006; Boyce et al., 2016; Yang et al., 2014) , corresponding to the episodic memory and memory replay in continual learning models. Yet later work (Nader et al., 2000; Lee et al., 2004; Alberini, 2005) in neuroscience indicates that reactivation of consolidated memory triggers a reconsolidation stage to continually maintain memory, and memory is easy to be changed or erased in this stage. To apply some reconsolidation exercises can help memory go through this stage and keep long-term memory stable. Intuitively, the ex-isting memory-based models seem like continual memory activation without reconsolidation exercises, and thus become sensitive and volatile.",
                "cite_spans": [
                    {
                        "start": 110,
                        "end": 135,
                        "text": "(McClelland et al., 1995;",
                        "ref_id": "BIBREF38"
                    },
                    {
                        "start": 136,
                        "end": 158,
                        "text": "Bontempi et al., 1999)",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 374,
                        "end": 400,
                        "text": "(Tononi and Cirelli, 2006;",
                        "ref_id": "BIBREF62"
                    },
                    {
                        "start": 401,
                        "end": 420,
                        "text": "Boyce et al., 2016;",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 421,
                        "end": 439,
                        "text": "Yang et al., 2014)",
                        "ref_id": "BIBREF67"
                    },
                    {
                        "start": 542,
                        "end": 562,
                        "text": "(Nader et al., 2000;",
                        "ref_id": "BIBREF41"
                    },
                    {
                        "start": 563,
                        "end": 580,
                        "text": "Lee et al., 2004;",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 581,
                        "end": 596,
                        "text": "Alberini, 2005)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Inspired by the reconsolidation mechanism in human long-term memory formation, we introduce episodic memory activation and reconsolidation (EMAR) to continual relation learning in this paper. More specifically, when training models on new relations and their examples, we first adopt memory replay to activate neural models on examples of both new relations and memory, and then utilize a special reconsolidation module to let models avoid excessively changing and erasing feature distribution of old relations. As the core of relation learning is to grasp relation prototypes rather than rote memorization of relation examples, our reconsolidation module requires models to be able to distinguish old relation prototypes after each time memory is replayed and activated. As compared with pioneering explorations to improve episodic memory replay (Chaudhry et al., 2019; Wang et al., 2019) , with toughly keeping feature distribution of old relations invariant, EMAR is more flexible in feature spaces and powerful in remembering relation prototypes.",
                "cite_spans": [
                    {
                        "start": 847,
                        "end": 870,
                        "text": "(Chaudhry et al., 2019;",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 871,
                        "end": 889,
                        "text": "Wang et al., 2019)",
                        "ref_id": "BIBREF63"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "We conduct sufficient experiments on several RE datasets, and the results show that EMAR effectively alleviates the catastrophic forgetting problem and significantly outperforms the stateof-the-art continual learning models. Further experiments and analyses indicate the reasons for the effectiveness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019) , focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning.",
                "cite_spans": [
                    {
                        "start": 62,
                        "end": 84,
                        "text": "(Zelenko et al., 2003;",
                        "ref_id": "BIBREF70"
                    },
                    {
                        "start": 85,
                        "end": 103,
                        "text": "Zhou et al., 2005;",
                        "ref_id": "BIBREF75"
                    },
                    {
                        "start": 104,
                        "end": 125,
                        "text": "Gormley et al., 2015;",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 126,
                        "end": 146,
                        "text": "Socher et al., 2012;",
                        "ref_id": "BIBREF58"
                    },
                    {
                        "start": 147,
                        "end": 164,
                        "text": "Liu et al., 2013;",
                        "ref_id": "BIBREF31"
                    },
                    {
                        "start": 165,
                        "end": 183,
                        "text": "Zeng et al., 2014;",
                        "ref_id": "BIBREF72"
                    },
                    {
                        "start": 184,
                        "end": 210,
                        "text": "Nguyen and Grishman, 2015;",
                        "ref_id": "BIBREF43"
                    },
                    {
                        "start": 211,
                        "end": 235,
                        "text": "dos Santos et al., 2015;",
                        "ref_id": "BIBREF54"
                    },
                    {
                        "start": 236,
                        "end": 252,
                        "text": "Xu et al., 2015;",
                        "ref_id": "BIBREF66"
                    },
                    {
                        "start": 253,
                        "end": 270,
                        "text": "Liu et al., 2015;",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 271,
                        "end": 293,
                        "text": "Miwa and Bansal, 2016)",
                        "ref_id": "BIBREF40"
                    },
                    {
                        "start": 326,
                        "end": 352,
                        "text": "(Bunescu and Mooney, 2007;",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 353,
                        "end": 372,
                        "text": "Mintz et al., 2009;",
                        "ref_id": "BIBREF39"
                    },
                    {
                        "start": 373,
                        "end": 393,
                        "text": "Riedel et al., 2010;",
                        "ref_id": "BIBREF47"
                    },
                    {
                        "start": 394,
                        "end": 416,
                        "text": "Hoffmann et al., 2011;",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 417,
                        "end": 435,
                        "text": "Zeng et al., 2015;",
                        "ref_id": "BIBREF71"
                    },
                    {
                        "start": 436,
                        "end": 453,
                        "text": "Lin et al., 2016;",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 454,
                        "end": 472,
                        "text": "Han et al., 2018a;",
                        "ref_id": null
                    },
                    {
                        "start": 473,
                        "end": 501,
                        "text": "Baldini Soares et al., 2019)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018) , relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016) , relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019) , and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020) . However, for continual relation learning, there are still only some preliminary explorations for it. Following continual learning setting 1 (Ring, 1994; Thrun and Pratt, 2012) in machine learning, Wang et al. (2019) first explore continual relation learning.",
                "cite_spans": [
                    {
                        "start": 86,
                        "end": 106,
                        "text": "(Banko et al., 2007;",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 107,
                        "end": 126,
                        "text": "Fader et al., 2011;",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 127,
                        "end": 147,
                        "text": "Mausam et al., 2012;",
                        "ref_id": "BIBREF36"
                    },
                    {
                        "start": 148,
                        "end": 176,
                        "text": "Del Corro and Gemulla, 2013;",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 177,
                        "end": 197,
                        "text": "Angeli et al., 2015;",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 198,
                        "end": 219,
                        "text": "Petroni et al., 2015;",
                        "ref_id": "BIBREF45"
                    },
                    {
                        "start": 220,
                        "end": 246,
                        "text": "Stanovsky and Dagan, 2016;",
                        "ref_id": "BIBREF59"
                    },
                    {
                        "start": 247,
                        "end": 260,
                        "text": "Mausam, 2016;",
                        "ref_id": "BIBREF37"
                    },
                    {
                        "start": 261,
                        "end": 278,
                        "text": "Cui et al., 2018)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 300,
                        "end": 318,
                        "text": "(Yao et al., 2011;",
                        "ref_id": "BIBREF68"
                    },
                    {
                        "start": 319,
                        "end": 348,
                        "text": "Marcheggiani and Titov, 2016)",
                        "ref_id": "BIBREF35"
                    },
                    {
                        "start": 371,
                        "end": 398,
                        "text": "(Shinyama and Sekine, 2006;",
                        "ref_id": "BIBREF57"
                    },
                    {
                        "start": 399,
                        "end": 420,
                        "text": "Elsahar et al., 2017;",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 421,
                        "end": 437,
                        "text": "Wu et al., 2019)",
                        "ref_id": "BIBREF64"
                    },
                    {
                        "start": 460,
                        "end": 481,
                        "text": "(Riloff et al., 1999;",
                        "ref_id": "BIBREF49"
                    },
                    {
                        "start": 482,
                        "end": 503,
                        "text": "Etzioni et al., 2005;",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 504,
                        "end": 535,
                        "text": "Pantel and Pennacchiotti, 2006;",
                        "ref_id": null
                    },
                    {
                        "start": 536,
                        "end": 564,
                        "text": "Rozenfeld and Feldman, 2008;",
                        "ref_id": "BIBREF52"
                    },
                    {
                        "start": 565,
                        "end": 588,
                        "text": "Nakashole et al., 2011;",
                        "ref_id": "BIBREF42"
                    },
                    {
                        "start": 589,
                        "end": 606,
                        "text": "Zhu et al., 2009;",
                        "ref_id": "BIBREF76"
                    },
                    {
                        "start": 607,
                        "end": 624,
                        "text": "Gao et al., 2020)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 767,
                        "end": 779,
                        "text": "(Ring, 1994;",
                        "ref_id": null
                    },
                    {
                        "start": 780,
                        "end": 802,
                        "text": "Thrun and Pratt, 2012)",
                        "ref_id": "BIBREF61"
                    },
                    {
                        "start": 824,
                        "end": 842,
                        "text": "Wang et al. (2019)",
                        "ref_id": "BIBREF63"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "Existing continual learning methods focus on three research directions: (1) consolidation-based methods (Kirkpatrick et al., 2017; Zenke et al., 2017; Li and Hoiem, 2017; Liu et al., 2018; Ritter et al., 2018) which consolidate the model parameters important to previous tasks and reduce their learning weights; (2) dynamic architecture methods (Chen et al., 2016; Rusu et al., 2016; Fernando et al., 2017) which dynamically expand model architectures to learn new tasks and ef-1 Some work names it lifelong or incremental learning.",
                "cite_spans": [
                    {
                        "start": 104,
                        "end": 130,
                        "text": "(Kirkpatrick et al., 2017;",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 131,
                        "end": 150,
                        "text": "Zenke et al., 2017;",
                        "ref_id": "BIBREF73"
                    },
                    {
                        "start": 151,
                        "end": 170,
                        "text": "Li and Hoiem, 2017;",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 171,
                        "end": 188,
                        "text": "Liu et al., 2018;",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 189,
                        "end": 209,
                        "text": "Ritter et al., 2018)",
                        "ref_id": "BIBREF51"
                    },
                    {
                        "start": 345,
                        "end": 364,
                        "text": "(Chen et al., 2016;",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 365,
                        "end": 383,
                        "text": "Rusu et al., 2016;",
                        "ref_id": null
                    },
                    {
                        "start": 384,
                        "end": 406,
                        "text": "Fernando et al., 2017)",
                        "ref_id": "BIBREF20"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "fectively prevent forgetting old tasks. Yet model size growing dramatically with increasing tasks makes these methods unsuitable for NLP applications; (3) memory-based methods (Lopez-Paz and Ranzato, 2017; Rebuffi et al., 2017; Shin et al., 2017; Kemker and Kanan, 2018; Aljundi et al., 2018; Chaudhry et al., 2019 ) remember a few examples in old tasks and continually learn them with emerging new tasks to alleviate catastrophic forgetting. Among these methods, the memorybased methods have been proven to be the most promising for NLP tasks, including both relation learning (Wang et al., 2019) and other NLP tasks (d'Autume et al., 2019; Sun et al., 2019) .",
                "cite_spans": [
                    {
                        "start": 176,
                        "end": 205,
                        "text": "(Lopez-Paz and Ranzato, 2017;",
                        "ref_id": "BIBREF34"
                    },
                    {
                        "start": 206,
                        "end": 227,
                        "text": "Rebuffi et al., 2017;",
                        "ref_id": "BIBREF46"
                    },
                    {
                        "start": 228,
                        "end": 246,
                        "text": "Shin et al., 2017;",
                        "ref_id": "BIBREF56"
                    },
                    {
                        "start": 247,
                        "end": 270,
                        "text": "Kemker and Kanan, 2018;",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 271,
                        "end": 292,
                        "text": "Aljundi et al., 2018;",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 293,
                        "end": 314,
                        "text": "Chaudhry et al., 2019",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 578,
                        "end": 597,
                        "text": "(Wang et al., 2019)",
                        "ref_id": "BIBREF63"
                    },
                    {
                        "start": 618,
                        "end": 641,
                        "text": "(d'Autume et al., 2019;",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 642,
                        "end": 659,
                        "text": "Sun et al., 2019)",
                        "ref_id": "BIBREF60"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "Inspired by reconsolidation in human memory formation, we introduce episodic memory activation and reconsolidation (EMAR) to alleviate the overfitting problem of the existing memory-based methods and better learn relations continually.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "Continual relation learning trains models on a sequence of tasks, where the k-th task has its own training set T k , validation set V k , and query set",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Task Definition and Overall Framework",
                "sec_num": "3.1"
            },
            {
                "text": "Q k . Each set of the k-th task, e.g. T k = {(x T k 1 , y T k 1 ), . . . , (x T k N , y T k N )}",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Task Definition and Overall Framework",
                "sec_num": "3.1"
            },
            {
                "text": ", consists of a series of examples and their corresponding relation labels, where N is the example number of T k . Each example x T k i and its label y",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Task Definition and Overall Framework",
                "sec_num": "3.1"
            },
            {
                "text": "T k i indicate that x T k i can express the relation y T k i \u2208 R k , where R k is the re- lation set of the k-th task.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Task Definition and Overall Framework",
                "sec_num": "3.1"
            },
            {
                "text": "More specifically, models will be trained on T k at the k-th step to learn the new relations in R k . As relations are emerging and accumulating, continual relation learning requires models to perform well on both the k-th task and previous k -1 tasks. Hence, after training on T k , models will be evaluated on Qk = k i=1 Q i , and required to classify each query example into the all known relation set Rk = k i=1 R i . Therefore, the evaluation will be more and more difficult with the growth of tasks.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Task Definition and Overall Framework",
                "sec_num": "3.1"
            },
            {
                "text": "For handling the catastrophic forgetting in continual relation learning, an episodic memory module M = {M 1 , M 2 , . . .} is set to store a few examples of historical tasks, each memory module Besides, we will introduce how to train models as well as predict relations for query examples in Section 3.6. As the example encoder is used in all other steps, we first introduce it in Section 3.2 before other steps.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Task Definition and Overall Framework",
                "sec_num": "3.1"
            },
            {
                "text": "M k = {(x M k 1 , y M k 1 ), . . . , (x M k B , y M k B )}",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Task Definition and Overall Framework",
                "sec_num": "3.1"
            },
            {
                "text": "Given an example x, we adopt an example encoder to encode its semantic features for detecting and learning relations. To be specific, we first tokenize the given example into several tokens, and then input the tokenized tokens into neural networks to compute its corresponding embedding. As extracting relations from sentences is related to those entities mentioned in sentences, we thus add special tokens into the tokenized tokens to indicate the beginning and ending positions of those entities. For simplicity, we denote such an example encoding operation as the following equation,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Example Encoder",
                "sec_num": "3.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "x = f (x),",
                        "eq_num": "(1)"
                    }
                ],
                "section": "Example Encoder",
                "sec_num": "3.2"
            },
            {
                "text": "where x \u2208 R d is the semantic embedding of x, and d is the embedding dimension. Note that the encoder is not our focus in this paper, we select bidirectional long short-term memory (BiL-STM) (Bengio et al., 1994) as representative encoders to encode examples. In fact, other neural text encoders like convolutional neural networks (Zeng et al., 2014) and pre-trained language models (Devlin et al., 2019) can also be adopted as example encoders.",
                "cite_spans": [
                    {
                        "start": 191,
                        "end": 212,
                        "text": "(Bengio et al., 1994)",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 331,
                        "end": 350,
                        "text": "(Zeng et al., 2014)",
                        "ref_id": "BIBREF72"
                    },
                    {
                        "start": 383,
                        "end": 404,
                        "text": "(Devlin et al., 2019)",
                        "ref_id": "BIBREF15"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Example Encoder",
                "sec_num": "3.2"
            },
            {
                "text": "When the k-th task is arising, the example encoder has not touched any examples of new relations before, and cannot extract the semantic features of them. Hence, we first fine-tune the example encoder on",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning for New Tasks",
                "sec_num": "3.3"
            },
            {
                "text": "T k = {(x T k 1 , y T k 1 ), . . . , (x T k N , y T k N )} to grasp new relation patterns in R k .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning for New Tasks",
                "sec_num": "3.3"
            },
            {
                "text": "The loss function of learning the k-th task is as follows,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning for New Tasks",
                "sec_num": "3.3"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "L(\u03b8) = - N i=1 | Rk | j=1 \u03b4 y T k i =r j \u00d7 log exp(g(f (x T k i ), r j )) | Rk | l=1 exp(g(f (x T k i ), r l )) ,",
                        "eq_num": "(2)"
                    }
                ],
                "section": "Learning for New Tasks",
                "sec_num": "3.3"
            },
            {
                "text": "where r j is the embedding of the j-th relation r j \u2208 Rk in the all known relation set Rk , g(\u2022, \u2022) is the function to compute similarities between embeddings (e.g. cosine similarity), and \u03b8 is the parameters that can be optimized, including the example encoder parameters and relation embed-",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning for New Tasks",
                "sec_num": "3.3"
            },
            {
                "text": "dings. If y T k i equals r j , \u03b4 y T k i =r j = 1, otherwise \u03b4 y T k i =r j = 0.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning for New Tasks",
                "sec_num": "3.3"
            },
            {
                "text": "For each new relation, we first randomly initialize its embedding and then optimize Eq. (2).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning for New Tasks",
                "sec_num": "3.3"
            },
            {
                "text": "After several epochs of learning for new tasks with Eq. ( 2 examples. If a relation does not have enough examples to fill its allocated memory, this memory will be re-allocated for other relations.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Selecting Examples for Memory",
                "sec_num": "3.4"
            },
            {
                "text": "For each relation, we also use K-Means to cluster its own examples, and the number of current clusters is its allocated example number in the memory. For each cluster, we select the example closest to the cluster centroid, and store this example into the memory M k .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Selecting Examples for Memory",
                "sec_num": "3.4"
            },
            {
                "text": "After fine-tuning the example encoder for T k and selecting informative examples for M k , we iteratively adopt computing prototypes, memory replay and activation, and memory reconsolidation to strengthen identifying new relation patterns and keep distinguishing old relation patterns.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Replay, Activation and Reconsolidation",
                "sec_num": "3.5"
            },
            {
                "text": "By combining all examples in the episodic memory, we achieve the whole memory set Mk = k i=1 M i . As we aim to grasp relation prototypes rather than rote memorization of relation examples, for each known relation r i \u2208 Rk , we sample a prototype set P i = {x P i 1 , . . . , x P i |P i | }, where each example x P i i comes from Mk and its label equals r i , and compute its prototype embedding,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Computing Prototypes",
                "sec_num": null
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "p i = |P i | j=1 f (x P i j ) |P i | ,",
                        "eq_num": "(3)"
                    }
                ],
                "section": "Computing Prototypes",
                "sec_num": null
            },
            {
                "text": "where p i is the relation prototype embedding of r i \u2208 Rk .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Computing Prototypes",
                "sec_num": null
            },
            {
                "text": "In memory replay and activation, the whole memory set Mk and the k-th training set T k will be combined into an activation set",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Memory Replay and Activation",
                "sec_num": null
            },
            {
                "text": "A k = Mk \u222a T k = {(x A k 1 , y A k 1 ), . . . , (x A k M , y A k M )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Memory Replay and Activation",
                "sec_num": null
            },
            {
                "text": "} to continually activate models to learn new relations and remember old relations, where M is the total example number of both Mk and T k . The loss function is",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Memory Replay and Activation",
                "sec_num": null
            },
            {
                "text": "L A (\u03b8) = - M i=1 | Rk | j=1 \u03b4 y A k i =r j \u00d7 log exp(g(f (x A k i ), r j )) | Rk | l=1 exp(g(f (x A k i ), r l ))",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Memory Replay and Activation",
                "sec_num": null
            },
            {
                "text": ".",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Memory Replay and Activation",
                "sec_num": null
            },
            {
                "text": "(4)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Memory Replay and Activation",
                "sec_num": null
            },
            {
                "text": "As we mentioned before, just conducting memory replay and activation will lead to the overfitting problem, and in the end, models only remember a handful of memorized examples after long-term training. Meanwhile, the core of learning relations is to grasp relation prototypes rather than rote memorization of relation examples. Hence, every time conducting memory replay and activation to grasp both new and old relations, we adopt a memory reconsolidation module to strengthen this process, which seems like conducting reconsolidation exercises to keep long-term memory stable in the human brain. For each known relation r i \u2208 Rk , we sample its instance set I i = {x I i 1 , . . . , x I i |I i | } as is similar to sampling P i , where each example x I i i \u2208 I i also comes from Mk and its label equals r i . The loss function of the memory reconsolidation is",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Memory Reconsolidation",
                "sec_num": null
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "L R (\u03b8) = - | Rk | i=1 |I i | j=1 log exp(g(f (x I i j ), p i )) | Rk | l=1 exp(g(f (x I i j ), p l )) ,",
                        "eq_num": "(5)"
                    }
                ],
                "section": "Memory Reconsolidation",
                "sec_num": null
            },
            {
                "text": "where p l is the relation prototype embedding of r l \u2208 Rk computed by Eq. (3). ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Memory Reconsolidation",
                "sec_num": null
            },
            {
                "text": "7: Mk \u2190 Mk-1 \u222a M k 8: A k \u2190 Mk \u222a T k 9: for i \u2190 1 to epoch2 do 10:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Memory Reconsolidation",
                "sec_num": null
            },
            {
                "text": "for relation rj \u2208 Rk do 11:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Memory Reconsolidation",
                "sec_num": null
            },
            {
                "text": "Sample Pj from Mk and compute its relation prototype embedding pj 12:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Memory Reconsolidation",
                "sec_num": null
            },
            {
                "text": "end for 13:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Memory Reconsolidation",
                "sec_num": null
            },
            {
                "text": "for j \u2190 1 to iter1 do 14:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Memory Reconsolidation",
                "sec_num": null
            },
            {
                "text": "Update \u03b8 with \u2207L A on A k 15:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Memory Reconsolidation",
                "sec_num": null
            },
            {
                "text": "end for 16:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Memory Reconsolidation",
                "sec_num": null
            },
            {
                "text": "for j \u2190 1 to iter2 do 17:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Memory Reconsolidation",
                "sec_num": null
            },
            {
                "text": "Sample Ii from Mk for each known relation ri 18:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Memory Reconsolidation",
                "sec_num": null
            },
            {
                "text": "Update \u03b8 with \u2207L R on {I1, . . . ,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Memory Reconsolidation",
                "sec_num": null
            },
            {
                "text": "I | Rk | } 19:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Memory Reconsolidation",
                "sec_num": null
            },
            {
                "text": "end for 20: end for",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Memory Reconsolidation",
                "sec_num": null
            },
            {
                "text": "For training the k-th task, we first use L(\u03b8) to optimize parameters for several epochs. Then, we select examples for the memory, and iteratively optimize parameters with L A (\u03b8) and L R (\u03b8) until convergence. More details about the training process are shown in Algorithm 1.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training and Prediction",
                "sec_num": "3.6"
            },
            {
                "text": "After finishing the k-th task, for each known relation r i \u2208 Rk , we collect all its memorized examples E i = {x E i 1 , . . . , x E i S } in the whole memory Mk , where S is the example number of r i in the memory, and compute final relation prototype for prediction,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training and Prediction",
                "sec_num": "3.6"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "pi = r i + S j=1 f (x E i j ) 1 + S ,",
                        "eq_num": "(6)"
                    }
                ],
                "section": "Training and Prediction",
                "sec_num": "3.6"
            },
            {
                "text": "where r i is the relation embedding of r i used in Eq. ( 2) and Eq. ( 4). For each query example x in Qk , we define its score function for the relation r i :",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training and Prediction",
                "sec_num": "3.6"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "s(x, r i ) = g(f (x), pi ), (",
                        "eq_num": "7"
                    }
                ],
                "section": "Training and Prediction",
                "sec_num": "3.6"
            },
            {
                "text": ")",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training and Prediction",
                "sec_num": "3.6"
            },
            {
                "text": "where pi is the final prototype of the relation r i computed by Eq. ( 6). Finally, the prediction y for the query x is calculated by: y = arg max ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training and Prediction",
                "sec_num": "3.6"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "r i \u2208 Rk s(x, r i ).",
                        "eq_num": "(8)"
                    }
                ],
                "section": "Training and Prediction",
                "sec_num": "3.6"
            },
            {
                "text": "We carry out our experiments on three benchmark datasets:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Datasets",
                "sec_num": "4.1"
            },
            {
                "text": "(1) FewRel (Han et al., 2018b) . FewRel is a RE dataset that contains 80 relations and 56, 000 examples in total. We follow the settings from Wang et al. (2019) to make FewRel a continual learning benchmark: FewRel is split into 10 clusters of relations, leading to 10 tasks and each relation just belongs to only one task. Each example in these tasks is related to a relation and a candidate set of 10 randomly selected relations for evaluation.",
                "cite_spans": [
                    {
                        "start": 11,
                        "end": 30,
                        "text": "(Han et al., 2018b)",
                        "ref_id": null
                    },
                    {
                        "start": 142,
                        "end": 160,
                        "text": "Wang et al. (2019)",
                        "ref_id": "BIBREF63"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Datasets",
                "sec_num": "4.1"
            },
            {
                "text": "(2) SimpleQuestions (Bordes et al., 2015) . SimpleQuestions (SimpleQ) is a knowledge base question answering dataset that contains 108, 442 questions, and Yu et al. (2017) construct a relation detection dataset based on it, where questions are linked to relations. Like FewRel, we follow the settings from Wang et al. (2019) : SimpleQ is split into 20 clusters of relations to construct 20 tasks. As each question in SimpleQ has been related to a candidate set for evaluation, we do not randomly sample candidate sets again for SimpleQ.",
                "cite_spans": [
                    {
                        "start": 20,
                        "end": 41,
                        "text": "(Bordes et al., 2015)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 155,
                        "end": 171,
                        "text": "Yu et al. (2017)",
                        "ref_id": "BIBREF69"
                    },
                    {
                        "start": 306,
                        "end": 324,
                        "text": "Wang et al. (2019)",
                        "ref_id": "BIBREF63"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Datasets",
                "sec_num": "4.1"
            },
            {
                "text": "(3) TACRED (Zhang et al., 2017) . TACRED is a RE dataset that contains 42 relations and 21, 784 examples. Similar to FewRel, we also split TA-CRED into 10 clusters of relations to construct 10 tasks, and randomly sample candidate relation sets consisting of 10 relations for each examples. Considering there is a special relation \"n/a\" (not available) in TACRED, we filter out these examples with the relation \"n/a\" and use the left examples for continual TACRED. Table 2 : Accuracy (%) of models with different memory sizes. All the results come from our implemented models.",
                "cite_spans": [
                    {
                        "start": 11,
                        "end": 31,
                        "text": "(Zhang et al., 2017)",
                        "ref_id": "BIBREF74"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 470,
                        "end": 471,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Datasets",
                "sec_num": "4.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "W A W A W A W A W A W A W A W A W A EWC - - - -",
                        "eq_num": "21"
                    }
                ],
                "section": "Datasets",
                "sec_num": "4.1"
            },
            {
                "text": "We use two evaluation settings including whole performance, which calculates the accuracy on the whole test set of all tasks, and average performance, which averages the accuracy on all seen tasks. After having seen all tasks, we use the final whole performance and average performance to evaluate the overall performance of continual relation learning. As average performance highlights the performance of handling catastrophic problem, and thus it is the main metric to evaluate models.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Settings",
                "sec_num": "4.2"
            },
            {
                "text": "As the task sequence has influence on final model performance, we implement the baseline models by ourselves based on the toolkit 2 released by Wang et al. (2019) . For fair comparison, we unify the random seeds in our experiments completely consistent with the seeds in Wang et al. (2019) , so that the task sequence can be completely consistent with Wang et al. (2019) . For other settings, such as hidden embedding dimension and pre-trained input embeddings, we also follow the settings in Wang et al. (2019) .",
                "cite_spans": [
                    {
                        "start": 144,
                        "end": 162,
                        "text": "Wang et al. (2019)",
                        "ref_id": "BIBREF63"
                    },
                    {
                        "start": 271,
                        "end": 289,
                        "text": "Wang et al. (2019)",
                        "ref_id": "BIBREF63"
                    },
                    {
                        "start": 352,
                        "end": 370,
                        "text": "Wang et al. (2019)",
                        "ref_id": "BIBREF63"
                    },
                    {
                        "start": 493,
                        "end": 511,
                        "text": "Wang et al. (2019)",
                        "ref_id": "BIBREF63"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Settings",
                "sec_num": "4.2"
            },
            {
                "text": "2 https://github.com/hongwang600/ Lifelong_Relation_Detection",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Settings",
                "sec_num": "4.2"
            },
            {
                "text": "We evaluate our model and several baselines on the benchmarks, and select two theoretical models to measure the lower and upper bounds: (1) Lower Bound, which continually fine-tunes models for each new task without memorizing any historical examples; (2) Upper Bound, which remembers all examples in history and continually re-train models with all data. In fact, this model serves as the ideal upper bound for the performance of continual relation learning; (3) EWC (Kirkpatrick et al., 2017) , which adopts elastic weight consolidation to add special L 2 regularization on parameter changes. Then, EWC uses Fisher information to measure the parameter importance to old tasks, and slow down the update of those parameters important to old tasks; 5) GEM (Lopez-Paz and Ranzato, 2017) , an extension of EMR, which adds a constraint on directions of new gradients to make sure that optimization directions do not conflict with gradients on old tasks; (6) AGEM For each image, we use the support vector machine to acquire its best linear boundary and draw it as the blue line.",
                "cite_spans": [
                    {
                        "start": 467,
                        "end": 493,
                        "text": "(Kirkpatrick et al., 2017)",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 754,
                        "end": 783,
                        "text": "(Lopez-Paz and Ranzato, 2017)",
                        "ref_id": "BIBREF34"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baselines",
                "sec_num": "4.3"
            },
            {
                "text": "Step 4 . (Chaudhry et al., 2019) , the extension of GEM, which takes the gradient on sampled memorized examples from memory as the only constraint on the optimization directions of the current task; (7) EA-EMR (Wang et al., 2019) , which introduces memory replay and embedding aligned mechanism to enhance previous tasks and mitigate the embedding distortion when trained on new tasks. EA-EMR is also an extension of EMR, and the state-of-the-art on continual relation learning.",
                "cite_spans": [
                    {
                        "start": 9,
                        "end": 32,
                        "text": "(Chaudhry et al., 2019)",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 210,
                        "end": 229,
                        "text": "(Wang et al., 2019)",
                        "ref_id": "BIBREF63"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 5,
                        "end": 6,
                        "text": "4",
                        "ref_id": "FIGREF5"
                    }
                ],
                "eq_spans": [],
                "section": "Baselines",
                "sec_num": "4.3"
            },
            {
                "text": "Table 1 shows the overall performance on three benchmarks under two different settings. From the table, we can see that (1) our proposed EMAR significantly outperforms other baselines and achieves state-of-the-arts almost in all settings. On the SimpleQ dataset, the performance of EMAR is close to EA-EMR and EMR. The reason is perhaps that the SimpleQ benchmark is over simple (even the weakest Lower Bound achieves relatively high results close to Upper Bound). On other benchmarks, EMAR outperforms all the baseline models with a large margin, showing the superiority of our proposed episodic memory activation and reconsolidation mechanism.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "1",
                        "ref_id": "TABREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Overall Results",
                "sec_num": "4.4"
            },
            {
                "text": "(2) There is still a huge gap between our model and the upper bound. It indicates there remains lots of things to be explored in continual relation learning.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overall Results",
                "sec_num": "4.4"
            },
            {
                "text": "To further investigate how accuracy changes while learning new tasks, we show the average performance of models at each step in Figure 3 . As shown in the figure, we can observe that: (1) With increasing numbers of tasks, the performance of all the models decreases in some degree. This indicates that catastrophically forgetting old relations is inevitable, and it is indeed one of the major difficulty for continual relation learning. (2) The memory-based methods significantly outperform the consolidation-based method, which demonstrates the memory-based methods could alleviate the problem of catastrophic forgetting to some extent. (3) Our proposed EMAR achieves a much better results compared to state-of-the-art model EA-EMR. It shows the effectiveness of our memory reconsolidation, and further indicates understanding relation prototypes is more important and reasonable than rote memorization of examples.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 135,
                        "end": 136,
                        "text": "3",
                        "ref_id": "FIGREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Overall Results",
                "sec_num": "4.4"
            },
            {
                "text": "Memory size indicates the number of remembered examples for each task. In this section, we investigate the effect of memory size for the performance of baselines and our proposed model. We compare three memory sizes: 10, 25 and 50. As ex-isting work does not report the results with different memory size, we re-implement baseline models by ourselves in this experiment. The results are shown in Table 2 . We can find that: (1) With the increasing memory size, the performance of all models improves respectively, which shows that the memory size is one of the key factor determining the performance of continual relation learning models. (2) On both FewRel and TACRED, our EMAR keeps performing the best under different memory sizes, and even achieves comparable results with other models of larger memory sizes. It indicates adopting relation prototypes in EMAR is a more effective way to utilize memory compared with existing memory-based methods.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 402,
                        "end": 403,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Effect of Memory Size",
                "sec_num": "4.5"
            },
            {
                "text": "To show the effectiveness of prototypes and reconsolidation, we give a case study demonstrating the changing of feature spaces learnt by EA-EMR and EMAR (ours). We sample two relations from the training set and 40 examples per relation from the test set. Then we train EA-EMR and EMAR with the sampled training data respectively and visualize the changes of the sampled 40 instances in the feature spaces at different steps.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Effect of Prototypes and Reconsolidation",
                "sec_num": "4.6"
            },
            {
                "text": "From Figure 4 , we can see that EMAR learns better features of instances after multi-step training: the embedding space of EMAR is more sparse and features from two relations are more distinguishable. On the other hand, the features learnt by EA-EMR become more dense with increasing steps, thus harder to classify.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 12,
                        "end": 13,
                        "text": "4",
                        "ref_id": "FIGREF5"
                    }
                ],
                "eq_spans": [],
                "section": "Effect of Prototypes and Reconsolidation",
                "sec_num": "4.6"
            },
            {
                "text": "This phenomenon is mainly due to the different approaches of constraining features used by EA-EMR and EMAR. The L 2 regularization used in EA-EMR for keeping the instance distribution of old relations leads to higher density in the feature space and smaller distances between different relations after several training steps. On the contrary, EMAR avoids models from forgetting previous relations by relation prototypes. Compared with EA-EMR, using prototypes for reconsolidation is a more flexible constraint, allowing EMAR to utilize larger feature spaces for representing examples and prototypes.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Effect of Prototypes and Reconsolidation",
                "sec_num": "4.6"
            },
            {
                "text": "To quantitatively analyze the case, we use the support vector machine to acquire linear boundaries for each image in Figure 4 and list the classification results in Table 3 . The quantitative results in the table show that embeddings learnt by EMAR achieve better classification performance, which further supports our above observations.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 124,
                        "end": 125,
                        "text": "4",
                        "ref_id": "FIGREF5"
                    },
                    {
                        "start": 171,
                        "end": 172,
                        "text": "3",
                        "ref_id": "TABREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Effect of Prototypes and Reconsolidation",
                "sec_num": "4.6"
            },
            {
                "text": "To alleviate catastrophically forgetting old relations in continual relation learning, we introduce episodic memory activation and reconsolidation (EMAR), inspired by the mechanism in human long-term memory formation. Compared with existing memory-based methods, EMAR requires models to understand the prototypes of old relations rather than to overfit a few specific memorized examples, which can keep better distinction among relations after long-term training. We conduct experiments on three benchmarks in relation extraction and carry out extensive experimental results as well as empirical analyses, showing the effectiveness of EMAR on utilizing memorized examples. For future work, how to combine open relation learning and continual relation learning together to complete the pipeline for emerging relations still remains a problem, and we will continue to work on it.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion and Future Work",
                "sec_num": "5"
            }
        ],
        "back_matter": [
            {
                "text": "This work is supported by the National Key Research and Development Program of China (No. 2018YFB1004503) and the National Natural Science Foundation of China (NSFC No. 61732008, 61772302). Tianyu Gao is supported by 2019 Tencent Rhino-Bird Elite Training Program and Tsinghua University Initiative Scientific Research Program.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgments",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Mechanisms of memory stabilization: are consolidation and reconsolidation similar or distinct processes? Trends in neurosciences",
                "authors": [
                    {
                        "first": "Cristina",
                        "middle": [
                            "M"
                        ],
                        "last": "Alberini",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "",
                "volume": "28",
                "issue": "",
                "pages": "51--56",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Cristina M Alberini. 2005. Mechanisms of memory stabilization: are consolidation and reconsolidation similar or distinct processes? Trends in neuro- sciences, 28(1):51-56.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars",
                "authors": [
                    {
                        "first": "Rahaf",
                        "middle": [],
                        "last": "Aljundi",
                        "suffix": ""
                    },
                    {
                        "first": "Francesca",
                        "middle": [],
                        "last": "Babiloni",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of ECCV",
                "volume": "",
                "issue": "",
                "pages": "139--154",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Rahaf Aljundi, Francesca Babiloni, Mohamed Elho- seiny, Marcus Rohrbach, and Tinne Tuytelaars. 2018. Memory aware synapses: Learning what (not) to forget. In Proceedings of ECCV, pages 139-154.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Leveraging linguistic structure for open domain information extraction",
                "authors": [
                    {
                        "first": "Gabor",
                        "middle": [],
                        "last": "Angeli",
                        "suffix": ""
                    },
                    {
                        "first": "Melvin",
                        "middle": [
                            "Jose"
                        ],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Johnson",
                        "middle": [],
                        "last": "Premkumar",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of ACL-IJCNLP",
                "volume": "",
                "issue": "",
                "pages": "344--354",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Gabor Angeli, Melvin Jose Johnson Premkumar, and Christopher D Manning. 2015. Leveraging linguis- tic structure for open domain information extraction. In Proceedings of ACL-IJCNLP, pages 344-354.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Matching the blanks: Distributional similarity for relation learning",
                "authors": [
                    {
                        "first": "Baldini",
                        "middle": [],
                        "last": "Livio",
                        "suffix": ""
                    },
                    {
                        "first": "Nicholas",
                        "middle": [],
                        "last": "Soares",
                        "suffix": ""
                    },
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Fitzgerald",
                        "suffix": ""
                    },
                    {
                        "first": "Tom",
                        "middle": [],
                        "last": "Ling",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Kwiatkowski",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of ACL",
                "volume": "",
                "issue": "",
                "pages": "2895--2905",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Livio Baldini Soares, Nicholas FitzGerald, Jeffrey Ling, and Tom Kwiatkowski. 2019. Matching the blanks: Distributional similarity for relation learn- ing. In Proceedings of ACL, pages 2895-2905.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Open information extraction from the web",
                "authors": [
                    {
                        "first": "Michele",
                        "middle": [],
                        "last": "Banko",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Michael",
                        "suffix": ""
                    },
                    {
                        "first": "Stephen",
                        "middle": [],
                        "last": "Cafarella",
                        "suffix": ""
                    },
                    {
                        "first": "Matthew",
                        "middle": [],
                        "last": "Soderland",
                        "suffix": ""
                    },
                    {
                        "first": "Oren",
                        "middle": [],
                        "last": "Broadhead",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Etzioni",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Proceedings of IJCAI",
                "volume": "",
                "issue": "",
                "pages": "2670--2676",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Michele Banko, Michael J Cafarella, Stephen Soder- land, Matthew Broadhead, and Oren Etzioni. 2007. Open information extraction from the web. In Pro- ceedings of IJCAI, pages 2670-2676.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Learning long-term dependencies with gradient descent is difficult",
                "authors": [
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    },
                    {
                        "first": "Patrice",
                        "middle": [],
                        "last": "Simard",
                        "suffix": ""
                    },
                    {
                        "first": "Paolo",
                        "middle": [],
                        "last": "Frasconi",
                        "suffix": ""
                    }
                ],
                "year": 1994,
                "venue": "IEEE transactions on neural networks",
                "volume": "5",
                "issue": "2",
                "pages": "157--166",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yoshua Bengio, Patrice Simard, and Paolo Frasconi. 1994. Learning long-term dependencies with gradi- ent descent is difficult. IEEE transactions on neural networks, 5(2):157-166.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Timedependent reorganization of brain circuitry underlying long-term memory storage",
                "authors": [
                    {
                        "first": "Bruno",
                        "middle": [],
                        "last": "Bontempi",
                        "suffix": ""
                    },
                    {
                        "first": "Catherine",
                        "middle": [],
                        "last": "Laurent-Demir",
                        "suffix": ""
                    },
                    {
                        "first": "Claude",
                        "middle": [],
                        "last": "Destrade",
                        "suffix": ""
                    },
                    {
                        "first": "Robert",
                        "middle": [],
                        "last": "Jaffard",
                        "suffix": ""
                    }
                ],
                "year": 1999,
                "venue": "Nature",
                "volume": "400",
                "issue": "6745",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Bruno Bontempi, Catherine Laurent-Demir, Claude Destrade, and Robert Jaffard. 1999. Time- dependent reorganization of brain circuitry un- derlying long-term memory storage. Nature, 400(6745):671.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Large-scale simple question answering with memory networks",
                "authors": [
                    {
                        "first": "Antoine",
                        "middle": [],
                        "last": "Bordes",
                        "suffix": ""
                    },
                    {
                        "first": "Nicolas",
                        "middle": [],
                        "last": "Usunier",
                        "suffix": ""
                    },
                    {
                        "first": "Sumit",
                        "middle": [],
                        "last": "Chopra",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Weston",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1506.02075"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Antoine Bordes, Nicolas Usunier, Sumit Chopra, and Jason Weston. 2015. Large-scale simple question answering with memory networks. arXiv preprint arXiv:1506.02075.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Causal evidence for the role of rem sleep theta rhythm in contextual memory consolidation",
                "authors": [
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Boyce",
                        "suffix": ""
                    },
                    {
                        "first": "Stephen",
                        "middle": [
                            "D"
                        ],
                        "last": "Glasgow",
                        "suffix": ""
                    },
                    {
                        "first": "Sylvain",
                        "middle": [],
                        "last": "Williams",
                        "suffix": ""
                    },
                    {
                        "first": "Antoine",
                        "middle": [],
                        "last": "Adamantidis",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Science",
                "volume": "352",
                "issue": "6287",
                "pages": "812--816",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Richard Boyce, Stephen D Glasgow, Sylvain Williams, and Antoine Adamantidis. 2016. Causal evidence for the role of rem sleep theta rhythm in contextual memory consolidation. Science, 352(6287):812- 816.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Learning to extract relations from the web using minimal supervision",
                "authors": [
                    {
                        "first": "Razvan",
                        "middle": [],
                        "last": "Bunescu",
                        "suffix": ""
                    },
                    {
                        "first": "Raymond",
                        "middle": [],
                        "last": "Mooney",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Proceedings of ACL",
                "volume": "",
                "issue": "",
                "pages": "576--583",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Razvan Bunescu and Raymond Mooney. 2007. Learn- ing to extract relations from the web using minimal supervision. In Proceedings of ACL, pages 576- 583.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Efficient lifelong learning with a-gem",
                "authors": [
                    {
                        "first": "Arslan",
                        "middle": [],
                        "last": "Chaudhry",
                        "suffix": ""
                    },
                    {
                        "first": "Marc'aurelio",
                        "middle": [],
                        "last": "Ranzato",
                        "suffix": ""
                    },
                    {
                        "first": "Marcus",
                        "middle": [],
                        "last": "Rohrbach",
                        "suffix": ""
                    },
                    {
                        "first": "Mohamed",
                        "middle": [],
                        "last": "Elhoseiny",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of ICLR",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Arslan Chaudhry, Marc'Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. 2019. Effi- cient lifelong learning with a-gem. In Proceedings of ICLR.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Net2Net: Accelerating learning via knowledge transfer",
                "authors": [
                    {
                        "first": "Tianqi",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Ian",
                        "middle": [],
                        "last": "Goodfellow",
                        "suffix": ""
                    },
                    {
                        "first": "Jonathon",
                        "middle": [],
                        "last": "Shlens",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of ICLR",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tianqi Chen, Ian Goodfellow, and Jonathon Shlens. 2016. Net2Net: Accelerating learning via knowl- edge transfer. In Proceedings of ICLR.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Neural open information extraction",
                "authors": [
                    {
                        "first": "Lei",
                        "middle": [],
                        "last": "Cui",
                        "suffix": ""
                    },
                    {
                        "first": "Furu",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Ming",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of ACL",
                "volume": "",
                "issue": "",
                "pages": "407--413",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Lei Cui, Furu Wei, and Ming Zhou. 2018. Neural open information extraction. In Proceedings of ACL, pages 407-413.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Episodic memory in lifelong language learning",
                "authors": [
                    {
                        "first": "Cyprien",
                        "middle": [],
                        "last": "De Masson D'autume",
                        "suffix": ""
                    },
                    {
                        "first": "Sebastian",
                        "middle": [],
                        "last": "Ruder",
                        "suffix": ""
                    },
                    {
                        "first": "Lingpeng",
                        "middle": [],
                        "last": "Kong",
                        "suffix": ""
                    },
                    {
                        "first": "Dani",
                        "middle": [],
                        "last": "Yogatama",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of NIPS",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Cyprien de Masson d'Autume, Sebastian Ruder, Ling- peng Kong, and Dani Yogatama. 2019. Episodic memory in lifelong language learning. In Proceed- ings of NIPS.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Clausie: Clause-based open information extraction",
                "authors": [
                    {
                        "first": "Luciano",
                        "middle": [],
                        "last": "Del",
                        "suffix": ""
                    },
                    {
                        "first": "Corro",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Rainer",
                        "middle": [],
                        "last": "Gemulla",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of WWW",
                "volume": "",
                "issue": "",
                "pages": "355--366",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Luciano Del Corro and Rainer Gemulla. 2013. Clausie: Clause-based open information extraction. In Pro- ceedings of WWW, pages 355-366.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
                "authors": [
                    {
                        "first": "Jacob",
                        "middle": [],
                        "last": "Devlin",
                        "suffix": ""
                    },
                    {
                        "first": "Ming-Wei",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "Kenton",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Kristina",
                        "middle": [],
                        "last": "Toutanova",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of NAACL-HLT",
                "volume": "",
                "issue": "",
                "pages": "4171--4186",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of NAACL-HLT, pages 4171-4186.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Question answering over Freebase with multicolumn convolutional neural networks",
                "authors": [
                    {
                        "first": "Li",
                        "middle": [],
                        "last": "Dong",
                        "suffix": ""
                    },
                    {
                        "first": "Furu",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Ming",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Ke",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of ACL-IJCNLP",
                "volume": "",
                "issue": "",
                "pages": "260--269",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Li Dong, Furu Wei, Ming Zhou, and Ke Xu. 2015. Question answering over Freebase with multi- column convolutional neural networks. In Proceed- ings of ACL-IJCNLP, pages 260-269.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Unsupervised open relation extraction",
                "authors": [
                    {
                        "first": "Hady",
                        "middle": [],
                        "last": "Elsahar",
                        "suffix": ""
                    },
                    {
                        "first": "Elena",
                        "middle": [],
                        "last": "Demidova",
                        "suffix": ""
                    },
                    {
                        "first": "Simon",
                        "middle": [],
                        "last": "Gottschalk",
                        "suffix": ""
                    },
                    {
                        "first": "Christophe",
                        "middle": [],
                        "last": "Gravier",
                        "suffix": ""
                    },
                    {
                        "first": "Frederique",
                        "middle": [],
                        "last": "Laforest",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of ESWC",
                "volume": "",
                "issue": "",
                "pages": "12--16",
                "other_ids": {
                    "DOI": [
                        "10.1007/978-3-319-70407-4_3"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Hady Elsahar, Elena Demidova, Simon Gottschalk, Christophe Gravier, and Frederique Laforest. 2017. Unsupervised open relation extraction. In Proceed- ings of ESWC, pages 12-16.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Unsupervised named-entity extraction from the web: An experimental study",
                "authors": [
                    {
                        "first": "Oren",
                        "middle": [],
                        "last": "Etzioni",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Cafarella",
                        "suffix": ""
                    },
                    {
                        "first": "Doug",
                        "middle": [],
                        "last": "Downey",
                        "suffix": ""
                    },
                    {
                        "first": "Ana-Maria",
                        "middle": [],
                        "last": "Popescu",
                        "suffix": ""
                    },
                    {
                        "first": "Tal",
                        "middle": [],
                        "last": "Shaked",
                        "suffix": ""
                    },
                    {
                        "first": "Stephen",
                        "middle": [],
                        "last": "Soderland",
                        "suffix": ""
                    },
                    {
                        "first": "Alexander",
                        "middle": [],
                        "last": "Daniel S Weld",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Yates",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "AI",
                "volume": "165",
                "issue": "",
                "pages": "91--134",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Oren Etzioni, Michael Cafarella, Doug Downey, Ana- Maria Popescu, Tal Shaked, Stephen Soderland, Daniel S Weld, and Alexander Yates. 2005. Unsu- pervised named-entity extraction from the web: An experimental study. AI, 165:91-134.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Identifying relations for open information extraction",
                "authors": [
                    {
                        "first": "Anthony",
                        "middle": [],
                        "last": "Fader",
                        "suffix": ""
                    },
                    {
                        "first": "Stephen",
                        "middle": [],
                        "last": "Soderland",
                        "suffix": ""
                    },
                    {
                        "first": "Oren",
                        "middle": [],
                        "last": "Etzioni",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Proceedings of EMNLP",
                "volume": "",
                "issue": "",
                "pages": "1535--1545",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Anthony Fader, Stephen Soderland, and Oren Etzioni. 2011. Identifying relations for open information ex- traction. In Proceedings of EMNLP, pages 1535- 1545.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "PathNet: Evolution channels gradient descent in super neural networks",
                "authors": [
                    {
                        "first": "Chrisantha",
                        "middle": [],
                        "last": "Fernando",
                        "suffix": ""
                    },
                    {
                        "first": "Dylan",
                        "middle": [],
                        "last": "Banarse",
                        "suffix": ""
                    },
                    {
                        "first": "Charles",
                        "middle": [],
                        "last": "Blundell",
                        "suffix": ""
                    },
                    {
                        "first": "Yori",
                        "middle": [],
                        "last": "Zwols",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Ha",
                        "suffix": ""
                    },
                    {
                        "first": "Andrei",
                        "middle": [
                            "A"
                        ],
                        "last": "Rusu",
                        "suffix": ""
                    },
                    {
                        "first": "Alexander",
                        "middle": [],
                        "last": "Pritzel",
                        "suffix": ""
                    },
                    {
                        "first": "Daan",
                        "middle": [],
                        "last": "Wierstra",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chrisantha Fernando, Dylan Banarse, Charles Blun- dell, Yori Zwols, David Ha, Andrei A Rusu, Alexan- der Pritzel, and Daan Wierstra. 2017. PathNet: Evo- lution channels gradient descent in super neural net- works. ai.google.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Neural snowball for few-shot relation learning",
                "authors": [
                    {
                        "first": "Tianyu",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Xu",
                        "middle": [],
                        "last": "Han",
                        "suffix": ""
                    },
                    {
                        "first": "Ruobing",
                        "middle": [],
                        "last": "Xie",
                        "suffix": ""
                    },
                    {
                        "first": "Zhiyuan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Fen",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Leyu",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Maosong",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of AAAI",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tianyu Gao, Xu Han, Ruobing Xie, Zhiyuan Liu, Fen Lin, Leyu Lin, and Maosong Sun. 2020. Neural snowball for few-shot relation learning. In Proceed- ings of AAAI.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Improved relation extraction with feature-rich compositional embedding models",
                "authors": [
                    {
                        "first": "Mo",
                        "middle": [],
                        "last": "Matthew R Gormley",
                        "suffix": ""
                    },
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Dredze",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of EMNLP",
                "volume": "",
                "issue": "",
                "pages": "1774--1784",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Matthew R Gormley, Mo Yu, and Mark Dredze. 2015. Improved relation extraction with feature-rich com- positional embedding models. In Proceedings of EMNLP, pages 1774-1784.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "2018a. Hierarchical relation extraction with coarse-to-fine grained attention",
                "authors": [
                    {
                        "first": "Xu",
                        "middle": [],
                        "last": "Han",
                        "suffix": ""
                    },
                    {
                        "first": "Pengfei",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Zhiyuan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Maosong",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Peng",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "Proceedings of EMNLP",
                "volume": "",
                "issue": "",
                "pages": "2236--2245",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xu Han, Pengfei Yu, Zhiyuan Liu, Maosong Sun, and Peng Li. 2018a. Hierarchical relation extraction with coarse-to-fine grained attention. In Proceed- ings of EMNLP, pages 2236-2245.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "FewRel: A large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation",
                "authors": [
                    {
                        "first": "Xu",
                        "middle": [],
                        "last": "Han",
                        "suffix": ""
                    },
                    {
                        "first": "Hao",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    },
                    {
                        "first": "Pengfei",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Ziyun",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Yuan",
                        "middle": [],
                        "last": "Yao",
                        "suffix": ""
                    },
                    {
                        "first": "Zhiyuan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Maosong",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of EMNLP",
                "volume": "",
                "issue": "",
                "pages": "4803--4809",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan Yao, Zhiyuan Liu, and Maosong Sun. 2018b. FewRel: A large-scale supervised few-shot relation classifica- tion dataset with state-of-the-art evaluation. In Pro- ceedings of EMNLP, pages 4803-4809.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Knowledgebased weak supervision for information extraction of overlapping relations",
                "authors": [
                    {
                        "first": "Raphael",
                        "middle": [],
                        "last": "Hoffmann",
                        "suffix": ""
                    },
                    {
                        "first": "Congle",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Xiao",
                        "middle": [],
                        "last": "Ling",
                        "suffix": ""
                    },
                    {
                        "first": "Luke",
                        "middle": [],
                        "last": "Zettlemoyer",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [
                            "S"
                        ],
                        "last": "Weld",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Proceedings of ACL",
                "volume": "",
                "issue": "",
                "pages": "541--550",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S Weld. 2011. Knowledge- based weak supervision for information extraction of overlapping relations. In Proceedings of ACL, pages 541-550.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Fearnet: Brain-inspired model for incremental learning",
                "authors": [
                    {
                        "first": "Ronald",
                        "middle": [],
                        "last": "Kemker",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [],
                        "last": "Kanan",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of ICLR",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ronald Kemker and Christopher Kanan. 2018. Fear- net: Brain-inspired model for incremental learning. In Proceedings of ICLR.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Overcoming catastrophic forgetting in neural networks. Proceedings of NAS",
                "authors": [
                    {
                        "first": "James",
                        "middle": [],
                        "last": "Kirkpatrick",
                        "suffix": ""
                    },
                    {
                        "first": "Razvan",
                        "middle": [],
                        "last": "Pascanu",
                        "suffix": ""
                    },
                    {
                        "first": "Neil",
                        "middle": [],
                        "last": "Rabinowitz",
                        "suffix": ""
                    },
                    {
                        "first": "Joel",
                        "middle": [],
                        "last": "Veness",
                        "suffix": ""
                    },
                    {
                        "first": "Guillaume",
                        "middle": [],
                        "last": "Desjardins",
                        "suffix": ""
                    },
                    {
                        "first": "Andrei",
                        "middle": [
                            "A"
                        ],
                        "last": "Rusu",
                        "suffix": ""
                    },
                    {
                        "first": "Kieran",
                        "middle": [],
                        "last": "Milan",
                        "suffix": ""
                    },
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Quan",
                        "suffix": ""
                    },
                    {
                        "first": "Tiago",
                        "middle": [],
                        "last": "Ramalho",
                        "suffix": ""
                    },
                    {
                        "first": "Agnieszka",
                        "middle": [],
                        "last": "Grabska-Barwinska",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "3521--3526",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Ag- nieszka Grabska-Barwinska, et al. 2017. Overcom- ing catastrophic forgetting in neural networks. Pro- ceedings of NAS, pages 3521-3526.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Independent cellular processes for hippocampal memory consolidation and reconsolidation",
                "authors": [
                    {
                        "first": "Jonathan Lc",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Barry",
                        "middle": [
                            "J"
                        ],
                        "last": "Everitt",
                        "suffix": ""
                    },
                    {
                        "first": "Kerrie",
                        "middle": [
                            "L"
                        ],
                        "last": "Thomas",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Science",
                "volume": "304",
                "issue": "5672",
                "pages": "839--843",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jonathan LC Lee, Barry J Everitt, and Kerrie L Thomas. 2004. Independent cellular processes for hippocampal memory consolidation and reconsoli- dation. Science, 304(5672):839-843.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Learning without forgetting",
                "authors": [
                    {
                        "first": "Zhizhong",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Derek",
                        "middle": [],
                        "last": "Hoiem",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "TPAMI",
                "volume": "40",
                "issue": "12",
                "pages": "2935--2947",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zhizhong Li and Derek Hoiem. 2017. Learning with- out forgetting. TPAMI, 40(12):2935-2947.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "Neural relation extraction with selective attention over instances",
                "authors": [
                    {
                        "first": "Yankai",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Shiqi",
                        "middle": [],
                        "last": "Shen",
                        "suffix": ""
                    },
                    {
                        "first": "Zhiyuan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Huanbo",
                        "middle": [],
                        "last": "Luan",
                        "suffix": ""
                    },
                    {
                        "first": "Maosong",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of ACL",
                "volume": "",
                "issue": "",
                "pages": "2124--2133",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan, and Maosong Sun. 2016. Neural relation extraction with selective attention over instances. In Proceed- ings of ACL, pages 2124-2133.",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "Convolution neural network for relation extraction",
                "authors": [
                    {
                        "first": "Chunyang",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Wenbo",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Wenhan",
                        "middle": [],
                        "last": "Chao",
                        "suffix": ""
                    },
                    {
                        "first": "Wanxiang",
                        "middle": [],
                        "last": "Che",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of ICDM",
                "volume": "",
                "issue": "",
                "pages": "231--242",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "ChunYang Liu, WenBo Sun, WenHan Chao, and Wanxiang Che. 2013. Convolution neural network for relation extraction. In Proceedings of ICDM, pages 231-242.",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "Rotate your networks: Better weight consolidation and less catastrophic forgetting",
                "authors": [
                    {
                        "first": "Xialei",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Marc",
                        "middle": [],
                        "last": "Masana",
                        "suffix": ""
                    },
                    {
                        "first": "Luis",
                        "middle": [],
                        "last": "Herranz",
                        "suffix": ""
                    },
                    {
                        "first": "Joost",
                        "middle": [],
                        "last": "Van De Weijer",
                        "suffix": ""
                    },
                    {
                        "first": "Antonio",
                        "middle": [
                            "M"
                        ],
                        "last": "Lopez",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [
                            "D"
                        ],
                        "last": "Bagdanov",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of ICPR",
                "volume": "",
                "issue": "",
                "pages": "2262--2268",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xialei Liu, Marc Masana, Luis Herranz, Joost Van de Weijer, Antonio M Lopez, and Andrew D Bagdanov. 2018. Rotate your networks: Better weight consoli- dation and less catastrophic forgetting. In Proceed- ings of ICPR, pages 2262-2268.",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "A dependency-based neural network for relation classification",
                "authors": [
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Furu",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Sujian",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Heng",
                        "middle": [],
                        "last": "Ji",
                        "suffix": ""
                    },
                    {
                        "first": "Ming",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Wang",
                        "middle": [],
                        "last": "Houfeng",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of ACL-IJCNLP",
                "volume": "",
                "issue": "",
                "pages": "285--290",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yang Liu, Furu Wei, Sujian Li, Heng Ji, Ming Zhou, and WANG Houfeng. 2015. A dependency-based neural network for relation classification. In Pro- ceedings of ACL-IJCNLP, pages 285-290.",
                "links": null
            },
            "BIBREF34": {
                "ref_id": "b34",
                "title": "Gradient episodic memory for continual learning",
                "authors": [
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Lopez",
                        "suffix": ""
                    },
                    {
                        "first": "-",
                        "middle": [],
                        "last": "Paz",
                        "suffix": ""
                    },
                    {
                        "first": "Marc'aurelio",
                        "middle": [],
                        "last": "Ranzato",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of NIPS",
                "volume": "",
                "issue": "",
                "pages": "6467--6476",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "David Lopez-Paz and Marc'Aurelio Ranzato. 2017. Gradient episodic memory for continual learning. In Proceedings of NIPS, pages 6467-6476.",
                "links": null
            },
            "BIBREF35": {
                "ref_id": "b35",
                "title": "Discretestate variational autoencoders for joint discovery and factorization of relations",
                "authors": [
                    {
                        "first": "Diego",
                        "middle": [],
                        "last": "Marcheggiani",
                        "suffix": ""
                    },
                    {
                        "first": "Ivan",
                        "middle": [],
                        "last": "Titov",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "TACL",
                "volume": "4",
                "issue": "",
                "pages": "231--244",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Diego Marcheggiani and Ivan Titov. 2016. Discrete- state variational autoencoders for joint discovery and factorization of relations. TACL, 4:231-244.",
                "links": null
            },
            "BIBREF36": {
                "ref_id": "b36",
                "title": "Open language learning for information extraction",
                "authors": [
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Mausam",
                        "suffix": ""
                    },
                    {
                        "first": "Stephen",
                        "middle": [],
                        "last": "Schmitz",
                        "suffix": ""
                    },
                    {
                        "first": "Robert",
                        "middle": [],
                        "last": "Soderland",
                        "suffix": ""
                    },
                    {
                        "first": "Oren",
                        "middle": [],
                        "last": "Bart",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Etzioni",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Proceedings of EMNLP-CoNLL",
                "volume": "",
                "issue": "",
                "pages": "523--534",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mausam, Michael Schmitz, Stephen Soderland, Robert Bart, and Oren Etzioni. 2012. Open language learn- ing for information extraction. In Proceedings of EMNLP-CoNLL, pages 523-534.",
                "links": null
            },
            "BIBREF37": {
                "ref_id": "b37",
                "title": "Open information extraction systems and downstream applications",
                "authors": [
                    {
                        "first": "Mausam",
                        "middle": [],
                        "last": "Mausam",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of IJCAI",
                "volume": "",
                "issue": "",
                "pages": "4074--4077",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mausam Mausam. 2016. Open information extraction systems and downstream applications. In Proceed- ings of IJCAI, pages 4074-4077.",
                "links": null
            },
            "BIBREF38": {
                "ref_id": "b38",
                "title": "Why there are complementary learning systems in the hippocampus and neocortex: Insights from the successes and failures of connectionist models of learning and memory",
                "authors": [
                    {
                        "first": "Bruce",
                        "middle": [
                            "L"
                        ],
                        "last": "James L Mcclelland",
                        "suffix": ""
                    },
                    {
                        "first": "Randall C O'",
                        "middle": [],
                        "last": "Mcnaughton",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Reilly",
                        "suffix": ""
                    }
                ],
                "year": 1995,
                "venue": "Psychological review",
                "volume": "102",
                "issue": "3",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "James L McClelland, Bruce L McNaughton, and Ran- dall C O'Reilly. 1995. Why there are complemen- tary learning systems in the hippocampus and neo- cortex: Insights from the successes and failures of connectionist models of learning and memory. Psy- chological review, 102(3):419.",
                "links": null
            },
            "BIBREF39": {
                "ref_id": "b39",
                "title": "Distant supervision for relation extraction without labeled data",
                "authors": [
                    {
                        "first": "Mike",
                        "middle": [],
                        "last": "Mintz",
                        "suffix": ""
                    },
                    {
                        "first": "Steven",
                        "middle": [],
                        "last": "Bills",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of ACL-IJCNLP",
                "volume": "",
                "issue": "",
                "pages": "1003--1011",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf- sky. 2009. Distant supervision for relation extrac- tion without labeled data. In Proceedings of ACL- IJCNLP, pages 1003-1011.",
                "links": null
            },
            "BIBREF40": {
                "ref_id": "b40",
                "title": "End-to-end relation extraction using LSTMs on sequences and tree structures",
                "authors": [
                    {
                        "first": "Makoto",
                        "middle": [],
                        "last": "Miwa",
                        "suffix": ""
                    },
                    {
                        "first": "Mohit",
                        "middle": [],
                        "last": "Bansal",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of ACL",
                "volume": "",
                "issue": "",
                "pages": "1105--1116",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Makoto Miwa and Mohit Bansal. 2016. End-to-end relation extraction using LSTMs on sequences and tree structures. In Proceedings of ACL, pages 1105- 1116.",
                "links": null
            },
            "BIBREF41": {
                "ref_id": "b41",
                "title": "Fear memories require protein synthesis in the amygdala for reconsolidation after retrieval",
                "authors": [
                    {
                        "first": "Karim",
                        "middle": [],
                        "last": "Nader",
                        "suffix": ""
                    },
                    {
                        "first": "Glenn",
                        "middle": [
                            "E"
                        ],
                        "last": "Schafe",
                        "suffix": ""
                    },
                    {
                        "first": "Joseph E Le",
                        "middle": [],
                        "last": "Doux",
                        "suffix": ""
                    }
                ],
                "year": 2000,
                "venue": "Nature",
                "volume": "406",
                "issue": "6797",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Karim Nader, Glenn E Schafe, and Joseph E Le Doux. 2000. Fear memories require protein synthesis in the amygdala for reconsolidation after retrieval. Nature, 406(6797):722.",
                "links": null
            },
            "BIBREF42": {
                "ref_id": "b42",
                "title": "Scalable knowledge harvesting with high precision and high recall",
                "authors": [
                    {
                        "first": "Ndapandula",
                        "middle": [],
                        "last": "Nakashole",
                        "suffix": ""
                    },
                    {
                        "first": "Martin",
                        "middle": [],
                        "last": "Theobald",
                        "suffix": ""
                    },
                    {
                        "first": "Gerhard",
                        "middle": [],
                        "last": "Weikum",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Proceedings of WSDM",
                "volume": "",
                "issue": "",
                "pages": "227--236",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ndapandula Nakashole, Martin Theobald, and Gerhard Weikum. 2011. Scalable knowledge harvesting with high precision and high recall. In Proceedings of WSDM, pages 227-236.",
                "links": null
            },
            "BIBREF43": {
                "ref_id": "b43",
                "title": "Espresso: Leveraging generic patterns for automatically harvesting semantic relations",
                "authors": [
                    {
                        "first": "Huu",
                        "middle": [],
                        "last": "Thien",
                        "suffix": ""
                    },
                    {
                        "first": "Ralph",
                        "middle": [],
                        "last": "Nguyen",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Grishman",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Proceedings of the NAACL Workshop on Vector Space Modeling for NLP",
                "volume": "",
                "issue": "",
                "pages": "113--120",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Thien Huu Nguyen and Ralph Grishman. 2015. Rela- tion extraction: Perspective from convolutional neu- ral networks. In Proceedings of the NAACL Work- shop on Vector Space Modeling for NLP, pages 39- 48. Patrick Pantel and Marco Pennacchiotti. 2006. Espresso: Leveraging generic patterns for automati- cally harvesting semantic relations. In Proceedings of COLING, pages 113-120.",
                "links": null
            },
            "BIBREF44": {
                "ref_id": "b44",
                "title": "Continual lifelong learning with neural networks: A review",
                "authors": [
                    {
                        "first": "Ronald",
                        "middle": [],
                        "last": "German I Parisi",
                        "suffix": ""
                    },
                    {
                        "first": "Jose",
                        "middle": [
                            "L"
                        ],
                        "last": "Kemker",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [],
                        "last": "Part",
                        "suffix": ""
                    },
                    {
                        "first": "Stefan",
                        "middle": [],
                        "last": "Kanan",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Wermter",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Neural Networks",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "German I Parisi, Ronald Kemker, Jose L Part, Christo- pher Kanan, and Stefan Wermter. 2019. Continual lifelong learning with neural networks: A review. Neural Networks.",
                "links": null
            },
            "BIBREF45": {
                "ref_id": "b45",
                "title": "CORE: Context-aware open relation extraction with factorization machines",
                "authors": [
                    {
                        "first": "Fabio",
                        "middle": [],
                        "last": "Petroni",
                        "suffix": ""
                    },
                    {
                        "first": "Luciano",
                        "middle": [],
                        "last": "Del Corro",
                        "suffix": ""
                    },
                    {
                        "first": "Rainer",
                        "middle": [],
                        "last": "Gemulla",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "1763--1773",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Fabio Petroni, Luciano Del Corro, and Rainer Gemulla. 2015. CORE: Context-aware open relation extrac- tion with factorization machines. In Proceedings of the 2015 Conference on Empirical Methods in Nat- ural Language Processing, pages 1763-1773.",
                "links": null
            },
            "BIBREF46": {
                "ref_id": "b46",
                "title": "iCaRL: Incremental classifier and representation learning",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Sylvestre-Alvise",
                        "suffix": ""
                    },
                    {
                        "first": "Alexander",
                        "middle": [],
                        "last": "Rebuffi",
                        "suffix": ""
                    },
                    {
                        "first": "Georg",
                        "middle": [],
                        "last": "Kolesnikov",
                        "suffix": ""
                    },
                    {
                        "first": "Christoph",
                        "middle": [
                            "H"
                        ],
                        "last": "Sperl",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Lampert",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of CVPR",
                "volume": "",
                "issue": "",
                "pages": "2001--2010",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. 2017. iCaRL: Incremental classifier and representa- tion learning. In Proceedings of CVPR, pages 2001-2010.",
                "links": null
            },
            "BIBREF47": {
                "ref_id": "b47",
                "title": "Modeling relations and their mentions without labeled text",
                "authors": [
                    {
                        "first": "Sebastian",
                        "middle": [],
                        "last": "Riedel",
                        "suffix": ""
                    },
                    {
                        "first": "Limin",
                        "middle": [],
                        "last": "Yao",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Mccallum",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of ECML-PKDD",
                "volume": "",
                "issue": "",
                "pages": "148--163",
                "other_ids": {
                    "DOI": [
                        "10.1007/978-3-642-15939-8_10"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling relations and their mentions with- out labeled text. In Proceedings of ECML-PKDD, pages 148-163.",
                "links": null
            },
            "BIBREF48": {
                "ref_id": "b48",
                "title": "Relation extraction with matrix factorization and universal schemas",
                "authors": [
                    {
                        "first": "Sebastian",
                        "middle": [],
                        "last": "Riedel",
                        "suffix": ""
                    },
                    {
                        "first": "Limin",
                        "middle": [],
                        "last": "Yao",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Mccallum",
                        "suffix": ""
                    },
                    {
                        "first": "Benjamin",
                        "middle": [
                            "M"
                        ],
                        "last": "Marlin",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of NAACL",
                "volume": "",
                "issue": "",
                "pages": "74--84",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sebastian Riedel, Limin Yao, Andrew McCallum, and Benjamin M Marlin. 2013. Relation extraction with matrix factorization and universal schemas. In Pro- ceedings of NAACL, pages 74-84.",
                "links": null
            },
            "BIBREF49": {
                "ref_id": "b49",
                "title": "Learning dictionaries for information extraction by multi-level bootstrapping",
                "authors": [
                    {
                        "first": "Ellen",
                        "middle": [],
                        "last": "Riloff",
                        "suffix": ""
                    },
                    {
                        "first": "Rosie",
                        "middle": [],
                        "last": "Jones",
                        "suffix": ""
                    }
                ],
                "year": 1999,
                "venue": "Proceedings of AAAI",
                "volume": "",
                "issue": "",
                "pages": "474--479",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ellen Riloff, Rosie Jones, et al. 1999. Learning dic- tionaries for information extraction by multi-level bootstrapping. In Proceedings of AAAI, pages 474- 479.",
                "links": null
            },
            "BIBREF50": {
                "ref_id": "b50",
                "title": "Continual learning in reinforcement environments",
                "authors": [],
                "year": 1994,
                "venue": "",
                "volume": "78712",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mark Bishop Ring. 1994. Continual learning in rein- forcement environments. Ph.D. thesis, University of Texas at Austin Austin, Texas 78712.",
                "links": null
            },
            "BIBREF51": {
                "ref_id": "b51",
                "title": "Online structured laplace approximations for overcoming catastrophic forgetting",
                "authors": [
                    {
                        "first": "Hippolyt",
                        "middle": [],
                        "last": "Ritter",
                        "suffix": ""
                    },
                    {
                        "first": "Aleksandar",
                        "middle": [],
                        "last": "Botev",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Barber",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of NIPS",
                "volume": "",
                "issue": "",
                "pages": "3738--3748",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Hippolyt Ritter, Aleksandar Botev, and David Barber. 2018. Online structured laplace approximations for overcoming catastrophic forgetting. In Proceedings of NIPS, pages 3738-3748.",
                "links": null
            },
            "BIBREF52": {
                "ref_id": "b52",
                "title": "Selfsupervised relation extraction from the web",
                "authors": [
                    {
                        "first": "Benjamin",
                        "middle": [],
                        "last": "Rozenfeld",
                        "suffix": ""
                    },
                    {
                        "first": "Ronen",
                        "middle": [],
                        "last": "Feldman",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Proceedings of KAIS",
                "volume": "17",
                "issue": "1",
                "pages": "17--33",
                "other_ids": {
                    "DOI": [
                        "10.1007/s10115-007-0110-6"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Benjamin Rozenfeld and Ronen Feldman. 2008. Self- supervised relation extraction from the web. Pro- ceedings of KAIS, 17(1):17-33.",
                "links": null
            },
            "BIBREF54": {
                "ref_id": "b54",
                "title": "Classifying relations by ranking with convolutional neural networks",
                "authors": [
                    {
                        "first": "Bing",
                        "middle": [],
                        "last": "Cicero Dos Santos",
                        "suffix": ""
                    },
                    {
                        "first": "Bowen",
                        "middle": [],
                        "last": "Xiang",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of ACL-IJCNLP",
                "volume": "",
                "issue": "",
                "pages": "626--634",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Cicero dos Santos, Bing Xiang, and Bowen Zhou. 2015. Classifying relations by ranking with convo- lutional neural networks. In Proceedings of ACL- IJCNLP, pages 626-634.",
                "links": null
            },
            "BIBREF55": {
                "ref_id": "b55",
                "title": "Modeling relational data with graph convolutional networks",
                "authors": [
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Schlichtkrull",
                        "suffix": ""
                    },
                    {
                        "first": "Thomas",
                        "middle": [
                            "N"
                        ],
                        "last": "Kipf",
                        "suffix": ""
                    },
                    {
                        "first": "Peter",
                        "middle": [],
                        "last": "Bloem",
                        "suffix": ""
                    },
                    {
                        "first": "Rianne",
                        "middle": [],
                        "last": "Van Den",
                        "suffix": ""
                    },
                    {
                        "first": "Ivan",
                        "middle": [],
                        "last": "Berg",
                        "suffix": ""
                    },
                    {
                        "first": "Max",
                        "middle": [],
                        "last": "Titov",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Welling",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of ESWC",
                "volume": "",
                "issue": "",
                "pages": "593--607",
                "other_ids": {
                    "DOI": [
                        "10.1007/978-3-319-93417-4_38"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max Welling. 2018. Modeling relational data with graph convo- lutional networks. In Proceedings of ESWC, pages 593-607.",
                "links": null
            },
            "BIBREF56": {
                "ref_id": "b56",
                "title": "Continual learning with deep generative replay",
                "authors": [
                    {
                        "first": "Hanul",
                        "middle": [],
                        "last": "Shin",
                        "suffix": ""
                    },
                    {
                        "first": "Jung",
                        "middle": [],
                        "last": "Kwon Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Jaehong",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "Jiwon",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of NIPS",
                "volume": "",
                "issue": "",
                "pages": "2990--2999",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. 2017. Continual learning with deep generative replay. In Proceedings of NIPS, pages 2990-2999.",
                "links": null
            },
            "BIBREF57": {
                "ref_id": "b57",
                "title": "Preemptive information extraction using unrestricted relation discovery",
                "authors": [
                    {
                        "first": "Yusuke",
                        "middle": [],
                        "last": "Shinyama",
                        "suffix": ""
                    },
                    {
                        "first": "Satoshi",
                        "middle": [],
                        "last": "Sekine",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Proceedings of NAACL-HLT",
                "volume": "",
                "issue": "",
                "pages": "304--311",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yusuke Shinyama and Satoshi Sekine. 2006. Preemp- tive information extraction using unrestricted rela- tion discovery. In Proceedings of NAACL-HLT, pages 304-311.",
                "links": null
            },
            "BIBREF58": {
                "ref_id": "b58",
                "title": "Semantic compositionality through recursive matrix-vector spaces",
                "authors": [
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    },
                    {
                        "first": "Brody",
                        "middle": [],
                        "last": "Huval",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [
                            "Y"
                        ],
                        "last": "Ng",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Proceedings of EMNLP",
                "volume": "",
                "issue": "",
                "pages": "1201--1211",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Richard Socher, Brody Huval, Christopher D Manning, and Andrew Y Ng. 2012. Semantic compositional- ity through recursive matrix-vector spaces. In Pro- ceedings of EMNLP, pages 1201-1211.",
                "links": null
            },
            "BIBREF59": {
                "ref_id": "b59",
                "title": "Creating a large benchmark for open information extraction",
                "authors": [
                    {
                        "first": "Gabriel",
                        "middle": [],
                        "last": "Stanovsky",
                        "suffix": ""
                    },
                    {
                        "first": "Ido",
                        "middle": [],
                        "last": "Dagan",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of EMNLP",
                "volume": "",
                "issue": "",
                "pages": "2300--2305",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Gabriel Stanovsky and Ido Dagan. 2016. Creating a large benchmark for open information extraction. In Proceedings of EMNLP, pages 2300-2305.",
                "links": null
            },
            "BIBREF60": {
                "ref_id": "b60",
                "title": "LAMAL: Language modeling is all you need for lifelong language learning",
                "authors": [
                    {
                        "first": "Fan-Keng",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Cheng-Hao",
                        "middle": [],
                        "last": "Ho",
                        "suffix": ""
                    },
                    {
                        "first": "Hung-Yi",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1909.03329"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Fan-Keng Sun, Cheng-Hao Ho, and Hung-Yi Lee. 2019. LAMAL: Language modeling is all you need for lifelong language learning. arXiv preprint arXiv:1909.03329.",
                "links": null
            },
            "BIBREF61": {
                "ref_id": "b61",
                "title": "Learning to learn",
                "authors": [
                    {
                        "first": "Sebastian",
                        "middle": [],
                        "last": "Thrun",
                        "suffix": ""
                    },
                    {
                        "first": "Lorien",
                        "middle": [],
                        "last": "Pratt",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sebastian Thrun and Lorien Pratt. 2012. Learning to learn. Springer Science & Business Media.",
                "links": null
            },
            "BIBREF62": {
                "ref_id": "b62",
                "title": "Sleep function and synaptic homeostasis",
                "authors": [
                    {
                        "first": "Giulio",
                        "middle": [],
                        "last": "Tononi",
                        "suffix": ""
                    },
                    {
                        "first": "Chiara",
                        "middle": [],
                        "last": "Cirelli",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Sleep medicine reviews",
                "volume": "10",
                "issue": "1",
                "pages": "49--62",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Giulio Tononi and Chiara Cirelli. 2006. Sleep function and synaptic homeostasis. Sleep medicine reviews, 10(1):49-62.",
                "links": null
            },
            "BIBREF63": {
                "ref_id": "b63",
                "title": "Sentence embedding alignment for lifelong relation extraction",
                "authors": [
                    {
                        "first": "Hong",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Wenhan",
                        "middle": [],
                        "last": "Xiong",
                        "suffix": ""
                    },
                    {
                        "first": "Mo",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaoxiao",
                        "middle": [],
                        "last": "Guo",
                        "suffix": ""
                    },
                    {
                        "first": "Shiyu",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "William",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Wang",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of NAACL-HLT",
                "volume": "",
                "issue": "",
                "pages": "796--806",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Hong Wang, Wenhan Xiong, Mo Yu, Xiaoxiao Guo, Shiyu Chang, and William Yang Wang. 2019. Sen- tence embedding alignment for lifelong relation ex- traction. In Proceedings of NAACL-HLT, pages 796-806.",
                "links": null
            },
            "BIBREF64": {
                "ref_id": "b64",
                "title": "Open relation extraction: Relational knowledge transfer from supervised data to unsupervised data",
                "authors": [
                    {
                        "first": "Ruidong",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Yuan",
                        "middle": [],
                        "last": "Yao",
                        "suffix": ""
                    },
                    {
                        "first": "Xu",
                        "middle": [],
                        "last": "Han",
                        "suffix": ""
                    },
                    {
                        "first": "Ruobing",
                        "middle": [],
                        "last": "Xie",
                        "suffix": ""
                    },
                    {
                        "first": "Zhiyuan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Fen",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Leyu",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Maosong",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of EMNLP-IJCNLP",
                "volume": "",
                "issue": "",
                "pages": "219--228",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ruidong Wu, Yuan Yao, Xu Han, Ruobing Xie, Zhiyuan Liu, Fen Lin, Leyu Lin, and Maosong Sun. 2019. Open relation extraction: Relational knowl- edge transfer from supervised data to unsupervised data. In Proceedings of EMNLP-IJCNLP, pages 219-228.",
                "links": null
            },
            "BIBREF65": {
                "ref_id": "b65",
                "title": "Explicit semantic ranking for academic search via knowledge graph embedding",
                "authors": [
                    {
                        "first": "Chenyan",
                        "middle": [],
                        "last": "Xiong",
                        "suffix": ""
                    },
                    {
                        "first": "Russell",
                        "middle": [],
                        "last": "Power",
                        "suffix": ""
                    },
                    {
                        "first": "Jamie",
                        "middle": [],
                        "last": "Callan",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of WWW",
                "volume": "",
                "issue": "",
                "pages": "1271--1279",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chenyan Xiong, Russell Power, and Jamie Callan. 2017. Explicit semantic ranking for academic search via knowledge graph embedding. In Pro- ceedings of WWW, pages 1271-1279.",
                "links": null
            },
            "BIBREF66": {
                "ref_id": "b66",
                "title": "Classifying relations via long short term memory networks along shortest dependency paths",
                "authors": [
                    {
                        "first": "Yan",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Lili",
                        "middle": [],
                        "last": "Mou",
                        "suffix": ""
                    },
                    {
                        "first": "Ge",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Yunchuan",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Hao",
                        "middle": [],
                        "last": "Peng",
                        "suffix": ""
                    },
                    {
                        "first": "Zhi",
                        "middle": [],
                        "last": "Jin",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of EMNLP",
                "volume": "",
                "issue": "",
                "pages": "1785--1794",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yan Xu, Lili Mou, Ge Li, Yunchuan Chen, Hao Peng, and Zhi Jin. 2015. Classifying relations via long short term memory networks along shortest depen- dency paths. In Proceedings of EMNLP, pages 1785-1794.",
                "links": null
            },
            "BIBREF67": {
                "ref_id": "b67",
                "title": "Sleep promotes branch-specific formation of dendritic spines after learning",
                "authors": [
                    {
                        "first": "Guang",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Cora",
                        "middle": [],
                        "last": "Sau",
                        "suffix": ""
                    },
                    {
                        "first": "Wan",
                        "middle": [],
                        "last": "Lai",
                        "suffix": ""
                    },
                    {
                        "first": "Joseph",
                        "middle": [],
                        "last": "Cichon",
                        "suffix": ""
                    },
                    {
                        "first": "Lei",
                        "middle": [],
                        "last": "Ma",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Wen-Biao",
                        "middle": [],
                        "last": "Gan",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Science",
                "volume": "344",
                "issue": "6188",
                "pages": "1173--1178",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Guang Yang, Cora Sau Wan Lai, Joseph Cichon, Lei Ma, Wei Li, and Wen-Biao Gan. 2014. Sleep pro- motes branch-specific formation of dendritic spines after learning. Science, 344(6188):1173-1178.",
                "links": null
            },
            "BIBREF68": {
                "ref_id": "b68",
                "title": "Structured relation discovery using generative models",
                "authors": [
                    {
                        "first": "Limin",
                        "middle": [],
                        "last": "Yao",
                        "suffix": ""
                    },
                    {
                        "first": "Aria",
                        "middle": [],
                        "last": "Haghighi",
                        "suffix": ""
                    },
                    {
                        "first": "Sebastian",
                        "middle": [],
                        "last": "Riedel",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Mccallum",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Proceedings of EMNLP",
                "volume": "",
                "issue": "",
                "pages": "1456--1466",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Limin Yao, Aria Haghighi, Sebastian Riedel, and An- drew McCallum. 2011. Structured relation discov- ery using generative models. In Proceedings of EMNLP, pages 1456-1466.",
                "links": null
            },
            "BIBREF69": {
                "ref_id": "b69",
                "title": "Improved neural relation detection for knowledge base question answering",
                "authors": [
                    {
                        "first": "Mo",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Wenpeng",
                        "middle": [],
                        "last": "Yin",
                        "suffix": ""
                    },
                    {
                        "first": "Kazi",
                        "middle": [],
                        "last": "Saidul Hasan",
                        "suffix": ""
                    },
                    {
                        "first": "Bing",
                        "middle": [],
                        "last": "Cicero Dos Santos",
                        "suffix": ""
                    },
                    {
                        "first": "Bowen",
                        "middle": [],
                        "last": "Xiang",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of ACL",
                "volume": "",
                "issue": "",
                "pages": "571--581",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mo Yu, Wenpeng Yin, Kazi Saidul Hasan, Cicero dos Santos, Bing Xiang, and Bowen Zhou. 2017. Im- proved neural relation detection for knowledge base question answering. In Proceedings of ACL, pages 571-581.",
                "links": null
            },
            "BIBREF70": {
                "ref_id": "b70",
                "title": "Kernel methods for relation extraction",
                "authors": [
                    {
                        "first": "Dmitry",
                        "middle": [],
                        "last": "Zelenko",
                        "suffix": ""
                    },
                    {
                        "first": "Chinatsu",
                        "middle": [],
                        "last": "Aone",
                        "suffix": ""
                    },
                    {
                        "first": "Anthony",
                        "middle": [],
                        "last": "Richardella",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Proceedings of JMLR",
                "volume": "",
                "issue": "",
                "pages": "1083--1106",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dmitry Zelenko, Chinatsu Aone, and Anthony Richardella. 2003. Kernel methods for relation ex- traction. Proceedings of JMLR, pages 1083-1106.",
                "links": null
            },
            "BIBREF71": {
                "ref_id": "b71",
                "title": "Distant supervision for relation extraction via piecewise convolutional neural networks",
                "authors": [
                    {
                        "first": "Daojian",
                        "middle": [],
                        "last": "Zeng",
                        "suffix": ""
                    },
                    {
                        "first": "Kang",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Yubo",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Jun",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of EMNLP",
                "volume": "",
                "issue": "",
                "pages": "1753--1762",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao. 2015. Distant supervision for relation extraction via piecewise convolutional neural networks. In Pro- ceedings of EMNLP, pages 1753-1762.",
                "links": null
            },
            "BIBREF72": {
                "ref_id": "b72",
                "title": "Relation classification via convolutional deep neural network",
                "authors": [
                    {
                        "first": "Daojian",
                        "middle": [],
                        "last": "Zeng",
                        "suffix": ""
                    },
                    {
                        "first": "Kang",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Siwei",
                        "middle": [],
                        "last": "Lai",
                        "suffix": ""
                    },
                    {
                        "first": "Guangyou",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Jun",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of COLING",
                "volume": "",
                "issue": "",
                "pages": "2335--2344",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou, and Jun Zhao. 2014. Relation classification via con- volutional deep neural network. In Proceedings of COLING, pages 2335-2344.",
                "links": null
            },
            "BIBREF73": {
                "ref_id": "b73",
                "title": "Continual learning through synaptic intelligence",
                "authors": [
                    {
                        "first": "Friedemann",
                        "middle": [],
                        "last": "Zenke",
                        "suffix": ""
                    },
                    {
                        "first": "Ben",
                        "middle": [],
                        "last": "Poole",
                        "suffix": ""
                    },
                    {
                        "first": "Surya",
                        "middle": [],
                        "last": "Ganguli",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of ICML",
                "volume": "",
                "issue": "",
                "pages": "3987--3995",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Friedemann Zenke, Ben Poole, and Surya Ganguli. 2017. Continual learning through synaptic intelli- gence. In Proceedings of ICML, pages 3987-3995.",
                "links": null
            },
            "BIBREF74": {
                "ref_id": "b74",
                "title": "Positionaware attention and supervised data improve slot filling",
                "authors": [
                    {
                        "first": "Yuhao",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Victor",
                        "middle": [],
                        "last": "Zhong",
                        "suffix": ""
                    },
                    {
                        "first": "Danqi",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Gabor",
                        "middle": [],
                        "last": "Angeli",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of EMNLP",
                "volume": "",
                "issue": "",
                "pages": "35--45",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yuhao Zhang, Victor Zhong, Danqi Chen, Gabor An- geli, and Christopher D Manning. 2017. Position- aware attention and supervised data improve slot fill- ing. In Proceedings of EMNLP, pages 35-45.",
                "links": null
            },
            "BIBREF75": {
                "ref_id": "b75",
                "title": "Exploring various knowledge in relation extraction",
                "authors": [
                    {
                        "first": "Guodong",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Jian",
                        "middle": [],
                        "last": "Su",
                        "suffix": ""
                    },
                    {
                        "first": "Jie",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Min",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of ACL",
                "volume": "",
                "issue": "",
                "pages": "427--434",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "GuoDong Zhou, Jian Su, Jie Zhang, and Min Zhang. 2005. Exploring various knowledge in relation ex- traction. In Proceedings of ACL, pages 427-434.",
                "links": null
            },
            "BIBREF76": {
                "ref_id": "b76",
                "title": "StatSnowball: A statistical approach to extracting entity relationships",
                "authors": [
                    {
                        "first": "Jun",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    },
                    {
                        "first": "Zaiqing",
                        "middle": [],
                        "last": "Nie",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaojiang",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Bo",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Ji-Rong",
                        "middle": [],
                        "last": "Wen",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of WWW",
                "volume": "",
                "issue": "",
                "pages": "101--110",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jun Zhu, Zaiqing Nie, Xiaojiang Liu, Bo Zhang, and Ji-Rong Wen. 2009. StatSnowball: A statistical ap- proach to extracting entity relationships. In Pro- ceedings of WWW, pages 101-110.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 1: The whole pipeline to detect and learn new relations in an open scenario.",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Figure 2: A simple example of continually learning four tasks (each task has only one relation: A, B, C, D respectively) to demonstrate the overall framework of episodic memory activation and reconsolidation during continual relation learning. The purple solid lines and dotted lines represent the forward and backward propagation respectively. The black dotted lines represent the data flow.",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "Figure 3: Changes in accuracy (%) with increasing tasks through the continual learning process.",
                "uris": null,
                "fig_num": "3",
                "type_str": "figure"
            },
            "FIGREF3": {
                "num": null,
                "text": ".0 33.8 39.0 41.2 47.5 69.1 66.1 72.2 69.2 76.2 73.1 14.7 14.5 15.0 15.5 15.7 16.0 EMR 42.0 54.1 49.0 60.5 53.6 65.1 81.5 77.4 84.9 81.0 86.9 82.9 21.8 26.5 25.7 31.6 28.7 35.6 EA-EMR 49.0 61.2 54.9 66.4 59.1 69.9 83.3 78.7 86.4 82.0 87.9 83.5 23.0 30.0 27.7 37.0 30.5 40.5 EMAR 53.8 69.1 62.5 74.9 66.0 77.9 80.9 78.7 84.6 81.4 85.2 83.7 31.0 36.3 37.8 48.5 44.5 54.4",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF4": {
                "num": null,
                "text": "(4) EMR (Parisi et al., 2019), a basic memorybased method, which memorizes a few historical examples and simply conduct memory replay. Every time a new task comes in, EMR mixes memorized examples and new examples together to fine-tune models; (",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF5": {
                "num": null,
                "text": "Figure 4: A visualization of features learnt by EA-EMR and EMAR at different training steps on FewRel. For each image, we use the support vector machine to acquire its best linear boundary and draw it as the blue line.",
                "uris": null,
                "fig_num": "4",
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table><tr><td>|R k |</td></tr></table>",
                "type_str": "table",
                "text": "), we store a few examples from T k into the memory M k . More specifically, we select informative and diverse examples from T k to cover new relation patterns as much as possible, which can make the memory effectively approximate the feature distribution of relations.After encoding all examples of the k-th taskT k into {x T k 1 , . . . , x T k N },we apply K-Means to cluster these example embeddings, where the number of clusters is the memory size B. Then, for each cluster, we select the example closest to the cluster centroid and record which relation these selected examples belong to. We denote this selected example set C k . By counting the example number in C k for each relation, we can describe the relation importance in this task: more selected examples of a relation indicates more importance. As the limited memory size, for those more important relations, we select at least B |R k | examples, yet for those less important ones, we select at most B",
                "html": null,
                "num": null
            },
            "TABREF1": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Algorithm 1 Train EMAR for the k-th task Require: The training set T k of the k-th task Require: The emerging relation set R k of the k-th task Require: The memory module Mk-1 before learning T k Require: The known relation set Rk-1 before learning T k 1: Initialize the relation embeddings for R k 2: Rk \u2190 Rk-1 \u222a R k 3: for i \u2190 1 to epoch1 do 4: Update \u03b8 with \u2207L on T k 5: end for 6: Select informative examples from T k to store into M k",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table><tr><td/><td colspan=\"2\">FewRel</td><td colspan=\"2\">SimpleQ</td><td colspan=\"2\">TACRED</td></tr><tr><td/><td>W</td><td>A</td><td>W</td><td>A</td><td>W</td><td>A</td></tr><tr><td colspan=\"6\">Lower Bound 18.9 20.8 63.2 56.9 12.3</td><td>9.5</td></tr><tr><td>EWC</td><td colspan=\"6\">27.1 30.2 67.2 59.0 14.5 14.5</td></tr><tr><td>GEM</td><td colspan=\"4\">49.2 59.8 84.1 79.6</td><td>-</td><td>-</td></tr><tr><td>AGEM</td><td colspan=\"6\">36.1 42.5 77.6 72.2 15.7 16.0</td></tr><tr><td>EMR</td><td colspan=\"6\">51.0 62.0 85.2 80.8 28.7 35.6</td></tr><tr><td>EA-EMR</td><td colspan=\"6\">56.6 67.3 87.8 82.4 30.5 40.5</td></tr><tr><td>EMAR</td><td colspan=\"6\">66.0 77.9 85.2 83.7 44.5 54.4</td></tr><tr><td colspan=\"7\">Upper Bound 81.9 85.8 88.9 84.1 74.3 77.0</td></tr></table>",
                "type_str": "table",
                "text": "Accuracy (%) of models on three benchmarks. \"W\" stands for the Whole performance, and \"A\" stands for the Average performance. The results of FewRel and SimpleQ come fromWang et al. (2019). The result of TACRED comes from our implemented models.",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table><tr><td/><td colspan=\"4\">-1 Step-4 Step-7 Step-10</td></tr><tr><td>EA-EMR</td><td>98.8</td><td>65.0</td><td>78.8</td><td>73.8</td></tr><tr><td>EMAR</td><td>92.5</td><td>75.0</td><td>87.5</td><td>80.0</td></tr></table>",
                "type_str": "table",
                "text": "Classification accuracy (%) based on the features learnt by EA-EMR and EMAR in Figure",
                "html": null,
                "num": null
            }
        }
    }
}